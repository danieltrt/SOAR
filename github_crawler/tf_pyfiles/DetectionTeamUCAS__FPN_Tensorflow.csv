file_path,api_count,code
data/__init__.py,0,b''
help_utils/__init__.py,0,b''
help_utils/tools.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport math\nimport sys\nimport os\n\n\ndef view_bar(message, num, total):\n    rate = num / total\n    rate_num = int(rate * 40)\n    rate_nums = math.ceil(rate * 100)\n    r = \'\\r%s:[%s%s]%d%%\\t%d/%d\' % (message, "">"" * rate_num, "" "" * (40 - rate_num), rate_nums, num, total,)\n    sys.stdout.write(r)\n    sys.stdout.flush()\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)'"
libs/__init__.py,0,b''
libs/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport subprocess\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    # Extension(\n    #     ""utils.cython_bbox"",\n    #     [""utils/bbox.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    # Extension(\n    #     ""nms.cpu_nms"",\n    #     [""nms/cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_cython_nms"",\n    #     [""rotation/rotate_cython_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_circle_nms"",\n    #     [""rotation/rotate_circle_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\'nms.gpu_nms\',\n    #     [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    # Extension(\'rotation.rotate_gpu_nms\',\n    #     [\'rotation/rotate_nms_kernel.cu\', \'rotation/rotate_gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc anrbd not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    Extension(\'rotation.rbbox_overlaps\',\n        [\'rotation/rbbox_overlaps_kernel.cu\', \'rotation/rbbox_overlaps.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n    # Extension(\'rotation.rotate_polygon_nms\',\n    #     [\'rotation/rotate_polygon_nms_kernel.cu\', \'rotation/rotate_polygon_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    #\n    # Extension(\n    #     \'pycocotools._mask\',\n    #     sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n    #     include_dirs = [numpy_include, \'pycocotools\'],\n    #     extra_compile_args={\n    #         \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    # ),\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
tools/__init__.py,0,b''
tools/eval.py,9,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\n\n\ndef eval_with_plac(det_net, real_test_imgname_list, img_root, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        all_boxes = []\n        for i, a_img_name in enumerate(real_test_imgname_list):\n\n            raw_img = cv2.imread(os.path.join(img_root, a_img_name))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(np.squeeze(resized_img, 0),\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + a_img_name + \'.jpg\',\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\n            dets = np.hstack((detected_categories.reshape(-1, 1),\n                              detected_scores.reshape(-1, 1),\n                              boxes))\n            all_boxes.append(dets)\n\n            tools.view_bar(\'{} image cost {}s\'.format(a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\n\n        # save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n        # if not os.path.exists(save_dir):\n        #     os.makedirs(save_dir)\n        # fw1 = open(os.path.join(save_dir, \'detections.pkl\'), \'wb\')\n        # pickle.dump(all_boxes, fw1)\n        return all_boxes\n\n\ndef eval(num_imgs, eval_dir, annotation_dir, showbox):\n\n    # with open(\'/home/yjr/DataSet/VOC/VOC_test/VOC2007/ImageSets/Main/aeroplane_test.txt\') as f:\n    #     all_lines = f.readlines()\n    # test_imgname_list = [a_line.split()[0].strip() for a_line in all_lines]\n\n    test_imgname_list = [item for item in os.listdir(eval_dir)\n                              if item.endswith((\'.jpg\', \'jpeg\', \'.png\', \'.tif\', \'.tiff\'))]\n    if num_imgs == np.inf:\n        real_test_imgname_list = test_imgname_list\n    else:\n        real_test_imgname_list = test_imgname_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    all_boxes = eval_with_plac(det_net=faster_rcnn, real_test_imgname_list=real_test_imgname_list,\n                   img_root=eval_dir,\n                   draw_imgs=showbox)\n\n    # save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n    # if not os.path.exists(save_dir):\n    #     os.makedirs(save_dir)\n    # with open(os.path.join(save_dir, \'detections.pkl\'), \'rb\') as f:\n    #     all_boxes = pickle.load(f)\n    #\n    #     print(len(all_boxes))\n\n    voc_eval.voc_evaluate_detections(all_boxes=all_boxes,\n                                     test_annotation_path=annotation_dir,\n                                     test_imgid_list=real_test_imgname_list)\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_imgs\', dest=\'eval_imgs\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/VOC/VOC_test/VOC2007/JPEGImages\', type=str)\n    parser.add_argument(\'--annotation_dir\', dest=\'test_annotation_dir\',\n                        help=\'the dir save annotations\',\n                        default=\'/data/VOC/VOC_test/VOC2007/Annotations\', type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=False, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_dir=args.eval_imgs,\n         annotation_dir=args.test_annotation_dir,\n         showbox=args.showbox)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/eval_coco.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport json\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\n\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nfrom data.lib_coco.PythonAPI.pycocotools.cocoeval import COCOeval\n\n\ndef cocoval(detected_json, eval_json):\n    eval_gt = COCO(eval_json)\n\n    eval_dt = eval_gt.loadRes(detected_json)\n    cocoEval = COCOeval(eval_gt, eval_dt, iouType=\'bbox\')\n\n    # cocoEval.params.imgIds = eval_gt.getImgIds()\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndef eval_coco(det_net, real_test_img_list, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n        tools.mkdir(save_path)\n        fw_json_dt = open(os.path.join(save_path, \'coco_minival.json\'), \'w\')\n        coco_det = []\n        for i, a_img in enumerate(real_test_img_list):\n\n            record = json.loads(a_img)\n            raw_img = cv2.imread(record[\'fpath\'])\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n\n            eval_indices = detected_scores >= 0.01\n            detected_scores = detected_scores[eval_indices]\n            detected_boxes = detected_boxes[eval_indices]\n            detected_categories = detected_categories[eval_indices]\n\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n                # draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + record[\'ID\'],\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax-xmin, ymax-ymin]))\n\n            # cost much time\n            for j, box in enumerate(boxes):\n                coco_det.append({\'bbox\': [float(box[0]), float(box[1]), float(box[2]), float(box[3])],\n                                 \'score\': float(detected_scores[j]), \'image_id\': int(record[\'ID\'].split(\'.jpg\')[0].split(\'_000000\')[-1]),\n                                 \'category_id\': int(classes_originID[LABEl_NAME_MAP[detected_categories[j]]])})\n\n            tools.view_bar(\'%s image cost %.3fs\' % (record[\'ID\'], (end - start)), i + 1, len(real_test_img_list))\n\n        json.dump(coco_det, fw_json_dt)\n        fw_json_dt.close()\n        return os.path.join(save_path, \'coco_minival.json\')\n\n\ndef eval(num_imgs, eval_data, eval_gt, showbox):\n\n    with open(eval_data) as f:\n        test_img_list = f.readlines()\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_img_list\n    else:\n        real_test_img_list = test_img_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    detected_json = eval_coco(det_net=faster_rcnn, real_test_img_list=real_test_img_list, draw_imgs=showbox)\n\n    # save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n    # detected_json = os.path.join(save_path, \'coco_res.json\')\n    cocoval(detected_json, eval_gt)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_data\', dest=\'eval_data\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/unsullied/sharefs/_research_detection/GeneralDetection/COCO/data/MSCOCO/odformat/coco_minival2014.odgt\', type=str)\n    parser.add_argument(\'--eval_gt\', dest=\'eval_gt\',\n                        help=\'eval gt\',\n                        default=\'/unsullied/sharefs/_research_detection/GeneralDetection/COCO/data/MSCOCO/instances_minival2014.json\',\n                        type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=True, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    # args = parse_args()\n    # print(20*""--"")\n    # print(args)\n    # print(20*""--"")\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    # eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n    #      eval_data=args.eval_data,\n    #      eval_gt=args.eval_gt,\n    #      showbox=args.showbox)\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_data=\'/unsullied/sharefs/_research_detection/GeneralDetection/COCO/data/MSCOCO/odformat/coco_minival2014.odgt\',\n         eval_gt=\'/unsullied/sharefs/_research_detection/GeneralDetection/COCO/data/MSCOCO/instances_minival2014.json\',\n         showbox=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/inference.py,9,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\ndef detect(det_net, inference_save_path, real_test_imgname_list):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0) # [1, None, None, 3]\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        for i, a_img_name in enumerate(real_test_imgname_list):\n\n            raw_img = cv2.imread(a_img_name)\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n\n            show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n            show_scores = detected_scores[show_indices]\n            show_boxes = detected_boxes[show_indices]\n            show_categories = detected_categories[show_indices]\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(np.squeeze(resized_img, 0),\n                                                                                boxes=show_boxes,\n                                                                                labels=show_categories,\n                                                                                scores=show_scores)\n            nake_name = a_img_name.split(\'/\')[-1]\n            # print (inference_save_path + \'/\' + nake_name)\n            cv2.imwrite(inference_save_path + \'/\' + nake_name,\n                        final_detections[:, :, ::-1])\n\n            tools.view_bar(\'{} image cost {}s\'.format(a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\n\n\ndef inference(test_dir, inference_save_path):\n\n    test_imgname_list = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)\n                                                          if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    detect(det_net=faster_rcnn, inference_save_path=inference_save_path, real_test_imgname_list=test_imgname_list)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'TestImgs...U need provide the test dir\')\n    parser.add_argument(\'--data_dir\', dest=\'data_dir\',\n                        help=\'data path\',\n                        default=\'demos\', type=str)\n    parser.add_argument(\'--save_dir\', dest=\'save_dir\',\n                        help=\'demo imgs to save\',\n                        default=\'inference_results\', type=str)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id \',\n                        default=\'0\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    inference(args.data_dir,\n              inference_save_path=args.save_dir)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/inference_for_coco.py,9,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nsys.path.append(""../"")\nsys.path.insert(0, \'/home/yjr/PycharmProjects/Faster-RCNN_TF/data/lib_coco/PythonAPI\')\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nfrom libs.label_name_dict.coco_dict import LABEL_NAME_MAP, classes_originID\nfrom help_utils import tools\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nimport json\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef eval_with_plac(det_net, imgId_list, coco, out_json_root, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    # coco_test_results = []\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        for i, imgid in enumerate(imgId_list):\n            imgname = coco.loadImgs(ids=[imgid])[0][\'file_name\']\n            raw_img = cv2.imread(os.path.join(""/home/yjr/DataSet/COCO/2017/test2017"", imgname))\n\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(np.squeeze(resized_img, 0),\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores)\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + str(imgid) + \'.jpg\',\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax-xmin, ymax-ymin]))\n\n            dets = np.hstack((detected_categories.reshape(-1, 1),\n                              detected_scores.reshape(-1, 1),\n                              boxes))\n\n            a_img_detect_result = []\n            for a_det in dets:\n                label, score, bbox = a_det[0], a_det[1], a_det[2:]\n                cat_id = classes_originID[LABEL_NAME_MAP[label]]\n                if score<0.00001:\n                   continue\n                det_object = {""image_id"": imgid,\n                              ""category_id"": cat_id,\n                              ""bbox"": bbox.tolist(),\n                              ""score"": float(score)}\n                # print (det_object)\n                a_img_detect_result.append(det_object)\n            f = open(os.path.join(out_json_root, \'each_img\', str(imgid)+\'.json\'), \'w\')\n            json.dump(a_img_detect_result, f)  # , indent=4\n            f.close()\n            del a_img_detect_result\n            del dets\n            del boxes\n            del resized_img\n            del raw_img\n            tools.view_bar(\'{} image cost {}s\'.format(imgid, (end - start)), i + 1, len(imgId_list))\n\n\ndef eval(num_imgs):\n\n\n   # annotation_path = \'/home/yjr/DataSet/COCO/2017/test_annotations/image_info_test2017.json\'\n    annotation_path = \'/home/yjr/DataSet/COCO/2017/test_annotations/image_info_test-dev2017.json\'\n    # annotation_path = \'/home/yjr/DataSet/COCO/2017/annotations/instances_train2017.json\'\n    print(""load coco .... it will cost about 17s.."")\n    coco = COCO(annotation_path)\n\n    imgId_list = coco.getImgIds()\n\n    if num_imgs !=np.inf:\n        imgId_list = imgId_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n    eval_with_plac(det_net=faster_rcnn, coco=coco, imgId_list=imgId_list, out_json_root=save_dir,\n                   draw_imgs=True)\n    print(""each img over**************"")\n\n    final_detections = []\n    with open(os.path.join(save_dir, \'coco2017test_results.json\'), \'w\') as wf:\n        for imgid in imgId_list:\n            f = open(os.path.join(save_dir, \'each_img\', str(imgid)+\'.json\'))\n            tmp_list = json.load(f)\n            # print (type(tmp_list))\n            final_detections.extend(tmp_list)\n            del tmp_list\n            f.close()\n        json.dump(final_detections, wf)\n\n\nif __name__ == \'__main__\':\n\n    eval(np.inf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train.py,61,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\n# from libs.networks import build_whole_network2\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label, num_objects):\n    return gtboxes_and_label[:int(num_objects), :]\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        global_step = slim.get_or_create_global_step()\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1]),\n                                                     np.int64(cfgs.DECAY_STEP[2])],\n                                         values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100., cfgs.LR / 1000.])\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                           is_training=True)\n\n        with tf.name_scope(\'get_batch\'):\n            num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                           is_training=True)\n            # gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n            # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            #     img_batch = img_batch / tf.constant([cfgs.PIXEL_STD])\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            # img_name = img_name_batch[i]\n            img = tf.expand_dims(img_batch[i], axis=0)\n\n            gtboxes_and_label = tf.cast(tf.reshape(gtboxes_and_label_batch[i], [-1, 5]), tf.float32)\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n            # img_h = tf.cast(tf.reshape(img_h, [-1, ]), tf.float32)\n            # img_w = tf.cast(tf.reshape(img_w, [-1, ]), tf.float32)\n\n            inputs_list.append([img, gtboxes_and_label, num_objects, img_h, img_w])\n\n        # put_op_list = []\n        # get_op_list = []\n        # for i in range(num_gpu):\n        #     with tf.device(""/GPU:%s"" % i):\n        #         area = tf.contrib.staging.StagingArea(\n        #             dtypes=[tf.float32, tf.float32, tf.float32])\n        #         put_op_list.append(area.put(inputs_list[i]))\n        #         get_op_list.append(area.get())\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'rpn_cls_loss\': tf.constant(0., tf.float32),\n            \'rpn_loc_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_cls_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_loc_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n\n        }\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label = tf.py_func(get_gtboxes_and_label,\n                                                               inp=[inputs_list[i][1], inputs_list[i][2]],\n                                                               Tout=tf.float32)\n                                gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = faster_rcnn.build_whole_detection_network(input_img_batch=img,\n                                                                                    gtboxes_batch=gtboxes_and_label)\n                                gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img,\n                                                                                               boxes=gtboxes_and_label[\n                                                                                                     :, :-1],\n                                                                                               labels=gtboxes_and_label[\n                                                                                                      :, -1])\n                                tf.summary.image(\'Compare/gtboxes_gpu:%d\' % i, gtboxes_in_img)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2])\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        # final_gvs = []\n        # with tf.variable_scope(\'Gradient_Mult\'):\n        #     for grad, var in grads:\n        #         scale = 1.\n        #         # if \'/biases:\' in var.name:\n        #         #    scale *= 2.\n        #         if \'conv_new\' in var.name:\n        #             scale *= 3.\n        #         if not np.allclose(scale, 1.0):\n        #             grad = tf.multiply(grad, scale)\n        #         final_gvs.append((grad, var))\n\n        apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = faster_rcnn.get_restorer()\n        saver = tf.train.Saver(max_to_keep=10)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                # start = time.time()\n                #\n                # _, global_stepnp, total_loss_dict_ = \\\n                #     sess.run([train_op, global_step, total_loss_dict])\n                #\n                # end = time.time()\n                #\n                # print(\'**\' * 20)\n                # print(""""""%s: global_step%d  current_step%d""""""\n                #       % (training_time, global_stepnp * num_gpu, step * num_gpu))\n                # print(""""""per_cost_time:%.3fs""""""\n                #       % ((end - start) / num_gpu))\n                # loss_str = \'\'\n                # for k in total_loss_dict_.keys():\n                #     loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                # print(loss_str)\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'coco_\' + str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/test.py,9,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\ndef detect(det_net, inference_save_path, real_test_imgname_list):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0) # [1, None, None, 3]\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        for i, a_img_name in enumerate(real_test_imgname_list):\n\n            raw_img = cv2.imread(a_img_name)\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img}  \n                )\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            detected_boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\n\n            show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n            show_scores = detected_scores[show_indices]\n            show_boxes = detected_boxes[show_indices]\n            show_categories = detected_categories[show_indices]\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(raw_img - np.array(cfgs.PIXEL_MEAN),\n                                                                                boxes=show_boxes,\n                                                                                labels=show_categories,\n                                                                                scores=show_scores)\n            nake_name = a_img_name.split(\'/\')[-1]\n            # print (inference_save_path + \'/\' + nake_name)\n            cv2.imwrite(inference_save_path + \'/\' + nake_name,\n                        final_detections[:, :, ::-1])\n\n            tools.view_bar(\'{} image cost {}s\'.format(a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\n\n\ndef test(test_dir, inference_save_path):\n\n    test_imgname_list = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)\n                                                          if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    detect(det_net=faster_rcnn, inference_save_path=inference_save_path, real_test_imgname_list=test_imgname_list)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'TestImgs...U need provide the test dir\')\n    parser.add_argument(\'--data_dir\', dest=\'data_dir\',\n                        help=\'data path\',\n                        default=\'demos\', type=str)\n    parser.add_argument(\'--save_dir\', dest=\'save_dir\',\n                        help=\'demo imgs to save\',\n                        default=\'inference_results\', type=str)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id \',\n                        default=\'0\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    test(args.data_dir,\n         inference_save_path=args.save_dir)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_coco.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport json\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\n\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nfrom data.lib_coco.PythonAPI.pycocotools.cocoeval import COCOeval\n\n\ndef test_coco(det_net, real_test_img_list, eval_data, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n        tools.mkdir(save_path)\n        fw_json_dt = open(os.path.join(save_path, \'coco_test-dev.json\'), \'w\')\n        coco_det = []\n        for i, a_img in enumerate(real_test_img_list):\n\n            raw_img = cv2.imread(os.path.join(eval_data, a_img[\'file_name\']))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n\n            eval_indices = detected_scores >= 0.01\n            detected_scores = detected_scores[eval_indices]\n            detected_boxes = detected_boxes[eval_indices]\n            detected_categories = detected_categories[eval_indices]\n\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n                # draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + \'{}.jpg\'.format(a_img[\'id\']),\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax-xmin, ymax-ymin]))\n\n            # cost much time\n            for j, box in enumerate(boxes):\n                coco_det.append({\'bbox\': [float(box[0]), float(box[1]), float(box[2]), float(box[3])],\n                                 \'score\': float(detected_scores[j]), \'image_id\': a_img[\'id\'],\n                                 \'category_id\': int(classes_originID[LABEl_NAME_MAP[detected_categories[j]]])})\n\n            tools.view_bar(\'%s image cost %.3fs\' % (a_img[\'id\'], (end - start)), i + 1, len(real_test_img_list))\n\n        json.dump(coco_det, fw_json_dt)\n        fw_json_dt.close()\n\n\ndef eval(num_imgs, eval_data, json_file, showbox):\n\n    with open(json_file) as f:\n        test_img_list = json.load(f)[\'images\']\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_img_list\n    else:\n        real_test_img_list = test_img_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    test_coco(det_net=faster_rcnn, real_test_img_list=real_test_img_list, eval_data=eval_data, draw_imgs=showbox)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_data\', dest=\'eval_data\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/unsullied/sharefs/_research_detection/GeneralDetection/COCO/data/MSCOCO/odformat/coco_minival2014.odgt\', type=str)\n    parser.add_argument(\'--json_file\', dest=\'json_file\',\n                        help=\'test-dev json file\',\n                        default=\'/home/yangxue/isilon/yangxue/code/yxdet/FPN_Tensorflow/tools/image_info_test-dev2017.json\', type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=False, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    # args = parse_args()\n    # print(20*""--"")\n    # print(args)\n    # print(20*""--"")\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    # eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n    #      eval_data=args.eval_data,\n    #      eval_gt=args.eval_gt,\n    #      showbox=args.showbox)\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_data=\'/unsullied/sharefs/yangxue/isilon/yangxue/data/COCO/test2017\',\n         json_file=\'/home/yangxue/isilon/yangxue/code/yxdet/FPN_TF_DEV/tools/image_info_test-dev2017.json\',\n         showbox=False)\n\n    # cocoval(\'./eval_coco/FPN_Res101_20190108_v1/coco_res.json\',\n    #         \'/unsullied/sharefs/_research_detection/GeneralDetection/COCO/data/MSCOCO/instances_minival2014.json\')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/train.py,31,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\n# from libs.networks import build_whole_network2\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord import next_batch\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef train():\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=True)\n\n    with tf.name_scope(\'get_batch\'):\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                       is_training=True)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n    biases_regularizer = tf.no_regularizer\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                         slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                        weights_regularizer=weights_regularizer,\n                        biases_regularizer=biases_regularizer,\n                        biases_initializer=tf.constant_initializer(0.0)):\n        final_bbox, final_scores, final_category, loss_dict = faster_rcnn.build_whole_detection_network(\n            input_img_batch=img_batch,\n            gtboxes_batch=gtboxes_and_label)\n\n    # ----------------------------------------------------------------------------------------------------build loss\n    weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n    rpn_location_loss = loss_dict[\'rpn_loc_loss\']\n    rpn_cls_loss = loss_dict[\'rpn_cls_loss\']\n    rpn_total_loss = rpn_location_loss + rpn_cls_loss\n\n    fastrcnn_cls_loss = loss_dict[\'fastrcnn_cls_loss\']\n    fastrcnn_loc_loss = loss_dict[\'fastrcnn_loc_loss\']\n    fastrcnn_total_loss = fastrcnn_cls_loss + fastrcnn_loc_loss\n\n    total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss\n    # ____________________________________________________________________________________________________build loss\n\n\n    # ---------------------------------------------------------------------------------------------------add summary\n\n    tf.summary.scalar(\'RPN_LOSS/cls_loss\', rpn_cls_loss)\n    tf.summary.scalar(\'RPN_LOSS/location_loss\', rpn_location_loss)\n    tf.summary.scalar(\'RPN_LOSS/rpn_total_loss\', rpn_total_loss)\n\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_cls_loss\', fastrcnn_cls_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_location_loss\', fastrcnn_loc_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_total_loss\', fastrcnn_total_loss)\n\n    tf.summary.scalar(\'LOSS/total_loss\', total_loss)\n    tf.summary.scalar(\'LOSS/regular_weights\', weight_decay_loss)\n\n    gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img_batch,\n                                                                   boxes=gtboxes_and_label[:, :-1],\n                                                                   labels=gtboxes_and_label[:, -1])\n    if cfgs.ADD_BOX_IN_TENSORBOARD:\n        detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(img_batch=img_batch,\n                                                                                     boxes=final_bbox,\n                                                                                     labels=final_category,\n                                                                                     scores=final_scores)\n        tf.summary.image(\'Compare/final_detection\', detections_in_img)\n    tf.summary.image(\'Compare/gtboxes\', gtboxes_in_img)\n\n    # ___________________________________________________________________________________________________add summary\n\n    global_step = slim.get_or_create_global_step()\n    lr = tf.train.piecewise_constant(global_step,\n                                     boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1])],\n                                     values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100.])\n    tf.summary.scalar(\'lr\', lr)\n    optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n    # optimizer = tf.train.AdamOptimizer(lr)\n\n    # ---------------------------------------------------------------------------------------------compute gradients\n    gradients = faster_rcnn.get_gradients(optimizer, total_loss)\n\n    # enlarge_gradients for bias\n    if cfgs.MUTILPY_BIAS_GRADIENT:\n        gradients = faster_rcnn.enlarge_gradients_for_bias(gradients)\n\n    if cfgs.GRADIENT_CLIPPING_BY_NORM:\n        with tf.name_scope(\'clip_gradients_YJR\'):\n            gradients = slim.learning.clip_gradient_norms(gradients,\n                                                          cfgs.GRADIENT_CLIPPING_BY_NORM)\n    # _____________________________________________________________________________________________compute gradients\n\n\n\n    # train_op\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients,\n                                         global_step=global_step)\n    summary_op = tf.summary.merge_all()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = faster_rcnn.get_restorer()\n    saver = tf.train.Saver(max_to_keep=30)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n        tools.mkdir(summary_path)\n        summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n        for step in range(cfgs.MAX_ITERATION):\n            training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n            if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                _, global_stepnp = sess.run([train_op, global_step])\n\n            else:\n                if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                    start = time.time()\n\n                    _, global_stepnp, img_name, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                    fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss = \\\n                        sess.run(\n                            [train_op, global_step, img_name_batch, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                             fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, total_loss])\n\n                    end = time.time()\n                    print("""""" {}: step{}    image_name:{} |\\t\n                              rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t rpn_total_loss:{} |\n                              fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t fast_rcnn_total_loss:{} |\n                              total_loss:{} |\\t per_cost_time:{}s"""""" \\\n                          .format(training_time, global_stepnp, str(img_name[0]), rpnLocLoss, rpnClsLoss,\n                                  rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss,\n                                  (end - start)))\n                else:\n                    if step % cfgs.SMRY_ITER == 0:\n                        _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                        summary_writer.add_summary(summary_str, global_stepnp)\n                        summary_writer.flush()\n\n            if (step > 0 and step % cfgs.SAVE_WEIGHTS_INTE == 0) or (step == cfgs.MAX_ITERATION - 1):\n\n                save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n\n                save_ckpt = os.path.join(save_dir, \'voc_\' + str(global_stepnp) + \'model.ckpt\')\n                saver.save(sess, save_ckpt)\n                print(\' weights had been saved\')\n\n        coord.request_stop()\n        coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/train_with_placeholder.py,35,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nsys.path.append(""../"")\nsys.path.append(""../data/lib_coco"")\nsys.path.append(\'../data/lib_coco/PythonAPI/\')\n\nimport numpy as np\nimport time\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom data.io import image_preprocess\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\nfrom data.lib_coco.get_coco_next_batch import next_img\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef preprocess_img(img_plac, gtbox_plac):\n    \'\'\'\n\n    :param img_plac: [H, W, 3] uint 8 img. In RGB.\n    :param gtbox_plac: shape of [-1, 5]. [xmin, ymin, xmax, ymax, label]\n    :return:\n    \'\'\'\n\n    img = tf.cast(img_plac, tf.float32)\n\n    # gtboxes_and_label = tf.cast(gtbox_plac, tf.float32)\n    img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img,\n                                                                gtboxes_and_label=gtbox_plac,\n                                                                target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                                length_limitation=cfgs.IMG_MAX_LENGTH)\n    img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                     gtboxes_and_label=gtboxes_and_label)\n    img = img - tf.constant([[cfgs.PIXEL_MEAN]])\n    img_batch = tf.expand_dims(img, axis=0)\n\n    # gtboxes_and_label = tf.Print(gtboxes_and_label, [tf.shape(gtboxes_and_label)], message=\'gtbox shape\')\n    return img_batch, gtboxes_and_label\n\ndef train():\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=True)\n\n    with tf.name_scope(\'get_batch\'):\n        img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\n        gtbox_plac = tf.placeholder(dtype=tf.int32, shape=[None, 5])\n\n        img_batch, gtboxes_and_label = preprocess_img(img_plac, gtbox_plac)\n        # gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n    biases_regularizer = tf.no_regularizer\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                         slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                        weights_regularizer=weights_regularizer,\n                        biases_regularizer=biases_regularizer,\n                        biases_initializer=tf.constant_initializer(0.0)):\n        final_bbox, final_scores, final_category, loss_dict = faster_rcnn.build_whole_detection_network(\n            input_img_batch=img_batch,\n            gtboxes_batch=gtboxes_and_label)\n\n    # ----------------------------------------------------------------------------------------------------build loss\n    weight_decay_loss = 0 # tf.add_n(slim.losses.get_regularization_losses())\n    rpn_location_loss = loss_dict[\'rpn_loc_loss\']\n    rpn_cls_loss = loss_dict[\'rpn_cls_loss\']\n    rpn_total_loss = rpn_location_loss + rpn_cls_loss\n\n    fastrcnn_cls_loss = loss_dict[\'fastrcnn_cls_loss\']\n    fastrcnn_loc_loss = loss_dict[\'fastrcnn_loc_loss\']\n    fastrcnn_total_loss = fastrcnn_cls_loss + fastrcnn_loc_loss\n\n    total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss\n    # ____________________________________________________________________________________________________build loss\n\n\n\n    # ---------------------------------------------------------------------------------------------------add summary\n    tf.summary.scalar(\'RPN_LOSS/cls_loss\', rpn_cls_loss)\n    tf.summary.scalar(\'RPN_LOSS/location_loss\', rpn_location_loss)\n    tf.summary.scalar(\'RPN_LOSS/rpn_total_loss\', rpn_total_loss)\n\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_cls_loss\', fastrcnn_cls_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_location_loss\', fastrcnn_loc_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_total_loss\', fastrcnn_total_loss)\n\n    tf.summary.scalar(\'LOSS/total_loss\', total_loss)\n    tf.summary.scalar(\'LOSS/regular_weights\', weight_decay_loss)\n\n    gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img_batch,\n                                                                   boxes=gtboxes_and_label[:, :-1],\n                                                                   labels=gtboxes_and_label[:, -1])\n    if cfgs.ADD_BOX_IN_TENSORBOARD:\n        detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(img_batch=img_batch,\n                                                                                     boxes=final_bbox,\n                                                                                     labels=final_category,\n                                                                                     scores=final_scores)\n        tf.summary.image(\'Compare/final_detection\', detections_in_img)\n    tf.summary.image(\'Compare/gtboxes\', gtboxes_in_img)\n\n    # ___________________________________________________________________________________________________add summary\n\n    global_step = slim.get_or_create_global_step()\n    lr = tf.train.piecewise_constant(global_step,\n                                     boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1])],\n                                     values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100.])\n    tf.summary.scalar(\'lr\', lr)\n    optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n\n    # ---------------------------------------------------------------------------------------------compute gradients\n    gradients = faster_rcnn.get_gradients(optimizer, total_loss)\n\n    # enlarge_gradients for bias\n    if cfgs.MUTILPY_BIAS_GRADIENT:\n        gradients = faster_rcnn.enlarge_gradients_for_bias(gradients)\n\n    if cfgs.GRADIENT_CLIPPING_BY_NORM:\n        with tf.name_scope(\'clip_gradients_YJR\'):\n            gradients = slim.learning.clip_gradient_norms(gradients,\n                                                          cfgs.GRADIENT_CLIPPING_BY_NORM)\n    # _____________________________________________________________________________________________compute gradients\n\n\n\n    # train_op\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients,\n                                         global_step=global_step)\n    summary_op = tf.summary.merge_all()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = faster_rcnn.get_restorer()\n    saver = tf.train.Saver(max_to_keep=30)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n        tools.mkdir(summary_path)\n        summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n        for step in range(cfgs.MAX_ITERATION):\n\n            img_id, img, gt_info = next_img(step=step)\n            training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n            if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                _, global_stepnp = sess.run([train_op, global_step],\n                                            feed_dict={img_plac: img,\n                                                       gtbox_plac: gt_info}\n                                            )\n\n            else:\n                if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                    start = time.time()\n\n                    _, global_stepnp, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                    fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss = \\\n                        sess.run(\n                            [train_op, global_step, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                             fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, total_loss],\n                        feed_dict={img_plac: img,\n                                   gtbox_plac: gt_info})\n                    end = time.time()\n                    print("""""" {}: step{}    image_name:{} |\\t\n                              rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t rpn_total_loss:{} |\n                              fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t fast_rcnn_total_loss:{} |\n                              total_loss:{} |\\t per_cost_time:{}s"""""" \\\n                          .format(training_time, global_stepnp, str(img_id), rpnLocLoss, rpnClsLoss,\n                                  rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss,\n                                  (end - start)))\n                else:\n                    if step % cfgs.SMRY_ITER == 0:\n                        _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op],\n                                                                 feed_dict={img_plac: img,\n                                                                            gtbox_plac: gt_info}\n                                                                 )\n                        summary_writer.add_summary(summary_str, global_stepnp)\n                        summary_writer.flush()\n\n            if (step > 0 and step % cfgs.SAVE_WEIGHTS_INTE == 0) or (step == cfgs.MAX_ITERATION - 1):\n\n                save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n\n                save_ckpt = os.path.join(save_dir, \'voc_\' + str(global_stepnp) + \'model.ckpt\')\n                saver.save(sess, save_ckpt)\n                print(\' weights had been saved\')\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
data/io/__init__.py,0,b''
data/io/convert_data_to_tfrecord.py,15,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport xml.etree.cElementTree as ET\nimport numpy as np\nimport tensorflow as tf\nimport glob\nimport cv2\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils.tools import *\n\ntf.app.flags.DEFINE_string(\'VOC_dir\', \'/data/VOC/VOCtrain_val/VOC2007/\', \'Voc dir\')\ntf.app.flags.DEFINE_string(\'xml_dir\', \'Annotations\', \'xml dir\')\ntf.app.flags.DEFINE_string(\'image_dir\', \'JPEGImages\', \'image dir\')\ntf.app.flags.DEFINE_string(\'save_name\', \'train\', \'save name\')\ntf.app.flags.DEFINE_string(\'save_dir\', \'../tfrecord/\', \'save name\')\ntf.app.flags.DEFINE_string(\'img_format\', \'.jpg\', \'format of image\')\ntf.app.flags.DEFINE_string(\'dataset\', \'pascal\', \'dataset\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 5],\n           and has [xmin, ymin, xmax, ymax, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef convert_pascal_to_tfrecord():\n    xml_path = FLAGS.VOC_dir + FLAGS.xml_dir\n    image_path = FLAGS.VOC_dir + FLAGS.image_dir\n    save_path = FLAGS.save_dir + FLAGS.dataset + \'_\' + FLAGS.save_name + \'.tfrecord\'\n    mkdir(FLAGS.save_dir)\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=save_path, options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=save_path)\n    for count, xml in enumerate(glob.glob(xml_path + \'/*.xml\')):\n        # to avoid path error in different development platform\n        xml = xml.replace(\'\\\\\', \'/\')\n\n        img_name = xml.split(\'/\')[-1].split(\'.\')[0] + FLAGS.img_format\n        img_path = image_path + \'/\' + img_name\n\n        if not os.path.exists(img_path):\n            print(\'{} is not exist!\'.format(img_path))\n            continue\n\n        img_height, img_width, gtbox_label = read_xml_gtbox_and_label(xml)\n\n        # img = np.array(Image.open(img_path))\n        img = cv2.imread(img_path)[:, :, ::-1]\n\n        feature = tf.train.Features(feature={\n            # do not need encode() in linux\n            \'img_name\': _bytes_feature(img_name.encode()),\n            # \'img_name\': _bytes_feature(img_name),\n            \'img_height\': _int64_feature(img_height),\n            \'img_width\': _int64_feature(img_width),\n            \'img\': _bytes_feature(img.tostring()),\n            \'gtboxes_and_label\': _bytes_feature(gtbox_label.tostring()),\n            \'num_objects\': _int64_feature(gtbox_label.shape[0])\n        })\n\n        example = tf.train.Example(features=feature)\n\n        writer.write(example.SerializeToString())\n\n        view_bar(\'Conversion progress\', count + 1, len(glob.glob(xml_path + \'/*.xml\')))\n\n    print(\'\\nConversion is complete!\')\n\n\nif __name__ == \'__main__\':\n    # xml_path = \'../data/dataset/VOCdevkit/VOC2007/Annotations/000005.xml\'\n    # read_xml_gtbox_and_label(xml_path)\n\n    convert_pascal_to_tfrecord()\n'"
data/io/convert_data_to_tfrecord_raw.py,8,"b'# -*- coding: utf-8 -*-\n\n\'\'\'\nthis file is to convert pascal to tfrecord\n\'\'\'\n\nimport numpy as np\nimport cv2\nimport os, sys\nimport tensorflow as tf\nimport xml.etree.cElementTree as ET\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\n\n\ntf.app.flags.DEFINE_string(\'VOC_dir\', \'/home/yjr/DataSet/VOC\', \'Voc dir \')\nFLAGS = tf.app.flags.FLAGS\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef read_xml_target_box_and_label(xml_path):\n    \'\'\'\n\n    :param xml_path:\n    :return:img_height, img_width, gtboxes\n    gtboxes is a array of shape [num_of_gtboxes, 5]\n    a row in gtboxes is [xmin. ymin. xmax, ymax, label]\n    \'\'\'\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        if child_of_root.tag == \'filename\':\n            assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] + \'.jpg\', \'xml_name and img_name cannot match\'\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    # print child_item.text\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))  # [xmin, ymin. xmax, ymax]\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label) #[xmin, ymin, xmax, ymax, label]\n                    box_list.append(tmp_box)\n\n    gtbox_list = np.array(box_list, dtype=np.int32)  # [xmin, ymin, xmax, ymax, label]\n\n    xmin, ymin, xmax, ymax, label = gtbox_list[:, 0], gtbox_list[:, 1], gtbox_list[:, 2], gtbox_list[:, 3],\\\n                                    gtbox_list[:, 4]\n\n    gtbox_list = np.transpose(np.stack([xmin, ymin, xmax, ymax, label], axis=0))  # [xmin, ymin, xmax, ymax, label]\n    # print gtbox_list.shape\n    return img_height, img_width, gtbox_list\n\ndef convert_pascal(dataset_name):\n\n    dataset_rootdir = os.path.join(FLAGS.VOC_dir, \'VOCtrain_val/VOC2007\') if dataset_name == \'train\' \\\n        else os.path.join(FLAGS.VOC_dir, \'VOC_test/VOC2007\')\n\n    imgname_list = []\n    part_name = \'trainval.txt\' if dataset_name == \'train\' else \'test.txt\'\n    with open(os.path.join(dataset_rootdir, \'ImageSets/Main/aeroplane_\'+part_name)) as f:\n        all_lines = f.readlines()\n\n    for a_line in all_lines:\n        imgname_list.append(a_line.split()[0].strip())\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=\'../data/tfrecords/pascal_\'+dataset_name+\'.tfrecord\', options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=\'../tfrecord/pascal_\' + dataset_name + \'.tfrecord\')\n    for i, img_name in enumerate(imgname_list):\n        img_np = cv2.imread(os.path.join(dataset_rootdir, \'JPEGImages/\'+img_name+\'.jpg\'))\n        # if img_np == None:\n        #     print img_name\n        img_np = img_np[:, :, ::-1]\n        assert img_np is not None, \'read img erro, imgnp is None\'\n        xml_path = os.path.join(dataset_rootdir, \'Annotations/\'+img_name+\'.xml\')\n        img_height, img_width, gtboxes = read_xml_target_box_and_label(xml_path)\n\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'img_name\': _bytes_feature(img_name),\n            \'img_height\': _int64_feature(img_height),\n            \'img_width\': _int64_feature(img_width),\n            \'img\': _bytes_feature(img_np.tostring()),\n            \'gtboxes_and_label\': _bytes_feature(gtboxes.tostring()),\n            \'num_objects\': _int64_feature(gtboxes.shape[0])\n        }))\n        writer.write(example.SerializeToString())\n        if i % 100 == 0:\n            print(\'{} {} imgs convert over\'.format(i, dataset_name))\n    print(20*""@"")\n    print(\'all {} imgs convert over, the num is {}\'.format(dataset_name, i))\n\nif __name__ == \'__main__\':\n    # w, h, gtboxes = read_xml_target_box_and_label(\'/home/yjr/DataSet/VOC/VOCtrain_val/VOC2007/Annotations/000005.xml\')\n    # print w, h\n    # print gtboxes\n    convert_pascal(\'train\')\n    convert_pascal(\'test\')\n\n'"
data/io/image_preprocess.py,18,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\nimport numpy as np\n\n\ndef max_length_limitation(length, length_limitation):\n    return tf.cond(tf.less(length, length_limitation),\n                   true_fn=lambda: length,\n                   false_fn=lambda: length_limitation)\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len, length_limitation=1200):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 5].  gtboxes: [xmin, ymin, xmax, ymax]\n    :param target_shortside_len:\n    :param length_limitation: set max length to avoid OUT OF MEMORY\n    :return:\n    '''\n    img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                           true_fn=lambda: (target_shortside_len,\n                                            max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                           false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                             target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    new_xmin, new_ymin = xmin * new_w // img_w, ymin * new_h // img_h\n    new_xmax, new_ymax = xmax * new_w // img_w, ymax * new_h // img_h\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, new_ymin, new_xmax, new_ymax, label], axis=0))\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, length_limitation=1200, is_resize=True):\n    if is_resize:\n      img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n      new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                             true_fn=lambda: (target_shortside_len,\n                                              max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                             false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                               target_shortside_len))\n\n      img_tensor = tf.expand_dims(img_tensor, axis=0)\n      img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n      img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor\n\ndef flip_left_to_right(img_tensor, gtboxes_and_label):\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_xmax = w - xmin\n    new_xmin = w - xmax\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, ymin, new_xmax, ymax, label], axis=0))\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label= tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_to_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\n\n"""
data/io/image_preprocess_multi_gpu.py,18,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\nimport numpy as np\n\n\ndef max_length_limitation(length, length_limitation):\n    return tf.cond(tf.less(length, length_limitation),\n                   true_fn=lambda: length,\n                   false_fn=lambda: length_limitation)\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len, length_limitation=1200):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 5].  gtboxes: [xmin, ymin, xmax, ymax]\n    :param target_shortside_len:\n    :param length_limitation: set max length to avoid OUT OF MEMORY\n    :return:\n    '''\n    img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                           true_fn=lambda: (target_shortside_len,\n                                            max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                           false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                             target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    new_xmin, new_ymin = xmin * new_w // img_w, ymin * new_h // img_h\n    new_xmax, new_ymax = xmax * new_w // img_w, ymax * new_h // img_h\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, new_ymin, new_xmax, new_ymax, label], axis=0)), new_h, new_w\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, length_limitation=1200, is_resize=True):\n    if is_resize:\n      img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n      new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                             true_fn=lambda: (target_shortside_len,\n                                              max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                             false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                               target_shortside_len))\n\n      img_tensor = tf.expand_dims(img_tensor, axis=0)\n      img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n      img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor\n\n\ndef flip_left_to_right(img_tensor, gtboxes_and_label):\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_xmax = w - xmin\n    new_xmin = w - xmax\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, ymin, new_xmax, ymax, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label= tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_to_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\n\n"""
data/io/read_tfrecord.py,22,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom data.io import image_preprocess\nfrom libs.configs import cfgs\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n\n    if is_training:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len,\n                                                                    length_limitation=cfgs.IMG_MAX_LENGTH)\n        img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                         gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len,\n                                                                    length_limitation=cfgs.IMG_MAX_LENGTH)\n    img = img - tf.constant([[cfgs.PIXEL_MEAN]])  # sub pixel mean at last\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n    assert batch_size == 1, ""we only support batch_size is 1.We may support large batch_size in the future""\n\n    if dataset_name not in [\'ship\', \'spacenet\', \'pascal\', \'coco\']:\n        raise ValueError(\'dataSet name must be in pascal, coco spacenet and ship\')\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    img_name, img, gtboxes_and_label, num_obs = read_and_prepocess_single_img(filename_queue, shortside_len,\n                                                                              is_training=is_training)\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs],\n                       batch_size=batch_size,\n                       capacity=1,\n                       num_threads=1,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch\n'"
data/io/read_tfrecord_multi_gpu.py,33,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport os\nimport sys\nsys.path.append(\'../../\')\n\nfrom data.io import image_preprocess_multi_gpu as image_preprocess\nfrom libs.configs import cfgs\n\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n\n    if is_training:\n        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                                  target_shortside_len=shortside_len,\n                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)\n        img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                         gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                                  target_shortside_len=shortside_len,\n                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255 - tf.constant([[cfgs.PIXEL_MEAN_]])\n    else:\n        img = img - tf.constant([[cfgs.PIXEL_MEAN]])  # sub pixel mean at last\n    return img_name, img, gtboxes_and_label, num_objects, img_h, img_w\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n    # assert batch_size == 1, ""we only support batch_size is 1.We may support large batch_size in the future""\n\n    if dataset_name not in [\'ship\', \'spacenet\', \'pascal\', \'coco\', \'bdd100k\', \'DOTA\']:\n        raise ValueError(\'dataSet name must be in pascal, coco spacenet and ship\')\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    # shortside_len = tf.constant(shortside_len)\n    # shortside_len = tf.random_shuffle(shortside_len)[0]\n\n    img_name, img, gtboxes_and_label, num_obs, img_h, img_w = read_and_prepocess_single_img(filename_queue, shortside_len,\n                                                                                            is_training=is_training)\n\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs, img_h, img_w],\n                       batch_size=batch_size,\n                       capacity=16,\n                       num_threads=16,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0,1\'\n    num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n        next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                   batch_size=cfgs.BATCH_SIZE * num_gpu,\n                   shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                   is_training=True)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        img_name_batch_, img_batch_, gtboxes_and_label_batch_, num_objects_batch_ \\\n            = sess.run([img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch])\n\n        print(img_name_batch_.shape)\n        print(img_batch_.shape)\n        print(\'debug\')\n\n        coord.request_stop()\n        coord.join(threads)\n'"
data/lib_coco/__init__.py,0,b''
data/lib_coco/get_coco_next_batch.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport sys, os\n# sys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, \'./PythonAPI/\')\n# sys.path.insert(0, os.path.abspath(\'data\'))\nfor _ in sys.path:\n    print (_)\nfrom PythonAPI.pycocotools.coco import COCO\nimport cv2\nimport numpy as np\nimport os\nfrom libs.label_name_dict import coco_dict\n\n\nannotation_path = \'/home/yjr/DataSet/COCO/2017/annotations/instances_train2017.json\'\nprint (""load coco .... it will cost about 17s.."")\ncoco = COCO(annotation_path)\n\nimgId_list = coco.getImgIds()\nimgId_list = np.array(imgId_list)\n\ntotal_imgs = len(imgId_list)\n\n# print (NAME_LABEL_DICT)\n\n\ndef next_img(step):\n\n    if step % total_imgs == 0:\n        np.random.shuffle(imgId_list)\n    imgid = imgId_list[step % total_imgs]\n\n    imgname = coco.loadImgs(ids=[imgid])[0][\'file_name\']\n    # print (type(imgname), imgname)\n    img = cv2.imread(os.path.join(""/home/yjr/DataSet/COCO/2017/train2017"", imgname))\n\n    annotation = coco.imgToAnns[imgid]\n    gtbox_and_label_list = []\n    for ann in annotation:\n        box = ann[\'bbox\']\n\n        box = [box[0], box[1], box[0]+box[2], box[1]+box[3]]  # [xmin, ymin, xmax, ymax]\n        cat_id = ann[\'category_id\']\n        cat_name = coco_dict.originID_classes[cat_id] #ID_NAME_DICT[cat_id]\n        label = coco_dict.NAME_LABEL_MAP[cat_name]\n        gtbox_and_label_list.append(box + [label])\n    gtbox_and_label_list = np.array(gtbox_and_label_list, dtype=np.int32)\n    # print (img.shape, gtbox_and_label_list.shape)\n    if gtbox_and_label_list.shape[0] == 0:\n        return next_img(step+1)\n    else:\n        return imgid, img[:, :, ::-1], gtbox_and_label_list\n\n\nif __name__ == \'__main__\':\n\n    imgid, img,  gtbox = next_img(3234)\n\n    print(""::"")\n    from libs.box_utils.draw_box_in_img import draw_boxes_with_label_and_scores\n\n    img = draw_boxes_with_label_and_scores(img_array=img, boxes=gtbox[:, :-1], labels=gtbox[:, -1],\n                                           scores=np.ones(shape=(len(gtbox), )))\n    print (""_----"")\n\n\n    cv2.imshow(""test"", img)\n    cv2.waitKey(0)\n\n\n'"
libs/box_utils/__init__.py,0,b''
libs/box_utils/anchor_utils.py,20,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\nfrom libs.configs import cfgs\n\n\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios,\n                 featuremap_height, featuremap_width,\n                 stride, name=\'make_anchors\'):\n    \'\'\'\n    :param base_anchor_size:256\n    :param anchor_scales:\n    :param anchor_ratios:\n    :param featuremap_height:\n    :param featuremap_width:\n    :param stride:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)  # [x_center, y_center, w, h]\n\n        ws, hs = enum_ratios(enum_scales(base_anchor, anchor_scales),\n                             anchor_ratios)  # per locations ws and hs\n\n        # featuremap_height = tf.Print(featuremap_height,\n        #                              [featuremap_height, featuremap_width], summarize=10,\n        #                              message=name+""_SHAPE***"")\n\n        x_centers = tf.range(featuremap_width, dtype=tf.float32) * stride\n        y_centers = tf.range(featuremap_height, dtype=tf.float32) * stride\n\n        if cfgs.USE_CENTER_OFFSET:\n            x_centers = x_centers + stride/2.\n            y_centers = y_centers + stride/2.\n\n        x_centers, y_centers = tf.meshgrid(x_centers, y_centers)\n\n        ws, x_centers = tf.meshgrid(ws, x_centers)\n        hs, y_centers = tf.meshgrid(hs, y_centers)\n\n        anchor_centers = tf.stack([x_centers, y_centers], 2)\n        anchor_centers = tf.reshape(anchor_centers, [-1, 2])\n\n        box_sizes = tf.stack([ws, hs], axis=2)\n        box_sizes = tf.reshape(box_sizes, [-1, 2])\n        # anchors = tf.concat([anchor_centers, box_sizes], axis=1)\n        anchors = tf.concat([anchor_centers - 0.5*box_sizes,\n                             anchor_centers + 0.5*box_sizes], axis=1)\n        return anchors\n\n\ndef enum_scales(base_anchor, anchor_scales):\n\n    anchor_scales = base_anchor * tf.constant(anchor_scales, dtype=tf.float32, shape=(len(anchor_scales), 1))\n\n    return anchor_scales\n\n\ndef enum_ratios(anchors, anchor_ratios):\n    \'\'\'\n    ratio = h /w\n    :param anchors:\n    :param anchor_ratios:\n    :return:\n    \'\'\'\n    ws = anchors[:, 2]  # for base anchor: w == h\n    hs = anchors[:, 3]\n    sqrt_ratios = tf.sqrt(tf.constant(anchor_ratios))\n\n    ws = tf.reshape(ws / sqrt_ratios[:, tf.newaxis], [-1, 1])\n    hs = tf.reshape(hs * sqrt_ratios[:, tf.newaxis], [-1, 1])\n\n    return ws, hs\n\n\nif __name__ == \'__main__\':\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    base_anchor_size = 256\n    anchor_scales = [1.0]\n    anchor_ratios = [0.5, 2.0, 1.0]\n    anchors = make_anchors(base_anchor_size=base_anchor_size, anchor_ratios=anchor_ratios,\n                           anchor_scales=anchor_scales,\n                           featuremap_width=32,\n                           featuremap_height=63,\n                           stride=16)\n    init = tf.global_variables_initializer()\n    with tf.Session() as sess:\n        sess.run(init)\n        anchor_result = sess.run(anchors)\n        print (anchor_result.shape)\n'"
libs/box_utils/boxes_utils.py,35,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef ious_calu(boxes_1, boxes_2):\n    '''\n\n    :param boxes_1: [N, 4] [xmin, ymin, xmax, ymax]\n    :param boxes_2: [M, 4] [xmin, ymin. xmax, ymax]\n    :return:\n    '''\n    boxes_1 = tf.cast(boxes_1, tf.float32)\n    boxes_2 = tf.cast(boxes_2, tf.float32)\n    xmin_1, ymin_1, xmax_1, ymax_1 = tf.split(boxes_1, 4, axis=1)  # xmin_1 shape is [N, 1]..\n    xmin_2, ymin_2, xmax_2, ymax_2 = tf.unstack(boxes_2, axis=1)  # xmin_2 shape is [M, ]..\n\n    max_xmin = tf.maximum(xmin_1, xmin_2)\n    min_xmax = tf.minimum(xmax_1, xmax_2)\n\n    max_ymin = tf.maximum(ymin_1, ymin_2)\n    min_ymax = tf.minimum(ymax_1, ymax_2)\n\n    overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n    overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n    overlaps = overlap_h * overlap_w\n\n    area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n    area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n    ious = overlaps / (area_1 + area_2 - overlaps)\n\n    return ious\n\n\ndef clip_boxes_to_img_boundaries(decode_boxes, img_shape):\n    '''\n\n    :param decode_boxes:\n    :return: decode boxes, and already clip to boundaries\n    '''\n\n    with tf.name_scope('clip_boxes_to_img_boundaries'):\n\n        # xmin, ymin, xmax, ymax = tf.unstack(decode_boxes, axis=1)\n        xmin = decode_boxes[:, 0]\n        ymin = decode_boxes[:, 1]\n        xmax = decode_boxes[:, 2]\n        ymax = decode_boxes[:, 3]\n        img_h, img_w = img_shape[1], img_shape[2]\n\n        img_h, img_w = tf.cast(img_h, tf.float32), tf.cast(img_w, tf.float32)\n\n        xmin = tf.maximum(tf.minimum(xmin, img_w-1.), 0.)\n        ymin = tf.maximum(tf.minimum(ymin, img_h-1.), 0.)\n\n        xmax = tf.maximum(tf.minimum(xmax, img_w-1.), 0.)\n        ymax = tf.maximum(tf.minimum(ymax, img_h-1.), 0.)\n\n        return tf.transpose(tf.stack([xmin, ymin, xmax, ymax]))\n\n\ndef filter_outside_boxes(boxes, img_h, img_w):\n    '''\n    :param anchors:boxes with format [xmin, ymin, xmax, ymax]\n    :param img_h: height of image\n    :param img_w: width of image\n    :return: indices of anchors that inside the image boundary\n    '''\n\n    with tf.name_scope('filter_outside_boxes'):\n        xmin, ymin, xmax, ymax = tf.unstack(boxes, axis=1)\n\n        xmin_index = tf.greater_equal(xmin, 0)\n        ymin_index = tf.greater_equal(ymin, 0)\n        xmax_index = tf.less_equal(xmax, tf.cast(img_w, tf.float32))\n        ymax_index = tf.less_equal(ymax, tf.cast(img_h, tf.float32))\n\n        indices = tf.transpose(tf.stack([xmin_index, ymin_index, xmax_index, ymax_index]))\n        indices = tf.cast(indices, dtype=tf.int32)\n        indices = tf.reduce_sum(indices, axis=1)\n        indices = tf.where(tf.equal(indices, 4))\n        # indices = tf.equal(indices, 4)\n        return tf.reshape(indices, [-1])\n\n\ndef padd_boxes_with_zeros(boxes, scores, max_num_of_boxes):\n\n    '''\n    num of boxes less than max num of boxes, so it need to pad with zeros[0, 0, 0, 0]\n    :param boxes:\n    :param scores: [-1]\n    :param max_num_of_boxes:\n    :return:\n    '''\n\n    pad_num = tf.cast(max_num_of_boxes, tf.int32) - tf.shape(boxes)[0]\n\n    zero_boxes = tf.zeros(shape=[pad_num, 4], dtype=boxes.dtype)\n    zero_scores = tf.zeros(shape=[pad_num], dtype=scores.dtype)\n\n    final_boxes = tf.concat([boxes, zero_boxes], axis=0)\n\n    final_scores = tf.concat([scores, zero_scores], axis=0)\n\n    return final_boxes, final_scores"""
libs/box_utils/draw_box_in_img.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\n\nfrom libs.configs import cfgs\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\nNOT_DRAW_BOXES = 0\nONLY_DRAW_BOXES = -1\nONLY_DRAW_BOXES_WITH_SCORES = -2\n\nSTANDARD_COLORS = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\', \'LightBlue\', \'LightGreen\'\n]\nFONT = ImageFont.load_default()\n\n\ndef draw_a_rectangel_in_img(draw_obj, box, color, width):\n    \'\'\'\n    use draw lines to draw rectangle. since the draw_rectangle func can not modify the width of rectangle\n    :param draw_obj:\n    :param box: [x1, y1, x2, y2]\n    :return:\n    \'\'\'\n    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n    top_left, top_right = (x1, y1), (x2, y1)\n    bottom_left, bottom_right = (x1, y2), (x2, y2)\n\n    draw_obj.line(xy=[top_left, top_right],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[top_left, bottom_left],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[bottom_left, bottom_right],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[top_right, bottom_right],\n                  fill=color,\n                  width=width)\n\n\ndef only_draw_scores(draw_obj, box, score, color):\n\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x+60, y+10],\n                       fill=color)\n    draw_obj.text(xy=(x, y),\n                  text=""obj:"" +str(round(score, 2)),\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_label_with_scores(draw_obj, box, label, score, color):\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x + 60, y + 10],\n                       fill=color)\n\n    txt = LABEl_NAME_MAP[label] + \':\' + str(round(score, 2))\n    draw_obj.text(xy=(x, y),\n                  text=txt,\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_boxes_with_label_and_scores(img_array, boxes, labels, scores):\n\n    img_array = img_array + np.array(cfgs.PIXEL_MEAN)\n    img_array.astype(np.float32)\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img_array = np.array(img_array * 255 / np.max(img_array), dtype=np.uint8)\n\n    img_obj = Image.fromarray(img_array)\n    raw_img_obj = img_obj.copy()\n\n    draw_obj = ImageDraw.Draw(img_obj)\n    num_of_objs = 0\n    for box, a_label, a_score in zip(boxes, labels, scores):\n\n        if a_label != NOT_DRAW_BOXES:\n            num_of_objs += 1\n            draw_a_rectangel_in_img(draw_obj, box, color=STANDARD_COLORS[a_label], width=3)\n            if a_label == ONLY_DRAW_BOXES:  # -1\n                continue\n            elif a_label == ONLY_DRAW_BOXES_WITH_SCORES:  # -2\n                 only_draw_scores(draw_obj, box, a_score, color=\'White\')\n                 continue\n            else:\n                draw_label_with_scores(draw_obj, box, a_label, a_score, color=\'White\')\n\n    out_img_obj = Image.blend(raw_img_obj, img_obj, alpha=0.7)\n\n    return np.array(out_img_obj)\n\n\nif __name__ == \'__main__\':\n    img_array = cv2.imread(""/home/yjr/PycharmProjects/FPN_TF/tools/inference_image/2.jpg"")\n    img_array = np.array(img_array, np.float32) - np.array(cfgs.PIXEL_MEAN)\n    boxes = np.array(\n        [[200, 200, 500, 500],\n         [300, 300, 400, 400],\n         [200, 200, 400, 400]]\n    )\n\n    # test only draw boxes\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES\n    scores = np.zeros_like(labes)\n    imm = draw_boxes_with_label_and_scores(img_array, boxes, labes ,scores)\n    # imm = np.array(imm)\n\n    cv2.imshow(""te"", imm)\n\n    # test only draw scores\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES_WITH_SCORES\n    scores = np.random.rand((len(boxes))) * 10\n    imm2 = draw_boxes_with_label_and_scores(img_array, boxes, labes, scores)\n\n    cv2.imshow(""te2"", imm2)\n    # test draw label and scores\n\n    labels = np.arange(1, 4)\n    imm3 = draw_boxes_with_label_and_scores(img_array, boxes, labels, scores)\n    cv2.imshow(""te3"", imm3)\n\n    cv2.waitKey(0)\n\n\n\n\n\n\n\n'"
libs/box_utils/encode_and_decode.py,15,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\n\n\n# def encode_boxes(ex_rois, gt_rois, scale_factor=None):\n#     ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n#     ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n#     ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n#     ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n#\n#     gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n#     gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n#     gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n#     gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n#\n#     targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n#     targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n#     targets_dw = np.log(gt_widths / ex_widths)\n#     targets_dh = np.log(gt_heights / ex_heights)\n#\n#     if scale_factor:\n#         targets_dx = targets_dx * scale_factor[0]\n#         targets_dy = targets_dy * scale_factor[1]\n#         targets_dw = targets_dw * scale_factor[2]\n#         targets_dh = targets_dh * scale_factor[3]\n#\n#     targets = np.vstack(\n#         (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n#     return targets\n#\n#\n# def _concat_new_axis(t1, t2, t3, t4, axis):\n#     return tf.concat(\n#         [tf.expand_dims(t1, -1), tf.expand_dims(t2, -1),\n#          tf.expand_dims(t3, -1), tf.expand_dims(t4, -1)], axis=axis)\n#\n#\n# def decode_boxes(boxes, deltas, scale_factor=None):\n#     widths = boxes[:, 2] - boxes[:, 0] + 1.0\n#     heights = boxes[:, 3] - boxes[:, 1] + 1.0\n#     ctr_x = tf.expand_dims(boxes[:, 0] + 0.5 * widths, -1)\n#     ctr_y = tf.expand_dims(boxes[:, 1] + 0.5 * heights, -1)\n#\n#     dx = deltas[:, 0::4]\n#     dy = deltas[:, 1::4]\n#     dw = deltas[:, 2::4]\n#     dh = deltas[:, 3::4]\n#\n#     if scale_factor:\n#         dx /= scale_factor[0]\n#         dy /= scale_factor[1]\n#         dw /= scale_factor[2]\n#         dh /= scale_factor[3]\n#\n#     widths = tf.expand_dims(widths, -1)\n#     heights = tf.expand_dims(heights, -1)\n#\n#     pred_ctr_x = dx * widths + ctr_x\n#     pred_ctr_y = dy * heights + ctr_y\n#     pred_w = tf.exp(dw) * widths\n#     pred_h = tf.exp(dh) * heights\n#\n#     # x1\n#     # pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n#     pred_x1 = pred_ctr_x - 0.5 * pred_w\n#     # y1\n#     # pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n#     pred_y1 = pred_ctr_y - 0.5 * pred_h\n#     # x2\n#     # pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n#     pred_x2 = pred_ctr_x + 0.5 * pred_w\n#     # y2\n#     # pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n#     pred_y2 = pred_ctr_y + 0.5 * pred_h\n#\n#     pred_boxes = _concat_new_axis(pred_x1, pred_y1, pred_x2, pred_y2, 2)\n#     pred_boxes = tf.reshape(pred_boxes, (tf.shape(pred_boxes)[0], -1))\n#     return pred_boxes\n\n\ndef decode_boxes(encoded_boxes, reference_boxes, scale_factors=None):\n    '''\n\n    :param encoded_boxes:[N, 4]\n    :param reference_boxes: [N, 4] .\n    :param scale_factors: use for scale.\n\n    in the first stage, reference_boxes  are anchors\n    in the second stage, reference boxes are proposals(decode) produced by first stage\n    :return:decode boxes [N, 4]\n    '''\n\n    t_xcenter, t_ycenter, t_w, t_h = tf.unstack(encoded_boxes, axis=1)\n    if scale_factors:\n        t_xcenter /= scale_factors[0]\n        t_ycenter /= scale_factors[1]\n        t_w /= scale_factors[2]\n        t_h /= scale_factors[3]\n\n    reference_xmin, reference_ymin, reference_xmax, reference_ymax = tf.unstack(reference_boxes, axis=1)\n    # reference boxes are anchors in the first stage\n\n    reference_xcenter = (reference_xmin + reference_xmax) / 2.\n    reference_ycenter = (reference_ymin + reference_ymax) / 2.\n    reference_w = reference_xmax - reference_xmin\n    reference_h = reference_ymax - reference_ymin\n\n    predict_xcenter = t_xcenter * reference_w + reference_xcenter\n    predict_ycenter = t_ycenter * reference_h + reference_ycenter\n    predict_w = tf.exp(t_w) * reference_w\n    predict_h = tf.exp(t_h) * reference_h\n\n    predict_xmin = predict_xcenter - predict_w / 2.\n    predict_xmax = predict_xcenter + predict_w / 2.\n    predict_ymin = predict_ycenter - predict_h / 2.\n    predict_ymax = predict_ycenter + predict_h / 2.\n\n    return tf.transpose(tf.stack([predict_xmin, predict_ymin,\n                                  predict_xmax, predict_ymax]))\n\n\ndef encode_boxes(unencode_boxes, reference_boxes, scale_factors=None):\n    '''\n\n    :param unencode_boxes: [-1, 4]\n    :param reference_boxes: [-1, 4]\n    :return: encode_boxes [-1, 4]\n    '''\n\n    xmin, ymin, xmax, ymax = unencode_boxes[:, 0], unencode_boxes[:, 1], unencode_boxes[:, 2], unencode_boxes[:, 3]\n\n    reference_xmin, reference_ymin, reference_xmax, reference_ymax = reference_boxes[:, 0], reference_boxes[:, 1], \\\n                                                                     reference_boxes[:, 2], reference_boxes[:, 3]\n\n    x_center = (xmin + xmax) / 2.\n    y_center = (ymin + ymax) / 2.\n    w = xmax - xmin + 1e-8\n    h = ymax - ymin + 1e-8\n\n    reference_xcenter = (reference_xmin + reference_xmax) / 2.\n    reference_ycenter = (reference_ymin + reference_ymax) / 2.\n    reference_w = reference_xmax - reference_xmin + 1e-8\n    reference_h = reference_ymax - reference_ymin + 1e-8\n\n    # w + 1e-8 to avoid NaN in division and log below\n\n    t_xcenter = (x_center - reference_xcenter) / reference_w\n    t_ycenter = (y_center - reference_ycenter) / reference_h\n    t_w = np.log(w/reference_w)\n    t_h = np.log(h/reference_h)\n\n    if scale_factors:\n        t_xcenter *= scale_factors[0]\n        t_ycenter *= scale_factors[1]\n        t_w *= scale_factors[2]\n        t_h *= scale_factors[3]\n\n    return np.transpose(np.stack([t_xcenter, t_ycenter, t_w, t_h], axis=0))\n"""
libs/box_utils/show_box_in_tensor.py,30,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\n\nfrom libs.configs import cfgs\n\nfrom libs.box_utils import draw_box_in_img\n\ndef only_draw_boxes(img_batch, boxes):\n\n    boxes = tf.stop_gradient(boxes)\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    labels = tf.ones(shape=(tf.shape(boxes)[0], ), dtype=tf.int32) * draw_box_in_img.ONLY_DRAW_BOXES\n    scores = tf.zeros_like(labels, dtype=tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=tf.uint8)\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))  # [batch_size, h, w, c]\n\n    return img_tensor_with_boxes\n\ndef draw_boxes_with_scores(img_batch, boxes, scores):\n\n    boxes = tf.stop_gradient(boxes)\n    scores = tf.stop_gradient(scores)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    labels = tf.ones(shape=(tf.shape(boxes)[0],), dtype=tf.int32) * draw_box_in_img.ONLY_DRAW_BOXES_WITH_SCORES\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\ndef draw_boxes_with_categories(img_batch, boxes, labels):\n    boxes = tf.stop_gradient(boxes)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    scores = tf.ones(shape=(tf.shape(boxes)[0],), dtype=tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\ndef draw_boxes_with_categories_and_scores(img_batch, boxes, labels, scores):\n    boxes = tf.stop_gradient(boxes)\n    scores = tf.stop_gradient(scores)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\nif __name__ == ""__main__"":\n    print (1)\n\n'"
libs/box_utils/tf_ops.py,7,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\n\n\'\'\'\nall of these ops are derived from tenosrflow Object Detection API\n\'\'\'\ndef indices_to_dense_vector(indices,\n                            size,\n                            indices_value=1.,\n                            default_value=0,\n                            dtype=tf.float32):\n  """"""Creates dense vector with indices set to specific (the para ""indices_value"" ) and rest to zeros.\n\n  This function exists because it is unclear if it is safe to use\n    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)\n  with indices which are not ordered.\n  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])\n\n  Args:\n    indices: 1d Tensor with integer indices which are to be set to\n        indices_values.\n    size: scalar with size (integer) of output Tensor.\n    indices_value: values of elements specified by indices in the output vector\n    default_value: values of other elements in the output vector.\n    dtype: data type.\n\n  Returns:\n    dense 1D Tensor of shape [size] with indices set to indices_values and the\n        rest set to default_value.\n  """"""\n  size = tf.to_int32(size)\n  zeros = tf.ones([size], dtype=dtype) * default_value\n  values = tf.ones_like(indices, dtype=dtype) * indices_value\n\n  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],\n                           [zeros, values])\n\n\n\n\ndef test_plt():\n  from PIL import Image\n  import matplotlib.pyplot as plt\n  import numpy as np\n\n  a = np.random.rand(20, 30)\n  print (a.shape)\n  # plt.subplot()\n  b = plt.imshow(a)\n  plt.show()\n\n\nif __name__ == \'__main__\':\n  test_plt()\n'"
libs/configs/__init__.py,0,b''
libs/configs/cfgs.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ncls : person|| Recall: 0.9200530035335689 || Precison: 0.009050166947990866|| AP: 0.8413662097687251\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007414480221227399|| AP: 0.886410933036462\n____________________\ncls : horse|| Recall: 0.9626436781609196 || Precison: 0.0007370072226707821|| AP: 0.880462817781879\n____________________\ncls : boat|| Recall: 0.8783269961977186 || Precison: 0.000509740231082238|| AP: 0.6456185469835614\n____________________\ncls : bottle|| Recall: 0.8656716417910447 || Precison: 0.0008665714718695106|| AP: 0.6480626365413494\n____________________\ncls : bicycle|| Recall: 0.9228486646884273 || Precison: 0.0006832296772124229|| AP: 0.8550508887926864\n____________________\ncls : bus|| Recall: 0.9577464788732394 || Precison: 0.00045156820340048565|| AP: 0.8631526839193041\n____________________\ncls : sheep|| Recall: 0.9132231404958677 || Precison: 0.0004864327094081809|| AP: 0.7741568397678364\n____________________\ncls : car|| Recall: 0.9600333055786844 || Precison: 0.0025449055537652685|| AP: 0.8914023804170609\n____________________\ncls : motorbike|| Recall: 0.9538461538461539 || Precison: 0.0006737519288865706|| AP: 0.8495072139551133\n____________________\ncls : chair|| Recall: 0.873015873015873 || Precison: 0.001433311906044233|| AP: 0.5759698175528438\n____________________\ncls : aeroplane|| Recall: 0.9438596491228071 || Precison: 0.0006024690030817745|| AP: 0.8353670052573003\n____________________\ncls : tvmonitor|| Recall: 0.9415584415584416 || Precison: 0.0006278089036291684|| AP: 0.7613581746623427\n____________________\ncls : sofa|| Recall: 0.9707112970711297 || Precison: 0.0005222178954168627|| AP: 0.7987407803525022\n____________________\ncls : bird|| Recall: 0.9281045751633987 || Precison: 0.0009227090390830091|| AP: 0.8159836473038345\n____________________\ncls : dog|| Recall: 0.9611451942740287 || Precison: 0.0010255447933963644|| AP: 0.8951754362513265\n____________________\ncls : cow|| Recall: 0.9467213114754098 || Precison: 0.0005056795917786568|| AP: 0.8497306852549179\n____________________\ncls : diningtable|| Recall: 0.883495145631068 || Precison: 0.0004040099093419522|| AP: 0.7307392356452687\n____________________\ncls : pottedplant|| Recall: 0.7729166666666667 || Precison: 0.0008064077902035582|| AP: 0.4738700691112566\n____________________\ncls : train|| Recall: 0.9290780141843972 || Precison: 0.0005804331981204598|| AP: 0.8427500500899303\n____________________\nmAP is : 0.7857438026222752   (USE_12_METRIC)\n\ncls : train|| Recall: 0.9290780141843972 || Precison: 0.0005804331981204598|| AP: 0.8101152343436091\n____________________\ncls : bus|| Recall: 0.9577464788732394 || Precison: 0.00045156820340048565|| AP: 0.830722622273239\n____________________\ncls : chair|| Recall: 0.873015873015873 || Precison: 0.001433311906044233|| AP: 0.5698849842652579\n____________________\ncls : pottedplant|| Recall: 0.7729166666666667 || Precison: 0.0008064077902035582|| AP: 0.48047763621440476\n____________________\ncls : horse|| Recall: 0.9626436781609196 || Precison: 0.0007370072226707821|| AP: 0.8512804991519783\n____________________\ncls : person|| Recall: 0.9200530035335689 || Precison: 0.009050166947990866|| AP: 0.8107708491164711\n____________________\ncls : bottle|| Recall: 0.8656716417910447 || Precison: 0.0008665714718695106|| AP: 0.63789938616088\n____________________\ncls : bicycle|| Recall: 0.9228486646884273 || Precison: 0.0006832296772124229|| AP: 0.8166723684624742\n____________________\ncls : dog|| Recall: 0.9611451942740287 || Precison: 0.0010255447933963644|| AP: 0.864470680916449\n____________________\ncls : diningtable|| Recall: 0.883495145631068 || Precison: 0.0004040099093419522|| AP: 0.7122255048941863\n____________________\ncls : bird|| Recall: 0.9281045751633987 || Precison: 0.0009227090390830091|| AP: 0.7832546811459113\n____________________\ncls : sofa|| Recall: 0.9707112970711297 || Precison: 0.0005222178954168627|| AP: 0.778305908921783\n____________________\ncls : sheep|| Recall: 0.9132231404958677 || Precison: 0.0004864327094081809|| AP: 0.7463330859344937\n____________________\ncls : boat|| Recall: 0.8783269961977186 || Precison: 0.000509740231082238|| AP: 0.6291419623367831\n____________________\ncls : car|| Recall: 0.9600333055786844 || Precison: 0.0025449055537652685|| AP: 0.8630428431995184\n____________________\ncls : motorbike|| Recall: 0.9538461538461539 || Precison: 0.0006737519288865706|| AP: 0.8224280778332824\n____________________\ncls : aeroplane|| Recall: 0.9438596491228071 || Precison: 0.0006024690030817745|| AP: 0.8001448356711514\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007414480221227399|| AP: 0.8582414148566436\n____________________\ncls : cow|| Recall: 0.9467213114754098 || Precison: 0.0005056795917786568|| AP: 0.8242904910827928\n____________________\ncls : tvmonitor|| Recall: 0.9415584415584416 || Precison: 0.0006278089036291684|| AP: 0.7388745216642896\n____________________\nmAP is : 0.76142887942228   (USE_07_METRIC)\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res101_20181201\'\nNET_NAME = \'resnet_v1_101\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint (20*""++--"")\nprint (ROOT_PATH)\nGPU_GROUP = ""4""\nSHOW_TRAIN_INFO_INTE = 10\nSMRY_ITER = 100\nSAVE_WEIGHTS_INTE = 10000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\ntest_annotate_path = \'/home/yjr/DataSet/VOC/VOC_test/VOC2007/Annotations\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nUSE_07_METRIC = False\nCUDA9 = True\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 0.001  # 0.001  # 0.0003\nDECAY_STEP = [60000, 80000]  # 50000, 70000\nMAX_ITERATION = 150000\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'pascal\'  # \'ship\', \'spacenet\', \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 600  # 600  # 600\nIMG_MAX_LENGTH = 1000  # 1000  # 1000\nCLASS_NUM = 20\n\n# --------------------------------------------- Network_config\nBATCH_SIZE = 1\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = False\n\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]  # addjust the base anchor size for voc.\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# specific settings for FPN\n# FPN_TOP_K_PER_LEVEL_TRAIN = 2000\n# FPN_TOP_K_PER_LEVEL_TEST = 1000\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.3  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 256  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/cfgs_res101_fpn.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ncls : person|| Recall: 0.9200530035335689 || Precison: 0.009050166947990866|| AP: 0.8413662097687251\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007414480221227399|| AP: 0.886410933036462\n____________________\ncls : horse|| Recall: 0.9626436781609196 || Precison: 0.0007370072226707821|| AP: 0.880462817781879\n____________________\ncls : boat|| Recall: 0.8783269961977186 || Precison: 0.000509740231082238|| AP: 0.6456185469835614\n____________________\ncls : bottle|| Recall: 0.8656716417910447 || Precison: 0.0008665714718695106|| AP: 0.6480626365413494\n____________________\ncls : bicycle|| Recall: 0.9228486646884273 || Precison: 0.0006832296772124229|| AP: 0.8550508887926864\n____________________\ncls : bus|| Recall: 0.9577464788732394 || Precison: 0.00045156820340048565|| AP: 0.8631526839193041\n____________________\ncls : sheep|| Recall: 0.9132231404958677 || Precison: 0.0004864327094081809|| AP: 0.7741568397678364\n____________________\ncls : car|| Recall: 0.9600333055786844 || Precison: 0.0025449055537652685|| AP: 0.8914023804170609\n____________________\ncls : motorbike|| Recall: 0.9538461538461539 || Precison: 0.0006737519288865706|| AP: 0.8495072139551133\n____________________\ncls : chair|| Recall: 0.873015873015873 || Precison: 0.001433311906044233|| AP: 0.5759698175528438\n____________________\ncls : aeroplane|| Recall: 0.9438596491228071 || Precison: 0.0006024690030817745|| AP: 0.8353670052573003\n____________________\ncls : tvmonitor|| Recall: 0.9415584415584416 || Precison: 0.0006278089036291684|| AP: 0.7613581746623427\n____________________\ncls : sofa|| Recall: 0.9707112970711297 || Precison: 0.0005222178954168627|| AP: 0.7987407803525022\n____________________\ncls : bird|| Recall: 0.9281045751633987 || Precison: 0.0009227090390830091|| AP: 0.8159836473038345\n____________________\ncls : dog|| Recall: 0.9611451942740287 || Precison: 0.0010255447933963644|| AP: 0.8951754362513265\n____________________\ncls : cow|| Recall: 0.9467213114754098 || Precison: 0.0005056795917786568|| AP: 0.8497306852549179\n____________________\ncls : diningtable|| Recall: 0.883495145631068 || Precison: 0.0004040099093419522|| AP: 0.7307392356452687\n____________________\ncls : pottedplant|| Recall: 0.7729166666666667 || Precison: 0.0008064077902035582|| AP: 0.4738700691112566\n____________________\ncls : train|| Recall: 0.9290780141843972 || Precison: 0.0005804331981204598|| AP: 0.8427500500899303\n____________________\nmAP is : 0.7857438026222752   (USE_12_METRIC)\n\ncls : train|| Recall: 0.9290780141843972 || Precison: 0.0005804331981204598|| AP: 0.8101152343436091\n____________________\ncls : bus|| Recall: 0.9577464788732394 || Precison: 0.00045156820340048565|| AP: 0.830722622273239\n____________________\ncls : chair|| Recall: 0.873015873015873 || Precison: 0.001433311906044233|| AP: 0.5698849842652579\n____________________\ncls : pottedplant|| Recall: 0.7729166666666667 || Precison: 0.0008064077902035582|| AP: 0.48047763621440476\n____________________\ncls : horse|| Recall: 0.9626436781609196 || Precison: 0.0007370072226707821|| AP: 0.8512804991519783\n____________________\ncls : person|| Recall: 0.9200530035335689 || Precison: 0.009050166947990866|| AP: 0.8107708491164711\n____________________\ncls : bottle|| Recall: 0.8656716417910447 || Precison: 0.0008665714718695106|| AP: 0.63789938616088\n____________________\ncls : bicycle|| Recall: 0.9228486646884273 || Precison: 0.0006832296772124229|| AP: 0.8166723684624742\n____________________\ncls : dog|| Recall: 0.9611451942740287 || Precison: 0.0010255447933963644|| AP: 0.864470680916449\n____________________\ncls : diningtable|| Recall: 0.883495145631068 || Precison: 0.0004040099093419522|| AP: 0.7122255048941863\n____________________\ncls : bird|| Recall: 0.9281045751633987 || Precison: 0.0009227090390830091|| AP: 0.7832546811459113\n____________________\ncls : sofa|| Recall: 0.9707112970711297 || Precison: 0.0005222178954168627|| AP: 0.778305908921783\n____________________\ncls : sheep|| Recall: 0.9132231404958677 || Precison: 0.0004864327094081809|| AP: 0.7463330859344937\n____________________\ncls : boat|| Recall: 0.8783269961977186 || Precison: 0.000509740231082238|| AP: 0.6291419623367831\n____________________\ncls : car|| Recall: 0.9600333055786844 || Precison: 0.0025449055537652685|| AP: 0.8630428431995184\n____________________\ncls : motorbike|| Recall: 0.9538461538461539 || Precison: 0.0006737519288865706|| AP: 0.8224280778332824\n____________________\ncls : aeroplane|| Recall: 0.9438596491228071 || Precison: 0.0006024690030817745|| AP: 0.8001448356711514\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007414480221227399|| AP: 0.8582414148566436\n____________________\ncls : cow|| Recall: 0.9467213114754098 || Precison: 0.0005056795917786568|| AP: 0.8242904910827928\n____________________\ncls : tvmonitor|| Recall: 0.9415584415584416 || Precison: 0.0006278089036291684|| AP: 0.7388745216642896\n____________________\nmAP is : 0.76142887942228   (USE_07_METRIC)\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res101_20181201\'\nNET_NAME = \'resnet_v1_101\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint (20*""++--"")\nprint (ROOT_PATH)\nGPU_GROUP = ""4""\nSHOW_TRAIN_INFO_INTE = 10\nSMRY_ITER = 100\nSAVE_WEIGHTS_INTE = 10000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\ntest_annotate_path = \'/home/yjr/DataSet/VOC/VOC_test/VOC2007/Annotations\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nUSE_07_METRIC = False\nCUDA9 = True\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 0.001  # 0.001  # 0.0003\nDECAY_STEP = [60000, 80000]  # 50000, 70000\nMAX_ITERATION = 150000\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'pascal\'  # \'ship\', \'spacenet\', \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 600  # 600  # 600\nIMG_MAX_LENGTH = 1000  # 1000  # 1000\nCLASS_NUM = 20\n\n# --------------------------------------------- Network_config\nBATCH_SIZE = 1\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = False\n\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]  # addjust the base anchor size for voc.\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# specific settings for FPN\n# FPN_TOP_K_PER_LEVEL_TRAIN = 2000\n# FPN_TOP_K_PER_LEVEL_TEST = 1000\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.5  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.3  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 256  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/cfgs_res101_fpn_v1.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res101_20181201_v1\'\nNET_NAME = \'resnet_v1_101\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint (20*""++--"")\nprint (ROOT_PATH)\nGPU_GROUP = ""2""\nSHOW_TRAIN_INFO_INTE = 10\nSMRY_ITER = 100\nSAVE_WEIGHTS_INTE = 10000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\ntest_annotate_path = \'/home/yjr/DataSet/VOC/VOC_test/VOC2007/Annotations\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nUSE_07_METRIC = False\nCUDA9 = True\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 0.001  # 0.001  # 0.0003\nDECAY_STEP = [60000, 80000]  # 50000, 70000\nMAX_ITERATION = 150000\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'pascal\'  # \'ship\', \'spacenet\', \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800  # 600  # 600\nIMG_MAX_LENGTH = 1200  # 1000  # 1000\nCLASS_NUM = 20\n\n# --------------------------------------------- Network_config\nBATCH_SIZE = 1\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = False\n\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]  # addjust the base anchor size for voc.\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# specific settings for FPN\n# FPN_TOP_K_PER_LEVEL_TRAIN = 2000\n# FPN_TOP_K_PER_LEVEL_TEST = 1000\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.5  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.3  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/cfgs_res101_fpn_v2.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\n""""""\ncls : car|| Recall: 0.9542048293089093 || Precison: 0.002486730925298256|| AP: 0.8861557852184543\n____________________\ncls : aeroplane|| Recall: 0.9333333333333333 || Precison: 0.0005925041542114572|| AP: 0.842025283046165\n____________________\ncls : diningtable|| Recall: 0.8932038834951457 || Precison: 0.00040969553387335954|| AP: 0.7380756727563009\n____________________\ncls : cow|| Recall: 0.9672131147540983 || Precison: 0.0005127837421479989|| AP: 0.8598673192007477\n____________________\ncls : boat|| Recall: 0.870722433460076 || Precison: 0.0005048556531707801|| AP: 0.614924861322453\n____________________\ncls : person|| Recall: 0.9112190812720848 || Precison: 0.00890183387270766|| AP: 0.8403573910074297\n____________________\ncls : bottle|| Recall: 0.8550106609808102 || Precison: 0.0008528612324589202|| AP: 0.6431100249936452\n____________________\ncls : bus|| Recall: 0.9624413145539906 || Precison: 0.00045113940207524124|| AP: 0.8462278674877455\n____________________\ncls : chair|| Recall: 0.8465608465608465 || Precison: 0.0013795928045612787|| AP: 0.5683751395230314\n____________________\ncls : train|| Recall: 0.9361702127659575 || Precison: 0.0005837401825514753|| AP: 0.8364460305413468\n____________________\ncls : dog|| Recall: 0.9754601226993865 || Precison: 0.0010383537847668929|| AP: 0.9033364857501106\n____________________\ncls : motorbike|| Recall: 0.92 || Precison: 0.0006513139551094382|| AP: 0.8295629935581179\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007397197236372707|| AP: 0.8871965890034861\n____________________\ncls : sofa|| Recall: 0.9539748953974896 || Precison: 0.0005096076691483964|| AP: 0.8040267075520897\n____________________\ncls : bird|| Recall: 0.9150326797385621 || Precison: 0.0009065830883400464|| AP: 0.8237404489482196\n____________________\ncls : horse|| Recall: 0.9568965517241379 || Precison: 0.0007317362585204424|| AP: 0.8735567893480496\n____________________\ncls : pottedplant|| Recall: 0.7895833333333333 || Precison: 0.0008196455411498826|| AP: 0.4870711669635938\n____________________\ncls : bicycle|| Recall: 0.9109792284866469 || Precison: 0.0006712950308861314|| AP: 0.8361496835573192\n____________________\ncls : tvmonitor|| Recall: 0.9512987012987013 || Precison: 0.0006329305331737686|| AP: 0.7944641602539069\n____________________\ncls : sheep|| Recall: 0.9214876033057852 || Precison: 0.0004898504308706817|| AP: 0.7823429444259854\n____________________\nmAP is : 0.7848506672229101    USE_12_METRIC\n\ncls : tvmonitor|| Recall: 0.9512987012987013 || Precison: 0.0006329305331737686|| AP: 0.7680923930233127\n____________________\ncls : sheep|| Recall: 0.9214876033057852 || Precison: 0.0004898504308706817|| AP: 0.7605331894258903\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007397197236372707|| AP: 0.8561115936556246\n____________________\ncls : train|| Recall: 0.9361702127659575 || Precison: 0.0005837401825514753|| AP: 0.7978443126776701\n____________________\ncls : aeroplane|| Recall: 0.9333333333333333 || Precison: 0.0005925041542114572|| AP: 0.8033498986573708\n____________________\ncls : motorbike|| Recall: 0.92 || Precison: 0.0006513139551094382|| AP: 0.7991141448766482\n____________________\ncls : bus|| Recall: 0.9624413145539906 || Precison: 0.00045113940207524124|| AP: 0.8181027979596772\n____________________\ncls : bird|| Recall: 0.9150326797385621 || Precison: 0.0009065830883400464|| AP: 0.7851247014320806\n____________________\ncls : pottedplant|| Recall: 0.7895833333333333 || Precison: 0.0008196455411498826|| AP: 0.49033575375142174\n____________________\ncls : cow|| Recall: 0.9672131147540983 || Precison: 0.0005127837421479989|| AP: 0.8304367006298838\n____________________\ncls : person|| Recall: 0.9112190812720848 || Precison: 0.00890183387270766|| AP: 0.797530517185023\n____________________\ncls : bottle|| Recall: 0.8550106609808102 || Precison: 0.0008528612324589202|| AP: 0.6320745719617634\n____________________\ncls : sofa|| Recall: 0.9539748953974896 || Precison: 0.0005096076691483964|| AP: 0.7868518534567335\n____________________\ncls : boat|| Recall: 0.870722433460076 || Precison: 0.0005048556531707801|| AP: 0.6036612374959088\n____________________\ncls : car|| Recall: 0.9542048293089093 || Precison: 0.002486730925298256|| AP: 0.8623955910304107\n____________________\ncls : bicycle|| Recall: 0.9109792284866469 || Precison: 0.0006712950308861314|| AP: 0.8029062441256611\n____________________\ncls : dog|| Recall: 0.9754601226993865 || Precison: 0.0010383537847668929|| AP: 0.8661350949646617\n____________________\ncls : diningtable|| Recall: 0.8932038834951457 || Precison: 0.00040969553387335954|| AP: 0.7127169697539509\n____________________\ncls : horse|| Recall: 0.9568965517241379 || Precison: 0.0007317362585204424|| AP: 0.8422342978325045\n____________________\ncls : chair|| Recall: 0.8465608465608465 || Precison: 0.0013795928045612787|| AP: 0.563170854101135\n____________________\nmAP is : 0.7589361358998666   USE_07_METRIC\n""""""\n\n# ------------------------------------------------\nVERSION = \'FPN_Res101_20181201_v2\'\nNET_NAME = \'resnet_v1_101\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint (20*""++--"")\nprint (ROOT_PATH)\nGPU_GROUP = ""3""\nSHOW_TRAIN_INFO_INTE = 10\nSMRY_ITER = 100\nSAVE_WEIGHTS_INTE = 10000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\ntest_annotate_path = \'/home/yjr/DataSet/VOC/VOC_test/VOC2007/Annotations\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nUSE_07_METRIC = False\nCUDA9 = True\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 0.001  # 0.001  # 0.0003\nDECAY_STEP = [60000, 80000]  # 50000, 70000\nMAX_ITERATION = 150000\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'pascal\'  # \'ship\', \'spacenet\', \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 600  # 600  # 600\nIMG_MAX_LENGTH = 1000  # 1000  # 1000\nCLASS_NUM = 20\n\n# --------------------------------------------- Network_config\nBATCH_SIZE = 1\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = False\n\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]  # addjust the base anchor size for voc.\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = False\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# specific settings for FPN\n# FPN_TOP_K_PER_LEVEL_TRAIN = 2000\n# FPN_TOP_K_PER_LEVEL_TEST = 1000\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.5  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.3  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 256  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/cfgs_res50_fpn.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ncls : aeroplane|| Recall: 0.9473684210526315 || Precison: 0.0006199030196164867|| AP: 0.826992691184208\n____________________\ncls : cow|| Recall: 0.9631147540983607 || Precison: 0.0005354526625668462|| AP: 0.8344652186720717\n____________________\ncls : dog|| Recall: 0.9652351738241309 || Precison: 0.0010528593384385115|| AP: 0.8848104631457077\n____________________\ncls : pottedplant|| Recall: 0.7708333333333334 || Precison: 0.000823124000293655|| AP: 0.4527288299945802\n____________________\ncls : diningtable|| Recall: 0.8980582524271845 || Precison: 0.00042887810125232407|| AP: 0.6700510019755388\n____________________\ncls : bird|| Recall: 0.9237472766884531 || Precison: 0.0009519832235409285|| AP: 0.7858394634006082\n____________________\ncls : tvmonitor|| Recall: 0.9415584415584416 || Precison: 0.0006423247726945524|| AP: 0.7532342429791412\n____________________\ncls : chair|| Recall: 0.8452380952380952 || Precison: 0.0014212159291838572|| AP: 0.5629849133883229\n____________________\ncls : train|| Recall: 0.925531914893617 || Precison: 0.0006022581218315107|| AP: 0.81368729431196\n____________________\ncls : horse|| Recall: 0.9454022988505747 || Precison: 0.0007562505602920185|| AP: 0.8603450848286776\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007696817008175631|| AP: 0.8780370107529119\n____________________\ncls : sofa|| Recall: 0.9623430962343096 || Precison: 0.0005431753558979397|| AP: 0.748024582610825\n____________________\ncls : bottle|| Recall: 0.8571428571428571 || Precison: 0.0008744472166693132|| AP: 0.6253912291817303\n____________________\ncls : person|| Recall: 0.9149734982332155 || Precison: 0.009253199981238986|| AP: 0.8351147684067881\n____________________\ncls : car|| Recall: 0.9533721898417985 || Precison: 0.00259228066362385|| AP: 0.8841614814276471\n____________________\ncls : boat|| Recall: 0.8821292775665399 || Precison: 0.0005342347777629379|| AP: 0.6106671555293245\n____________________\ncls : motorbike|| Recall: 0.9323076923076923 || Precison: 0.0006731029825348658|| AP: 0.8421918380864666\n____________________\ncls : bicycle|| Recall: 0.9317507418397626 || Precison: 0.0007036524942688176|| AP: 0.8552669093308443\n____________________\ncls : bus|| Recall: 0.9765258215962441 || Precison: 0.00047651993823568495|| AP: 0.8420876315549962\n____________________\ncls : sheep|| Recall: 0.9049586776859504 || Precison: 0.000502333902950925|| AP: 0.7647489734437813\n____________________\nmAP is : 0.7665415392103065   USE_12_METRIC\n\ncls : bicycle|| Recall: 0.9317507418397626 || Precison: 0.0007036524942688176|| AP: 0.8298982119397122\n____________________\ncls : sofa|| Recall: 0.9623430962343096 || Precison: 0.0005431753558979397|| AP: 0.7272523895735249\n____________________\ncls : bus|| Recall: 0.9765258215962441 || Precison: 0.00047651993823568495|| AP: 0.8137027123104137\n____________________\ncls : diningtable|| Recall: 0.8980582524271845 || Precison: 0.00042887810125232407|| AP: 0.6530525394835751\n____________________\ncls : person|| Recall: 0.9149734982332155 || Precison: 0.009253199981238986|| AP: 0.803256081733613\n____________________\ncls : car|| Recall: 0.9533721898417985 || Precison: 0.00259228066362385|| AP: 0.8577825832291308\n____________________\ncls : boat|| Recall: 0.8821292775665399 || Precison: 0.0005342347777629379|| AP: 0.5979719282542533\n____________________\ncls : chair|| Recall: 0.8452380952380952 || Precison: 0.0014212159291838572|| AP: 0.5599343653732526\n____________________\ncls : aeroplane|| Recall: 0.9473684210526315 || Precison: 0.0006199030196164867|| AP: 0.7917730109896329\n____________________\ncls : cat|| Recall: 0.9608938547486033 || Precison: 0.0007696817008175631|| AP: 0.8475644227001603\n____________________\ncls : sheep|| Recall: 0.9049586776859504 || Precison: 0.000502333902950925|| AP: 0.7327379110779253\n____________________\ncls : train|| Recall: 0.925531914893617 || Precison: 0.0006022581218315107|| AP: 0.7743045860493956\n____________________\ncls : horse|| Recall: 0.9454022988505747 || Precison: 0.0007562505602920185|| AP: 0.8223412836194737\n____________________\ncls : cow|| Recall: 0.9631147540983607 || Precison: 0.0005354526625668462|| AP: 0.8058877343148467\n____________________\ncls : tvmonitor|| Recall: 0.9415584415584416 || Precison: 0.0006423247726945524|| AP: 0.7310441973657807\n____________________\ncls : pottedplant|| Recall: 0.7708333333333334 || Precison: 0.000823124000293655|| AP: 0.4646864671975241\n____________________\ncls : dog|| Recall: 0.9652351738241309 || Precison: 0.0010528593384385115|| AP: 0.8525619478862897\n____________________\ncls : bird|| Recall: 0.9237472766884531 || Precison: 0.0009519832235409285|| AP: 0.7610720209528306\n____________________\ncls : bottle|| Recall: 0.8571428571428571 || Precison: 0.0008744472166693132|| AP: 0.6127328834288011\n____________________\ncls : motorbike|| Recall: 0.9323076923076923 || Precison: 0.0006731029825348658|| AP: 0.8119378019468331\n____________________\nmAP is : 0.7425747539713485    USE_07_METRIC\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_20181201\'\nNET_NAME = \'resnet_v1_50\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint (20*""++--"")\nprint (ROOT_PATH)\nGPU_GROUP = ""1""\nSHOW_TRAIN_INFO_INTE = 10\nSMRY_ITER = 100\nSAVE_WEIGHTS_INTE = 10000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\ntest_annotate_path = \'/home/yjr/DataSet/VOC/VOC_test/VOC2007/Annotations\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nUSE_07_METRIC = False\nCUDA9 = True\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 0.001  # 0.001  # 0.0003\nDECAY_STEP = [60000, 80000]  # 50000, 70000\nMAX_ITERATION = 150000\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'pascal\'  # \'ship\', \'spacenet\', \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 600  # 600  # 600\nIMG_MAX_LENGTH = 1000  # 1000  # 1000\nCLASS_NUM = 20\n\n# --------------------------------------------- Network_config\nBATCH_SIZE = 1\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = False\n\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]  # addjust the base anchor size for voc.\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# specific settings for FPN\n# FPN_TOP_K_PER_LEVEL_TRAIN = 2000\n# FPN_TOP_K_PER_LEVEL_TEST = 1000\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.5  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.3  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 256  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/detection_oprations/__init__.py,0,b''
libs/detection_oprations/anchor_target_layer_without_boxweight.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom libs.configs import cfgs\nimport numpy as np\nimport numpy.random as npr\nfrom libs.box_utils.cython_utils.cython_bbox import bbox_overlaps\nfrom libs.box_utils import encode_and_decode\n\n\ndef anchor_target_layer(\n        gt_boxes, img_shape, all_anchors, is_restrict_bg=False):\n    """"""Same as the anchor target layer in original Fast/er RCNN """"""\n\n    total_anchors = all_anchors.shape[0]\n    img_h, img_w = img_shape[1], img_shape[2]\n    gt_boxes = gt_boxes[:, :-1]  # remove class label\n\n\n    # allow boxes to sit over the edge by a small amount\n    _allowed_border = 0\n\n    # only keep anchors inside the image\n    if cfgs.IS_FILTER_OUTSIDE_BOXES:\n        inds_inside = np.where(\n            (all_anchors[:, 0] >= -_allowed_border) &\n            (all_anchors[:, 1] >= -_allowed_border) &\n            (all_anchors[:, 2] < img_w + _allowed_border) &  # width\n            (all_anchors[:, 3] < img_h + _allowed_border)  # height\n        )[0]\n    else:\n        inds_inside = range(all_anchors.shape[0])\n\n    anchors = all_anchors[inds_inside, :]\n\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    labels = np.empty((len(inds_inside),), dtype=np.float32)\n    labels.fill(-1)\n\n    # overlaps between the anchors and the gt boxes\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(anchors, dtype=np.float),\n        np.ascontiguousarray(gt_boxes, dtype=np.float))\n\n    argmax_overlaps = overlaps.argmax(axis=1)\n    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0)\n    gt_max_overlaps = overlaps[\n        gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    if not cfgs.TRAIN_RPN_CLOOBER_POSITIVES:\n        labels[max_overlaps < cfgs.RPN_IOU_NEGATIVE_THRESHOLD] = 0\n\n    labels[gt_argmax_overlaps] = 1\n    labels[max_overlaps >= cfgs.RPN_IOU_POSITIVE_THRESHOLD] = 1\n\n    if cfgs.TRAIN_RPN_CLOOBER_POSITIVES:\n        labels[max_overlaps < cfgs.RPN_IOU_NEGATIVE_THRESHOLD] = 0\n\n    num_fg = int(cfgs.RPN_MINIBATCH_SIZE * cfgs.RPN_POSITIVE_RATE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n\n    num_bg = cfgs.RPN_MINIBATCH_SIZE - np.sum(labels == 1)\n    if is_restrict_bg:\n        num_bg = max(num_bg, num_fg * 1.5)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n    # map up to original set of anchors\n    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n\n    # labels = labels.reshape((1, height, width, A))\n    rpn_labels = labels.reshape((-1, 1))\n\n    # bbox_targets\n    bbox_targets = bbox_targets.reshape((-1, 4))\n    rpn_bbox_targets = bbox_targets\n\n    return rpn_labels, rpn_bbox_targets\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count,), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # targets = bbox_transform(ex_rois, gt_rois[:, :4]).astype(\n    #     np.float32, copy=False)\n    targets = encode_and_decode.encode_boxes(unencode_boxes=gt_rois,\n                                             reference_boxes=ex_rois,\n                                             scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n    # targets = encode_and_decode.encode_boxes(ex_rois=ex_rois,\n    #                                          gt_rois=gt_rois,\n    #                                          scale_factor=None)\n    return targets\n'"
libs/detection_oprations/proposal_opr.py,6,"b'# encoding: utf-8\n""""""\n@author: zeming li\n@contact: zengarden2009@gmail.com\n""""""\n\nfrom libs.configs import cfgs\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import boxes_utils\nimport tensorflow as tf\nimport numpy as np\n\n\ndef postprocess_rpn_proposals(rpn_bbox_pred, rpn_cls_prob, img_shape, anchors, is_training):\n    \'\'\'\n\n    :param rpn_bbox_pred: [-1, 4]\n    :param rpn_cls_prob: [-1, 2]\n    :param img_shape:\n    :param anchors:[-1, 4]\n    :param is_training:\n    :return:\n    \'\'\'\n\n    if is_training:\n        pre_nms_topN = cfgs.RPN_TOP_K_NMS_TRAIN\n        post_nms_topN = cfgs.RPN_MAXIMUM_PROPOSAL_TARIN\n        # pre_nms_topN = cfgs.FPN_TOP_K_PER_LEVEL_TRAIN\n        # post_nms_topN = pre_nms_topN\n    else:\n        pre_nms_topN = cfgs.RPN_TOP_K_NMS_TEST\n        post_nms_topN = cfgs.RPN_MAXIMUM_PROPOSAL_TEST\n        # pre_nms_topN = cfgs.FPN_TOP_K_PER_LEVEL_TEST\n        # post_nms_topN = pre_nms_topN\n\n    nms_thresh = cfgs.RPN_NMS_IOU_THRESHOLD\n\n    cls_prob = rpn_cls_prob[:, 1]\n\n    # 1. decode boxes\n    decode_boxes = encode_and_decode.decode_boxes(encoded_boxes=rpn_bbox_pred,\n                                                  reference_boxes=anchors,\n                                                  scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # 2. clip to img boundaries\n    decode_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=decode_boxes,\n                                                            img_shape=img_shape)\n\n    # 3. get top N to NMS\n    if pre_nms_topN > 0:\n        pre_nms_topN = tf.minimum(pre_nms_topN, tf.shape(decode_boxes)[0], name=\'avoid_unenough_boxes\')\n        cls_prob, top_k_indices = tf.nn.top_k(cls_prob, k=pre_nms_topN)\n        decode_boxes = tf.gather(decode_boxes, top_k_indices)\n\n    # 4. NMS\n    keep = tf.image.non_max_suppression(\n        boxes=decode_boxes,\n        scores=cls_prob,\n        max_output_size=post_nms_topN,\n        iou_threshold=nms_thresh)\n\n    final_boxes = tf.gather(decode_boxes, keep)\n    final_probs = tf.gather(cls_prob, keep)\n\n    return final_boxes, final_probs\n\n'"
libs/detection_oprations/proposal_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom libs.configs import cfgs\nimport numpy as np\nimport numpy.random as npr\n\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils.cython_utils.cython_bbox import bbox_overlaps\n\n\ndef proposal_target_layer(rpn_rois, gt_boxes):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    """"""\n    # Proposal ROIs (x1, y1, x2, y2) coming from RPN\n    # gt_boxes (x1, y1, x2, y2, label)\n\n    if cfgs.ADD_GTBOXES_TO_TRAIN:\n        all_rois = np.vstack((rpn_rois, gt_boxes[:, :-1]))\n    else:\n        all_rois = rpn_rois\n\n    rois_per_image = np.inf if cfgs.FAST_RCNN_MINIBATCH_SIZE == -1 else cfgs.FAST_RCNN_MINIBATCH_SIZE\n\n    fg_rois_per_image = np.round(cfgs.FAST_RCNN_POSITIVE_RATE * rois_per_image)\n\n    # Sample rois with classification labels and bounding box regression\n    labels, rois, bbox_targets = _sample_rois(all_rois, gt_boxes, fg_rois_per_image,\n                                              rois_per_image, cfgs.CLASS_NUM+1)\n\n    rois = rois.reshape(-1, 4)\n    labels = labels.reshape(-1)\n    bbox_targets = bbox_targets.reshape(-1, (cfgs.CLASS_NUM+1) * 4)\n\n    return rois, labels, bbox_targets\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets (bbox_target_data) are stored in a\n    compact form N x (class, tx, ty, tw, th)\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets).\n\n    Returns:\n        bbox_target (ndarray): N x 4K blob of regression targets\n    """"""\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = int(4 * cls)\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n\n    return bbox_targets\n\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n    """"""Compute bounding-box regression targets for an image.\n    that is : [label, tx, ty, tw, th]\n    """"""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 4\n\n    targets = encode_and_decode.encode_boxes(unencode_boxes=gt_rois,\n                                             reference_boxes=ex_rois,\n                                             scale_factors=cfgs.ROI_SCALE_FACTORS)\n    # targets = encode_and_decode.encode_boxes(ex_rois=ex_rois,\n    #                                          gt_rois=gt_rois,\n    #                                          scale_factor=cfgs.ROI_SCALE_FACTORS)\n\n    return np.hstack(\n        (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n\n\ndef _sample_rois(all_rois, gt_boxes, fg_rois_per_image,\n                 rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n\n    all_rois shape is [-1, 4]\n    gt_boxes shape is [-1, 5]. that is [x1, y1, x2, y2, label]\n    """"""\n    # overlaps: (rois x gt_boxes)\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(all_rois, dtype=np.float),\n        np.ascontiguousarray(gt_boxes[:, :-1], dtype=np.float))\n    gt_assignment = overlaps.argmax(axis=1)\n    max_overlaps = overlaps.max(axis=1)\n    labels = gt_boxes[gt_assignment, -1]\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD) &\n                       (max_overlaps >= cfgs.FAST_RCNN_IOU_NEGATIVE_THRESHOLD))[0]\n    # print(""first fileter, fg_size: {} || bg_size: {}"".format(fg_inds.shape, bg_inds.shape))\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = min(fg_rois_per_image, fg_inds.size)\n\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=int(fg_rois_per_this_image), replace=False)\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=int(bg_rois_per_this_image), replace=False)\n\n    # print(""second fileter, fg_size: {} || bg_size: {}"".format(fg_inds.shape, bg_inds.shape))\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n\n\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n\n    # Clamp labels for the background RoIs to 0\n    labels[int(fg_rois_per_this_image):] = 0\n    rois = all_rois[keep_inds]\n\n    bbox_target_data = _compute_targets(\n        rois, gt_boxes[gt_assignment[keep_inds], :-1], labels)\n\n    bbox_targets = \\\n        _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n    return labels, rois, bbox_targets\n'"
libs/export_pbs/__init__.py,0,b''
libs/export_pbs/exportPb.py,15,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport os, sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.tools import freeze_graph\n\nsys.path.append(\'../../\')\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\n\nCKPT_PATH = \'/home/yjr/PycharmProjects/Faster-RCNN_Tensorflow/output/trained_weights/FasterRCNN_20180517/voc_200000model.ckpt\'\nOUT_DIR = \'../../output/Pbs\'\nPB_NAME = \'FasterRCNN_Res101_Pascal.pb\'\n\n\ndef build_detection_graph():\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3],\n                              name=\'input_img\')  # is RGB. not GBR\n    raw_shape = tf.shape(img_plac)\n    raw_h, raw_w = tf.to_float(raw_shape[0]), tf.to_float(raw_shape[1])\n\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0)  # [1, None, None, 3]\n\n    det_net = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                   is_training=False)\n\n    detected_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                             detected_boxes[:, 2], detected_boxes[:, 3]\n\n    resized_shape = tf.shape(img_batch)\n    resized_h, resized_w = tf.to_float(resized_shape[1]), tf.to_float(resized_shape[2])\n\n    xmin = xmin * raw_w / resized_w\n    xmax = xmax * raw_w / resized_w\n\n    ymin = ymin * raw_h / resized_h\n    ymax = ymax * raw_h / resized_h\n\n    boxes = tf.transpose(tf.stack([xmin, ymin, xmax, ymax]))\n    dets = tf.concat([tf.reshape(detection_category, [-1, 1]),\n                     tf.reshape(detection_scores, [-1, 1]),\n                     boxes], axis=1, name=\'DetResults\')\n\n    return dets\n\n\ndef export_frozenPB():\n\n    tf.reset_default_graph()\n\n    dets = build_detection_graph()\n\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        print(""we have restred the weights from =====>>\\n"", CKPT_PATH)\n        saver.restore(sess, CKPT_PATH)\n\n        tf.train.write_graph(sess.graph_def, OUT_DIR, PB_NAME)\n        freeze_graph.freeze_graph(input_graph=os.path.join(OUT_DIR, PB_NAME),\n                                  input_saver=\'\',\n                                  input_binary=False,\n                                  input_checkpoint=CKPT_PATH,\n                                  output_node_names=""DetResults"",\n                                  restore_op_name=""save/restore_all"",\n                                  filename_tensor_name=\'save/Const:0\',\n                                  output_graph=os.path.join(OUT_DIR, PB_NAME.replace(\'.pb\', \'_Frozen.pb\')),\n                                  clear_devices=False,\n                                  initializer_nodes=\'\')\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'\'\n    export_frozenPB()\n'"
libs/export_pbs/test_exportPb.py,5,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\n\n\n\ndef load_graph(frozen_graph_file):\n\n    # we parse the graph_def file\n    with tf.gfile.GFile(frozen_graph_file, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # we load the graph_def in the default graph\n\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def,\n                            input_map=None,\n                            return_elements=None,\n                            name="""",\n                            op_dict=None,\n                            producer_op_list=None)\n    return graph\n\n\ndef test(frozen_graph_path, test_dir):\n\n    graph = load_graph(frozen_graph_path)\n    print(""we are testing ====>>>>"", frozen_graph_path)\n\n    img = graph.get_tensor_by_name(""input_img:0"")\n    dets = graph.get_tensor_by_name(""DetResults:0"")\n\n    with tf.Session(graph=graph) as sess:\n        for img_path in os.listdir(test_dir):\n            a_img = cv2.imread(os.path.join(test_dir, img_path))[:, :, ::-1]\n            st = time.time()\n            dets_val = sess.run(dets, feed_dict={img: a_img})\n\n            show_indices = dets_val[:, 1] >= 0.5\n            dets_val = dets_val[show_indices]\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(a_img,\n                                                                                boxes=dets_val[:, 2:],\n                                                                                labels=dets_val[:, 0],\n                                                                                scores=dets_val[:, 1])\n            cv2.imwrite(img_path,\n                        final_detections[:, :, ::-1])\n            print (""%s cost time: %f"" % (img_path, time.time() - st))\n\nif __name__ == \'__main__\':\n    test(\'/home/yjr/PycharmProjects/Faster-RCNN_Tensorflow/output/Pbs/FasterRCNN_Res101_Pascal_Frozen.pb\',\n         \'/home/yjr/PycharmProjects/Faster-RCNN_Tensorflow/tools/demos\')\n\n\n\n\n\n\n\n\n\n\n\n'"
libs/label_name_dict/__init__.py,0,b''
libs/label_name_dict/coco_dict.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nclass_names = [\n    'back_ground', 'person', 'bicycle', 'car', 'motorcycle',\n    'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n    'fire hydrant', 'stop sign', 'parking meter', 'bench',\n    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n    'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n    'sports ball', 'kite', 'baseball bat', 'baseball glove',\n    'skateboard', 'surfboard', 'tennis racket', 'bottle',\n    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n    'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear',\n    'hair drier', 'toothbrush']\n\n\nclasses_originID = {\n    'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4,\n    'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9,\n    'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13,\n    'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17,\n    'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22,\n    'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27,\n    'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33,\n    'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37,\n    'kite': 38, 'baseball bat': 39, 'baseball glove': 40,\n    'skateboard': 41, 'surfboard': 42, 'tennis racket': 43,\n    'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48,\n    'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53,\n    'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57,\n    'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61,\n    'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65,\n    'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73,\n    'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77,\n    'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81,\n    'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86,\n    'scissors': 87, 'teddy bear': 88, 'hair drier': 89,\n    'toothbrush': 90}\n\noriginID_classes = {item: key for key, item in classes_originID.items()}\nNAME_LABEL_MAP = dict(zip(class_names, range(len(class_names))))\nLABEL_NAME_MAP = dict(zip(range(len(class_names)), class_names))\n\n# print (originID_classes)\n\n\n\n"""
libs/label_name_dict/label_dict.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nfrom libs.configs import cfgs\n\n\nclass_names = [\n        'back_ground', 'person', 'bicycle', 'car', 'motorcycle',\n        'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n        'fire hydrant', 'stop sign', 'parking meter', 'bench',\n        'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n        'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n        'sports ball', 'kite', 'baseball bat', 'baseball glove',\n        'skateboard', 'surfboard', 'tennis racket', 'bottle',\n        'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n        'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n        'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n        'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n        'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n        'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n        'book', 'clock', 'vase', 'scissors', 'teddy bear',\n        'hair drier', 'toothbrush']\n\nclasses_originID = {\n    'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4,\n    'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9,\n    'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13,\n    'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17,\n    'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22,\n    'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27,\n    'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33,\n    'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37,\n    'kite': 38, 'baseball bat': 39, 'baseball glove': 40,\n    'skateboard': 41, 'surfboard': 42, 'tennis racket': 43,\n    'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48,\n    'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53,\n    'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57,\n    'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61,\n    'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65,\n    'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73,\n    'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77,\n    'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81,\n    'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86,\n    'scissors': 87, 'teddy bear': 88, 'hair drier': 89,\n    'toothbrush': 90}\n\n\ndef get_coco_label_dict():\n    originID_classes = {item: key for key, item in classes_originID.items()}\n    NAME_LABEL_MAP = dict(zip(class_names, range(len(class_names))))\n    return NAME_LABEL_MAP\n\nif cfgs.DATASET_NAME == 'ship':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'ship': 1\n    }\nelif cfgs.DATASET_NAME == 'aeroplane':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1\n    }\nelif cfgs.DATASET_NAME == 'WIDER':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'face': 1\n    }\nelif cfgs.DATASET_NAME == 'icdar':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'text': 1\n    }\nelif cfgs.DATASET_NAME.startswith('DOTA'):\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'roundabout': 1,\n        'tennis-court': 2,\n        'swimming-pool': 3,\n        'storage-tank': 4,\n        'soccer-ball-field': 5,\n        'small-vehicle': 6,\n        'ship': 7,\n        'plane': 8,\n        'large-vehicle': 9,\n        'helicopter': 10,\n        'harbor': 11,\n        'ground-track-field': 12,\n        'bridge': 13,\n        'basketball-court': 14,\n        'baseball-diamond': 15\n    }\nelif cfgs.DATASET_NAME == 'coco':\n    NAME_LABEL_MAP = get_coco_label_dict()\nelif cfgs.DATASET_NAME == 'pascal':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1,\n        'bicycle': 2,\n        'bird': 3,\n        'boat': 4,\n        'bottle': 5,\n        'bus': 6,\n        'car': 7,\n        'cat': 8,\n        'chair': 9,\n        'cow': 10,\n        'diningtable': 11,\n        'dog': 12,\n        'horse': 13,\n        'motorbike': 14,\n        'person': 15,\n        'pottedplant': 16,\n        'sheep': 17,\n        'sofa': 18,\n        'train': 19,\n        'tvmonitor': 20\n    }\nelif cfgs.DATASET_NAME == 'bdd100k':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'bus': 1,\n        'traffic light': 2,\n        'traffic sign': 3,\n        'person': 4,\n        'bike': 5,\n        'truck': 6,\n        'motor': 7,\n        'car': 8,\n        'train': 9,\n        'rider': 10\n    }\nelse:\n    assert 'please set label dict!'\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\n\nLABEl_NAME_MAP = get_label_name_map()"""
libs/label_name_dict/remote_sensing_dict.py,0,"b""# -*- coding: utf-8 -*-\n\nNAME_LABEL_MAP = {\n    'back_ground': 0,\n    'building': 1\n}\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\nLABEl_NAME_MAP = get_label_name_map()"""
libs/losses/__init__.py,0,b''
libs/losses/losses.py,22,"b'# -*- coding: utf-8 -*-\n""""""\n@author: jemmy li\n@contact: zengarden2009@gmail.com\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4] in RPN. [-1, cls_num+1, 4] in Fast-rcnn\n    :param bbox_targets: shape is same as bbox_pred\n    :param sigma:\n    :return:\n    \'\'\'\n    sigma_2 = sigma**2\n\n    box_diff = bbox_pred - bbox_targets\n\n    abs_box_diff = tf.abs(box_diff)\n\n    smoothL1_sign = tf.stop_gradient(\n        tf.to_float(tf.less(abs_box_diff, 1. / sigma_2)))\n    loss_box = tf.pow(box_diff, 2) * (sigma_2 / 2.0) * smoothL1_sign \\\n               + (abs_box_diff - (0.5 / sigma_2)) * (1.0 - smoothL1_sign)\n    return loss_box\n\ndef smooth_l1_loss_rpn(bbox_pred, bbox_targets, label, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4]\n    :param bbox_targets: [-1, 4]\n    :param label: [-1]\n    :param sigma:\n    :return:\n    \'\'\'\n    value = _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=sigma)\n    value = tf.reduce_sum(value, axis=1)  # to sum in axis 1\n    rpn_positive = tf.where(tf.greater(label, 0))\n\n    # rpn_select = tf.stop_gradient(rpn_select) # to avoid\n    selected_value = tf.gather(value, rpn_positive)\n    non_ignored_mask = tf.stop_gradient(\n        1.0 - tf.to_float(tf.equal(label, -1)))  # positve is 1.0 others is 0.0\n\n    bbox_loss = tf.reduce_sum(selected_value) / tf.maximum(1.0, tf.reduce_sum(non_ignored_mask))\n\n    return bbox_loss\n\ndef smooth_l1_loss_rcnn(bbox_pred, bbox_targets, label, num_classes, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, (cfgs.CLS_NUM +1) * 4]\n    :param bbox_targets:[-1, (cfgs.CLS_NUM +1) * 4]\n    :param label:[-1]\n    :param num_classes:\n    :param sigma:\n    :return:\n    \'\'\'\n\n    outside_mask = tf.stop_gradient(tf.to_float(tf.greater(label, 0)))\n\n    bbox_pred = tf.reshape(bbox_pred, [-1, num_classes, 4])\n    bbox_targets = tf.reshape(bbox_targets, [-1, num_classes, 4])\n\n    value = _smooth_l1_loss_base(bbox_pred,\n                                 bbox_targets,\n                                 sigma=sigma)\n    value = tf.reduce_sum(value, 2)\n    value = tf.reshape(value, [-1, num_classes])\n\n    inside_mask = tf.one_hot(tf.reshape(label, [-1, 1]),\n                             depth=num_classes, axis=1)\n\n    inside_mask = tf.stop_gradient(\n        tf.to_float(tf.reshape(inside_mask, [-1, num_classes])))\n\n    normalizer = tf.to_float(tf.shape(bbox_pred)[0])\n    bbox_loss = tf.reduce_sum(\n        tf.reduce_sum(value * inside_mask, 1)*outside_mask) / normalizer\n\n    return bbox_loss\n\ndef sum_ohem_loss(cls_score, label, bbox_pred, bbox_targets,\n                  nr_ohem_sampling, nr_classes, sigma=1.0):\n\n    raise NotImplementedError(\'not implement Now. YJR will implemetn in the future\')'"
libs/networks/__init__.py,0,b''
libs/networks/build_whole_network.py,133,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet\nfrom libs.networks import mobilenet_v2\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import boxes_utils\nfrom libs.box_utils import anchor_utils\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.proposal_opr import postprocess_rpn_proposals\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.proposal_target_layer import proposal_target_layer\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet or mobilenet_v2\')\n\n    def postprocess_fastrcnn(self, rois, bbox_ppred, scores, img_shape):\n        \'\'\'\n\n        :param rois:[-1, 4]\n        :param bbox_ppred: [-1, (cfgs.Class_num+1) * 4]\n        :param scores: [-1, cfgs.Class_num + 1]\n        :return:\n        \'\'\'\n\n        with tf.name_scope(\'postprocess_fastrcnn\'):\n            rois = tf.stop_gradient(rois)\n            scores = tf.stop_gradient(scores)\n            bbox_ppred = tf.reshape(bbox_ppred, [-1, cfgs.CLASS_NUM + 1, 4])\n            bbox_ppred = tf.stop_gradient(bbox_ppred)\n\n            bbox_pred_list = tf.unstack(bbox_ppred, axis=1)\n            score_list = tf.unstack(scores, axis=1)\n\n            allclasses_boxes = []\n            allclasses_scores = []\n            categories = []\n            for i in range(1, cfgs.CLASS_NUM+1):\n\n                # 1. decode boxes in each class\n                tmp_encoded_box = bbox_pred_list[i]\n                tmp_score = score_list[i]\n                tmp_decoded_boxes = encode_and_decode.decode_boxes(encoded_boxes=tmp_encoded_box,\n                                                                   reference_boxes=rois,\n                                                                   scale_factors=cfgs.ROI_SCALE_FACTORS)\n                # tmp_decoded_boxes = encode_and_decode.decode_boxes(boxes=rois,\n                #                                                    deltas=tmp_encoded_box,\n                #                                                    scale_factor=cfgs.ROI_SCALE_FACTORS)\n\n                # 2. clip to img boundaries\n                tmp_decoded_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=tmp_decoded_boxes,\n                                                                             img_shape=img_shape)\n\n                # 3. NMS\n                keep = tf.image.non_max_suppression(\n                    boxes=tmp_decoded_boxes,\n                    scores=tmp_score,\n                    max_output_size=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                    iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD)\n\n                perclass_boxes = tf.gather(tmp_decoded_boxes, keep)\n                perclass_scores = tf.gather(tmp_score, keep)\n\n                allclasses_boxes.append(perclass_boxes)\n                allclasses_scores.append(perclass_scores)\n                categories.append(tf.ones_like(perclass_scores) * i)\n\n            final_boxes = tf.concat(allclasses_boxes, axis=0)\n            final_scores = tf.concat(allclasses_scores, axis=0)\n            final_category = tf.concat(categories, axis=0)\n\n            if self.is_training:\n                \'\'\'\n                in training. We should show the detecitons in the tensorboard. So we add this.\n                \'\'\'\n                kept_indices = tf.reshape(tf.where(tf.greater_equal(final_scores, cfgs.SHOW_SCORE_THRSHOLD)), [-1])\n\n                final_boxes = tf.gather(final_boxes, kept_indices)\n                final_scores = tf.gather(final_scores, kept_indices)\n                final_category = tf.gather(final_category, kept_indices)\n\n        return final_boxes, final_scores, final_category\n\n    def roi_pooling(self, feature_maps, rois, img_shape, scope):\n        \'\'\'\n        Here use roi warping as roi_pooling\n\n        :param featuremaps_dict: feature map to crop\n        :param rois: shape is [-1, 4]. [x1, y1, x2, y2]\n        :return:\n        \'\'\'\n\n        with tf.variable_scope(\'ROI_Warping_\'+scope):\n            img_h, img_w = tf.cast(img_shape[1], tf.float32), tf.cast(img_shape[2], tf.float32)\n            N = tf.shape(rois)[0]\n            x1, y1, x2, y2 = tf.unstack(rois, axis=1)\n\n            normalized_x1 = x1 / img_w\n            normalized_x2 = x2 / img_w\n            normalized_y1 = y1 / img_h\n            normalized_y2 = y2 / img_h\n\n            normalized_rois = tf.transpose(\n                tf.stack([normalized_y1, normalized_x1, normalized_y2, normalized_x2]), name=\'get_normalized_rois\')\n\n            normalized_rois = tf.stop_gradient(normalized_rois)\n\n            cropped_roi_features = tf.image.crop_and_resize(feature_maps, normalized_rois,\n                                                            box_ind=tf.zeros(shape=[N, ],\n                                                                             dtype=tf.int32),\n                                                            crop_size=[cfgs.ROI_SIZE, cfgs.ROI_SIZE],\n                                                            name=\'CROP_AND_RESIZE\'\n                                                            )\n            roi_features = slim.max_pool2d(cropped_roi_features,\n                                           [cfgs.ROI_POOL_KERNEL_SIZE, cfgs.ROI_POOL_KERNEL_SIZE],\n                                           stride=cfgs.ROI_POOL_KERNEL_SIZE)\n\n        return roi_features\n\n    def build_fastrcnn(self, P_list, rois_list, img_shape):\n\n        with tf.variable_scope(\'Fast-RCNN\'):\n            # 5. ROI Pooling\n            with tf.variable_scope(\'rois_pooling\'):\n                pooled_features_list = []\n                for level_name, p, rois in zip(cfgs.LEVLES, P_list, rois_list):  # exclude P6_rois\n                    # p = tf.Print(p, [tf.shape(p)], summarize=10, message=level_name+\'SHPAE***\')\n                    pooled_features = self.roi_pooling(feature_maps=p, rois=rois, img_shape=img_shape,\n                                                       scope=level_name)\n                    pooled_features_list.append(pooled_features)\n\n                pooled_features = tf.concat(pooled_features_list, axis=0) # [minibatch_size, H, W, C]\n\n            # 6. inferecne rois in Fast-RCNN to obtain fc_flatten features\n            if self.base_network_name.startswith(\'resnet\'):\n                fc_flatten = resnet.restnet_head(inputs=pooled_features,\n                                                 is_training=self.is_training,\n                                                 scope_name=self.base_network_name)\n            elif self.base_network_name.startswith(\'Mobile\'):\n                fc_flatten = mobilenet_v2.mobilenetv2_head(inputs=pooled_features,\n                                                           is_training=self.is_training)\n            else:\n                raise NotImplementedError(\'only support resnet and mobilenet\')\n\n            # 7. cls and reg in Fast-RCNN\n            with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n\n                cls_score = slim.fully_connected(fc_flatten,\n                                                 num_outputs=cfgs.CLASS_NUM+1,\n                                                 weights_initializer=cfgs.INITIALIZER,\n                                                 activation_fn=None, trainable=self.is_training,\n                                                 scope=\'cls_fc\')\n\n                bbox_pred = slim.fully_connected(fc_flatten,\n                                                 num_outputs=(cfgs.CLASS_NUM+1)*4,\n                                                 weights_initializer=cfgs.BBOX_INITIALIZER,\n                                                 activation_fn=None, trainable=self.is_training,\n                                                 scope=\'reg_fc\')\n                # for convient. It also produce (cls_num +1) bboxes\n\n                cls_score = tf.reshape(cls_score, [-1, cfgs.CLASS_NUM+1])\n                bbox_pred = tf.reshape(bbox_pred, [-1, 4*(cfgs.CLASS_NUM+1)])\n\n                return bbox_pred, cls_score\n\n    def assign_levels(self, all_rois, labels=None, bbox_targets=None):\n        \'\'\'\n\n        :param all_rois:\n        :param labels:\n        :param bbox_targets:\n        :return:\n        \'\'\'\n        with tf.name_scope(\'assign_levels\'):\n            # all_rois = tf.Print(all_rois, [tf.shape(all_rois)], summarize=10, message=\'ALL_ROIS_SHAPE*****\')\n            xmin, ymin, xmax, ymax = tf.unstack(all_rois, axis=1)\n\n            h = tf.maximum(0., ymax - ymin)\n            w = tf.maximum(0., xmax - xmin)\n\n            levels = tf.floor(4. + tf.log(tf.sqrt(w * h + 1e-8) / 224.0) / tf.log(2.))  # 4 + log_2(***)\n            # use floor instead of round\n\n            min_level = int(cfgs.LEVLES[0][-1])\n            max_level = min(5, int(cfgs.LEVLES[-1][-1]))\n            levels = tf.maximum(levels, tf.ones_like(levels) * min_level)  # level minimum is 2\n            levels = tf.minimum(levels, tf.ones_like(levels) * max_level)  # level maximum is 5\n\n            levels = tf.stop_gradient(tf.reshape(levels, [-1]))\n\n            def get_rois(levels, level_i, rois, labels, bbox_targets):\n\n                level_i_indices = tf.reshape(tf.where(tf.equal(levels, level_i)), [-1])\n                # level_i_indices = tf.Print(level_i_indices, [tf.shape(tf.where(tf.equal(levels, level_i)))[0]], message=""SHAPE%d***""%level_i,\n                #                            summarize=10)\n                tf.summary.scalar(\'LEVEL/LEVEL_%d_rois_NUM\'%level_i, tf.shape(level_i_indices)[0])\n                level_i_rois = tf.gather(rois, level_i_indices)\n\n                if self.is_training:\n                    if cfgs.CUDA9:\n                        # Note: for cuda 9\n                        level_i_rois = tf.stop_gradient(level_i_rois)\n                        level_i_labels = tf.gather(labels, level_i_indices)\n    \n                        level_i_targets = tf.gather(bbox_targets, level_i_indices)\n                    else:\n                        \n                        # Note: for cuda 8\n                        level_i_rois = tf.stop_gradient(tf.concat([level_i_rois, [[0, 0, 0., 0.]]], axis=0))\n                        # to avoid the num of level i rois is 0.0, which will broken the BP in tf\n    \n                        level_i_labels = tf.gather(labels, level_i_indices)\n                        level_i_labels = tf.stop_gradient(tf.concat([level_i_labels, [0]], axis=0))\n    \n                        level_i_targets = tf.gather(bbox_targets, level_i_indices)\n                        level_i_targets = tf.stop_gradient(tf.concat([level_i_targets,\n                                                                      tf.zeros(shape=(1, 4*(cfgs.CLASS_NUM+1)),\n                                                                               dtype=tf.float32)], axis=0))\n           \n                    return level_i_rois, level_i_labels, level_i_targets\n                else:\n                    if not cfgs.CUDA9:\n                        # Note: for cuda 8\n                        level_i_rois = tf.concat([level_i_rois, [[0, 0, 0., 0.]]], axis=0)\n                    return level_i_rois, None, None\n\n            rois_list = []\n            labels_list = []\n            targets_list = []\n            for i in range(min_level, max_level+1):\n                P_i_rois, P_i_labels, P_i_targets = get_rois(levels, level_i=i, rois=all_rois,\n                                                             labels=labels,\n                                                             bbox_targets=bbox_targets)\n                rois_list.append(P_i_rois)\n                labels_list.append(P_i_labels)\n                targets_list.append(P_i_targets)\n\n            if self.is_training:\n                all_labels = tf.concat(labels_list, axis=0)\n                all_targets = tf.concat(targets_list, axis=0)\n                return rois_list, all_labels, all_targets\n            else:\n                return rois_list  # [P2_rois, P3_rois, P4_rois, P5_rois] Note: P6 do not assign rois\n\n    def add_anchor_img_smry(self, img, anchors, labels):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor)\n        neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def add_roi_batch_img_smry(self, img, rois, labels):\n        positive_roi_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n\n        negative_roi_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        pos_roi = tf.gather(rois, positive_roi_indices)\n        neg_roi = tf.gather(rois, negative_roi_indices)\n\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                               boxes=pos_roi)\n        neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                               boxes=neg_roi)\n        tf.summary.image(\'pos_rois\', pos_in_img)\n        tf.summary.image(\'neg_rois\', neg_in_img)\n\n    def build_loss(self, rpn_box_pred, rpn_bbox_targets, rpn_cls_score, rpn_labels,\n                   bbox_pred, bbox_targets, cls_score, labels):\n        \'\'\'\n\n        :param rpn_box_pred: [-1, 4]\n        :param rpn_bbox_targets: [-1, 4]\n        :param rpn_cls_score: [-1]\n        :param rpn_labels: [-1]\n        :param bbox_pred: [-1, 4*(cls_num+1)]\n        :param bbox_targets: [-1, 4*(cls_num+1)]\n        :param cls_score: [-1, cls_num+1]\n        :param labels: [-1]\n        :return:\n        \'\'\'\n        with tf.variable_scope(\'build_loss\') as sc:\n            with tf.variable_scope(\'rpn_loss\'):\n\n                rpn_bbox_loss = losses.smooth_l1_loss_rpn(bbox_pred=rpn_box_pred,\n                                                          bbox_targets=rpn_bbox_targets,\n                                                          label=rpn_labels,\n                                                          sigma=cfgs.RPN_SIGMA)\n                # rpn_cls_loss:\n                # rpn_cls_score = tf.reshape(rpn_cls_score, [-1, 2])\n                # rpn_labels = tf.reshape(rpn_labels, [-1])\n                # ensure rpn_labels shape is [-1]\n                rpn_select = tf.reshape(tf.where(tf.not_equal(rpn_labels, -1)), [-1])\n                rpn_cls_score = tf.reshape(tf.gather(rpn_cls_score, rpn_select), [-1, 2])\n                rpn_labels = tf.reshape(tf.gather(rpn_labels, rpn_select), [-1])\n                rpn_cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rpn_cls_score,\n                                                                                             labels=rpn_labels))\n\n                rpn_cls_loss = rpn_cls_loss * cfgs.RPN_CLASSIFICATION_LOSS_WEIGHT\n                rpn_bbox_loss = rpn_bbox_loss * cfgs.RPN_LOCATION_LOSS_WEIGHT\n\n            with tf.variable_scope(\'FastRCNN_loss\'):\n                if not cfgs.FAST_RCNN_MINIBATCH_SIZE == -1:\n                    bbox_loss = losses.smooth_l1_loss_rcnn(bbox_pred=bbox_pred,\n                                                           bbox_targets=bbox_targets,\n                                                           label=labels,\n                                                           num_classes=cfgs.CLASS_NUM + 1,\n                                                           sigma=cfgs.FASTRCNN_SIGMA)\n\n                    # cls_score = tf.reshape(cls_score, [-1, cfgs.CLASS_NUM + 1])\n                    # labels = tf.reshape(labels, [-1])\n                    cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                        logits=cls_score,\n                        labels=labels))  # beacause already sample before\n                else:\n                    \'\'\' \n                    applying OHEM here\n                    \'\'\'\n                    print(20 * ""@@"")\n                    print(""@@"" + 10 * "" "" + ""TRAIN WITH OHEM ..."")\n                    print(20 * ""@@"")\n                    cls_loss = bbox_loss = losses.sum_ohem_loss(\n                        cls_score=cls_score,\n                        label=labels,\n                        bbox_targets=bbox_targets,\n                        nr_ohem_sampling=128,\n                        nr_classes=cfgs.CLASS_NUM + 1)\n                cls_loss = cls_loss * cfgs.FAST_RCNN_CLASSIFICATION_LOSS_WEIGHT\n                bbox_loss = bbox_loss * cfgs.FAST_RCNN_LOCATION_LOSS_WEIGHT\n            loss_dict = {\n                \'rpn_cls_loss\': rpn_cls_loss,\n                \'rpn_loc_loss\': rpn_bbox_loss,\n                \'fastrcnn_cls_loss\': cls_loss,\n                \'fastrcnn_loc_loss\': bbox_loss\n            }\n        return loss_dict\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch):\n\n        if self.is_training:\n            # ensure shape is [M, 5]\n            gtboxes_batch = tf.reshape(gtboxes_batch, [-1, 5])\n            gtboxes_batch = tf.cast(gtboxes_batch, tf.float32)\n\n        img_shape = tf.shape(input_img_batch)\n\n        # 1. build base network\n        P_list = self.build_base_network(input_img_batch)  # [P2, P3, P4, P5, P6]\n\n        # 2. build rpn\n        with tf.variable_scope(\'build_rpn\',\n                               regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n\n            fpn_cls_score =[]\n            fpn_box_pred = []\n            for level_name, p in zip(cfgs.LEVLES, P_list):\n                if cfgs.SHARE_HEADS:\n                    reuse_flag = None if level_name==cfgs.LEVLES[0] else True\n                    scope_list=[\'rpn_conv/3x3\', \'rpn_cls_score\', \'rpn_bbox_pred\']\n                else:\n                    reuse_flag = None\n                    scope_list= [\'rpn_conv/3x3_%s\' % level_name, \'rpn_cls_score_%s\' % level_name, \'rpn_bbox_pred_%s\' % level_name]\n                rpn_conv3x3 = slim.conv2d(\n                    p, 512, [3, 3],\n                    trainable=self.is_training, weights_initializer=cfgs.INITIALIZER, padding=""SAME"",\n                    activation_fn=tf.nn.relu,\n                    scope=scope_list[0],\n                    reuse=reuse_flag)\n                rpn_cls_score = slim.conv2d(rpn_conv3x3, self.num_anchors_per_location*2, [1, 1], stride=1,\n                                            trainable=self.is_training, weights_initializer=cfgs.INITIALIZER,\n                                            activation_fn=None, padding=""VALID"",\n                                            scope=scope_list[1],\n                                            reuse=reuse_flag)\n                rpn_box_pred = slim.conv2d(rpn_conv3x3, self.num_anchors_per_location*4, [1, 1], stride=1,\n                                           trainable=self.is_training, weights_initializer=cfgs.BBOX_INITIALIZER,\n                                           activation_fn=None, padding=""VALID"",\n                                           scope=scope_list[2],\n                                           reuse=reuse_flag)\n                rpn_box_pred = tf.reshape(rpn_box_pred, [-1, 4])\n                rpn_cls_score = tf.reshape(rpn_cls_score, [-1, 2])\n\n                fpn_cls_score.append(rpn_cls_score)\n                fpn_box_pred.append(rpn_box_pred)\n\n            fpn_cls_score = tf.concat(fpn_cls_score, axis=0, name=\'fpn_cls_score\')\n            fpn_box_pred = tf.concat(fpn_box_pred, axis=0, name=\'fpn_box_pred\')\n            fpn_cls_prob = slim.softmax(fpn_cls_score, scope=\'fpn_cls_prob\')\n\n        # 3. generate_anchors\n        all_anchors = []\n        for i in range(len(cfgs.LEVLES)):\n            level_name, p = cfgs.LEVLES[i], P_list[i]\n\n            p_h, p_w = tf.shape(p)[1], tf.shape(p)[2]\n            featuremap_height = tf.cast(p_h, tf.float32)\n            featuremap_width = tf.cast(p_w, tf.float32)\n            anchors = anchor_utils.make_anchors(base_anchor_size=cfgs.BASE_ANCHOR_SIZE_LIST[i],\n                                                anchor_scales=cfgs.ANCHOR_SCALES,\n                                                anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                featuremap_height=featuremap_height,\n                                                featuremap_width=featuremap_width,\n                                                stride=cfgs.ANCHOR_STRIDE_LIST[i],\n                                                name=""make_anchors_for%s"" % level_name)\n            all_anchors.append(anchors)\n        all_anchors = tf.concat(all_anchors, axis=0, name=\'all_anchors_of_FPN\')\n\n        # 4. postprocess rpn proposals. such as: decode, clip, NMS\n        with tf.variable_scope(\'postprocess_FPN\'):\n            rois, roi_scores = postprocess_rpn_proposals(rpn_bbox_pred=fpn_box_pred,\n                                                         rpn_cls_prob=fpn_cls_prob,\n                                                         img_shape=img_shape,\n                                                         anchors=all_anchors,\n                                                         is_training=self.is_training)\n            # rois shape [-1, 4]\n            # +++++++++++++++++++++++++++++++++++++add img smry+++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n            if self.is_training:\n                score_gre_05 = tf.reshape(tf.where(tf.greater_equal(roi_scores, 0.5)), [-1])\n                score_gre_05_rois = tf.gather(rois, score_gre_05)\n                score_gre_05_score = tf.gather(roi_scores, score_gre_05)\n                score_gre_05_in_img = show_box_in_tensor.draw_boxes_with_scores(img_batch=input_img_batch,\n                                                                                boxes=score_gre_05_rois,\n                                                                                scores=score_gre_05_score)\n                tf.summary.image(\'score_greater_05_rois\', score_gre_05_in_img)\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n        if self.is_training:\n            with tf.variable_scope(\'sample_anchors_minibatch\'):\n                fpn_labels, fpn_bbox_targets = \\\n                    tf.py_func(\n                        anchor_target_layer,\n                        [gtboxes_batch, img_shape, all_anchors],\n                        [tf.float32, tf.float32])\n                fpn_bbox_targets = tf.reshape(fpn_bbox_targets, [-1, 4])\n                fpn_labels = tf.to_int32(fpn_labels, name=""to_int32"")\n                fpn_labels = tf.reshape(fpn_labels, [-1])\n                self.add_anchor_img_smry(input_img_batch, all_anchors, fpn_labels)\n\n            # --------------------------------------add smry-----------------------------------------------------------\n\n            fpn_cls_category = tf.argmax(fpn_cls_prob, axis=1)\n            kept_rpppn = tf.reshape(tf.where(tf.not_equal(fpn_labels, -1)), [-1])\n            fpn_cls_category = tf.gather(fpn_cls_category, kept_rpppn)\n            acc = tf.reduce_mean(tf.to_float(tf.equal(fpn_cls_category,\n                                                      tf.to_int64(tf.gather(fpn_labels, kept_rpppn)))))\n            tf.summary.scalar(\'ACC/fpn_accuracy\', acc)\n\n            with tf.control_dependencies([fpn_labels]):\n                with tf.variable_scope(\'sample_RCNN_minibatch\'):\n                    rois, labels, bbox_targets = \\\n                    tf.py_func(proposal_target_layer,\n                               [rois, gtboxes_batch],\n                               [tf.float32, tf.float32, tf.float32])\n                    rois = tf.reshape(rois, [-1, 4])\n                    labels = tf.to_int32(labels)\n                    labels = tf.reshape(labels, [-1])\n                    bbox_targets = tf.reshape(bbox_targets, [-1, 4*(cfgs.CLASS_NUM+1)])\n                    self.add_roi_batch_img_smry(input_img_batch, rois, labels)\n        if self.is_training:\n            rois_list, labels, bbox_targets = self.assign_levels(all_rois=rois,\n                                                                 labels=labels,\n                                                                 bbox_targets=bbox_targets)\n        else:\n            rois_list = self.assign_levels(all_rois=rois)  # rois_list: [P2_rois, P3_rois, P4_rois, P5_rois]\n\n        # -------------------------------------------------------------------------------------------------------------#\n        #                                            Fast-RCNN                                                         #\n        # -------------------------------------------------------------------------------------------------------------#\n\n        # 5. build Fast-RCNN\n        # rois = tf.Print(rois, [tf.shape(rois)], \'rois shape\', summarize=10)\n        bbox_pred, cls_score = self.build_fastrcnn(P_list=P_list, rois_list=rois_list,\n                                                   img_shape=img_shape)\n        # bbox_pred shape: [-1, 4*(cls_num+1)].\n        # cls_score shape\xef\xbc\x9a [-1, cls_num+1]\n\n        cls_prob = slim.softmax(cls_score, \'cls_prob\')\n\n\n        # ----------------------------------------------add smry-------------------------------------------------------\n        if self.is_training:\n            cls_category = tf.argmax(cls_prob, axis=1)\n            fast_acc = tf.reduce_mean(tf.to_float(tf.equal(cls_category, tf.to_int64(labels))))\n            tf.summary.scalar(\'ACC/fast_acc\', fast_acc)\n\n        rois = tf.concat(rois_list, axis=0, name=\'concat_rois\')\n        #  6. postprocess_fastrcnn\n        if not self.is_training:\n            return self.postprocess_fastrcnn(rois=rois, bbox_ppred=bbox_pred, scores=cls_prob, img_shape=img_shape)\n        else:\n            \'\'\'\n            when trian. We need build Loss\n            \'\'\'\n            loss_dict = self.build_loss(rpn_box_pred=fpn_box_pred,\n                                        rpn_bbox_targets=fpn_bbox_targets,\n                                        rpn_cls_score=fpn_cls_score,\n                                        rpn_labels=fpn_labels,\n                                        bbox_pred=bbox_pred,\n                                        bbox_targets=bbox_targets,\n                                        cls_score=cls_score,\n                                        labels=labels)\n\n            final_bbox, final_scores, final_category = self.postprocess_fastrcnn(rois=rois,\n                                                                                 bbox_ppred=bbox_pred,\n                                                                                 scores=cls_prob,\n                                                                                 img_shape=img_shape)\n            return final_bbox, final_scores, final_category, loss_dict\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(self.base_network_name):\n                    var_name_in_ckpt = name_in_ckpt_rpn(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
libs/networks/mobilenet_v2.py,4,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport tensorflow.contrib.slim as slim\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import mobilenet_v2\nfrom libs.networks.mobilenet.mobilenet import training_scope\nfrom libs.networks.mobilenet.mobilenet_v2 import op\nfrom libs.networks.mobilenet.mobilenet_v2 import ops\nexpand_input = ops.expand_input_by_factor\n\nV2_BASE_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16, scope=\'expanded_conv\'),\n        op(ops.expanded_conv, stride=2, num_outputs=24, scope=\'expanded_conv_1\'),\n        op(ops.expanded_conv, stride=1, num_outputs=24, scope=\'expanded_conv_2\'),\n        op(ops.expanded_conv, stride=2, num_outputs=32, scope=\'expanded_conv_3\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_4\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_5\'),\n        op(ops.expanded_conv, stride=2, num_outputs=64, scope=\'expanded_conv_6\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_7\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_8\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_9\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_10\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_11\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_12\')\n    ],\n)\n\n\nV2_HEAD_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(ops.expanded_conv, stride=2, num_outputs=160, scope=\'expanded_conv_13\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_14\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_15\'),\n        op(ops.expanded_conv, stride=1, num_outputs=320, scope=\'expanded_conv_16\'),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280, scope=\'Conv_1\')\n    ],\n)\ndef mobilenetv2_scope(is_training=True,\n                      trainable=True,\n                      weight_decay=0.00004,\n                      stddev=0.09,\n                      dropout_keep_prob=0.8,\n                      bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n  In default. We do not use BN\n\n  ReWrite the scope.\n  """"""\n  batch_norm_params = {\n      \'is_training\': False,\n      \'trainable\': False,\n      \'decay\': bn_decay,\n  }\n  with slim.arg_scope(training_scope(is_training=is_training, weight_decay=weight_decay)):\n      with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n                          trainable=trainable):\n          with slim.arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n              return sc\n\n\n\ndef mobilenetv2_base(img_batch, is_training=True):\n\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n\n        feature_to_crop, endpoints = mobilenet_v2.mobilenet_base(input_tensor=img_batch,\n                                                      num_classes=None,\n                                                      is_training=False,\n                                                      depth_multiplier=1.0,\n                                                      scope=\'MobilenetV2\',\n                                                      conv_defs=V2_BASE_DEF,\n                                                      finegrain_classification_mode=False)\n\n        # feature_to_crop = tf.Print(feature_to_crop, [tf.shape(feature_to_crop)], summarize=10, message=\'rpn_shape\')\n        return feature_to_crop\n\n\ndef mobilenetv2_head(inputs, is_training=True):\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n        net, _ = mobilenet_v2.mobilenet(input_tensor=inputs,\n                                        num_classes=None,\n                                        is_training=False,\n                                        depth_multiplier=1.0,\n                                        scope=\'MobilenetV2\',\n                                        conv_defs=V2_HEAD_DEF,\n                                        finegrain_classification_mode=False)\n\n        net = tf.squeeze(net, [1, 2])\n\n        return net'"
libs/networks/resnet.py,16,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.configs import cfgs\nfrom tensorflow.contrib.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim.nets import resnet_utils\nfrom tensorflow.contrib.slim.python.slim.nets.resnet_v1 import resnet_v1_block\nimport tfplot as tfp\n\n\ndef resnet_arg_scope(\n        is_training=True, weight_decay=cfgs.WEIGHT_DECAY, batch_norm_decay=0.997,\n        batch_norm_epsilon=1e-5, batch_norm_scale=True):\n    \'\'\'\n\n    In Default, we do not use BN to train resnet, since batch_size is too small.\n    So is_training is False and trainable is False in the batch_norm params.\n\n    \'\'\'\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS\n    }\n\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef fusion_two_layer(C_i, P_j, scope):\n    \'\'\'\n    i = j+1\n    :param C_i: shape is [1, h, w, c]\n    :param P_j: shape is [1, h/2, w/2, 256]\n    :return:\n    P_i\n    \'\'\'\n    with tf.variable_scope(scope):\n        level_name = scope.split(\'_\')[1]\n        h, w = tf.shape(C_i)[1], tf.shape(C_i)[2]\n        upsample_p = tf.image.resize_bilinear(P_j,\n                                              size=[h, w],\n                                              name=\'up_sample_\'+level_name)\n\n        reduce_dim_c = slim.conv2d(C_i,\n                                   num_outputs=256,\n                                   kernel_size=[1, 1], stride=1,\n                                   scope=\'reduce_dim_\'+level_name)\n\n        add_f = 0.5*upsample_p + 0.5*reduce_dim_c\n\n        # P_i = slim.conv2d(add_f,\n        #                   num_outputs=256, kernel_size=[3, 3], stride=1,\n        #                   padding=\'SAME\',\n        #                   scope=\'fusion_\'+level_name)\n        return add_f\n\n\ndef add_heatmap(feature_maps, name):\n    \'\'\'\n\n    :param feature_maps:[B, H, W, C]\n    :return:\n    \'\'\'\n\n    def figure_attention(activation):\n        fig, ax = tfp.subplots()\n        im = ax.imshow(activation, cmap=\'jet\')\n        fig.colorbar(im)\n        return fig\n\n    heatmap = tf.reduce_sum(feature_maps, axis=-1)\n    heatmap = tf.squeeze(heatmap, axis=0)\n    tfp.summary.plot(name, figure_attention, [heatmap])\n\n\ndef resnet_base(img_batch, scope_name, is_training=True):\n    \'\'\'\n    this code is derived from light-head rcnn.\n    https://github.com/zengarden/light_head_rcnn\n\n    It is convenient to freeze blocks. So we adapt this mode.\n    \'\'\'\n    if scope_name == \'resnet_v1_50\':\n        middle_num_units = 6\n    elif scope_name == \'resnet_v1_101\':\n        middle_num_units = 23\n    else:\n        raise NotImplementedError(\'We only support resnet_v1_50 or resnet_v1_101. Check your network name....yjr\')\n\n    blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n              resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n              resnet_v1_block(\'block3\', base_depth=256, num_units=middle_num_units, stride=2),\n              resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n    # when use fpn . stride list is [1, 2, 2]\n\n    with slim.arg_scope(resnet_arg_scope(is_training=False)):\n        with tf.variable_scope(scope_name, scope_name):\n            # Do the first few layers manually, because \'SAME\' padding can behave inconsistently\n            # for images of different sizes: sometimes 0, sometimes 1\n            net = resnet_utils.conv2d_same(\n                img_batch, 64, 7, stride=2, scope=\'conv1\')\n            net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\n            net = slim.max_pool2d(\n                net, [3, 3], stride=2, padding=\'VALID\', scope=\'pool1\')\n\n    not_freezed = [False] * cfgs.FIXED_BLOCKS + (4-cfgs.FIXED_BLOCKS)*[True]\n    # Fixed_Blocks can be 1~3\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[0]))):\n        C2, end_points_C2 = resnet_v1.resnet_v1(net,\n                                                blocks[0:1],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # C2 = tf.Print(C2, [tf.shape(C2)], summarize=10, message=\'C2_shape\')\n    add_heatmap(C2, name=\'Layer2/C2_heat\')\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[1]))):\n        C3, end_points_C3 = resnet_v1.resnet_v1(C2,\n                                                blocks[1:2],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # C3 = tf.Print(C3, [tf.shape(C3)], summarize=10, message=\'C3_shape\')\n    add_heatmap(C3, name=\'Layer3/C3_heat\')\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[2]))):\n        C4, end_points_C4 = resnet_v1.resnet_v1(C3,\n                                                blocks[2:3],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    add_heatmap(C4, name=\'Layer4/C4_heat\')\n\n    # C4 = tf.Print(C4, [tf.shape(C4)], summarize=10, message=\'C4_shape\')\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n        C5, end_points_C5 = resnet_v1.resnet_v1(C4,\n                                                blocks[3:4],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n    # C5 = tf.Print(C5, [tf.shape(C5)], summarize=10, message=\'C5_shape\')\n    add_heatmap(C5, name=\'Layer5/C5_heat\')\n\n    feature_dict = {\'C2\': end_points_C2[\'{}/block1/unit_2/bottleneck_v1\'.format(scope_name)],\n                    \'C3\': end_points_C3[\'{}/block2/unit_3/bottleneck_v1\'.format(scope_name)],\n                    \'C4\': end_points_C4[\'{}/block3/unit_{}/bottleneck_v1\'.format(scope_name, middle_num_units - 1)],\n                    \'C5\': end_points_C5[\'{}/block4/unit_3/bottleneck_v1\'.format(scope_name)],\n                    # \'C5\': end_points_C5[\'{}/block4\'.format(scope_name)],\n                    }\n\n    # feature_dict = {\'C2\': C2,\n    #                 \'C3\': C3,\n    #                 \'C4\': C4,\n    #                 \'C5\': C5}\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(C5,\n                             num_outputs=256,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n            if ""P6"" in cfgs.LEVLES:\n                P6 = slim.max_pool2d(P5, kernel_size=[1, 1], stride=2, scope=\'build_P6\')\n                pyramid_dict[\'P6\'] = P6\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 1, -1):  # build [P4, P3, P2]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level+1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(4, 1, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level)\n    for level in range(5, 1, -1):\n        add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    # return [P2, P3, P4, P5, P6]\n    print(""we are in Pyramid::-======>>>>"")\n    print(cfgs.LEVLES)\n    print(""base_anchor_size are: "", cfgs.BASE_ANCHOR_SIZE_LIST)\n    print(20 * ""__"")\n    return [pyramid_dict[level_name] for level_name in cfgs.LEVLES]\n    # return pyramid_dict  # return the dict. And get each level by key. But ensure the levels are consitant\n    # return list rather than dict, to avoid dict is unordered\n\n\n\ndef restnet_head(inputs, is_training, scope_name):\n    \'\'\'\n\n    :param inputs: [minibatch_size, 7, 7, 256]\n    :param is_training:\n    :param scope_name:\n    :return:\n    \'\'\'\n\n    with tf.variable_scope(\'build_fc_layers\'):\n\n        # fc1 = slim.conv2d(inputs=inputs,\n        #                   num_outputs=1024,\n        #                   kernel_size=[7, 7],\n        #                   padding=\'VALID\',\n        #                   scope=\'fc1\') # shape is [minibatch_size, 1, 1, 1024]\n        # fc1 = tf.squeeze(fc1, [1, 2], name=\'squeeze_fc1\')\n\n        inputs = slim.flatten(inputs=inputs, scope=\'flatten_inputs\')\n\n        fc1 = slim.fully_connected(inputs, num_outputs=1024, scope=\'fc1\')\n\n        fc2 = slim.fully_connected(fc1, num_outputs=1024, scope=\'fc2\')\n\n        # fc3 = slim.fully_connected(fc2, num_outputs=1024, scope=\'fc3\')\n\n        # we add fc3 to increase the ability of fast-rcnn head\n        return fc2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
libs/networks/some_test.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow.contrib.slim as slim\nimport os\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\n\n\n@slim.add_arg_scope\ndef fn(a, c=100):\n\n    return a+c\n\n\nwith slim.arg_scope([fn], a=2):\n    with slim.arg_scope([fn], a=1):\n\n        print(fn())\n\n\n'"
libs/quantization/graph_to_dot.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts a GraphDef file into a DOT format suitable for visualization.\n\nThis script takes a GraphDef representing a network, and produces a DOT file\nthat can then be visualized by GraphViz tools like dot and xdot.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nfrom google.protobuf import text_format\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import gfile\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""graph"", """", """"""TensorFlow \'GraphDef\' file to load."""""")\nflags.DEFINE_bool(""input_binary"", True,\n                  """"""Whether the input files are in binary format."""""")\nflags.DEFINE_string(""dot_output"", """", """"""Where to write the DOT output."""""")\n\n\ndef main(unused_args):\n  if not gfile.Exists(FLAGS.graph):\n    print(""Input graph file \'"" + FLAGS.graph + ""\' does not exist!"")\n    return -1\n\n  graph = graph_pb2.GraphDef()\n  with open(FLAGS.graph, ""r"") as f:\n    if FLAGS.input_binary:\n      graph.ParseFromString(f.read())\n    else:\n      text_format.Merge(f.read(), graph)\n\n  with open(FLAGS.dot_output, ""wb"") as f:\n    print(""digraph graphname {"", file=f)\n    for node in graph.node:\n      output_name = node.name\n      print(""  \\"""" + output_name + ""\\"" [label=\\"""" + node.op + ""\\""];"", file=f)\n      for input_full_name in node.input:\n        parts = input_full_name.split("":"")\n        input_name = re.sub(r""^\\^"", """", parts[0])\n        print(""  \\"""" + input_name + ""\\"" -> \\"""" + output_name + ""\\"";"", file=f)\n    print(""}"", file=f)\n  print(""Created DOT file \'"" + FLAGS.dot_output + ""\'."")\n\n\nif __name__ == ""__main__"":\n  app.run()\n'"
libs/quantization/quantize_graph.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Transforms a float-trained graph into an equivalent quantized version.\n\nAn example of command-line usage is:\nbazel build tensorflow/tools/quantization:quantize_graph \\\n&& bazel-bin/tensorflow/tools/quantization/quantize_graph \\\n--input=tensorflow_inception_graph.pb\n--output_node_names=""softmax2"" --print_nodes --output=/tmp/quantized_graph.pb \\\n--mode=eightbit --logtostderr\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport numpy as np\n\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags as flags_lib\nfrom tensorflow.python.platform import gfile\n\nflags = flags_lib\nFLAGS = flags.FLAGS\n\nflags.DEFINE_boolean(""print_nodes"", False, """"""Lists all nodes in the model."""""")\nflags.DEFINE_string(""input"", """", """"""TensorFlow \'GraphDef\' file to load."""""")\nflags.DEFINE_string(""output_node_names"", """",\n                    """"""Output node names, comma separated."""""")\nflags.DEFINE_string(""output"", """", """"""File to save the output graph to."""""")\nflags.DEFINE_integer(""bitdepth"", 8,\n                     """"""How many bits to quantize the graph to."""""")\nflags.DEFINE_string(""mode"", ""round"",\n                    """"""What transformation to apply (round, quantize,""""""\n                    """""" eightbit, weights, or weights_rounded)."""""")\nflags.DEFINE_string(""test_input_dims"", ""1,224,224,3"",\n                    """"""The size of the input tensor to use when testing a""""""\n                    """""" graph loaded from a file."""""")\nflags.DEFINE_boolean(""strip_redundant_quantization"", True,\n                     """"""Removes redundant dequantize/quantize pairs."""""")\nflags.DEFINE_boolean(""quantized_input"", False,\n                     ""If true, assume Placeholders are quantized with values ""\n                     ""covering [--quantized_input_min,--quantized_input_max]. ""\n                     ""Only supported when --mode=eightbit"")\nflags.DEFINE_float(""quantized_input_min"", 0,\n                   ""The minimum of the actual input range when ""\n                   ""--quantized_input"")\nflags.DEFINE_float(""quantized_input_max"", 1,\n                   ""The maximum of the actual input range when ""\n                   ""--quantized_input"")\nflags.DEFINE_float(\n    ""quantized_fallback_min"", None,\n    ""The fallback \'min\' value to use for layers which lack min-max ""\n    ""information. Note: this should be considered a coarse tool just good ""\n    ""enough for experimentation purposes, since graphs quantized in this way ""\n    ""would be very inaccurate."")\nflags.DEFINE_float(\n    ""quantized_fallback_max"", None,\n    ""The fallback \'max\' value to use for layers which lack min-max ""\n    ""information. Note: this should be considered a coarse tool just good ""\n    ""enough for experimentation purposes, since graphs quantized in this way ""\n    ""would be very inaccurate."")\n\n\ndef print_input_nodes(current_node, nodes_map, indent, already_visited):\n  print("" "" * indent + current_node.op + "":"" + current_node.name)\n  already_visited[current_node.name] = True\n  for input_node_name in current_node.input:\n    if input_node_name in already_visited:\n      continue\n    input_node = nodes_map[input_node_name]\n    print_input_nodes(input_node, nodes_map, indent + 1, already_visited)\n\n\ndef create_node(op, name, inputs):\n  new_node = node_def_pb2.NodeDef()\n  new_node.op = op\n  new_node.name = name\n  for input_name in inputs:\n    new_node.input.extend([input_name])\n  return new_node\n\n\ndef create_constant_node(name, value, dtype, shape=None):\n  node = create_node(""Const"", name, [])\n  set_attr_dtype(node, ""dtype"", dtype)\n  set_attr_tensor(node, ""value"", value, dtype, shape)\n  return node\n\n\ndef copy_attr(node, key, attr_value):\n  try:\n    node.attr[key].CopyFrom(attr_value)\n  except KeyError:\n    pass\n\n\ndef set_attr_dtype(node, key, value):\n  try:\n    node.attr[key].CopyFrom(\n        attr_value_pb2.AttrValue(type=value.as_datatype_enum))\n  except KeyError:\n    pass\n\n\ndef set_attr_shape(node, key, value):\n  try:\n    node.attr[key].CopyFrom(\n        attr_value_pb2.AttrValue(shape=tensor_shape.as_shape(value).as_proto()))\n  except KeyError:\n    pass\n\n\ndef set_attr_tensor(node, key, value, dtype, shape=None):\n  try:\n    node.attr[key].CopyFrom(\n        attr_value_pb2.AttrValue(tensor=tensor_util.make_tensor_proto(\n            value, dtype=dtype, shape=shape)))\n  except KeyError:\n    pass\n\n\ndef set_attr_string(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(s=value))\n  except KeyError:\n    pass\n\n\ndef set_attr_int_list(node, key, value):\n  list_value = attr_value_pb2.AttrValue.ListValue(i=value)\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(list=list_value))\n  except KeyError:\n    pass\n\n\ndef set_attr_bool(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(b=value))\n  except KeyError:\n    pass\n\n\ndef set_attr_int(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(i=value))\n  except KeyError:\n    pass\n\n\ndef set_attr_float(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(f=value))\n  except KeyError:\n    pass\n\n\ndef node_name_from_input(node_name):\n  """"""Strips off ports and other decorations to get the underlying node name.""""""\n  if node_name.startswith(""^""):\n    node_name = node_name[1:]\n  m = re.search(r""(.*):\\d+$"", node_name)\n  if m:\n    node_name = m.group(1)\n  return node_name\n\n\ndef ensure_tensor_name_has_port(node_name):\n  """"""Makes sure that a tensor name has :0 if no explicit port exists.""""""\n  m = re.search(r""(.*):\\d+$"", node_name)\n  if m:\n    name_with_port = node_name\n  else:\n    name_with_port = node_name + "":0""\n  return name_with_port\n\n\ndef unique_node_name_from_input(node_name):\n  """"""Replaces invalid characters in input names to get a unique node name.""""""\n  return node_name.replace("":"", ""__port__"").replace(""^"", ""__hat__"")\n\n\ndef quantize_array(arr, num_buckets):\n  """"""Quantizes a numpy array.\n\n  This function maps each scalar in arr to the center of one of num_buckets\n  buckets. For instance,\n  quantize_array([0, 0.3, 0.6, 1], 2) => [0.25, 0.25, 0.75, 0.75]\n\n  Args:\n    arr: The numpy array to quantize.\n    num_buckets: The number of buckets to map ""var"" to.\n  Returns:\n    The quantized numpy array.\n  Raises:\n    ValueError: when num_buckets < 1.\n  """"""\n  if num_buckets < 1:\n    raise ValueError(""num_buckets must be >= 1"")\n  arr_max = arr.max()\n  arr_min = arr.min()\n  if arr_max == arr_min:\n    return arr\n  bucket_width = (arr_max - arr_min) / num_buckets\n  # Map scalars to bucket indices. Take special care of max(arr).\n  bucket_indices = np.floor((arr - arr_min) / bucket_width)\n  bucket_indices[bucket_indices == num_buckets] = num_buckets - 1\n  # Map each scalar to the center of a bucket.\n  arr = arr_min + bucket_width * (bucket_indices + 0.5)\n  return arr\n\n\ndef quantize_weight_rounded(input_node):\n  """"""Returns a replacement node for input_node containing bucketed floats.""""""\n  input_tensor = input_node.attr[""value""].tensor\n  tensor_value = tensor_util.MakeNdarray(input_tensor)\n  shape = input_tensor.tensor_shape\n  # Currently, the parameter FLAGS.bitdepth is used to compute the\n  # number of buckets as 1 << FLAGS.bitdepth, meaning the number of\n  # buckets can only be a power of 2.\n  # This could be fixed by introducing a new parameter, num_buckets,\n  # which would allow for more flexibility in chosing the right model\n  # size/accuracy tradeoff. But I didn\'t want to add more parameters\n  # to this script than absolutely necessary.\n  num_buckets = 1 << FLAGS.bitdepth\n  tensor_value_rounded = quantize_array(tensor_value, num_buckets)\n  tensor_shape_list = tensor_util.TensorShapeProtoToList(shape)\n  return [\n      create_constant_node(\n          input_node.name,\n          tensor_value_rounded,\n          dtypes.float32,\n          shape=tensor_shape_list)\n  ]\n\n\ndef quantize_weight_eightbit(input_node, quantization_mode):\n  """"""Returns replacement nodes for input_node using the Dequantize op.""""""\n  base_name = input_node.name + ""_""\n  quint8_const_name = base_name + ""quint8_const""\n  min_name = base_name + ""min""\n  max_name = base_name + ""max""\n  float_tensor = tensor_util.MakeNdarray(input_node.attr[""value""].tensor)\n  min_value = np.min(float_tensor.flatten())\n  max_value = np.max(float_tensor.flatten())\n  # Make sure that the range includes zero.\n  if min_value > 0.0:\n    min_value = 0.0\n  # min_value == max_value is a tricky case. It can occur for general\n  # tensors, and of course for scalars. The quantized ops cannot deal\n  # with this case, so we set max_value to something else.\n  # It\'s a tricky question what is the numerically best solution to\n  # deal with this degeneracy.\n  # TODO(petewarden): Better use a tolerance than a hard comparison?\n  if min_value == max_value:\n    if abs(min_value) < 0.000001:\n      max_value = min_value + 1.0\n    elif min_value > 0:\n      max_value = 2 * min_value\n    else:\n      max_value = min_value / 2.0\n\n  sess = session.Session()\n  with sess.as_default():\n    quantize_op = array_ops.quantize_v2(\n        float_tensor,\n        min_value,\n        max_value,\n        dtypes.quint8,\n        mode=quantization_mode)\n    quint8_tensor = quantize_op[0].eval()\n  shape = tensor_util.TensorShapeProtoToList(input_node.attr[""value""]\n                                             .tensor.tensor_shape)\n  quint8_const_node = create_constant_node(\n      quint8_const_name, quint8_tensor, dtypes.quint8, shape=shape)\n  min_node = create_constant_node(min_name, min_value, dtypes.float32)\n  max_node = create_constant_node(max_name, max_value, dtypes.float32)\n  dequantize_node = create_node(""Dequantize"", input_node.name,\n                                [quint8_const_name, min_name, max_name])\n  set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n  set_attr_string(dequantize_node, ""mode"", quantization_mode)\n  return [quint8_const_node, min_node, max_node, dequantize_node]\n\n\nEightbitizeRecursionState = collections.namedtuple(\n    ""EightbitizeRecursionState"",\n    [""already_visited"", ""output_node_stack"", ""merged_with_fake_quant""])\n\n\nclass GraphRewriter(object):\n  """"""Takes a float graph, and rewrites it in quantized form.""""""\n\n  def __init__(self,\n               input_graph,\n               mode,\n               quantized_input_range,\n               fallback_quantization_range=None):\n    """"""Sets up the class to rewrite a float graph.\n\n    Args:\n      input_graph: A float graph to transform.\n      mode: A string controlling how quantization is performed -\n        round, quantize, eightbit, or weights.\n      quantized_input_range: if set, assume the input is\n        quantized and represents the range\n        [quantized_input_range[0], quantized_input_range[1]]\n      fallback_quantization_range: if set, then for nodes where the quantization\n        range can\'t be inferred from the graph, use the range\n        [fallback_quantization_range[0], fallback_quantization_range[1]) instead\n        of using a RequantizationRange node in the graph.\n\n    Raises:\n      ValueError: Two nodes with the same name were found in the graph.\n    """"""\n    self.input_graph = input_graph\n    self.nodes_map = self.create_nodes_map(input_graph)\n    self.output_graph = None\n    self.mode = mode\n    self.final_node_renames = {}\n    if quantized_input_range:\n      self.input_range = (quantized_input_range[0], quantized_input_range[1])\n      if self.input_range[0] >= self.input_range[1]:\n        raise ValueError(""Invalid quantized_input_range: [%s,%s]"" %\n                         self.input_range)\n      if self.mode != ""eightbit"":\n        raise ValueError(\n            ""quantized_input_range can only be specified in eightbit mode"")\n    else:\n      self.input_range = None\n\n    if fallback_quantization_range:\n      self.fallback_quantization_range = [\n          fallback_quantization_range[0], fallback_quantization_range[1]\n      ]\n      if (self.fallback_quantization_range[0] >=\n          self.fallback_quantization_range[1]):\n        raise ValueError(""Invalid fallback_quantization_range: [%s,%s]"" %\n                         self.fallback_quantization_range)\n      if self.mode != ""eightbit"":\n        raise ValueError(""fallback_quantization_range can only be ""\n                         ""specified in eightbit mode"")\n    else:\n      self.fallback_quantization_range = None\n\n    # Data that is valid only during the recursive call to rewrite the graph.\n    self.state = None\n\n  def create_nodes_map(self, graph):\n    """"""Builds a mapping of node names to their defs from the graph.""""""\n    nodes_map = {}\n    for node in graph.node:\n      if node.name not in nodes_map.keys():\n        nodes_map[node.name] = node\n      else:\n        raise ValueError(""Duplicate node names detected."")\n    return nodes_map\n\n  def rewrite(self, output_node_names):\n    """"""Triggers rewriting of the float graph.\n\n    Args:\n      output_node_names: A list of names of the nodes that produce the final\n        results.\n\n    Returns:\n      A quantized version of the float graph.\n    """"""\n    self.output_graph = graph_pb2.GraphDef()\n    output_nodes = [\n        self.nodes_map[output_node_name]\n        for output_node_name in output_node_names\n    ]\n    if self.mode == ""round"":\n      self.already_visited = {}\n      for output_node in output_nodes:\n        self.round_nodes_recursively(output_node)\n    elif self.mode == ""quantize"":\n      self.already_visited = {}\n      self.already_quantized = {}\n      for output_node in output_nodes:\n        self.quantize_nodes_recursively(output_node)\n    elif self.mode == ""eightbit"":\n      self.set_input_graph(graph_util.remove_training_nodes(\n          self.input_graph, protected_nodes=output_node_names))\n      output_nodes = [\n          self.nodes_map[output_node_name]\n          for output_node_name in output_node_names\n      ]\n\n      self.state = EightbitizeRecursionState(\n          already_visited={}, output_node_stack=[], merged_with_fake_quant={})\n      for output_node in output_nodes:\n        self.eightbitize_nodes_recursively(output_node)\n      self.state = None\n      if self.input_range:\n        self.add_output_graph_node(\n            create_constant_node(""quantized_input_min_value"", self.input_range[\n                0], dtypes.float32, []))\n        self.add_output_graph_node(\n            create_constant_node(""quantized_input_max_value"", self.input_range[\n                1], dtypes.float32, []))\n      if self.fallback_quantization_range:\n        self.add_output_graph_node(\n            create_constant_node(""fallback_quantization_min_value"",\n                                 self.fallback_quantization_range[0],\n                                 dtypes.float32, []))\n        self.add_output_graph_node(\n            create_constant_node(""fallback_quantization_max_value"",\n                                 self.fallback_quantization_range[1],\n                                 dtypes.float32, []))\n      if FLAGS.strip_redundant_quantization:\n        self.output_graph = self.remove_redundant_quantization(\n            self.output_graph)\n        self.remove_dead_nodes(output_node_names)\n      self.apply_final_node_renames()\n    elif self.mode == ""weights"":\n      self.output_graph = self.quantize_weights(self.input_graph,\n                                                b""MIN_COMBINED"")\n      self.remove_dead_nodes(output_node_names)\n    elif self.mode == ""weights_rounded"":\n      self.output_graph = self.quantize_weights(self.input_graph, self.mode)\n      self.remove_dead_nodes(output_node_names)\n    else:\n      print(""Bad mode - "" + self.mode + ""."")\n    return self.output_graph\n\n  def round_nodes_recursively(self, current_node):\n    """"""The entry point for simple rounding quantization.""""""\n    if (current_node.name in self.already_visited\n       ) and self.already_visited[current_node.name]:\n      return\n    self.already_visited[current_node.name] = True\n    for input_node_name in current_node.input:\n      input_node_name = node_name_from_input(input_node_name)\n      input_node = self.nodes_map[input_node_name]\n      self.round_nodes_recursively(input_node)\n    nodes_to_quantize = [""Conv2D"", ""BiasAdd"", ""MatMul""]\n    if any(current_node.op in s for s in nodes_to_quantize):\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      new_node.name = current_node.name + ""_original""\n      self.add_output_graph_node(new_node)\n      levels = 1 << FLAGS.bitdepth\n      constant_name = current_node.name + ""_round_depth""\n      constant_tensor = constant_op.constant(\n          levels, dtype=dtypes.int32, name=constant_name)\n      constant_node = constant_tensor.op.node_def\n      self.add_output_graph_node(constant_node)\n      quantize_node = node_def_pb2.NodeDef()\n      quantize_node.op = ""RoundToSteps""\n      quantize_node.name = current_node.name\n      quantize_node.input.extend([current_node.name + ""_original""])\n      quantize_node.input.extend([constant_node.name])\n      self.add_output_graph_node(quantize_node)\n    else:\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      self.add_output_graph_node(new_node)\n\n  def quantize_nodes_recursively(self, current_node):\n    """"""The entry point for quantizing nodes to eight bit and back.""""""\n    if self.already_visited[current_node.name]:\n      return\n    self.already_visited[current_node.name] = True\n    for input_node_name in current_node.input:\n      input_node_name = node_name_from_input(input_node_name)\n      input_node = self.nodes_map[input_node_name]\n      self.quantize_nodes_recursively(input_node)\n    nodes_to_quantize = [""Conv2D"", ""BiasAdd"", ""MatMul""]\n    if any(current_node.op in s for s in nodes_to_quantize):\n      for input_name in current_node.input:\n        input_name = node_name_from_input(input_name)\n        input_node = self.nodes_map[input_name]\n        self.quantize_node(input_node)\n      self.quantize_node(current_node)\n    else:\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      self.add_output_graph_node(new_node)\n\n  def quantize_node(self, input_node):\n    """"""Handles quantizing a single node.""""""\n    input_name = input_node.name\n    if input_name in self.already_quantized:\n      return\n    self.already_quantized[input_name] = True\n    original_input_name = input_name + ""_original""\n    reshape_name = input_name + ""_reshape""\n    reshape_dims_name = input_name + ""_reshape_dims""\n    max_name = input_name + ""_max""\n    min_name = input_name + ""_min""\n    dims_name = input_name + ""_dims""\n    quantize_name = input_name + ""_quantize""\n    dequantize_name = input_name\n    original_input_node = node_def_pb2.NodeDef()\n    original_input_node.CopyFrom(input_node)\n    original_input_node.name = original_input_name\n    self.add_output_graph_node(original_input_node)\n    reshape_dims_node = create_constant_node(reshape_dims_name, -1,\n                                             dtypes.int32, [1])\n    self.add_output_graph_node(reshape_dims_node)\n    reshape_node = create_node(""Reshape"", reshape_name,\n                               [original_input_name, reshape_dims_name])\n    set_attr_dtype(reshape_node, ""T"", dtypes.float32)\n    self.add_output_graph_node(reshape_node)\n    dims_node = create_constant_node(dims_name, 0, dtypes.int32, [1])\n    self.add_output_graph_node(dims_node)\n    max_node = create_node(""Max"", max_name, [reshape_name, dims_name])\n    set_attr_dtype(max_node, ""T"", dtypes.float32)\n    set_attr_bool(max_node, ""keep_dims"", False)\n    self.add_output_graph_node(max_node)\n    min_node = create_node(""Min"", min_name, [reshape_name, dims_name])\n    set_attr_dtype(min_node, ""T"", dtypes.float32)\n    set_attr_bool(min_node, ""keep_dims"", False)\n    self.add_output_graph_node(min_node)\n    quantize_node = create_node(""Quantize"", quantize_name,\n                                [original_input_name, min_name, max_name])\n    set_attr_dtype(quantize_node, ""T"", dtypes.quint8)\n    set_attr_string(quantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(quantize_node)\n    dequantize_node = create_node(""Dequantize"", dequantize_name,\n                                  [quantize_name, min_name, max_name])\n    set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n    set_attr_string(dequantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(dequantize_node)\n\n  def should_merge_with_fake_quant_node(self):\n    """"""Should the current node merge with self.state.output_node_stack[-1]?""""""\n    if not self.state.output_node_stack:\n      return False\n    top = self.state.output_node_stack[-1]\n    return top[1] == 0 and top[0].op in [""FakeQuantWithMinMaxVars""]\n\n  def should_quantize_const(self, node):\n    if not self.state.output_node_stack:\n      return False\n    top = self.state.output_node_stack[-1]\n    if not top[2]:\n      return False\n    dtype = dtypes.as_dtype(node.attr[""dtype""].type)\n    assert dtype == dtypes.float32, (\n        ""Failed to quantized constant %s of type %s"" % (node.name, dtype))\n    return True\n\n  def eightbitize_nodes_recursively(self, current_node):\n    """"""The entry point for transforming a graph into full eight bit.""""""\n    if current_node.name in self.state.already_visited:\n      if (self.should_merge_with_fake_quant_node() or\n          current_node.name in self.state.merged_with_fake_quant):\n        raise ValueError(""Unsupported graph structure: output of node %s ""\n                         ""is processed by a FakeQuant* node and should have ""\n                         ""no other outputs."", current_node.name)\n      return\n    self.state.already_visited[current_node.name] = True\n\n    for i, input_node_name in enumerate(current_node.input):\n      quantize_input = False\n      if current_node.op in (""MatMul"", ""Conv2D"", ""BiasAdd"", ""MaxPool"",\n                             ""AvgPool"", ""Relu"", ""Relu6"",\n                             ""BatchNormWithGlobalNormalization""):\n        quantize_input = True\n      elif current_node.op == ""Concat"" and i > 0:\n        quantize_input = (\n            dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32)\n      elif current_node.op == ""Reshape"" and i == 0:\n        quantize_input = (\n            dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32)\n\n      self.state.output_node_stack.append((current_node, i, quantize_input))\n\n      input_node_name = node_name_from_input(input_node_name)\n      input_node = self.nodes_map[input_node_name]\n      self.eightbitize_nodes_recursively(input_node)\n\n      self.state.output_node_stack.pop()\n\n    if current_node.op == ""MatMul"":\n      self.eightbitize_mat_mul_node(current_node)\n    elif current_node.op == ""Conv2D"":\n      self.eightbitize_conv_node(current_node)\n    elif current_node.op == ""BiasAdd"":\n      self.eightbitize_bias_add_node(current_node)\n    elif current_node.op == ""MaxPool"" or current_node.op == ""AvgPool"":\n      self.eightbitize_single_input_tensor_node(current_node,\n                                                self.add_pool_function)\n    elif current_node.op == ""Relu"" or current_node.op == ""Relu6"":\n      self.eightbitize_single_input_tensor_node(current_node,\n                                                self.add_relu_function)\n    elif (current_node.op == ""Concat"" and\n          dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32):\n      self.eightbitize_concat_node(current_node)\n    elif current_node.op == ""BatchNormWithGlobalNormalization"":\n      self.eightbitize_batch_norm_node(current_node)\n    elif (current_node.op == ""Reshape"" and\n          dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32):\n      self.eightbitize_reshape_node(current_node)\n    elif (self.input_range and\n          current_node.op in (""Placeholder"", ""PlaceholderV2"")):\n      self.eightbitize_placeholder_node(current_node)\n    elif current_node.op == ""FakeQuantWithMinMaxVars"":\n      # It will have been merged into the underlying node.\n      pass\n    elif current_node.op == ""Const"":\n      if self.should_quantize_const(current_node):\n        for n in quantize_weight_eightbit(current_node, b""MIN_FIRST""):\n          self.add_output_graph_node(n)\n      else:\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(current_node)\n        self.add_output_graph_node(new_node)\n\n    ###################################################################\n    # Note: if more cases are added here, you may need to update the op\n    # name lists in the loop over children at the start of the function.\n    ###################################################################\n    else:\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      self.add_output_graph_node(new_node)\n\n    if (self.should_merge_with_fake_quant_node() and\n        current_node.name not in self.state.merged_with_fake_quant):\n      raise ValueError(\n          ""FakeQuant* node %s failed to merge with node %s of type %s"" %\n          (self.state.output_node_stack[-1][0], current_node.name,\n           current_node.op))\n\n  def add_eightbit_prologue_nodes(self, original_node):\n    """"""Adds input conversion nodes to handle quantizing the underlying node.""""""\n    namespace_prefix = original_node.name + ""_eightbit""\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    input_names = []\n    min_max_names = []\n    for original_input_name in original_node.input:\n      quantize_input_name, min_input_name, max_input_name = (\n          self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                         reshape_dims_name,\n                                         reduction_dims_name))\n      input_names.append(quantize_input_name)\n      min_max_names.append(min_input_name)\n      min_max_names.append(max_input_name)\n    all_input_names = []\n    all_input_names.extend(input_names)\n    all_input_names.extend(min_max_names)\n    return all_input_names\n\n  def add_common_quantization_nodes(self, namespace_prefix):\n    """"""Builds constant nodes needed for quantization of inputs.""""""\n    reshape_dims_name = namespace_prefix + ""_reshape_dims""\n    reduction_dims_name = namespace_prefix + ""_reduction_dims""\n\n    reshape_dims_node = create_constant_node(reshape_dims_name, -1,\n                                             dtypes.int32, [1])\n    self.add_output_graph_node(reshape_dims_node)\n    reduction_dims_node = create_constant_node(reduction_dims_name, 0,\n                                               dtypes.int32, [1])\n    self.add_output_graph_node(reduction_dims_node)\n    return reshape_dims_name, reduction_dims_name\n\n  def eightbitize_input_to_node(self, namespace_prefix, original_input_name,\n                                reshape_dims_name, reduction_dims_name):\n    """"""Takes one float input to an op, and converts it to quantized form.""""""\n    unique_input_name = unique_node_name_from_input(original_input_name)\n    reshape_input_name = namespace_prefix + ""_reshape_"" + unique_input_name\n    min_input_name = namespace_prefix + ""_min_"" + unique_input_name\n    max_input_name = namespace_prefix + ""_max_"" + unique_input_name\n    quantize_input_name = namespace_prefix + ""_quantize_"" + unique_input_name\n    reshape_input_node = create_node(""Reshape"", reshape_input_name,\n                                     [original_input_name, reshape_dims_name])\n    set_attr_dtype(reshape_input_node, ""T"", dtypes.float32)\n    self.add_output_graph_node(reshape_input_node)\n    min_input_node = create_node(""Min"", min_input_name,\n                                 [reshape_input_name, reduction_dims_name])\n    set_attr_dtype(min_input_node, ""T"", dtypes.float32)\n    set_attr_bool(min_input_node, ""keep_dims"", False)\n    self.add_output_graph_node(min_input_node)\n    max_input_node = create_node(""Max"", max_input_name,\n                                 [reshape_input_name, reduction_dims_name])\n    set_attr_dtype(max_input_node, ""T"", dtypes.float32)\n    set_attr_bool(max_input_node, ""keep_dims"", False)\n    self.add_output_graph_node(max_input_node)\n    quantize_input_node = create_node(\n        ""QuantizeV2"", quantize_input_name,\n        [original_input_name, min_input_name, max_input_name])\n    set_attr_dtype(quantize_input_node, ""T"", dtypes.quint8)\n    set_attr_string(quantize_input_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(quantize_input_node)\n    min_output_name = quantize_input_name + "":1""\n    max_output_name = quantize_input_name + "":2""\n    return quantize_input_name, min_output_name, max_output_name\n\n  def add_quantize_down_nodes(self, original_node, quantized_output_name):\n    quantized_outputs = [\n        quantized_output_name, quantized_output_name + "":1"",\n        quantized_output_name + "":2""\n    ]\n    min_max_inputs = None\n    if self.should_merge_with_fake_quant_node():\n      # Use the inputs to the FakeQuantWithMinMaxVars node as the inputs to\n      # Requantize.\n      fake_quant_node = self.state.output_node_stack[-1][0]\n      min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]\n      assert original_node.name not in self.state.merged_with_fake_quant\n      self.state.merged_with_fake_quant[original_node.name] = True\n    elif self.fallback_quantization_range:\n      min_max_inputs = [\n          ""fallback_quantization_min_value:0"",\n          ""fallback_quantization_max_value:0""\n      ]\n    else:\n      # Add a RequantizationRange node for finding the min and max values.\n      requant_range_node = create_node(\n          ""RequantizationRange"", original_node.name + ""_eightbit_requant_range"",\n          quantized_outputs)\n      set_attr_dtype(requant_range_node, ""Tinput"", dtypes.qint32)\n      self.add_output_graph_node(requant_range_node)\n      min_max_inputs = [\n          requant_range_node.name + "":0"", requant_range_node.name + "":1""\n      ]\n    requantize_node = create_node(""Requantize"",\n                                  original_node.name + ""_eightbit_requantize"",\n                                  quantized_outputs + min_max_inputs)\n    set_attr_dtype(requantize_node, ""Tinput"", dtypes.qint32)\n    set_attr_dtype(requantize_node, ""out_type"", dtypes.quint8)\n    self.add_output_graph_node(requantize_node)\n    return requantize_node.name\n\n  def add_dequantize_result_node(self,\n                                 quantized_output_name,\n                                 original_node_name,\n                                 min_tensor_index=1):\n    min_max_inputs = [\n        ""%s:%s"" % (quantized_output_name, min_tensor_index),\n        ""%s:%s"" % (quantized_output_name, (min_tensor_index + 1))\n    ]\n    dequantize_name = original_node_name\n    if self.should_merge_with_fake_quant_node():\n      fake_quant_node = self.state.output_node_stack[-1][0]\n      if original_node_name not in self.state.merged_with_fake_quant:\n        min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]\n        self.state.merged_with_fake_quant[original_node_name] = True\n      dequantize_name = fake_quant_node.name\n\n    dequantize_node = create_node(\n        ""Dequantize"", dequantize_name,\n        [quantized_output_name, min_max_inputs[0], min_max_inputs[1]])\n    set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n    set_attr_string(dequantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(dequantize_node)\n\n  def eightbitize_mat_mul_node(self, original_node):\n    """"""Replaces a MatMul node with the eight bit equivalent sub-graph.""""""\n    quantized_mat_mul_name = original_node.name + ""_eightbit_quantized_mat_mul""\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_mat_mul_node = create_node(""QuantizedMatMul"",\n                                         quantized_mat_mul_name,\n                                         all_input_names)\n    set_attr_dtype(quantized_mat_mul_node, ""T1"", dtypes.quint8)\n    set_attr_dtype(quantized_mat_mul_node, ""T2"", dtypes.quint8)\n    set_attr_dtype(quantized_mat_mul_node, ""Toutput"", dtypes.qint32)\n    copy_attr(quantized_mat_mul_node, ""transpose_a"",\n              original_node.attr[""transpose_a""])\n    copy_attr(quantized_mat_mul_node, ""transpose_b"",\n              original_node.attr[""transpose_b""])\n    self.add_output_graph_node(quantized_mat_mul_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_mat_mul_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def eightbitize_conv_node(self, original_node):\n    """"""Replaces a Conv2D node with the eight bit equivalent sub-graph.""""""\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_conv_name = original_node.name + ""_eightbit_quantized_conv""\n    quantized_conv_node = create_node(""QuantizedConv2D"", quantized_conv_name,\n                                      all_input_names)\n    copy_attr(quantized_conv_node, ""strides"", original_node.attr[""strides""])\n    copy_attr(quantized_conv_node, ""padding"", original_node.attr[""padding""])\n    set_attr_dtype(quantized_conv_node, ""Tinput"", dtypes.quint8)\n    set_attr_dtype(quantized_conv_node, ""Tfilter"", dtypes.quint8)\n    set_attr_dtype(quantized_conv_node, ""out_type"", dtypes.qint32)\n    self.add_output_graph_node(quantized_conv_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_conv_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def eightbitize_bias_add_node(self, original_node):\n    """"""Replaces a BiasAdd node with the eight bit equivalent sub-graph.""""""\n    quantized_bias_add_name = (\n        original_node.name + ""_eightbit_quantized_bias_add"")\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_bias_add_node = create_node(""QuantizedBiasAdd"",\n                                          quantized_bias_add_name,\n                                          all_input_names)\n    set_attr_dtype(quantized_bias_add_node, ""T1"", dtypes.quint8)\n    set_attr_dtype(quantized_bias_add_node, ""T2"", dtypes.quint8)\n    set_attr_dtype(quantized_bias_add_node, ""out_type"", dtypes.qint32)\n    self.add_output_graph_node(quantized_bias_add_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_bias_add_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def eightbitize_single_input_tensor_node(self, original_node,\n                                           add_op_function):\n    """"""Replaces a single-tensor node with the eight bit equivalent sub-graph.\n\n    Converts a node like this:\n\n       Shape(f)   Input(f)\n         |          |\n         +--------v v\n                Operation\n                    |\n                    v\n                   (f)\n\n     Into a quantized equivalent:\n\n                    Input(f)              ReshapeDims\n                       +------v v-------------+\n                       |    Reshape\n                       |      |\n                       |      |          ReductionDims\n                       |      +-----+         |\n                       |      | +---c---------+\n                       |      v v   v v-------+\n                       |      Min   Max\n                       |  +----+      |\n                       v  v  v--------+\n                      Quantize\n                          |\n                          v\n                   QuantizedOperation\n                      |   |   |\n                      v   v   v\n                      Dequantize\n                          |\n                          v\n                         (f)\n\n\n    Args:\n      original_node: Float node to be converted.\n      add_op_function: Function to create the actual node.\n\n    Returns:\n      Subgraph representing the quantized version of the original node.\n\n    """"""\n    quantized_op_name = original_node.name + ""_eightbit_quantized""\n    quantized_op_type = ""Quantized"" + original_node.op\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_op_node = create_node(quantized_op_type, quantized_op_name,\n                                    all_input_names)\n    add_op_function(original_node, quantized_op_node)\n    self.add_output_graph_node(quantized_op_node)\n    self.add_dequantize_result_node(quantized_op_name, original_node.name)\n\n  def add_pool_function(self, original_node, quantized_op_node):\n    set_attr_dtype(quantized_op_node, ""T"", dtypes.quint8)\n    copy_attr(quantized_op_node, ""ksize"", original_node.attr[""ksize""])\n    copy_attr(quantized_op_node, ""strides"", original_node.attr[""strides""])\n    copy_attr(quantized_op_node, ""padding"", original_node.attr[""padding""])\n\n  def add_relu_function(self, unused_arg_node, quantized_op_node):\n    set_attr_dtype(quantized_op_node, ""Tinput"", dtypes.quint8)\n\n  def eightbitize_concat_node(self, original_node):\n    """"""Replaces a Concat node with the eight bit equivalent sub-graph.\n\n    Converts a node like this:\n\n       Shape(f)   Input0(f)   Input1(f)\n         |          |            |\n         +--------v v v----------+\n                  Concat\n                    |\n                    v\n                   (f)\n\n     Into a quantized equivalent:\n\n       Shape(f)     Input0(f)             ReshapeDims                  Input1(f)\n         |             +------v v--------------+------------------v v------+\n         |             |    Reshape                             Reshape    |\n         |             |      |                                     |      |\n         |             |      |           ReductionDims             |      |\n         |             |      +------+         |           +--------+      |\n         |             |      |  +---c---------+-----------c-----+  |      |\n         |             |      +v v   v v-------+---------v v     v v+      |\n         |             |       Min   Max                 Min     Max       |\n         |             |  +----+      |                   |       +-----+  |\n         |             v  v  v--------+                   +----------v  v  v\n         |            Quantize                                       Quantize\n         |                +------------------+   +----------------------+\n         +-------------------------------+   |   |\n                                         v   v   v\n                                      QuantizedConcat\n                                         |   |   |\n                                         v   v   v\n                                        Dequantize\n                                             |\n                                             v\n                                            (f)\n    Args:\n      original_node: Float node to be converted.\n\n    Returns:\n      Subgraph representing the quantized version of the original node.\n\n    """"""\n    namespace_prefix = original_node.name + ""_eightbit""\n    quantized_concat_name = namespace_prefix + ""_quantized_concat""\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    shape_input_name = original_node.input[0]\n    original_inputs = original_node.input[1:]\n    input_names = []\n    min_names = []\n    max_names = []\n    for original_input_name in original_inputs:\n      quantize_input_name, min_input_name, max_input_name = (\n          self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                         reshape_dims_name,\n                                         reduction_dims_name))\n      input_names.append(quantize_input_name)\n      min_names.append(min_input_name)\n      max_names.append(max_input_name)\n    all_input_names = [shape_input_name]\n    all_input_names.extend(input_names)\n    all_input_names.extend(min_names)\n    all_input_names.extend(max_names)\n    quantized_concat_node = create_node(""QuantizedConcat"",\n                                        quantized_concat_name, all_input_names)\n    set_attr_int(quantized_concat_node, ""N"", len(original_inputs))\n    set_attr_dtype(quantized_concat_node, ""T"", dtypes.quint8)\n    self.add_output_graph_node(quantized_concat_node)\n    self.add_dequantize_result_node(quantized_concat_name, original_node.name)\n\n  def eightbitize_placeholder_node(self, current_node):\n    """"""Replaces a placeholder node with a quint8 placeholder node+dequantize.""""""\n    name = current_node.name\n\n    # Convert the placeholder into a quantized type.\n    output_node = node_def_pb2.NodeDef()\n    output_node.CopyFrom(current_node)\n    set_attr_dtype(output_node, ""dtype"", dtypes.quint8)\n    output_node.name += ""_original_input""\n    self.add_output_graph_node(output_node)\n\n    # Add a dequantize to convert back to float.\n    dequantize_node = create_node(""Dequantize"", name, [\n        output_node.name, ""quantized_input_min_value"",\n        ""quantized_input_max_value""\n    ])\n    set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n    set_attr_string(dequantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(dequantize_node)\n\n    # For the descent over the graph to work, the dequantize node must be named\n    # current_node.name.  However, for the feeding of the graph to work, the\n    # placeholder must have the name current_node.name; so record a final set\n    # of renames to apply after all processing has been done.\n    self.final_node_renames[output_node.name] = name\n    self.final_node_renames[dequantize_node.name] = name + ""_dequantize""\n\n  def eightbitize_reshape_node(self, original_node):\n    """"""Replaces a Reshape node with the eight bit equivalent sub-graph.\n\n    Args:\n      original_node: Float node to be converted.\n\n    Returns:\n      Subgraph representing the quantized version of the original node.\n\n    """"""\n    namespace_prefix = original_node.name + ""_eightbit""\n    quantized_reshape_name = namespace_prefix + ""_quantized_reshape""\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    shape_input_name = original_node.input[1]\n    quantize_input_name, min_input_name, max_input_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_node.input[0],\n                                       reshape_dims_name, reduction_dims_name))\n    quantized_reshape_node = create_node(\n        ""QuantizedReshape"", quantized_reshape_name,\n        [quantize_input_name, shape_input_name, min_input_name, max_input_name])\n    set_attr_dtype(quantized_reshape_node, ""T"", dtypes.quint8)\n    self.add_output_graph_node(quantized_reshape_node)\n    self.add_dequantize_result_node(quantized_reshape_name, original_node.name)\n\n  def eightbitize_batch_norm_node(self, original_node):\n    """"""Replaces a MatMul node with the eight bit equivalent sub-graph.""""""\n    namespace_prefix = original_node.name + ""_eightbit""\n    original_input_name = original_node.input[0]\n    original_mean_name = original_node.input[1]\n    original_variance_name = original_node.input[2]\n    original_beta_name = original_node.input[3]\n    original_gamma_name = original_node.input[4]\n    quantized_batch_norm_name = namespace_prefix + ""_quantized_batch_norm""\n\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    quantize_input_name, min_input_name, max_input_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_mean_name, min_mean_name, max_mean_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_mean_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_variance_name, min_variance_name, max_variance_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_variance_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_beta_name, min_beta_name, max_beta_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_beta_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_gamma_name, min_gamma_name, max_gamma_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_gamma_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantized_batch_norm_node = create_node(\n        ""QuantizedBatchNormWithGlobalNormalization"", quantized_batch_norm_name,\n        [\n            quantize_input_name, min_input_name, max_input_name,\n            quantize_mean_name, min_mean_name, max_mean_name,\n            quantize_variance_name, min_variance_name, max_variance_name,\n            quantize_beta_name, min_beta_name, max_beta_name,\n            quantize_gamma_name, min_gamma_name, max_gamma_name\n        ])\n    set_attr_dtype(quantized_batch_norm_node, ""Tinput"", dtypes.quint8)\n    set_attr_dtype(quantized_batch_norm_node, ""out_type"", dtypes.qint32)\n    copy_attr(quantized_batch_norm_node, ""scale_after_normalization"",\n              original_node.attr[""scale_after_normalization""])\n    copy_attr(quantized_batch_norm_node, ""variance_epsilon"",\n              original_node.attr[""variance_epsilon""])\n    self.add_output_graph_node(quantized_batch_norm_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_batch_norm_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def add_output_graph_node(self, output_node):\n    """"""Inserts one node into the new graph.""""""\n    self.output_graph.node.extend([output_node])\n\n  def remove_redundant_quantization(self, old_graph):\n    """"""Removes unneeded pairs of quantize/dequantize ops from the graph.\n\n    This is a bit of a tricky function, because it\'s attempting to spot the\n    pattern of dequantizing from eight-bit up to float, and then immediately\n    quantizing back down to eight bits again, that\'s introduced by previous\n    passes that do \'key-hole\' conversions of individual nodes but have to\n    convert back to float to match the previous output interface, since they\n    don\'t know that the next op can handle quantized tensors.\n    It works by:\n     - Looking for Quantize nodes.\n     - Checking to see if their first input is a Dequantize node.\n     - Seeing if their min/max inputs come from Min/Max nodes.\n     - Making sure those Min/Max nodes are being fed from the same Dequantize.\n     - Or that the Min is indirectly being fed from the same Dequantize as Max.\n     - Making sure the Dequantize is going through a Reshape (which we add\n       during the previous pass when we create the quantize sub-graph).\n     - Looking for the dims Const op for the Min/Max dims.\n    If all of these conditions are met, then it\'s a sub-graph pattern that\n    we know how to optimize out (and is likely the common one we\'ve introduced).\n    We then rewire the graph to skip it entirely, and then rely on the dead node\n    removal pass to get rid of any nodes that are no longer needed.\n\n    Args:\n      old_graph: The model we\'ll be stripping redundant nodes from.\n\n    Returns:\n      A graph with the unnecessary nodes removed.\n\n    Raises:\n      ValueError: Two nodes with the same name were found in the graph.\n    """"""\n    old_nodes_map = self.create_nodes_map(old_graph)\n    self.output_graph = graph_pb2.GraphDef()\n    inputs_to_rename = {}\n    # We go through all the nodes, looking for any that match the patterns we\n    # know how to optimize away.\n    for node in old_graph.node:\n      # We always start with a Quantize node, and examine its inputs to see if\n      # they are in a form that can be removed.\n      if node.op not in [""Quantize"", ""QuantizeV2""]:\n        continue\n      dequantize_node_name = node_name_from_input(node.input[0])\n      if dequantize_node_name not in old_nodes_map:\n        raise ValueError(""Input node name \'"" + dequantize_node_name +\n                         ""\' not found in node \'"" + node.name + ""\'"")\n      dequantize_node = old_nodes_map[dequantize_node_name]\n      # Do we have a Dequantize feeding in, with the same type as the Quantize?\n      if dequantize_node.op != ""Dequantize"":\n        continue\n      if node.attr[""T""] != dequantize_node.attr[""T""]:\n        continue\n      # Now look at the other inputs, and ensure they\'re Min/Max nodes.\n      min_node_name = node_name_from_input(node.input[1])\n      max_node_name = node_name_from_input(node.input[2])\n      min_node = old_nodes_map[min_node_name]\n      max_node = old_nodes_map[max_node_name]\n      is_min_right_type = (min_node.op in [""Min"", ""Dequantize""])\n      is_max_right_type = (max_node.op in [""Max"", ""Dequantize""])\n      if not is_min_right_type or not is_max_right_type:\n        print(""Didn\'t find expected types on inputs : %s, %s."" % (min_node.op,\n                                                                  max_node.op))\n        continue\n      min_node_input_name = node_name_from_input(min_node.input[0])\n      max_node_input_name = node_name_from_input(max_node.input[0])\n      # There are two different patterns for Min nodes we can recognize, one\n      # where the input comes directly from the same one as the Max, and\n      # another where we run it through another Min first, so check for both.\n      is_same_input = False\n      if min_node_input_name == max_node_input_name:\n        is_same_input = True\n      else:\n        first_min_node_input = old_nodes_map[min_node_input_name]\n        if first_min_node_input.op == ""Concat"":\n          second_min_node_name = node_name_from_input(\n              first_min_node_input.input[1])\n          second_min_node = old_nodes_map[second_min_node_name]\n          if second_min_node.op == ""Min"":\n            second_min_node_input_name = node_name_from_input(\n                second_min_node.input[0])\n            is_same_input = (second_min_node_input_name == max_node_input_name)\n      if not is_same_input:\n        print(""Different min/max inputs: "" + min_node_input_name)\n        continue\n      # We recognize this pattern, so mark the graph edges to be rewired to\n      # route around it entirely, since we know it\'s a no-op.\n      dequantize_source_name = node_name_from_input(dequantize_node.input[0])\n      node_tensor_name = ensure_tensor_name_has_port(node.name)\n      min_tensor_name = node.name + "":1""\n      max_tensor_name = node.name + "":2""\n      inputs_to_rename[node_tensor_name] = dequantize_source_name\n      inputs_to_rename[min_tensor_name] = dequantize_node.input[1]\n      inputs_to_rename[max_tensor_name] = dequantize_node.input[2]\n    # Finally we apply all the rewiring we\'ve marked to the graph.\n    for node in old_graph.node:\n      for index, input_full_name in enumerate(node.input):\n        input_name = ensure_tensor_name_has_port(input_full_name)\n        if input_name in inputs_to_rename:\n          node.input[index] = inputs_to_rename[input_name]\n      self.add_output_graph_node(node)\n    return self.output_graph\n\n  def apply_final_node_renames(self):\n    """"""Applies node renames in self.final_node_renames to self.output_graph.""""""\n    old_graph = self.output_graph\n    self.output_graph = graph_pb2.GraphDef()\n    for node in old_graph.node:\n      node.name = self.final_node_renames.get(node.name, node.name)\n      for index, input_name in enumerate(node.input):\n        node_name = node_name_from_input(input_name)\n        input_full_name = ensure_tensor_name_has_port(input_name)\n        if node_name in self.final_node_renames:\n          node.input[index] = ""%s%s"" % (self.final_node_renames[node_name],\n                                        input_full_name[len(node_name):])\n      self.add_output_graph_node(node)\n    return self.output_graph\n\n  def remove_dead_nodes(self, output_names):\n    """"""Removes nodes that are no longer needed for inference from the graph.""""""\n    old_output_graph = self.output_graph\n    self.output_graph = graph_util.extract_sub_graph(old_output_graph,\n                                                     output_names)\n\n  def quantize_weights(self, input_graph, quantization_mode):\n    """"""Quantize float Const ops.\n\n    There are two modes of operations, both replace float Const ops with\n    quantized values.\n    1. If quantization_mode is ""weights_rounded"", this function replaces float\n    Const ops with quantized float Const ops - same as the original op, but\n    float values being mapped to the center of one of 1<<FLAGS.bitdepth buckets.\n    This does not change the raw model size, but compression algorithms such as\n    zip (as used for compressing apks) or bzip2 will achieve a very good\n    compression ratio.\n    2. For other quantization modes (""MIN_COMBINED"" or ""MIN_FIRST""), float\n    Const ops are quantized and replaced by a tuple of four ops to perform\n    the dequantization at runtime:\n    * eight-bit Const (bucket indices, same shape as original float Const op\n    * two float Const ops (min and max value of original float Const op)\n    * Dequantize op to convert the eight-bit consts to float tensors.\n    The quantization mode is important because we see accuracy problems when\n    quantizing weights for different situations depending on the algorithm\n    used. We haven\'t figured out exactly what the underlying cause is yet,\n    unfortunately.\n\n    Args:\n      input_graph: A GraphDef of the model containing float Const ops.\n      quantization_mode: How to quantize and dequantize the values.\n\n    Returns:\n      A GraphDef of the converted graph.\n\n    Raises:\n      ValueError: If quantization_mode is unsupported.\n    """"""\n    output_graph = graph_pb2.GraphDef()\n    for input_node in input_graph.node:\n      should_quantize = False\n      if input_node.op == ""Const"":\n        dtype = dtypes.as_dtype(input_node.attr[""dtype""].type)\n        if dtype == dtypes.float32:\n          should_quantize = True\n      if should_quantize:\n        if quantization_mode == ""weights_rounded"":\n          output_graph.node.extend(quantize_weight_rounded(input_node))\n        elif quantization_mode in (b""MIN_COMBINED"", b""MIN_FIRST""):\n          output_graph.node.extend(\n              quantize_weight_eightbit(input_node, quantization_mode))\n        else:\n          raise ValueError(""Unsupported quantization mode %s."" %\n                           quantization_mode)\n      else:\n        output_node = node_def_pb2.NodeDef()\n        output_node.CopyFrom(input_node)\n        output_graph.node.extend([output_node])\n    return output_graph\n\n  def set_input_graph(self, new_input_graph):\n    self.input_graph = new_input_graph\n    self.nodes_map = self.create_nodes_map(self.input_graph)\n\n\ndef main(unused_args):\n  if not gfile.Exists(FLAGS.input):\n    print(""Input graph file \'"" + FLAGS.input + ""\' does not exist!"")\n    return -1\n\n  known_modes = [\n      ""round"", ""quantize"", ""eightbit"", ""weights"", ""test"", ""weights_rounded""\n  ]\n  if not any(FLAGS.mode in s for s in known_modes):\n    print(""mode is \'"" + FLAGS.mode + ""\', not in "" + "", "".join(known_modes) +\n          ""."")\n    return -1\n\n  tf_graph = graph_pb2.GraphDef()\n  with gfile.Open(FLAGS.input, ""rb"") as f:\n    data = f.read()\n    tf_graph.ParseFromString(data)\n\n  graph = ops.Graph()\n  with graph.as_default():\n    importer.import_graph_def(tf_graph, input_map={}, name="""")\n\n  quantized_input_range = None\n  if FLAGS.quantized_input:\n    quantized_input_range = [\n        FLAGS.quantized_input_min, FLAGS.quantized_input_max\n    ]\n\n  fallback_quantization_range = None\n  if (FLAGS.quantized_fallback_min is not None or\n      FLAGS.quantized_fallback_max is not None):\n    assert FLAGS.quantized_fallback_min is not None\n    assert FLAGS.quantized_fallback_max is not None\n    fallback_quantization_range = [\n        FLAGS.quantized_fallback_min, FLAGS.quantized_fallback_max\n    ]\n\n  rewriter = GraphRewriter(tf_graph, FLAGS.mode, quantized_input_range,\n                           fallback_quantization_range)\n\n  output_graph = rewriter.rewrite(FLAGS.output_node_names.split("",""))\n\n  f = gfile.FastGFile(FLAGS.output, ""wb"")\n  f.write(output_graph.SerializeToString())\n\n  return 0\n\n\nif __name__ == ""__main__"":\n  app.run()\n'"
libs/quantization/quantize_graph_test.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests the graph quantization script.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport numpy as np\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import ops as ops_lib\nfrom tensorflow.python.platform import flags as flags_lib\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.tools.quantization import quantize_graph\n\nflags = flags_lib\nFLAGS = flags.FLAGS\n\n\ndef run_graph_def(graph_def, input_map, outputs):\n  graph = ops_lib.Graph()\n  with graph.as_default():\n    importer.import_graph_def(graph_def, input_map={}, name="""")\n  with session.Session(graph=graph) as sess:\n    results = sess.run(outputs, feed_dict=input_map)\n  return results\n\n\ndef test_mat_mul(m, n, k, a, b):\n  """"""Tests a MatMul replacement.""""""\n  a_constant_name = ""a_constant""\n  b_constant_name = ""b_constant""\n  mat_mul_name = ""mat_mul""\n\n  float_graph_def = graph_pb2.GraphDef()\n  a_constant = quantize_graph.create_constant_node(\n      a_constant_name, value=a, dtype=dtypes.float32, shape=[m, k])\n  float_graph_def.node.extend([a_constant])\n  b_constant = quantize_graph.create_constant_node(\n      b_constant_name, value=b, dtype=dtypes.float32, shape=[k, n])\n  float_graph_def.node.extend([b_constant])\n  mat_mul_node = quantize_graph.create_node(""MatMul"", mat_mul_name,\n                                            [a_constant_name, b_constant_name])\n  quantize_graph.set_attr_dtype(mat_mul_node, ""T"", dtypes.float32)\n  quantize_graph.set_attr_bool(mat_mul_node, ""transpose_a"", False)\n  quantize_graph.set_attr_bool(mat_mul_node, ""transpose_b"", False)\n  float_graph_def.node.extend([mat_mul_node])\n\n  test_graph(float_graph_def, {}, [mat_mul_name])\n\n\ndef test_conv(depth, image_width, image_height, image_batch_count, filter_size,\n              filter_count, stride, padding, input_values, filter_values):\n  """"""Tests a Conv replacement.""""""\n  input_constant_name = ""input_constant""\n  filter_constant_name = ""filter_constant""\n  conv_name = ""conv""\n\n  float_graph_def = graph_pb2.GraphDef()\n  input_constant = quantize_graph.create_constant_node(\n      input_constant_name,\n      value=input_values,\n      dtype=dtypes.float32,\n      shape=[image_batch_count, image_height, image_width, depth])\n  float_graph_def.node.extend([input_constant])\n  filter_constant = quantize_graph.create_constant_node(\n      filter_constant_name,\n      value=filter_values,\n      dtype=dtypes.float32,\n      shape=[filter_size, filter_size, depth, filter_count])\n  float_graph_def.node.extend([filter_constant])\n  conv_node = quantize_graph.create_node(\n      ""Conv2D"", conv_name, [input_constant_name, filter_constant_name])\n  quantize_graph.set_attr_dtype(conv_node, ""T"", dtypes.float32)\n  quantize_graph.set_attr_int_list(conv_node, ""strides"", [1, stride, stride, 1])\n  quantize_graph.set_attr_string(conv_node, ""padding"", padding)\n  float_graph_def.node.extend([conv_node])\n\n  test_graph(float_graph_def, {}, [conv_name])\n\n\ndef are_tensors_near(a, b, tolerance):\n  """"""Tests whether two tensors are nearly identical.\n\n  This is a specialized comparison function designed to help debug problems with\n  quantization. It prints out information about the differences between tensors\n  on failure, paying special attention to possible biases by looking at the mean\n  and absolute average errors.\n\n  Args:\n    a: First comparison tensor.\n    b: Second comparison tensor.\n    tolerance: Float value indicating how large an error between values is ok.\n\n  Returns:\n    Boolean indicating whether the two inputs were close enough.\n  """"""\n  flat_a = a.flatten()\n  flat_b = b.flatten()\n  if len(flat_a) != len(flat_b):\n    tf_logging.info(""Tensors are different sizes: "" + str(len(flat_a)) + "" vs ""\n                    + str(len(flat_b)))\n    return False\n  value_count = len(flat_a)\n  how_many_different = 0\n  total_difference = 0\n  total_abs_difference = 0\n  for index in range(value_count):\n    a_value = flat_a[index]\n    b_value = flat_b[index]\n    difference = a_value - b_value\n    total_difference += difference\n    total_abs_difference += abs(difference)\n    if abs(difference) > tolerance:\n      how_many_different += 1\n  mean_difference = total_difference / value_count\n  mean_abs_difference = total_abs_difference / value_count\n  proportion_different = (how_many_different * 1.0) / value_count\n  if how_many_different == 0:\n    return True\n  else:\n    tf_logging.info(""Tensors have {0} different values ({1}%), with mean""\n                    "" difference {2} and mean absolute difference {3}"".format(\n                        how_many_different, proportion_different * 100,\n                        mean_difference, mean_abs_difference))\n    return False\n\n\ndef get_top_value(input_values):\n  max_value = None\n  max_index = None\n  for index, value in enumerate(input_values.flatten()):\n    if max_value is None or value > max:\n      max_value = value\n      max_index = index\n  return max_index, max_value\n\n\ndef test_graph(float_graph_def, input_map, output_names, log_graph=False):\n  """"""Runs the float graph through the rewriter and tests the results.""""""\n  float_results = run_graph_def(\n      float_graph_def, input_map,\n      [output_name + "":0"" for output_name in output_names])\n  # TODO(petewarden): round test is currently failing because there is no\n  # RoundToSteps op available.\n  # round_rewriter = quantize_graph.GraphRewriter(float_graph_def, ""round"")\n  # round_graph_def = round_rewriter.rewrite(output_name)\n  # round_results = run_graph_def(round_graph_def, input_map,\n  #                               [output_name + "":0""])\n  # assert are_tensors_near(expected, round_results[0], 1.0)\n  #\n  # TODO(petewarden): Add test for ""quantize"" mode.\n\n  eightbit_rewriter = quantize_graph.GraphRewriter(\n      float_graph_def, ""eightbit"", quantized_input_range=None)\n  eightbit_graph_def = eightbit_rewriter.rewrite(output_names)\n  eightbit_results = run_graph_def(\n      eightbit_graph_def, input_map,\n      [output_name + "":0"" for output_name in output_names])\n  for expected, result in zip(float_results, eightbit_results):\n    assert are_tensors_near(expected, result, 1.0)\n\n  if log_graph:\n    tf_logging.info(""8bit:\\n%s"", str(eightbit_graph_def))\n\n  # Test the weights_rounded mode. This uses the default bit_depth.\n  weights_rounded_rewriter = quantize_graph.GraphRewriter(\n      float_graph_def, ""weights_rounded"", quantized_input_range=None)\n  weights_rounded_graph_def = weights_rounded_rewriter.rewrite(output_names)\n  weights_rounded_results = run_graph_def(\n      weights_rounded_graph_def, input_map,\n      [output_name + "":0"" for output_name in output_names])\n  for expected, result in zip(float_results, weights_rounded_results):\n    assert are_tensors_near(expected, result, 1.0)\n\n\nclass QuantizeGraphTest(test.TestCase):\n\n  def test_negative_const_problem(self):\n    shape_constant_name = ""shape_constant""\n    shape_constant = quantize_graph.create_constant_node(\n        shape_constant_name, value=-0.8, dtype=dtypes.float32, shape=[1])\n    quantization_result = quantize_graph.quantize_weight_eightbit(\n        shape_constant, b""MIN_COMBINED"")\n    self.assertEqual(4, len(quantization_result))\n\n  def test_odd_padding_problem(self):\n    """"""Tests one error case we ran into in a real graph.""""""\n    test_conv(1, 4, 4, 1, 3, 1, 2, b""SAME"",\n              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n              [1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n  def test_mat_mul_tiny(self):\n    # These tests are added to test the generate case where\n    # min(matrix) == max(matrix), which used to cause problems.\n    test_mat_mul(1, 1, 1, [2], [3])\n    test_mat_mul(1, 2, 1, [1], [2, 3])\n    test_mat_mul(1, 1, 2, [1, 1], [1, 1])\n    test_mat_mul(1, 1, 2, [0, 0], [1, 1])\n    # The general case.\n    test_mat_mul(1, 1, 2, [1, 2], [1, 2])\n\n  def test_mat_mul_small(self):\n    test_mat_mul(2, 4, 3, [1, 2, 3, 4, 5, 6],\n                 [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18])\n\n  def test_conv(self):\n    test_conv(1, 4, 3, 1, 3, 1, 1, b""SAME"",\n              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n              [1, 4, 7, 2, 5, 8, 3, 6, 9])\n\n  def test_reshape(self):\n    """"""Tests that MatMul->Reshape->MatMul avoids extra quantize/dequantize.""""""\n\n    def make_matmul(name, a, b):\n      n = quantize_graph.create_node(""MatMul"", name, [a.name, b.name])\n      quantize_graph.set_attr_dtype(n, ""T"", dtypes.float32)\n      quantize_graph.set_attr_bool(n, ""transpose_a"", False)\n      quantize_graph.set_attr_bool(n, ""transpose_b"", False)\n      return n\n\n    # matmul_1 = input*weight_1\n    input_node = quantize_graph.create_constant_node(\n        ""input"", value=[0, 1, 2, 3], dtype=dtypes.float32, shape=[4, 1])\n    weight_1_node = quantize_graph.create_constant_node(\n        ""weight_1"",\n        value=[.5, .6, .7, .8, .9],\n        dtype=dtypes.float32,\n        shape=[1, 5])\n    matmul_1_node = make_matmul(""matmul_1"", input_node, weight_1_node)\n\n    # Reshape 4x5 to 10x2.\n    new_shape_node = quantize_graph.create_constant_node(\n        ""new_shape_node"", value=[10, 2], dtype=dtypes.int32, shape=[2])\n    reshape_node = quantize_graph.create_node(\n        ""Reshape"", ""reshape"", [matmul_1_node.name, new_shape_node.name])\n    quantize_graph.set_attr_dtype(reshape_node, ""T"", dtypes.float32)\n\n    # matmul_2_node = reshape*weight_2\n    weight_2_node = quantize_graph.create_constant_node(\n        ""weight_2"", value=[1.5, 2.5], dtype=dtypes.float32, shape=[2, 1])\n    matmul_2_node = make_matmul(""matmul_2"", reshape_node, weight_2_node)\n\n    g = graph_pb2.GraphDef()\n    g.node.extend([\n        input_node, weight_1_node, matmul_1_node, new_shape_node, reshape_node,\n        weight_2_node, matmul_2_node\n    ])\n\n    # Test the graph\n    test_graph(g, {}, [""matmul_2""])\n\n    # Verify there is only one Quantize and one Requantize op.\n    eightbit_rewriter = quantize_graph.GraphRewriter(\n        g, ""eightbit"", quantized_input_range=None)\n    eightbit_graph_def = eightbit_rewriter.rewrite([""matmul_2""])\n\n    ops = [node.op for node in eightbit_graph_def.node]\n    # No quantize since all inputs are const and can be quantized up-front.\n    self.assertEqual(0, ops.count(""QuantizeV2"") + ops.count(""Quantize""))\n    self.assertEqual(1, ops.count(""QuantizedReshape""))\n\n    # One dequantize at the end.\n    self.assertEqual(1, ops.count(""Dequantize""))\n\n  def test_quantize_array(self):\n    # Test invalid parameters (empty array, or 0 buckets.\n    self.assertRaises(ValueError, quantize_graph.quantize_array, np.array([]),\n                      2)\n    self.assertRaises(ValueError, quantize_graph.quantize_array,\n                      np.array([1, 2]), 0)\n    # Test input array of length 1.\n    arr = np.array([1])\n    qarr = quantize_graph.quantize_array(arr, 1)\n    self.assertEqual(arr, qarr)\n    qarr = quantize_graph.quantize_array(arr, 2)\n    self.assertEqual(arr, qarr)\n    # Test input array with all elements equal.\n    arr = np.array([1, 1, 1])\n    qarr = quantize_graph.quantize_array(arr, 10)\n    self.assertTrue((np.array([1, 1, 1]) == qarr).all())\n    # Test ""normal"" input arrays.\n    arr = np.array([0, 0.3, 0.6, 1])\n    qarr = quantize_graph.quantize_array(arr, 1)\n    self.assertTrue((np.array([0.5, 0.5, 0.5, 0.5]) == qarr).all())\n    qarr = quantize_graph.quantize_array(arr, 2)\n    self.assertTrue((np.array([0.25, 0.25, 0.75, 0.75]) == qarr).all())\n    qarr = quantize_graph.quantize_array(arr.reshape((2, 2)), 2)\n    self.assertTrue((np.array([[0.25, 0.25], [0.75, 0.75]]) == qarr).all())\n\n  def test_non_float_concat(self):\n    concat_dim = quantize_graph.create_constant_node(\n        ""concat_dim"", value=0, dtype=dtypes.int32, shape=[])\n    a = quantize_graph.create_constant_node(\n        ""a"",\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.int32,\n        shape=[2, 2, 3])\n    b = quantize_graph.create_constant_node(\n        ""b"",\n        value=[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n        dtype=dtypes.int32,\n        shape=[2, 2, 3])\n    concat = quantize_graph.create_node(""Concat"", ""concat"",\n                                        [concat_dim.name, a.name, b.name])\n    quantize_graph.set_attr_int(concat, ""N"", 2)\n    quantize_graph.set_attr_dtype(concat, ""T"", dtypes.int32)\n\n    g = graph_pb2.GraphDef()\n    g.node.extend([concat_dim, a, b, concat])\n    test_graph(g, {}, [concat.name])\n\n  def test_non_float_reshape(self):\n    a = quantize_graph.create_constant_node(\n        ""a"",\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.int32,\n        shape=[2, 2, 3])\n    shape = quantize_graph.create_constant_node(\n        ""shape"", value=[12], dtype=dtypes.int32, shape=[1])\n    reshape = quantize_graph.create_node(""Reshape"", ""reshape"",\n                                         [a.name, shape.name])\n    quantize_graph.set_attr_dtype(reshape, ""T"", dtypes.int32)\n\n    g = graph_pb2.GraphDef()\n    g.node.extend([a, shape, reshape])\n    test_graph(g, {}, [reshape.name])\n\n  def test_concat(self):\n    shape_constant_name = ""shape_constant""\n    a_constant_name = ""a_constant""\n    b_constant_name = ""b_constant""\n    concat_name = ""concat""\n\n    float_graph_def = graph_pb2.GraphDef()\n    shape_constant = quantize_graph.create_constant_node(\n        shape_constant_name, value=0, dtype=dtypes.int32, shape=[])\n    float_graph_def.node.extend([shape_constant])\n    a_constant = quantize_graph.create_constant_node(\n        a_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[2, 2, 3])\n    float_graph_def.node.extend([a_constant])\n    b_constant = quantize_graph.create_constant_node(\n        b_constant_name,\n        value=[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n        dtype=dtypes.float32,\n        shape=[2, 2, 3])\n    float_graph_def.node.extend([b_constant])\n    concat_node = quantize_graph.create_node(\n        ""Concat"", concat_name,\n        [shape_constant_name, a_constant_name, b_constant_name])\n    quantize_graph.set_attr_int(concat_node, ""N"", 2)\n    quantize_graph.set_attr_dtype(concat_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([concat_node])\n\n    test_graph(float_graph_def, {}, [concat_name])\n\n    # Verify the concat is quantized.\n    eightbit_rewriter = quantize_graph.GraphRewriter(\n        float_graph_def, ""eightbit"", quantized_input_range=None)\n    eightbit_graph_def = eightbit_rewriter.rewrite([concat_name])\n\n    ops = [node.op for node in eightbit_graph_def.node]\n    self.assertEqual(1, ops.count(""QuantizedConcat""))\n\n  def test_multiple_outputs(self):\n    input_constant_name = ""input_constant""\n    split_constant_name = ""split_constant""\n    split_name = ""split""\n    concat_constant_name = ""concat_constant""\n    concat_name = ""concat""\n\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[2, 6])\n    float_graph_def.node.extend([input_constant])\n    split_constant = quantize_graph.create_constant_node(\n        split_constant_name, value=1, dtype=dtypes.int32, shape=[])\n    float_graph_def.node.extend([split_constant])\n    split_node = quantize_graph.create_node(\n        ""Split"", split_name, [split_constant_name, input_constant_name])\n    quantize_graph.set_attr_int(split_node, ""num_split"", 2)\n    quantize_graph.set_attr_dtype(split_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([split_node])\n    concat_constant = quantize_graph.create_constant_node(\n        concat_constant_name, value=1, dtype=dtypes.int32, shape=[])\n    float_graph_def.node.extend([concat_constant])\n    concat_node = quantize_graph.create_node(\n        ""Concat"", concat_name,\n        [concat_constant_name, split_name + "":0"", split_name + "":1""])\n    quantize_graph.set_attr_int(concat_node, ""N"", 2)\n    quantize_graph.set_attr_dtype(concat_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([concat_node])\n\n    test_graph(float_graph_def, {}, [concat_name])\n\n  def test_node_name_from_input(self):\n    self.assertEqual(""SomeName"",\n                     quantize_graph.node_name_from_input(""^SomeName:2""))\n\n  def test_unique_node_name_from_input(self):\n    self.assertEqual(""__hat__SomeName__port__2"",\n                     quantize_graph.unique_node_name_from_input(""^SomeName:2""))\n\n  def test_identity(self):\n    input_constant_name = ""input_constant""\n    identity_name = ""identity""\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[2, 6])\n    float_graph_def.node.extend([input_constant])\n    identity_node = quantize_graph.create_node(""Identity"", identity_name,\n                                               [input_constant_name])\n    quantize_graph.set_attr_dtype(identity_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([identity_node])\n\n    mul_name = ""mul""\n    mul_node = quantize_graph.create_node(""Mul"", mul_name,\n                                          [identity_name, identity_name])\n    quantize_graph.set_attr_dtype(mul_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([mul_node])\n\n    test_graph(float_graph_def, {}, [mul_name])\n\n  def test_keep_control_edges(self):\n    no_op_name = ""no_op""\n    a_constant_name = ""a_constant""\n    b_constant_name = ""b_constant""\n    a_check_name = ""a_check""\n    b_check_name = ""b_check""\n    a_identity_name = ""a_identity""\n    b_identity_name = ""b_identity""\n    add_name = ""add""\n    graph_def = graph_pb2.GraphDef()\n    no_op = quantize_graph.create_node(""NoOp"", no_op_name, [])\n    graph_def.node.extend([no_op])\n    a_constant = quantize_graph.create_constant_node(\n        a_constant_name, value=1, dtype=dtypes.float32, shape=[])\n    graph_def.node.extend([a_constant])\n    a_check_node = quantize_graph.create_node(""CheckNumerics"", a_check_name,\n                                              [a_constant_name])\n    graph_def.node.extend([a_check_node])\n    a_identity_node = quantize_graph.create_node(\n        ""Identity"", a_identity_name,\n        [a_constant_name, ""^"" + a_check_name, ""^"" + no_op_name])\n    graph_def.node.extend([a_identity_node])\n    b_constant = quantize_graph.create_constant_node(\n        b_constant_name, value=1, dtype=dtypes.float32, shape=[])\n    graph_def.node.extend([b_constant])\n    b_check_node = quantize_graph.create_node(""CheckNumerics"", b_check_name,\n                                              [b_constant_name])\n    graph_def.node.extend([b_check_node])\n    b_identity_node = quantize_graph.create_node(\n        ""Identity"", b_identity_name, [b_constant_name, ""^"" + b_check_name])\n    graph_def.node.extend([b_identity_node])\n    add_node = quantize_graph.create_node(""Add"", add_name,\n                                          [a_identity_name, b_identity_name])\n    quantize_graph.set_attr_dtype(add_node, ""T"", dtypes.float32)\n    graph_def.node.extend([add_node])\n\n    expected_output = graph_pb2.GraphDef()\n    no_op = quantize_graph.create_node(""NoOp"", no_op_name, [])\n    expected_output.node.extend([no_op])\n    a_constant = quantize_graph.create_constant_node(\n        a_constant_name, value=1, dtype=dtypes.float32, shape=[])\n    expected_output.node.extend([a_constant])\n    a_identity_node = quantize_graph.create_node(\n        ""Identity"", a_identity_name, [a_constant_name, ""^"" + no_op_name])\n    expected_output.node.extend([a_identity_node])\n    b_constant = quantize_graph.create_constant_node(\n        b_constant_name, value=1, dtype=dtypes.float32, shape=[])\n    expected_output.node.extend([b_constant])\n    add_node = quantize_graph.create_node(""Add"", add_name,\n                                          [a_identity_name, b_constant_name])\n    quantize_graph.set_attr_dtype(add_node, ""T"", dtypes.float32)\n    expected_output.node.extend([add_node])\n    expected_output.versions.CopyFrom(graph_def.versions)\n    expected_output.library.CopyFrom(graph_def.library)\n\n    output = graph_util.remove_training_nodes(graph_def)\n    stripped_output = graph_util.extract_sub_graph(output, [add_name])\n    self.assertProtoEquals(expected_output, stripped_output)\n\n  def test_batch_norm(self):\n    input_constant_name = ""input_constant""\n    mean_constant_name = ""mean_constant""\n    variance_constant_name = ""variance_constant""\n    beta_constant_name = ""beta_constant""\n    gamma_constant_name = ""gamma_constant""\n    batch_norm_name = ""batch_norm""\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 4, 2, 5, 3, 6, -1, -4, -2, -5, -3, -6],\n        dtype=dtypes.float32,\n        shape=[1, 1, 6, 2])\n    float_graph_def.node.extend([input_constant])\n    mean_constant = quantize_graph.create_constant_node(\n        mean_constant_name, value=[10, 20], dtype=dtypes.float32, shape=[2])\n    float_graph_def.node.extend([mean_constant])\n    variance_constant = quantize_graph.create_constant_node(\n        variance_constant_name,\n        value=[0.25, 0.5],\n        dtype=dtypes.float32,\n        shape=[2])\n    float_graph_def.node.extend([variance_constant])\n    beta_constant = quantize_graph.create_constant_node(\n        beta_constant_name, value=[0.1, 0.6], dtype=dtypes.float32, shape=[2])\n    float_graph_def.node.extend([beta_constant])\n    gamma_constant = quantize_graph.create_constant_node(\n        gamma_constant_name, value=[0, 0], dtype=dtypes.float32, shape=[2])\n    float_graph_def.node.extend([gamma_constant])\n    batch_norm_node = quantize_graph.create_node(\n        ""BatchNormWithGlobalNormalization"", batch_norm_name, [\n            input_constant_name, mean_constant_name, variance_constant_name,\n            beta_constant_name, gamma_constant_name\n        ])\n    quantize_graph.set_attr_dtype(batch_norm_node, ""T"", dtypes.float32)\n    quantize_graph.set_attr_bool(batch_norm_node, ""scale_after_normalization"",\n                                 False)\n    quantize_graph.set_attr_float(batch_norm_node, ""variance_epsilon"", 0.001)\n    float_graph_def.node.extend([batch_norm_node])\n    test_graph(float_graph_def, {}, [batch_norm_name])\n\n  def test_max_pool(self):\n    input_constant_name = ""input_constant""\n    max_pool_name = ""max_pool""\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[1, 2, 6, 1])\n    float_graph_def.node.extend([input_constant])\n    max_pool_node = quantize_graph.create_node(""MaxPool"", max_pool_name,\n                                               [input_constant_name])\n    quantize_graph.set_attr_int_list(max_pool_node, ""ksize"", [1, 2, 2, 1])\n    quantize_graph.set_attr_int_list(max_pool_node, ""strides"", [1, 1, 1, 1])\n    quantize_graph.set_attr_string(max_pool_node, ""padding"", b""SAME"")\n    float_graph_def.node.extend([max_pool_node])\n    test_graph(float_graph_def, {}, [max_pool_name])\n\n  def test_avg_pool(self):\n    input_constant_name = ""input_constant""\n    avg_pool_name = ""avg_pool""\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[1, 2, 6, 1])\n    float_graph_def.node.extend([input_constant])\n    avg_pool_node = quantize_graph.create_node(""AvgPool"", avg_pool_name,\n                                               [input_constant_name])\n    quantize_graph.set_attr_dtype(avg_pool_node, ""T"", dtypes.float32)\n    quantize_graph.set_attr_int_list(avg_pool_node, ""ksize"", [1, 2, 2, 1])\n    quantize_graph.set_attr_int_list(avg_pool_node, ""strides"", [1, 1, 1, 1])\n    quantize_graph.set_attr_string(avg_pool_node, ""padding"", b""SAME"")\n    float_graph_def.node.extend([avg_pool_node])\n    test_graph(float_graph_def, {}, [avg_pool_name])\n\n  def test_relu(self):\n    input_constant_name = ""input_constant""\n    relu_name = ""relu""\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[1, 2, 6, 1])\n    float_graph_def.node.extend([input_constant])\n    relu_node = quantize_graph.create_node(""Relu"", relu_name,\n                                           [input_constant_name])\n    quantize_graph.set_attr_dtype(relu_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([relu_node])\n    test_graph(float_graph_def, {}, [relu_name])\n\n  def test_relu_w_fake_quant_w_min_max_vars(self):\n    input_node = quantize_graph.create_constant_node(\n        ""input"",\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[1, 2, 6, 1])\n    relu_node = quantize_graph.create_node(""Relu"", ""relu"", [input_node.name])\n    quantize_graph.set_attr_dtype(relu_node, ""T"", dtypes.float32)\n\n    min_node = quantize_graph.create_constant_node(\n        ""min_bias_add"", value=0, dtype=dtypes.float32, shape=[])\n    max_node = quantize_graph.create_constant_node(\n        ""max_bias_add"", value=12, dtype=dtypes.float32, shape=[])\n    fake_quant_node = quantize_graph.create_node(\n        ""FakeQuantWithMinMaxVars"", ""fake_quant"",\n        [relu_node.name, min_node.name, max_node.name])\n\n    float_graph_def = graph_pb2.GraphDef()\n    float_graph_def.node.extend(\n        [input_node, relu_node, min_node, max_node, fake_quant_node])\n    test_graph(float_graph_def, {}, [fake_quant_node.name], log_graph=True)\n\n    # Verify there is only one Quantize and one Requantize op.\n    eightbit_rewriter = quantize_graph.GraphRewriter(\n        float_graph_def, ""eightbit"", quantized_input_range=None)\n    eightbit_graph_def = eightbit_rewriter.rewrite([fake_quant_node.name])\n\n    ops = [node.op for node in eightbit_graph_def.node]\n    # No quantize since all inputs are const and can be quantized up-front.\n    self.assertEqual(0, ops.count(""QuantizeV2"") + ops.count(""Quantize""))\n\n    # One dequantize at the end.\n    self.assertEqual(1, ops.count(""Dequantize""))\n\n  def test_relu6(self):\n    input_constant_name = ""input_constant""\n    relu6_name = ""relu6""\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[1, 2, 6, 1])\n    float_graph_def.node.extend([input_constant])\n    relu6_node = quantize_graph.create_node(""Relu6"", relu6_name,\n                                            [input_constant_name])\n    quantize_graph.set_attr_dtype(relu6_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([relu6_node])\n    test_graph(float_graph_def, {}, [relu6_name])\n\n  def test_bias_add(self):\n    input_constant_name = ""input_constant""\n    offset_constant_name = ""offset_constant""\n    bias_add_name = ""bias_add""\n    float_graph_def = graph_pb2.GraphDef()\n    input_constant = quantize_graph.create_constant_node(\n        input_constant_name,\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        dtype=dtypes.float32,\n        shape=[1, 1, 2, 6])\n    float_graph_def.node.extend([input_constant])\n    offset_constant = quantize_graph.create_constant_node(\n        offset_constant_name,\n        value=[1, 2, 3, 4, 5, 6],\n        dtype=dtypes.float32,\n        shape=[6])\n    float_graph_def.node.extend([offset_constant])\n    bias_add_node = quantize_graph.create_node(\n        ""BiasAdd"", bias_add_name, [input_constant_name, offset_constant_name])\n    quantize_graph.set_attr_dtype(bias_add_node, ""T"", dtypes.float32)\n    float_graph_def.node.extend([bias_add_node])\n    test_graph(float_graph_def, {}, [bias_add_name])\n\n  def test_quantized_input_range_errors(self):\n    with self.assertRaises(ValueError):\n      # Invalid mode.\n      quantize_graph.GraphRewriter(graph_pb2.GraphDef(), ""weights_rounded"",\n                                   [0, 1])\n    with self.assertRaises(ValueError):\n      # Invalid range.\n      quantize_graph.GraphRewriter(graph_pb2.GraphDef(), ""eightbit"", [0, -1])\n\n  def test_quantized_input_range_bias_add(self):\n    input_shape = [1, 1, 2, 6]\n    input_n = quantize_graph.create_node(""Placeholder"", ""input"", [])\n    quantize_graph.set_attr_dtype(input_n, ""dtype"", dtypes.float32)\n    quantize_graph.set_attr_shape(input_n, ""shape"", input_shape)\n    offset_n = quantize_graph.create_constant_node(\n        ""offset"", value=[1, 2, 3, 4, 5, 6], dtype=dtypes.float32, shape=[6])\n    bias_add_n = quantize_graph.create_node(""BiasAdd"", ""bias_add"",\n                                            [input_n.name, offset_n.name])\n    quantize_graph.set_attr_dtype(bias_add_n, ""T"", dtypes.float32)\n\n    float_graph_def = graph_pb2.GraphDef()\n    float_graph_def.node.extend([input_n, offset_n, bias_add_n])\n\n    input_map = {\n        input_n.name + "":0"":\n            np.reshape([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], input_shape)\n    }\n    self._RunTestsForQuantizedInputRange(float_graph_def, input_map,\n                                         [bias_add_n.name], [-1, 20.])\n    self._RunTestsForQuantizedInputRange(float_graph_def, input_map,\n                                         [bias_add_n.name], [0, 12.])\n\n  def test_quantized_input_range_mat_mul(self):\n    shapes = [[3, 2], [2, 4]]\n    inputs = []\n    for i, shape in enumerate(shapes):\n      node = quantize_graph.create_node(""Placeholder"", ""input_%s"" % i, [])\n      quantize_graph.set_attr_dtype(node, ""dtype"", dtypes.float32)\n      quantize_graph.set_attr_shape(node, ""shape"", shape)\n      inputs.append(node)\n    mat_mul_node = quantize_graph.create_node(""MatMul"", ""mat_mul"",\n                                              [n.name for n in inputs])\n    quantize_graph.set_attr_dtype(mat_mul_node, ""T"", dtypes.float32)\n\n    float_graph_def = graph_pb2.GraphDef()\n    float_graph_def.node.extend(inputs + [mat_mul_node])\n\n    input_map = {\n        inputs[0].name + "":0"":\n            np.reshape([1, 2, 3, 4, 5, 6], shapes[0]),\n        inputs[1].name + "":0"":\n            np.reshape([.8, .7, .6, .5, .4, .3, .2, .1], shapes[1])\n    }\n    self._RunTestsForQuantizedInputRange(float_graph_def, input_map,\n                                         [mat_mul_node.name], [-1, 20.])\n    self._RunTestsForQuantizedInputRange(float_graph_def, input_map,\n                                         [mat_mul_node.name], [0, 6.])\n\n  def _RunTestsForQuantizedInputRange(self, float_graph_def, input_map,\n                                      output_names, input_range):\n    if sys.version_info[0] == 3:\n      # uint8->quint8 conversion for numpy is not working currently.\n      return\n\n    quantized_input_map = {}\n    for k, v in input_map.items():\n      arr = [\n          int(\n              round((n - input_range[0]) * 255 / (input_range[1] - input_range[\n                  0]))) for n in v.flat\n      ]\n      arr = np.array(arr, np.uint8)\n      arr = arr.reshape(v.shape)\n      arr = arr.astype(dtypes.quint8.as_numpy_dtype)\n      quantized_input_map[k] = arr\n    output_tensors = [output_name + "":0"" for output_name in output_names]\n    float_results = run_graph_def(float_graph_def, input_map, output_tensors)\n\n    # Quantize treating the input as quantized in range <input_range>.\n    rewriter = quantize_graph.GraphRewriter(float_graph_def, ""eightbit"",\n                                            input_range)\n    graph_def = rewriter.rewrite(output_names)\n    results = run_graph_def(graph_def, quantized_input_map, output_tensors)\n    for expected, result in zip(float_results, results):\n      assert are_tensors_near(expected, result, .5)\n    ops = [node.op for node in graph_def.node]\n    self.assertEqual(0, ops.count(""QuantizeV2"") + ops.count(""Quantize""))\n    self.assertEqual(len(output_names), ops.count(""Dequantize""))\n\n    # Quantize without treating input as quantized.\n    rewriter = quantize_graph.GraphRewriter(\n        float_graph_def, ""eightbit"", quantized_input_range=None)\n    graph_def = rewriter.rewrite(output_names)\n    results = run_graph_def(graph_def, input_map, output_tensors)\n    for expected, result in zip(float_results, results):\n      assert are_tensors_near(expected, result, .5)\n    ops = [node.op for node in graph_def.node]\n    self.assertEqual(\n        len(input_map), ops.count(""QuantizeV2"") + ops.count(""Quantize""))\n    self.assertEqual(len(output_names), ops.count(""Dequantize""))\n\n  def test_bias_add_w_fake_quant_w_min_max_vars(self):\n    input_node = quantize_graph.create_constant_node(\n        ""input"",\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        dtype=dtypes.float32,\n        shape=[1, 1, 2, 5])\n    offset_node = quantize_graph.create_constant_node(\n        ""offset"", value=[1, 2, 3, 4, 5], dtype=dtypes.float32, shape=[5])\n    bias_add_node = quantize_graph.create_node(\n        ""BiasAdd"", ""bias_add"", [input_node.name, offset_node.name])\n    quantize_graph.set_attr_dtype(bias_add_node, ""T"", dtypes.float32)\n\n    min_node = quantize_graph.create_constant_node(\n        ""min_bias_add"", value=-.5, dtype=dtypes.float32, shape=[])\n    max_node = quantize_graph.create_constant_node(\n        ""max_bias_add"", value=15.5, dtype=dtypes.float32, shape=[])\n    fake_quant_node = quantize_graph.create_node(\n        ""FakeQuantWithMinMaxVars"", ""fake_quant"",\n        [bias_add_node.name, min_node.name, max_node.name])\n\n    float_graph_def = graph_pb2.GraphDef()\n    float_graph_def.node.extend([\n        input_node, offset_node, bias_add_node, min_node, max_node,\n        fake_quant_node\n    ])\n    test_graph(float_graph_def, {}, [fake_quant_node.name], log_graph=True)\n\n    # Verify there is only one Quantize and one Requantize op.\n    # Pass in fallback_quantization_range, although it will have no effect\n    # because the FakeQuantWithMinMaxVars are used instead.\n    eightbit_rewriter = quantize_graph.GraphRewriter(\n        float_graph_def,\n        ""eightbit"",\n        quantized_input_range=None,\n        fallback_quantization_range=[-100, 100])\n    eightbit_graph_def = eightbit_rewriter.rewrite([fake_quant_node.name])\n\n    ops = [node.op for node in eightbit_graph_def.node]\n    node_names = [node.name for node in eightbit_graph_def.node]\n    # No quantize since all inputs are const and can be quantized up-front.\n    self.assertEqual(0, ops.count(""QuantizeV2"") + ops.count(""Quantize""))\n\n    # One dequantize at the end.\n    self.assertEqual(1, ops.count(""Dequantize""))\n\n    # The fallback constants are not in the graph.\n    self.assertEqual(0, node_names.count(""fallback_quantization_min_value""))\n    self.assertEqual(0, node_names.count(""fallback_quantization_max_value""))\n\n  def test_bias_add_w_fallback_min_max_vars(self):\n    input_node = quantize_graph.create_constant_node(\n        ""input"",\n        value=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        dtype=dtypes.float32,\n        shape=[1, 1, 2, 5])\n    offset_node = quantize_graph.create_constant_node(\n        ""offset"", value=[1, 2, 3, 4, 5], dtype=dtypes.float32, shape=[5])\n    bias_add_node = quantize_graph.create_node(\n        ""BiasAdd"", ""bias_add"", [input_node.name, offset_node.name])\n    quantize_graph.set_attr_dtype(bias_add_node, ""T"", dtypes.float32)\n\n    float_graph_def = graph_pb2.GraphDef()\n    float_graph_def.node.extend([input_node, offset_node, bias_add_node])\n    test_graph(float_graph_def, {}, [bias_add_node.name], log_graph=True)\n\n    # Verify there is only one Quantize, one Requantize op, and no\n    # RequantizationRange op.\n    eightbit_rewriter = quantize_graph.GraphRewriter(\n        float_graph_def,\n        ""eightbit"",\n        quantized_input_range=None,\n        fallback_quantization_range=[-.5, 15.5])\n    eightbit_graph_def = eightbit_rewriter.rewrite([bias_add_node.name])\n\n    ops = [node.op for node in eightbit_graph_def.node]\n    node_names = [node.name for node in eightbit_graph_def.node]\n    # No quantize since all inputs are const and can be quantized up-front.\n    self.assertEqual(0, ops.count(""QuantizeV2"") + ops.count(""Quantize""))\n\n    # One dequantize at the end.\n    self.assertEqual(1, ops.count(""Dequantize""))\n\n    # No RequantizationRange\n    self.assertEqual(0, ops.count(""RequantizationRange""))\n\n    # The fallback constants are in the graph.\n    self.assertEqual(1, node_names.count(""fallback_quantization_min_value""))\n    self.assertEqual(1, node_names.count(""fallback_quantization_max_value""))\n\n  def test_remove_redundant_quantization(self):\n    a_constant_name = ""a_constant""\n    a_constant_min_name = ""a_constant_min""\n    a_constant_max_name = ""a_constant_max""\n    a_dequantize_name = ""a_dequantize""\n    a_quantize_name = ""a_quantize""\n    b_constant_name = ""b_constant""\n    b_constant_min_name = ""b_constant_min""\n    b_constant_max_name = ""b_constant_max""\n    b_dequantize_name = ""b_dequantize""\n    b_quantize_name = ""b_quantize""\n    mat_mul_name = ""mat_mul""\n    graph_def = graph_pb2.GraphDef()\n    a_constant = quantize_graph.create_constant_node(\n        a_constant_name, value=(0,), dtype=dtypes.quint8, shape=[])\n    graph_def.node.extend([a_constant])\n    a_constant_min = quantize_graph.create_constant_node(\n        a_constant_min_name, value=2, dtype=dtypes.float32, shape=[])\n    graph_def.node.extend([a_constant_min])\n    a_constant_max = quantize_graph.create_constant_node(\n        a_constant_max_name, value=2, dtype=dtypes.float32, shape=[])\n    graph_def.node.extend([a_constant_max])\n    a_dequantize_node = quantize_graph.create_node(\n        ""Dequantize"", a_dequantize_name,\n        [a_constant_name, a_constant_min_name, a_constant_max_name])\n    quantize_graph.set_attr_dtype(a_dequantize_node, ""T"", dtypes.uint8)\n    graph_def.node.extend([a_dequantize_node])\n    a_quantize_node = quantize_graph.create_node(\n        ""QuantizeV2"", a_quantize_name,\n        [a_dequantize_name, a_dequantize_name + "":1"", a_dequantize_name + "":2""])\n    quantize_graph.set_attr_dtype(a_quantize_node, ""T"", dtypes.uint8)\n    graph_def.node.extend([a_quantize_node])\n    b_constant = quantize_graph.create_constant_node(\n        b_constant_name, value=(0,), dtype=dtypes.quint8, shape=[])\n    graph_def.node.extend([b_constant])\n    b_constant_min = quantize_graph.create_constant_node(\n        b_constant_min_name, value=3, dtype=dtypes.float32, shape=[])\n    graph_def.node.extend([b_constant_min])\n    b_constant_max = quantize_graph.create_constant_node(\n        b_constant_max_name, value=3, dtype=dtypes.float32, shape=[])\n    graph_def.node.extend([b_constant_max])\n    b_dequantize_node = quantize_graph.create_node(\n        ""Dequantize"", b_dequantize_name,\n        [b_constant_name, b_constant_min_name, b_constant_max_name])\n    quantize_graph.set_attr_dtype(b_dequantize_node, ""T"", dtypes.uint8)\n    graph_def.node.extend([b_dequantize_node])\n    b_quantize_node = quantize_graph.create_node(\n        ""QuantizeV2"", b_quantize_name,\n        [b_dequantize_name, b_dequantize_name + "":1"", b_dequantize_name + "":2""])\n    quantize_graph.set_attr_dtype(b_quantize_node, ""T"", dtypes.uint8)\n    graph_def.node.extend([b_quantize_node])\n    mat_mul_node = quantize_graph.create_node(""QuantizedMatMul"", mat_mul_name, [\n        a_quantize_name, b_quantize_name, a_quantize_name + "":1"",\n        a_quantize_name + "":2"", b_quantize_name + "":1"", b_quantize_name + "":2""\n    ])\n    quantize_graph.set_attr_dtype(mat_mul_node, ""T1"", dtypes.uint8)\n    quantize_graph.set_attr_dtype(mat_mul_node, ""T2"", dtypes.int32)\n    graph_def.node.extend([mat_mul_node])\n\n    expected_output = graph_pb2.GraphDef()\n    a_constant = quantize_graph.create_constant_node(\n        a_constant_name, value=(0,), dtype=dtypes.quint8, shape=[])\n    expected_output.node.extend([a_constant])\n    a_constant_min = quantize_graph.create_constant_node(\n        a_constant_min_name, value=2, dtype=dtypes.float32, shape=[])\n    expected_output.node.extend([a_constant_min])\n    a_constant_max = quantize_graph.create_constant_node(\n        a_constant_max_name, value=2, dtype=dtypes.float32, shape=[])\n    expected_output.node.extend([a_constant_max])\n    b_constant = quantize_graph.create_constant_node(\n        b_constant_name, value=(0,), dtype=dtypes.quint8, shape=[])\n    expected_output.node.extend([b_constant])\n    b_constant_min = quantize_graph.create_constant_node(\n        b_constant_min_name, value=3, dtype=dtypes.float32, shape=[])\n    expected_output.node.extend([b_constant_min])\n    b_constant_max = quantize_graph.create_constant_node(\n        b_constant_max_name, value=3, dtype=dtypes.float32, shape=[])\n    expected_output.node.extend([b_constant_max])\n    mat_mul_node = quantize_graph.create_node(""QuantizedMatMul"", mat_mul_name, [\n        a_constant_name, b_constant_name, a_constant_min_name,\n        a_constant_max_name, b_constant_min_name, b_constant_max_name\n    ])\n    quantize_graph.set_attr_dtype(mat_mul_node, ""T1"", dtypes.uint8)\n    quantize_graph.set_attr_dtype(mat_mul_node, ""T2"", dtypes.int32)\n    expected_output.node.extend([mat_mul_node])\n    expected_output.versions.CopyFrom(graph_def.versions)\n    expected_output.library.CopyFrom(graph_def.library)\n\n    rewriter = quantize_graph.GraphRewriter(\n        graph_def, [mat_mul_name], quantized_input_range=None)\n    output = rewriter.remove_redundant_quantization(graph_def)\n    stripped_output = graph_util.extract_sub_graph(output, [mat_mul_name])\n    self.assertProtoEquals(expected_output, stripped_output)\n\n\nif __name__ == ""__main__"":\n  test.main()\n'"
libs/val_libs/__init__.py,0,b''
libs/val_libs/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\nfrom libs.configs import cfgs\nfrom help_utils.tools import *\n\ndef write_voc_results_file(all_boxes, test_imgid_list, det_save_dir):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n  the detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n\n  :param test_imgid_list:\n  :param det_save_path:\n  :return:\n  \'\'\'\n  for cls, cls_id in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    print(""Writing {} VOC resutls file"".format(cls))\n\n    mkdir(det_save_dir)\n    det_save_path = os.path.join(det_save_dir, ""det_""+cls+"".txt"")\n    with open(det_save_path, \'wt\') as f:\n      for index, img_name in enumerate(test_imgid_list):\n        this_img_detections = all_boxes[index]\n\n        this_cls_detections = this_img_detections[this_img_detections[:, 0]==cls_id]\n        if this_cls_detections.shape[0] == 0:\n          continue # this cls has none detections in this img\n        for a_det in this_cls_detections:\n          f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                  format(img_name, a_det[1],\n                         a_det[2], a_det[3],\n                         a_det[4], a_det[5]))  # that is [img_name, score, xmin, ymin, xmax, ymax]\n\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    bbox = obj.find(\'bndbox\')\n    obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                          int(bbox.find(\'ymin\').text),\n                          int(bbox.find(\'xmax\').text),\n                          int(bbox.find(\'ymax\').text)]\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath, annopath, test_imgid_list, cls_name, ovthresh=0.5,\n                 use_07_metric=False, use_diff=False):\n  \'\'\'\n\n  :param detpath:\n  :param annopath:\n  :param test_imgid_list: it \'s a list that contains the img_name of test_imgs\n  :param cls_name:\n  :param ovthresh:\n  :param use_07_metric:\n  :param use_diff:\n  :return:\n  \'\'\'\n  # 1. parse xml to get gtboxes\n\n  # read list of images\n  imagenames = test_imgid_list\n\n  recs = {}\n  for i, imagename in enumerate(imagenames):\n    recs[imagename] = parse_rec(os.path.join(annopath, imagename+\'.xml\'))\n    # if i % 100 == 0:\n    #   print(\'Reading annotation for {:d}/{:d}\'.format(\n    #     i + 1, len(imagenames)))\n\n  # 2. get gtboxes for this class.\n  class_recs = {}\n  num_pos = 0\n  # if cls_name == \'person\':\n  #   print (""aaa"")\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == cls_name]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    if use_diff:\n      difficult = np.array([False for x in R]).astype(np.bool)\n    else:\n      difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    num_pos = num_pos + sum(~difficult)  # ignored the diffcult boxes\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det} # det means that gtboxes has already been detected\n\n  # 3. read the detection file\n  detfile = os.path.join(detpath, ""det_""+cls_name+"".txt"")\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  # for a line. that is [img_name, confidence, xmin, ymin, xmax, ymax]\n  splitlines = [x.strip().split(\' \') for x in lines]  # a list that include a list\n  image_ids = [x[0] for x in splitlines]  # img_id is img_name\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids) # num of detections. That, a line is a det_box.\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]  #reorder the img_name\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]  # img_id is img_name\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        ixmin = np.maximum(BBGT[:, 0], bb[0])\n        iymin = np.maximum(BBGT[:, 1], bb[1])\n        ixmax = np.minimum(BBGT[:, 2], bb[2])\n        iymax = np.minimum(BBGT[:, 3], bb[3])\n        iw = np.maximum(ixmax - ixmin + 1., 0.)\n        ih = np.maximum(iymax - iymin + 1., 0.)\n        inters = iw * ih\n\n        # union\n        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n        overlaps = inters / uni\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # 4. get recall, precison and AP\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(num_pos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric=cfgs.USE_07_METRIC)\n\n  return rec, prec, ap\n\n\ndef do_python_eval(test_imgid_list, test_annotation_path):\n  AP_list = []\n  # import matplotlib.pyplot as plt\n  # import matplotlib.colors as colors\n  # color_list = colors.cnames.keys()[::6]\n\n  for cls, index in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    recall, precision, AP = voc_eval(detpath=os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION),\n                                     test_imgid_list=test_imgid_list,\n                                     cls_name=cls,\n                                     annopath=test_annotation_path)\n    AP_list += [AP]\n    print(""cls : {}|| Recall: {} || Precison: {}|| AP: {}"".format(cls, recall[-1], precision[-1], AP))\n    # plt.plot(recall, precision, label=cls, color=color_list[index])\n    # plt.legend(loc=\'upper right\')\n    print(10*""__"")\n  # plt.show()\n  # plt.savefig(cfgs.VERSION+\'.jpg\')\n  print(""mAP is : {}"".format(np.mean(AP_list)))\n\n\ndef voc_evaluate_detections(all_boxes, test_annotation_path, test_imgid_list):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n\n  The detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n  :return:\n  \'\'\'\n  test_imgid_list = [item.split(\'.\')[0] for item in test_imgid_list]\n\n  write_voc_results_file(all_boxes, test_imgid_list=test_imgid_list,\n                         det_save_dir=os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION))\n  do_python_eval(test_imgid_list, test_annotation_path=test_annotation_path)\n\n\n\n\n\n\n\n\n'"
data/lib_coco/PythonAPI/__init__.py,0,b''
data/lib_coco/PythonAPI/setup.py,0,"b'from setuptools import setup, Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'../common/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [np.get_include(), \'../common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(\n    name=\'pycocotools\',\n    packages=[\'pycocotools\'],\n    package_dir = {\'pycocotools\': \'pycocotools\'},\n    install_requires=[\n        \'setuptools>=18.0\',\n        \'cython>=0.27.3\',\n        \'matplotlib>=2.1.0\'\n    ],\n    version=\'2.0\',\n    ext_modules= ext_modules\n)\n'"
libs/box_utils/cython_utils/__init__.py,0,b''
libs/box_utils/cython_utils/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""cython_bbox"",\n        [""bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""cython_nms"",\n        [""nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    )\n    # Extension(\n    #     ""cpu_nms"",\n    #     [""cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # )\n]\n\nsetup(\n    name=\'tf_faster_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
libs/networks/mobilenet/__init__.py,0,b''
libs/networks/mobilenet/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional scope.\n\n  Returns:\n    Tensor of depth num_outputs\n\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=tf.identity)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')\n'"
libs/networks/mobilenet/mobilenet.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = defaults.items()\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode).\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      slim.arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n    set to non-training mode. This might be helpful for code that is reused\n    across both training/evaluation, but most of the time training_scope with\n    value False is not needed.\n\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability\n    bn_decay: decay for the batch norm moving averages.\n\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'decay\': bn_decay,\n  }\n\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      slim.arg_scope([slim.batch_norm], **batch_norm_params), \\\n      slim.arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s\n'"
libs/networks/mobilenet/mobilenet_v2.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\n\nArchitecture: https://arxiv.org/abs/1801.04381\n\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import conv_blocks as ops\nfrom libs.networks.mobilenet import mobilenet as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\nV2_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24),\n        op(ops.expanded_conv, stride=1, num_outputs=24),\n        op(ops.expanded_conv, stride=2, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=2, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=2, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=320),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n    ],\n)\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediciton_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_classes=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  with slim.\n\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']\n'"
libs/networks/mobilenet/mobilenet_v2_test.py,25,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for mobilenet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport copy\nimport tensorflow as tf\nfrom nets.mobilenet import conv_blocks as ops\nfrom nets.mobilenet import mobilenet\nfrom nets.mobilenet import mobilenet_v2\n\n\nslim = tf.contrib.slim\n\n\ndef find_ops(optype):\n  """"""Find ops of a given type in graphdef or a graph.\n\n  Args:\n    optype: operation type (e.g. Conv2D)\n  Returns:\n     List of operations.\n  """"""\n  gd = tf.get_default_graph()\n  return [var for var in gd.get_operations() if var.type == optype]\n\n\nclass MobilenetV2Test(tf.test.TestCase):\n\n  def setUp(self):\n    tf.reset_default_graph()\n\n  def testCreation(self):\n    spec = dict(mobilenet_v2.V2_DEF)\n    _, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n\n    # This is mostly a sanity test. No deep reason for these particular\n    # constants.\n    #\n    # All but first 2 and last one have  two convolutions, and there is one\n    # extra conv that is not in the spec. (logits)\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 2 - 2)\n    # Check that depthwise are exposed.\n    for i in range(2, 17):\n      self.assertIn(\'layer_%d/depthwise_output\' % i, ep)\n\n  def testCreationNoClasses(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    net, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec,\n        num_classes=None)\n    self.assertIs(net, ep[\'global_pool\'])\n\n  def testImageSizes(self):\n    for input_size, output_size in [(224, 7), (192, 6), (160, 5),\n                                    (128, 4), (96, 3)]:\n      tf.reset_default_graph()\n      _, ep = mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, input_size, input_size, 3)))\n\n      self.assertEqual(ep[\'layer_18/output\'].get_shape().as_list()[1:3],\n                       [output_size] * 2)\n\n  def testWithSplits(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    spec[\'overrides\'] = {\n        (ops.expanded_conv,): dict(split_expansion=2),\n    }\n    _, _ = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n    # All but 3 op has 3 conv operatore, the remainign 3 have one\n    # and there is one unaccounted.\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 3 - 5)\n\n  def testWithOutputStride8(self):\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testDivisibleBy(self):\n    tf.reset_default_graph()\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        divisible_by=16,\n        min_depth=32)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    self.assertSameElements([32, 64, 96, 160, 192, 320, 384, 576, 960, 1280,\n                             1001], s)\n\n  def testDivisibleByWithArgScope(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, 224, 224, 2)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n      s = set(s)\n      self.assertSameElements(s, [32, 192, 128, 1001])\n\n  def testFineGrained(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 2)),\n        conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.01,\n        finegrain_classification_mode=True)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    # All convolutions will be 8->48, except for the last one.\n    self.assertSameElements(s, [8, 48, 1001, 1280])\n\n  def testMobilenetBase(self):\n    tf.reset_default_graph()\n    # Verifies that mobilenet_base returns pre-pooling layer.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      net, _ = mobilenet_v2.mobilenet_base(\n          tf.placeholder(tf.float32, (10, 224, 224, 16)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      self.assertEqual(net.get_shape().as_list(), [10, 7, 7, 128])\n\n  def testWithOutputStride16(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n  def testWithOutputStride8AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        use_explicit_padding=True,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testWithOutputStride16AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16,\n        use_explicit_padding=True)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/__init__.py,0,b'\n'
libs/networks/slim_nets/alexnet.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
libs/networks/slim_nets/alexnet_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_resnet_v2 import inception_resnet_v2_base\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
libs/networks/slim_nets/inception_resnet_v2.py,41,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint=\'Conv2d_7b_1x1\',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None):\n  """"""Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_6a\', \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after \'PreAuxLogits\'.\n  """"""\n  if output_stride != 8 and output_stride != 16:\n    raise ValueError(\'output_stride must be 8 or 16.\')\n\n  padding = \'SAME\' if align_feature_maps else \'VALID\'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_3a_3x3\')\n      if add_and_check_final(\'MaxPool_3a_3x3\', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope=\'Conv2d_3b_1x1\')\n      if add_and_check_final(\'Conv2d_3b_1x1\', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope=\'Conv2d_4a_3x3\')\n      if add_and_check_final(\'Conv2d_4a_3x3\', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_5a_3x3\')\n      if add_and_check_final(\'MaxPool_5a_3x3\', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope(\'Mixed_5b\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                       scope=\'AvgPool_0a_3x3\')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final(\'Mixed_5b\', net): return net, end_points\n      # TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17)\n\n      # 17 x 17 x 1088 if output_stride == 8,\n      # 33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope(\'Mixed_6a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10)\n      if add_and_check_final(\'PreAuxLogits\', net): return net, end_points\n\n      if output_stride == 8:\n        # TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError(\'output_stride==8 is only supported up to the \'\n                         \'PreAuxlogits end_point for now.\')\n\n      # 8 x 8 x 2080\n      with tf.variable_scope(\'Mixed_7a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20)\n      net = block8(net, activation_fn=None)\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n      if add_and_check_final(\'Conv2d_7b_1x1\', net): return net, end_points\n\n    raise ValueError(\'final_endpoint (%s) not recognized\', final_endpoint)\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,#0.8\n                        reuse=None,\n                        scope=\'InceptionResnetV2\',\n                        create_aux_logits=True):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n\n      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n\n      if create_aux_logits:\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = end_points[\'PreAuxLogits\']\n          aux = slim.avg_pool2d(aux, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                              scope=\'AvgPool_1a_8x8\')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'Dropout\')\n\n        end_points[\'PreLogitsFlatten\'] = net\n        # end_points[\'yjr_feature\'] = tf.squeeze(net, axis=0)\n\n        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                      scope=\'Logits\')\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
libs/networks/slim_nets/inception_resnet_v2_test.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'AuxLogits\' in endpoints)\n      auxlogits = endpoints[\'AuxLogits\']\n      self.assertTrue(\n          auxlogits.op.name.startswith(\'InceptionResnetV2/AuxLogits\'))\n      self.assertListEqual(auxlogits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes,\n                                                        create_aux_logits=False)\n      self.assertTrue(\'AuxLogits\' not in endpoints)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_resnet_v2_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'InceptionResnetV2/Conv2d_7b_1x1\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 8, 8, 1536])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                          \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                 \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_resnet_v2_base(\n            inputs, final_endpoint=endpoint)\n        if endpoint != \'PreAuxLogits\':\n          self.assertTrue(out_tensor.op.name.startswith(\n              \'InceptionResnetV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 17, 17, 1088],\n                        \'PreAuxLogits\': [5, 17, 17, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithAlignedFeatureMaps(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', align_feature_maps=True)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2b_3x3\': [5, 150, 150, 64],\n                        \'MaxPool_3a_3x3\': [5, 75, 75, 64],\n                        \'Conv2d_3b_1x1\': [5, 75, 75, 80],\n                        \'Conv2d_4a_3x3\': [5, 75, 75, 192],\n                        \'MaxPool_5a_3x3\': [5, 38, 38, 192],\n                        \'Mixed_5b\': [5, 38, 38, 320],\n                        \'Mixed_6a\': [5, 19, 19, 1088],\n                        \'PreAuxLogits\': [5, 19, 19, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithOutputStrideEight(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', output_stride=8)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 33, 33, 1088],\n                        \'PreAuxLogits\': [5, 33, 33, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v4_test.py,24,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
libs/networks/slim_nets/mobilenet_v1.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""MobileNet v1.\n\nMobileNet is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nhead (for example: embeddings, localization and classification).\n\nAs described in https://arxiv.org/abs/1704.04861.\n\n  MobileNets: Efficient Convolutional Neural Networks for\n    Mobile Vision Applications\n  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n    Tobias Weyand, Marco Andreetto, Hartwig Adam\n\n100% Mobilenet V1 (base) with input size 224x224:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 864      10,838,016\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    288       3,612,672\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     2,048      25,690,112\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    576       1,806,336\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     8,192      25,690,112\nMobilenetV1/Conv2d_3_depthwise/depthwise:                  1,152       3,612,672\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                    16,384      51,380,224\nMobilenetV1/Conv2d_4_depthwise/depthwise:                  1,152         903,168\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    32,768      25,690,112\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  2,304       1,806,336\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    65,536      51,380,224\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  2,304         451,584\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                   131,072      25,690,112\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 4,608         225,792\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  524,288      25,690,112\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 9,216         451,584\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                1,048,576      51,380,224\n--------------------------------------------------------------------------------\nTotal:                                                 3,185,088     567,716,352\n\n\n75% Mobilenet V1 (base) with input size 128x128:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 648       2,654,208\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    216         884,736\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     1,152       4,718,592\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    432         442,368\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     4,608       4,718,592\nMobilenetV1/Conv2d_3_depthwise/depthwise:                    864         884,736\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                     9,216       9,437,184\nMobilenetV1/Conv2d_4_depthwise/depthwise:                    864         221,184\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    18,432       4,718,592\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  1,728         442,368\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    36,864       9,437,184\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  1,728         110,592\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                    73,728       4,718,592\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 3,456          55,296\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  294,912       4,718,592\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 6,912         110,592\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                  589,824       9,437,184\n--------------------------------------------------------------------------------\nTotal:                                                 1,800,144     106,002,432\n\n""""""\n\n# Tensorflow mandates these.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# Conv and DepthSepConv namedtuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=[3, 3], stride=2, depth=32),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)\n]\n\n\ndef mobilenet_v1_base(inputs,\n                      final_endpoint=\'Conv2d_13_pointwise\',\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      conv_defs=None,\n                      output_stride=None,\n                      scope=None):\n  """"""Mobilenet v1.\n\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_0\', \'Conv2d_1_pointwise\', \'Conv2d_2_pointwise\',\n      \'Conv2d_3_pointwise\', \'Conv2d_4_pointwise\', \'Conv2d_5\'_pointwise,\n      \'Conv2d_6_pointwise\', \'Conv2d_7_pointwise\', \'Conv2d_8_pointwise\',\n      \'Conv2d_9_pointwise\', \'Conv2d_10_pointwise\', \'Conv2d_11_pointwise\',\n      \'Conv2d_12_pointwise\', \'Conv2d_13_pointwise\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 8 (accurate fully convolutional\n      mode), 16 (fast fully convolutional mode), 32 (classification mode).\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  if conv_defs is None:\n    conv_defs = _CONV_DEFS\n\n  if output_stride is not None and output_stride not in [8, 16, 32]:\n    raise ValueError(\'Only allowed output_stride values are 8, 16, 32.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n      # The current_stride variable keeps track of the output stride of the\n      # activations, i.e., the running product of convolution strides up to the\n      # current network layer. This allows us to invoke atrous convolution\n      # whenever applying the next convolution would result in the activations\n      # having output stride larger than the target output_stride.\n      current_stride = 1\n\n      # The atrous convolution rate parameter.\n      rate = 1\n\n      net = inputs\n      for i, conv_def in enumerate(conv_defs):\n        end_point_base = \'Conv2d_%d\' % i\n\n        if output_stride is not None and current_stride == output_stride:\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          layer_stride = 1\n          layer_rate = rate\n          rate *= conv_def.stride\n        else:\n          layer_stride = conv_def.stride\n          layer_rate = 1\n          current_stride *= conv_def.stride\n\n        if isinstance(conv_def, Conv):\n          end_point = end_point_base\n          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n                            stride=conv_def.stride,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        elif isinstance(conv_def, DepthSepConv):\n          end_point = end_point_base + \'_depthwise\'\n\n          # By passing filters=None\n          # separable_conv2d produces only a depthwise convolution layer\n          net = slim.separable_conv2d(net, None, conv_def.kernel,\n                                      depth_multiplier=1,\n                                      stride=layer_stride,\n                                      rate=layer_rate,\n                                      normalizer_fn=slim.batch_norm,\n                                      scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n          end_point = end_point_base + \'_pointwise\'\n\n          net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                            stride=1,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n        else:\n          raise ValueError(\'Unknown convolution type %s for layer %d\'\n                           % (conv_def.ltype, i))\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef mobilenet_v1(inputs,\n                 num_classes=1000,\n                 dropout_keep_prob=0.999,\n                 is_training=True,\n                 min_depth=8,\n                 depth_multiplier=1.0,\n                 conv_defs=None,\n                 prediction_fn=tf.contrib.layers.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'MobilenetV1\'):\n  """"""Mobilenet v1 model for classification.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    is_training: whether is training or not.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\n                     len(input_shape))\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = mobilenet_v1_base(inputs, scope=scope,\n                                          min_depth=min_depth,\n                                          depth_multiplier=depth_multiplier,\n                                          conv_defs=conv_defs)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a\')\n        end_points[\'AvgPool_1a\'] = net\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      if prediction_fn:\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\n\nmobilenet_v1.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef mobilenet_v1_arg_scope(is_training=True,\n                           weight_decay=0.00004,\n                           stddev=0.09,\n                           regularize_depthwise=False):\n  """"""Defines the default MobilenetV1 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    regularize_depthwise: Whether or not apply regularization on depthwise.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v1 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  if regularize_depthwise:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n'"
libs/networks/slim_nets/mobilenet_v1_test.py,32,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""Tests for MobileNet v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\nclass MobilenetV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_13\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                          \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                          \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                          \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                          \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                          \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                          \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                          \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                          \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                          \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                          \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_0\',\n                 \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                 \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                 \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                 \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                 \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                 \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                 \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                 \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                 \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                 \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                 \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                 \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                 \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = mobilenet_v1.mobilenet_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'MobilenetV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildCustomNetworkUsingConvDefs(self):\n    batch_size = 5\n    height, width = 224, 224\n    conv_defs = [\n        mobilenet_v1.Conv(kernel=[3, 3], stride=2, depth=32),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=512)\n    ]\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(\n        inputs, final_endpoint=\'Conv2d_3_pointwise\', conv_defs=conv_defs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_3\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 56, 56, 512])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 7, 7, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 7, 7, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride16BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 16\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 14, 14, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride8BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 8\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 28, 28, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsApproximateFaceNet(self):\n    batch_size = 5\n    height, width = 128, 128\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\', depth_multiplier=0.75)\n    # For the Conv2d_0 layer FaceNet has depth=16\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_depthwise\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_pointwise\': [batch_size, 64, 64, 48],\n                        \'Conv2d_2_depthwise\': [batch_size, 32, 32, 48],\n                        \'Conv2d_2_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_depthwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_4_depthwise\': [batch_size, 16, 16, 96],\n                        \'Conv2d_4_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_depthwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_6_depthwise\': [batch_size, 8, 8, 192],\n                        \'Conv2d_6_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_12_depthwise\': [batch_size, 4, 4, 384],\n                        \'Conv2d_12_pointwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_depthwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_pointwise\': [batch_size, 4, 4, 768]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      mobilenet_v1.mobilenet_v1_base(inputs)\n      total_params, _ = slim.model_analyzer.analyze_vars(\n          slim.get_model_variables())\n      self.assertAlmostEqual(3217920L, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys() if key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Conv2d_13_pointwise\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_13_pointwise\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    mobilenet_v1.mobilenet_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = mobilenet_v1.mobilenet_v1(images,\n                                          num_classes=num_classes,\n                                          spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import mobilenet_v1\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n                \'mobilenet_v1\': mobilenet_v1.mobilenet_v1,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                  \'mobilenet_v1\': mobilenet_v1.mobilenet_v1_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
libs/networks/slim_nets/nets_factory_test.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnArgScope(self):\n    batch_size = 5\n    num_classes = 10\n    net = \'cifarnet\'\n    with self.test_session(use_gpu=True):\n      net_fn = nets_factory.get_network_fn(net, num_classes)\n      image_size = getattr(net_fn, \'default_image_size\', 224)\n      with slim.arg_scope([slim.model_variable, slim.variable],\n                          device=\'/CPU:0\'):\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        net_fn(inputs)\n      weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \'CifarNet/conv1\')[0]\n      self.assertDeviceEqual(\'/CPU:0\', weights.device)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
libs/networks/slim_nets/overfeat_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997, #0.997\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
libs/networks/slim_nets/resnet_v1.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.networks.slim_nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          # yjr_feature = tf.squeeze(net, [0, 1, 2])\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n\n        ###\n        # end_points[\'yjr_feature\'] = yjr_feature\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
libs/networks/slim_nets/resnet_v1_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    blocks = [\n        resnet_v1.resnet_v1_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v1.resnet_v1_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v2 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v2 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=False,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
libs/networks/slim_nets/resnet_v2_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    blocks = [\n        resnet_v2.resnet_v2_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v2.resnet_v2_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/vgg.py,10,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      # yjr_feature = tf.squeeze(net)\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      # end_points[\'yjr_feature\'] = yjr_feature\n      end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
libs/networks/slim_nets/vgg_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
data/lib_coco/PythonAPI/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
data/lib_coco/PythonAPI/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
data/lib_coco/PythonAPI/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
data/lib_coco/PythonAPI/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
