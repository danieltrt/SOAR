file_path,api_count,code
data/__init__.py,0,b''
help_utils/__init__.py,0,b''
help_utils/tools.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport math\nimport sys\nimport os\n\n\ndef view_bar(message, num, total):\n    rate = num / total\n    rate_num = int(rate * 40)\n    rate_nums = math.ceil(rate * 100)\n    r = \'\\r%s:[%s%s]%d%%\\t%d/%d\' % (message, "">"" * rate_num, "" "" * (40 - rate_num), rate_nums, num, total,)\n    sys.stdout.write(r)\n    sys.stdout.flush()\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)'"
libs/__init__.py,0,b''
libs/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport subprocess\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    # Extension(\n    #     ""utils.cython_bbox"",\n    #     [""utils/bbox.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    # Extension(\n    #     ""nms.cpu_nms"",\n    #     [""nms/cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_cython_nms"",\n    #     [""rotation/rotate_cython_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_circle_nms"",\n    #     [""rotation/rotate_circle_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\'nms.gpu_nms\',\n    #     [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    # Extension(\'rotation.rotate_gpu_nms\',\n    #     [\'rotation/rotate_nms_kernel.cu\', \'rotation/rotate_gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc anrbd not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    Extension(\'rotation.rbbox_overlaps\',\n        [\'rotation/rbbox_overlaps_kernel.cu\', \'rotation/rbbox_overlaps.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n    # Extension(\'rotation.rotate_polygon_nms\',\n    #     [\'rotation/rotate_polygon_nms_kernel.cu\', \'rotation/rotate_polygon_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    #\n    # Extension(\n    #     \'pycocotools._mask\',\n    #     sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n    #     include_dirs = [numpy_include, \'pycocotools\'],\n    #     extra_compile_args={\n    #         \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    # ),\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
tools/__init__.py,0,b''
tools/cocoval.py,0,"b""from data.lib_coco.PythonAPI.pycocotools.coco import COCO\nfrom data.lib_coco.PythonAPI.pycocotools.cocoeval import COCOeval\n\n\ndef cocoval(detected_json, eval_json):\n    eval_gt = COCO(eval_json)\n\n    eval_dt = eval_gt.loadRes(detected_json)\n    cocoEval = COCOeval(eval_gt, eval_dt, iouType='bbox')\n\n    # cocoEval.params.imgIds = eval_gt.getImgIds()\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndetected_json = 'epoch-2.coco'\neval_gt = 'instances_minival2014.json'\ncocoval(detected_json, eval_gt)"""
tools/demo.py,10,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\n\r\nimport os, sys\r\n\r\nsys.path.append(""../"")\r\nimport cv2\r\nimport numpy as np\r\nfrom timeit import default_timer as timer\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\r\nfrom libs.networks import build_whole_network\r\nfrom help_utils.tools import *\r\nfrom libs.box_utils import draw_box_in_img\r\nfrom libs.label_name_dict.label_dict import *\r\nfrom help_utils import tools\r\nfrom libs.box_utils import nms\r\n\r\n\r\ndef get_file_paths_recursive(folder=None, file_ext=None):\r\n    """""" Get the absolute path of all files in given folder recursively\r\n    :param folder:\r\n    :param file_ext:\r\n    :return:\r\n    """"""\r\n    file_list = []\r\n    if folder is None:\r\n        return file_list\r\n\r\n    for dir_path, dir_names, file_names in os.walk(folder):\r\n        for file_name in file_names:\r\n            if file_ext is None:\r\n                file_list.append(os.path.join(dir_path, file_name))\r\n                continue\r\n            if file_name.endswith(file_ext):\r\n                file_list.append(os.path.join(dir_path, file_name))\r\n    return file_list\r\n\r\n\r\ndef inference(det_net, file_paths, des_folder, h_len, w_len, h_overlap, w_overlap, save_res=False):\r\n\r\n    if save_res:\r\n        assert cfgs.SHOW_SCORE_THRSHOLD >= 0.5, \\\r\n            \'please set score threshold (example: SHOW_SCORE_THRSHOLD = 0.5) in cfgs.py\'\r\n\r\n    else:\r\n        assert cfgs.SHOW_SCORE_THRSHOLD <= 0.005, \\\r\n            \'please set score threshold (example: SHOW_SCORE_THRSHOLD = 0.00) in cfgs.py\'\r\n\r\n    # 1. preprocess img\r\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\r\n    img_batch = tf.cast(img_plac, tf.float32)\r\n    if cfgs.NET_NAME in [\'resnet101_v1d\']:\r\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\r\n    else:\r\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\r\n\r\n    img_batch = tf.expand_dims(img_batch, axis=0)\r\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\r\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\r\n                                                     is_resize=False)\r\n\r\n    det_boxes_h, det_scores_h, det_category_h = det_net.build_whole_detection_network(input_img_batch=img_batch,\r\n                                                                                      gtboxes_batch=None)\r\n\r\n    init_op = tf.group(\r\n        tf.global_variables_initializer(),\r\n        tf.local_variables_initializer()\r\n    )\r\n\r\n    restorer, restore_ckpt = det_net.get_restorer()\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(init_op)\r\n        if not restorer is None:\r\n            restorer.restore(sess, restore_ckpt)\r\n            print(\'restore model\')\r\n\r\n        if not os.path.exists(\'./tmp.txt\'):\r\n            fw = open(\'./tmp.txt\', \'w\')\r\n            fw.close()\r\n\r\n        fr = open(\'./tmp.txt\', \'r\')\r\n        pass_img = fr.readlines()\r\n        fr.close()\r\n\r\n        for count, img_path in enumerate(file_paths):\r\n            fw = open(\'./tmp.txt\', \'a+\')\r\n            if img_path + \'\\n\' in pass_img:\r\n                continue\r\n            start = timer()\r\n            img = cv2.imread(img_path)\r\n\r\n            box_res = []\r\n            label_res = []\r\n            score_res = []\r\n\r\n            imgH = img.shape[0]\r\n            imgW = img.shape[1]\r\n\r\n            if imgH < h_len:\r\n                temp = np.zeros([h_len, imgW, 3], np.float32)\r\n                temp[0:imgH, :, :] = img\r\n                img = temp\r\n                imgH = h_len\r\n\r\n            if imgW < w_len:\r\n                temp = np.zeros([imgH, w_len, 3], np.float32)\r\n                temp[:, 0:imgW, :] = img\r\n                img = temp\r\n                imgW = w_len\r\n\r\n            for hh in range(0, imgH, h_len - h_overlap):\r\n                if imgH - hh - 1 < h_len:\r\n                    hh_ = imgH - h_len\r\n                else:\r\n                    hh_ = hh\r\n                for ww in range(0, imgW, w_len - w_overlap):\r\n                    if imgW - ww - 1 < w_len:\r\n                        ww_ = imgW - w_len\r\n                    else:\r\n                        ww_ = ww\r\n                    src_img = img[hh_:(hh_ + h_len), ww_:(ww_ + w_len), :]\r\n\r\n                    det_boxes_h_, det_scores_h_, det_category_h_ = \\\r\n                        sess.run(\r\n                            [det_boxes_h, det_scores_h, det_category_h],\r\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\r\n                        )\r\n\r\n                    if len(det_boxes_h_) > 0:\r\n                        for ii in range(len(det_boxes_h_)):\r\n                            box = det_boxes_h_[ii]\r\n                            box[0] = box[0] + ww_\r\n                            box[1] = box[1] + hh_\r\n                            box[2] = box[2] + ww_\r\n                            box[3] = box[3] + hh_\r\n                            box_res.append(box)\r\n                            label_res.append(det_category_h_[ii])\r\n                            score_res.append(det_scores_h_[ii])\r\n\r\n            box_res = np.array(box_res)\r\n            label_res = np.array(label_res)\r\n            score_res = np.array(score_res)\r\n\r\n            box_res_, label_res_, score_res_ = [], [], []\r\n\r\n            h_threshold = {\'roundabout\': 0.35, \'tennis-court\': 0.35, \'swimming-pool\': 0.4, \'storage-tank\': 0.3,\r\n                           \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.4, \'ship\': 0.35, \'plane\': 0.35,\r\n                           \'large-vehicle\': 0.4, \'helicopter\': 0.4, \'harbor\': 0.3, \'ground-track-field\': 0.4,\r\n                           \'bridge\': 0.3, \'basketball-court\': 0.4, \'baseball-diamond\': 0.3}\r\n\r\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\r\n                index = np.where(label_res == sub_class)[0]\r\n                if len(index) == 0:\r\n                    continue\r\n                tmp_boxes_h = box_res[index]\r\n                tmp_label_h = label_res[index]\r\n                tmp_score_h = score_res[index]\r\n\r\n                tmp_boxes_h = np.array(tmp_boxes_h)\r\n                tmp = np.zeros([tmp_boxes_h.shape[0], tmp_boxes_h.shape[1] + 1])\r\n                tmp[:, 0:-1] = tmp_boxes_h\r\n                tmp[:, -1] = np.array(tmp_score_h)\r\n\r\n                inx = nms.py_cpu_nms(dets=np.array(tmp, np.float32),\r\n                                     thresh=h_threshold[LABEl_NAME_MAP[sub_class]],\r\n                                     max_output_size=500)\r\n\r\n                box_res_.extend(np.array(tmp_boxes_h)[inx])\r\n                score_res_.extend(np.array(tmp_score_h)[inx])\r\n                label_res_.extend(np.array(tmp_label_h)[inx])\r\n\r\n            time_elapsed = timer() - start\r\n\r\n            if save_res:\r\n\r\n                scores = np.array(score_res_)\r\n                labels = np.array(label_res_)\r\n                boxes = np.array(box_res_)\r\n                valid_show = scores > cfgs.SHOW_SCORE_THRSHOLD\r\n                scores = scores[valid_show]\r\n                boxes = boxes[valid_show]\r\n                labels = labels[valid_show]\r\n\r\n                det_detections_h = draw_box_in_img.draw_boxes_with_label_and_scores(np.array(img, np.float32),\r\n                                                                                    boxes=np.array(boxes),\r\n                                                                                    labels=np.array(labels),\r\n                                                                                    scores=np.array(scores),\r\n                                                                                    in_graph=False)\r\n\r\n                save_dir = os.path.join(des_folder, cfgs.VERSION)\r\n                tools.mkdir(save_dir)\r\n                cv2.imwrite(save_dir + \'/\' + img_path.split(\'/\')[-1].split(\'.\')[0] + \'_h.jpg\',\r\n                            det_detections_h)\r\n\r\n                view_bar(\'{} cost {}s\'.format(img_path.split(\'/\')[-1].split(\'.\')[0],\r\n                                              time_elapsed), count + 1, len(file_paths))\r\n\r\n            else:\r\n                # eval txt\r\n                CLASS_DOTA = NAME_LABEL_MAP.keys()\r\n\r\n                # Task2\r\n                write_handle_h = {}\r\n                txt_dir_h = os.path.join(\'txt_output\', cfgs.VERSION + \'_h\')\r\n                tools.mkdir(txt_dir_h)\r\n                for sub_class in CLASS_DOTA:\r\n                    if sub_class == \'back_ground\':\r\n                        continue\r\n                    write_handle_h[sub_class] = open(os.path.join(txt_dir_h, \'Task2_%s.txt\' % sub_class), \'a+\')\r\n\r\n                for i, hbox in enumerate(box_res_):\r\n                    command = \'%s %.3f %.1f %.1f %.1f %.1f\\n\' % (img_path.split(\'/\')[-1].split(\'.\')[0],\r\n                                                                 score_res_[i],\r\n                                                                 hbox[0], hbox[1], hbox[2], hbox[3])\r\n                    write_handle_h[LABEl_NAME_MAP[label_res_[i]]].write(command)\r\n\r\n                for sub_class in CLASS_DOTA:\r\n                    if sub_class == \'back_ground\':\r\n                        continue\r\n                    write_handle_h[sub_class].close()\r\n\r\n            view_bar(\'%s cost %.3fs\' % (img_path.split(\'/\')[-1].split(\'.\')[0],\r\n                                        time_elapsed), count + 1, len(file_paths))\r\n            fw.write(\'{}\\n\'.format(img_path))\r\n            fw.close()\r\n        os.remove(\'./tmp.txt\')\r\n\r\n\r\ndef parse_args():\r\n    """"""\r\n    Parse input arguments\r\n    """"""\r\n    parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\r\n    parser.add_argument(\'--src_folder\', dest=\'src_folder\',\r\n                        help=\'images path\',\r\n                        default=None, type=str)\r\n    parser.add_argument(\'--des_folder\', dest=\'des_folder\',\r\n                        help=\'output path\',\r\n                        default=None, type=str)\r\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\r\n                        help=\'image height\',\r\n                        default=800, type=int)\r\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\r\n                        help=\'image width\',\r\n                        default=800, type=int)\r\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\r\n                        help=\'height overlap\',\r\n                        default=200, type=int)\r\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\r\n                        help=\'width overlap\',\r\n                        default=200, type=int)\r\n    parser.add_argument(\'--image_ext\', dest=\'image_ext\',\r\n                        help=\'image format\',\r\n                        default=\'.png\', type=str)\r\n    parser.add_argument(\'--save_res\', dest=\'save_res\',\r\n                        help=\'save results\',\r\n                        default=True, type=bool)\r\n    parser.add_argument(\'--gpu\', dest=\'gpu\',\r\n                        help=\'gpu index\',\r\n                        default=\'0\', type=str)\r\n\r\n    if len(sys.argv) == 1:\r\n        parser.print_help()\r\n        sys.exit(1)\r\n\r\n    args = parser.parse_args()\r\n    return args\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    # args = parse_args()\r\n    # print(\'Called with args:\')\r\n    # print(args)\r\n\r\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\r\n\r\n    # file_paths = get_file_paths_recursive(args.src_folder, args.image_ext)\r\n    #\r\n    # det_net = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\r\n    #                                                is_training=False)\r\n    #\r\n    # inference(det_net, file_paths, args.des_folder, args.h_len, args.w_len,\r\n    #            args.h_overlap, args.w_overlap,  args.save_res)\r\n\r\n    file_paths = get_file_paths_recursive(\'/data/DOTA/test/images\', \'.png\')\r\n    det_net = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\r\n                                                   is_training=False)\r\n    inference(det_net, file_paths, \'./demos/\', 800, 800,\r\n              200, 200, False)\r\n'"
tools/eval.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\n\n\ndef eval_with_plac(det_net, real_test_imgname_list, img_root, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        all_boxes = []\n        for i, a_img_name in enumerate(real_test_imgname_list):\n\n            raw_img = cv2.imread(os.path.join(img_root, a_img_name))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n                if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                    draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                else:\n                    draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    in_graph=False)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + a_img_name + \'.jpg\',\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\n            dets = np.hstack((detected_categories.reshape(-1, 1),\n                              detected_scores.reshape(-1, 1),\n                              boxes))\n            all_boxes.append(dets)\n\n            tools.view_bar(\'%s image cost %.3fs\' % (a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\n\n        save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        fw1 = open(os.path.join(save_dir, \'detections.pkl\'), \'wb\')\n        pickle.dump(all_boxes, fw1)\n        return all_boxes\n\n\ndef eval(num_imgs, eval_dir, annotation_dir, showbox):\n\n    # with open(\'/home/yjr/DataSet/VOC/VOC_test/VOC2007/ImageSets/Main/aeroplane_test.txt\') as f:\n    #     all_lines = f.readlines()\n    # test_imgname_list = [a_line.split()[0].strip() for a_line in all_lines]\n\n    test_imgname_list = [item for item in os.listdir(eval_dir)\n                              if item.endswith((\'.jpg\', \'jpeg\', \'.png\', \'.tif\', \'.tiff\'))]\n    if num_imgs == np.inf:\n        real_test_imgname_list = test_imgname_list\n    else:\n        real_test_imgname_list = test_imgname_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    all_boxes = eval_with_plac(det_net=faster_rcnn, real_test_imgname_list=real_test_imgname_list,\n                               img_root=eval_dir,\n                               draw_imgs=showbox)\n\n    save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    with open(os.path.join(save_dir, \'detections.pkl\'), \'rb\') as f:\n        all_boxes = pickle.load(f)\n\n        print(len(all_boxes))\n\n    voc_eval.voc_evaluate_detections(all_boxes=all_boxes,\n                                     test_annotation_path=annotation_dir,\n                                     test_imgid_list=real_test_imgname_list)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_imgs\', dest=\'eval_imgs\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/VOC2007/VOCdevkit/VOC2007/JPEGImages\', type=str)\n    parser.add_argument(\'--annotation_dir\', dest=\'test_annotation_dir\',\n                        help=\'the dir save annotations\',\n                        default=\'/data/VOC2007/VOCdevkit/VOC2007/Annotations\', type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=True, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_dir=args.eval_imgs,\n         annotation_dir=args.test_annotation_dir,\n         showbox=args.showbox)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/eval_bdd.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\n\n\ndef eval_with_plac(det_net, real_test_imgname_list, img_root, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n    #     img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    # else:\n    #     img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        all_boxes = []\n        for i, a_img_name in enumerate(real_test_imgname_list):\n\n            raw_img = cv2.imread(os.path.join(img_root, a_img_name))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n                # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                #     draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                # else:\n                #     draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n\n                draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    in_graph=False)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + a_img_name + \'.jpg\',\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\n            dets = np.hstack((detected_categories.reshape(-1, 1),\n                              detected_scores.reshape(-1, 1),\n                              boxes))\n            all_boxes.append(dets)\n\n            tools.view_bar(\'{} image cost {}s\'.format(a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\n\n        save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        fw1 = open(os.path.join(save_dir, \'detections.pkl\'), \'wb\')\n        pickle.dump(all_boxes, fw1)\n        return all_boxes\n\n\ndef eval(num_imgs, eval_dir, annotation_dir, showbox):\n\n    # with open(\'/home/yjr/DataSet/VOC/VOC_test/VOC2007/ImageSets/Main/aeroplane_test.txt\') as f:\n    #     all_lines = f.readlines()\n    # test_imgname_list = [a_line.split()[0].strip() for a_line in all_lines]\n\n    test_imgname_list = [item for item in os.listdir(eval_dir)\n                              if item.endswith((\'.jpg\', \'jpeg\', \'.png\', \'.tif\', \'.tiff\'))]\n    if num_imgs == np.inf:\n        real_test_imgname_list = test_imgname_list\n    else:\n        real_test_imgname_list = test_imgname_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    all_boxes = eval_with_plac(det_net=faster_rcnn, real_test_imgname_list=real_test_imgname_list,\n                               img_root=eval_dir,\n                               draw_imgs=showbox)\n\n    save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    with open(os.path.join(save_dir, \'detections.pkl\'), \'rb\') as f:\n        all_boxes = pickle.load(f)\n\n        print(len(all_boxes))\n\n    voc_eval.voc_evaluate_detections(all_boxes=all_boxes,\n                                     test_annotation_path=annotation_dir,\n                                     test_imgid_list=real_test_imgname_list)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_imgs\', dest=\'eval_imgs\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/BDD100K/BDD100K_VOC/bdd100k_val/val\', type=str)\n    parser.add_argument(\'--annotation_dir\', dest=\'test_annotation_dir\',\n                        help=\'the dir save annotations\',\n                        default=\'/data/BDD100K/BDD100K_VOC/bdd100k_val/Annotations\', type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=True, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_dir=args.eval_imgs,\n         annotation_dir=args.test_annotation_dir,\n         showbox=args.showbox)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/eval_coco.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport json\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\n\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nfrom data.lib_coco.PythonAPI.pycocotools.cocoeval import COCOeval\n\n\ndef cocoval(detected_json, eval_json):\n    eval_gt = COCO(eval_json)\n\n    eval_dt = eval_gt.loadRes(detected_json)\n    cocoEval = COCOeval(eval_gt, eval_dt, iouType=\'bbox\')\n\n    # cocoEval.params.imgIds = eval_gt.getImgIds()\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndef eval_coco(det_net, real_test_img_list, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n        tools.mkdir(save_path)\n        fw_json_dt = open(os.path.join(save_path, \'coco_minival.json\'), \'w\')\n        coco_det = []\n        for i, a_img in enumerate(real_test_img_list):\n\n            record = json.loads(a_img)\n\n            img_path = os.path.join(\'/data/COCO/val2017\', record[\'fpath\'].split(\'_\')[-1])\n\n            raw_img = cv2.imread(img_path)\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n\n            eval_indices = detected_scores >= 0.01\n            detected_scores = detected_scores[eval_indices]\n            detected_boxes = detected_boxes[eval_indices]\n            detected_categories = detected_categories[eval_indices]\n\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n                if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                    draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                else:\n                    draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    in_graph=False)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + record[\'ID\'],\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax-xmin, ymax-ymin]))\n\n            # cost much time\n            for j, box in enumerate(boxes):\n                coco_det.append({\'bbox\': [float(box[0]), float(box[1]), float(box[2]), float(box[3])],\n                                 \'score\': float(detected_scores[j]), \'image_id\': int(record[\'ID\'].split(\'.jpg\')[0].split(\'_000000\')[-1]),\n                                 \'category_id\': int(classes_originID[LABEl_NAME_MAP[detected_categories[j]]])})\n\n            tools.view_bar(\'%s image cost %.3fs\' % (record[\'ID\'], (end - start)), i + 1, len(real_test_img_list))\n\n        json.dump(coco_det, fw_json_dt)\n        fw_json_dt.close()\n        return os.path.join(save_path, \'coco_minival.json\')\n\n\ndef eval(num_imgs, eval_data, eval_gt, showbox):\n\n    with open(eval_data) as f:\n        test_img_list = f.readlines()\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_img_list\n    else:\n        real_test_img_list = test_img_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    detected_json = eval_coco(det_net=faster_rcnn, real_test_img_list=real_test_img_list, draw_imgs=showbox)\n\n    # save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n    # detected_json = os.path.join(save_path, \'coco_res.json\')\n    cocoval(detected_json, eval_gt)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_data\', dest=\'eval_data\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/COCO/coco_minival2014.odgt\', type=str)\n    parser.add_argument(\'--eval_gt\', dest=\'eval_gt\',\n                        help=\'eval gt\',\n                        default=\'/data/COCO/instances_minival2014.json\',\n                        type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=True, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    # args = parse_args()\n    # print(20*""--"")\n    # print(args)\n    # print(20*""--"")\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    # eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n    #      eval_data=args.eval_data,\n    #      eval_gt=args.eval_gt,\n    #      showbox=args.showbox)\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_data=\'/data/COCO/coco_minival2014.odgt\',\n         eval_gt=\'/data/COCO/instances_minival2014.json\',\n         showbox=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/eval_coco_pyramid.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport json\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import nms\n\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nfrom data.lib_coco.PythonAPI.pycocotools.cocoeval import COCOeval\n\n\ndef cocoval(detected_json, eval_json):\n    eval_gt = COCO(eval_json)\n\n    eval_dt = eval_gt.loadRes(detected_json)\n    cocoEval = COCOeval(eval_gt, eval_dt, iouType=\'bbox\')\n\n    # cocoEval.params.imgIds = eval_gt.getImgIds()\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndef eval_coco(det_net, real_test_img_list, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH,\n                                                     is_resize=False)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n        tools.mkdir(save_path)\n        fw_json_dt = open(os.path.join(save_path, \'coco_minival_ms.json\'), \'w\')\n        coco_det = []\n        for i, a_img in enumerate(real_test_img_list):\n\n            record = json.loads(a_img)\n            raw_img = cv2.imread(record[\'fpath\'])\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n\n            detected_scores_, detected_boxes_, detected_categories_ = [], [], []\n\n            for ss in [600, 800, 1000, 1200]:  # cfgs.IMG_SHORT_SIDE_LEN:\n                img_resize = cv2.resize(raw_img, (ss, ss))\n\n                resized_img, tmp_detected_boxes, tmp_detected_scores, tmp_detected_categories = \\\n                    sess.run(\n                        [img_batch, detection_boxes, detection_scores, detection_category],\n                        feed_dict={img_plac: img_resize[:, :, ::-1]}  # cv is BGR. But need RGB\n                    )\n\n                eval_indices = tmp_detected_scores >= 0.01\n                tmp_detected_scores = tmp_detected_scores[eval_indices]\n                tmp_detected_boxes = tmp_detected_boxes[eval_indices]\n                tmp_detected_categories = tmp_detected_categories[eval_indices]\n\n                xmin, ymin, xmax, ymax = tmp_detected_boxes[:, 0], tmp_detected_boxes[:, 1], \\\n                                         tmp_detected_boxes[:, 2], tmp_detected_boxes[:, 3]\n\n                resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n                xmin = xmin * raw_w / resized_w\n                xmax = xmax * raw_w / resized_w\n\n                ymin = ymin * raw_h / resized_h\n                ymax = ymax * raw_h / resized_h\n\n                resize_boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\n\n                detected_scores_.append(tmp_detected_scores)\n                detected_boxes_.append(resize_boxes)\n                detected_categories_.append(tmp_detected_categories)\n\n            detected_scores_ = np.concatenate(detected_scores_)\n            detected_boxes_ = np.concatenate(detected_boxes_)\n            detected_categories_ = np.concatenate(detected_categories_)\n\n            detected_scores, detected_boxes, detected_categories = [], [], []\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(detected_categories_ == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_h = detected_boxes_[index]\n                tmp_label_h = detected_categories_[index]\n                tmp_score_h = detected_scores_[index]\n\n                tmp_boxes_h = np.array(tmp_boxes_h)\n                tmp = np.zeros([tmp_boxes_h.shape[0], tmp_boxes_h.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_h\n                tmp[:, -1] = np.array(tmp_score_h)\n\n                inx = nms.py_cpu_nms(dets=np.array(tmp, np.float32),\n                                     thresh=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                     max_output_size=500)\n\n                detected_boxes.extend(np.array(tmp_boxes_h)[inx])\n                detected_scores.extend(np.array(tmp_score_h)[inx])\n                detected_categories.extend(np.array(tmp_label_h)[inx])\n\n            detected_scores = np.array(detected_scores)\n            detected_boxes = np.array(detected_boxes)\n            detected_categories = np.array(detected_categories)\n\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                #     draw_img = (raw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                # else:\n                #     draw_img = raw_img + np.array(cfgs.PIXEL_MEAN)\n\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                raw_img = np.array(raw_img, np.float32)\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(raw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    in_graph=False)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + record[\'ID\'],\n                            final_detections)\n\n            # cost much time\n            for j, box in enumerate(detected_boxes):\n                coco_det.append({\'bbox\': [float(box[0]), float(box[1]), float(box[2]-box[0]), float(box[3]-box[1])],\n                                 \'score\': float(detected_scores[j]), \'image_id\': int(record[\'ID\'].split(\'.jpg\')[0].split(\'_000000\')[-1]),\n                                 \'category_id\': int(classes_originID[LABEl_NAME_MAP[detected_categories[j]]])})\n            end = time.time()\n            tools.view_bar(\'%s image cost %.3fs\' % (record[\'ID\'], (end - start)), i + 1, len(real_test_img_list))\n\n        json.dump(coco_det, fw_json_dt)\n        fw_json_dt.close()\n        return os.path.join(save_path, \'coco_minival_ms.json\')\n\n\ndef eval(num_imgs, eval_data, eval_gt, showbox):\n\n    with open(eval_data) as f:\n        test_img_list = f.readlines()\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_img_list\n    else:\n        real_test_img_list = test_img_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    detected_json = eval_coco(det_net=faster_rcnn, real_test_img_list=real_test_img_list, draw_imgs=showbox)\n\n    # save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n    # detected_json = os.path.join(save_path, \'coco_res.json\')\n    cocoval(detected_json, eval_gt)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_data\', dest=\'eval_data\',\n                        help=\'evaluate imgs dir \',\n                        default=\'coco_minival2014.odgt\', type=str)\n    parser.add_argument(\'--eval_gt\', dest=\'eval_gt\',\n                        help=\'eval gt\',\n                        default=\'instances_minival2014.json\',\n                        type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=True, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_data=args.eval_data,\n         eval_gt=args.eval_gt,\n         showbox=args.showbox)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/eval_voc2012.py,11,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\n\r\nimport os, sys\r\nimport tensorflow as tf\r\nimport time\r\nimport cv2\r\nimport pickle\r\nimport numpy as np\r\nsys.path.append(""../"")\r\n\r\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\r\nfrom libs.configs import cfgs\r\nfrom libs.networks import build_whole_network\r\nfrom libs.val_libs import voc_eval\r\nfrom libs.box_utils import draw_box_in_img\r\nimport argparse\r\nfrom help_utils import tools\r\nfrom libs.label_name_dict.label_dict import *\r\n\r\n\r\ndef eval_with_plac(det_net, real_test_imgname_list, img_root, draw_imgs=False):\r\n\r\n    # 1. preprocess img\r\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\r\n    img_batch = tf.cast(img_plac, tf.float32)\r\n\r\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\r\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\r\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\r\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\r\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\r\n    else:\r\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\r\n\r\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\r\n    img_batch = tf.expand_dims(img_batch, axis=0)\r\n\r\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\r\n        input_img_batch=img_batch,\r\n        gtboxes_batch=None)\r\n\r\n    init_op = tf.group(\r\n        tf.global_variables_initializer(),\r\n        tf.local_variables_initializer()\r\n    )\r\n\r\n    restorer, restore_ckpt = det_net.get_restorer()\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(init_op)\r\n        if not restorer is None:\r\n            restorer.restore(sess, restore_ckpt)\r\n            print(\'restore model\')\r\n\r\n        for i, a_img_name in enumerate(real_test_imgname_list):\r\n\r\n            raw_img = cv2.imread(os.path.join(img_root, a_img_name))\r\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\r\n\r\n            start = time.time()\r\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\r\n                sess.run(\r\n                    [img_batch, detection_boxes, detection_scores, detection_category],\r\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\r\n                )\r\n            end = time.time()\r\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\r\n            if draw_imgs:\r\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\r\n                show_scores = detected_scores[show_indices]\r\n                show_boxes = detected_boxes[show_indices]\r\n                show_categories = detected_categories[show_indices]\r\n\r\n                draw_img = np.squeeze(resized_img, 0)\r\n                if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\r\n                    draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\r\n                else:\r\n                    draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\r\n\r\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\r\n\r\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\r\n                                                                                    boxes=show_boxes,\r\n                                                                                    labels=show_categories,\r\n                                                                                    scores=show_scores,\r\n                                                                                    in_graph=False)\r\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\r\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\r\n\r\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + a_img_name,\r\n                            final_detections[:, :, ::-1])\r\n\r\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\r\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\r\n\r\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\r\n\r\n            xmin = xmin * raw_w / resized_w\r\n            xmax = xmax * raw_w / resized_w\r\n\r\n            ymin = ymin * raw_h / resized_h\r\n            ymax = ymax * raw_h / resized_h\r\n\r\n            boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\r\n            dets = np.hstack((detected_categories.reshape(-1, 1),\r\n                              detected_scores.reshape(-1, 1),\r\n                              boxes))\r\n            # all_boxes.append(dets)\r\n\r\n            # eval txt\r\n            CLASS_VOC = NAME_LABEL_MAP.keys()\r\n\r\n            write_handle = {}\r\n            txt_dir = os.path.join(\'voc2012_eval\', cfgs.VERSION, \'results\', \'VOC2012\', \'Main\')\r\n            tools.mkdir(txt_dir)\r\n            for sub_class in CLASS_VOC:\r\n                if sub_class == \'back_ground\':\r\n                    continue\r\n                write_handle[sub_class] = open(os.path.join(txt_dir, \'comp3_det_test_%s.txt\' % sub_class), \'a+\')\r\n\r\n            for det in dets:\r\n                command = \'%s %.6f %.6f %.6f %.6f %.6f\\n\' % (a_img_name.split(\'/\')[-1].split(\'.\')[0],\r\n                                                             det[1],\r\n                                                             det[2], det[3], det[4], det[5])\r\n                write_handle[LABEl_NAME_MAP[det[0]]].write(command)\r\n\r\n            for sub_class in CLASS_VOC:\r\n                if sub_class == \'back_ground\':\r\n                    continue\r\n                write_handle[sub_class].close()\r\n\r\n            tools.view_bar(\'%s image cost %.3fs\' % (a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\r\n\r\n\r\ndef eval(num_imgs, eval_dir, showbox):\r\n\r\n    test_imgname_list = [item for item in os.listdir(eval_dir)\r\n                              if item.endswith((\'.jpg\', \'jpeg\', \'.png\', \'.tif\', \'.tiff\'))]\r\n    if num_imgs == np.inf:\r\n        real_test_imgname_list = test_imgname_list\r\n    else:\r\n        real_test_imgname_list = test_imgname_list[: num_imgs]\r\n\r\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\r\n                                                       is_training=False)\r\n    eval_with_plac(det_net=faster_rcnn, real_test_imgname_list=real_test_imgname_list,\r\n                   img_root=eval_dir,\r\n                   draw_imgs=showbox)\r\n\r\n\r\ndef parse_args():\r\n\r\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\r\n\r\n    parser.add_argument(\'--eval_imgs\', dest=\'eval_imgs\',\r\n                        help=\'evaluate imgs dir \',\r\n                        default=\'/data/VOC2007/test/VOCdevkit/VOC2007/JPEGImages\', type=str)\r\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\r\n                        help=\'whether show detecion results when evaluation\',\r\n                        default=True, type=bool)\r\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\r\n                        help=\'gpu id\',\r\n                        default=\'0\', type=str)\r\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\r\n                        help=\'the num of eval imgs\',\r\n                        default=np.inf, type=int)\r\n    args = parser.parse_args()\r\n    return args\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    args = parse_args()\r\n    print(20*""--"")\r\n    print(args)\r\n    print(20*""--"")\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\r\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\r\n         eval_dir=args.eval_imgs,\r\n         showbox=args.showbox)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
tools/inference.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\ndef detect(det_net, inference_save_path, real_test_imgname_list):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0)  # [1, None, None, 3]\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        for i, a_img_name in enumerate(real_test_imgname_list):\n\n            raw_img = cv2.imread(a_img_name)\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n\n            show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n            show_scores = detected_scores[show_indices]\n            show_boxes = detected_boxes[show_indices]\n            show_categories = detected_categories[show_indices]\n\n            draw_img = np.squeeze(resized_img, 0)\n\n            if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n            else:\n                draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=show_boxes,\n                                                                                labels=show_categories,\n                                                                                scores=show_scores,\n                                                                                in_graph=False)\n            nake_name = a_img_name.split(\'/\')[-1]\n            # print (inference_save_path + \'/\' + nake_name)\n            cv2.imwrite(inference_save_path + \'/\' + nake_name,\n                        final_detections[:, :, ::-1])\n\n            tools.view_bar(\'{} image cost {}s\'.format(a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\n\n\ndef inference(test_dir, inference_save_path):\n\n    test_imgname_list = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)\n                                                          if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    detect(det_net=faster_rcnn, inference_save_path=inference_save_path, real_test_imgname_list=test_imgname_list)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'TestImgs...U need provide the test dir\')\n    parser.add_argument(\'--data_dir\', dest=\'data_dir\',\n                        help=\'data path\',\n                        default=\'demos\', type=str)\n    parser.add_argument(\'--save_dir\', dest=\'save_dir\',\n                        help=\'demo imgs to save\',\n                        default=\'inference_results\', type=str)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id \',\n                        default=\'0\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    inference(args.data_dir,\n              inference_save_path=args.save_dir)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/inference_for_coco.py,9,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nsys.path.append(""../"")\nsys.path.insert(0, \'/home/yjr/PycharmProjects/Faster-RCNN_TF/data/lib_coco/PythonAPI\')\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nfrom libs.label_name_dict.coco_dict import LABEL_NAME_MAP, classes_originID\nfrom help_utils import tools\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nimport json\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef eval_with_plac(det_net, imgId_list, coco, out_json_root, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    # coco_test_results = []\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        for i, imgid in enumerate(imgId_list):\n            imgname = coco.loadImgs(ids=[imgid])[0][\'file_name\']\n            raw_img = cv2.imread(os.path.join(""/home/yjr/DataSet/COCO/2017/test2017"", imgname))\n\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n                if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                    draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                else:\n                    draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    in_graph=False)\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + str(imgid) + \'.jpg\',\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax-xmin, ymax-ymin]))\n\n            dets = np.hstack((detected_categories.reshape(-1, 1),\n                              detected_scores.reshape(-1, 1),\n                              boxes))\n\n            a_img_detect_result = []\n            for a_det in dets:\n                label, score, bbox = a_det[0], a_det[1], a_det[2:]\n                cat_id = classes_originID[LABEL_NAME_MAP[label]]\n                if score<0.00001:\n                   continue\n                det_object = {""image_id"": imgid,\n                              ""category_id"": cat_id,\n                              ""bbox"": bbox.tolist(),\n                              ""score"": float(score)}\n                # print (det_object)\n                a_img_detect_result.append(det_object)\n            f = open(os.path.join(out_json_root, \'each_img\', str(imgid)+\'.json\'), \'w\')\n            json.dump(a_img_detect_result, f)  # , indent=4\n            f.close()\n            del a_img_detect_result\n            del dets\n            del boxes\n            del resized_img\n            del raw_img\n            tools.view_bar(\'{} image cost {}s\'.format(imgid, (end - start)), i + 1, len(imgId_list))\n\n\ndef eval(num_imgs):\n\n\n   # annotation_path = \'/home/yjr/DataSet/COCO/2017/test_annotations/image_info_test2017.json\'\n    annotation_path = \'/home/yjr/DataSet/COCO/2017/test_annotations/image_info_test-dev2017.json\'\n    # annotation_path = \'/home/yjr/DataSet/COCO/2017/annotations/instances_train2017.json\'\n    print(""load coco .... it will cost about 17s.."")\n    coco = COCO(annotation_path)\n\n    imgId_list = coco.getImgIds()\n\n    if num_imgs !=np.inf:\n        imgId_list = imgId_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    save_dir = os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION)\n    eval_with_plac(det_net=faster_rcnn, coco=coco, imgId_list=imgId_list, out_json_root=save_dir,\n                   draw_imgs=True)\n    print(""each img over**************"")\n\n    final_detections = []\n    with open(os.path.join(save_dir, \'coco2017test_results.json\'), \'w\') as wf:\n        for imgid in imgId_list:\n            f = open(os.path.join(save_dir, \'each_img\', str(imgid)+\'.json\'))\n            tmp_list = json.load(f)\n            # print (type(tmp_list))\n            final_detections.extend(tmp_list)\n            del tmp_list\n            f.close()\n        json.dump(final_detections, wf)\n\n\nif __name__ == \'__main__\':\n\n    eval(np.inf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train.py,65,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label, num_objects):\n    return gtboxes_and_label[:int(num_objects), :]\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                           is_training=True)\n\n        with tf.name_scope(\'get_batch\'):\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                           is_training=True)\n            # gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n            # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            #     img_batch = img_batch / tf.constant([cfgs.PIXEL_STD])\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            # img_name = img_name_batch[i]\n            img = tf.expand_dims(img_batch[i], axis=0)\n            if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label = tf.cast(tf.reshape(gtboxes_and_label_batch[i], [-1, 5]), tf.float32)\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n            # img_h = tf.cast(tf.reshape(img_h, [-1, ]), tf.float32)\n            # img_w = tf.cast(tf.reshape(img_w, [-1, ]), tf.float32)\n\n            inputs_list.append([img, gtboxes_and_label, num_objects, img_h, img_w])\n\n        # put_op_list = []\n        # get_op_list = []\n        # for i in range(num_gpu):\n        #     with tf.device(""/GPU:%s"" % i):\n        #         area = tf.contrib.staging.StagingArea(\n        #             dtypes=[tf.float32, tf.float32, tf.float32])\n        #         put_op_list.append(area.put(inputs_list[i]))\n        #         get_op_list.append(area.get())\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'rpn_cls_loss\': tf.constant(0., tf.float32),\n            \'rpn_loc_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_cls_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_loc_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n\n        }\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label = tf.py_func(get_gtboxes_and_label,\n                                                               inp=[inputs_list[i][1], inputs_list[i][2]],\n                                                               Tout=tf.float32)\n                                gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = faster_rcnn.build_whole_detection_network(input_img_batch=img,\n                                                                                    gtboxes_batch=gtboxes_and_label)\n                                gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img,\n                                                                                               boxes=gtboxes_and_label[\n                                                                                                     :, :-1],\n                                                                                               labels=gtboxes_and_label[\n                                                                                                      :, -1])\n                                tf.summary.image(\'Compare/gtboxes_gpu:%d\' % i, gtboxes_in_img)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2])\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        # final_gvs = []\n        # with tf.variable_scope(\'Gradient_Mult\'):\n        #     for grad, var in grads:\n        #         scale = 1.\n        #         # if \'/biases:\' in var.name:\n        #         #    scale *= 2.\n        #         if \'conv_new\' in var.name:\n        #             scale *= 3.\n        #         if not np.allclose(scale, 1.0):\n        #             grad = tf.multiply(grad, scale)\n        #         final_gvs.append((grad, var))\n\n        apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = faster_rcnn.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'coco_\' + str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_aug.py,63,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\n# from libs.networks import build_whole_network2\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord_multi_gpu_aug import next_batch\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label, num_objects):\n    return gtboxes_and_label[:int(num_objects), :]\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100., cfgs.LR / 1000.])\n        tf.summary.scalar(\'lr\', lr)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                           is_training=True)\n\n        with tf.name_scope(\'get_batch\'):\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                           is_training=True)\n            if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                img_batch = img_batch / tf.constant([cfgs.PIXEL_STD])\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            # img_name = img_name_batch[i]\n            img_ex = tf.expand_dims(img_batch[i], axis=0)\n            # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            #     img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_pad = tf.cast(tf.reshape(gtboxes_and_label_batch[i], [-1, 5]), tf.float32)\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n            # img_h = tf.cast(tf.reshape(img_h, [-1, ]), tf.float32)\n            # img_w = tf.cast(tf.reshape(img_w, [-1, ]), tf.float32)\n\n            inputs_list.append([img_ex, gtboxes_and_label_pad, num_objects, img_h, img_w])\n\n        # put_op_list = []\n        # get_op_list = []\n        # for i in range(num_gpu):\n        #     with tf.device(""/GPU:%s"" % i):\n        #         area = tf.contrib.staging.StagingArea(\n        #             dtypes=[tf.float32, tf.float32, tf.float32])\n        #         put_op_list.append(area.put(inputs_list[i]))\n        #         get_op_list.append(area.get())\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'rpn_cls_loss\': tf.constant(0., tf.float32),\n            \'rpn_loc_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_cls_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_loc_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n\n        }\n        if cfgs.USE_ATTENTION:\n            total_loss_dict[\'mask_loss\'] = tf.constant(0., tf.float32)\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label = tf.py_func(get_gtboxes_and_label,\n                                                               inp=[inputs_list[i][1], inputs_list[i][2]],\n                                                               Tout=tf.float32)\n                                gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = faster_rcnn.build_whole_detection_network(input_img_batch=img,\n                                                                                    gtboxes_batch=gtboxes_and_label)\n                                gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img,\n                                                                                               boxes=gtboxes_and_label[\n                                                                                                     :, :-1],\n                                                                                               labels=gtboxes_and_label[\n                                                                                                      :, -1])\n                                tf.summary.image(\'Compare/gtboxes_gpu:%d\' % i, gtboxes_in_img)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2])\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        # final_gvs = []\n        # with tf.variable_scope(\'Gradient_Mult\'):\n        #     for grad, var in grads:\n        #         scale = 1.\n        #         # if \'/biases:\' in var.name:\n        #         #    scale *= 2.\n        #         if \'conv_new\' in var.name:\n        #             scale *= 3.\n        #         if not np.allclose(scale, 1.0):\n        #             grad = tf.multiply(grad, scale)\n        #         final_gvs.append((grad, var))\n\n        apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = faster_rcnn.get_restorer()\n        saver = tf.train.Saver(max_to_keep=10)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                # inputs_list_ = sess.run(inputs_list)\n                # import cv2\n                # for kk in range(4):\n                #     cv2.imwrite(\'./img{}.jpg\'.format(kk), inputs_list_[kk][0][0])\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'coco_\' + str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_warmup_cosine.py,67,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\n# from libs.networks import build_whole_network2\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef warmup_and_cosine_lr(init_lr, global_step, warmup_step, decay_steps, alpha=1e-6):\n    def warmup_lr(init_lr, global_step, warmup_step):\n        global_step = tf.cast(global_step, tf.float32)\n        return 1e-6 + (init_lr - 1e-6) * global_step / warmup_step\n\n    def cosine_lr(init_lr, global_step, decay_steps, alpha=0.0):\n        return tf.train.cosine_decay(learning_rate=init_lr,\n                                     global_step=global_step - warmup_step,\n                                     decay_steps=decay_steps - warmup_step,\n                                     alpha=alpha)\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup_lr(init_lr, global_step, warmup_step),\n                   false_fn=lambda: cosine_lr(init_lr, global_step, decay_steps, alpha))\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label, num_objects):\n    return gtboxes_and_label[:int(num_objects), :]\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        # lr = tf.train.piecewise_constant(global_step,\n        #                                  boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n        #                                              np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n        #                                              np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n        #                                  values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100., cfgs.LR / 1000.])\n\n        lr = warmup_and_cosine_lr(init_lr=cfgs.LR,\n                                  global_step=global_step,\n                                  warmup_step=5000, decay_steps=cfgs.MAX_ITERATION, alpha=1e-6)\n        tf.summary.scalar(\'lr\', lr)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                           is_training=True)\n\n        with tf.name_scope(\'get_batch\'):\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                           is_training=True)\n            # gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n            # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            #     img_batch = img_batch / tf.constant([cfgs.PIXEL_STD])\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            # img_name = img_name_batch[i]\n            img = tf.expand_dims(img_batch[i], axis=0)\n            if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label = tf.cast(tf.reshape(gtboxes_and_label_batch[i], [-1, 5]), tf.float32)\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n            # img_h = tf.cast(tf.reshape(img_h, [-1, ]), tf.float32)\n            # img_w = tf.cast(tf.reshape(img_w, [-1, ]), tf.float32)\n\n            inputs_list.append([img, gtboxes_and_label, num_objects, img_h, img_w])\n\n        # put_op_list = []\n        # get_op_list = []\n        # for i in range(num_gpu):\n        #     with tf.device(""/GPU:%s"" % i):\n        #         area = tf.contrib.staging.StagingArea(\n        #             dtypes=[tf.float32, tf.float32, tf.float32])\n        #         put_op_list.append(area.put(inputs_list[i]))\n        #         get_op_list.append(area.get())\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'rpn_cls_loss\': tf.constant(0., tf.float32),\n            \'rpn_loc_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_cls_loss\': tf.constant(0., tf.float32),\n            \'fastrcnn_loc_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n\n        }\n        if cfgs.USE_ATTENTION:\n            total_loss_dict[\'mask_loss\'] = tf.constant(0., tf.float32)\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label = tf.py_func(get_gtboxes_and_label,\n                                                               inp=[inputs_list[i][1], inputs_list[i][2]],\n                                                               Tout=tf.float32)\n                                gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = faster_rcnn.build_whole_detection_network(input_img_batch=img,\n                                                                                    gtboxes_batch=gtboxes_and_label)\n                                gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img,\n                                                                                               boxes=gtboxes_and_label[\n                                                                                                     :, :-1],\n                                                                                               labels=gtboxes_and_label[\n                                                                                                      :, -1])\n                                tf.summary.image(\'Compare/gtboxes_gpu:%d\' % i, gtboxes_in_img)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2])\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        # final_gvs = []\n        # with tf.variable_scope(\'Gradient_Mult\'):\n        #     for grad, var in grads:\n        #         scale = 1.\n        #         # if \'/biases:\' in var.name:\n        #         #    scale *= 2.\n        #         if \'conv_new\' in var.name:\n        #             scale *= 3.\n        #         if not np.allclose(scale, 1.0):\n        #             grad = tf.multiply(grad, scale)\n        #         final_gvs.append((grad, var))\n\n        apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = faster_rcnn.get_restorer()\n        saver = tf.train.Saver(max_to_keep=10)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                # start = time.time()\n                #\n                # _, global_stepnp, total_loss_dict_ = \\\n                #     sess.run([train_op, global_step, total_loss_dict])\n                #\n                # end = time.time()\n                #\n                # print(\'**\' * 20)\n                # print(""""""%s: global_step%d  current_step%d""""""\n                #       % (training_time, global_stepnp * num_gpu, step * num_gpu))\n                # print(""""""per_cost_time:%.3fs""""""\n                #       % ((end - start) / num_gpu))\n                # loss_str = \'\'\n                # for k in total_loss_dict_.keys():\n                #     loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                # print(loss_str)\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'coco_\' + str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/test.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\ndef detect(det_net, inference_save_path, real_test_imgname_list):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0)  # [1, None, None, 3]\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        for i, a_img_name in enumerate(real_test_imgname_list):\n\n            raw_img = cv2.imread(a_img_name)\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}\n                )\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            detected_boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\n\n            show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n            show_scores = detected_scores[show_indices]\n            show_boxes = detected_boxes[show_indices]\n            show_categories = detected_categories[show_indices]\n\n            # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            #     raw_img = (raw_img / 255 - np.array(cfgs.PIXEL_MEAN_)) /np.array(cfgs.PIXEL_STD)\n            # else:\n            #     raw_img = raw_img - np.array(cfgs.PIXEL_MEAN)\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(raw_img,\n                                                                                boxes=show_boxes,\n                                                                                labels=show_categories,\n                                                                                scores=show_scores,\n                                                                                in_graph=False)\n            nake_name = a_img_name.split(\'/\')[-1]\n            # print (inference_save_path + \'/\' + nake_name)\n            cv2.imwrite(inference_save_path + \'/\' + nake_name,\n                        final_detections[:, :, ::-1])\n\n            tools.view_bar(\'{} image cost {}s\'.format(a_img_name, (end - start)), i + 1, len(real_test_imgname_list))\n\n\ndef test(test_dir, inference_save_path):\n\n    test_imgname_list = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)\n                                                          if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    detect(det_net=faster_rcnn, inference_save_path=inference_save_path, real_test_imgname_list=test_imgname_list)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'TestImgs...U need provide the test dir\')\n    parser.add_argument(\'--data_dir\', dest=\'data_dir\',\n                        help=\'data path\',\n                        default=\'demos\', type=str)\n    parser.add_argument(\'--save_dir\', dest=\'save_dir\',\n                        help=\'demo imgs to save\',\n                        default=\'inference_results\', type=str)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id \',\n                        default=\'0\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    test(args.data_dir,\n         inference_save_path=args.save_dir)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_coco.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport json\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\n\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nfrom data.lib_coco.PythonAPI.pycocotools.cocoeval import COCOeval\n\n\ndef test_coco(det_net, real_test_img_list, eval_data, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n        tools.mkdir(save_path)\n        fw_json_dt = open(os.path.join(save_path, \'coco_test-dev.json\'), \'w\')\n        coco_det = []\n        for i, a_img in enumerate(real_test_img_list):\n\n            raw_img = cv2.imread(os.path.join(eval_data, a_img[\'file_name\']))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n            end = time.time()\n\n            eval_indices = detected_scores >= 0.01\n            detected_scores = detected_scores[eval_indices]\n            detected_boxes = detected_boxes[eval_indices]\n            detected_categories = detected_categories[eval_indices]\n\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n                if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                    draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                else:\n                    draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    in_graph=False)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + \'{}.jpg\'.format(a_img[\'id\']),\n                            final_detections[:, :, ::-1])\n\n            xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                                     detected_boxes[:, 2], detected_boxes[:, 3]\n\n            resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n            xmin = xmin * raw_w / resized_w\n            xmax = xmax * raw_w / resized_w\n\n            ymin = ymin * raw_h / resized_h\n            ymax = ymax * raw_h / resized_h\n\n            boxes = np.transpose(np.stack([xmin, ymin, xmax-xmin, ymax-ymin]))\n\n            # cost much time\n            for j, box in enumerate(boxes):\n                coco_det.append({\'bbox\': [float(box[0]), float(box[1]), float(box[2]), float(box[3])],\n                                 \'score\': float(detected_scores[j]), \'image_id\': a_img[\'id\'],\n                                 \'category_id\': int(classes_originID[LABEl_NAME_MAP[detected_categories[j]]])})\n\n            tools.view_bar(\'%s image cost %.3fs\' % (a_img[\'id\'], (end - start)), i + 1, len(real_test_img_list))\n\n        json.dump(coco_det, fw_json_dt)\n        fw_json_dt.close()\n\n\ndef eval(num_imgs, eval_data, json_file, showbox):\n\n    with open(json_file) as f:\n        test_img_list = json.load(f)[\'images\']\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_img_list\n    else:\n        real_test_img_list = test_img_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    test_coco(det_net=faster_rcnn, real_test_img_list=real_test_img_list, eval_data=eval_data, draw_imgs=showbox)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_data\', dest=\'eval_data\',\n                        help=\'evaluate imgs dir \',\n                        default=\'coco_minival2014.odgt\', type=str)\n    parser.add_argument(\'--json_file\', dest=\'json_file\',\n                        help=\'test-dev json file\',\n                        default=\'image_info_test-dev2017.json\', type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=False, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_data=args.eval_data,\n         json_file=args.json_file,\n         showbox=args.showbox)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_coco_pyramid.py,11,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport json\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval\nfrom libs.box_utils import draw_box_in_img\nimport argparse\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import nms\n\nfrom data.lib_coco.PythonAPI.pycocotools.coco import COCO\nfrom data.lib_coco.PythonAPI.pycocotools.cocoeval import COCOeval\n\n\ndef cocoval(detected_json, eval_json):\n    eval_gt = COCO(eval_json)\n\n    eval_dt = eval_gt.loadRes(detected_json)\n    cocoEval = COCOeval(eval_gt, eval_dt, iouType=\'bbox\')\n\n    # cocoEval.params.imgIds = eval_gt.getImgIds()\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndef test_coco(det_net, real_test_img_list, eval_data, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH,\n                                                     is_resize=False)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    # img_batch = (img_batch - tf.constant(cfgs.PIXEL_MEAN)) / (tf.constant(cfgs.PIXEL_STD)*255)\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        save_path = os.path.join(\'./eval_coco\', cfgs.VERSION)\n        tools.mkdir(save_path)\n        fw_json_dt = open(os.path.join(save_path, \'coco_test-dev_ms.json\'), \'w\')\n        coco_det = []\n        for i, a_img in enumerate(real_test_img_list):\n\n            raw_img = cv2.imread(os.path.join(eval_data, a_img[\'file_name\']))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n\n            detected_scores_, detected_boxes_, detected_categories_ = [], [], []\n\n            for ss in [600, 800, 1000, 1200]:  # cfgs.IMG_SHORT_SIDE_LEN:\n                img_resize = cv2.resize(raw_img, (ss, ss))\n\n                resized_img, tmp_detected_boxes, tmp_detected_scores, tmp_detected_categories = \\\n                    sess.run(\n                        [img_batch, detection_boxes, detection_scores, detection_category],\n                        feed_dict={img_plac: img_resize[:, :, ::-1]}  # cv is BGR. But need RGB\n                    )\n\n                eval_indices = tmp_detected_scores >= 0.01\n                tmp_detected_scores = tmp_detected_scores[eval_indices]\n                tmp_detected_boxes = tmp_detected_boxes[eval_indices]\n                tmp_detected_categories = tmp_detected_categories[eval_indices]\n\n                xmin, ymin, xmax, ymax = tmp_detected_boxes[:, 0], tmp_detected_boxes[:, 1], \\\n                                         tmp_detected_boxes[:, 2], tmp_detected_boxes[:, 3]\n\n                resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n\n                xmin = xmin * raw_w / resized_w\n                xmax = xmax * raw_w / resized_w\n\n                ymin = ymin * raw_h / resized_h\n                ymax = ymax * raw_h / resized_h\n\n                resize_boxes = np.transpose(np.stack([xmin, ymin, xmax, ymax]))\n\n                detected_scores_.append(tmp_detected_scores)\n                detected_boxes_.append(resize_boxes)\n                detected_categories_.append(tmp_detected_categories)\n\n            detected_scores_ = np.concatenate(detected_scores_)\n            detected_boxes_ = np.concatenate(detected_boxes_)\n            detected_categories_ = np.concatenate(detected_categories_)\n\n            detected_scores, detected_boxes, detected_categories = [], [], []\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(detected_categories_ == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_h = detected_boxes_[index]\n                tmp_label_h = detected_categories_[index]\n                tmp_score_h = detected_scores_[index]\n\n                tmp_boxes_h = np.array(tmp_boxes_h)\n                tmp = np.zeros([tmp_boxes_h.shape[0], tmp_boxes_h.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_h\n                tmp[:, -1] = np.array(tmp_score_h)\n\n                inx = nms.py_cpu_nms(dets=np.array(tmp, np.float32),\n                                     thresh=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                     max_output_size=120)\n\n                detected_boxes.extend(np.array(tmp_boxes_h)[inx])\n                detected_scores.extend(np.array(tmp_score_h)[inx])\n                detected_categories.extend(np.array(tmp_label_h)[inx])\n\n            detected_scores = np.array(detected_scores)\n            detected_boxes = np.array(detected_boxes)\n            detected_categories = np.array(detected_categories)\n\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.SHOW_SCORE_THRSHOLD\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                # if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n                #     draw_img = (raw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                # else:\n                #     draw_img = raw_img + np.array(cfgs.PIXEL_MEAN)\n\n                # draw_img = draw_img * (np.array(cfgs.PIXEL_STD)*255) + np.array(cfgs.PIXEL_MEAN)\n\n                raw_img = np.array(raw_img, np.float32)\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(raw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    in_graph=False)\n                if not os.path.exists(cfgs.TEST_SAVE_PATH):\n                    os.makedirs(cfgs.TEST_SAVE_PATH)\n\n                cv2.imwrite(cfgs.TEST_SAVE_PATH + \'/\' + \'{}.jpg\'.format(a_img[\'id\']),\n                            final_detections[:, :, ::-1])\n\n            # cost much time\n            for j, box in enumerate(detected_boxes):\n                coco_det.append({\'bbox\': [float(box[0]), float(box[1]), float(box[2]-box[0]), float(box[3]-box[1])],\n                                 \'score\': float(detected_scores[j]), \'image_id\': a_img[\'id\'],\n                                 \'category_id\': int(classes_originID[LABEl_NAME_MAP[detected_categories[j]]])})\n            end = time.time()\n            tools.view_bar(\'%s image cost %.3fs\' % (a_img[\'id\'], (end - start)), i + 1, len(real_test_img_list))\n\n        json.dump(coco_det, fw_json_dt)\n        fw_json_dt.close()\n\n\ndef eval(num_imgs, eval_data, json_file, showbox):\n\n    with open(json_file) as f:\n        test_img_list = json.load(f)[\'images\']\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_img_list\n    else:\n        real_test_img_list = test_img_list[: num_imgs]\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    test_coco(det_net=faster_rcnn, real_test_img_list=real_test_img_list, eval_data=eval_data, draw_imgs=showbox)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 stdand\')\n\n    parser.add_argument(\'--eval_data\', dest=\'eval_data\',\n                        help=\'evaluate imgs dir \',\n                        default=\'coco_minival2014.odgt\', type=str)\n    parser.add_argument(\'--json_file\', dest=\'json_file\',\n                        help=\'test-dev json file\',\n                        default=\'image_info_test-dev2017.json\', type=str)\n    parser.add_argument(\'--showbox\', dest=\'showbox\',\n                        help=\'whether show detecion results when evaluation\',\n                        default=False, type=bool)\n    parser.add_argument(\'--GPU\', dest=\'GPU\',\n                        help=\'gpu id\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.GPU\n    eval(np.inf,  # use np.inf to test all the imgs. use 10 to test 10 imgs.\n         eval_data=args.eval_data,\n         json_file=args.json_file,\n         showbox=args.showbox)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_pyramid_dota.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\n\nsys.path.append(""../"")\nimport cv2\nimport numpy as np\nfrom timeit import default_timer as timer\nimport tensorflow as tf\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network\nfrom help_utils.tools import *\nfrom libs.box_utils import draw_box_in_img\n# from libs.box_utils import coordinate_convert\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP, NAME_LABEL_MAP\nfrom help_utils import tools\n# from libs.box_utils import nms\nfrom libs.box_utils.cython_utils.cython_nms import nms\nfrom libs.configs import cfgs\n\n\ndef get_file_paths_recursive(folder=None, file_ext=None):\n    """""" Get the absolute path of all files in given folder recursively\n    :param folder:\n    :param file_ext:\n    :return:\n    """"""\n    file_list = []\n    if folder is None:\n        return file_list\n\n    # for dir_path, dir_names, file_names in os.walk(folder):\n    #     for file_name in file_names:\n    #         if file_ext is None:\n    #             file_list.append(os.path.join(dir_path, file_name))\n    #             continue\n    #         if file_name.endswith(file_ext):\n    #             file_list.append(os.path.join(dir_path, file_name))\n    file_list = [os.path.join(folder, f) for f in sorted(os.listdir(folder)) if f.endswith(file_ext)]\n\n    return file_list\n\n\ndef inference(det_net, file_paths, des_folder, h_len, w_len, h_overlap, w_overlap, save_res=False):\n\n    if save_res:\n        assert cfgs.SHOW_SCORE_THRSHOLD >= 0.5, \\\n            \'please set score threshold (example: SHOW_SCORE_THRSHOLD = 0.5) in cfgs.py\'\n\n    else:\n        assert cfgs.SHOW_SCORE_THRSHOLD < 0.005, \\\n            \'please set score threshold (example: SHOW_SCORE_THRSHOLD = 0.00) in cfgs.py\'\n\n    tmp_file = \'./tmp_%s.txt\' % cfgs.VERSION\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\n    img_batch = tf.cast(img_plac, tf.float32)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN[0],\n                                                     is_resize=False)\n\n    det_boxes_h, det_scores_h, det_category_h = det_net.build_whole_detection_network(input_img_batch=img_batch,\n                                                                                      gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        if not os.path.exists(tmp_file):\n            fw = open(tmp_file, \'w\')\n            fw.close()\n\n        fr = open(tmp_file, \'r\')\n        pass_img = fr.readlines()\n        fr.close()\n\n        for count, img_path in enumerate(file_paths):\n            fw = open(tmp_file, \'a+\')\n            if img_path + \'\\n\' in pass_img:\n                continue\n            start = timer()\n            img = cv2.imread(img_path)\n\n            box_res = []\n            label_res = []\n            score_res = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n            ori_H = imgH\n            ori_W = imgW\n            # print(""  ori_h, ori_w: "", imgH, imgW)\n            if imgH < h_len:\n                temp = np.zeros([h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = h_len\n\n            if imgW < w_len:\n                temp = np.zeros([imgH, w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = w_len\n\n            for hh in range(0, imgH, h_len - h_overlap):\n                if imgH - hh - 1 < h_len:\n                    hh_ = imgH - h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, w_len - w_overlap):\n                    if imgW - ww - 1 < w_len:\n                        ww_ = imgW - w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + h_len), ww_:(ww_ + w_len), :]\n\n                    for short_size in cfgs.IMG_SHORT_SIDE_LEN:\n                        max_len = 1200\n\n                        if h_len < w_len:\n                            new_h, new_w = short_size,  min(int(short_size*float(w_len)/h_len), max_len)\n                        else:\n                            new_h, new_w = min(int(short_size*float(h_len)/w_len), max_len), short_size\n\n                        img_resize = cv2.resize(src_img, (new_h, new_w))\n\n                        det_boxes_h_, det_scores_h_, det_category_h_ = \\\n                            sess.run(\n                                [det_boxes_h, det_scores_h, det_category_h],\n                                feed_dict={img_plac: img_resize[:, :, ::-1]}\n                            )\n\n                        valid = det_scores_h_ > 1e-4\n                        det_boxes_h_ = det_boxes_h_[valid]\n                        det_scores_h_ = det_scores_h_[valid]\n                        det_category_h_ = det_category_h_[valid]\n                        det_boxes_h_[:, 0] = det_boxes_h_[:, 0] * w_len / new_w\n                        det_boxes_h_[:, 1] = det_boxes_h_[:, 1] * h_len / new_h\n                        det_boxes_h_[:, 2] = det_boxes_h_[:, 2] * w_len / new_w\n                        det_boxes_h_[:, 3] = det_boxes_h_[:, 3] * h_len / new_h\n\n                        if len(det_boxes_h_) > 0:\n                            for ii in range(len(det_boxes_h_)):\n                                box = det_boxes_h_[ii]\n                                box[0] = box[0] + ww_\n                                box[1] = box[1] + hh_\n                                box[2] = box[2] + ww_\n                                box[3] = box[3] + hh_\n                                box_res.append(box)\n                                label_res.append(det_category_h_[ii])\n                                score_res.append(det_scores_h_[ii])\n\n            box_res = np.array(box_res)\n            label_res = np.array(label_res)\n            score_res = np.array(score_res)\n\n            box_res_, label_res_, score_res_ = [], [], []\n\n            # h_threshold = {\'roundabout\': 0.35, \'tennis-court\': 0.35, \'swimming-pool\': 0.4, \'storage-tank\': 0.3,\n            #                \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.4, \'ship\': 0.35, \'plane\': 0.35,\n            #                \'large-vehicle\': 0.4, \'helicopter\': 0.4, \'harbor\': 0.3, \'ground-track-field\': 0.4,\n            #                \'bridge\': 0.3, \'basketball-court\': 0.4, \'baseball-diamond\': 0.3}\n            h_threshold = {\'turntable\': 0.5, \'tennis-court\': 0.5, \'swimming-pool\': 0.5, \'storage-tank\': 0.5,\n                           \'soccer-ball-field\': 0.5, \'small-vehicle\': 0.5, \'ship\': 0.5, \'plane\': 0.5,\n                           \'large-vehicle\': 0.5, \'helicopter\': 0.5, \'harbor\': 0.5, \'ground-track-field\': 0.5,\n                           \'bridge\': 0.5, \'basketball-court\': 0.5, \'baseball-diamond\': 0.5, \'container-crane\': 0.5}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_h = box_res[index]\n                tmp_label_h = label_res[index]\n                tmp_score_h = score_res[index]\n\n                tmp_boxes_h = np.array(tmp_boxes_h)\n                tmp = np.zeros([tmp_boxes_h.shape[0], tmp_boxes_h.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_h\n                tmp[:, -1] = np.array(tmp_score_h)\n\n                # inx = nms.py_cpu_nms(dets=np.array(tmp, np.float32),\n                #                      thresh=h_threshold[LABEL_NAME_MAP[sub_class]],\n                #                      max_output_size=500)\n\n                inx = nms(np.array(tmp, np.float32),\n                          h_threshold[LABEl_NAME_MAP[sub_class]])\n\n                inx = inx[:500]  # max_outpus is 500\n\n                box_res_.extend(np.array(tmp_boxes_h)[inx])\n                score_res_.extend(np.array(tmp_score_h)[inx])\n                label_res_.extend(np.array(tmp_label_h)[inx])\n\n            time_elapsed = timer() - start\n\n            if save_res:\n                scores = np.array(score_res_)\n                labels = np.array(label_res_)\n                boxes = np.array(box_res_)\n                valid_show = scores > cfgs.SHOW_SCORE_THRSHOLD\n                scores = scores[valid_show]\n                boxes = boxes[valid_show]\n                labels = labels[valid_show]\n                det_detections_h = draw_box_in_img.draw_boxes_with_label_and_scores(np.array(img, np.float32),\n                                                                                    boxes=boxes,\n                                                                                    labels=labels,\n                                                                                    scores=scores,\n                                                                                    in_graph=False)\n                det_detections_h = det_detections_h[:ori_H, :ori_W]\n                save_dir = os.path.join(des_folder, cfgs.VERSION)\n                tools.mkdir(save_dir)\n                cv2.imwrite(save_dir + \'/\' + img_path.split(\'/\')[-1].split(\'.\')[0] + \'_h_s%d_t%f.jpg\' %(h_len, cfgs.FAST_RCNN_NMS_IOU_THRESHOLD),\n                            det_detections_h)\n\n                view_bar(\'{} cost {}s\'.format(img_path.split(\'/\')[-1].split(\'.\')[0],\n                                              time_elapsed), count + 1, len(file_paths))\n\n            else:\n                # eval txt\n                CLASS_DOTA = NAME_LABEL_MAP.keys()\n\n                # Task2\n                write_handle_h = {}\n                txt_dir_h = os.path.join(\'txt_output\', cfgs.VERSION + \'_h\')\n                tools.mkdir(txt_dir_h)\n                for sub_class in CLASS_DOTA:\n                    if sub_class == \'back_ground\':\n                        continue\n                    write_handle_h[sub_class] = open(os.path.join(txt_dir_h, \'Task2_%s.txt\' % sub_class), \'a+\')\n\n                for i, hbox in enumerate(box_res_):\n                    command = \'%s %.3f %.1f %.1f %.1f %.1f\\n\' % (img_path.split(\'/\')[-1].split(\'.\')[0],\n                                                                 score_res_[i],\n                                                                 hbox[0], hbox[1], hbox[2], hbox[3])\n                    write_handle_h[LABEl_NAME_MAP[label_res_[i]]].write(command)\n\n                for sub_class in CLASS_DOTA:\n                    if sub_class == \'back_ground\':\n                        continue\n                    write_handle_h[sub_class].close()\n\n            view_bar(\'{} cost {}s\'.format(img_path.split(\'/\')[-1].split(\'.\')[0],\n                                          time_elapsed), count + 1, len(file_paths))\n            fw.write(\'{}\\n\'.format(img_path))\n            fw.close()\n        os.remove(tmp_file)\n\n\nif __name__ == ""__main__"":\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n\n    file_paths = get_file_paths_recursive(\'/data/DOTA/test/images\', \'.png\')\n\n    det_net = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                   is_training=False)\n    inference(det_net, file_paths, \'./doai2019\', 800, 800,\n              200, 200, save_res=False)\n\n'"
tools/train.py,36,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\n# from libs.networks import build_whole_network2\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord import next_batch\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef warmup_and_cosine_lr(init_lr, global_step, warmup_step, decay_steps, alpha=1e-6):\n    def warmup_lr(init_lr, global_step, warmup_step):\n        global_step = tf.cast(global_step, tf.float32)\n        return 1e-6 + (init_lr - 1e-6) * global_step / warmup_step\n\n    def cosine_lr(init_lr, global_step, decay_steps, alpha=0.0):\n        return tf.train.cosine_decay(learning_rate=init_lr,\n                                     global_step=global_step - warmup_step,\n                                     decay_steps=decay_steps - warmup_step,\n                                     alpha=alpha)\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup_lr(init_lr, global_step, warmup_step),\n                   false_fn=lambda: cosine_lr(init_lr, global_step, decay_steps, alpha))\n\n\ndef train():\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=True)\n\n    with tf.name_scope(\'get_batch\'):\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                       is_training=True)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n        if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            img_batch = img_batch / tf.constant([cfgs.PIXEL_STD])\n\n    biases_regularizer = tf.no_regularizer\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                         slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                        weights_regularizer=weights_regularizer,\n                        biases_regularizer=biases_regularizer,\n                        biases_initializer=tf.constant_initializer(0.0)):\n        final_bbox, final_scores, final_category, loss_dict = faster_rcnn.build_whole_detection_network(\n            input_img_batch=img_batch,\n            gtboxes_batch=gtboxes_and_label)\n\n    # ----------------------------------------------------------------------------------------------------build loss\n    weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n    rpn_location_loss = loss_dict[\'rpn_loc_loss\']\n    rpn_cls_loss = loss_dict[\'rpn_cls_loss\']\n    rpn_total_loss = rpn_location_loss + rpn_cls_loss\n\n    fastrcnn_cls_loss = loss_dict[\'fastrcnn_cls_loss\']\n    fastrcnn_loc_loss = loss_dict[\'fastrcnn_loc_loss\']\n    fastrcnn_total_loss = fastrcnn_cls_loss + fastrcnn_loc_loss\n\n    if cfgs.USE_ATTENTION:\n        mask_total_loss = loss_dict[\'mask_loss\']\n        total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss + mask_total_loss\n    else:\n        total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss\n\n    # ---------------------------------------------------------------------------------------------------add summary\n\n    tf.summary.scalar(\'RPN_LOSS/cls_loss\', rpn_cls_loss)\n    tf.summary.scalar(\'RPN_LOSS/location_loss\', rpn_location_loss)\n    tf.summary.scalar(\'RPN_LOSS/rpn_total_loss\', rpn_total_loss)\n\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_cls_loss\', fastrcnn_cls_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_location_loss\', fastrcnn_loc_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_total_loss\', fastrcnn_total_loss)\n\n    tf.summary.scalar(\'LOSS/total_loss\', total_loss)\n    tf.summary.scalar(\'LOSS/regular_weights\', weight_decay_loss)\n    if cfgs.USE_ATTENTION:\n        tf.summary.scalar(\'LOSS/mask_loss\', mask_total_loss)\n\n    gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img_batch,\n                                                                   boxes=gtboxes_and_label[:, :-1],\n                                                                   labels=gtboxes_and_label[:, -1])\n    if cfgs.ADD_BOX_IN_TENSORBOARD:\n        detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(img_batch=img_batch,\n                                                                                     boxes=final_bbox,\n                                                                                     labels=final_category,\n                                                                                     scores=final_scores)\n        tf.summary.image(\'Compare/final_detection\', detections_in_img)\n    tf.summary.image(\'Compare/gtboxes\', gtboxes_in_img)\n\n    # ___________________________________________________________________________________________________add summary\n\n    global_step = slim.get_or_create_global_step()\n    lr = tf.train.piecewise_constant(global_step,\n                                     boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1]), np.int64(cfgs.DECAY_STEP[2])],\n                                     values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100., cfgs.LR / 1000.])\n    tf.summary.scalar(\'lr\', lr)\n    optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n    # optimizer = tf.train.AdamOptimizer(lr)\n\n    # ---------------------------------------------------------------------------------------------compute gradients\n    gradients = faster_rcnn.get_gradients(optimizer, total_loss)\n\n    # enlarge_gradients for bias\n    if cfgs.MUTILPY_BIAS_GRADIENT:\n        gradients = faster_rcnn.enlarge_gradients_for_bias(gradients)\n\n    if cfgs.GRADIENT_CLIPPING_BY_NORM:\n        with tf.name_scope(\'clip_gradients\'):\n            gradients = slim.learning.clip_gradient_norms(gradients,\n                                                          cfgs.GRADIENT_CLIPPING_BY_NORM)\n\n    # train_op\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients,\n                                         global_step=global_step)\n    summary_op = tf.summary.merge_all()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = faster_rcnn.get_restorer()\n    saver = tf.train.Saver(max_to_keep=10)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n        tools.mkdir(summary_path)\n        summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n        for step in range(cfgs.MAX_ITERATION):\n            training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n            if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                _, global_stepnp = sess.run([train_op, global_step])\n\n            else:\n                if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                    start = time.time()\n\n                    if cfgs.USE_ATTENTION:\n                        _, global_stepnp, img_name, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                        fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, maskLoss, totalLoss = \\\n                            sess.run(\n                                [train_op, global_step, img_name_batch, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                                 fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, mask_total_loss, total_loss])\n                    else:\n\n                        _, global_stepnp, img_name, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                        fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss = \\\n                            sess.run(\n                                [train_op, global_step, img_name_batch, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                                 fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, total_loss])\n\n                    end = time.time()\n                    if cfgs.USE_ATTENTION:\n                        print("""""" %s: step%d    image_name:%s |\\t\n                                  rpn_loc_loss:%.3f |\\t rpn_cla_loss:%.3f |\\t rpn_total_loss:%.3f |\n                                  fast_rcnn_loc_loss:%.3f |\\t fast_rcnn_cla_loss:%.3f |\\t fast_rcnn_total_loss:%.3f |\n                                  mask_loss:%.3f |\\t total_loss:%.3f |\\t per_cost_time:%.3fs""""""\n                              % (training_time, global_stepnp, str(img_name[0]), rpnLocLoss, rpnClsLoss,\n                                      rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, maskLoss,\n                                      totalLoss, (end - start)))\n                    else:\n\n                        print("""""" %s: step%d    image_name:%s |\\t\n                                  rpn_loc_loss:%.3f |\\t rpn_cla_loss:%.3f |\\t rpn_total_loss:%.3f |\n                                  fast_rcnn_loc_loss:%.3f |\\t fast_rcnn_cla_loss:%.3f |\\t fast_rcnn_total_loss:%.3f |\n                                  total_loss:%.3f |\\t per_cost_time:%.3fs""""""\n                              % (training_time, global_stepnp, str(img_name[0]), rpnLocLoss, rpnClsLoss,\n                                      rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss,\n                                      (end - start)))\n                else:\n                    if step % cfgs.SMRY_ITER == 0:\n                        _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                        summary_writer.add_summary(summary_str, global_stepnp)\n                        summary_writer.flush()\n\n            if (step > 0 and step % cfgs.SAVE_WEIGHTS_INTE == 0) or (step == cfgs.MAX_ITERATION - 1):\n\n                save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n\n                save_ckpt = os.path.join(save_dir, \'voc_\' + str(global_stepnp) + \'model.ckpt\')\n                saver.save(sess, save_ckpt)\n                print(\' weights had been saved\')\n\n        coord.request_stop()\n        coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/train_for_coco.py,38,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nsys.path.append(""../"")\n# sys.path.append(""../data/lib_coco"")\n# sys.path.append(\'../data/lib_coco/PythonAPI/\')\n\nimport numpy as np\nimport time\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom data.io import image_preprocess\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\nfrom data.io.COCO.get_coco_next_batch import next_img\n# from data.lib_coco.get_coco_next_batch import next_img\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef preprocess_img(img_plac, gtbox_plac):\n    \'\'\'\n\n    :param img_plac: [H, W, 3] uint 8 img. In RGB.\n    :param gtbox_plac: shape of [-1, 5]. [xmin, ymin, xmax, ymax, label]\n    :return:\n    \'\'\'\n\n    img = tf.cast(img_plac, tf.float32)\n\n    # gtboxes_and_label = tf.cast(gtbox_plac, tf.float32)\n    img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img,\n                                                                gtboxes_and_label=gtbox_plac,\n                                                                target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                                length_limitation=cfgs.IMG_MAX_LENGTH)\n    img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                     gtboxes_and_label=gtboxes_and_label)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255 - tf.constant([[cfgs.PIXEL_MEAN_]])\n    else:\n        img = img - tf.constant([[cfgs.PIXEL_MEAN]])\n    img_batch = tf.expand_dims(img, axis=0)\n\n    # gtboxes_and_label = tf.Print(gtboxes_and_label, [tf.shape(gtboxes_and_label)], message=\'gtbox shape\')\n    return img_batch, gtboxes_and_label\n\n\ndef train():\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=True)\n\n    with tf.name_scope(\'get_batch\'):\n        img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\n        gtbox_plac = tf.placeholder(dtype=tf.int32, shape=[None, 5])\n\n        img_batch, gtboxes_and_label = preprocess_img(img_plac, gtbox_plac)\n        if cfgs.NET_NAME in [\'resnet101_v1d\']:\n            img_batch = img_batch / tf.constant([cfgs.PIXEL_STD])\n        # gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n    biases_regularizer = tf.no_regularizer\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                         slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                        weights_regularizer=weights_regularizer,\n                        biases_regularizer=biases_regularizer,\n                        biases_initializer=tf.constant_initializer(0.0)):\n        final_bbox, final_scores, final_category, loss_dict = faster_rcnn.build_whole_detection_network(\n            input_img_batch=img_batch,\n            gtboxes_batch=gtboxes_and_label)\n\n    # ----------------------------------------------------------------------------------------------------build loss\n    weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n    rpn_location_loss = loss_dict[\'rpn_loc_loss\']\n    rpn_cls_loss = loss_dict[\'rpn_cls_loss\']\n    rpn_total_loss = rpn_location_loss + rpn_cls_loss\n\n    fastrcnn_cls_loss = loss_dict[\'fastrcnn_cls_loss\']\n    fastrcnn_loc_loss = loss_dict[\'fastrcnn_loc_loss\']\n    fastrcnn_total_loss = fastrcnn_cls_loss + fastrcnn_loc_loss\n\n    if cfgs.USE_ATTENTION:\n        mask_total_loss = loss_dict[\'mask_loss\']\n        total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss + mask_total_loss\n    else:\n        total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss\n\n    # ---------------------------------------------------------------------------------------------------add summary\n    tf.summary.scalar(\'RPN_LOSS/cls_loss\', rpn_cls_loss)\n    tf.summary.scalar(\'RPN_LOSS/location_loss\', rpn_location_loss)\n    tf.summary.scalar(\'RPN_LOSS/rpn_total_loss\', rpn_total_loss)\n\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_cls_loss\', fastrcnn_cls_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_location_loss\', fastrcnn_loc_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_total_loss\', fastrcnn_total_loss)\n\n    tf.summary.scalar(\'LOSS/total_loss\', total_loss)\n    tf.summary.scalar(\'LOSS/regular_weights\', weight_decay_loss)\n    if cfgs.USE_ATTENTION:\n        tf.summary.scalar(\'LOSS/mask_loss\', mask_total_loss)\n\n    gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img_batch,\n                                                                   boxes=gtboxes_and_label[:, :-1],\n                                                                   labels=gtboxes_and_label[:, -1])\n    if cfgs.ADD_BOX_IN_TENSORBOARD:\n        detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(img_batch=img_batch,\n                                                                                     boxes=final_bbox,\n                                                                                     labels=final_category,\n                                                                                     scores=final_scores)\n        tf.summary.image(\'Compare/final_detection\', detections_in_img)\n    tf.summary.image(\'Compare/gtboxes\', gtboxes_in_img)\n\n    # ___________________________________________________________________________________________________add summary\n\n    global_step = slim.get_or_create_global_step()\n    lr = tf.train.piecewise_constant(global_step,\n                                     boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1])],\n                                     values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100.])\n    tf.summary.scalar(\'lr\', lr)\n    optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n\n    # ---------------------------------------------------------------------------------------------compute gradients\n    gradients = faster_rcnn.get_gradients(optimizer, total_loss)\n\n    # enlarge_gradients for bias\n    if cfgs.MUTILPY_BIAS_GRADIENT:\n        gradients = faster_rcnn.enlarge_gradients_for_bias(gradients)\n\n    if cfgs.GRADIENT_CLIPPING_BY_NORM:\n        with tf.name_scope(\'clip_gradients\'):\n            gradients = slim.learning.clip_gradient_norms(gradients,\n                                                          cfgs.GRADIENT_CLIPPING_BY_NORM)\n\n    # train_op\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients,\n                                         global_step=global_step)\n    summary_op = tf.summary.merge_all()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = faster_rcnn.get_restorer()\n    saver = tf.train.Saver(max_to_keep=5)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n        tools.mkdir(summary_path)\n        summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n        for step in range(cfgs.MAX_ITERATION):\n\n            img_id, img, gt_info = next_img(step=step)\n\n            training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n            if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                _, global_stepnp = sess.run([train_op, global_step],\n                                            feed_dict={img_plac: img,\n                                                       gtbox_plac: gt_info}\n                                            )\n\n            else:\n                if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                    start = time.time()\n\n                    if cfgs.USE_ATTENTION:\n                        _, global_stepnp, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                        fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, maskLoss, totalLoss = \\\n                            sess.run(\n                                [train_op, global_step, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                                 fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, mask_total_loss, total_loss],\n                                feed_dict={img_plac: img,\n                                           gtbox_plac: gt_info}\n                            )\n                    else:\n\n                        _, global_stepnp, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                        fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss = \\\n                            sess.run(\n                                [train_op, global_step, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                                 fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, total_loss],\n                                feed_dict={img_plac: img,\n                                           gtbox_plac: gt_info}\n                            )\n\n                    end = time.time()\n                    if cfgs.USE_ATTENTION:\n                        print("""""" {}: step{}    image_name:{} |\\t\n                                  rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t rpn_total_loss:{} |\n                                  fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t fast_rcnn_total_loss:{} |\n                                  mask_loss:{} |\\t total_loss:{} |\\t per_cost_time:{}s"""""" \\\n                              .format(training_time, global_stepnp, str(img_id), rpnLocLoss, rpnClsLoss,\n                                      rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, maskLoss,\n                                      totalLoss, (end - start)))\n                    else:\n\n                        print("""""" {}: step{}    image_name:{} |\\t\n                                  rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t rpn_total_loss:{} |\n                                  fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t fast_rcnn_total_loss:{} |\n                                  total_loss:{} |\\t per_cost_time:{}s"""""" \\\n                              .format(training_time, global_stepnp, str(img_id), rpnLocLoss, rpnClsLoss,\n                                      rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss,\n                                      (end - start)))\n\n                else:\n                    if step % cfgs.SMRY_ITER == 0:\n                        _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op],\n                                                                 feed_dict={img_plac: img,\n                                                                            gtbox_plac: gt_info}\n                                                                 )\n                        summary_writer.add_summary(summary_str, global_stepnp)\n                        summary_writer.flush()\n\n            if (step > 0 and step % cfgs.SAVE_WEIGHTS_INTE == 0) or (step == cfgs.MAX_ITERATION - 1):\n\n                save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n\n                save_ckpt = os.path.join(save_dir, \'voc_\' + str(global_stepnp) + \'model.ckpt\')\n                saver.save(sess, save_ckpt)\n                print(\' weights had been saved\')\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/train_with_placeholder.py,37,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nsys.path.append(""../"")\n# sys.path.append(""../data/lib_coco"")\n# sys.path.append(\'../data/lib_coco/PythonAPI/\')\n\nimport numpy as np\nimport time\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom data.io import image_preprocess\nfrom libs.box_utils import show_box_in_tensor\nfrom help_utils import tools\nfrom data.io.BDD100K.get_bdd100k_next_batch import next_img\n# from data.lib_coco.get_coco_next_batch import next_img\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef preprocess_img(img_plac, gtbox_plac):\n    \'\'\'\n\n    :param img_plac: [H, W, 3] uint 8 img. In RGB.\n    :param gtbox_plac: shape of [-1, 5]. [xmin, ymin, xmax, ymax, label]\n    :return:\n    \'\'\'\n\n    img = tf.cast(img_plac, tf.float32)\n\n    # gtboxes_and_label = tf.cast(gtbox_plac, tf.float32)\n    img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img,\n                                                                gtboxes_and_label=gtbox_plac,\n                                                                target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                                length_limitation=cfgs.IMG_MAX_LENGTH)\n    img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                     gtboxes_and_label=gtboxes_and_label)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255 - tf.constant([[cfgs.PIXEL_MEAN_]])\n    else:\n        img = img - tf.constant([[cfgs.PIXEL_MEAN]])\n    img_batch = tf.expand_dims(img, axis=0)\n\n    # gtboxes_and_label = tf.Print(gtboxes_and_label, [tf.shape(gtboxes_and_label)], message=\'gtbox shape\')\n    return img_batch, gtboxes_and_label\n\n\ndef train():\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=True)\n\n    with tf.name_scope(\'get_batch\'):\n        img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\n        gtbox_plac = tf.placeholder(dtype=tf.int32, shape=[None, 5])\n\n        img_batch, gtboxes_and_label = preprocess_img(img_plac, gtbox_plac)\n        if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            img_batch = img_batch / tf.constant([cfgs.PIXEL_STD])\n        # gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n    biases_regularizer = tf.no_regularizer\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                         slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                        weights_regularizer=weights_regularizer,\n                        biases_regularizer=biases_regularizer,\n                        biases_initializer=tf.constant_initializer(0.0)):\n        final_bbox, final_scores, final_category, loss_dict = faster_rcnn.build_whole_detection_network(\n            input_img_batch=img_batch,\n            gtboxes_batch=gtboxes_and_label)\n\n    # ----------------------------------------------------------------------------------------------------build loss\n    weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n    rpn_location_loss = loss_dict[\'rpn_loc_loss\']\n    rpn_cls_loss = loss_dict[\'rpn_cls_loss\']\n    rpn_total_loss = rpn_location_loss + rpn_cls_loss\n\n    fastrcnn_cls_loss = loss_dict[\'fastrcnn_cls_loss\']\n    fastrcnn_loc_loss = loss_dict[\'fastrcnn_loc_loss\']\n    fastrcnn_total_loss = fastrcnn_cls_loss + fastrcnn_loc_loss\n\n    total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss\n\n    # ---------------------------------------------------------------------------------------------------add summary\n    tf.summary.scalar(\'RPN_LOSS/cls_loss\', rpn_cls_loss)\n    tf.summary.scalar(\'RPN_LOSS/location_loss\', rpn_location_loss)\n    tf.summary.scalar(\'RPN_LOSS/rpn_total_loss\', rpn_total_loss)\n\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_cls_loss\', fastrcnn_cls_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_location_loss\', fastrcnn_loc_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_total_loss\', fastrcnn_total_loss)\n\n    tf.summary.scalar(\'LOSS/total_loss\', total_loss)\n    tf.summary.scalar(\'LOSS/regular_weights\', weight_decay_loss)\n\n    gtboxes_in_img = show_box_in_tensor.draw_boxes_with_categories(img_batch=img_batch,\n                                                                   boxes=gtboxes_and_label[:, :-1],\n                                                                   labels=gtboxes_and_label[:, -1])\n    if cfgs.ADD_BOX_IN_TENSORBOARD:\n        detections_in_img = show_box_in_tensor.draw_boxes_with_categories_and_scores(img_batch=img_batch,\n                                                                                     boxes=final_bbox,\n                                                                                     labels=final_category,\n                                                                                     scores=final_scores)\n        tf.summary.image(\'Compare/final_detection\', detections_in_img)\n    tf.summary.image(\'Compare/gtboxes\', gtboxes_in_img)\n\n    # ___________________________________________________________________________________________________add summary\n\n    global_step = slim.get_or_create_global_step()\n    lr = tf.train.piecewise_constant(global_step,\n                                     boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1])],\n                                     values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100.])\n    tf.summary.scalar(\'lr\', lr)\n    optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n\n    # ---------------------------------------------------------------------------------------------compute gradients\n    gradients = faster_rcnn.get_gradients(optimizer, total_loss)\n\n    # enlarge_gradients for bias\n    if cfgs.MUTILPY_BIAS_GRADIENT:\n        gradients = faster_rcnn.enlarge_gradients_for_bias(gradients)\n\n    if cfgs.GRADIENT_CLIPPING_BY_NORM:\n        with tf.name_scope(\'clip_gradients_YJR\'):\n            gradients = slim.learning.clip_gradient_norms(gradients,\n                                                          cfgs.GRADIENT_CLIPPING_BY_NORM)\n\n    # train_op\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients,\n                                         global_step=global_step)\n    summary_op = tf.summary.merge_all()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = faster_rcnn.get_restorer()\n    saver = tf.train.Saver(max_to_keep=5)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n        tools.mkdir(summary_path)\n        summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n        for step in range(cfgs.MAX_ITERATION):\n\n            img_id, img, gt_info = next_img(step=step)\n            training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n            if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                _, global_stepnp = sess.run([train_op, global_step],\n                                            feed_dict={img_plac: img,\n                                                       gtbox_plac: gt_info}\n                                            )\n\n            else:\n                if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                    start = time.time()\n\n                    _, global_stepnp, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                    fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss = \\\n                        sess.run(\n                            [train_op, global_step, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                             fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, total_loss],\n                            feed_dict={img_plac: img, gtbox_plac: gt_info})\n                    end = time.time()\n                    print("""""" {}: step{}    image_name:{} |\\t\n                              rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t rpn_total_loss:{} |\n                              fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t fast_rcnn_total_loss:{} |\n                              total_loss:{} |\\t per_cost_time:{}s"""""" \\\n                          .format(training_time, global_stepnp, str(img_id), rpnLocLoss, rpnClsLoss,\n                                  rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss,\n                                  (end - start)))\n                else:\n                    if step % cfgs.SMRY_ITER == 0:\n                        _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op],\n                                                                 feed_dict={img_plac: img,\n                                                                            gtbox_plac: gt_info}\n                                                                 )\n                        summary_writer.add_summary(summary_str, global_stepnp)\n                        summary_writer.flush()\n\n            if (step > 0 and step % cfgs.SAVE_WEIGHTS_INTE == 0) or (step == cfgs.MAX_ITERATION - 1):\n\n                save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n\n                save_ckpt = os.path.join(save_dir, \'voc_\' + str(global_stepnp) + \'model.ckpt\')\n                saver.save(sess, save_ckpt)\n                print(\' weights had been saved\')\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
data/io/__init__.py,0,b''
data/io/convert_data_to_tfrecord.py,15,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport xml.etree.cElementTree as ET\nimport numpy as np\nimport tensorflow as tf\nimport glob\nimport cv2\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils.tools import *\n\ntf.app.flags.DEFINE_string(\'VOC_dir\', \'/data/DOTA/DOTA_TOTAL/\', \'Voc dir\')\ntf.app.flags.DEFINE_string(\'xml_dir\', \'xml\', \'xml dir\')\ntf.app.flags.DEFINE_string(\'image_dir\', \'img\', \'image dir\')\ntf.app.flags.DEFINE_string(\'save_name\', \'train\', \'save name\')\ntf.app.flags.DEFINE_string(\'save_dir\', \'../tfrecord/\', \'save name\')\ntf.app.flags.DEFINE_string(\'img_format\', \'.png\', \'format of image\')\ntf.app.flags.DEFINE_string(\'dataset\', \'DOAI2019\', \'dataset\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 5],\n           and has [xmin, ymin, xmax, ymax, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef convert_pascal_to_tfrecord():\n    xml_path = FLAGS.VOC_dir + FLAGS.xml_dir\n    image_path = FLAGS.VOC_dir + FLAGS.image_dir\n    save_path = FLAGS.save_dir + FLAGS.dataset + \'_\' + FLAGS.save_name + \'.tfrecord\'\n    mkdir(FLAGS.save_dir)\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=save_path, options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=save_path)\n    for count, xml in enumerate(glob.glob(xml_path + \'/*.xml\')):\n        # to avoid path error in different development platform\n        xml = xml.replace(\'\\\\\', \'/\')\n\n        img_name = xml.split(\'/\')[-1].split(\'.\')[0] + FLAGS.img_format\n        img_path = image_path + \'/\' + img_name\n\n        if not os.path.exists(img_path):\n            print(\'{} is not exist!\'.format(img_path))\n            continue\n\n        img_height, img_width, gtbox_label = read_xml_gtbox_and_label(xml)\n\n        # img = np.array(Image.open(img_path))\n        img = cv2.imread(img_path)[:, :, ::-1]\n\n        feature = tf.train.Features(feature={\n            # do not need encode() in linux\n            \'img_name\': _bytes_feature(img_name.encode()),\n            # \'img_name\': _bytes_feature(img_name),\n            \'img_height\': _int64_feature(img_height),\n            \'img_width\': _int64_feature(img_width),\n            \'img\': _bytes_feature(img.tostring()),\n            \'gtboxes_and_label\': _bytes_feature(gtbox_label.tostring()),\n            \'num_objects\': _int64_feature(gtbox_label.shape[0])\n        })\n\n        example = tf.train.Example(features=feature)\n\n        writer.write(example.SerializeToString())\n\n        view_bar(\'Conversion progress\', count + 1, len(glob.glob(xml_path + \'/*.xml\')))\n\n    print(\'\\nConversion is complete!\')\n\n\nif __name__ == \'__main__\':\n    # xml_path = \'../data/dataset/VOCdevkit/VOC2007/Annotations/000005.xml\'\n    # read_xml_gtbox_and_label(xml_path)\n\n    convert_pascal_to_tfrecord()\n'"
data/io/convert_data_to_tfrecord_coco.py,12,"b""# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append('../../')\nimport xml.etree.cElementTree as ET\nimport numpy as np\nimport tensorflow as tf\nimport glob\nimport cv2\nimport json\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils.tools import *\n\ntf.app.flags.DEFINE_string('coco_dir', '/data/COCO/coco_trainvalmini.odgt', 'coco dir')\ntf.app.flags.DEFINE_string('save_name', 'train', 'save name')\ntf.app.flags.DEFINE_string('save_dir', '../tfrecord/', 'save name')\ntf.app.flags.DEFINE_string('dataset', 'coco', 'dataset')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef convert_pascal_to_tfrecord(coco_trainvalmini):\n    save_path = FLAGS.save_dir + FLAGS.dataset + '_' + FLAGS.save_name + '.tfrecord'\n    mkdir(FLAGS.save_dir)\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=save_path, options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=save_path)\n\n    with open(coco_trainvalmini) as f:\n        files = f.readlines()\n\n    img_count = 0\n    gt_count = 0\n\n    for count, raw_line in enumerate(files):\n        file = json.loads(raw_line)\n        img_path = os.path.join('/data/COCO/train2017', file['fpath'].split('_')[-1])\n        img_name = file['ID']\n\n        if not os.path.exists(img_path):\n            # print('{} is not exist!'.format(img_path))\n            img_count += 1\n            continue\n        # img = np.array(Image.open(img_path))\n        img = cv2.imread(img_path)[:, :, ::-1]\n\n        if img is None:\n            continue\n\n        gtboxes = file['gtboxes']\n        img_height = file['height']\n        img_width = file['width']\n\n        if len(gtboxes) == 0:\n            # print('{}: gt is not exist!'.format(img_path))\n            gt_count += 1\n            continue\n\n        gtbox_label = []\n        for gt in gtboxes:\n            box = gt['box']\n            label = gt['tag']\n            gtbox_label.append([box[0], box[1], box[0]+box[2], box[1]+box[3], NAME_LABEL_MAP[label]])\n\n        gtbox_label = np.array(gtbox_label, np.int32)\n\n        feature = tf.train.Features(feature={\n            # do not need encode() in linux\n            'img_name': _bytes_feature(img_name.encode()),\n            # 'img_name': _bytes_feature(img_name),\n            'img_height': _int64_feature(img_height),\n            'img_width': _int64_feature(img_width),\n            'img': _bytes_feature(img.tostring()),\n            'gtboxes_and_label': _bytes_feature(gtbox_label.tostring()),\n            'num_objects': _int64_feature(gtbox_label.shape[0])\n        })\n\n        example = tf.train.Example(features=feature)\n\n        writer.write(example.SerializeToString())\n\n        view_bar('Conversion progress', count + 1, len(files))\n\n    print('{} images not exist!'.format(img_count))\n    print('{} gts not exist!'.format(gt_count))\n    print('\\nConversion is complete!')\n\n\nif __name__ == '__main__':\n    convert_pascal_to_tfrecord(FLAGS.coco_dir)\n"""
data/io/convert_data_to_tfrecord_voc2012.py,15,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport xml.etree.cElementTree as ET\nimport numpy as np\nimport tensorflow as tf\nimport glob\nimport cv2\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils.tools import *\n\ntf.app.flags.DEFINE_string(\'VOC_dir\', \'/data/VOC2012/VOCdevkit/VOC2012/\', \'Voc dir\')\ntf.app.flags.DEFINE_string(\'xml_dir\', \'Annotations\', \'xml dir\')\ntf.app.flags.DEFINE_string(\'image_dir\', \'JPEGImages\', \'image dir\')\ntf.app.flags.DEFINE_string(\'save_name\', \'train2012\', \'save name\')\ntf.app.flags.DEFINE_string(\'save_dir\', \'../tfrecord/\', \'save name\')\ntf.app.flags.DEFINE_string(\'img_format\', \'.jpg\', \'format of image\')\ntf.app.flags.DEFINE_string(\'dataset\', \'pascal\', \'dataset\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 5],\n           and has [xmin, ymin, xmax, ymax, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = [0, 0, 0, 0]\n                    for node in child_item:\n                        if node.tag == \'xmin\':\n                            tmp_box[0] = int(node.text)\n                        if node.tag == \'ymin\':\n                            tmp_box[1] = int(node.text)\n                        if node.tag == \'xmax\':\n                            tmp_box[2] = int(node.text)\n                        if node.tag == \'ymax\':\n                            tmp_box[3] = int(node.text)\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef convert_pascal_to_tfrecord():\n    xml_path = FLAGS.VOC_dir + FLAGS.xml_dir\n    image_path = FLAGS.VOC_dir + FLAGS.image_dir\n    save_path = FLAGS.save_dir + FLAGS.dataset + \'_\' + FLAGS.save_name + \'.tfrecord\'\n    mkdir(FLAGS.save_dir)\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=save_path, options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=save_path)\n\n    fr = open(\'/data/VOC2012/VOCdevkit/VOC2012/ImageSets/Main/trainval.txt\', \'r\')\n    lines = fr.readlines()\n\n    real_cnt = 0\n\n    for count, xml in enumerate(glob.glob(xml_path + \'/*.xml\')):\n        # to avoid path error in different development platform\n        xml = xml.replace(\'\\\\\', \'/\')\n\n        tmp = xml.split(\'/\')[-1].split(\'.\')[0] + ""\\n""\n        if tmp not in lines:\n            continue\n\n        img_name = xml.split(\'/\')[-1].split(\'.\')[0] + FLAGS.img_format\n        img_path = image_path + \'/\' + img_name\n\n        if not os.path.exists(img_path):\n            print(\'{} is not exist!\'.format(img_path))\n            continue\n\n        img_height, img_width, gtbox_label = read_xml_gtbox_and_label(xml)\n\n        # img = np.array(Image.open(img_path))\n        img = cv2.imread(img_path)[:, :, ::-1]\n\n        feature = tf.train.Features(feature={\n            # do not need encode() in linux\n            \'img_name\': _bytes_feature(img_name.encode()),\n            # \'img_name\': _bytes_feature(img_name),\n            \'img_height\': _int64_feature(img_height),\n            \'img_width\': _int64_feature(img_width),\n            \'img\': _bytes_feature(img.tostring()),\n            \'gtboxes_and_label\': _bytes_feature(gtbox_label.tostring()),\n            \'num_objects\': _int64_feature(gtbox_label.shape[0])\n        })\n\n        example = tf.train.Example(features=feature)\n\n        writer.write(example.SerializeToString())\n        real_cnt += 1\n\n        view_bar(\'Conversion progress\', count + 1, len(glob.glob(xml_path + \'/*.xml\')))\n\n    print(\'\\nConversion is complete! {} images.\'.format(real_cnt))\n\n\nif __name__ == \'__main__\':\n    # xml_path = \'../data/dataset/VOCdevkit/VOC2007/Annotations/000005.xml\'\n    # read_xml_gtbox_and_label(xml_path)\n\n    convert_pascal_to_tfrecord()\n'"
data/io/image_preprocess.py,18,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\nimport numpy as np\n\n\ndef max_length_limitation(length, length_limitation):\n    return tf.cond(tf.less(length, length_limitation),\n                   true_fn=lambda: length,\n                   false_fn=lambda: length_limitation)\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len, length_limitation=1200):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 5].  gtboxes: [xmin, ymin, xmax, ymax]\n    :param target_shortside_len:\n    :param length_limitation: set max length to avoid OUT OF MEMORY\n    :return:\n    '''\n    img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                           true_fn=lambda: (target_shortside_len,\n                                            max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                           false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                             target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    new_xmin, new_ymin = xmin * new_w // img_w, ymin * new_h // img_h\n    new_xmax, new_ymax = xmax * new_w // img_w, ymax * new_h // img_h\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, new_ymin, new_xmax, new_ymax, label], axis=0))\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, length_limitation=1200, is_resize=True):\n    if is_resize:\n      img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n      new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                             true_fn=lambda: (target_shortside_len,\n                                              max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                             false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                               target_shortside_len))\n\n      img_tensor = tf.expand_dims(img_tensor, axis=0)\n      img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n      img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor\n\n\ndef flip_left_to_right(img_tensor, gtboxes_and_label):\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_xmax = w - xmin\n    new_xmin = w - xmax\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, ymin, new_xmax, ymax, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label= tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_to_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\n\n"""
data/io/image_preprocess_multi_gpu.py,18,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\nimport numpy as np\n\n\ndef max_length_limitation(length, length_limitation):\n    return tf.cond(tf.less(length, length_limitation),\n                   true_fn=lambda: length,\n                   false_fn=lambda: length_limitation)\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len, length_limitation=1200):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 5].  gtboxes: [xmin, ymin, xmax, ymax]\n    :param target_shortside_len:\n    :param length_limitation: set max length to avoid OUT OF MEMORY\n    :return:\n    '''\n    img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                           true_fn=lambda: (target_shortside_len,\n                                            max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                           false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                             target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    new_xmin, new_ymin = xmin * new_w // img_w, ymin * new_h // img_h\n    new_xmax, new_ymax = xmax * new_w // img_w, ymax * new_h // img_h\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, new_ymin, new_xmax, new_ymax, label], axis=0)), new_h, new_w\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, length_limitation=1200, is_resize=True):\n    if is_resize:\n      img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n      new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                             true_fn=lambda: (target_shortside_len,\n                                              max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                             false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                               target_shortside_len))\n\n      img_tensor = tf.expand_dims(img_tensor, axis=0)\n      img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n      img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor\n\n\ndef flip_left_to_right(img_tensor, gtboxes_and_label):\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_xmax = w - xmin\n    new_xmin = w - xmax\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, ymin, new_xmax, ymax, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label= tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_to_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\n\n"""
data/io/image_preprocess_multi_gpu_aug.py,39,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len, max_len=1200):\n    '''\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 9]\n    :param target_shortside_len:\n    :return:\n    '''\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    new_h, new_w = tf.cond(tf.less(h, w),\n                           true_fn=lambda: (target_shortside_len, tf.minimum(target_shortside_len * w//h, max_len)),\n                           false_fn=lambda: (tf.minimum(target_shortside_len * h//w, max_len),\n                                             target_shortside_len))\n    # new_h, new_w = 1200, 1200\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    x1, x2, x3, x4 = x1 * new_w//w, x2 * new_w//w, x3 * new_w//w, x4 * new_w//w\n    y1, y2, y3, y4 = y1 * new_h//h, y2 * new_h//h, y3 * new_h//h, y4 * new_h//h\n\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor, tf.transpose(tf.stack([x1, y1, x2, y2, x3, y3, x4, y4, label], axis=0)), new_h, new_w\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, max_len=1200, is_resize=True):\n    h, w, = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n\n    if is_resize:\n        new_h, new_w = tf.cond(tf.less(h, w),\n                               true_fn=lambda: (\n                               target_shortside_len, tf.minimum(target_shortside_len * w // h, max_len)),\n                               false_fn=lambda: (tf.minimum(target_shortside_len * h // w, max_len),\n                                                 target_shortside_len))\n        img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    return img_tensor  # [1, h, w, c]\n\n\ndef flip_left_right(img_tensor, gtboxes_and_label):\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_x1 = w - x1\n    new_x2 = w - x2\n    new_x3 = w - x3\n    new_x4 = w - x4\n    return img_tensor, tf.transpose(tf.stack([new_x1, y1, new_x2, y2, new_x3, y3, new_x4, y4, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\ndef flip_up_down(img_tensor, gtboxes_and_label):\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    img_tensor = tf.image.flip_up_down(img_tensor)\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    new_y1 = h - y1\n    new_y2 = h - y2\n    new_y3 = h - y3\n    new_y4 = h - y4\n\n    return img_tensor, tf.transpose(tf.stack([x1, new_y1, x2, new_y2, x3, new_y3, x4, new_y4, label], axis=0))\n\n\ndef random_flip_up_dowm(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_up_down(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor, gtboxes_and_label\n\n\ndef random_rgb2gray(img_tensor, gtboxes_and_label):\n    '''\n    :param img_tensor: tf.float32\n    :return:\n    '''\n    def rgb2gray(img, gtboxes_and_label):\n\n        label = gtboxes_and_label[:, -1]\n        if NAME_LABEL_MAP['swimming-pool'] in label:\n            # do not change color, because swimming-pool need color\n            return img\n\n        coin = np.random.rand()\n        if coin < 0.3:\n            img = np.asarray(img, dtype=np.float32)\n            r, g, b = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n            gray = r * 0.299 + g * 0.587 + b * 0.114\n            img = np.stack([gray, gray, gray], axis=2)\n            return img\n        else:\n            return img\n\n    h, w, c = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1], tf.shape(img_tensor)[2]\n    img_tensor = tf.py_func(rgb2gray,\n                            inp=[img_tensor, gtboxes_and_label],\n                            Tout=tf.float32)\n    img_tensor = tf.reshape(img_tensor, shape=[h, w, c])\n\n    return img_tensor\n\n\ndef rotate_img_np_OLD(img, gtboxes_and_label, r_theta):\n    h, w, c = img.shape\n    center = (w // 2, h // 2)\n\n    M = cv2.getRotationMatrix2D(center, r_theta, 1.0)\n    rotated_img = cv2.warpAffine(img, M, (w, h))\n    # print (M)\n    new_points_list = []\n    obj_num = len(gtboxes_and_label)\n    for st in range(0, 7, 2):\n        points = gtboxes_and_label[:, st:st+2]\n        expand_points = np.concatenate((points, np.ones(shape=(obj_num, 1))), axis=1)\n        new_points = np.dot(M, expand_points.T)\n        new_points = new_points.T\n        new_points_list.append(new_points)\n    gtboxes = np.concatenate(new_points_list, axis=1)\n    gtboxes_and_label = np.concatenate((gtboxes, gtboxes_and_label[:, -1].reshape(-1, 1)), axis=1)\n\n    x1, y1, x2, y2, x3, y3, x4, y4 = np.split(gtboxes, 8, 1)\n\n    xc = 0.25*(x1+x2+x3+x4)\n    yc = 0.25*(y1+y2+y3+y4)\n\n    valid = (xc>0) & (yc>0) & (xc<w) & (yc<h)\n    valid = valid.reshape((-1, ))\n    # print (valid)\n    if np.sum(valid) != 0:\n        gtboxes_and_label = gtboxes_and_label[valid]\n    gtboxes_and_label = np.asarray(gtboxes_and_label, dtype=np.int32)\n\n    return rotated_img, gtboxes_and_label\n\n\ndef rotate_img_np(img, gtboxes_and_label, r_theta):\n    h, w, c = img.shape\n    center = (w // 2, h // 2)\n\n    M = cv2.getRotationMatrix2D(center, r_theta, 1.0)\n    cos, sin = np.abs(M[0, 0]), np.abs(M[0, 1])\n    nW, nH = int(h*sin + w*cos), int(h*cos + w*sin)  # new W and new H\n    M[0, 2] += (nW/2) - center[0]\n    M[1, 2] += (nH/2) - center[1]\n    rotated_img = cv2.warpAffine(img, M, (nW, nH))\n    # -------\n\n    new_points_list = []\n    obj_num = len(gtboxes_and_label)\n    for st in range(0, 7, 2):\n        points = gtboxes_and_label[:, st:st+2]\n        expand_points = np.concatenate((points, np.ones(shape=(obj_num, 1))), axis=1)\n        new_points = np.dot(M, expand_points.T)\n        new_points = new_points.T\n        new_points_list.append(new_points)\n    gtboxes = np.concatenate(new_points_list, axis=1)\n    gtboxes_and_label = np.concatenate((gtboxes, gtboxes_and_label[:, -1].reshape(-1, 1)), axis=1)\n    gtboxes_and_label = np.asarray(gtboxes_and_label, dtype=np.int32)\n\n    return rotated_img, gtboxes_and_label\n\n\ndef rotate_img(img_tensor, gtboxes_and_label):\n\n    # thetas = tf.constant([-30, -60, -90, 30, 60, 90])\n    thetas = tf.range(-90, 90+16, delta=15)\n    # -90, -75, -60, -45, -30, -15,   0,  15,  30,  45,  60,  75,  90\n\n    theta = tf.random_shuffle(thetas)[0]\n\n    img_tensor, gtboxes_and_label = tf.py_func(rotate_img_np,\n                                               inp=[img_tensor, gtboxes_and_label, theta],\n                                               Tout=[tf.float32, tf.int32])\n\n    h, w, c = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1], tf.shape(img_tensor)[2]\n    img_tensor = tf.reshape(img_tensor, [h, w, c])\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 9])\n\n    return img_tensor, gtboxes_and_label\n\n\ndef random_rotate_img(img_tensor, gtboxes_and_label):\n\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.6),\n                                            lambda: rotate_img(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor, gtboxes_and_label\n\n\ndef get_mask(img, boxes):\n    h, w, _ = img.shape\n    mask = np.zeros([h, w])\n    for b in boxes:\n        b = np.reshape(b[0:-1], [4, 2])\n        rect = np.array(b, np.int32)\n        cv2.fillConvexPoly(mask, rect, 1)\n    # mask = cv2.resize(mask, dsize=(h // 16, w // 16))\n    mask = np.expand_dims(mask, axis=-1)\n    return np.array(mask, np.float32)\n"""
data/io/read_tfrecord.py,33,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom data.io import image_preprocess\nfrom libs.configs import cfgs\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n\n    if is_training:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len,\n                                                                    length_limitation=cfgs.IMG_MAX_LENGTH)\n        img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                         gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len,\n                                                                    length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255 - tf.constant([[cfgs.PIXEL_MEAN_]])\n    else:\n        img = img - tf.constant([[cfgs.PIXEL_MEAN]])  # sub pixel mean at last\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n    assert batch_size == 1, ""we only support batch_size is 1.We may support large batch_size in the future""\n\n    if dataset_name not in [\'ship\', \'spacenet\', \'pascal\', \'coco\', \'bdd100k\', \'DOTA\', \'DOTA_H\']:\n        raise ValueError(\'dataSet name must be in pascal, coco spacenet and ship\')\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    # shortside_len = tf.constant(shortside_len)\n    # shortside_len = tf.random_shuffle(shortside_len)[0]\n\n    img_name, img, gtboxes_and_label, num_obs = read_and_prepocess_single_img(filename_queue, shortside_len,\n                                                                              is_training=is_training)\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs],\n                       batch_size=batch_size,\n                       capacity=1,\n                       num_threads=1,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n        next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                   batch_size=cfgs.BATCH_SIZE,\n                   shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                   is_training=True)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        img_name_batch_, img_batch_, gtboxes_and_label_batch_, num_objects_batch_ \\\n            = sess.run([img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch])\n\n        print(img_name_batch_)\n\n        print(\'debug\')\n\n        coord.request_stop()\n        coord.join(threads)\n'"
data/io/read_tfrecord_multi_gpu.py,33,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport os\nimport sys\nsys.path.append(\'../../\')\n\nfrom data.io import image_preprocess_multi_gpu as image_preprocess\nfrom libs.configs import cfgs\n\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n\n    if is_training:\n        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                                  target_shortside_len=shortside_len,\n                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)\n        img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                         gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                                  target_shortside_len=shortside_len,\n                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255 - tf.constant([[cfgs.PIXEL_MEAN_]])\n    else:\n        img = img - tf.constant([[cfgs.PIXEL_MEAN]])  # sub pixel mean at last\n    return img_name, img, gtboxes_and_label, num_objects, img_h, img_w\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n    # assert batch_size == 1, ""we only support batch_size is 1.We may support large batch_size in the future""\n\n    if dataset_name not in [\'ship\', \'spacenet\', \'pascal\', \'coco\', \'bdd100k\', \'DOTA\', \'DOTA_H\']:\n        raise ValueError(\'dataSet name must be in pascal, coco spacenet and ship\')\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    # shortside_len = tf.constant(shortside_len)\n    # shortside_len = tf.random_shuffle(shortside_len)[0]\n\n    img_name, img, gtboxes_and_label, num_obs, img_h, img_w = read_and_prepocess_single_img(filename_queue, shortside_len,\n                                                                                            is_training=is_training)\n\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs, img_h, img_w],\n                       batch_size=batch_size,\n                       capacity=16,\n                       num_threads=16,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0,1\'\n    num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n        next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                   batch_size=cfgs.BATCH_SIZE * num_gpu,\n                   shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                   is_training=True)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        img_name_batch_, img_batch_, gtboxes_and_label_batch_, num_objects_batch_ \\\n            = sess.run([img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch])\n\n        print(img_name_batch_.shape)\n        print(img_batch_.shape)\n        print(\'debug\')\n\n        coord.request_stop()\n        coord.join(threads)\n'"
data/io/read_tfrecord_multi_gpu_aug.py,36,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport sys\nsys.path.append(\'../../\')\nfrom data.io import image_preprocess_multi_gpu_aug\nfrom libs.configs import cfgs\nfrom libs.box_utils.boxes_utils import get_horizen_minAreaRectangle\n\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 9])\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n    if is_training:\n        # prob is 0.3: convert to gray\n        img = image_preprocess_multi_gpu_aug.random_rgb2gray(img_tensor=img, gtboxes_and_label=gtboxes_and_label)\n\n        # rotate with 0.5 prob. and if rotate, if will random choose a theta from : tf.range(-90, 90+16, delta=15)\n        img, gtboxes_and_label = image_preprocess_multi_gpu_aug.random_rotate_img(img_tensor=img,\n                                                                                  gtboxes_and_label=gtboxes_and_label)\n\n        img, gtboxes_and_label, img_h, img_w = image_preprocess_multi_gpu_aug.short_side_resize(img_tensor=img,\n                                                                                                gtboxes_and_label=gtboxes_and_label,\n                                                                                                target_shortside_len=shortside_len,\n                                                                                                max_len=cfgs.IMG_MAX_LENGTH)\n        img, gtboxes_and_label = image_preprocess_multi_gpu_aug.random_flip_left_right(img_tensor=img,\n                                                                                       gtboxes_and_label=gtboxes_and_label)\n        img, gtboxes_and_label = image_preprocess_multi_gpu_aug.random_flip_up_dowm(img_tensor=img,\n                                                                                    gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label, img_h, img_w = image_preprocess_multi_gpu_aug.short_side_resize(img_tensor=img,\n                                                                                                gtboxes_and_label=gtboxes_and_label,\n                                                                                                target_shortside_len=shortside_len)\n\n    gtboxes_and_label = get_horizen_minAreaRectangle(gtboxes_and_label)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255. - tf.constant([cfgs.PIXEL_MEAN_])\n    else:\n        img = img - tf.constant([cfgs.PIXEL_MEAN])  # sub pixel mean at last\n    return img_name, img, gtboxes_and_label, num_objects, img_h, img_w\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n\n    if dataset_name not in [\'DOAI2019\', \'DOTA\', \'ship\', \'ICDAR2015\', \'pascal\', \'coco\', \'DOTA_TOTAL\', \'WIDER\']:\n        raise ValueError(\'dataSet name must be in pascal, coco spacenet and ship\')\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n    # filename_tensorlist = tf.Print(filename_tensorlist,\n    #                                [tf.shape(filename_tensorlist)], summarize=10, message=""record_list-->:"")\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    shortside_len = tf.constant(shortside_len)\n    shortside_len = tf.random_shuffle(shortside_len)[0]\n\n    img_name, img, gtboxes_and_label, num_obs, img_h, img_w = read_and_prepocess_single_img(filename_queue,\n                                                                                            shortside_len,\n                                                                                            is_training=is_training)\n    img_name_batch, img_batch, gtboxes_and_label_batch , num_obs_batch, img_h_batch, img_w_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs, img_h, img_w],\n                       batch_size=batch_size,\n                       capacity=16,\n                       num_threads=16,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n        next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                   batch_size=cfgs.BATCH_SIZE * len(cfgs.GPU_GROUP.strip().split(\',\')),\n                   shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                   is_training=True)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 9])\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        img_name_batch_, img_batch_, gtboxes_and_label_batch_, num_objects_batch_ \\\n            = sess.run([img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch])\n\n        print(img_name_batch_)\n        print(img_batch_.shape)\n\n        coord.request_stop()\n        coord.join(threads)'"
data/lib_coco/__init__.py,0,b''
data/lib_coco/get_coco_next_batch.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport sys, os\n# sys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, \'./PythonAPI/\')\n# sys.path.insert(0, os.path.abspath(\'data\'))\nfor _ in sys.path:\n    print (_)\nfrom PythonAPI.pycocotools.coco import COCO\nimport cv2\nimport numpy as np\nimport os\nfrom libs.label_name_dict import coco_dict\n\n\nannotation_path = \'/home/yjr/DataSet/COCO/2017/annotations/instances_train2017.json\'\nprint (""load coco .... it will cost about 17s.."")\ncoco = COCO(annotation_path)\n\nimgId_list = coco.getImgIds()\nimgId_list = np.array(imgId_list)\n\ntotal_imgs = len(imgId_list)\n\n# print (NAME_LABEL_DICT)\n\n\ndef next_img(step):\n\n    if step % total_imgs == 0:\n        np.random.shuffle(imgId_list)\n    imgid = imgId_list[step % total_imgs]\n\n    imgname = coco.loadImgs(ids=[imgid])[0][\'file_name\']\n    # print (type(imgname), imgname)\n    img = cv2.imread(os.path.join(""/home/yjr/DataSet/COCO/2017/train2017"", imgname))\n\n    annotation = coco.imgToAnns[imgid]\n    gtbox_and_label_list = []\n    for ann in annotation:\n        box = ann[\'bbox\']\n\n        box = [box[0], box[1], box[0]+box[2], box[1]+box[3]]  # [xmin, ymin, xmax, ymax]\n        cat_id = ann[\'category_id\']\n        cat_name = coco_dict.originID_classes[cat_id] #ID_NAME_DICT[cat_id]\n        label = coco_dict.NAME_LABEL_MAP[cat_name]\n        gtbox_and_label_list.append(box + [label])\n    gtbox_and_label_list = np.array(gtbox_and_label_list, dtype=np.int32)\n    # print (img.shape, gtbox_and_label_list.shape)\n    if gtbox_and_label_list.shape[0] == 0:\n        return next_img(step+1)\n    else:\n        return imgid, img[:, :, ::-1], gtbox_and_label_list\n\n\nif __name__ == \'__main__\':\n\n    imgid, img,  gtbox = next_img(3234)\n\n    print(""::"")\n    from libs.box_utils.draw_box_in_img import draw_boxes_with_label_and_scores\n\n    img = draw_boxes_with_label_and_scores(img_array=img, boxes=gtbox[:, :-1], labels=gtbox[:, -1],\n                                           scores=np.ones(shape=(len(gtbox), )))\n    print (""_----"")\n\n\n    cv2.imshow(""test"", img)\n    cv2.waitKey(0)\n\n\n'"
libs/box_utils/__init__.py,0,b''
libs/box_utils/anchor_utils.py,20,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\nfrom libs.configs import cfgs\n\n\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios,\n                 featuremap_height, featuremap_width,\n                 stride, name=\'make_anchors\'):\n    \'\'\'\n    :param base_anchor_size:256\n    :param anchor_scales:\n    :param anchor_ratios:\n    :param featuremap_height:\n    :param featuremap_width:\n    :param stride:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)  # [x_center, y_center, w, h]\n\n        ws, hs = enum_ratios(enum_scales(base_anchor, anchor_scales),\n                             anchor_ratios)  # per locations ws and hs\n\n        # featuremap_height = tf.Print(featuremap_height,\n        #                              [featuremap_height, featuremap_width], summarize=10,\n        #                              message=name+""_SHAPE***"")\n\n        x_centers = tf.range(featuremap_width, dtype=tf.float32) * stride\n        y_centers = tf.range(featuremap_height, dtype=tf.float32) * stride\n\n        if cfgs.USE_CENTER_OFFSET:\n            x_centers = x_centers + stride/2.\n            y_centers = y_centers + stride/2.\n\n        x_centers, y_centers = tf.meshgrid(x_centers, y_centers)\n\n        ws, x_centers = tf.meshgrid(ws, x_centers)\n        hs, y_centers = tf.meshgrid(hs, y_centers)\n\n        anchor_centers = tf.stack([x_centers, y_centers], 2)\n        anchor_centers = tf.reshape(anchor_centers, [-1, 2])\n\n        box_sizes = tf.stack([ws, hs], axis=2)\n        box_sizes = tf.reshape(box_sizes, [-1, 2])\n        # anchors = tf.concat([anchor_centers, box_sizes], axis=1)\n        anchors = tf.concat([anchor_centers - 0.5*box_sizes,\n                             anchor_centers + 0.5*box_sizes], axis=1)\n        return anchors\n\n\ndef enum_scales(base_anchor, anchor_scales):\n\n    anchor_scales = base_anchor * tf.constant(anchor_scales, dtype=tf.float32, shape=(len(anchor_scales), 1))\n\n    return anchor_scales\n\n\ndef enum_ratios(anchors, anchor_ratios):\n    \'\'\'\n    ratio = h /w\n    :param anchors:\n    :param anchor_ratios:\n    :return:\n    \'\'\'\n    ws = anchors[:, 2]  # for base anchor: w == h\n    hs = anchors[:, 3]\n    sqrt_ratios = tf.sqrt(tf.constant(anchor_ratios))\n\n    ws = tf.reshape(ws / sqrt_ratios[:, tf.newaxis], [-1, 1])\n    hs = tf.reshape(hs * sqrt_ratios[:, tf.newaxis], [-1, 1])\n\n    return ws, hs\n\n\nif __name__ == \'__main__\':\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    base_anchor_size = 256\n    anchor_scales = [1.0]\n    anchor_ratios = [0.5, 2.0, 1.0]\n    anchors = make_anchors(base_anchor_size=base_anchor_size, anchor_ratios=anchor_ratios,\n                           anchor_scales=anchor_scales,\n                           featuremap_width=32,\n                           featuremap_height=63,\n                           stride=16)\n    init = tf.global_variables_initializer()\n    with tf.Session() as sess:\n        sess.run(init)\n        anchor_result = sess.run(anchors)\n        print (anchor_result.shape)\n'"
libs/box_utils/boxes_utils.py,68,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.box_utils.coordinate_convert import forward_convert\n\ndef ious_calu(boxes_1, boxes_2):\n    '''\n\n    :param boxes_1: [N, 4] [xmin, ymin, xmax, ymax]\n    :param boxes_2: [M, 4] [xmin, ymin. xmax, ymax]\n    :return:\n    '''\n    boxes_1 = tf.cast(boxes_1, tf.float32)\n    boxes_2 = tf.cast(boxes_2, tf.float32)\n    xmin_1, ymin_1, xmax_1, ymax_1 = tf.split(boxes_1, 4, axis=1)  # xmin_1 shape is [N, 1]..\n    xmin_2, ymin_2, xmax_2, ymax_2 = tf.unstack(boxes_2, axis=1)  # xmin_2 shape is [M, ]..\n\n    max_xmin = tf.maximum(xmin_1, xmin_2)\n    min_xmax = tf.minimum(xmax_1, xmax_2)\n\n    max_ymin = tf.maximum(ymin_1, ymin_2)\n    min_ymax = tf.minimum(ymax_1, ymax_2)\n\n    overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n    overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n    overlaps = overlap_h * overlap_w\n\n    area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n    area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n    ious = overlaps / (area_1 + area_2 - overlaps)\n\n    return ious\n\n\ndef clip_boxes_to_img_boundaries(decode_boxes, img_shape):\n    '''\n\n    :param decode_boxes:\n    :return: decode boxes, and already clip to boundaries\n    '''\n\n    with tf.name_scope('clip_boxes_to_img_boundaries'):\n\n        # xmin, ymin, xmax, ymax = tf.unstack(decode_boxes, axis=1)\n        xmin = decode_boxes[:, 0]\n        ymin = decode_boxes[:, 1]\n        xmax = decode_boxes[:, 2]\n        ymax = decode_boxes[:, 3]\n        img_h, img_w = img_shape[1], img_shape[2]\n\n        img_h, img_w = tf.cast(img_h, tf.float32), tf.cast(img_w, tf.float32)\n\n        xmin = tf.maximum(tf.minimum(xmin, img_w-1.), 0.)\n        ymin = tf.maximum(tf.minimum(ymin, img_h-1.), 0.)\n\n        xmax = tf.maximum(tf.minimum(xmax, img_w-1.), 0.)\n        ymax = tf.maximum(tf.minimum(ymax, img_h-1.), 0.)\n\n        return tf.transpose(tf.stack([xmin, ymin, xmax, ymax]))\n\n\ndef filter_outside_boxes(boxes, img_h, img_w):\n    '''\n    :param anchors:boxes with format [xmin, ymin, xmax, ymax]\n    :param img_h: height of image\n    :param img_w: width of image\n    :return: indices of anchors that inside the image boundary\n    '''\n\n    with tf.name_scope('filter_outside_boxes'):\n        xmin, ymin, xmax, ymax = tf.unstack(boxes, axis=1)\n\n        xmin_index = tf.greater_equal(xmin, 0)\n        ymin_index = tf.greater_equal(ymin, 0)\n        xmax_index = tf.less_equal(xmax, tf.cast(img_w, tf.float32))\n        ymax_index = tf.less_equal(ymax, tf.cast(img_h, tf.float32))\n\n        indices = tf.transpose(tf.stack([xmin_index, ymin_index, xmax_index, ymax_index]))\n        indices = tf.cast(indices, dtype=tf.int32)\n        indices = tf.reduce_sum(indices, axis=1)\n        indices = tf.where(tf.equal(indices, 4))\n        # indices = tf.equal(indices, 4)\n        return tf.reshape(indices, [-1])\n\n\ndef padd_boxes_with_zeros(boxes, scores, max_num_of_boxes):\n\n    '''\n    num of boxes less than max num of boxes, so it need to pad with zeros[0, 0, 0, 0]\n    :param boxes:\n    :param scores: [-1]\n    :param max_num_of_boxes:\n    :return:\n    '''\n\n    pad_num = tf.cast(max_num_of_boxes, tf.int32) - tf.shape(boxes)[0]\n\n    zero_boxes = tf.zeros(shape=[pad_num, 4], dtype=boxes.dtype)\n    zero_scores = tf.zeros(shape=[pad_num], dtype=scores.dtype)\n\n    final_boxes = tf.concat([boxes, zero_boxes], axis=0)\n\n    final_scores = tf.concat([scores, zero_scores], axis=0)\n\n    return final_boxes, final_scores\n\n\ndef get_horizen_minAreaRectangle(boxs, with_label=True):\n\n    # rpn_proposals_boxes_convert = tf.py_func(forward_convert,\n    #                                          inp=[boxs, with_label],\n    #                                          Tout=tf.float32)\n    if with_label:\n        rpn_proposals_boxes_convert = tf.reshape(boxs, [-1, 9])\n\n        boxes_shape = tf.shape(rpn_proposals_boxes_convert)\n        x_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n        y_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n\n        label = tf.unstack(rpn_proposals_boxes_convert, axis=1)[-1]\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n\n        ''' The following codes aims to avoid gtbox out_sde'''\n\n        # img_h, img_w = img_shape[0], img_shape[1]\n        # img_h = tf.cast(img_h, tf.float32)\n        # img_w = tf.cast(img_w, tf.float32)\n        # x_min = tf.maximum(x_min, 0)\n        # y_min = tf.maximum(y_min, 0)\n        # x_max = tf.minimum(x_max, img_w)\n        # y_max = tf.minimum(y_max, img_h)\n        return tf.transpose(tf.stack([x_min, y_min, x_max, y_max, label], axis=0))\n    else:\n        rpn_proposals_boxes_convert = tf.reshape(boxs, [-1, 8])\n\n        boxes_shape = tf.shape(rpn_proposals_boxes_convert)\n        x_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n        y_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n\n        ''' The following codes aims to avoid gtbox out_sde'''\n\n        # img_h, img_w = img_shape[0], img_shape[1]\n        # img_h = tf.cast(img_h, tf.float32)\n        # img_w = tf.cast(img_w, tf.float32)\n        # x_min = tf.maximum(x_min, 0)\n        # y_min = tf.maximum(y_min, 0)\n        # x_max = tf.minimum(x_max, img_w)\n        # y_max = tf.minimum(y_max, img_h)\n\n    return tf.transpose(tf.stack([x_min, y_min, x_max, y_max], axis=0))"""
libs/box_utils/coordinate_convert.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\n\n\ndef forward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [x_c, y_c, w, h, theta]\n    :return: format [x1, y1, x2, y2, x3, y3, x4, y4]\n    """"""\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[0], rect[1]), (rect[2], rect[3]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7], rect[5]])\n    else:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[0], rect[1]), (rect[2], rect[3]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]])\n\n    return np.array(boxes, dtype=np.float32)\n\n\ndef back_forward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [x1, y1, x2, y2, x3, y3, x4, y4, (label)]\n    :param with_label: default True\n    :return: format [x_c, y_c, w, h, theta, (label)]\n    """"""\n\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = np.int0(rect[:-1])\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([x, y, w, h, theta, rect[-1]])\n\n    else:\n        for rect in coordinate:\n            box = np.int0(rect)\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([x, y, w, h, theta])\n\n    return np.array(boxes, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    coord = np.array([[150, 150, 50, 100, -90, 1],\n                      [150, 150, 100, 50, -90, 1],\n                      [150, 150, 50, 100, -45, 1],\n                      [150, 150, 100, 50, -45, 1]])\n\n    coord1 = np.array([[150, 150, 100, 50, 0],\n                      [150, 150, 100, 50, -90],\n                      [150, 150, 100, 50, 45],\n                      [150, 150, 100, 50, -45]])\n\n    coord2 = forward_convert(coord)\n    # coord3 = forward_convert(coord1, mode=-1)\n    print(coord2)\n    # print(coord3-coord2)\n    # coord_label = np.array([[167., 203., 96., 132., 132., 96., 203., 167., 1.]])\n    #\n    # coord4 = back_forward_convert(coord_label, mode=1)\n    # coord5 = back_forward_convert(coord_label)\n\n    # print(coord4)\n    # print(coord5)\n\n    # coord3 = coordinate_present_convert(coord, -1)\n    # print(coord3)\n    # coord4 = coordinate_present_convert(coord3, mode=1)\n# print(coord4)'"
libs/box_utils/draw_box_in_img.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\n\nfrom libs.configs import cfgs\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\nNOT_DRAW_BOXES = 0\nONLY_DRAW_BOXES = -1\nONLY_DRAW_BOXES_WITH_SCORES = -2\n\nSTANDARD_COLORS = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\', \'LightBlue\', \'LightGreen\'\n]\nFONT = ImageFont.load_default()\n\n\ndef draw_a_rectangel_in_img(draw_obj, box, color, width):\n    \'\'\'\n    use draw lines to draw rectangle. since the draw_rectangle func can not modify the width of rectangle\n    :param draw_obj:\n    :param box: [x1, y1, x2, y2]\n    :return:\n    \'\'\'\n    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n    top_left, top_right = (x1, y1), (x2, y1)\n    bottom_left, bottom_right = (x1, y2), (x2, y2)\n\n    draw_obj.line(xy=[top_left, top_right],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[top_left, bottom_left],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[bottom_left, bottom_right],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[top_right, bottom_right],\n                  fill=color,\n                  width=width)\n\n\ndef only_draw_scores(draw_obj, box, score, color):\n\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x+60, y+10],\n                       fill=color)\n    draw_obj.text(xy=(x, y),\n                  text=""obj:"" +str(round(score, 2)),\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_label_with_scores(draw_obj, box, label, score, color):\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x + 60, y + 10],\n                       fill=color)\n\n    txt = LABEl_NAME_MAP[label] + \':\' + str(round(score, 2))\n    draw_obj.text(xy=(x, y),\n                  text=txt,\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_boxes_with_label_and_scores(img_array, boxes, labels, scores, in_graph=True):\n    if in_graph:\n        if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            img_array = (img_array * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n        else:\n            img_array = img_array + np.array(cfgs.PIXEL_MEAN)\n    img_array.astype(np.float32)\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img_array = np.array(img_array * 255 / np.max(img_array), dtype=np.uint8)\n\n    img_obj = Image.fromarray(img_array)\n    raw_img_obj = img_obj.copy()\n\n    draw_obj = ImageDraw.Draw(img_obj)\n    num_of_objs = 0\n    for box, a_label, a_score in zip(boxes, labels, scores):\n\n        if a_label != NOT_DRAW_BOXES:\n            num_of_objs += 1\n            draw_a_rectangel_in_img(draw_obj, box, color=STANDARD_COLORS[a_label], width=3)\n            if a_label == ONLY_DRAW_BOXES:  # -1\n                continue\n            elif a_label == ONLY_DRAW_BOXES_WITH_SCORES:  # -2\n                 only_draw_scores(draw_obj, box, a_score, color=\'White\')\n                 continue\n            else:\n                draw_label_with_scores(draw_obj, box, a_label, a_score, color=\'White\')\n\n    out_img_obj = Image.blend(raw_img_obj, img_obj, alpha=0.7)\n\n    return np.array(out_img_obj)\n\n\nif __name__ == \'__main__\':\n    img_array = cv2.imread(""/home/yjr/PycharmProjects/FPN_TF/tools/inference_image/2.jpg"")\n    img_array = np.array(img_array, np.float32) - np.array(cfgs.PIXEL_MEAN)\n    boxes = np.array(\n        [[200, 200, 500, 500],\n         [300, 300, 400, 400],\n         [200, 200, 400, 400]]\n    )\n\n    # test only draw boxes\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES\n    scores = np.zeros_like(labes)\n    imm = draw_boxes_with_label_and_scores(img_array, boxes, labes ,scores)\n    # imm = np.array(imm)\n\n    cv2.imshow(""te"", imm)\n\n    # test only draw scores\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES_WITH_SCORES\n    scores = np.random.rand((len(boxes))) * 10\n    imm2 = draw_boxes_with_label_and_scores(img_array, boxes, labes, scores)\n\n    cv2.imshow(""te2"", imm2)\n    # test draw label and scores\n\n    labels = np.arange(1, 4)\n    imm3 = draw_boxes_with_label_and_scores(img_array, boxes, labels, scores)\n    cv2.imshow(""te3"", imm3)\n\n    cv2.waitKey(0)\n\n\n\n\n\n\n\n'"
libs/box_utils/encode_and_decode.py,15,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\n\n\n# def encode_boxes(ex_rois, gt_rois, scale_factor=None):\n#     ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n#     ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n#     ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n#     ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n#\n#     gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n#     gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n#     gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n#     gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n#\n#     targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n#     targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n#     targets_dw = np.log(gt_widths / ex_widths)\n#     targets_dh = np.log(gt_heights / ex_heights)\n#\n#     if scale_factor:\n#         targets_dx = targets_dx * scale_factor[0]\n#         targets_dy = targets_dy * scale_factor[1]\n#         targets_dw = targets_dw * scale_factor[2]\n#         targets_dh = targets_dh * scale_factor[3]\n#\n#     targets = np.vstack(\n#         (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n#     return targets\n#\n#\n# def _concat_new_axis(t1, t2, t3, t4, axis):\n#     return tf.concat(\n#         [tf.expand_dims(t1, -1), tf.expand_dims(t2, -1),\n#          tf.expand_dims(t3, -1), tf.expand_dims(t4, -1)], axis=axis)\n#\n#\n# def decode_boxes(boxes, deltas, scale_factor=None):\n#     widths = boxes[:, 2] - boxes[:, 0] + 1.0\n#     heights = boxes[:, 3] - boxes[:, 1] + 1.0\n#     ctr_x = tf.expand_dims(boxes[:, 0] + 0.5 * widths, -1)\n#     ctr_y = tf.expand_dims(boxes[:, 1] + 0.5 * heights, -1)\n#\n#     dx = deltas[:, 0::4]\n#     dy = deltas[:, 1::4]\n#     dw = deltas[:, 2::4]\n#     dh = deltas[:, 3::4]\n#\n#     if scale_factor:\n#         dx /= scale_factor[0]\n#         dy /= scale_factor[1]\n#         dw /= scale_factor[2]\n#         dh /= scale_factor[3]\n#\n#     widths = tf.expand_dims(widths, -1)\n#     heights = tf.expand_dims(heights, -1)\n#\n#     pred_ctr_x = dx * widths + ctr_x\n#     pred_ctr_y = dy * heights + ctr_y\n#     pred_w = tf.exp(dw) * widths\n#     pred_h = tf.exp(dh) * heights\n#\n#     # x1\n#     # pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n#     pred_x1 = pred_ctr_x - 0.5 * pred_w\n#     # y1\n#     # pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n#     pred_y1 = pred_ctr_y - 0.5 * pred_h\n#     # x2\n#     # pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n#     pred_x2 = pred_ctr_x + 0.5 * pred_w\n#     # y2\n#     # pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n#     pred_y2 = pred_ctr_y + 0.5 * pred_h\n#\n#     pred_boxes = _concat_new_axis(pred_x1, pred_y1, pred_x2, pred_y2, 2)\n#     pred_boxes = tf.reshape(pred_boxes, (tf.shape(pred_boxes)[0], -1))\n#     return pred_boxes\n\n\ndef decode_boxes(encoded_boxes, reference_boxes, scale_factors=None):\n    '''\n\n    :param encoded_boxes:[N, 4]\n    :param reference_boxes: [N, 4] .\n    :param scale_factors: use for scale.\n\n    in the first stage, reference_boxes  are anchors\n    in the second stage, reference boxes are proposals(decode) produced by first stage\n    :return:decode boxes [N, 4]\n    '''\n\n    t_xcenter, t_ycenter, t_w, t_h = tf.unstack(encoded_boxes, axis=1)\n    if scale_factors:\n        t_xcenter /= scale_factors[0]\n        t_ycenter /= scale_factors[1]\n        t_w /= scale_factors[2]\n        t_h /= scale_factors[3]\n\n    reference_xmin, reference_ymin, reference_xmax, reference_ymax = tf.unstack(reference_boxes, axis=1)\n    # reference boxes are anchors in the first stage\n\n    reference_xcenter = (reference_xmin + reference_xmax) / 2.\n    reference_ycenter = (reference_ymin + reference_ymax) / 2.\n    reference_w = reference_xmax - reference_xmin\n    reference_h = reference_ymax - reference_ymin\n\n    predict_xcenter = t_xcenter * reference_w + reference_xcenter\n    predict_ycenter = t_ycenter * reference_h + reference_ycenter\n    predict_w = tf.exp(t_w) * reference_w\n    predict_h = tf.exp(t_h) * reference_h\n\n    predict_xmin = predict_xcenter - predict_w / 2.\n    predict_xmax = predict_xcenter + predict_w / 2.\n    predict_ymin = predict_ycenter - predict_h / 2.\n    predict_ymax = predict_ycenter + predict_h / 2.\n\n    return tf.transpose(tf.stack([predict_xmin, predict_ymin,\n                                  predict_xmax, predict_ymax]))\n\n\ndef encode_boxes(unencode_boxes, reference_boxes, scale_factors=None):\n    '''\n\n    :param unencode_boxes: [-1, 4]\n    :param reference_boxes: [-1, 4]\n    :return: encode_boxes [-1, 4]\n    '''\n\n    xmin, ymin, xmax, ymax = unencode_boxes[:, 0], unencode_boxes[:, 1], unencode_boxes[:, 2], unencode_boxes[:, 3]\n\n    reference_xmin, reference_ymin, reference_xmax, reference_ymax = reference_boxes[:, 0], reference_boxes[:, 1], \\\n                                                                     reference_boxes[:, 2], reference_boxes[:, 3]\n\n    x_center = (xmin + xmax) / 2.\n    y_center = (ymin + ymax) / 2.\n    w = xmax - xmin + 1e-8\n    h = ymax - ymin + 1e-8\n\n    reference_xcenter = (reference_xmin + reference_xmax) / 2.\n    reference_ycenter = (reference_ymin + reference_ymax) / 2.\n    reference_w = reference_xmax - reference_xmin + 1e-8\n    reference_h = reference_ymax - reference_ymin + 1e-8\n\n    # w + 1e-8 to avoid NaN in division and log below\n\n    t_xcenter = (x_center - reference_xcenter) / reference_w\n    t_ycenter = (y_center - reference_ycenter) / reference_h\n    t_w = np.log(w/reference_w)\n    t_h = np.log(h/reference_h)\n\n    if scale_factors:\n        t_xcenter *= scale_factors[0]\n        t_ycenter *= scale_factors[1]\n        t_w *= scale_factors[2]\n        t_h *= scale_factors[3]\n\n    return np.transpose(np.stack([t_xcenter, t_ycenter, t_w, t_h], axis=0))\n"""
libs/box_utils/iou.py,7,"b'# -*- coding: utf-8 -*-\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef iou_calculate(boxes_1, boxes_2):\r\n\r\n    with tf.name_scope(\'iou_caculate\'):\r\n        xmin_1, ymin_1, xmax_1, ymax_1 = boxes_1[:, 0], boxes_1[:, 1], boxes_1[:, 2], boxes_1[:, 3]\r\n\r\n        xmin_2, ymin_2, xmax_2, ymax_2 = boxes_2[:, 0], boxes_2[:, 1], boxes_2[:, 2], boxes_2[:, 3]\r\n\r\n        max_xmin = tf.maximum(xmin_1, xmin_2)\r\n        min_xmax = tf.minimum(xmax_1, xmax_2)\r\n\r\n        max_ymin = tf.maximum(ymin_1, ymin_2)\r\n        min_ymax = tf.minimum(ymax_1, ymax_2)\r\n\r\n        overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\r\n        overlap_w = tf.maximum(0., min_xmax - max_xmin)\r\n\r\n        overlaps = overlap_h * overlap_w\r\n\r\n        area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\r\n        area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\r\n\r\n        iou = overlaps / (area_1 + area_2 - overlaps)\r\n\r\n        return iou\r\n\r\n\r\ndef iou_calculate1(boxes_1, boxes_2):\r\n    xmin_1, ymin_1, xmax_1, ymax_1 = boxes_1[:, 0], boxes_1[:, 1], boxes_1[:, 2], boxes_1[:, 3]\r\n\r\n    xmin_2, ymin_2, xmax_2, ymax_2 = boxes_2[:, 0], boxes_2[:, 1], boxes_2[:, 2], boxes_2[:, 3]\r\n\r\n    max_xmin = np.maximum(xmin_1, xmin_2)\r\n    min_xmax = np.minimum(xmax_1, xmax_2)\r\n\r\n    max_ymin = np.maximum(ymin_1, ymin_2)\r\n    min_ymax = np.minimum(ymax_1, ymax_2)\r\n\r\n    overlap_h = np.maximum(0., min_ymax - max_ymin)  # avoid h < 0\r\n    overlap_w = np.maximum(0., min_xmax - max_xmin)\r\n\r\n    overlaps = overlap_h * overlap_w\r\n\r\n    area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\r\n    area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\r\n\r\n    iou = overlaps / (area_1 + area_2 - overlaps)\r\n\r\n    return iou\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    import os\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'13\'\r\n    boxes1 = np.array([[50, 50, 100, 300],\r\n                       [60, 60, 100, 200]], np.float32)\r\n\r\n    boxes2 = np.array([[50, 50, 100, 300],\r\n                       [200, 200, 100, 200]], np.float32)\r\n\r\n    print(iou_calculate1(boxes1, boxes2))\r\n\r\n\r\n\r\n'"
libs/box_utils/mask_utils.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport numpy as np\n# import tfplot as tfp\nimport tensorflow as tf\n# from libs.box_utils.coordinate_convert import forward_convert, back_forward_convert\nimport cv2\n\n\ndef make_gt_mask(fet_h, fet_w, img_h, img_w, gtboxes):\n    \'\'\'\n\n    :param fet_h:\n    :param fet_w:\n    :param img_h:\n    :param img_w:\n    :param gtboxes: [xmin, ymin, xmax, ymax, label]. shape is (N, 5)\n    :return:\n    \'\'\'\n    gtboxes = np.reshape(gtboxes, [-1, 5])\n    # xmin, ymin, xmax, ymax, label = gtboxes[:, 0], gtboxes[:, 1], gtboxes[:, 2], gtboxes[:, 3], gtboxes[:, 4]\n\n    areas = (gtboxes[:, 2]-gtboxes[:, 0])*(gtboxes[:, 3]-gtboxes[:, 1])\n    arg_areas = np.argsort(-1*areas)  # sort from large to small\n    gtboxes = gtboxes[arg_areas]\n\n    fet_h, fet_w = int(fet_h), int(fet_w)\n    mask = np.zeros(shape=[fet_h, fet_w], dtype=np.int32)\n    for a_box in gtboxes:\n        xmin, ymin, xmax, ymax, label = a_box[0], a_box[1], a_box[2], a_box[3], a_box[4]\n\n        new_xmin, new_ymin, new_xmax, new_ymax = int(xmin*fet_w/float(img_w)), int(ymin*fet_h/float(img_h)),\\\n                                                 int(xmax*fet_w/float(img_w)), int(ymax*fet_h/float(img_h))\n\n        new_xmin, new_ymin = max(0, new_xmin), max(0, new_ymin)\n        new_xmax, new_ymax = min(fet_w, new_xmax), min(fet_h, new_ymax)\n\n        mask[new_ymin:new_ymax, new_xmin:new_xmax] = np.int32(label)\n    return mask\n\n\n# def make_r_gt_mask(fet_h, fet_w, img_h, img_w, gtboxes):\n#     assert gtboxes.shape[1] == 8, ""gtboxes shape should be (-1, 8)""\n#     gtboxes = back_forward_convert(gtboxes)\n#     gtboxes = np.reshape(gtboxes, [-1, 6])  # [x, y, w, h, theta, label]\n#\n#     areas = gtboxes[:, 2] * gtboxes[:, 3]\n#     arg_areas = np.argsort(-1 * areas)  # sort from large to small\n#     gtboxes = gtboxes[arg_areas]\n#\n#     fet_h, fet_w = int(fet_h), int(fet_w)\n#     mask = np.zeros(shape=[fet_h, fet_w], dtype=np.int32)\n#     for a_box in gtboxes:\n#         # print(a_box)\n#         box = cv2.boxPoints(((a_box[0], a_box[1]), (a_box[2], a_box[3]), a_box[4]))\n#         box = np.reshape(box, [-1, ])\n#         label = a_box[-1]\n#         new_box = []\n#         for i in range(8):\n#             if i % 2 == 0:\n#                 x = box[i]\n#                 new_x = int(x * fet_w / float(img_w))\n#                 new_box.append(new_x)\n#             else:\n#                 y = box[i]\n#                 new_y = int(y*fet_h/float(img_h))\n#                 new_box.append(new_y)\n#\n#         new_box = np.int0(new_box).reshape([4, 2])\n#         color = int(label)\n#         # print(type(color), color)\n#         cv2.fillConvexPoly(mask, new_box, color=color)\n#     # print (mask.dtype)\n#     return mask\n\n# def vis_mask_tfsmry(mask, name):\n#     \'\'\'\n#\n#     :param mask:[H, W]. It\'s a tensor, not array\n#     :return:\n#     \'\'\'\n#\n#     def figure_attention(activation):\n#         fig, ax = tfp.subplots()\n#         im = ax.imshow(activation, cmap=\'jet\')\n#         fig.colorbar(im)\n#         return fig\n#\n#     heatmap = mask*10\n#\n#     tfp.summary.plot(name, figure_attention, [heatmap])'"
libs/box_utils/nms.py,0,"b'# -*- coding: utf-8 -*-\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\n\r\n\r\ndef py_cpu_nms(dets, thresh, max_output_size):\r\n    """"""Pure Python NMS baseline.""""""\r\n    x1 = dets[:, 0]\r\n    y1 = dets[:, 1]\r\n    x2 = dets[:, 2]\r\n    y2 = dets[:, 3]\r\n    scores = dets[:, 4]\r\n\r\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\r\n    order = scores.argsort()[::-1]\r\n    keep = []\r\n    while order.size > 0:\r\n        if len(keep) >= max_output_size:\r\n            break\r\n        i = order[0]\r\n        keep.append(i)\r\n        xx1 = np.maximum(x1[i], x1[order[1:]])\r\n        yy1 = np.maximum(y1[i], y1[order[1:]])\r\n        xx2 = np.minimum(x2[i], x2[order[1:]])\r\n        yy2 = np.minimum(y2[i], y2[order[1:]])\r\n\r\n        w = np.maximum(0.0, xx2 - xx1 + 1)\r\n        h = np.maximum(0.0, yy2 - yy1 + 1)\r\n        inter = w * h\r\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\r\n        inds = np.where(ovr <= thresh)[0]\r\n        order = order[inds + 1]\r\n\r\n    return np.array(keep, np.int64)\r\n'"
libs/box_utils/show_box_in_tensor.py,30,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\n\nfrom libs.configs import cfgs\n\nfrom libs.box_utils import draw_box_in_img\n\ndef only_draw_boxes(img_batch, boxes):\n\n    boxes = tf.stop_gradient(boxes)\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    labels = tf.ones(shape=(tf.shape(boxes)[0], ), dtype=tf.int32) * draw_box_in_img.ONLY_DRAW_BOXES\n    scores = tf.zeros_like(labels, dtype=tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=tf.uint8)\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))  # [batch_size, h, w, c]\n\n    return img_tensor_with_boxes\n\ndef draw_boxes_with_scores(img_batch, boxes, scores):\n\n    boxes = tf.stop_gradient(boxes)\n    scores = tf.stop_gradient(scores)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    labels = tf.ones(shape=(tf.shape(boxes)[0],), dtype=tf.int32) * draw_box_in_img.ONLY_DRAW_BOXES_WITH_SCORES\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\ndef draw_boxes_with_categories(img_batch, boxes, labels):\n    boxes = tf.stop_gradient(boxes)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    scores = tf.ones(shape=(tf.shape(boxes)[0],), dtype=tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\ndef draw_boxes_with_categories_and_scores(img_batch, boxes, labels, scores):\n    boxes = tf.stop_gradient(boxes)\n    scores = tf.stop_gradient(scores)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\nif __name__ == ""__main__"":\n    print (1)\n\n'"
libs/box_utils/tf_ops.py,7,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\n\n\'\'\'\nall of these ops are derived from tenosrflow Object Detection API\n\'\'\'\ndef indices_to_dense_vector(indices,\n                            size,\n                            indices_value=1.,\n                            default_value=0,\n                            dtype=tf.float32):\n  """"""Creates dense vector with indices set to specific (the para ""indices_value"" ) and rest to zeros.\n\n  This function exists because it is unclear if it is safe to use\n    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)\n  with indices which are not ordered.\n  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])\n\n  Args:\n    indices: 1d Tensor with integer indices which are to be set to\n        indices_values.\n    size: scalar with size (integer) of output Tensor.\n    indices_value: values of elements specified by indices in the output vector\n    default_value: values of other elements in the output vector.\n    dtype: data type.\n\n  Returns:\n    dense 1D Tensor of shape [size] with indices set to indices_values and the\n        rest set to default_value.\n  """"""\n  size = tf.to_int32(size)\n  zeros = tf.ones([size], dtype=dtype) * default_value\n  values = tf.ones_like(indices, dtype=dtype) * indices_value\n\n  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],\n                           [zeros, values])\n\n\n\n\ndef test_plt():\n  from PIL import Image\n  import matplotlib.pyplot as plt\n  import numpy as np\n\n  a = np.random.rand(20, 30)\n  print (a.shape)\n  # plt.subplot()\n  b = plt.imshow(a)\n  plt.show()\n\n\nif __name__ == \'__main__\':\n  test_plt()\n'"
libs/configs/__init__.py,0,b''
libs/configs/cfgs.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ngluoncv backbone\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_COCO_20190503_v5\'\nNET_NAME = \'resnet50_v1d\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nSHOW_TRAIN_INFO_INTE = 100\nSMRY_ITER = 2000\nSAVE_WEIGHTS_INTE = 80000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nCUDA9 = True\nEVAL_THRESHOLD = 0.5\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nBATCH_SIZE = 1\nWARM_SETP = int(0.25 * SAVE_WEIGHTS_INTE)\nLR = 5e-4 * 2 * 1.25 * len(GPU_GROUP.strip().split(\',\')) * BATCH_SIZE\nDECAY_STEP = [12*SAVE_WEIGHTS_INTE, 16*SAVE_WEIGHTS_INTE, 20*SAVE_WEIGHTS_INTE]\nMAX_ITERATION = 20*SAVE_WEIGHTS_INTE\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1333\nCLASS_NUM = 80\n\n# --------------------------------------------- Network_config\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\nIS_ASSIGN = True\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = True\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# --------------------------------------------NAS FPN config\nNUM_FPN = 0\nNUM_NAS_FPN = 7\nUSE_RELU = True\nFPN_CHANNEL = 384\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.5  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/detection_oprations/__init__.py,0,b''
libs/detection_oprations/anchor_target_layer_without_boxweight.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom libs.configs import cfgs\nimport numpy as np\nimport numpy.random as npr\nfrom libs.box_utils.cython_utils.cython_bbox import bbox_overlaps\nfrom libs.box_utils import encode_and_decode\n\n\ndef anchor_target_layer(\n        gt_boxes, img_shape, all_anchors, is_restrict_bg=False):\n    """"""Same as the anchor target layer in original Fast/er RCNN """"""\n\n    total_anchors = all_anchors.shape[0]\n    img_h, img_w = img_shape[1], img_shape[2]\n    gt_boxes = gt_boxes[:, :-1]  # remove class label\n\n\n    # allow boxes to sit over the edge by a small amount\n    _allowed_border = 0\n\n    # only keep anchors inside the image\n    if cfgs.IS_FILTER_OUTSIDE_BOXES:\n        inds_inside = np.where(\n            (all_anchors[:, 0] >= -_allowed_border) &\n            (all_anchors[:, 1] >= -_allowed_border) &\n            (all_anchors[:, 2] < img_w + _allowed_border) &  # width\n            (all_anchors[:, 3] < img_h + _allowed_border)  # height\n        )[0]\n    else:\n        inds_inside = range(all_anchors.shape[0])\n\n    anchors = all_anchors[inds_inside, :]\n\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    labels = np.empty((len(inds_inside),), dtype=np.float32)\n    labels.fill(-1)\n\n    # overlaps between the anchors and the gt boxes\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(anchors, dtype=np.float),\n        np.ascontiguousarray(gt_boxes, dtype=np.float))\n\n    argmax_overlaps = overlaps.argmax(axis=1)\n    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0)\n    gt_max_overlaps = overlaps[\n        gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    if not cfgs.TRAIN_RPN_CLOOBER_POSITIVES:\n        labels[max_overlaps < cfgs.RPN_IOU_NEGATIVE_THRESHOLD] = 0\n\n    labels[gt_argmax_overlaps] = 1\n    labels[max_overlaps >= cfgs.RPN_IOU_POSITIVE_THRESHOLD] = 1\n\n    if cfgs.TRAIN_RPN_CLOOBER_POSITIVES:\n        labels[max_overlaps < cfgs.RPN_IOU_NEGATIVE_THRESHOLD] = 0\n\n    num_fg = int(cfgs.RPN_MINIBATCH_SIZE * cfgs.RPN_POSITIVE_RATE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n\n    num_bg = cfgs.RPN_MINIBATCH_SIZE - np.sum(labels == 1)\n    if is_restrict_bg:\n        num_bg = max(num_bg, num_fg * 1.5)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n    # map up to original set of anchors\n    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n\n    # labels = labels.reshape((1, height, width, A))\n    rpn_labels = labels.reshape((-1, 1))\n\n    # bbox_targets\n    bbox_targets = bbox_targets.reshape((-1, 4))\n    rpn_bbox_targets = bbox_targets\n\n    return rpn_labels, rpn_bbox_targets\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count,), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # targets = bbox_transform(ex_rois, gt_rois[:, :4]).astype(\n    #     np.float32, copy=False)\n    targets = encode_and_decode.encode_boxes(unencode_boxes=gt_rois,\n                                             reference_boxes=ex_rois,\n                                             scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n    # targets = encode_and_decode.encode_boxes(ex_rois=ex_rois,\n    #                                          gt_rois=gt_rois,\n    #                                          scale_factor=None)\n    return targets\n'"
libs/detection_oprations/proposal_opr.py,6,"b'# encoding: utf-8\n""""""\n@author: zeming li\n@contact: zengarden2009@gmail.com\n""""""\n\nfrom libs.configs import cfgs\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import boxes_utils\nimport tensorflow as tf\nimport numpy as np\n\n\ndef postprocess_rpn_proposals(rpn_bbox_pred, rpn_cls_prob, img_shape, anchors, is_training):\n    \'\'\'\n\n    :param rpn_bbox_pred: [-1, 4]\n    :param rpn_cls_prob: [-1, 2]\n    :param img_shape:\n    :param anchors:[-1, 4]\n    :param is_training:\n    :return:\n    \'\'\'\n\n    if is_training:\n        pre_nms_topN = cfgs.RPN_TOP_K_NMS_TRAIN\n        post_nms_topN = cfgs.RPN_MAXIMUM_PROPOSAL_TARIN\n        # pre_nms_topN = cfgs.FPN_TOP_K_PER_LEVEL_TRAIN\n        # post_nms_topN = pre_nms_topN\n    else:\n        pre_nms_topN = cfgs.RPN_TOP_K_NMS_TEST\n        post_nms_topN = cfgs.RPN_MAXIMUM_PROPOSAL_TEST\n        # pre_nms_topN = cfgs.FPN_TOP_K_PER_LEVEL_TEST\n        # post_nms_topN = pre_nms_topN\n\n    nms_thresh = cfgs.RPN_NMS_IOU_THRESHOLD\n\n    cls_prob = rpn_cls_prob[:, 1]\n\n    # 1. decode boxes\n    decode_boxes = encode_and_decode.decode_boxes(encoded_boxes=rpn_bbox_pred,\n                                                  reference_boxes=anchors,\n                                                  scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # 2. clip to img boundaries\n    decode_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=decode_boxes,\n                                                            img_shape=img_shape)\n\n    # 3. get top N to NMS\n    if pre_nms_topN > 0:\n        pre_nms_topN = tf.minimum(pre_nms_topN, tf.shape(decode_boxes)[0], name=\'avoid_unenough_boxes\')\n        cls_prob, top_k_indices = tf.nn.top_k(cls_prob, k=pre_nms_topN)\n        decode_boxes = tf.gather(decode_boxes, top_k_indices)\n\n    # 4. NMS\n    keep = tf.image.non_max_suppression(\n        boxes=decode_boxes,\n        scores=cls_prob,\n        max_output_size=post_nms_topN,\n        iou_threshold=nms_thresh)\n\n    final_boxes = tf.gather(decode_boxes, keep)\n    final_probs = tf.gather(cls_prob, keep)\n\n    return final_boxes, final_probs\n\n'"
libs/detection_oprations/proposal_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom libs.configs import cfgs\nimport numpy as np\nimport numpy.random as npr\n\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils.cython_utils.cython_bbox import bbox_overlaps\n\n\ndef proposal_target_layer(rpn_rois, gt_boxes, fg_threshold):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    """"""\n    # Proposal ROIs (x1, y1, x2, y2) coming from RPN\n    # gt_boxes (x1, y1, x2, y2, label)\n\n    if cfgs.ADD_GTBOXES_TO_TRAIN:\n\n        num_jitter = 10\n        jitter_gtboxes = []\n        for n in range(num_jitter):\n            tmp_gtboxes = gt_boxes[:, :-1] + 0\n            w = gt_boxes[:, 2] - gt_boxes[:, 0]\n            h = gt_boxes[:, 3] - gt_boxes[:, 1]\n            scale = np.random.uniform(-0.2, 0.2, 1)\n            delta_w = np.reshape(w * scale, [-1, 1])\n            delta_h = np.reshape(h * scale, [-1, 1])\n            tmp_gtboxes[:, 0::2] = tmp_gtboxes[:, 0::2] + delta_w\n            tmp_gtboxes[:, 1::2] = tmp_gtboxes[:, 1::2] + delta_h\n            jitter_gtboxes.append(tmp_gtboxes)\n        jitter_gtboxes = np.concatenate(jitter_gtboxes, axis=0)\n        all_rois = np.vstack((rpn_rois, jitter_gtboxes))\n        # all_rois = np.vstack((rpn_rois, gt_boxes[:, :-1]))\n    else:\n        all_rois = rpn_rois\n\n    rois_per_image = np.inf if cfgs.FAST_RCNN_MINIBATCH_SIZE == -1 else cfgs.FAST_RCNN_MINIBATCH_SIZE\n\n    fg_rois_per_image = np.round(cfgs.FAST_RCNN_POSITIVE_RATE * rois_per_image)\n\n    # Sample rois with classification labels and bounding box regression\n    labels, rois, bbox_targets, gtboxes = _sample_rois(all_rois, gt_boxes, fg_rois_per_image,\n                                                       rois_per_image, cfgs.CLASS_NUM+1, fg_threshold)\n\n    rois = rois.reshape(-1, 4)\n    labels = labels.reshape(-1)\n    bbox_targets = bbox_targets.reshape(-1, (cfgs.CLASS_NUM+1) * 4)\n    gtboxes = gtboxes.reshape(-1, 4)\n\n    return rois, labels, bbox_targets, gtboxes\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets (bbox_target_data) are stored in a\n    compact form N x (class, tx, ty, tw, th)\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets).\n\n    Returns:\n        bbox_target (ndarray): N x 4K blob of regression targets\n    """"""\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = int(4 * cls)\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n\n    return bbox_targets\n\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n    """"""Compute bounding-box regression targets for an image.\n    that is : [label, tx, ty, tw, th]\n    """"""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 4\n\n    targets = encode_and_decode.encode_boxes(unencode_boxes=gt_rois,\n                                             reference_boxes=ex_rois,\n                                             scale_factors=cfgs.ROI_SCALE_FACTORS)\n    # targets = encode_and_decode.encode_boxes(ex_rois=ex_rois,\n    #                                          gt_rois=gt_rois,\n    #                                          scale_factor=cfgs.ROI_SCALE_FACTORS)\n\n    return np.hstack(\n        (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n\n\ndef _sample_rois(all_rois, gt_boxes, fg_rois_per_image,\n                 rois_per_image, num_classes, fg_threshold):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n\n    all_rois shape is [-1, 4]\n    gt_boxes shape is [-1, 5]. that is [x1, y1, x2, y2, label]\n    """"""\n    # overlaps: (rois x gt_boxes)\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(all_rois, dtype=np.float),\n        np.ascontiguousarray(gt_boxes[:, :-1], dtype=np.float))\n    gt_assignment = overlaps.argmax(axis=1)\n    max_overlaps = overlaps.max(axis=1)\n    labels = gt_boxes[gt_assignment, -1]\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    # fg_inds = np.where(max_overlaps >= cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD)[0]\n    fg_inds = np.where(max_overlaps >= fg_threshold)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD) &\n                       (max_overlaps >= cfgs.FAST_RCNN_IOU_NEGATIVE_THRESHOLD))[0]\n    # print(""first fileter, fg_size: {} || bg_size: {}"".format(fg_inds.shape, bg_inds.shape))\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = min(fg_rois_per_image, fg_inds.size)\n\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=int(fg_rois_per_this_image), replace=False)\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=int(bg_rois_per_this_image), replace=False)\n\n    # print(""second fileter, fg_size: {} || bg_size: {}"".format(fg_inds.shape, bg_inds.shape))\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n\n\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n\n    # Clamp labels for the background RoIs to 0\n    labels[int(fg_rois_per_this_image):] = 0\n    rois = all_rois[keep_inds]\n\n    bbox_target_data = _compute_targets(\n        rois, gt_boxes[gt_assignment[keep_inds], :-1], labels)\n\n    bbox_targets = \\\n        _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n    return labels, rois, bbox_targets, gt_boxes[gt_assignment[keep_inds], :-1]\n'"
libs/export_pbs/__init__.py,0,b''
libs/export_pbs/exportPb.py,16,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport os, sys\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.tools import freeze_graph\n\nsys.path.append(\'../../\')\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\n\nCKPT_PATH = \'/data/code/yxdet/FPN_TF_DEV/output/trained_weights/FPN_Res50_COCO_20190211_v18/voc_1599999model.ckpt\'\nOUT_DIR = \'../../output/Pbs\'\nPB_NAME = \'FPN_Res50_COCO.pb\'\n\n\ndef build_detection_graph():\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3],\n                              name=\'input_img\')  # is RGB. not GBR\n    raw_shape = tf.shape(img_plac)\n    raw_h, raw_w = tf.to_float(raw_shape[0]), tf.to_float(raw_shape[1])\n\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)  # [1, None, None, 3]\n\n    det_net = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                   is_training=False)\n\n    detected_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch=None)\n\n    xmin, ymin, xmax, ymax = detected_boxes[:, 0], detected_boxes[:, 1], \\\n                             detected_boxes[:, 2], detected_boxes[:, 3]\n\n    resized_shape = tf.shape(img_batch)\n    resized_h, resized_w = tf.to_float(resized_shape[1]), tf.to_float(resized_shape[2])\n\n    xmin = xmin * raw_w / resized_w\n    xmax = xmax * raw_w / resized_w\n\n    ymin = ymin * raw_h / resized_h\n    ymax = ymax * raw_h / resized_h\n\n    boxes = tf.transpose(tf.stack([xmin, ymin, xmax, ymax]))\n    dets = tf.concat([tf.reshape(detection_category, [-1, 1]),\n                     tf.reshape(detection_scores, [-1, 1]),\n                     boxes], axis=1, name=\'DetResults\')\n\n    return dets\n\n\ndef export_frozenPB():\n\n    tf.reset_default_graph()\n\n    dets = build_detection_graph()\n\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        print(""we have restred the weights from =====>>\\n"", CKPT_PATH)\n        saver.restore(sess, CKPT_PATH)\n\n        tf.train.write_graph(sess.graph_def, OUT_DIR, PB_NAME)\n        freeze_graph.freeze_graph(input_graph=os.path.join(OUT_DIR, PB_NAME),\n                                  input_saver=\'\',\n                                  input_binary=False,\n                                  input_checkpoint=CKPT_PATH,\n                                  output_node_names=""DetResults"",\n                                  restore_op_name=""save/restore_all"",\n                                  filename_tensor_name=\'save/Const:0\',\n                                  output_graph=os.path.join(OUT_DIR, PB_NAME.replace(\'.pb\', \'_Frozen.pb\')),\n                                  clear_devices=False,\n                                  initializer_nodes=\'\')\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'\'\n    export_frozenPB()\n'"
libs/export_pbs/test_TensorRT.py,8,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport tensorflow.contrib.tensorrt as trt\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nsys.path.append(\'../../\')\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\ndef load_graph(frozen_graph_file):\n\n    # we parse the graph_def file\n    with tf.gfile.GFile(frozen_graph_file, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # we load the graph_def in the default graph\n\n    graph_def = trt.create_inference_graph(graph_def, [""DetResults""],\n                                           max_batch_size=1000,\n                                           max_workspace_size_bytes=(1 << 10)*10000,\n                                           precision_mode=""INT8"",\n                                           maximum_cached_engines=10)  # Get optimized graph\n\n    # graph_def = trt.calib_graph_to_infer_graph(graph_def)\n    tf.reset_default_graph()\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def,\n                            input_map=None,\n                            return_elements=None,\n                            name="""",\n                            op_dict=None,\n                            producer_op_list=None)\n    return graph\n\n\ndef test(frozen_graph_path, test_dir):\n\n    graph = load_graph(frozen_graph_path)\n    print(""we are testing ====>>>>"", frozen_graph_path)\n\n    img = graph.get_tensor_by_name(""input_img:0"")\n    dets = graph.get_tensor_by_name(""DetResults:0"")\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n    config = tf.ConfigProto(gpu_options=gpu_options)\n\n    with tf.Session(graph=graph, config=config) as sess:\n        for img_path in os.listdir(test_dir):\n            a_img = cv2.imread(os.path.join(test_dir, img_path))[:, :, ::-1]\n            st = time.time()\n            dets_val = sess.run(dets, feed_dict={img: a_img})\n            end = time.time()\n\n            show_indices = dets_val[:, 1] >= 0.5\n            dets_val = dets_val[show_indices]\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(a_img,\n                                                                                boxes=dets_val[:, 2:],\n                                                                                labels=dets_val[:, 0],\n                                                                                scores=dets_val[:, 1])\n            cv2.imwrite(img_path,\n                        final_detections[:, :, ::-1])\n            print(""%s cost time: %f"" % (img_path, end - st))\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    test(\'/data/code/yxdet/FPN_TF_DEV/output/Pbs/FPN_Res50_COCO_Frozen.pb\',\n         \'/data/COCO/train2017\')\n\n\n\n\n\n\n\n\n\n\n\n'"
libs/export_pbs/test_exportPb.py,5,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport tensorflow.contrib.tensorrt as trt\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nsys.path.append(\'../../\')\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\ndef load_graph(frozen_graph_file):\n\n    # we parse the graph_def file\n    with tf.gfile.GFile(frozen_graph_file, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def,\n                            input_map=None,\n                            return_elements=None,\n                            name="""",\n                            op_dict=None,\n                            producer_op_list=None)\n    return graph\n\n\ndef test(frozen_graph_path, test_dir):\n\n    graph = load_graph(frozen_graph_path)\n    print(""we are testing ====>>>>"", frozen_graph_path)\n\n    img = graph.get_tensor_by_name(""input_img:0"")\n    dets = graph.get_tensor_by_name(""DetResults:0"")\n\n    with tf.Session(graph=graph) as sess:\n        for img_path in os.listdir(test_dir):\n            a_img = cv2.imread(os.path.join(test_dir, img_path))[:, :, ::-1]\n            st = time.time()\n            dets_val = sess.run(dets, feed_dict={img: a_img})\n\n            show_indices = dets_val[:, 1] >= 0.5\n            dets_val = dets_val[show_indices]\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(a_img,\n                                                                                boxes=dets_val[:, 2:],\n                                                                                labels=dets_val[:, 0],\n                                                                                scores=dets_val[:, 1])\n            cv2.imwrite(img_path,\n                        final_detections[:, :, ::-1])\n            print(""%s cost time: %f"" % (img_path, time.time() - st))\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    test(\'/data/code/yxdet/FPN_TF_DEV/output/Pbs/FPN_Res50_COCO_Frozen.pb\',\n         \'/data/COCO/train2017\')\n\n\n\n\n\n\n\n\n\n\n\n'"
libs/label_name_dict/__init__.py,0,b''
libs/label_name_dict/coco_dict.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nclass_names = [\n    'back_ground', 'person', 'bicycle', 'car', 'motorcycle',\n    'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n    'fire hydrant', 'stop sign', 'parking meter', 'bench',\n    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n    'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n    'sports ball', 'kite', 'baseball bat', 'baseball glove',\n    'skateboard', 'surfboard', 'tennis racket', 'bottle',\n    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n    'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear',\n    'hair drier', 'toothbrush']\n\n\nclasses_originID = {\n    'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4,\n    'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9,\n    'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13,\n    'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17,\n    'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22,\n    'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27,\n    'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33,\n    'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37,\n    'kite': 38, 'baseball bat': 39, 'baseball glove': 40,\n    'skateboard': 41, 'surfboard': 42, 'tennis racket': 43,\n    'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48,\n    'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53,\n    'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57,\n    'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61,\n    'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65,\n    'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73,\n    'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77,\n    'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81,\n    'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86,\n    'scissors': 87, 'teddy bear': 88, 'hair drier': 89,\n    'toothbrush': 90}\n\noriginID_classes = {item: key for key, item in classes_originID.items()}\nNAME_LABEL_MAP = dict(zip(class_names, range(len(class_names))))\nLABEL_NAME_MAP = dict(zip(range(len(class_names)), class_names))\n\n# print (originID_classes)\n\n\n\n"""
libs/label_name_dict/label_dict.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nfrom libs.configs import cfgs\n\n\nclass_names = [\n        'back_ground', 'person', 'bicycle', 'car', 'motorcycle',\n        'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n        'fire hydrant', 'stop sign', 'parking meter', 'bench',\n        'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n        'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n        'sports ball', 'kite', 'baseball bat', 'baseball glove',\n        'skateboard', 'surfboard', 'tennis racket', 'bottle',\n        'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n        'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n        'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n        'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n        'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n        'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n        'book', 'clock', 'vase', 'scissors', 'teddy bear',\n        'hair drier', 'toothbrush']\n\nclasses_originID = {\n    'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4,\n    'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9,\n    'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13,\n    'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17,\n    'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22,\n    'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27,\n    'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33,\n    'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37,\n    'kite': 38, 'baseball bat': 39, 'baseball glove': 40,\n    'skateboard': 41, 'surfboard': 42, 'tennis racket': 43,\n    'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48,\n    'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53,\n    'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57,\n    'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61,\n    'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65,\n    'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73,\n    'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77,\n    'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81,\n    'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86,\n    'scissors': 87, 'teddy bear': 88, 'hair drier': 89,\n    'toothbrush': 90}\n\n\ndef get_coco_label_dict():\n    originID_classes = {item: key for key, item in classes_originID.items()}\n    NAME_LABEL_MAP = dict(zip(class_names, range(len(class_names))))\n    return NAME_LABEL_MAP\n\nif cfgs.DATASET_NAME == 'ship':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'ship': 1\n    }\nelif cfgs.DATASET_NAME == 'aeroplane':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1\n    }\nelif cfgs.DATASET_NAME == 'WIDER':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'face': 1\n    }\nelif cfgs.DATASET_NAME == 'icdar':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'text': 1\n    }\nelif cfgs.DATASET_NAME.startswith('DOTA'):\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'roundabout': 1,\n        'tennis-court': 2,\n        'swimming-pool': 3,\n        'storage-tank': 4,\n        'soccer-ball-field': 5,\n        'small-vehicle': 6,\n        'ship': 7,\n        'plane': 8,\n        'large-vehicle': 9,\n        'helicopter': 10,\n        'harbor': 11,\n        'ground-track-field': 12,\n        'bridge': 13,\n        'basketball-court': 14,\n        'baseball-diamond': 15\n    }\nelif cfgs.DATASET_NAME.startswith('DOAI2019'):\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'turntable': 1,\n        'tennis-court': 2,\n        'swimming-pool': 3,\n        'storage-tank': 4,\n        'soccer-ball-field': 5,\n        'small-vehicle': 6,\n        'ship': 7,\n        'plane': 8,\n        'large-vehicle': 9,\n        'helicopter': 10,\n        'harbor': 11,\n        'ground-track-field': 12,\n        'bridge': 13,\n        'basketball-court': 14,\n        'baseball-diamond': 15,\n        'container-crane': 16\n    }\nelif cfgs.DATASET_NAME == 'coco':\n    NAME_LABEL_MAP = get_coco_label_dict()\nelif cfgs.DATASET_NAME == 'pascal':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1,\n        'bicycle': 2,\n        'bird': 3,\n        'boat': 4,\n        'bottle': 5,\n        'bus': 6,\n        'car': 7,\n        'cat': 8,\n        'chair': 9,\n        'cow': 10,\n        'diningtable': 11,\n        'dog': 12,\n        'horse': 13,\n        'motorbike': 14,\n        'person': 15,\n        'pottedplant': 16,\n        'sheep': 17,\n        'sofa': 18,\n        'train': 19,\n        'tvmonitor': 20\n    }\nelif cfgs.DATASET_NAME == 'bdd100k':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'bus': 1,\n        'traffic light': 2,\n        'traffic sign': 3,\n        'person': 4,\n        'bike': 5,\n        'truck': 6,\n        'motor': 7,\n        'car': 8, \n        'train': 9,\n        'rider': 10\n    }\nelse:\n    assert 'please set label dict!'\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\n\nLABEl_NAME_MAP = get_label_name_map()"""
libs/losses/__init__.py,0,b''
libs/losses/losses.py,61,"b'# -*- coding: utf-8 -*-\n""""""\n@author: jemmy li\n@contact: zengarden2009@gmail.com\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom libs.box_utils import encode_and_decode\nfrom libs.configs import cfgs\n\n\ndef _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4] in RPN. [-1, cls_num+1, 4] in Fast-rcnn\n    :param bbox_targets: shape is same as bbox_pred\n    :param sigma:\n    :return:\n    \'\'\'\n    sigma_2 = sigma**2\n\n    box_diff = bbox_pred - bbox_targets\n\n    abs_box_diff = tf.abs(box_diff)\n\n    smoothL1_sign = tf.stop_gradient(\n        tf.to_float(tf.less(abs_box_diff, 1. / sigma_2)))\n    loss_box = tf.pow(box_diff, 2) * (sigma_2 / 2.0) * smoothL1_sign \\\n               + (abs_box_diff - (0.5 / sigma_2)) * (1.0 - smoothL1_sign)\n    return loss_box\n\ndef smooth_l1_loss_rpn(bbox_pred, bbox_targets, label, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4]\n    :param bbox_targets: [-1, 4]\n    :param label: [-1]\n    :param sigma:\n    :return:\n    \'\'\'\n    value = _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=sigma)\n    value = tf.reduce_sum(value, axis=1)  # to sum in axis 1\n    rpn_positive = tf.where(tf.greater(label, 0))\n\n    # rpn_select = tf.stop_gradient(rpn_select) # to avoid\n    selected_value = tf.gather(value, rpn_positive)\n    non_ignored_mask = tf.stop_gradient(\n        1.0 - tf.to_float(tf.equal(label, -1)))  # positve is 1.0 others is 0.0\n\n    bbox_loss = tf.reduce_sum(selected_value) / tf.maximum(1.0, tf.reduce_sum(non_ignored_mask))\n\n    return bbox_loss\n\ndef smooth_l1_loss_rcnn(bbox_pred, bbox_targets, label, num_classes, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, (cfgs.CLS_NUM +1) * 4]\n    :param bbox_targets:[-1, (cfgs.CLS_NUM +1) * 4]\n    :param label:[-1]\n    :param num_classes:\n    :param sigma:\n    :return:\n    \'\'\'\n\n    outside_mask = tf.stop_gradient(tf.to_float(tf.greater(label, 0)))\n\n    bbox_pred = tf.reshape(bbox_pred, [-1, num_classes, 4])\n    bbox_targets = tf.reshape(bbox_targets, [-1, num_classes, 4])\n\n    value = _smooth_l1_loss_base(bbox_pred,\n                                 bbox_targets,\n                                 sigma=sigma)\n    value = tf.reduce_sum(value, 2)\n    value = tf.reshape(value, [-1, num_classes])\n\n    inside_mask = tf.one_hot(tf.reshape(label, [-1, 1]),\n                             depth=num_classes, axis=1)\n\n    inside_mask = tf.stop_gradient(\n        tf.to_float(tf.reshape(inside_mask, [-1, num_classes])))\n\n    normalizer = tf.to_float(tf.shape(bbox_pred)[0])\n    bbox_loss = tf.reduce_sum(\n        tf.reduce_sum(value * inside_mask, 1)*outside_mask) / normalizer\n\n    return bbox_loss\n\ndef smooth_l1_loss_rcnn_iou(bbox_pred, bbox_targets, label, ious, num_classes, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, (cfgs.CLS_NUM +1) * 4]\n    :param bbox_targets:[-1, (cfgs.CLS_NUM +1) * 4]\n    :param label:[-1]\n    :param num_classes:\n    :param sigma:\n    :return:\n    \'\'\'\n\n    outside_mask = tf.stop_gradient(tf.to_float(tf.greater(label, 0)))\n\n    ious = tf.reshape(ious, [-1, ])\n    ious = tf.stop_gradient(ious)\n\n    bbox_pred = tf.reshape(bbox_pred, [-1, num_classes, 4])\n    bbox_targets = tf.reshape(bbox_targets, [-1, num_classes, 4])\n\n    value = _smooth_l1_loss_base(bbox_pred,\n                                 bbox_targets,\n                                 sigma=sigma)\n    value = tf.reduce_sum(value, 2)\n    value = tf.reshape(value, [-1, num_classes])\n\n    inside_mask = tf.one_hot(tf.reshape(label, [-1, 1]),\n                             depth=num_classes, axis=1)\n\n    inside_mask = tf.stop_gradient(\n        tf.to_float(tf.reshape(inside_mask, [-1, num_classes])))\n\n    normalizer = tf.to_float(tf.shape(bbox_pred)[0])\n\n    tmp = tf.reduce_sum(value * inside_mask, 1)\n    tmp = tf.stop_gradient(tmp)\n    iou_factor = (1 - ious) / tmp\n    iou_factor = tf.stop_gradient(iou_factor)\n\n    bbox_loss = tf.reduce_sum(\n        tf.reduce_sum(value * inside_mask, 1)*outside_mask*iou_factor) / normalizer\n\n    return bbox_loss\n\n\ndef iou_loss_(label, ious):\n    outside_mask = tf.stop_gradient(tf.to_float(tf.greater(label, 0)))\n\n    ious = tf.reshape(ious, [-1, ])\n\n    normalizer = tf.to_float(tf.shape(ious)[0])\n\n    bbox_loss = tf.reduce_sum(-tf.log(ious+1e-5) * outside_mask) / normalizer\n\n    return bbox_loss\n\n\ndef iou_loss(bbox_pred, bbox_targets, gtbox, label, num_classes):\n    """"""\n    :param bbox_pred: [-1, (cfgs.CLS_NUM +1) * 4]\n    :param bbox_targets: [-1, (cfgs.CLS_NUM +1) * 4]\n    :param gtbox: [-1, 4]\n    :param label: [-1]\n    :param num_classes:\n    :return:\n    """"""\n\n    gtbox = tf.tile(gtbox, [1, num_classes])\n    bbox_pred = tf.reshape(bbox_pred, [-1, 4])\n    bbox_targets = tf.reshape(bbox_targets, [-1, 4])\n    gtbox = tf.reshape(gtbox, [-1, 4])\n    pred_box = encode_and_decode.decode_boxes(bbox_pred, gtbox, scale_factors=cfgs.ROI_SCALE_FACTORS)\n    gt_box = encode_and_decode.decode_boxes(bbox_targets, gtbox, scale_factors=cfgs.ROI_SCALE_FACTORS)\n\n    inside_mask = tf.one_hot(tf.reshape(label, [-1, 1]),\n                             depth=num_classes, axis=1)\n\n    inside_mask = tf.reshape(inside_mask, [-1, ])\n    iou = iou_calculate(pred_box, gt_box)\n    iou_loss = tf.reduce_mean(-tf.log(iou*inside_mask+1e-5))\n\n    pred = tf.cast(tf.greater(iou, 0.5), tf.float32)\n    pred = tf.reshape(pred, [-1, num_classes])\n    pred_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label))\n\n    loss = iou_loss * 0.1 + pred_loss * 0.0\n    return loss\n\ndef iou_calculate(boxes_1, boxes_2):\n\n    with tf.name_scope(\'iou_caculate\'):\n\n        xmin_1, ymin_1, xmax_1, ymax_1 = tf.unstack(boxes_1, axis=1)  # ymin_1 shape is [N, 1]..\n\n        xmin_2, ymin_2, xmax_2, ymax_2 = tf.unstack(boxes_2, axis=1)  # ymin_2 shape is [M, ]..\n\n        max_xmin = tf.maximum(xmin_1, xmin_2)\n        min_xmax = tf.minimum(xmax_1, xmax_2)\n\n        max_ymin = tf.maximum(ymin_1, ymin_2)\n        min_ymax = tf.minimum(ymax_1, ymax_2)\n\n        overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n        overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n        overlaps = overlap_h * overlap_w\n\n        area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n        area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n        iou = overlaps / (area_1 + area_2 - overlaps)\n\n        return iou\n\n\n\ndef sum_ohem_loss(cls_score, label, bbox_pred, bbox_targets,\n                  nr_ohem_sampling, nr_classes, sigma=1.0):\n\n    raise NotImplementedError(\'not implement Now. YJR will implemetn in the future\')'"
libs/networks/__init__.py,0,b''
libs/networks/build_whole_network.py,130,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv, resnet_gluoncv_v1\nfrom libs.networks import mobilenet_v2\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import boxes_utils\nfrom libs.box_utils import anchor_utils\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.proposal_opr import postprocess_rpn_proposals\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.proposal_target_layer import proposal_target_layer\nfrom libs.box_utils import mask_utils\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n        elif self.base_network_name in [\'resnet101_v1d\', \'resnet50_v1d\']:\n            return resnet_gluoncv.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                              is_training=self.is_training)\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet or mobilenet_v2\')\n\n    def postprocess_fastrcnn(self, rois, bbox_ppred, scores, img_shape):\n        \'\'\'\n\n        :param rois:[-1, 4]\n        :param bbox_ppred: [-1, (cfgs.Class_num+1) * 4]\n        :param scores: [-1, cfgs.Class_num + 1]\n        :return:\n        \'\'\'\n\n        with tf.name_scope(\'postprocess_fastrcnn\'):\n            rois = tf.stop_gradient(rois)\n            scores = tf.stop_gradient(scores)\n            bbox_ppred = tf.reshape(bbox_ppred, [-1, cfgs.CLASS_NUM + 1, 4])\n            bbox_ppred = tf.stop_gradient(bbox_ppred)\n\n            bbox_pred_list = tf.unstack(bbox_ppred, axis=1)\n            score_list = tf.unstack(scores, axis=1)\n\n            allclasses_boxes = []\n            allclasses_scores = []\n            categories = []\n            for i in range(1, cfgs.CLASS_NUM + 1):\n                # 1. decode boxes in each class\n                tmp_encoded_box = bbox_pred_list[i]\n                tmp_score = score_list[i]\n                tmp_decoded_boxes = encode_and_decode.decode_boxes(encoded_boxes=tmp_encoded_box,\n                                                                   reference_boxes=rois,\n                                                                   scale_factors=cfgs.ROI_SCALE_FACTORS)\n                # tmp_decoded_boxes = encode_and_decode.decode_boxes(boxes=rois,\n                #                                                    deltas=tmp_encoded_box,\n                #                                                    scale_factor=cfgs.ROI_SCALE_FACTORS)\n\n                # 2. clip to img boundaries\n                tmp_decoded_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=tmp_decoded_boxes,\n                                                                             img_shape=img_shape)\n\n                # 3. NMS\n                keep = tf.image.non_max_suppression(\n                    boxes=tmp_decoded_boxes,\n                    scores=tmp_score,\n                    max_output_size=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                    iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD)\n\n                perclass_boxes = tf.gather(tmp_decoded_boxes, keep)\n                perclass_scores = tf.gather(tmp_score, keep)\n\n                allclasses_boxes.append(perclass_boxes)\n                allclasses_scores.append(perclass_scores)\n                categories.append(tf.ones_like(perclass_scores) * i)\n\n            final_boxes = tf.concat(allclasses_boxes, axis=0)\n            final_scores = tf.concat(allclasses_scores, axis=0)\n            final_category = tf.concat(categories, axis=0)\n\n            if self.is_training:\n                \'\'\'\n                in training. We should show the detecitons in the tensorboard. So we add this.\n                \'\'\'\n                kept_indices = tf.reshape(tf.where(tf.greater_equal(final_scores, cfgs.SHOW_SCORE_THRSHOLD)), [-1])\n\n                final_boxes = tf.gather(final_boxes, kept_indices)\n                final_scores = tf.gather(final_scores, kept_indices)\n                final_category = tf.gather(final_category, kept_indices)\n\n        return final_boxes, final_scores, final_category\n\n    def roi_pooling(self, feature_maps, rois, img_shape, scope):\n        \'\'\'\n        Here use roi warping as roi_pooling\n\n        :param featuremaps_dict: feature map to crop\n        :param rois: shape is [-1, 4]. [x1, y1, x2, y2]\n        :return:\n        \'\'\'\n\n        with tf.variable_scope(\'ROI_Warping_\' + scope):\n            img_h, img_w = tf.cast(img_shape[1], tf.float32), tf.cast(img_shape[2], tf.float32)\n            N = tf.shape(rois)[0]\n            x1, y1, x2, y2 = tf.unstack(rois, axis=1)\n\n            normalized_x1 = x1 / img_w\n            normalized_x2 = x2 / img_w\n            normalized_y1 = y1 / img_h\n            normalized_y2 = y2 / img_h\n\n            normalized_rois = tf.transpose(\n                tf.stack([normalized_y1, normalized_x1, normalized_y2, normalized_x2]), name=\'get_normalized_rois\')\n\n            normalized_rois = tf.stop_gradient(normalized_rois)\n\n            cropped_roi_features = tf.image.crop_and_resize(feature_maps, normalized_rois,\n                                                            box_ind=tf.zeros(shape=[N, ],\n                                                                             dtype=tf.int32),\n                                                            crop_size=[cfgs.ROI_SIZE, cfgs.ROI_SIZE],\n                                                            name=\'CROP_AND_RESIZE\'\n                                                            )\n            roi_features = slim.max_pool2d(cropped_roi_features,\n                                           [cfgs.ROI_POOL_KERNEL_SIZE, cfgs.ROI_POOL_KERNEL_SIZE],\n                                           stride=cfgs.ROI_POOL_KERNEL_SIZE)\n\n        return roi_features\n\n    def build_fastrcnn(self, P_list, rois_list, img_shape):\n\n        with tf.variable_scope(\'Fast-RCNN\'):\n            # 5. ROI Pooling\n            with tf.variable_scope(\'rois_pooling\'):\n                pooled_features_list = []\n                for level_name, p, rois in zip(cfgs.LEVLES, P_list, rois_list):  # exclude P6_rois\n                    # p = tf.Print(p, [tf.shape(p)], summarize=10, message=level_name+\'SHPAE***\')\n                    pooled_features = self.roi_pooling(feature_maps=p, rois=rois, img_shape=img_shape,\n                                                       scope=level_name)\n                    pooled_features_list.append(pooled_features)\n\n                pooled_features = tf.concat(pooled_features_list, axis=0)  # [minibatch_size, H, W, C]\n\n            # 6. inferecne rois in Fast-RCNN to obtain fc_flatten features\n            if self.base_network_name.startswith(\'resnet\'):\n                fc_flatten = resnet.restnet_head(inputs=pooled_features,\n                                                 is_training=self.is_training,\n                                                 scope_name=self.base_network_name)\n            elif self.base_network_name.startswith(\'Mobile\'):\n                fc_flatten = mobilenet_v2.mobilenetv2_head(inputs=pooled_features,\n                                                           is_training=self.is_training)\n            else:\n                raise NotImplementedError(\'only support resnet and mobilenet\')\n\n            # 7. cls and reg in Fast-RCNN\n            with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n\n                cls_score = slim.fully_connected(fc_flatten,\n                                                 num_outputs=cfgs.CLASS_NUM + 1,\n                                                 weights_initializer=cfgs.INITIALIZER,\n                                                 activation_fn=None, trainable=self.is_training,\n                                                 scope=\'cls_fc\')\n\n                bbox_pred = slim.fully_connected(fc_flatten,\n                                                 num_outputs=(cfgs.CLASS_NUM + 1) * 4,\n                                                 weights_initializer=cfgs.BBOX_INITIALIZER,\n                                                 activation_fn=None, trainable=self.is_training,\n                                                 scope=\'reg_fc\')\n                # for convient. It also produce (cls_num +1) bboxes\n\n                cls_score = tf.reshape(cls_score, [-1, cfgs.CLASS_NUM + 1])\n                bbox_pred = tf.reshape(bbox_pred, [-1, 4 * (cfgs.CLASS_NUM + 1)])\n\n                return bbox_pred, cls_score\n\n    def assign_levels(self, all_rois, labels=None, bbox_targets=None):\n        \'\'\'\n\n        :param all_rois:\n        :param labels:\n        :param bbox_targets:\n        :return:\n        \'\'\'\n        with tf.name_scope(\'assign_levels\'):\n            # all_rois = tf.Print(all_rois, [tf.shape(all_rois)], summarize=10, message=\'ALL_ROIS_SHAPE*****\')\n            xmin, ymin, xmax, ymax = tf.unstack(all_rois, axis=1)\n\n            h = tf.maximum(0., ymax - ymin)\n            w = tf.maximum(0., xmax - xmin)\n\n            levels = tf.floor(4. + tf.log(tf.sqrt(w * h + 1e-8) / 224.0) / tf.log(2.))  # 4 + log_2(***)\n            # use floor instead of round\n\n            min_level = int(cfgs.LEVLES[0][-1])\n            max_level = min(5, int(cfgs.LEVLES[-1][-1]))\n            levels = tf.maximum(levels, tf.ones_like(levels) * min_level)  # level minimum is 2\n            levels = tf.minimum(levels, tf.ones_like(levels) * max_level)  # level maximum is 5\n\n            levels = tf.stop_gradient(tf.reshape(levels, [-1]))\n\n            def get_rois(levels, level_i, rois, labels, bbox_targets):\n\n                level_i_indices = tf.reshape(tf.where(tf.equal(levels, level_i)), [-1])\n\n                tf.summary.scalar(\'LEVEL/LEVEL_%d_rois_NUM\' % level_i, tf.shape(level_i_indices)[0])\n                level_i_rois = tf.gather(rois, level_i_indices)\n\n                if self.is_training:\n                    if cfgs.CUDA9:\n                        # Note: for cuda 9\n                        level_i_rois = tf.stop_gradient(level_i_rois)\n                        level_i_labels = tf.gather(labels, level_i_indices)\n\n                        level_i_targets = tf.gather(bbox_targets, level_i_indices)\n                    else:\n\n                        # Note: for cuda 8\n                        level_i_rois = tf.stop_gradient(tf.concat([level_i_rois, [[0, 0, 0., 0.]]], axis=0))\n                        # to avoid the num of level i rois is 0.0, which will broken the BP in tf\n\n                        level_i_labels = tf.gather(labels, level_i_indices)\n                        level_i_labels = tf.stop_gradient(tf.concat([level_i_labels, [0]], axis=0))\n\n                        level_i_targets = tf.gather(bbox_targets, level_i_indices)\n                        level_i_targets = tf.stop_gradient(tf.concat([level_i_targets,\n                                                                      tf.zeros(shape=(1, 4 * (cfgs.CLASS_NUM + 1)),\n                                                                               dtype=tf.float32)], axis=0))\n\n                    return level_i_rois, level_i_labels, level_i_targets\n                else:\n                    if not cfgs.CUDA9:\n                        # Note: for cuda 8\n                        level_i_rois = tf.concat([level_i_rois, [[0, 0, 0., 0.]]], axis=0)\n                    return level_i_rois, None, None\n\n            rois_list = []\n            labels_list = []\n            targets_list = []\n            for i in range(min_level, max_level + 1):\n                P_i_rois, P_i_labels, P_i_targets = get_rois(levels, level_i=i, rois=all_rois,\n                                                             labels=labels,\n                                                             bbox_targets=bbox_targets)\n                rois_list.append(P_i_rois)\n                labels_list.append(P_i_labels)\n                targets_list.append(P_i_targets)\n\n            if self.is_training:\n                all_labels = tf.concat(labels_list, axis=0)\n                all_targets = tf.concat(targets_list, axis=0)\n                return rois_list, all_labels, all_targets\n            else:\n                return rois_list  # [P2_rois, P3_rois, P4_rois, P5_rois] Note: P6 do not assign rois\n\n    def add_anchor_img_smry(self, img, anchors, labels):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor)\n        neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def add_roi_batch_img_smry(self, img, rois, labels):\n        positive_roi_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n\n        negative_roi_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        pos_roi = tf.gather(rois, positive_roi_indices)\n        neg_roi = tf.gather(rois, negative_roi_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=pos_roi)\n        neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=neg_roi)\n        tf.summary.image(\'pos_rois\', pos_in_img)\n        tf.summary.image(\'neg_rois\', neg_in_img)\n\n    def build_loss(self, rpn_box_pred, rpn_bbox_targets, rpn_cls_score, rpn_labels,\n                   bbox_pred, bbox_targets, cls_score, labels):\n        \'\'\'\n\n        :param rpn_box_pred: [-1, 4]\n        :param rpn_bbox_targets: [-1, 4]\n        :param rpn_cls_score: [-1]\n        :param rpn_labels: [-1]\n        :param bbox_pred: [-1, 4*(cls_num+1)]\n        :param bbox_targets: [-1, 4*(cls_num+1)]\n        :param cls_score: [-1, cls_num+1]\n        :param labels: [-1]\n        :return:\n        \'\'\'\n        with tf.variable_scope(\'build_loss\') as sc:\n\n            with tf.variable_scope(\'rpn_loss\'):\n\n                rpn_bbox_loss = losses.smooth_l1_loss_rpn(bbox_pred=rpn_box_pred,\n                                                          bbox_targets=rpn_bbox_targets,\n                                                          label=rpn_labels,\n                                                          sigma=cfgs.RPN_SIGMA)\n\n                rpn_select = tf.reshape(tf.where(tf.not_equal(rpn_labels, -1)), [-1])\n                rpn_cls_score = tf.reshape(tf.gather(rpn_cls_score, rpn_select), [-1, 2])\n                rpn_labels = tf.reshape(tf.gather(rpn_labels, rpn_select), [-1])\n                rpn_cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rpn_cls_score,\n                                                                                             labels=rpn_labels))\n\n                rpn_cls_loss = rpn_cls_loss * cfgs.RPN_CLASSIFICATION_LOSS_WEIGHT\n                rpn_bbox_loss = rpn_bbox_loss * cfgs.RPN_LOCATION_LOSS_WEIGHT\n\n            with tf.variable_scope(\'FastRCNN_loss\'):\n                if not cfgs.FAST_RCNN_MINIBATCH_SIZE == -1:\n                    bbox_loss = losses.smooth_l1_loss_rcnn(bbox_pred=bbox_pred,\n                                                           bbox_targets=bbox_targets,\n                                                           label=labels,\n                                                           num_classes=cfgs.CLASS_NUM + 1,\n                                                           sigma=cfgs.FASTRCNN_SIGMA)\n                    # iou_loss = losses.iou_loss(bbox_pred, bbox_targets, gtbox, labels, cfgs.CLASS_NUM + 1)\n\n                    # cls_score = tf.reshape(cls_score, [-1, cfgs.CLASS_NUM + 1])\n                    # labels = tf.reshape(labels, [-1])\n                    cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                        logits=cls_score,\n                        labels=labels))  # beacause already sample before\n                else:\n                    \'\'\' \n                    applying OHEM here\n                    \'\'\'\n                    print(20 * ""@@"")\n                    print(""@@"" + 10 * "" "" + ""TRAIN WITH OHEM ..."")\n                    print(20 * ""@@"")\n                    cls_loss = bbox_loss = losses.sum_ohem_loss(\n                        cls_score=cls_score,\n                        label=labels,\n                        bbox_targets=bbox_targets,\n                        nr_ohem_sampling=128,\n                        nr_classes=cfgs.CLASS_NUM + 1)\n                cls_loss = cls_loss * cfgs.FAST_RCNN_CLASSIFICATION_LOSS_WEIGHT\n                # bbox_loss = bbox_loss * cfgs.FAST_RCNN_LOCATION_LOSS_WEIGHT\n            loss_dict = {\n                \'rpn_cls_loss\': rpn_cls_loss,\n                \'rpn_loc_loss\': rpn_bbox_loss,\n                \'fastrcnn_cls_loss\': cls_loss,\n                \'fastrcnn_loc_loss\': bbox_loss\n            }\n        return loss_dict\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch):\n\n        if self.is_training:\n            # ensure shape is [M, 5]\n            gtboxes_batch = tf.reshape(gtboxes_batch, [-1, 5])\n            gtboxes_batch = tf.cast(gtboxes_batch, tf.float32)\n\n        img_shape = tf.shape(input_img_batch)\n\n        # 1. build base network\n        P_list = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        with tf.variable_scope(\'build_rpn\',\n                               regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n\n            fpn_cls_score = []\n            fpn_box_pred = []\n            for level_name, p in zip(cfgs.LEVLES, P_list):\n                if cfgs.SHARE_HEADS:\n                    reuse_flag = None if level_name == cfgs.LEVLES[0] else True\n                    scope_list = [\'rpn_conv/3x3\', \'rpn_cls_score\', \'rpn_bbox_pred\']\n                else:\n                    reuse_flag = None\n                    scope_list = [\'rpn_conv/3x3_%s\' % level_name, \'rpn_cls_score_%s\' % level_name,\n                                  \'rpn_bbox_pred_%s\' % level_name]\n                rpn_conv3x3 = slim.conv2d(\n                    p, 512, [3, 3],\n                    trainable=self.is_training, weights_initializer=cfgs.INITIALIZER, padding=""SAME"",\n                    activation_fn=tf.nn.relu,\n                    scope=scope_list[0],\n                    reuse=reuse_flag)\n                rpn_cls_score = slim.conv2d(rpn_conv3x3, self.num_anchors_per_location * 2, [1, 1], stride=1,\n                                            trainable=self.is_training, weights_initializer=cfgs.INITIALIZER,\n                                            activation_fn=None, padding=""VALID"",\n                                            scope=scope_list[1],\n                                            reuse=reuse_flag)\n                rpn_box_pred = slim.conv2d(rpn_conv3x3, self.num_anchors_per_location * 4, [1, 1], stride=1,\n                                           trainable=self.is_training, weights_initializer=cfgs.BBOX_INITIALIZER,\n                                           activation_fn=None, padding=""VALID"",\n                                           scope=scope_list[2],\n                                           reuse=reuse_flag)\n                rpn_box_pred = tf.reshape(rpn_box_pred, [-1, 4])\n                rpn_cls_score = tf.reshape(rpn_cls_score, [-1, 2])\n\n                fpn_cls_score.append(rpn_cls_score)\n                fpn_box_pred.append(rpn_box_pred)\n\n            fpn_cls_score = tf.concat(fpn_cls_score, axis=0, name=\'fpn_cls_score\')\n            fpn_box_pred = tf.concat(fpn_box_pred, axis=0, name=\'fpn_box_pred\')\n            fpn_cls_prob = slim.softmax(fpn_cls_score, scope=\'fpn_cls_prob\')\n\n        # 3. generate_anchors\n        all_anchors = []\n        mask_gt_list = []\n        for i in range(len(cfgs.LEVLES)):\n            level_name, p = cfgs.LEVLES[i], P_list[i]\n\n            p_h, p_w = tf.shape(p)[1], tf.shape(p)[2]\n\n            featuremap_height = tf.cast(p_h, tf.float32)\n            featuremap_width = tf.cast(p_w, tf.float32)\n            anchors = anchor_utils.make_anchors(base_anchor_size=cfgs.BASE_ANCHOR_SIZE_LIST[i],\n                                                anchor_scales=cfgs.ANCHOR_SCALES,\n                                                anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                featuremap_height=featuremap_height,\n                                                featuremap_width=featuremap_width,\n                                                stride=cfgs.ANCHOR_STRIDE_LIST[i],\n                                                name=""make_anchors_for%s"" % level_name)\n            all_anchors.append(anchors)\n        all_anchors = tf.concat(all_anchors, axis=0, name=\'all_anchors_of_FPN\')\n\n        # 4. postprocess rpn proposals. such as: decode, clip, NMS\n        with tf.variable_scope(\'postprocess_FPN\'):\n            rois, roi_scores = postprocess_rpn_proposals(rpn_bbox_pred=fpn_box_pred,\n                                                         rpn_cls_prob=fpn_cls_prob,\n                                                         img_shape=img_shape,\n                                                         anchors=all_anchors,\n                                                         is_training=self.is_training)\n            # rois shape [-1, 4]\n            # +++++++++++++++++++++++++++++++++++++add img smry+++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n            if self.is_training:\n                score_gre_05 = tf.reshape(tf.where(tf.greater_equal(roi_scores, 0.5)), [-1])\n                score_gre_05_rois = tf.gather(rois, score_gre_05)\n                score_gre_05_score = tf.gather(roi_scores, score_gre_05)\n                score_gre_05_in_img = show_box_in_tensor.draw_boxes_with_scores(img_batch=input_img_batch,\n                                                                                boxes=score_gre_05_rois,\n                                                                                scores=score_gre_05_score)\n                tf.summary.image(\'score_greater_05_rois\', score_gre_05_in_img)\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n        if self.is_training:\n            with tf.variable_scope(\'sample_anchors_minibatch\'):\n                fpn_labels, fpn_bbox_targets = \\\n                    tf.py_func(\n                        anchor_target_layer,\n                        [gtboxes_batch, img_shape, all_anchors],\n                        [tf.float32, tf.float32])\n                fpn_bbox_targets = tf.reshape(fpn_bbox_targets, [-1, 4])\n                fpn_labels = tf.to_int32(fpn_labels, name=""to_int32"")\n                fpn_labels = tf.reshape(fpn_labels, [-1])\n                self.add_anchor_img_smry(input_img_batch, all_anchors, fpn_labels)\n\n            # --------------------------------------add smry-----------------------------------------------------------\n\n            fpn_cls_category = tf.argmax(fpn_cls_prob, axis=1)\n            kept_rpppn = tf.reshape(tf.where(tf.not_equal(fpn_labels, -1)), [-1])\n            fpn_cls_category = tf.gather(fpn_cls_category, kept_rpppn)\n            acc = tf.reduce_mean(tf.to_float(tf.equal(fpn_cls_category,\n                                                      tf.to_int64(tf.gather(fpn_labels, kept_rpppn)))))\n            tf.summary.scalar(\'ACC/fpn_accuracy\', acc)\n\n            with tf.control_dependencies([fpn_labels]):\n                with tf.variable_scope(\'sample_RCNN_minibatch\'):\n                    rois, labels, bbox_targets, gtboxes = \\\n                        tf.py_func(proposal_target_layer,\n                                   [rois, gtboxes_batch, cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD],\n                                   [tf.float32, tf.float32, tf.float32, tf.float32])\n                    rois = tf.reshape(rois, [-1, 4])\n                    labels = tf.to_int32(labels)\n                    labels = tf.reshape(labels, [-1])\n                    bbox_targets = tf.reshape(bbox_targets, [-1, 4 * (cfgs.CLASS_NUM + 1)])\n                    self.add_roi_batch_img_smry(input_img_batch, rois, labels)\n        if self.is_training:\n            rois_list, labels, bbox_targets = self.assign_levels(all_rois=rois,\n                                                                 labels=labels,\n                                                                 bbox_targets=bbox_targets)\n        else:\n            rois_list = self.assign_levels(all_rois=rois)  # rois_list: [P2_rois, P3_rois, P4_rois, P5_rois]\n\n        # -------------------------------------------------------------------------------------------------------------#\n        #                                            Fast-RCNN                                                         #\n        # -------------------------------------------------------------------------------------------------------------#\n\n        # 5. build Fast-RCNN\n        # rois = tf.Print(rois, [tf.shape(rois)], \'rois shape\', summarize=10)\n        bbox_pred, cls_score = self.build_fastrcnn(P_list=P_list,\n                                                   rois_list=rois_list,\n                                                   img_shape=img_shape)\n        # bbox_pred shape: [-1, 4*(cls_num+1)].\n        # cls_score shape\xef\xbc\x9a [-1, cls_num+1]\n\n        cls_prob = slim.softmax(cls_score, \'cls_prob\')\n\n        # ----------------------------------------------add smry-------------------------------------------------------\n        if self.is_training:\n            cls_category = tf.argmax(cls_prob, axis=1)\n            fast_acc = tf.reduce_mean(tf.to_float(tf.equal(cls_category, tf.to_int64(labels))))\n            tf.summary.scalar(\'ACC/fast_acc\', fast_acc)\n\n        rois = tf.concat(rois_list, axis=0, name=\'concat_rois\')\n        #  6. postprocess_fastrcnn\n        if not self.is_training:\n            return self.postprocess_fastrcnn(rois=rois, bbox_ppred=bbox_pred, scores=cls_prob, img_shape=img_shape)\n        else:\n            \'\'\'\n            when trian. We need build Loss\n            \'\'\'\n            loss_dict = self.build_loss(rpn_box_pred=fpn_box_pred,\n                                        rpn_bbox_targets=fpn_bbox_targets,\n                                        rpn_cls_score=fpn_cls_score,\n                                        rpn_labels=fpn_labels,\n                                        bbox_pred=bbox_pred,\n                                        bbox_targets=bbox_targets,\n                                        cls_score=cls_score,\n                                        labels=labels)\n\n            final_bbox, final_scores, final_category = self.postprocess_fastrcnn(rois=rois,\n                                                                                 bbox_ppred=bbox_pred,\n                                                                                 scores=cls_prob,\n                                                                                 img_shape=img_shape)\n            return final_bbox, final_scores, final_category, loss_dict\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(self.base_network_name):\n                    var_name_in_ckpt = name_in_ckpt_rpn(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20 * ""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n\n'"
libs/networks/layer.py,30,"b'# -*-coding: utf-8 -*-\r\n\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\n\r\nfrom libs.configs import cfgs\r\n\r\n\r\ndef build_attention(inputs, is_training, scope):\r\n    with tf.variable_scope(scope):\r\n        attention_conv3x3_1 = slim.conv2d(inputs, 256, [3, 3],\r\n                                          trainable=is_training,\r\n                                          weights_initializer=cfgs.INITIALIZER,\r\n                                          activation_fn=tf.nn.relu,\r\n                                          scope=\'attention_conv/3x3_1\')\r\n        attention_conv3x3_2 = slim.conv2d(attention_conv3x3_1, 256, [3, 3],\r\n                                          trainable=is_training,\r\n                                          weights_initializer=cfgs.INITIALIZER,\r\n                                          activation_fn=tf.nn.relu,\r\n                                          scope=\'attention_conv/3x3_2\')\r\n        attention_conv3x3_3 = slim.conv2d(attention_conv3x3_2, 256, [3, 3],\r\n                                          trainable=is_training,\r\n                                          weights_initializer=cfgs.INITIALIZER,\r\n                                          activation_fn=tf.nn.relu,\r\n                                          scope=\'attention_conv/3x3_3\')\r\n        attention_conv3x3_4 = slim.conv2d(attention_conv3x3_3, 256, [3, 3],\r\n                                          trainable=is_training,\r\n                                          weights_initializer=cfgs.INITIALIZER,\r\n                                          activation_fn=tf.nn.relu,\r\n                                          scope=\'attention_conv/3x3_4\')\r\n        attention_conv3x3_5 = slim.conv2d(attention_conv3x3_4, 2, [3, 3],\r\n                                          trainable=is_training,\r\n                                          weights_initializer=cfgs.INITIALIZER,\r\n                                          activation_fn=None,\r\n                                          scope=\'attention_conv/3x3_5\')\r\n        return attention_conv3x3_5\r\n\r\n\r\ndef build_inception(inputs, is_training):\r\n    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\r\n                        stride=1, padding=\'SAME\'):\r\n        with tf.variable_scope(\'Branch_0\'):\r\n            branch_0 = slim.conv2d(inputs, 384, [1, 1],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0a_1x1\')\r\n        with tf.variable_scope(\'Branch_1\'):\r\n            branch_1 = slim.conv2d(inputs, 192, [1, 1],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0a_1x1\')\r\n            branch_1 = slim.conv2d(branch_1, 224, [1, 7],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0b_1x7\')\r\n            branch_1 = slim.conv2d(branch_1, 256, [7, 1],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0c_7x1\')\r\n        with tf.variable_scope(\'Branch_2\'):\r\n            branch_2 = slim.conv2d(inputs, 192, [1, 1],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0a_1x1\')\r\n            branch_2 = slim.conv2d(branch_2, 192, [7, 1],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0b_7x1\')\r\n            branch_2 = slim.conv2d(branch_2, 224, [1, 7],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'Conv2d_0c_1x7\')\r\n            branch_2 = slim.conv2d(branch_2, 224, [7, 1],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0d_7x1\')\r\n            branch_2 = slim.conv2d(branch_2, 256, [1, 7],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0e_1x7\')\r\n        with tf.variable_scope(\'Branch_3\'):\r\n            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'avgPool_0a_3x3\')\r\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1],\r\n                                   trainable=is_training,\r\n                                   weights_initializer=cfgs.INITIALIZER,\r\n                                   activation_fn=tf.nn.relu,\r\n                                   scope=\'conv2d_0b_1x1\')\r\n        inception_out = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\r\n        return inception_out\r\n\r\n\r\ndef build_inception_attention(inputs, is_training):\r\n    """"""Builds Inception-B block for Inception v4 network.""""""\r\n    # By default use stride=1 and SAME padding\r\n    inception_out = build_inception(inputs, is_training)\r\n\r\n    inception_attention_out = slim.conv2d(inception_out, 2, [3, 3],\r\n                                          trainable=is_training,\r\n                                          weights_initializer=cfgs.INITIALIZER,\r\n                                          activation_fn=None,\r\n                                          scope=\'inception_attention_out\')\r\n    return inception_attention_out\r\n\r\n\r\ndef build_context(inputs, is_training):\r\n    conv3x3 = slim.conv2d(inputs, 512, [3, 3],\r\n                          trainable=is_training,\r\n                          weights_initializer=cfgs.INITIALIZER,\r\n                          activation_fn=None,\r\n                          scope=\'conv3x3\')\r\n\r\n    conv3x3_dimred = slim.conv2d(inputs, 256, [3, 3],\r\n                                 trainable=is_training,\r\n                                 weights_initializer=cfgs.INITIALIZER,\r\n                                 activation_fn=tf.nn.relu,\r\n                                 scope=\'conv3x3_dimred\')\r\n    conv3x3_5x5 = slim.conv2d(conv3x3_dimred, 256, [3, 3],\r\n                              trainable=is_training,\r\n                              weights_initializer=cfgs.INITIALIZER,\r\n                              activation_fn=None,\r\n                              scope=\'conv3x3_5x5\')\r\n\r\n    conv3x3_7x7_1 = slim.conv2d(conv3x3_dimred, 256, [3, 3],\r\n                                trainable=is_training,\r\n                                weights_initializer=cfgs.INITIALIZER,\r\n                                activation_fn=tf.nn.relu,\r\n                                scope=\'conv3x3_7x7_1\')\r\n\r\n    conv3x3_7x7 = slim.conv2d(conv3x3_7x7_1, 256, [3, 3],\r\n                              trainable=is_training,\r\n                              weights_initializer=cfgs.INITIALIZER,\r\n                              activation_fn=None,\r\n                              scope=\'conv3x3_7x7\')\r\n\r\n    concat_layer = tf.concat([conv3x3, conv3x3_5x5, conv3x3_7x7], axis=-1)\r\n\r\n    outputs = tf.nn.relu(concat_layer)\r\n    return outputs\r\n\r\n\r\ndef squeeze_excitation_layer(input_x, out_dim, ratio, layer_name, is_training, mode):\r\n    with tf.name_scope(layer_name):\r\n        if mode == \'avg\':\r\n            # Global_Average_Pooling\r\n            squeeze = tf.reduce_mean(input_x, [1, 2])\r\n        else:\r\n            squeeze = tf.reduce_max(input_x, [1, 2])\r\n\r\n        excitation = slim.fully_connected(inputs=squeeze,\r\n                                          num_outputs=out_dim // ratio,\r\n                                          weights_initializer=cfgs.BBOX_INITIALIZER,\r\n                                          activation_fn=tf.nn.relu,\r\n                                          trainable=is_training,\r\n                                          scope=layer_name+\'_fully_connected1\')\r\n\r\n        excitation = slim.fully_connected(inputs=excitation,\r\n                                          num_outputs=out_dim,\r\n                                          weights_initializer=cfgs.BBOX_INITIALIZER,\r\n                                          activation_fn=tf.nn.sigmoid,\r\n                                          trainable=is_training,\r\n                                          scope=layer_name + \'_fully_connected2\')\r\n\r\n        excitation = tf.reshape(excitation, [-1, 1, 1, out_dim])\r\n\r\n        # scale = input_x * excitation\r\n\r\n        return excitation\r\n\r\n'"
libs/networks/mobilenet_v2.py,4,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport tensorflow.contrib.slim as slim\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import mobilenet_v2\nfrom libs.networks.mobilenet.mobilenet import training_scope\nfrom libs.networks.mobilenet.mobilenet_v2 import op\nfrom libs.networks.mobilenet.mobilenet_v2 import ops\nexpand_input = ops.expand_input_by_factor\n\nV2_BASE_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16, scope=\'expanded_conv\'),\n        op(ops.expanded_conv, stride=2, num_outputs=24, scope=\'expanded_conv_1\'),\n        op(ops.expanded_conv, stride=1, num_outputs=24, scope=\'expanded_conv_2\'),\n        op(ops.expanded_conv, stride=2, num_outputs=32, scope=\'expanded_conv_3\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_4\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_5\'),\n        op(ops.expanded_conv, stride=2, num_outputs=64, scope=\'expanded_conv_6\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_7\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_8\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_9\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_10\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_11\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_12\')\n    ],\n)\n\n\nV2_HEAD_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(ops.expanded_conv, stride=2, num_outputs=160, scope=\'expanded_conv_13\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_14\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_15\'),\n        op(ops.expanded_conv, stride=1, num_outputs=320, scope=\'expanded_conv_16\'),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280, scope=\'Conv_1\')\n    ],\n)\ndef mobilenetv2_scope(is_training=True,\n                      trainable=True,\n                      weight_decay=0.00004,\n                      stddev=0.09,\n                      dropout_keep_prob=0.8,\n                      bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n  In default. We do not use BN\n\n  ReWrite the scope.\n  """"""\n  batch_norm_params = {\n      \'is_training\': False,\n      \'trainable\': False,\n      \'decay\': bn_decay,\n  }\n  with slim.arg_scope(training_scope(is_training=is_training, weight_decay=weight_decay)):\n      with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n                          trainable=trainable):\n          with slim.arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n              return sc\n\n\n\ndef mobilenetv2_base(img_batch, is_training=True):\n\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n\n        feature_to_crop, endpoints = mobilenet_v2.mobilenet_base(input_tensor=img_batch,\n                                                      num_classes=None,\n                                                      is_training=False,\n                                                      depth_multiplier=1.0,\n                                                      scope=\'MobilenetV2\',\n                                                      conv_defs=V2_BASE_DEF,\n                                                      finegrain_classification_mode=False)\n\n        # feature_to_crop = tf.Print(feature_to_crop, [tf.shape(feature_to_crop)], summarize=10, message=\'rpn_shape\')\n        return feature_to_crop\n\n\ndef mobilenetv2_head(inputs, is_training=True):\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n        net, _ = mobilenet_v2.mobilenet(input_tensor=inputs,\n                                        num_classes=None,\n                                        is_training=False,\n                                        depth_multiplier=1.0,\n                                        scope=\'MobilenetV2\',\n                                        conv_defs=V2_HEAD_DEF,\n                                        finegrain_classification_mode=False)\n\n        net = tf.squeeze(net, [1, 2])\n\n        return net'"
libs/networks/nas_fpn.py,11,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.configs import cfgs\nfrom libs.networks.resnet import fusion_two_layer\n\n\ndef fpn(feature_dict, scope):\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid_{}\'.format(scope)):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'P5\'],\n                             num_outputs=cfgs.FPN_CHANNEL,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 1, -1):  # build [P4, P3, P2]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""P%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 1, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3],\n                                                          padding=""SAME"", stride=1, scope=""fuse_P%d"" % level,\n                                                          activation_fn=tf.nn.relu if cfgs.USE_RELU else None)\n            if ""P6"" in cfgs.LEVLES:\n                P6 = slim.avg_pool2d(P5, kernel_size=[1, 1], stride=2, scope=\'build_P6\')\n                pyramid_dict[\'P6\'] = P6\n            return pyramid_dict\n\n\ndef gp(fm1, fm2, scope):\n    h, w = tf.shape(fm2)[1], tf.shape(fm2)[2]\n    global_ctx = tf.reduce_mean(fm1, axis=[1, 2], keep_dims=True)\n    global_ctx = tf.sigmoid(global_ctx)\n    output = (global_ctx * fm2) + tf.image.resize_bilinear(fm1, size=[h, w], name=\'resize_\' + scope)\n    return output\n\n\ndef rcb(fm, scope):\n    fm = tf.nn.relu(fm)\n    fm = slim.conv2d(fm, num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3],\n                     padding=""SAME"", stride=1, scope=\'RCB_%s\' % scope,\n                     activation_fn=None)\n    fm = slim.batch_norm(fm, scope=\'BN_%s\' % scope)\n    return fm\n\n\ndef sum_fm(fm1, fm2, scope):\n    h, w = tf.shape(fm2)[1], tf.shape(fm2)[2]\n    output = fm2 + tf.image.resize_bilinear(fm1, size=[h, w], name=\'resize_\' + scope)\n    return output\n\n\ndef nas_fpn(feature_dict, scope):\n    GP_P5_P3 = gp(feature_dict[\'P5\'], feature_dict[\'P3\'], \'GP_P5_P3_{}\'.format(scope))\n    GP_P5_P3_RCB = rcb(GP_P5_P3, \'GP_P5_P3_RCB_{}\'.format(scope))\n    SUM1 = sum_fm(GP_P5_P3_RCB, feature_dict[\'P3\'], \'SUM1_{}\'.format(scope))\n    SUM1_RCB = rcb(SUM1, \'SUM1_RCB_{}\'.format(scope))\n    SUM2 = sum_fm(SUM1_RCB, feature_dict[\'P2\'], \'SUM2_{}\'.format(scope))\n    SUM2_RCB = rcb(SUM2, \'SUM2_RCB_{}\'.format(scope))  # P2\n    SUM3 = sum_fm(SUM2_RCB, SUM1_RCB, \'SUM3_{}\'.format(scope))\n    SUM3_RCB = rcb(SUM3, \'SUM3_RCB_{}\'.format(scope))  # P3\n    SUM3_RCB_GP = gp(SUM2_RCB, SUM3_RCB, \'SUM3_RCB_GP_{}\'.format(scope))\n    SUM4 = sum_fm(SUM3_RCB_GP, feature_dict[\'P4\'], \'SUM4_{}\'.format(scope))\n    SUM4_RCB = rcb(SUM4, \'SUM4_RCB_{}\'.format(scope))  # P4\n    SUM4_RCB_GP = gp(SUM1_RCB, SUM4_RCB, \'SUM4_RCB_GP_{}\'.format(scope))\n    SUM5 = sum_fm(SUM4_RCB_GP, feature_dict[\'P6\'], \'SUM5_{}\'.format(scope))\n    SUM5_RCB = rcb(SUM5, \'SUM5_RCB_{}\'.format(scope))  # P6\n    h, w = tf.shape(feature_dict[\'P5\'])[1], tf.shape(feature_dict[\'P5\'])[2]\n    SUM5_RCB_resize = tf.image.resize_bilinear(SUM5_RCB, size=[h, w], name=\'resize_SUM5_RCB_resize_\'.format(scope))\n    SUM4_RCB_GP1 = gp(SUM4_RCB, SUM5_RCB_resize, \'SUM4_RCB_GP1_{}\'.format(scope))\n    SUM4_RCB_GP1_RCB = rcb(SUM4_RCB_GP1, \'SUM4_RCB_GP1_RCB_{}\'.format(scope))  # P5\n    pyramid_dict = {\'P2\': SUM2_RCB, \'P3\': SUM3_RCB, \'P4\': SUM4_RCB,\n                    \'P5\': SUM4_RCB_GP1_RCB, \'P6\': SUM5_RCB}\n    return pyramid_dict\n'"
libs/networks/resnet.py,16,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.configs import cfgs\nfrom tensorflow.contrib.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim.nets import resnet_utils\nfrom tensorflow.contrib.slim.python.slim.nets.resnet_v1 import resnet_v1_block\n# import tfplot as tfp\n\n\ndef resnet_arg_scope(\n        is_training=True, weight_decay=cfgs.WEIGHT_DECAY, batch_norm_decay=0.997,\n        batch_norm_epsilon=1e-5, batch_norm_scale=True):\n    \'\'\'\n\n    In Default, we do not use BN to train resnet, since batch_size is too small.\n    So is_training is False and trainable is False in the batch_norm params.\n\n    \'\'\'\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS\n    }\n\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef fusion_two_layer(C_i, P_j, scope):\n    \'\'\'\n    i = j+1\n    :param C_i: shape is [1, h, w, c]\n    :param P_j: shape is [1, h/2, w/2, 256]\n    :return:\n    P_i\n    \'\'\'\n    with tf.variable_scope(scope):\n        level_name = scope.split(\'_\')[1]\n\n        h, w = tf.shape(C_i)[1], tf.shape(C_i)[2]\n        upsample_p = tf.image.resize_bilinear(P_j,\n                                              size=[h, w],\n                                              name=\'up_sample_\'+level_name)\n\n        reduce_dim_c = slim.conv2d(C_i,\n                                   num_outputs=cfgs.FPN_CHANNEL,\n                                   kernel_size=[1, 1], stride=1,\n                                   scope=\'reduce_dim_\'+level_name)\n\n        add_f = 0.5*upsample_p + 0.5*reduce_dim_c\n\n        # P_i = slim.conv2d(add_f,\n        #                   num_outputs=256, kernel_size=[3, 3], stride=1,\n        #                   padding=\'SAME\',\n        #                   scope=\'fusion_\'+level_name)\n        return add_f\n\n\n# def add_heatmap(feature_maps, name):\n#     \'\'\'\n#\n#     :param feature_maps:[B, H, W, C]\n#     :return:\n#     \'\'\'\n#\n#     def figure_attention(activation):\n#         fig, ax = tfp.subplots()\n#         im = ax.imshow(activation, cmap=\'jet\')\n#         fig.colorbar(im)\n#         return fig\n#\n#     heatmap = tf.reduce_sum(feature_maps, axis=-1)\n#     heatmap = tf.squeeze(heatmap, axis=0)\n#     tfp.summary.plot(name, figure_attention, [heatmap])\n\n\ndef resnet_base(img_batch, scope_name, is_training=True):\n    \'\'\'\n    this code is derived from light-head rcnn.\n    https://github.com/zengarden/light_head_rcnn\n\n    It is convenient to freeze blocks. So we adapt this mode.\n    \'\'\'\n    if scope_name == \'resnet_v1_50\':\n        middle_num_units = 6\n    elif scope_name == \'resnet_v1_101\':\n        middle_num_units = 23\n    else:\n        raise NotImplementedError(\'We only support resnet_v1_50 or resnet_v1_101. Check your network name....\')\n\n    blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n              resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n              resnet_v1_block(\'block3\', base_depth=256, num_units=middle_num_units, stride=2),\n              resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n    # when use fpn . stride list is [1, 2, 2]\n\n    with slim.arg_scope(resnet_arg_scope(is_training=False)):\n        with tf.variable_scope(scope_name, scope_name):\n            # Do the first few layers manually, because \'SAME\' padding can behave inconsistently\n            # for images of different sizes: sometimes 0, sometimes 1\n            net = resnet_utils.conv2d_same(\n                img_batch, 64, 7, stride=2, scope=\'conv1\')\n            net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\n            net = slim.max_pool2d(\n                net, [3, 3], stride=2, padding=\'VALID\', scope=\'pool1\')\n\n    not_freezed = [False] * cfgs.FIXED_BLOCKS + (4-cfgs.FIXED_BLOCKS)*[True]\n    # Fixed_Blocks can be 1~3\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[0]))):\n        C2, end_points_C2 = resnet_v1.resnet_v1(net,\n                                                blocks[0:1],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # C2 = tf.Print(C2, [tf.shape(C2)], summarize=10, message=\'C2_shape\')\n    # add_heatmap(C2, name=\'Layer2/C2_heat\')\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[1]))):\n        C3, end_points_C3 = resnet_v1.resnet_v1(C2,\n                                                blocks[1:2],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # C3 = tf.Print(C3, [tf.shape(C3)], summarize=10, message=\'C3_shape\')\n    # add_heatmap(C3, name=\'Layer3/C3_heat\')\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[2]))):\n        C4, end_points_C4 = resnet_v1.resnet_v1(C3,\n                                                blocks[2:3],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # add_heatmap(C4, name=\'Layer4/C4_heat\')\n\n    # C4 = tf.Print(C4, [tf.shape(C4)], summarize=10, message=\'C4_shape\')\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n        C5, end_points_C5 = resnet_v1.resnet_v1(C4,\n                                                blocks[3:4],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n    # C5 = tf.Print(C5, [tf.shape(C5)], summarize=10, message=\'C5_shape\')\n    # add_heatmap(C5, name=\'Layer5/C5_heat\')\n\n    feature_dict = {\'C2\': end_points_C2[\'{}/block1/unit_2/bottleneck_v1\'.format(scope_name)],\n                    \'C3\': end_points_C3[\'{}/block2/unit_3/bottleneck_v1\'.format(scope_name)],\n                    \'C4\': end_points_C4[\'{}/block3/unit_{}/bottleneck_v1\'.format(scope_name, middle_num_units - 1)],\n                    \'C5\': end_points_C5[\'{}/block4/unit_3/bottleneck_v1\'.format(scope_name)],\n                    # \'C5\': end_points_C5[\'{}/block4\'.format(scope_name)],\n                    }\n\n    # feature_dict = {\'C2\': C2,\n    #                 \'C3\': C3,\n    #                 \'C4\': C4,\n    #                 \'C5\': C5}\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(C5,\n                             num_outputs=256,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 1, -1):  # build [P4, P3, P2]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level+1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 1, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level)\n            if ""P6"" in cfgs.LEVLES:\n                P6 = slim.avg_pool2d(P5, kernel_size=[1, 1], stride=2, scope=\'build_P6\')\n                pyramid_dict[\'P6\'] = P6\n\n    # for level in range(5, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    # return [P2, P3, P4, P5, P6]\n    print(""we are in Pyramid::-======>>>>"")\n    print(cfgs.LEVLES)\n    print(""base_anchor_size are: "", cfgs.BASE_ANCHOR_SIZE_LIST)\n    print(20 * ""__"")\n    return [pyramid_dict[level_name] for level_name in cfgs.LEVLES]\n\n\ndef restnet_head(inputs, is_training, scope_name):\n    \'\'\'\n\n    :param inputs: [minibatch_size, 7, 7, 256]\n    :param is_training:\n    :param scope_name:\n    :return:\n    \'\'\'\n\n    with tf.variable_scope(\'build_fc_layers\'):\n\n        # fc1 = slim.conv2d(inputs=inputs,\n        #                   num_outputs=1024,\n        #                   kernel_size=[7, 7],\n        #                   padding=\'VALID\',\n        #                   scope=\'fc1\') # shape is [minibatch_size, 1, 1, 1024]\n        # fc1 = tf.squeeze(fc1, [1, 2], name=\'squeeze_fc1\')\n\n        inputs = slim.flatten(inputs=inputs, scope=\'flatten_inputs\')\n\n        fc1 = slim.fully_connected(inputs, num_outputs=1024, scope=\'fc1\')\n\n        fc2 = slim.fully_connected(fc1, num_outputs=1024, scope=\'fc2\')\n\n        # fc3 = slim.fully_connected(fc2, num_outputs=1024, scope=\'fc3\')\n\n        # we add fc3 to increase the ability of fast-rcnn head\n        return fc2\n\n'"
libs/networks/resnet_gluoncv.py,31,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.configs import cfgs\nfrom libs.networks.resnet import fusion_two_layer\nfrom libs.networks.nas_fpn import fpn, nas_fpn\nDATA_FORMAT = ""NHWC""\nDEBUG = False\ndebug_dict = {}\nBottleNeck_NUM_DICT = {\n    \'resnet50_v1b\': [3, 4, 6, 3],\n    \'resnet101_v1b\': [3, 4, 23, 3],\n    \'resnet50_v1d\': [3, 4, 6, 3],\n    \'resnet101_v1d\': [3, 4, 23, 3]\n}\n\nBASE_CHANNELS_DICT = {\n    \'resnet50_v1b\': [64, 128, 256, 512],\n    \'resnet101_v1b\': [64, 128, 256, 512],\n    \'resnet50_v1d\': [64, 128, 256, 512],\n    \'resnet101_v1d\': [64, 128, 256, 512]\n}\n\n\ndef resnet_arg_scope(freeze_norm, is_training=True, weight_decay=0.0001,\n                     batch_norm_decay=0.9, batch_norm_epsilon=1e-5, batch_norm_scale=True):\n\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n        \'data_format\': DATA_FORMAT\n    }\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef stem_7x7(net, scope=""C1""):\n\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [3, 3], [3, 3], [0, 0]])  # pad for data\n        net = slim.conv2d(net, num_outputs=64, kernel_size=[7, 7], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=""conv0"")\n        if DEBUG:\n            debug_dict[\'conv_7x7_bn_relu\'] = tf.transpose(net, [0, 3, 1, 2])  # NHWC --> NCHW\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef stem_stack_3x3(net, input_channel=32, scope=""C1""):\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel*2, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv2\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef bottleneck_v1b(input_x, base_channel, scope, stride=1, projection=False, avg_down=True):\n    \'\'\'\n    for bottleneck_v1b: reduce spatial dim in conv_3x3 with stride 2.\n    \'\'\'\n    with tf.variable_scope(scope):\n        if DEBUG:\n            debug_dict[input_x.op.name] = tf.transpose(input_x, [0, 3, 1, 2])\n        net = slim.conv2d(input_x, num_outputs=base_channel, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = slim.conv2d(net, num_outputs=base_channel, kernel_size=[3, 3], stride=stride,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = slim.conv2d(net, num_outputs=base_channel * 4, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          activation_fn=None, scope=\'conv2\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        # Note that : gamma in the last conv should be init with 0.\n        # But we just reload params from mxnet, so don\'t specific batch norm initializer\n        if projection:\n\n            if avg_down:  # design for resnet_v1d\n                \'\'\'\n                In GluonCV, padding is ""ceil mode"". Here we use ""SAME"" to replace it, which may cause Erros.\n                And the erro will grow with depth of resnet. e.g. res101 erro > res50 erro\n                \'\'\'\n                shortcut = slim.avg_pool2d(input_x, kernel_size=[stride, stride], stride=stride, padding=""SAME"",\n                                           data_format=DATA_FORMAT)\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n                shortcut = slim.conv2d(shortcut, num_outputs=base_channel*4, kernel_size=[1, 1],\n                                       stride=1, padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                                       activation_fn=None,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n                # shortcut should have batch norm.\n            else:\n                shortcut = slim.conv2d(input_x, num_outputs=base_channel * 4, kernel_size=[1, 1],\n                                       stride=stride, padding=""VALID"", biases_initializer=None, activation_fn=None,\n                                       data_format=DATA_FORMAT,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n        else:\n            shortcut = tf.identity(input_x, name=\'shortcut/Identity\')\n            if DEBUG:\n                debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n        net = net + shortcut\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = tf.nn.relu(net)\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        return net\n\n\ndef make_block(net, base_channel, bottleneck_nums, scope, avg_down=True, spatial_downsample=False):\n    with tf.variable_scope(scope):\n        first_stride = 2 if spatial_downsample else 1\n\n        net = bottleneck_v1b(input_x=net, base_channel=base_channel,scope=\'bottleneck_0\',\n                             stride=first_stride, avg_down=avg_down, projection=True)\n        for i in range(1, bottleneck_nums):\n            net = bottleneck_v1b(input_x=net, base_channel=base_channel, scope=""bottleneck_%d"" % i,\n                                 stride=1, avg_down=avg_down, projection=False)\n        return net\n\n\ndef get_resnet_v1_b_base(input_x, freeze_norm, scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_7x7(net=input_x, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-1]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=False, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_d_base(input_x, freeze_norm, scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) + 1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[0]) and is_training),\n                                             freeze_norm=freeze_norm)):\n            net = stem_stack_3x3(net=input_x, input_channel=32, scope=""C1"")\n            feature_dict[""C1""] = net\n            # print (net)\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True  # do not downsample in C2\n            with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[i-1]) and is_training),\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=True, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\n# -----------------------------------\ndef resnet_base(img_batch, scope_name, is_training=True):\n    if scope_name.endswith(\'b\'):\n        get_resnet_fn = get_resnet_v1_b_base\n    elif scope_name.endswith(\'d\'):\n        get_resnet_fn = get_resnet_v1_d_base\n    else:\n        raise ValueError(""scope Name erro...."")\n\n    _, feature_dict = get_resnet_fn(input_x=img_batch, scope=scope_name,\n                                    bottleneck_nums=BottleNeck_NUM_DICT[scope_name],\n                                    base_channels=BASE_CHANNELS_DICT[scope_name],\n                                    is_training=is_training, freeze_norm=True,\n                                    freeze=cfgs.FREEZE_BLOCKS)\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 1, -1):  # build [P4, P3, P2]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 1, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3],\n                                                          padding=""SAME"", stride=1, scope=""fuse_P%d"" % level,\n                                                          activation_fn=tf.nn.relu if cfgs.USE_RELU else None)\n            if ""P6"" in cfgs.LEVLES:\n                P6 = slim.avg_pool2d(P5, kernel_size=[1, 1], stride=2, scope=\'build_P6\')\n                pyramid_dict[\'P6\'] = P6\n\n    # for level in range(5, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    for i in range(cfgs.NUM_FPN):\n        pyramid_dict = fpn(pyramid_dict, i)\n\n    for i in range(cfgs.NUM_NAS_FPN):\n        pyramid_dict = nas_fpn(pyramid_dict, i)\n\n    print(""we are in Pyramid::-======>>>>"")\n    print(cfgs.LEVLES)\n    print(""base_anchor_size are: "", cfgs.BASE_ANCHOR_SIZE_LIST)\n    print(20 * ""__"")\n    return [pyramid_dict[level_name] for level_name in cfgs.LEVLES]\n\n\n\n'"
libs/networks/resnet_gluoncv_v1.py,30,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.configs import cfgs\nfrom libs.networks.resnet import fusion_two_layer\nfrom libs.networks.nas_fpn import fpn, nas_fpn, rcb, gp\nDATA_FORMAT = ""NHWC""\nDEBUG = False\ndebug_dict = {}\nBottleNeck_NUM_DICT = {\n    \'resnet50_v1b\': [3, 4, 6, 3],\n    \'resnet101_v1b\': [3, 4, 23, 3],\n    \'resnet50_v1d\': [3, 4, 6, 3],\n    \'resnet101_v1d\': [3, 4, 23, 3]\n}\n\nBASE_CHANNELS_DICT = {\n    \'resnet50_v1b\': [64, 128, 256, 512],\n    \'resnet101_v1b\': [64, 128, 256, 512],\n    \'resnet50_v1d\': [64, 128, 256, 512],\n    \'resnet101_v1d\': [64, 128, 256, 512]\n}\n\n\ndef resnet_arg_scope(freeze_norm, is_training=True, weight_decay=0.0001,\n                     batch_norm_decay=0.9, batch_norm_epsilon=1e-5, batch_norm_scale=True):\n\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n        \'data_format\': DATA_FORMAT\n    }\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef stem_7x7(net, scope=""C1""):\n\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [3, 3], [3, 3], [0, 0]])  # pad for data\n        net = slim.conv2d(net, num_outputs=64, kernel_size=[7, 7], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=""conv0"")\n        if DEBUG:\n            debug_dict[\'conv_7x7_bn_relu\'] = tf.transpose(net, [0, 3, 1, 2])  # NHWC --> NCHW\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef stem_stack_3x3(net, input_channel=32, scope=""C1""):\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel*2, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv2\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef bottleneck_v1b(input_x, base_channel, scope, stride=1, projection=False, avg_down=True):\n    \'\'\'\n    for bottleneck_v1b: reduce spatial dim in conv_3x3 with stride 2.\n    \'\'\'\n    with tf.variable_scope(scope):\n        if DEBUG:\n            debug_dict[input_x.op.name] = tf.transpose(input_x, [0, 3, 1, 2])\n        net = slim.conv2d(input_x, num_outputs=base_channel, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = slim.conv2d(net, num_outputs=base_channel, kernel_size=[3, 3], stride=stride,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = slim.conv2d(net, num_outputs=base_channel * 4, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          activation_fn=None, scope=\'conv2\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        # Note that : gamma in the last conv should be init with 0.\n        # But we just reload params from mxnet, so don\'t specific batch norm initializer\n        if projection:\n\n            if avg_down:  # design for resnet_v1d\n                \'\'\'\n                In GluonCV, padding is ""ceil mode"". Here we use ""SAME"" to replace it, which may cause Erros.\n                And the erro will grow with depth of resnet. e.g. res101 erro > res50 erro\n                \'\'\'\n                shortcut = slim.avg_pool2d(input_x, kernel_size=[stride, stride], stride=stride, padding=""SAME"",\n                                           data_format=DATA_FORMAT)\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n                shortcut = slim.conv2d(shortcut, num_outputs=base_channel*4, kernel_size=[1, 1],\n                                       stride=1, padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                                       activation_fn=None,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n                # shortcut should have batch norm.\n            else:\n                shortcut = slim.conv2d(input_x, num_outputs=base_channel * 4, kernel_size=[1, 1],\n                                       stride=stride, padding=""VALID"", biases_initializer=None, activation_fn=None,\n                                       data_format=DATA_FORMAT,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n        else:\n            shortcut = tf.identity(input_x, name=\'shortcut/Identity\')\n            if DEBUG:\n                debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n        net = net + shortcut\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = tf.nn.relu(net)\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        return net\n\n\ndef make_block(net, base_channel, bottleneck_nums, scope, avg_down=True, spatial_downsample=False):\n    with tf.variable_scope(scope):\n        first_stride = 2 if spatial_downsample else 1\n\n        net = bottleneck_v1b(input_x=net, base_channel=base_channel,scope=\'bottleneck_0\',\n                             stride=first_stride, avg_down=avg_down, projection=True)\n        for i in range(1, bottleneck_nums):\n            net = bottleneck_v1b(input_x=net, base_channel=base_channel, scope=""bottleneck_%d"" % i,\n                                 stride=1, avg_down=avg_down, projection=False)\n        return net\n\n\ndef get_resnet_v1_b_base(input_x, freeze_norm, scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_7x7(net=input_x, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-1]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=False, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_d_base(input_x, freeze_norm, scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) + 1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[0]) and is_training),\n                                             freeze_norm=freeze_norm)):\n            net = stem_stack_3x3(net=input_x, input_channel=32, scope=""C1"")\n            feature_dict[""C1""] = net\n            # print (net)\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True  # do not downsample in C2\n            with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[i-1]) and is_training),\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=True, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\n# -----------------------------------\ndef resnet_base(img_batch, scope_name, is_training=True):\n    if scope_name.endswith(\'b\'):\n        get_resnet_fn = get_resnet_v1_b_base\n    elif scope_name.endswith(\'d\'):\n        get_resnet_fn = get_resnet_v1_d_base\n    else:\n        raise ValueError(""scope Name erro...."")\n\n    _, feature_dict = get_resnet_fn(input_x=img_batch, scope=scope_name,\n                                    bottleneck_nums=BottleNeck_NUM_DICT[scope_name],\n                                    base_channels=BASE_CHANNELS_DICT[scope_name],\n                                    is_training=is_training, freeze_norm=True,\n                                    freeze=cfgs.FREEZE_BLOCKS)\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 1, -1):  # build [P4, P3, P2]\n                C_i = slim.conv2d(feature_dict[""C%d"" % level],\n                                  num_outputs=cfgs.FPN_CHANNEL,\n                                  kernel_size=[1, 1], stride=1,\n                                  scope=\'reduce_dim_{}\'.format(level))\n                pyramid_dict[\'P%d\' % level] = gp(pyramid_dict[""P%d"" % (level + 1)], C_i,\n                                                 \'build_P%d\' % level)\n\n            for level in range(5, 1, -1):\n                pyramid_dict[\'P%d\' % level] = rcb(pyramid_dict[\'P%d\' % level], \'P{}\'.format(level))\n\n            if ""P6"" in cfgs.LEVLES:\n                P6 = slim.avg_pool2d(P5, kernel_size=[1, 1], stride=2, scope=\'build_P6\')\n                pyramid_dict[\'P6\'] = P6\n\n    # for level in range(5, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    for i in range(cfgs.NUM_FPN):\n        pyramid_dict = fpn(pyramid_dict, i)\n\n    for i in range(cfgs.NUM_NAS_FPN):\n        pyramid_dict = nas_fpn(pyramid_dict, i)\n\n    print(""we are in Pyramid::-======>>>>"")\n    print(cfgs.LEVLES)\n    print(""base_anchor_size are: "", cfgs.BASE_ANCHOR_SIZE_LIST)\n    print(20 * ""__"")\n    return [pyramid_dict[level_name] for level_name in cfgs.LEVLES]\n\n\n\n'"
libs/val_libs/__init__.py,0,b''
libs/val_libs/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\nfrom libs.configs import cfgs\nfrom help_utils.tools import *\n\ndef write_voc_results_file(all_boxes, test_imgid_list, det_save_dir):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n  the detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n\n  :param test_imgid_list:\n  :param det_save_path:\n  :return:\n  \'\'\'\n  for cls, cls_id in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    print(""Writing {} VOC resutls file"".format(cls))\n\n    mkdir(det_save_dir)\n    det_save_path = os.path.join(det_save_dir, ""det_""+cls+"".txt"")\n    with open(det_save_path, \'wt\') as f:\n      for index, img_name in enumerate(test_imgid_list):\n        this_img_detections = all_boxes[index]\n\n        this_cls_detections = this_img_detections[this_img_detections[:, 0]==cls_id]\n        if this_cls_detections.shape[0] == 0:\n          continue # this cls has none detections in this img\n        for a_det in this_cls_detections:\n          f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                  format(img_name, a_det[1],\n                         a_det[2], a_det[3],\n                         a_det[4], a_det[5]))  # that is [img_name, score, xmin, ymin, xmax, ymax]\n\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    # obj_struct[\'difficult\'] = int(0)\n    bbox = obj.find(\'bndbox\')\n    obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                          int(bbox.find(\'ymin\').text),\n                          int(bbox.find(\'xmax\').text),\n                          int(bbox.find(\'ymax\').text)]\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath, annopath, test_imgid_list, cls_name, ovthresh=0.5,\n                 use_07_metric=False, use_diff=False):\n  \'\'\'\n\n  :param detpath:\n  :param annopath:\n  :param test_imgid_list: it \'s a list that contains the img_name of test_imgs\n  :param cls_name:\n  :param ovthresh:\n  :param use_07_metric:\n  :param use_diff:\n  :return:\n  \'\'\'\n  # 1. parse xml to get gtboxes\n\n  # read list of images\n  imagenames = test_imgid_list\n\n  recs = {}\n  for i, imagename in enumerate(imagenames):\n    recs[imagename] = parse_rec(os.path.join(annopath, imagename+\'.xml\'))\n    # if i % 100 == 0:\n    #   print(\'Reading annotation for {:d}/{:d}\'.format(\n    #     i + 1, len(imagenames)))\n\n  # 2. get gtboxes for this class.\n  class_recs = {}\n  num_pos = 0\n  # if cls_name == \'person\':\n  #   print (""aaa"")\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == cls_name]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    if use_diff:\n      difficult = np.array([False for x in R]).astype(np.bool)\n    else:\n      difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    num_pos = num_pos + sum(~difficult)  # ignored the diffcult boxes\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det} # det means that gtboxes has already been detected\n\n  # 3. read the detection file\n  detfile = os.path.join(detpath, ""det_""+cls_name+"".txt"")\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  # for a line. that is [img_name, confidence, xmin, ymin, xmax, ymax]\n  splitlines = [x.strip().split(\' \') for x in lines]  # a list that include a list\n  image_ids = [x[0] for x in splitlines]  # img_id is img_name\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids) # num of detections. That, a line is a det_box.\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]  #reorder the img_name\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]  # img_id is img_name\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        ixmin = np.maximum(BBGT[:, 0], bb[0])\n        iymin = np.maximum(BBGT[:, 1], bb[1])\n        ixmax = np.minimum(BBGT[:, 2], bb[2])\n        iymax = np.minimum(BBGT[:, 3], bb[3])\n        iw = np.maximum(ixmax - ixmin + 1., 0.)\n        ih = np.maximum(iymax - iymin + 1., 0.)\n        inters = iw * ih\n\n        # union\n        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n        overlaps = inters / uni\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # 4. get recall, precison and AP\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(num_pos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric=cfgs.USE_07_METRIC)\n\n  return rec, prec, ap\n\n\ndef do_python_eval(test_imgid_list, test_annotation_path):\n  AP_list = []\n  # import matplotlib.pyplot as plt\n  # import matplotlib.colors as colors\n  # color_list = colors.cnames.keys()[::6]\n\n  for cls, index in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    recall, precision, AP = voc_eval(detpath=os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION),\n                                     test_imgid_list=test_imgid_list,\n                                     cls_name=cls,\n                                     annopath=test_annotation_path,\n                                     ovthresh=cfgs.EVAL_THRESHOLD,\n                                     use_07_metric=cfgs.USE_07_METRIC)\n    AP_list += [AP]\n    print(""cls : {}|| Recall: {} || Precison: {}|| AP: {}"".format(cls, recall[-1], precision[-1], AP))\n    # plt.plot(recall, precision, label=cls, color=color_list[index])\n    # plt.legend(loc=\'upper right\')\n    print(10*""__"")\n  # plt.show()\n  # plt.savefig(cfgs.VERSION+\'.jpg\')\n  print(""mAP is : {}"".format(np.mean(AP_list)))\n\n\ndef voc_evaluate_detections(all_boxes, test_annotation_path, test_imgid_list):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n\n  The detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n  :return:\n  \'\'\'\n  test_imgid_list = [item.split(\'.\')[0] for item in test_imgid_list]\n\n  write_voc_results_file(all_boxes, test_imgid_list=test_imgid_list,\n                         det_save_dir=os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION))\n  do_python_eval(test_imgid_list, test_annotation_path=test_annotation_path)\n\n\n\n\n\n\n\n\n'"
data/io/BDD100K/BDD2VOC.py,0,"b'import json\nimport os\nimport cv2\nfrom xml.dom.minidom import Document\nimport xml.dom.minidom\n\nlabel_map = {\'bus\': 1, \'traffic light\': 2, \'traffic sign\': 3, \'person\': 4, \'bike\': 5,\n             \'truck\': 6, \'motor\': 7, \'car\': 8, \'train\': 9, \'rider\': 10}\nFLAG = [\'train\', \'val\']\n\n\ndef write_xml(save_path, name, box_list, label_list, w, h, d):\n\n    # dict_box[filename]=json_dict[filename]\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(\'annotation\')\n    doc.appendChild(root)\n\n    foldername = doc.createElement(""folder"")\n    foldername.appendChild(doc.createTextNode(""JPEGImages""))\n    root.appendChild(foldername)\n\n    nodeFilename = doc.createElement(\'filename\')\n    nodeFilename.appendChild(doc.createTextNode(name))\n    root.appendChild(nodeFilename)\n\n    pathname = doc.createElement(""path"")\n    pathname.appendChild(doc.createTextNode(""xxxx""))\n    root.appendChild(pathname)\n\n    sourcename=doc.createElement(""source"")\n\n    databasename = doc.createElement(""database"")\n    databasename.appendChild(doc.createTextNode(""Unknown""))\n    sourcename.appendChild(databasename)\n\n    annotationname = doc.createElement(""annotation"")\n    annotationname.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(annotationname)\n\n    imagename = doc.createElement(""image"")\n    imagename.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(imagename)\n\n    flickridname = doc.createElement(""flickrid"")\n    flickridname.appendChild(doc.createTextNode(""0""))\n    sourcename.appendChild(flickridname)\n\n    root.appendChild(sourcename)\n\n    nodesize = doc.createElement(\'size\')\n    nodewidth = doc.createElement(\'width\')\n    nodewidth.appendChild(doc.createTextNode(str(w)))\n    nodesize.appendChild(nodewidth)\n    nodeheight = doc.createElement(\'height\')\n    nodeheight.appendChild(doc.createTextNode(str(h)))\n    nodesize.appendChild(nodeheight)\n    nodedepth = doc.createElement(\'depth\')\n    nodedepth.appendChild(doc.createTextNode(str(d)))\n    nodesize.appendChild(nodedepth)\n    root.appendChild(nodesize)\n\n    segname = doc.createElement(""segmented"")\n    segname.appendChild(doc.createTextNode(""0""))\n    root.appendChild(segname)\n\n    for (box, label) in zip(box_list, label_list):\n\n        nodeobject = doc.createElement(\'object\')\n        nodename = doc.createElement(\'name\')\n        nodename.appendChild(doc.createTextNode(str(label)))\n        nodeobject.appendChild(nodename)\n        nodebndbox = doc.createElement(\'bndbox\')\n        nodex1 = doc.createElement(\'x1\')\n        nodex1.appendChild(doc.createTextNode(str(box[0])))\n        nodebndbox.appendChild(nodex1)\n        nodey1 = doc.createElement(\'y1\')\n        nodey1.appendChild(doc.createTextNode(str(box[1])))\n        nodebndbox.appendChild(nodey1)\n        nodex2 = doc.createElement(\'x2\')\n        nodex2.appendChild(doc.createTextNode(str(box[2])))\n        nodebndbox.appendChild(nodex2)\n        nodey2 = doc.createElement(\'y2\')\n        nodey2.appendChild(doc.createTextNode(str(box[3])))\n        nodebndbox.appendChild(nodey2)\n\n        nodeobject.appendChild(nodebndbox)\n        root.appendChild(nodeobject)\n    fp = open(save_path, \'w\')\n    doc.writexml(fp, indent=\'\\n\')\n    fp.close()\n\n\nfor flag in FLAG:\n    BDD_path = \'/BDD100K/bdd100k/\'\n    BDD_labels_dir = os.path.join(BDD_path, \'labels/bdd100k_labels_images_{}.json\'.format(flag))\n    BDD_labels = json.load(open(BDD_labels_dir, \'r\'))\n    BDD_images_dir = os.path.join(BDD_path, \'images/100k/{}\'.format(flag))\n\n    for cnt, bdd in enumerate(BDD_labels):\n        img_name = bdd[\'name\']\n        img_path = os.path.join(BDD_images_dir, img_name)\n        # img = cv2.imread(img_path)\n        # h, w, d = img.shape\n        h, w, d = 720, 1280, 3\n        bdd_boxes = bdd[\'labels\']\n        box_list, label_list = [], []\n        for bb in bdd_boxes:\n            if bb[\'category\'] not in label_map.keys():\n                continue\n            box = bb[\'box2d\']\n            box_list.append([round(box[\'x1\']), round(box[\'y1\']),\n                             round(box[\'x2\']), round(box[\'y2\'])])\n            label_list.append(bb[\'category\'])\n\n        if len(box_list) != 0:\n            save_path = os.path.join(\'/data/BDD100K/BDD100K_VOC/bdd100k_{}/Annotations\'.format(flag),\n                                     img_name.replace(\'.jpg\', \'.xml\'))\n            write_xml(save_path, img_name, box_list, label_list, w, h, d)\n        if cnt % 100 == 0:\n            print(\'{} process: {}/{}\'.format(flag, cnt+1, len(BDD_labels)))\n    print(\'Finish!\')\n\n\n\n\n\n\n\n\n\n'"
data/io/BDD100K/get_bdd100k_next_batch.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport xml.etree.cElementTree as ET\nimport cv2\nimport numpy as np\nimport os\nfrom libs.label_name_dict import coco_dict\nfrom libs.label_name_dict.label_dict import *\n\n\nroot_path = \'/data/BDD100K/BDD100K_VOC/bdd100k_train/\'\nxmls = os.listdir(os.path.join(root_path, \'Annotations\'))\ntotal_imgs = len(xmls)\n\n# print (NAME_LABEL_DICT)\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 5],\n           and has [xmin, ymin, xmax, ymax, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef next_img(step):\n\n    if step % total_imgs == 0:\n        np.random.shuffle(xmls)\n    xml_name = xmls[step % total_imgs]\n    img_name = xml_name.replace(\'.xml\', \'.jpg\')\n\n    img = cv2.imread(os.path.join(root_path, \'train\', img_name))\n\n    img_height, img_width, gtbox_label = read_xml_gtbox_and_label(os.path.join(root_path, \'Annotations\', xml_name))\n\n    gtbox_and_label_list = np.array(gtbox_label, dtype=np.int32)\n    if gtbox_and_label_list.shape[0] == 0:\n        return next_img(step+1)\n    else:\n        return img_name, img[:, :, ::-1], gtbox_and_label_list\n\n\nif __name__ == \'__main__\':\n\n    imgid, img,  gtbox = next_img(3234)\n\n    print(""::"")\n    from libs.box_utils.draw_box_in_img import draw_boxes_with_label_and_scores\n\n    img = draw_boxes_with_label_and_scores(img_array=img, boxes=gtbox[:, :-1], labels=gtbox[:, -1],\n                                           scores=np.ones(shape=(len(gtbox), )))\n    print (""_----"")\n\n\n    cv2.imshow(""test"", img)\n    cv2.waitKey(0)\n\n\n'"
data/io/COCO/get_coco_next_batch.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport xml.etree.cElementTree as ET\nimport cv2\nimport numpy as np\nimport json\nimport os\nfrom libs.label_name_dict import coco_dict\nfrom libs.label_name_dict.label_dict import *\n\n\ncoco_trainvalmini = \'./coco_trainvalmini.odgt\'\n\n\ndef next_img(step):\n    with open(coco_trainvalmini) as f:\n        files = f.readlines()\n\n    total_imgs = len(files)\n    if step % total_imgs == 0:\n        np.random.shuffle(files)\n\n    raw_line = files[step % total_imgs]\n    file = json.loads(raw_line)\n    img_name = file[\'ID\']\n    # img_height, img_width = file[\'height\'], file[\'width\']\n\n    img = cv2.imread(file[\'fpath\'])\n\n    gtboxes = file[\'gtboxes\']\n\n    gtbox_label = []\n    for gt in gtboxes:\n        box = gt[\'box\']\n        label = gt[\'tag\']\n        gtbox_label.append([box[0], box[1], box[0]+box[2], box[1]+box[3], NAME_LABEL_MAP[label]])\n\n    gtbox_and_label_list = np.array(gtbox_label, dtype=np.int32)\n    if gtbox_and_label_list.shape[0] == 0:\n        return next_img(step+1)\n    else:\n        return img_name, img[:, :, ::-1], gtbox_and_label_list\n\n\nif __name__ == \'__main__\':\n\n    imgid, img,  gtbox = next_img(3234)\n\n    print(""::"")\n    from libs.box_utils.draw_box_in_img import draw_boxes_with_label_and_scores\n\n    img = draw_boxes_with_label_and_scores(img_array=img, boxes=gtbox[:, :-1], labels=gtbox[:, -1],\n                                           scores=np.ones(shape=(len(gtbox), )))\n    print(""_----"")\n\n    cv2.imwrite(""test.jpg"", img)\n\n\n\n'"
data/io/DOAI2019/__init__.py,0,b''
data/io/DOAI2019/train_crop.py,0,"b'import os\nimport scipy.misc as misc\nfrom xml.dom.minidom import Document\nimport numpy as np\nimport copy, cv2\n\n\ndef save_to_xml(save_path, im_width, im_height, objects_axis, label_name):\n    im_depth = 0\n    object_num = len(objects_axis)\n    doc = Document()\n\n    annotation = doc.createElement(\'annotation\')\n    doc.appendChild(annotation)\n\n    folder = doc.createElement(\'folder\')\n    folder_name = doc.createTextNode(\'VOC2007\')\n    folder.appendChild(folder_name)\n    annotation.appendChild(folder)\n\n    filename = doc.createElement(\'filename\')\n    filename_name = doc.createTextNode(\'000024.jpg\')\n    filename.appendChild(filename_name)\n    annotation.appendChild(filename)\n\n    source = doc.createElement(\'source\')\n    annotation.appendChild(source)\n\n    database = doc.createElement(\'database\')\n    database.appendChild(doc.createTextNode(\'The VOC2007 Database\'))\n    source.appendChild(database)\n\n    annotation_s = doc.createElement(\'annotation\')\n    annotation_s.appendChild(doc.createTextNode(\'PASCAL VOC2007\'))\n    source.appendChild(annotation_s)\n\n    image = doc.createElement(\'image\')\n    image.appendChild(doc.createTextNode(\'flickr\'))\n    source.appendChild(image)\n\n    flickrid = doc.createElement(\'flickrid\')\n    flickrid.appendChild(doc.createTextNode(\'322409915\'))\n    source.appendChild(flickrid)\n\n    owner = doc.createElement(\'owner\')\n    annotation.appendChild(owner)\n\n    flickrid_o = doc.createElement(\'flickrid\')\n    flickrid_o.appendChild(doc.createTextNode(\'knautia\'))\n    owner.appendChild(flickrid_o)\n\n    name_o = doc.createElement(\'name\')\n    name_o.appendChild(doc.createTextNode(\'yang\'))\n    owner.appendChild(name_o)\n\n    size = doc.createElement(\'size\')\n    annotation.appendChild(size)\n    width = doc.createElement(\'width\')\n    width.appendChild(doc.createTextNode(str(im_width)))\n    height = doc.createElement(\'height\')\n    height.appendChild(doc.createTextNode(str(im_height)))\n    depth = doc.createElement(\'depth\')\n    depth.appendChild(doc.createTextNode(str(im_depth)))\n    size.appendChild(width)\n    size.appendChild(height)\n    size.appendChild(depth)\n    segmented = doc.createElement(\'segmented\')\n    segmented.appendChild(doc.createTextNode(\'0\'))\n    annotation.appendChild(segmented)\n    for i in range(object_num):\n        objects = doc.createElement(\'object\')\n        annotation.appendChild(objects)\n        object_name = doc.createElement(\'name\')\n        object_name.appendChild(doc.createTextNode(label_name[int(objects_axis[i][-1])]))\n        objects.appendChild(object_name)\n        pose = doc.createElement(\'pose\')\n        pose.appendChild(doc.createTextNode(\'Unspecified\'))\n        objects.appendChild(pose)\n        truncated = doc.createElement(\'truncated\')\n        truncated.appendChild(doc.createTextNode(\'1\'))\n        objects.appendChild(truncated)\n        difficult = doc.createElement(\'difficult\')\n        difficult.appendChild(doc.createTextNode(\'0\'))\n        objects.appendChild(difficult)\n        bndbox = doc.createElement(\'bndbox\')\n        objects.appendChild(bndbox)\n\n        x0 = doc.createElement(\'x0\')\n        x0.appendChild(doc.createTextNode(str((objects_axis[i][0]))))\n        bndbox.appendChild(x0)\n        y0 = doc.createElement(\'y0\')\n        y0.appendChild(doc.createTextNode(str((objects_axis[i][1]))))\n        bndbox.appendChild(y0)\n\n        x1 = doc.createElement(\'x1\')\n        x1.appendChild(doc.createTextNode(str((objects_axis[i][2]))))\n        bndbox.appendChild(x1)\n        y1 = doc.createElement(\'y1\')\n        y1.appendChild(doc.createTextNode(str((objects_axis[i][3]))))\n        bndbox.appendChild(y1)\n\n        x2 = doc.createElement(\'x2\')\n        x2.appendChild(doc.createTextNode(str((objects_axis[i][4]))))\n        bndbox.appendChild(x2)\n        y2 = doc.createElement(\'y2\')\n        y2.appendChild(doc.createTextNode(str((objects_axis[i][5]))))\n        bndbox.appendChild(y2)\n\n        x3 = doc.createElement(\'x3\')\n        x3.appendChild(doc.createTextNode(str((objects_axis[i][6]))))\n        bndbox.appendChild(x3)\n        y3 = doc.createElement(\'y3\')\n        y3.appendChild(doc.createTextNode(str((objects_axis[i][7]))))\n        bndbox.appendChild(y3)\n\n    f = open(save_path, \'w\')\n    f.write(doc.toprettyxml(indent=\'\'))\n    f.close()\n\n\nclass_list = [\'plane\', \'baseball-diamond\', \'bridge\', \'ground-track-field\',\n              \'small-vehicle\', \'large-vehicle\', \'ship\',\n              \'tennis-court\', \'basketball-court\',\n              \'storage-tank\', \'soccer-ball-field\',\n              \'turntable\', \'harbor\',\n              \'swimming-pool\', \'helicopter\', \'container-crane\']\n\n\ndef format_label(txt_list):\n    format_data = []\n    for i in txt_list[4:]:\n        items = i.strip().split()\n        if items[8].strip() in class_list:\n            format_data.append([int(xy) for xy in items[:8]] + [class_list.index(items[8].strip())])\n        else:\n            print(\'warning found a new label :\', i.split(\' \')[8])\n            print(20 * ""..Warn.."")\n    return np.array(format_data)\n\n\ndef clip_image(file_idx, image, boxes_all, width, height, overlap=200):\n    # print (\'image shape\', image.shape)\n    if len(boxes_all) > 0:\n        shape = image.shape\n        step = width - overlap\n        for start_h in range(0, shape[0], step):  # 256\n            for start_w in range(0, shape[1], step):\n                boxes = copy.deepcopy(boxes_all)\n                box = np.zeros_like(boxes_all)\n                start_h_new = start_h\n                start_w_new = start_w\n                if start_h + height > shape[0]:\n                    start_h_new = shape[0] - height\n                if start_w + width > shape[1]:\n                    start_w_new = shape[1] - width\n                top_left_row = max(start_h_new, 0)\n                top_left_col = max(start_w_new, 0)\n                bottom_right_row = min(start_h + height, shape[0])\n                bottom_right_col = min(start_w + width, shape[1])\n\n                subImage = image[top_left_row:bottom_right_row, top_left_col: bottom_right_col]\n\n                box[:, 0] = boxes[:, 0] - top_left_col\n                box[:, 2] = boxes[:, 2] - top_left_col\n                box[:, 4] = boxes[:, 4] - top_left_col\n                box[:, 6] = boxes[:, 6] - top_left_col\n\n                box[:, 1] = boxes[:, 1] - top_left_row\n                box[:, 3] = boxes[:, 3] - top_left_row\n                box[:, 5] = boxes[:, 5] - top_left_row\n                box[:, 7] = boxes[:, 7] - top_left_row\n                box[:, 8] = boxes[:, 8]\n                center_y = 0.25 * (box[:, 1] + box[:, 3] + box[:, 5] + box[:, 7])\n                center_x = 0.25 * (box[:, 0] + box[:, 2] + box[:, 4] + box[:, 6])\n                # print(\'center_y\', center_y)\n                # print(\'center_x\', center_x)\n                # print (\'boxes\', boxes)\n                # print (\'boxes_all\', boxes_all)\n                # print (\'top_left_col\', top_left_col, \'top_left_row\', top_left_row)\n\n                cond1 = np.intersect1d(np.where(center_y[:] >= 0)[0], np.where(center_x[:] >= 0)[0])\n                cond2 = np.intersect1d(np.where(center_y[:] <= (bottom_right_row - top_left_row))[0],\n                                       np.where(center_x[:] <= (bottom_right_col - top_left_col))[0])\n                idx = np.intersect1d(cond1, cond2)\n                # idx = np.where(center_y[:]>=0 and center_x[:]>=0 and center_y[:] <= (bottom_right_row - top_left_row) and center_x[:] <= (bottom_right_col - top_left_col))[0]\n                # save_path, im_width, im_height, objects_axis, label_name\n                if len(idx) > 0:\n                    xml = os.path.join(save_dir, \'xml\',\n                                       ""%s_%04d_%04d.xml"" % (file_idx, top_left_row, top_left_col))\n                    save_to_xml(xml, subImage.shape[1], subImage.shape[0], box[idx, :], class_list)\n                    # print (\'save xml : \', xml)\n                    if subImage.shape[0] > 5 and subImage.shape[1] > 5:\n                        img_path = os.path.join(save_dir, ""images1.5"")\n                        if not os.path.exists(img_path):\n                            os.makedirs(img_path)\n                        img = os.path.join(save_dir, \'img\',\n                                           ""%s_%04d_%04d.png"" % (file_idx, top_left_row, top_left_col))\n                        cv2.imwrite(img, subImage)\n\n\nprint(\'class_list\', len(class_list))\nraw_data = \'/data/DOTA/val\'\nraw_images_dir = os.path.join(raw_data, \'images\')\nraw_label_dir = os.path.join(raw_data, \'labelTxt-v1.5\', \'labelTxt1.5\')\n\nsave_dir = \'/data/DOTA/DOTA_TOTAL\'\n\nimages = [i for i in os.listdir(raw_images_dir) if \'png\' in i]\nlabels = [i for i in os.listdir(raw_label_dir) if \'txt\' in i]\n\nprint(\'find image\', len(images))\nprint(\'find label\', len(labels))\n\nmin_length = 1e10\nmax_length = 1\n\nfor idx, img in enumerate(images):\n    # img = \'P1524.png\'\n    print(idx, \'read image\', img)\n\n    # img_data = misc.imread(os.path.join(raw_images_dir, img))\n    img_data = cv2.imread(os.path.join(raw_images_dir, img))\n\n    # if len(img_data.shape) == 2:\n    # img_data = img_data[:, :, np.newaxis]\n    # print (\'find gray image\')\n\n    txt_data = open(os.path.join(raw_label_dir, img.replace(\'png\', \'txt\')), \'r\').readlines()\n    # print (idx, len(format_label(txt_data)), img_data.shape)\n    # if max(img_data.shape[:2]) > max_length:\n    # max_length = max(img_data.shape[:2])\n    # if min(img_data.shape[:2]) < min_length:\n    # min_length = min(img_data.shape[:2])\n    # if idx % 50 ==0:\n    # print (idx, len(format_label(txt_data)), img_data.shape)\n    # print (idx, \'min_length\', min_length, \'max_length\', max_length)\n    box = format_label(txt_data)\n    clip_image(img.strip(\'.png\'), img_data, box, 800, 800, overlap=200)\n'"
data/io/DOTA/get_dota_next_batch.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport xml.etree.cElementTree as ET\nimport cv2\nimport numpy as np\nimport os\nfrom libs.label_name_dict import coco_dict\nfrom libs.label_name_dict.label_dict import *\n\n\nroot_path = \'/data/DOTA/DOTA_TOTAL/\'\nxmls = os.listdir(os.path.join(root_path, \'xml_h\'))\ntotal_imgs = len(xmls)\n\n# print (NAME_LABEL_DICT)\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 5],\n           and has [xmin, ymin, xmax, ymax, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef next_img(step):\n\n    if step % total_imgs == 0:\n        np.random.shuffle(xmls)\n    xml_name = xmls[step % total_imgs]\n    img_name = xml_name.replace(\'.xml\', \'.jpg\')\n\n    img = cv2.imread(os.path.join(root_path, \'img\', img_name))\n\n    img_height, img_width, gtbox_label = read_xml_gtbox_and_label(os.path.join(root_path, \'xml_h\', xml_name))\n\n    gtbox_and_label_list = np.array(gtbox_label, dtype=np.int32)\n    if gtbox_and_label_list.shape[0] == 0:\n        return next_img(step+1)\n    else:\n        return img_name, img[:, :, ::-1], gtbox_and_label_list\n\n\nif __name__ == \'__main__\':\n\n    imgid, img,  gtbox = next_img(3234)\n\n    print(""::"")\n    from libs.box_utils.draw_box_in_img import draw_boxes_with_label_and_scores\n\n    img = draw_boxes_with_label_and_scores(img_array=img, boxes=gtbox[:, :-1], labels=gtbox[:, -1],\n                                           scores=np.ones(shape=(len(gtbox), )))\n    print (""_----"")\n\n\n    cv2.imshow(""test"", img)\n    cv2.waitKey(0)\n\n\n'"
data/io/DOTA/train_crop.py,0,"b'import os\r\nimport scipy.misc as misc\r\nfrom xml.dom.minidom import Document\r\nimport numpy as np\r\nimport copy, cv2\r\n\r\ndef save_rbox_to_xml(save_path, im_width, im_height, objects_axis, label_name):\r\n    im_depth = 0\r\n    object_num = len(objects_axis)\r\n    doc = Document()\r\n\r\n    annotation = doc.createElement(\'annotation\')\r\n    doc.appendChild(annotation)\r\n\r\n    folder = doc.createElement(\'folder\')\r\n    folder_name = doc.createTextNode(\'DOTA\')\r\n    folder.appendChild(folder_name)\r\n    annotation.appendChild(folder)\r\n\r\n    filename = doc.createElement(\'filename\')\r\n    filename_name = doc.createTextNode(\'000024.jpg\')\r\n    filename.appendChild(filename_name)\r\n    annotation.appendChild(filename)\r\n\r\n    source = doc.createElement(\'source\')\r\n    annotation.appendChild(source)\r\n\r\n    database = doc.createElement(\'database\')\r\n    database.appendChild(doc.createTextNode(\'The DOTA Database\'))\r\n    source.appendChild(database)\r\n\r\n    annotation_s = doc.createElement(\'annotation\')\r\n    annotation_s.appendChild(doc.createTextNode(\'PASCAL VOC2007\'))\r\n    source.appendChild(annotation_s)\r\n\r\n    image = doc.createElement(\'image\')\r\n    image.appendChild(doc.createTextNode(\'flickr\'))\r\n    source.appendChild(image)\r\n\r\n    flickrid = doc.createElement(\'flickrid\')\r\n    flickrid.appendChild(doc.createTextNode(\'322409915\'))\r\n    source.appendChild(flickrid)\r\n\r\n    owner = doc.createElement(\'owner\')\r\n    annotation.appendChild(owner)\r\n\r\n    flickrid_o = doc.createElement(\'flickrid\')\r\n    flickrid_o.appendChild(doc.createTextNode(\'knautia\'))\r\n    owner.appendChild(flickrid_o)\r\n\r\n    name_o = doc.createElement(\'name\')\r\n    name_o.appendChild(doc.createTextNode(\'yang\'))\r\n    owner.appendChild(name_o)\r\n\r\n\r\n    size = doc.createElement(\'size\')\r\n    annotation.appendChild(size)\r\n    width = doc.createElement(\'width\')\r\n    width.appendChild(doc.createTextNode(str(im_width)))\r\n    height = doc.createElement(\'height\')\r\n    height.appendChild(doc.createTextNode(str(im_height)))\r\n    depth = doc.createElement(\'depth\')\r\n    depth.appendChild(doc.createTextNode(str(im_depth)))\r\n    size.appendChild(width)\r\n    size.appendChild(height)\r\n    size.appendChild(depth)\r\n    segmented = doc.createElement(\'segmented\')\r\n    segmented.appendChild(doc.createTextNode(\'0\'))\r\n    annotation.appendChild(segmented)\r\n    for i in range(object_num):\r\n        objects = doc.createElement(\'object\')\r\n        annotation.appendChild(objects)\r\n        object_name = doc.createElement(\'name\')\r\n        object_name.appendChild(doc.createTextNode(label_name[int(objects_axis[i][-1])]))\r\n        objects.appendChild(object_name)\r\n        pose = doc.createElement(\'pose\')\r\n        pose.appendChild(doc.createTextNode(\'Unspecified\'))\r\n        objects.appendChild(pose)\r\n        truncated = doc.createElement(\'truncated\')\r\n        truncated.appendChild(doc.createTextNode(\'1\'))\r\n        objects.appendChild(truncated)\r\n        difficult = doc.createElement(\'difficult\')\r\n        difficult.appendChild(doc.createTextNode(\'0\'))\r\n        objects.appendChild(difficult)\r\n        bndbox = doc.createElement(\'bndbox\')\r\n        objects.appendChild(bndbox)\r\n        \r\n        x0 = doc.createElement(\'x0\')\r\n        x0.appendChild(doc.createTextNode(str((objects_axis[i][0]))))\r\n        bndbox.appendChild(x0)\r\n        y0 = doc.createElement(\'y0\')\r\n        y0.appendChild(doc.createTextNode(str((objects_axis[i][1]))))\r\n        bndbox.appendChild(y0)\r\n\r\n        x1 = doc.createElement(\'x1\')\r\n        x1.appendChild(doc.createTextNode(str((objects_axis[i][2]))))\r\n        bndbox.appendChild(x1)\r\n        y1 = doc.createElement(\'y1\')\r\n        y1.appendChild(doc.createTextNode(str((objects_axis[i][3]))))\r\n        bndbox.appendChild(y1)\r\n        \r\n        x2 = doc.createElement(\'x2\')\r\n        x2.appendChild(doc.createTextNode(str((objects_axis[i][4]))))\r\n        bndbox.appendChild(x2)\r\n        y2 = doc.createElement(\'y2\')\r\n        y2.appendChild(doc.createTextNode(str((objects_axis[i][5]))))\r\n        bndbox.appendChild(y2)\r\n\r\n        x3 = doc.createElement(\'x3\')\r\n        x3.appendChild(doc.createTextNode(str((objects_axis[i][6]))))\r\n        bndbox.appendChild(x3)\r\n        y3 = doc.createElement(\'y3\')\r\n        y3.appendChild(doc.createTextNode(str((objects_axis[i][7]))))\r\n        bndbox.appendChild(y3)\r\n        \r\n    f = open(save_path,\'w\')\r\n    f.write(doc.toprettyxml(indent = \'\'))\r\n    f.close()\r\n\r\n\r\ndef save_hbox_to_xml(save_path, im_width, im_height, objects_axis, label_name):\r\n    im_depth = 0\r\n    object_num = len(objects_axis)\r\n    doc = Document()\r\n\r\n    annotation = doc.createElement(\'annotation\')\r\n    doc.appendChild(annotation)\r\n\r\n    folder = doc.createElement(\'folder\')\r\n    folder_name = doc.createTextNode(\'DOTA\')\r\n    folder.appendChild(folder_name)\r\n    annotation.appendChild(folder)\r\n\r\n    filename = doc.createElement(\'filename\')\r\n    filename_name = doc.createTextNode(\'000024.jpg\')\r\n    filename.appendChild(filename_name)\r\n    annotation.appendChild(filename)\r\n\r\n    source = doc.createElement(\'source\')\r\n    annotation.appendChild(source)\r\n\r\n    database = doc.createElement(\'database\')\r\n    database.appendChild(doc.createTextNode(\'The DOTA Database\'))\r\n    source.appendChild(database)\r\n\r\n    annotation_s = doc.createElement(\'annotation\')\r\n    annotation_s.appendChild(doc.createTextNode(\'PASCAL VOC2007\'))\r\n    source.appendChild(annotation_s)\r\n\r\n    image = doc.createElement(\'image\')\r\n    image.appendChild(doc.createTextNode(\'flickr\'))\r\n    source.appendChild(image)\r\n\r\n    flickrid = doc.createElement(\'flickrid\')\r\n    flickrid.appendChild(doc.createTextNode(\'322409915\'))\r\n    source.appendChild(flickrid)\r\n\r\n    owner = doc.createElement(\'owner\')\r\n    annotation.appendChild(owner)\r\n\r\n    flickrid_o = doc.createElement(\'flickrid\')\r\n    flickrid_o.appendChild(doc.createTextNode(\'knautia\'))\r\n    owner.appendChild(flickrid_o)\r\n\r\n    name_o = doc.createElement(\'name\')\r\n    name_o.appendChild(doc.createTextNode(\'yang\'))\r\n    owner.appendChild(name_o)\r\n\r\n    size = doc.createElement(\'size\')\r\n    annotation.appendChild(size)\r\n    width = doc.createElement(\'width\')\r\n    width.appendChild(doc.createTextNode(str(im_width)))\r\n    height = doc.createElement(\'height\')\r\n    height.appendChild(doc.createTextNode(str(im_height)))\r\n    depth = doc.createElement(\'depth\')\r\n    depth.appendChild(doc.createTextNode(str(im_depth)))\r\n    size.appendChild(width)\r\n    size.appendChild(height)\r\n    size.appendChild(depth)\r\n    segmented = doc.createElement(\'segmented\')\r\n    segmented.appendChild(doc.createTextNode(\'0\'))\r\n    annotation.appendChild(segmented)\r\n    for i in range(object_num):\r\n        objects = doc.createElement(\'object\')\r\n        annotation.appendChild(objects)\r\n        object_name = doc.createElement(\'name\')\r\n        object_name.appendChild(doc.createTextNode(label_name[int(objects_axis[i][-1])]))\r\n        objects.appendChild(object_name)\r\n        pose = doc.createElement(\'pose\')\r\n        pose.appendChild(doc.createTextNode(\'Unspecified\'))\r\n        objects.appendChild(pose)\r\n        truncated = doc.createElement(\'truncated\')\r\n        truncated.appendChild(doc.createTextNode(\'1\'))\r\n        objects.appendChild(truncated)\r\n        difficult = doc.createElement(\'difficult\')\r\n        difficult.appendChild(doc.createTextNode(\'0\'))\r\n        objects.appendChild(difficult)\r\n        bndbox = doc.createElement(\'bndbox\')\r\n        objects.appendChild(bndbox)\r\n\r\n        x0 = doc.createElement(\'xmin\')\r\n        x0.appendChild(doc.createTextNode(str(min(objects_axis[i][0:-1:2]))))\r\n        bndbox.appendChild(x0)\r\n        y0 = doc.createElement(\'ymin\')\r\n        y0.appendChild(doc.createTextNode(str(min(objects_axis[i][1:-1:2]))))\r\n        bndbox.appendChild(y0)\r\n\r\n        x1 = doc.createElement(\'xmax\')\r\n        x1.appendChild(doc.createTextNode(str(max(objects_axis[i][0:-1:2]))))\r\n        bndbox.appendChild(x1)\r\n        y1 = doc.createElement(\'ymax\')\r\n        y1.appendChild(doc.createTextNode(str(max(objects_axis[i][1:-1:2]))))\r\n        bndbox.appendChild(y1)\r\n\r\n    f = open(save_path, \'w\')\r\n    f.write(doc.toprettyxml(indent=\'\'))\r\n    f.close()\r\n\r\n\r\nclass_list = [\'plane\', \'baseball-diamond\', \'bridge\', \'ground-track-field\',\r\n              \'small-vehicle\', \'large-vehicle\', \'ship\',\r\n              \'tennis-court\', \'basketball-court\',\r\n              \'storage-tank\', \'soccer-ball-field\',\r\n              \'roundabout\', \'harbor\',\r\n              \'swimming-pool\', \'helicopter\']\r\n\r\n\r\ndef format_label(txt_list):\r\n    format_data = []\r\n    for i in txt_list[2:]:\r\n        format_data.append(\r\n        [int(xy) for xy in i.split(\' \')[:8]] + [class_list.index(i.split(\' \')[8])]\r\n        # {\'x0\': int(i.split(\' \')[0]),\r\n        # \'x1\': int(i.split(\' \')[2]),\r\n        # \'x2\': int(i.split(\' \')[4]),\r\n        # \'x3\': int(i.split(\' \')[6]),\r\n        # \'y1\': int(i.split(\' \')[1]),\r\n        # \'y2\': int(i.split(\' \')[3]),\r\n        # \'y3\': int(i.split(\' \')[5]),\r\n        # \'y4\': int(i.split(\' \')[7]),\r\n        # \'class\': class_list.index(i.split(\' \')[8]) if i.split(\' \')[8] in class_list else 0, \r\n        # \'difficulty\': int(i.split(\' \')[9])}\r\n        )\r\n        if i.split(\' \')[8] not in class_list :\r\n            print (\'warning found a new label :\', i.split(\' \')[8])\r\n            exit()\r\n    return np.array(format_data)\r\n\r\n\r\ndef clip_image(file_idx, image, boxes_all, width, height):\r\n    # print (\'image shape\', image.shape)\r\n    if len(boxes_all) > 0:\r\n        shape = image.shape\r\n        for start_h in range(0, shape[0], 600):\r\n            for start_w in range(0, shape[1], 600):\r\n                boxes = copy.deepcopy(boxes_all)\r\n                box = np.zeros_like(boxes_all)\r\n                start_h_new = start_h\r\n                start_w_new = start_w\r\n                if start_h + height > shape[0]:\r\n                  start_h_new = shape[0] - height\r\n                if start_w + width > shape[1]:\r\n                  start_w_new = shape[1] - width\r\n                top_left_row = max(start_h_new, 0)\r\n                top_left_col = max(start_w_new, 0)\r\n                bottom_right_row = min(start_h + height, shape[0])\r\n                bottom_right_col = min(start_w + width, shape[1])\r\n\r\n                subImage = image[top_left_row:bottom_right_row, top_left_col: bottom_right_col]\r\n\r\n                box[:, 0] = boxes[:, 0] - top_left_col\r\n                box[:, 2] = boxes[:, 2] - top_left_col\r\n                box[:, 4] = boxes[:, 4] - top_left_col\r\n                box[:, 6] = boxes[:, 6] - top_left_col\r\n\r\n                box[:, 1] = boxes[:, 1] - top_left_row\r\n                box[:, 3] = boxes[:, 3] - top_left_row\r\n                box[:, 5] = boxes[:, 5] - top_left_row\r\n                box[:, 7] = boxes[:, 7] - top_left_row\r\n                box[:, 8] = boxes[:, 8]\r\n                center_y = 0.25*(box[:, 1] + box[:, 3] + box[:, 5] + box[:, 7])\r\n                center_x = 0.25*(box[:, 0] + box[:, 2] + box[:, 4] + box[:, 6])\r\n                # print(\'center_y\', center_y)\r\n                # print(\'center_x\', center_x)\r\n                # print (\'boxes\', boxes)\r\n                # print (\'boxes_all\', boxes_all)\r\n                # print (\'top_left_col\', top_left_col, \'top_left_row\', top_left_row)\r\n\r\n                cond1 = np.intersect1d(np.where(center_y[:]>=0 )[0], np.where(center_x[:]>=0 )[0])\r\n                cond2 = np.intersect1d(np.where(center_y[:] <= (bottom_right_row - top_left_row))[0],\r\n                                        np.where(center_x[:] <= (bottom_right_col - top_left_col))[0])\r\n                idx = np.intersect1d(cond1, cond2)\r\n                # idx = np.where(center_y[:]>=0 and center_x[:]>=0 and center_y[:] <= (bottom_right_row - top_left_row) and center_x[:] <= (bottom_right_col - top_left_col))[0]\r\n                # save_path, im_width, im_height, objects_axis, label_name\r\n                if len(idx) > 0:\r\n                    xml_r = os.path.join(save_dir, \'xml_r\', ""%s_%04d_%04d.xml"" % (file_idx, top_left_row, top_left_col))\r\n                    xml_h = os.path.join(save_dir, \'xml_h\', ""%s_%04d_%04d.xml"" % (file_idx, top_left_row, top_left_col))\r\n                    save_rbox_to_xml(xml_r, subImage.shape[1], subImage.shape[0], box[idx, :], class_list)\r\n                    save_hbox_to_xml(xml_h, subImage.shape[1], subImage.shape[0], box[idx, :], class_list)\r\n                    # print (\'save xml : \', xml)\r\n                    if subImage.shape[0] > 5 and subImage.shape[1] >5:\r\n                        img = os.path.join(save_dir, \'img\', ""%s_%04d_%04d.png"" % (file_idx, top_left_row, top_left_col))\r\n                        cv2.imwrite(img, subImage)\r\n        \r\n\r\nprint(\'class_list\', len(class_list))\r\nraw_data = \'/data/DOTA/DOTA_TOTAL/\'\r\nraw_images_dir = os.path.join(raw_data, \'images\')\r\nraw_label_dir = os.path.join(raw_data, \'labelTxt\')\r\n\r\nsave_dir = \'/data/DOTA/DOTA_TOTAL/\'\r\n\r\nimages = [i for i in os.listdir(raw_images_dir) if \'png\' in i]\r\nlabels = [i for i in os.listdir(raw_label_dir) if \'txt\' in i]\r\n\r\nprint(\'find image\', len(images))\r\nprint(\'find label\', len(labels))\r\n\r\nmin_length = 1e10\r\nmax_length = 1\r\n\r\nfor idx, img in enumerate(images):\r\n# img = \'P1524.png\'\r\n    print (idx, \'read image\', img)\r\n    # img_data = misc.imread(os.path.join(raw_images_dir, img))\r\n    img_data = cv2.imread(os.path.join(raw_images_dir, img))\r\n\r\n    # if len(img_data.shape) == 2:\r\n        # img_data = img_data[:, :, np.newaxis]\r\n        # print (\'find gray image\')\r\n\r\n    txt_data = open(os.path.join(raw_label_dir, img.replace(\'png\', \'txt\')), \'r\').readlines()\r\n    # print (idx, len(format_label(txt_data)), img_data.shape)\r\n    # if max(img_data.shape[:2]) > max_length:\r\n        # max_length = max(img_data.shape[:2])\r\n    # if min(img_data.shape[:2]) < min_length:\r\n        # min_length = min(img_data.shape[:2])\r\n    # if idx % 50 ==0:\r\n        # print (idx, len(format_label(txt_data)), img_data.shape)\r\n        # print (idx, \'min_length\', min_length, \'max_length\', max_length)\r\n    box = format_label(txt_data)\r\n    clip_image(img.strip(\'.png\'), img_data, box, 800, 800)\r\n\r\n    \r\n#     rm train/images/*   &&   rm train/labeltxt/*\r\n\r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
data/io/DOTA/val_crop.py,0,"b'import os\r\nimport scipy.misc as misc\r\nfrom xml.dom.minidom import Document\r\nimport numpy as np\r\nimport copy, cv2\r\n\r\ndef save_to_xml(save_path, im_width, im_height, objects_axis, label_name):\r\n    im_depth = 0\r\n    object_num = len(objects_axis)\r\n    doc = Document()\r\n\r\n    annotation = doc.createElement(\'annotation\')\r\n    doc.appendChild(annotation)\r\n\r\n    folder = doc.createElement(\'folder\')\r\n    folder_name = doc.createTextNode(\'VOC2007\')\r\n    folder.appendChild(folder_name)\r\n    annotation.appendChild(folder)\r\n\r\n    filename = doc.createElement(\'filename\')\r\n    filename_name = doc.createTextNode(\'000024.jpg\')\r\n    filename.appendChild(filename_name)\r\n    annotation.appendChild(filename)\r\n\r\n    source = doc.createElement(\'source\')\r\n    annotation.appendChild(source)\r\n\r\n    database = doc.createElement(\'database\')\r\n    database.appendChild(doc.createTextNode(\'The VOC2007 Database\'))\r\n    source.appendChild(database)\r\n\r\n    annotation_s = doc.createElement(\'annotation\')\r\n    annotation_s.appendChild(doc.createTextNode(\'PASCAL VOC2007\'))\r\n    source.appendChild(annotation_s)\r\n\r\n    image = doc.createElement(\'image\')\r\n    image.appendChild(doc.createTextNode(\'flickr\'))\r\n    source.appendChild(image)\r\n\r\n    flickrid = doc.createElement(\'flickrid\')\r\n    flickrid.appendChild(doc.createTextNode(\'322409915\'))\r\n    source.appendChild(flickrid)\r\n\r\n    owner = doc.createElement(\'owner\')\r\n    annotation.appendChild(owner)\r\n\r\n    flickrid_o = doc.createElement(\'flickrid\')\r\n    flickrid_o.appendChild(doc.createTextNode(\'knautia\'))\r\n    owner.appendChild(flickrid_o)\r\n\r\n    name_o = doc.createElement(\'name\')\r\n    name_o.appendChild(doc.createTextNode(\'yang\'))\r\n    owner.appendChild(name_o)\r\n\r\n\r\n    size = doc.createElement(\'size\')\r\n    annotation.appendChild(size)\r\n    width = doc.createElement(\'width\')\r\n    width.appendChild(doc.createTextNode(str(im_width)))\r\n    height = doc.createElement(\'height\')\r\n    height.appendChild(doc.createTextNode(str(im_height)))\r\n    depth = doc.createElement(\'depth\')\r\n    depth.appendChild(doc.createTextNode(str(im_depth)))\r\n    size.appendChild(width)\r\n    size.appendChild(height)\r\n    size.appendChild(depth)\r\n    segmented = doc.createElement(\'segmented\')\r\n    segmented.appendChild(doc.createTextNode(\'0\'))\r\n    annotation.appendChild(segmented)\r\n    for i in range(object_num):\r\n        objects = doc.createElement(\'object\')\r\n        annotation.appendChild(objects)\r\n        object_name = doc.createElement(\'name\')\r\n        object_name.appendChild(doc.createTextNode(label_name[int(objects_axis[i][-1])]))\r\n        objects.appendChild(object_name)\r\n        pose = doc.createElement(\'pose\')\r\n        pose.appendChild(doc.createTextNode(\'Unspecified\'))\r\n        objects.appendChild(pose)\r\n        truncated = doc.createElement(\'truncated\')\r\n        truncated.appendChild(doc.createTextNode(\'1\'))\r\n        objects.appendChild(truncated)\r\n        difficult = doc.createElement(\'difficult\')\r\n        difficult.appendChild(doc.createTextNode(\'0\'))\r\n        objects.appendChild(difficult)\r\n        bndbox = doc.createElement(\'bndbox\')\r\n        objects.appendChild(bndbox)\r\n        \r\n        x0 = doc.createElement(\'x0\')\r\n        x0.appendChild(doc.createTextNode(str((objects_axis[i][0]))))\r\n        bndbox.appendChild(x0)\r\n        y0 = doc.createElement(\'y0\')\r\n        y0.appendChild(doc.createTextNode(str((objects_axis[i][1]))))\r\n        bndbox.appendChild(y0)\r\n\r\n        x1 = doc.createElement(\'x1\')\r\n        x1.appendChild(doc.createTextNode(str((objects_axis[i][2]))))\r\n        bndbox.appendChild(x1)\r\n        y1 = doc.createElement(\'y1\')\r\n        y1.appendChild(doc.createTextNode(str((objects_axis[i][3]))))\r\n        bndbox.appendChild(y1)\r\n        \r\n        x2 = doc.createElement(\'x2\')\r\n        x2.appendChild(doc.createTextNode(str((objects_axis[i][4]))))\r\n        bndbox.appendChild(x2)\r\n        y2 = doc.createElement(\'y2\')\r\n        y2.appendChild(doc.createTextNode(str((objects_axis[i][5]))))\r\n        bndbox.appendChild(y2)\r\n\r\n        x3 = doc.createElement(\'x3\')\r\n        x3.appendChild(doc.createTextNode(str((objects_axis[i][6]))))\r\n        bndbox.appendChild(x3)\r\n        y3 = doc.createElement(\'y3\')\r\n        y3.appendChild(doc.createTextNode(str((objects_axis[i][7]))))\r\n        bndbox.appendChild(y3)\r\n        \r\n    f = open(save_path,\'w\')\r\n    f.write(doc.toprettyxml(indent = \'\'))\r\n    f.close() \r\n\r\nclass_list = [\'plane\', \'baseball-diamond\', \'bridge\', \'ground-track-field\', \r\n\'small-vehicle\', \'large-vehicle\', \'ship\', \r\n\'tennis-court\', \'basketball-court\',  \r\n\'storage-tank\', \'soccer-ball-field\', \r\n\'roundabout\', \'harbor\', \r\n\'swimming-pool\', \'helicopter\']\r\n\r\n\r\n\r\n\r\ndef format_label(txt_list):\r\n    format_data = []\r\n    for i in txt_list[2:]:\r\n        format_data.append(\r\n        [int(xy) for xy in i.split(\' \')[:8]] + [class_list.index(i.split(\' \')[8])]\r\n        # {\'x0\': int(i.split(\' \')[0]),\r\n        # \'x1\': int(i.split(\' \')[2]),\r\n        # \'x2\': int(i.split(\' \')[4]),\r\n        # \'x3\': int(i.split(\' \')[6]),\r\n        # \'y1\': int(i.split(\' \')[1]),\r\n        # \'y2\': int(i.split(\' \')[3]),\r\n        # \'y3\': int(i.split(\' \')[5]),\r\n        # \'y4\': int(i.split(\' \')[7]),\r\n        # \'class\': class_list.index(i.split(\' \')[8]) if i.split(\' \')[8] in class_list else 0, \r\n        # \'difficulty\': int(i.split(\' \')[9])}\r\n        )\r\n        if i.split(\' \')[8] not in class_list :\r\n            print (\'warning found a new label :\', i.split(\' \')[8])\r\n            exit()\r\n    return np.array(format_data)\r\n\r\ndef clip_image(file_idx, image, boxes_all, width, height):\r\n    if len(boxes_all) > 0:\r\n    # print (\'image shape\', image.shape)\r\n        shape = image.shape\r\n        for start_h in range(0, shape[0], 600):\r\n            for start_w in range(0, shape[1], 600):\r\n                boxes = copy.deepcopy(boxes_all)\r\n                box = np.zeros_like(boxes_all)\r\n                start_h_new = start_h\r\n                start_w_new = start_w\r\n                if start_h + height > shape[0]:\r\n                  start_h_new = shape[0] - height\r\n                if start_w + width > shape[1]:\r\n                  start_w_new = shape[1] - width\r\n                top_left_row = max(start_h_new, 0)\r\n                top_left_col = max(start_w_new, 0)\r\n                bottom_right_row = min(start_h + height, shape[0])\r\n                bottom_right_col = min(start_w + width, shape[1])\r\n\r\n\r\n                subImage = image[top_left_row:bottom_right_row, top_left_col: bottom_right_col]\r\n                box[:, 0] = boxes[:, 0] - top_left_col\r\n                box[:, 2] = boxes[:, 2] - top_left_col\r\n                box[:, 4] = boxes[:, 4] - top_left_col\r\n                box[:, 6] = boxes[:, 6] - top_left_col\r\n\r\n                box[:, 1] = boxes[:, 1] - top_left_row\r\n                box[:, 3] = boxes[:, 3] - top_left_row\r\n                box[:, 5] = boxes[:, 5] - top_left_row\r\n                box[:, 7] = boxes[:, 7] - top_left_row\r\n                box[:, 8] = boxes[:, 8]\r\n                center_y = 0.25*(box[:, 1] + box[:, 3] + box[:, 5] + box[:, 7])\r\n                center_x = 0.25*(box[:, 0] + box[:, 2] + box[:, 4] + box[:, 6])\r\n                # print(\'center_y\', center_y)\r\n                # print(\'center_x\', center_x)\r\n                # print (\'boxes\', boxes)\r\n                # print (\'boxes_all\', boxes_all)\r\n                # print (\'top_left_col\', top_left_col, \'top_left_row\', top_left_row)\r\n\r\n                cond1 = np.intersect1d(np.where(center_y[:]>=0 )[0], np.where(center_x[:]>=0 )[0])\r\n                cond2 = np.intersect1d(np.where(center_y[:] <= (bottom_right_row - top_left_row))[0],\r\n                                        np.where(center_x[:] <= (bottom_right_col - top_left_col))[0])\r\n                idx = np.intersect1d(cond1, cond2)\r\n                # idx = np.where(center_y[:]>=0 and center_x[:]>=0 and center_y[:] <= (bottom_right_row - top_left_row) and center_x[:] <= (bottom_right_col - top_left_col))[0]\r\n                # save_path, im_width, im_height, objects_axis, label_name\r\n                if len(idx) > 0:\r\n                    xml = os.path.join(save_dir, \'labeltxt\', ""%s_%04d_%04d.xml"" % (file_idx, top_left_row, top_left_col))\r\n                    save_to_xml(xml, subImage.shape[1], subImage.shape[0], box[idx, :], class_list)\r\n                    # print (\'save xml : \', xml)\r\n                    if subImage.shape[0] > 5 and subImage.shape[1] >5:\r\n                        img = os.path.join(save_dir, \'images\', ""%s_%04d_%04d.png"" % (file_idx, top_left_row, top_left_col))\r\n                        cv2.imwrite(img, subImage)\r\n        \r\n    \r\n    \r\n\r\nprint (\'class_list\', len(class_list))\r\nraw_data = \'/dataset/DOTA/val/\'\r\nraw_images_dir = os.path.join(raw_data, \'images\')\r\nraw_label_dir = os.path.join(raw_data, \'labelTxt\')\r\n\r\nsave_dir = \'/dataset/DOTA_clip/val/\'\r\n\r\nimages = [i for i in os.listdir(raw_images_dir) if \'png\' in i]\r\nlabels = [i for i in os.listdir(raw_label_dir) if \'txt\' in i]\r\n\r\nprint (\'find image\', len(images))\r\nprint (\'find label\', len(labels))\r\n\r\nmin_length = 1e10\r\nmax_length = 1\r\n\r\n\r\nfor idx, img in enumerate(images):\r\n    # img = \'P2330.png\'\r\n    print (idx, \'read image\', img)\r\n    # img_data = misc.imread(os.path.join(raw_images_dir, img))\r\n    img_data = cv2.imread(os.path.join(raw_images_dir, img))\r\n\r\n    # if len(img_data.shape) == 2:\r\n        # img_data = img_data[:, :, np.newaxis]\r\n        # print (\'find gray image\')\r\n\r\n    txt_data = open(os.path.join(raw_label_dir, img.replace(\'png\', \'txt\')), \'r\').readlines()\r\n    # print (idx, len(format_label(txt_data)), img_data.shape)\r\n    # if max(img_data.shape[:2]) > max_length:\r\n        # max_length = max(img_data.shape[:2])\r\n    # if min(img_data.shape[:2]) < min_length:\r\n        # min_length = min(img_data.shape[:2])\r\n    # if idx % 50 ==0:\r\n        # print (idx, len(format_label(txt_data)), img_data.shape)\r\n        # print (idx, \'min_length\', min_length, \'max_length\', max_length)\r\n    box = format_label(txt_data)\r\n    clip_image(img.strip(\'.png\'), img_data, box, 800, 800)\r\n    \r\n    \r\n    \r\n    \r\n\r\n\r\n'"
data/lib_coco/PythonAPI/__init__.py,0,b''
data/lib_coco/PythonAPI/setup.py,0,"b'from setuptools import setup, Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'../common/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [np.get_include(), \'../common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(\n    name=\'pycocotools\',\n    packages=[\'pycocotools\'],\n    package_dir = {\'pycocotools\': \'pycocotools\'},\n    install_requires=[\n        \'setuptools>=18.0\',\n        \'cython>=0.27.3\',\n        \'matplotlib>=2.1.0\'\n    ],\n    version=\'2.0\',\n    ext_modules= ext_modules\n)\n'"
libs/box_utils/cython_utils/__init__.py,0,b''
libs/box_utils/cython_utils/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""cython_bbox"",\n        [""bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""cython_nms"",\n        [""nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    )\n    # Extension(\n    #     ""cpu_nms"",\n    #     [""cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # )\n]\n\nsetup(\n    name=\'tf_faster_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
libs/configs/COCO/__init__.py,0,b''
libs/configs/COCO/cfgs_res50_1x_coco_v1.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ngluoncv backbone   baseline\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_COCO_20190429_v1\'\nNET_NAME = \'resnet50_v1d\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nSHOW_TRAIN_INFO_INTE = 100\nSMRY_ITER = 2000\nSAVE_WEIGHTS_INTE = 80000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nCUDA9 = True\nEVAL_THRESHOLD = 0.5\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nBATCH_SIZE = 1\nWARM_SETP = int(0.25 * SAVE_WEIGHTS_INTE)\nLR = 5e-4 * 2 * 1.25 * len(GPU_GROUP.strip().split(\',\')) * BATCH_SIZE\nDECAY_STEP = [12*SAVE_WEIGHTS_INTE, 16*SAVE_WEIGHTS_INTE, 20*SAVE_WEIGHTS_INTE]\nMAX_ITERATION = 20*SAVE_WEIGHTS_INTE\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1333\nCLASS_NUM = 80\n\n# --------------------------------------------- Network_config\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\nIS_ASSIGN = True\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = True\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# --------------------------------------------NAS FPN config\nNUM_FPN = 0\nNUM_NAS_FPN = 0\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.5  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/COCO/cfgs_res50_1x_coco_v2.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ngluoncv backbone\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_COCO_20190429_v2\'\nNET_NAME = \'resnet50_v1d\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nSHOW_TRAIN_INFO_INTE = 100\nSMRY_ITER = 2000\nSAVE_WEIGHTS_INTE = 80000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nCUDA9 = True\nEVAL_THRESHOLD = 0.5\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nBATCH_SIZE = 1\nWARM_SETP = int(0.25 * SAVE_WEIGHTS_INTE)\nLR = 5e-4 * 2 * 1.25 * len(GPU_GROUP.strip().split(\',\')) * BATCH_SIZE\nDECAY_STEP = [12*SAVE_WEIGHTS_INTE, 16*SAVE_WEIGHTS_INTE, 20*SAVE_WEIGHTS_INTE]\nMAX_ITERATION = 20*SAVE_WEIGHTS_INTE\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1333\nCLASS_NUM = 80\n\n# --------------------------------------------- Network_config\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\nIS_ASSIGN = True\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = True\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# --------------------------------------------NAS FPN config\nNUM_FPN = 3\nNUM_NAS_FPN = 0\nUSE_RELU = True\nFPN_CHANNEL = 256\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.5  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/COCO/cfgs_res50_1x_coco_v3.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ngluoncv backbone\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_COCO_20190429_v3\'\nNET_NAME = \'resnet50_v1d\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nSHOW_TRAIN_INFO_INTE = 100\nSMRY_ITER = 2000\nSAVE_WEIGHTS_INTE = 80000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nCUDA9 = True\nEVAL_THRESHOLD = 0.5\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nBATCH_SIZE = 1\nWARM_SETP = int(0.25 * SAVE_WEIGHTS_INTE)\nLR = 5e-4 * 2 * 1.25 * len(GPU_GROUP.strip().split(\',\')) * BATCH_SIZE\nDECAY_STEP = [12*SAVE_WEIGHTS_INTE, 16*SAVE_WEIGHTS_INTE, 20*SAVE_WEIGHTS_INTE]\nMAX_ITERATION = 20*SAVE_WEIGHTS_INTE\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1333\nCLASS_NUM = 80\n\n# --------------------------------------------- Network_config\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\nIS_ASSIGN = True\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = True\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# --------------------------------------------NAS FPN config\nNUM_FPN = 0\nNUM_NAS_FPN = 3\nUSE_RELU = True\nFPN_CHANNEL = 256\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.5  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/COCO/cfgs_res50_1x_coco_v4.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ngluoncv backbone\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_COCO_20190503_v4\'\nNET_NAME = \'resnet50_v1d\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nSHOW_TRAIN_INFO_INTE = 100\nSMRY_ITER = 2000\nSAVE_WEIGHTS_INTE = 80000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nCUDA9 = True\nEVAL_THRESHOLD = 0.5\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nBATCH_SIZE = 1\nWARM_SETP = int(0.25 * SAVE_WEIGHTS_INTE)\nLR = 5e-4 * 2 * 1.25 * len(GPU_GROUP.strip().split(\',\')) * BATCH_SIZE\nDECAY_STEP = [12*SAVE_WEIGHTS_INTE, 16*SAVE_WEIGHTS_INTE, 20*SAVE_WEIGHTS_INTE]\nMAX_ITERATION = 20*SAVE_WEIGHTS_INTE\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1333\nCLASS_NUM = 80\n\n# --------------------------------------------- Network_config\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\nIS_ASSIGN = True\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = True\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# --------------------------------------------NAS FPN config\nNUM_FPN = 0\nNUM_NAS_FPN = 7\nUSE_RELU = True\nFPN_CHANNEL = 256\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.5  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/COCO/cfgs_res50_1x_coco_v5.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ngluoncv backbone\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_COCO_20190503_v5\'\nNET_NAME = \'resnet50_v1d\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nSHOW_TRAIN_INFO_INTE = 100\nSMRY_ITER = 2000\nSAVE_WEIGHTS_INTE = 80000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nCUDA9 = True\nEVAL_THRESHOLD = 0.5\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nBATCH_SIZE = 1\nWARM_SETP = int(0.25 * SAVE_WEIGHTS_INTE)\nLR = 5e-4 * 2 * 1.25 * len(GPU_GROUP.strip().split(\',\')) * BATCH_SIZE\nDECAY_STEP = [12*SAVE_WEIGHTS_INTE, 16*SAVE_WEIGHTS_INTE, 20*SAVE_WEIGHTS_INTE]\nMAX_ITERATION = 20*SAVE_WEIGHTS_INTE\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1333\nCLASS_NUM = 80\n\n# --------------------------------------------- Network_config\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\nIS_ASSIGN = True\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = True\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# --------------------------------------------NAS FPN config\nNUM_FPN = 0\nNUM_NAS_FPN = 7\nUSE_RELU = True\nFPN_CHANNEL = 384\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.5  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/configs/COCO/cfgs_res50_1x_coco_v6.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\'\'\'\ngluoncv backbone\n\n\'\'\'\n\n# ------------------------------------------------\nVERSION = \'FPN_Res50_COCO_20190505_v6\'\nNET_NAME = \'resnet50_v1d\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nSHOW_TRAIN_INFO_INTE = 100\nSMRY_ITER = 2000\nSAVE_WEIGHTS_INTE = 80000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = False\nFIXED_BLOCKS = 0  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nCUDA9 = True\nEVAL_THRESHOLD = 0.5\n\nRPN_LOCATION_LOSS_WEIGHT = 1.\nRPN_CLASSIFICATION_LOSS_WEIGHT = 1.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 1.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 1.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\nMUTILPY_BIAS_GRADIENT = None   # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nBATCH_SIZE = 1\nWARM_SETP = int(0.25 * SAVE_WEIGHTS_INTE)\nLR = 5e-4 * 2 * 1.25 * len(GPU_GROUP.strip().split(\',\')) * BATCH_SIZE\nDECAY_STEP = [12*SAVE_WEIGHTS_INTE, 16*SAVE_WEIGHTS_INTE, 20*SAVE_WEIGHTS_INTE]\nMAX_ITERATION = 20*SAVE_WEIGHTS_INTE\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1333\nCLASS_NUM = 80\n\n# --------------------------------------------- Network_config\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.00004 if NET_NAME.startswith(\'Mobilenet\') else 0.0001\nIS_ASSIGN = True\n\n# ---------------------------------------------Anchor config\nUSE_CENTER_OFFSET = True\nLEVLES = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE_LIST = [4, 8, 16, 32, 64]\nANCHOR_SCALES = [1.0]\nANCHOR_RATIOS = [0.5, 1., 2.0]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n# --------------------------------------------FPN config\nSHARE_HEADS = True\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 256\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 6000\nRPN_MAXIMUM_PROPOSAL_TEST = 1000\n\n# --------------------------------------------NAS FPN config\nNUM_FPN = 0\nNUM_NAS_FPN = 0\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.6  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.5  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 100\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.5\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.25\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/gluon2TF/mxnet_weights/mxnet_weights_namefile.py,0,"b""\n\n# for resNet 50:::\n'''\nname: bn1.beta || shape: (64,) || dtype: float32\nname: bn1.gamma || shape: (64,) || dtype: float32\nname: bn1.running_mean || shape: (64,) || dtype: float32\nname: bn1.running_var || shape: (64,) || dtype: float32\nname: conv1.0.weight || shape: (32, 3, 3, 3) || dtype: float32\nname: conv1.1.beta || shape: (32,) || dtype: float32\nname: conv1.1.gamma || shape: (32,) || dtype: float32\nname: conv1.1.running_mean || shape: (32,) || dtype: float32\nname: conv1.1.running_var || shape: (32,) || dtype: float32\nname: conv1.3.weight || shape: (32, 32, 3, 3) || dtype: float32\nname: conv1.4.beta || shape: (32,) || dtype: float32\nname: conv1.4.gamma || shape: (32,) || dtype: float32\nname: conv1.4.running_mean || shape: (32,) || dtype: float32\nname: conv1.4.running_var || shape: (32,) || dtype: float32\nname: conv1.6.weight || shape: (64, 32, 3, 3) || dtype: float32\nname: fc.bias || shape: (1000,) || dtype: float32\nname: fc.weight || shape: (1000, 2048) || dtype: float32\nname: layer1.0.bn1.beta || shape: (64,) || dtype: float32\nname: layer1.0.bn1.gamma || shape: (64,) || dtype: float32\nname: layer1.0.bn1.running_mean || shape: (64,) || dtype: float32\nname: layer1.0.bn1.running_var || shape: (64,) || dtype: float32\nname: layer1.0.bn2.beta || shape: (64,) || dtype: float32\nname: layer1.0.bn2.gamma || shape: (64,) || dtype: float32\nname: layer1.0.bn2.running_mean || shape: (64,) || dtype: float32\nname: layer1.0.bn2.running_var || shape: (64,) || dtype: float32\nname: layer1.0.bn3.beta || shape: (256,) || dtype: float32\nname: layer1.0.bn3.gamma || shape: (256,) || dtype: float32\nname: layer1.0.bn3.running_mean || shape: (256,) || dtype: float32\nname: layer1.0.bn3.running_var || shape: (256,) || dtype: float32\nname: layer1.0.conv1.weight || shape: (64, 64, 1, 1) || dtype: float32\nname: layer1.0.conv2.weight || shape: (64, 64, 3, 3) || dtype: float32\nname: layer1.0.conv3.weight || shape: (256, 64, 1, 1) || dtype: float32\nname: layer1.0.downsample.1.weight || shape: (256, 64, 1, 1) || dtype: float32\nname: layer1.0.downsample.2.beta || shape: (256,) || dtype: float32\nname: layer1.0.downsample.2.gamma || shape: (256,) || dtype: float32\nname: layer1.0.downsample.2.running_mean || shape: (256,) || dtype: float32\nname: layer1.0.downsample.2.running_var || shape: (256,) || dtype: float32\nname: layer1.1.bn1.beta || shape: (64,) || dtype: float32\nname: layer1.1.bn1.gamma || shape: (64,) || dtype: float32\nname: layer1.1.bn1.running_mean || shape: (64,) || dtype: float32\nname: layer1.1.bn1.running_var || shape: (64,) || dtype: float32\nname: layer1.1.bn2.beta || shape: (64,) || dtype: float32\nname: layer1.1.bn2.gamma || shape: (64,) || dtype: float32\nname: layer1.1.bn2.running_mean || shape: (64,) || dtype: float32\nname: layer1.1.bn2.running_var || shape: (64,) || dtype: float32\nname: layer1.1.bn3.beta || shape: (256,) || dtype: float32\nname: layer1.1.bn3.gamma || shape: (256,) || dtype: float32\nname: layer1.1.bn3.running_mean || shape: (256,) || dtype: float32\nname: layer1.1.bn3.running_var || shape: (256,) || dtype: float32\nname: layer1.1.conv1.weight || shape: (64, 256, 1, 1) || dtype: float32\nname: layer1.1.conv2.weight || shape: (64, 64, 3, 3) || dtype: float32\nname: layer1.1.conv3.weight || shape: (256, 64, 1, 1) || dtype: float32\nname: layer1.2.bn1.beta || shape: (64,) || dtype: float32\nname: layer1.2.bn1.gamma || shape: (64,) || dtype: float32\nname: layer1.2.bn1.running_mean || shape: (64,) || dtype: float32\nname: layer1.2.bn1.running_var || shape: (64,) || dtype: float32\nname: layer1.2.bn2.beta || shape: (64,) || dtype: float32\nname: layer1.2.bn2.gamma || shape: (64,) || dtype: float32\nname: layer1.2.bn2.running_mean || shape: (64,) || dtype: float32\nname: layer1.2.bn2.running_var || shape: (64,) || dtype: float32\nname: layer1.2.bn3.beta || shape: (256,) || dtype: float32\nname: layer1.2.bn3.gamma || shape: (256,) || dtype: float32\nname: layer1.2.bn3.running_mean || shape: (256,) || dtype: float32\nname: layer1.2.bn3.running_var || shape: (256,) || dtype: float32\nname: layer1.2.conv1.weight || shape: (64, 256, 1, 1) || dtype: float32\nname: layer1.2.conv2.weight || shape: (64, 64, 3, 3) || dtype: float32\nname: layer1.2.conv3.weight || shape: (256, 64, 1, 1) || dtype: float32\nname: layer2.0.bn1.beta || shape: (128,) || dtype: float32\nname: layer2.0.bn1.gamma || shape: (128,) || dtype: float32\nname: layer2.0.bn1.running_mean || shape: (128,) || dtype: float32\nname: layer2.0.bn1.running_var || shape: (128,) || dtype: float32\nname: layer2.0.bn2.beta || shape: (128,) || dtype: float32\nname: layer2.0.bn2.gamma || shape: (128,) || dtype: float32\nname: layer2.0.bn2.running_mean || shape: (128,) || dtype: float32\nname: layer2.0.bn2.running_var || shape: (128,) || dtype: float32\nname: layer2.0.bn3.beta || shape: (512,) || dtype: float32\nname: layer2.0.bn3.gamma || shape: (512,) || dtype: float32\nname: layer2.0.bn3.running_mean || shape: (512,) || dtype: float32\nname: layer2.0.bn3.running_var || shape: (512,) || dtype: float32\nname: layer2.0.conv1.weight || shape: (128, 256, 1, 1) || dtype: float32\nname: layer2.0.conv2.weight || shape: (128, 128, 3, 3) || dtype: float32\nname: layer2.0.conv3.weight || shape: (512, 128, 1, 1) || dtype: float32\nname: layer2.0.downsample.1.weight || shape: (512, 256, 1, 1) || dtype: float32\nname: layer2.0.downsample.2.beta || shape: (512,) || dtype: float32\nname: layer2.0.downsample.2.gamma || shape: (512,) || dtype: float32\nname: layer2.0.downsample.2.running_mean || shape: (512,) || dtype: float32\nname: layer2.0.downsample.2.running_var || shape: (512,) || dtype: float32\nname: layer2.1.bn1.beta || shape: (128,) || dtype: float32\nname: layer2.1.bn1.gamma || shape: (128,) || dtype: float32\nname: layer2.1.bn1.running_mean || shape: (128,) || dtype: float32\nname: layer2.1.bn1.running_var || shape: (128,) || dtype: float32\nname: layer2.1.bn2.beta || shape: (128,) || dtype: float32\nname: layer2.1.bn2.gamma || shape: (128,) || dtype: float32\nname: layer2.1.bn2.running_mean || shape: (128,) || dtype: float32\nname: layer2.1.bn2.running_var || shape: (128,) || dtype: float32\nname: layer2.1.bn3.beta || shape: (512,) || dtype: float32\nname: layer2.1.bn3.gamma || shape: (512,) || dtype: float32\nname: layer2.1.bn3.running_mean || shape: (512,) || dtype: float32\nname: layer2.1.bn3.running_var || shape: (512,) || dtype: float32\nname: layer2.1.conv1.weight || shape: (128, 512, 1, 1) || dtype: float32\nname: layer2.1.conv2.weight || shape: (128, 128, 3, 3) || dtype: float32\nname: layer2.1.conv3.weight || shape: (512, 128, 1, 1) || dtype: float32\nname: layer2.2.bn1.beta || shape: (128,) || dtype: float32\nname: layer2.2.bn1.gamma || shape: (128,) || dtype: float32\nname: layer2.2.bn1.running_mean || shape: (128,) || dtype: float32\nname: layer2.2.bn1.running_var || shape: (128,) || dtype: float32\nname: layer2.2.bn2.beta || shape: (128,) || dtype: float32\nname: layer2.2.bn2.gamma || shape: (128,) || dtype: float32\nname: layer2.2.bn2.running_mean || shape: (128,) || dtype: float32\nname: layer2.2.bn2.running_var || shape: (128,) || dtype: float32\nname: layer2.2.bn3.beta || shape: (512,) || dtype: float32\nname: layer2.2.bn3.gamma || shape: (512,) || dtype: float32\nname: layer2.2.bn3.running_mean || shape: (512,) || dtype: float32\nname: layer2.2.bn3.running_var || shape: (512,) || dtype: float32\nname: layer2.2.conv1.weight || shape: (128, 512, 1, 1) || dtype: float32\nname: layer2.2.conv2.weight || shape: (128, 128, 3, 3) || dtype: float32\nname: layer2.2.conv3.weight || shape: (512, 128, 1, 1) || dtype: float32\nname: layer2.3.bn1.beta || shape: (128,) || dtype: float32\nname: layer2.3.bn1.gamma || shape: (128,) || dtype: float32\nname: layer2.3.bn1.running_mean || shape: (128,) || dtype: float32\nname: layer2.3.bn1.running_var || shape: (128,) || dtype: float32\nname: layer2.3.bn2.beta || shape: (128,) || dtype: float32\nname: layer2.3.bn2.gamma || shape: (128,) || dtype: float32\nname: layer2.3.bn2.running_mean || shape: (128,) || dtype: float32\nname: layer2.3.bn2.running_var || shape: (128,) || dtype: float32\nname: layer2.3.bn3.beta || shape: (512,) || dtype: float32\nname: layer2.3.bn3.gamma || shape: (512,) || dtype: float32\nname: layer2.3.bn3.running_mean || shape: (512,) || dtype: float32\nname: layer2.3.bn3.running_var || shape: (512,) || dtype: float32\nname: layer2.3.conv1.weight || shape: (128, 512, 1, 1) || dtype: float32\nname: layer2.3.conv2.weight || shape: (128, 128, 3, 3) || dtype: float32\nname: layer2.3.conv3.weight || shape: (512, 128, 1, 1) || dtype: float32\nname: layer3.0.bn1.beta || shape: (256,) || dtype: float32\nname: layer3.0.bn1.gamma || shape: (256,) || dtype: float32\nname: layer3.0.bn1.running_mean || shape: (256,) || dtype: float32\nname: layer3.0.bn1.running_var || shape: (256,) || dtype: float32\nname: layer3.0.bn2.beta || shape: (256,) || dtype: float32\nname: layer3.0.bn2.gamma || shape: (256,) || dtype: float32\nname: layer3.0.bn2.running_mean || shape: (256,) || dtype: float32\nname: layer3.0.bn2.running_var || shape: (256,) || dtype: float32\nname: layer3.0.bn3.beta || shape: (1024,) || dtype: float32\nname: layer3.0.bn3.gamma || shape: (1024,) || dtype: float32\nname: layer3.0.bn3.running_mean || shape: (1024,) || dtype: float32\nname: layer3.0.bn3.running_var || shape: (1024,) || dtype: float32\nname: layer3.0.conv1.weight || shape: (256, 512, 1, 1) || dtype: float32\nname: layer3.0.conv2.weight || shape: (256, 256, 3, 3) || dtype: float32\nname: layer3.0.conv3.weight || shape: (1024, 256, 1, 1) || dtype: float32\nname: layer3.0.downsample.1.weight || shape: (1024, 512, 1, 1) || dtype: float32\nname: layer3.0.downsample.2.beta || shape: (1024,) || dtype: float32\nname: layer3.0.downsample.2.gamma || shape: (1024,) || dtype: float32\nname: layer3.0.downsample.2.running_mean || shape: (1024,) || dtype: float32\nname: layer3.0.downsample.2.running_var || shape: (1024,) || dtype: float32\nname: layer3.1.bn1.beta || shape: (256,) || dtype: float32\nname: layer3.1.bn1.gamma || shape: (256,) || dtype: float32\nname: layer3.1.bn1.running_mean || shape: (256,) || dtype: float32\nname: layer3.1.bn1.running_var || shape: (256,) || dtype: float32\nname: layer3.1.bn2.beta || shape: (256,) || dtype: float32\nname: layer3.1.bn2.gamma || shape: (256,) || dtype: float32\nname: layer3.1.bn2.running_mean || shape: (256,) || dtype: float32\nname: layer3.1.bn2.running_var || shape: (256,) || dtype: float32\nname: layer3.1.bn3.beta || shape: (1024,) || dtype: float32\nname: layer3.1.bn3.gamma || shape: (1024,) || dtype: float32\nname: layer3.1.bn3.running_mean || shape: (1024,) || dtype: float32\nname: layer3.1.bn3.running_var || shape: (1024,) || dtype: float32\nname: layer3.1.conv1.weight || shape: (256, 1024, 1, 1) || dtype: float32\nname: layer3.1.conv2.weight || shape: (256, 256, 3, 3) || dtype: float32\nname: layer3.1.conv3.weight || shape: (1024, 256, 1, 1) || dtype: float32\nname: layer3.2.bn1.beta || shape: (256,) || dtype: float32\nname: layer3.2.bn1.gamma || shape: (256,) || dtype: float32\nname: layer3.2.bn1.running_mean || shape: (256,) || dtype: float32\nname: layer3.2.bn1.running_var || shape: (256,) || dtype: float32\nname: layer3.2.bn2.beta || shape: (256,) || dtype: float32\nname: layer3.2.bn2.gamma || shape: (256,) || dtype: float32\nname: layer3.2.bn2.running_mean || shape: (256,) || dtype: float32\nname: layer3.2.bn2.running_var || shape: (256,) || dtype: float32\nname: layer3.2.bn3.beta || shape: (1024,) || dtype: float32\nname: layer3.2.bn3.gamma || shape: (1024,) || dtype: float32\nname: layer3.2.bn3.running_mean || shape: (1024,) || dtype: float32\nname: layer3.2.bn3.running_var || shape: (1024,) || dtype: float32\nname: layer3.2.conv1.weight || shape: (256, 1024, 1, 1) || dtype: float32\nname: layer3.2.conv2.weight || shape: (256, 256, 3, 3) || dtype: float32\nname: layer3.2.conv3.weight || shape: (1024, 256, 1, 1) || dtype: float32\nname: layer3.3.bn1.beta || shape: (256,) || dtype: float32\nname: layer3.3.bn1.gamma || shape: (256,) || dtype: float32\nname: layer3.3.bn1.running_mean || shape: (256,) || dtype: float32\nname: layer3.3.bn1.running_var || shape: (256,) || dtype: float32\nname: layer3.3.bn2.beta || shape: (256,) || dtype: float32\nname: layer3.3.bn2.gamma || shape: (256,) || dtype: float32\nname: layer3.3.bn2.running_mean || shape: (256,) || dtype: float32\nname: layer3.3.bn2.running_var || shape: (256,) || dtype: float32\nname: layer3.3.bn3.beta || shape: (1024,) || dtype: float32\nname: layer3.3.bn3.gamma || shape: (1024,) || dtype: float32\nname: layer3.3.bn3.running_mean || shape: (1024,) || dtype: float32\nname: layer3.3.bn3.running_var || shape: (1024,) || dtype: float32\nname: layer3.3.conv1.weight || shape: (256, 1024, 1, 1) || dtype: float32\nname: layer3.3.conv2.weight || shape: (256, 256, 3, 3) || dtype: float32\nname: layer3.3.conv3.weight || shape: (1024, 256, 1, 1) || dtype: float32\nname: layer3.4.bn1.beta || shape: (256,) || dtype: float32\nname: layer3.4.bn1.gamma || shape: (256,) || dtype: float32\nname: layer3.4.bn1.running_mean || shape: (256,) || dtype: float32\nname: layer3.4.bn1.running_var || shape: (256,) || dtype: float32\nname: layer3.4.bn2.beta || shape: (256,) || dtype: float32\nname: layer3.4.bn2.gamma || shape: (256,) || dtype: float32\nname: layer3.4.bn2.running_mean || shape: (256,) || dtype: float32\nname: layer3.4.bn2.running_var || shape: (256,) || dtype: float32\nname: layer3.4.bn3.beta || shape: (1024,) || dtype: float32\nname: layer3.4.bn3.gamma || shape: (1024,) || dtype: float32\nname: layer3.4.bn3.running_mean || shape: (1024,) || dtype: float32\nname: layer3.4.bn3.running_var || shape: (1024,) || dtype: float32\nname: layer3.4.conv1.weight || shape: (256, 1024, 1, 1) || dtype: float32\nname: layer3.4.conv2.weight || shape: (256, 256, 3, 3) || dtype: float32\nname: layer3.4.conv3.weight || shape: (1024, 256, 1, 1) || dtype: float32\nname: layer3.5.bn1.beta || shape: (256,) || dtype: float32\nname: layer3.5.bn1.gamma || shape: (256,) || dtype: float32\nname: layer3.5.bn1.running_mean || shape: (256,) || dtype: float32\nname: layer3.5.bn1.running_var || shape: (256,) || dtype: float32\nname: layer3.5.bn2.beta || shape: (256,) || dtype: float32\nname: layer3.5.bn2.gamma || shape: (256,) || dtype: float32\nname: layer3.5.bn2.running_mean || shape: (256,) || dtype: float32\nname: layer3.5.bn2.running_var || shape: (256,) || dtype: float32\nname: layer3.5.bn3.beta || shape: (1024,) || dtype: float32\nname: layer3.5.bn3.gamma || shape: (1024,) || dtype: float32\nname: layer3.5.bn3.running_mean || shape: (1024,) || dtype: float32\nname: layer3.5.bn3.running_var || shape: (1024,) || dtype: float32\nname: layer3.5.conv1.weight || shape: (256, 1024, 1, 1) || dtype: float32\nname: layer3.5.conv2.weight || shape: (256, 256, 3, 3) || dtype: float32\nname: layer3.5.conv3.weight || shape: (1024, 256, 1, 1) || dtype: float32\nname: layer4.0.bn1.beta || shape: (512,) || dtype: float32\nname: layer4.0.bn1.gamma || shape: (512,) || dtype: float32\nname: layer4.0.bn1.running_mean || shape: (512,) || dtype: float32\nname: layer4.0.bn1.running_var || shape: (512,) || dtype: float32\nname: layer4.0.bn2.beta || shape: (512,) || dtype: float32\nname: layer4.0.bn2.gamma || shape: (512,) || dtype: float32\nname: layer4.0.bn2.running_mean || shape: (512,) || dtype: float32\nname: layer4.0.bn2.running_var || shape: (512,) || dtype: float32\nname: layer4.0.bn3.beta || shape: (2048,) || dtype: float32\nname: layer4.0.bn3.gamma || shape: (2048,) || dtype: float32\nname: layer4.0.bn3.running_mean || shape: (2048,) || dtype: float32\nname: layer4.0.bn3.running_var || shape: (2048,) || dtype: float32\nname: layer4.0.conv1.weight || shape: (512, 1024, 1, 1) || dtype: float32\nname: layer4.0.conv2.weight || shape: (512, 512, 3, 3) || dtype: float32\nname: layer4.0.conv3.weight || shape: (2048, 512, 1, 1) || dtype: float32\nname: layer4.0.downsample.1.weight || shape: (2048, 1024, 1, 1) || dtype: float32\nname: layer4.0.downsample.2.beta || shape: (2048,) || dtype: float32\nname: layer4.0.downsample.2.gamma || shape: (2048,) || dtype: float32\nname: layer4.0.downsample.2.running_mean || shape: (2048,) || dtype: float32\nname: layer4.0.downsample.2.running_var || shape: (2048,) || dtype: float32\nname: layer4.1.bn1.beta || shape: (512,) || dtype: float32\nname: layer4.1.bn1.gamma || shape: (512,) || dtype: float32\nname: layer4.1.bn1.running_mean || shape: (512,) || dtype: float32\nname: layer4.1.bn1.running_var || shape: (512,) || dtype: float32\nname: layer4.1.bn2.beta || shape: (512,) || dtype: float32\nname: layer4.1.bn2.gamma || shape: (512,) || dtype: float32\nname: layer4.1.bn2.running_mean || shape: (512,) || dtype: float32\nname: layer4.1.bn2.running_var || shape: (512,) || dtype: float32\nname: layer4.1.bn3.beta || shape: (2048,) || dtype: float32\nname: layer4.1.bn3.gamma || shape: (2048,) || dtype: float32\nname: layer4.1.bn3.running_mean || shape: (2048,) || dtype: float32\nname: layer4.1.bn3.running_var || shape: (2048,) || dtype: float32\nname: layer4.1.conv1.weight || shape: (512, 2048, 1, 1) || dtype: float32\nname: layer4.1.conv2.weight || shape: (512, 512, 3, 3) || dtype: float32\nname: layer4.1.conv3.weight || shape: (2048, 512, 1, 1) || dtype: float32\nname: layer4.2.bn1.beta || shape: (512,) || dtype: float32\nname: layer4.2.bn1.gamma || shape: (512,) || dtype: float32\nname: layer4.2.bn1.running_mean || shape: (512,) || dtype: float32\nname: layer4.2.bn1.running_var || shape: (512,) || dtype: float32\nname: layer4.2.bn2.beta || shape: (512,) || dtype: float32\nname: layer4.2.bn2.gamma || shape: (512,) || dtype: float32\nname: layer4.2.bn2.running_mean || shape: (512,) || dtype: float32\nname: layer4.2.bn2.running_var || shape: (512,) || dtype: float32\nname: layer4.2.bn3.beta || shape: (2048,) || dtype: float32\nname: layer4.2.bn3.gamma || shape: (2048,) || dtype: float32\nname: layer4.2.bn3.running_mean || shape: (2048,) || dtype: float32\nname: layer4.2.bn3.running_var || shape: (2048,) || dtype: float32\nname: layer4.2.conv1.weight || shape: (512, 2048, 1, 1) || dtype: float32\nname: layer4.2.conv2.weight || shape: (512, 512, 3, 3) || dtype: float32\nname: layer4.2.conv3.weight || shape: (2048, 512, 1, 1) || dtype: float32\n'''\n\n'''\nC1/conv0/BatchNorm/beta :: conv1.1.beta\nC1/conv0/BatchNorm/gamma :: conv1.1.gamma\nC1/conv0/BatchNorm/moving_mean :: conv1.1.running_mean\nC1/conv0/BatchNorm/moving_variance :: conv1.1.running_var\nC1/conv0/weights :: conv1.0.weight\nC1/conv1/beta :: conv1.4.beta\nC1/conv1/gamma :: conv1.4.gamma\nC1/conv1/moving_mean :: conv1.4.running_mean\nC1/conv1/moving_variance :: conv1.4.running_var\nC1/conv1/weights :: conv1.3.weight\nC1/conv2/beta :: bn1.beta\nC1/conv2/gamma :: bn1.gamma\nC1/conv2/moving_mean :: bn1.running_mean\nC1/conv2/moving_variance :: bn1.running_var\nC1/conv2/weights :: conv1.6.weight\nC2/bottleneck_0/conv0/beta :: layer1.0.bn1.beta\nC2/bottleneck_0/conv0/gamma :: layer1.0.bn1.gamma\nC2/bottleneck_0/conv0/moving_mean :: layer1.0.bn1.running_mean\nC2/bottleneck_0/conv0/moving_variance :: layer1.0.bn1.running_var\nC2/bottleneck_0/conv0/weights :: layer1.0.conv1.weight\nC2/bottleneck_0/conv1/beta :: layer1.0.bn2.beta\nC2/bottleneck_0/conv1/gamma :: layer1.0.bn2.gamma\nC2/bottleneck_0/conv1/moving_mean :: layer1.0.bn2.running_mean\nC2/bottleneck_0/conv1/moving_variance :: layer1.0.bn2.running_var\nC2/bottleneck_0/conv1/weights :: layer1.0.conv2.weight\nC2/bottleneck_0/conv2/beta :: layer1.0.bn3.beta\nC2/bottleneck_0/conv2/gamma :: layer1.0.bn3.gamma\nC2/bottleneck_0/conv2/moving_mean :: layer1.0.bn3.running_mean\nC2/bottleneck_0/conv2/moving_variance :: layer1.0.bn3.running_var\nC2/bottleneck_0/conv2/weights :: layer1.0.conv3.weight\nC2/bottleneck_0/shortcut/beta :: layer1.0.downsample.2.beta\nC2/bottleneck_0/shortcut/gamma :: layer1.0.downsample.2.gamma\nC2/bottleneck_0/shortcut/moving_mean :: layer1.0.downsample.2.running_mean\nC2/bottleneck_0/shortcut/moving_variance :: layer1.0.downsample.2.running_var\nC2/bottleneck_0/shortcut/weights :: layer1.0.downsample.1.weight\nC2/bottleneck_1/conv0/beta :: layer1.1.bn1.beta\nC2/bottleneck_1/conv0/gamma :: layer1.1.bn1.gamma\nC2/bottleneck_1/conv0/moving_mean :: layer1.1.bn1.running_mean\nC2/bottleneck_1/conv0/moving_variance :: layer1.1.bn1.running_var\nC2/bottleneck_1/conv0/weights :: layer1.1.conv1.weight\nC2/bottleneck_1/conv1/beta :: layer1.1.bn2.beta\nC2/bottleneck_1/conv1/gamma :: layer1.1.bn2.gamma\nC2/bottleneck_1/conv1/moving_mean :: layer1.1.bn2.running_mean\nC2/bottleneck_1/conv1/moving_variance :: layer1.1.bn2.running_var\nC2/bottleneck_1/conv1/weights :: layer1.1.conv2.weight\nC2/bottleneck_1/conv2/beta :: layer1.1.bn3.beta\nC2/bottleneck_1/conv2/gamma :: layer1.1.bn3.gamma\nC2/bottleneck_1/conv2/moving_mean :: layer1.1.bn3.running_mean\nC2/bottleneck_1/conv2/moving_variance :: layer1.1.bn3.running_var\nC2/bottleneck_1/conv2/weights :: layer1.1.conv3.weight\nC2/bottleneck_2/conv0/beta :: layer1.2.bn1.beta\nC2/bottleneck_2/conv0/gamma :: layer1.2.bn1.gamma\nC2/bottleneck_2/conv0/moving_mean :: layer1.2.bn1.running_mean\nC2/bottleneck_2/conv0/moving_variance :: layer1.2.bn1.running_var\nC2/bottleneck_2/conv0/weights :: layer1.2.conv1.weight\nC2/bottleneck_2/conv1/beta :: layer1.2.bn2.beta\nC2/bottleneck_2/conv1/gamma :: layer1.2.bn2.gamma\nC2/bottleneck_2/conv1/moving_mean :: layer1.2.bn2.running_mean\nC2/bottleneck_2/conv1/moving_variance :: layer1.2.bn2.running_var\nC2/bottleneck_2/conv1/weights :: layer1.2.conv2.weight\nC2/bottleneck_2/conv2/beta :: layer1.2.bn3.beta\nC2/bottleneck_2/conv2/gamma :: layer1.2.bn3.gamma\nC2/bottleneck_2/conv2/moving_mean :: layer1.2.bn3.running_mean\nC2/bottleneck_2/conv2/moving_variance :: layer1.2.bn3.running_var\nC2/bottleneck_2/conv2/weights :: layer1.2.conv3.weight\nC3/bottleneck_0/conv0/beta :: layer2.0.bn1.beta\nC3/bottleneck_0/conv0/gamma :: layer2.0.bn1.gamma\nC3/bottleneck_0/conv0/moving_mean :: layer2.0.bn1.running_mean\nC3/bottleneck_0/conv0/moving_variance :: layer2.0.bn1.running_var\nC3/bottleneck_0/conv0/weights :: layer2.0.conv1.weight\nC3/bottleneck_0/conv1/beta :: layer2.0.bn2.beta\nC3/bottleneck_0/conv1/gamma :: layer2.0.bn2.gamma\nC3/bottleneck_0/conv1/moving_mean :: layer2.0.bn2.running_mean\nC3/bottleneck_0/conv1/moving_variance :: layer2.0.bn2.running_var\nC3/bottleneck_0/conv1/weights :: layer2.0.conv2.weight\nC3/bottleneck_0/conv2/beta :: layer2.0.bn3.beta\nC3/bottleneck_0/conv2/gamma :: layer2.0.bn3.gamma\nC3/bottleneck_0/conv2/moving_mean :: layer2.0.bn3.running_mean\nC3/bottleneck_0/conv2/moving_variance :: layer2.0.bn3.running_var\nC3/bottleneck_0/conv2/weights :: layer2.0.conv3.weight\nC3/bottleneck_0/shortcut/beta :: layer2.0.downsample.2.beta\nC3/bottleneck_0/shortcut/gamma :: layer2.0.downsample.2.gamma\nC3/bottleneck_0/shortcut/moving_mean :: layer2.0.downsample.2.running_mean\nC3/bottleneck_0/shortcut/moving_variance :: layer2.0.downsample.2.running_var\nC3/bottleneck_0/shortcut/weights :: layer2.0.downsample.1.weight\nC3/bottleneck_1/conv0/beta :: layer2.1.bn1.beta\nC3/bottleneck_1/conv0/gamma :: layer2.1.bn1.gamma\nC3/bottleneck_1/conv0/moving_mean :: layer2.1.bn1.running_mean\nC3/bottleneck_1/conv0/moving_variance :: layer2.1.bn1.running_var\nC3/bottleneck_1/conv0/weights :: layer2.1.conv1.weight\nC3/bottleneck_1/conv1/beta :: layer2.1.bn2.beta\nC3/bottleneck_1/conv1/gamma :: layer2.1.bn2.gamma\nC3/bottleneck_1/conv1/moving_mean :: layer2.1.bn2.running_mean\nC3/bottleneck_1/conv1/moving_variance :: layer2.1.bn2.running_var\nC3/bottleneck_1/conv1/weights :: layer2.1.conv2.weight\nC3/bottleneck_1/conv2/beta :: layer2.1.bn3.beta\nC3/bottleneck_1/conv2/gamma :: layer2.1.bn3.gamma\nC3/bottleneck_1/conv2/moving_mean :: layer2.1.bn3.running_mean\nC3/bottleneck_1/conv2/moving_variance :: layer2.1.bn3.running_var\nC3/bottleneck_1/conv2/weights :: layer2.1.conv3.weight\nC3/bottleneck_2/conv0/beta :: layer2.2.bn1.beta\nC3/bottleneck_2/conv0/gamma :: layer2.2.bn1.gamma\nC3/bottleneck_2/conv0/moving_mean :: layer2.2.bn1.running_mean\nC3/bottleneck_2/conv0/moving_variance :: layer2.2.bn1.running_var\nC3/bottleneck_2/conv0/weights :: layer2.2.conv1.weight\nC3/bottleneck_2/conv1/beta :: layer2.2.bn2.beta\nC3/bottleneck_2/conv1/gamma :: layer2.2.bn2.gamma\nC3/bottleneck_2/conv1/moving_mean :: layer2.2.bn2.running_mean\nC3/bottleneck_2/conv1/moving_variance :: layer2.2.bn2.running_var\nC3/bottleneck_2/conv1/weights :: layer2.2.conv2.weight\nC3/bottleneck_2/conv2/beta :: layer2.2.bn3.beta\nC3/bottleneck_2/conv2/gamma :: layer2.2.bn3.gamma\nC3/bottleneck_2/conv2/moving_mean :: layer2.2.bn3.running_mean\nC3/bottleneck_2/conv2/moving_variance :: layer2.2.bn3.running_var\nC3/bottleneck_2/conv2/weights :: layer2.2.conv3.weight\nC3/bottleneck_3/conv0/beta :: layer2.3.bn1.beta\nC3/bottleneck_3/conv0/gamma :: layer2.3.bn1.gamma\nC3/bottleneck_3/conv0/moving_mean :: layer2.3.bn1.running_mean\nC3/bottleneck_3/conv0/moving_variance :: layer2.3.bn1.running_var\nC3/bottleneck_3/conv0/weights :: layer2.3.conv1.weight\nC3/bottleneck_3/conv1/beta :: layer2.3.bn2.beta\nC3/bottleneck_3/conv1/gamma :: layer2.3.bn2.gamma\nC3/bottleneck_3/conv1/moving_mean :: layer2.3.bn2.running_mean\nC3/bottleneck_3/conv1/moving_variance :: layer2.3.bn2.running_var\nC3/bottleneck_3/conv1/weights :: layer2.3.conv2.weight\nC3/bottleneck_3/conv2/beta :: layer2.3.bn3.beta\nC3/bottleneck_3/conv2/gamma :: layer2.3.bn3.gamma\nC3/bottleneck_3/conv2/moving_mean :: layer2.3.bn3.running_mean\nC3/bottleneck_3/conv2/moving_variance :: layer2.3.bn3.running_var\nC3/bottleneck_3/conv2/weights :: layer2.3.conv3.weight\nC4/bottleneck_0/conv0/beta :: layer3.0.bn1.beta\nC4/bottleneck_0/conv0/gamma :: layer3.0.bn1.gamma\nC4/bottleneck_0/conv0/moving_mean :: layer3.0.bn1.running_mean\nC4/bottleneck_0/conv0/moving_variance :: layer3.0.bn1.running_var\nC4/bottleneck_0/conv0/weights :: layer3.0.conv1.weight\nC4/bottleneck_0/conv1/beta :: layer3.0.bn2.beta\nC4/bottleneck_0/conv1/gamma :: layer3.0.bn2.gamma\nC4/bottleneck_0/conv1/moving_mean :: layer3.0.bn2.running_mean\nC4/bottleneck_0/conv1/moving_variance :: layer3.0.bn2.running_var\nC4/bottleneck_0/conv1/weights :: layer3.0.conv2.weight\nC4/bottleneck_0/conv2/beta :: layer3.0.bn3.beta\nC4/bottleneck_0/conv2/gamma :: layer3.0.bn3.gamma\nC4/bottleneck_0/conv2/moving_mean :: layer3.0.bn3.running_mean\nC4/bottleneck_0/conv2/moving_variance :: layer3.0.bn3.running_var\nC4/bottleneck_0/conv2/weights :: layer3.0.conv3.weight\nC4/bottleneck_0/shortcut/beta :: layer3.0.downsample.2.beta\nC4/bottleneck_0/shortcut/gamma :: layer3.0.downsample.2.gamma\nC4/bottleneck_0/shortcut/moving_mean :: layer3.0.downsample.2.running_mean\nC4/bottleneck_0/shortcut/moving_variance :: layer3.0.downsample.2.running_var\nC4/bottleneck_0/shortcut/weights :: layer3.0.downsample.1.weight\nC4/bottleneck_1/conv0/beta :: layer3.1.bn1.beta\nC4/bottleneck_1/conv0/gamma :: layer3.1.bn1.gamma\nC4/bottleneck_1/conv0/moving_mean :: layer3.1.bn1.running_mean\nC4/bottleneck_1/conv0/moving_variance :: layer3.1.bn1.running_var\nC4/bottleneck_1/conv0/weights :: layer3.1.conv1.weight\nC4/bottleneck_1/conv1/beta :: layer3.1.bn2.beta\nC4/bottleneck_1/conv1/gamma :: layer3.1.bn2.gamma\nC4/bottleneck_1/conv1/moving_mean :: layer3.1.bn2.running_mean\nC4/bottleneck_1/conv1/moving_variance :: layer3.1.bn2.running_var\nC4/bottleneck_1/conv1/weights :: layer3.1.conv2.weight\nC4/bottleneck_1/conv2/beta :: layer3.1.bn3.beta\nC4/bottleneck_1/conv2/gamma :: layer3.1.bn3.gamma\nC4/bottleneck_1/conv2/moving_mean :: layer3.1.bn3.running_mean\nC4/bottleneck_1/conv2/moving_variance :: layer3.1.bn3.running_var\nC4/bottleneck_1/conv2/weights :: layer3.1.conv3.weight\nC4/bottleneck_2/conv0/beta :: layer3.2.bn1.beta\nC4/bottleneck_2/conv0/gamma :: layer3.2.bn1.gamma\nC4/bottleneck_2/conv0/moving_mean :: layer3.2.bn1.running_mean\nC4/bottleneck_2/conv0/moving_variance :: layer3.2.bn1.running_var\nC4/bottleneck_2/conv0/weights :: layer3.2.conv1.weight\nC4/bottleneck_2/conv1/beta :: layer3.2.bn2.beta\nC4/bottleneck_2/conv1/gamma :: layer3.2.bn2.gamma\nC4/bottleneck_2/conv1/moving_mean :: layer3.2.bn2.running_mean\nC4/bottleneck_2/conv1/moving_variance :: layer3.2.bn2.running_var\nC4/bottleneck_2/conv1/weights :: layer3.2.conv2.weight\nC4/bottleneck_2/conv2/beta :: layer3.2.bn3.beta\nC4/bottleneck_2/conv2/gamma :: layer3.2.bn3.gamma\nC4/bottleneck_2/conv2/moving_mean :: layer3.2.bn3.running_mean\nC4/bottleneck_2/conv2/moving_variance :: layer3.2.bn3.running_var\nC4/bottleneck_2/conv2/weights :: layer3.2.conv3.weight\nC4/bottleneck_3/conv0/beta :: layer3.3.bn1.beta\nC4/bottleneck_3/conv0/gamma :: layer3.3.bn1.gamma\nC4/bottleneck_3/conv0/moving_mean :: layer3.3.bn1.running_mean\nC4/bottleneck_3/conv0/moving_variance :: layer3.3.bn1.running_var\nC4/bottleneck_3/conv0/weights :: layer3.3.conv1.weight\nC4/bottleneck_3/conv1/beta :: layer3.3.bn2.beta\nC4/bottleneck_3/conv1/gamma :: layer3.3.bn2.gamma\nC4/bottleneck_3/conv1/moving_mean :: layer3.3.bn2.running_mean\nC4/bottleneck_3/conv1/moving_variance :: layer3.3.bn2.running_var\nC4/bottleneck_3/conv1/weights :: layer3.3.conv2.weight\nC4/bottleneck_3/conv2/beta :: layer3.3.bn3.beta\nC4/bottleneck_3/conv2/gamma :: layer3.3.bn3.gamma\nC4/bottleneck_3/conv2/moving_mean :: layer3.3.bn3.running_mean\nC4/bottleneck_3/conv2/moving_variance :: layer3.3.bn3.running_var\nC4/bottleneck_3/conv2/weights :: layer3.3.conv3.weight\nC4/bottleneck_4/conv0/beta :: layer3.4.bn1.beta\nC4/bottleneck_4/conv0/gamma :: layer3.4.bn1.gamma\nC4/bottleneck_4/conv0/moving_mean :: layer3.4.bn1.running_mean\nC4/bottleneck_4/conv0/moving_variance :: layer3.4.bn1.running_var\nC4/bottleneck_4/conv0/weights :: layer3.4.conv1.weight\nC4/bottleneck_4/conv1/beta :: layer3.4.bn2.beta\nC4/bottleneck_4/conv1/gamma :: layer3.4.bn2.gamma\nC4/bottleneck_4/conv1/moving_mean :: layer3.4.bn2.running_mean\nC4/bottleneck_4/conv1/moving_variance :: layer3.4.bn2.running_var\nC4/bottleneck_4/conv1/weights :: layer3.4.conv2.weight\nC4/bottleneck_4/conv2/beta :: layer3.4.bn3.beta\nC4/bottleneck_4/conv2/gamma :: layer3.4.bn3.gamma\nC4/bottleneck_4/conv2/moving_mean :: layer3.4.bn3.running_mean\nC4/bottleneck_4/conv2/moving_variance :: layer3.4.bn3.running_var\nC4/bottleneck_4/conv2/weights :: layer3.4.conv3.weight\nC4/bottleneck_5/conv0/beta :: layer3.5.bn1.beta\nC4/bottleneck_5/conv0/gamma :: layer3.5.bn1.gamma\nC4/bottleneck_5/conv0/moving_mean :: layer3.5.bn1.running_mean\nC4/bottleneck_5/conv0/moving_variance :: layer3.5.bn1.running_var\nC4/bottleneck_5/conv0/weights :: layer3.5.conv1.weight\nC4/bottleneck_5/conv1/beta :: layer3.5.bn2.beta\nC4/bottleneck_5/conv1/gamma :: layer3.5.bn2.gamma\nC4/bottleneck_5/conv1/moving_mean :: layer3.5.bn2.running_mean\nC4/bottleneck_5/conv1/moving_variance :: layer3.5.bn2.running_var\nC4/bottleneck_5/conv1/weights :: layer3.5.conv2.weight\nC4/bottleneck_5/conv2/beta :: layer3.5.bn3.beta\nC4/bottleneck_5/conv2/gamma :: layer3.5.bn3.gamma\nC4/bottleneck_5/conv2/moving_mean :: layer3.5.bn3.running_mean\nC4/bottleneck_5/conv2/moving_variance :: layer3.5.bn3.running_var\nC4/bottleneck_5/conv2/weights :: layer3.5.conv3.weight\nC5/bottleneck_0/conv0/beta :: layer4.0.bn1.beta\nC5/bottleneck_0/conv0/gamma :: layer4.0.bn1.gamma\nC5/bottleneck_0/conv0/moving_mean :: layer4.0.bn1.running_mean\nC5/bottleneck_0/conv0/moving_variance :: layer4.0.bn1.running_var\nC5/bottleneck_0/conv0/weights :: layer4.0.conv1.weight\nC5/bottleneck_0/conv1/beta :: layer4.0.bn2.beta\nC5/bottleneck_0/conv1/gamma :: layer4.0.bn2.gamma\nC5/bottleneck_0/conv1/moving_mean :: layer4.0.bn2.running_mean\nC5/bottleneck_0/conv1/moving_variance :: layer4.0.bn2.running_var\nC5/bottleneck_0/conv1/weights :: layer4.0.conv2.weight\nC5/bottleneck_0/conv2/beta :: layer4.0.bn3.beta\nC5/bottleneck_0/conv2/gamma :: layer4.0.bn3.gamma\nC5/bottleneck_0/conv2/moving_mean :: layer4.0.bn3.running_mean\nC5/bottleneck_0/conv2/moving_variance :: layer4.0.bn3.running_var\nC5/bottleneck_0/conv2/weights :: layer4.0.conv3.weight\nC5/bottleneck_0/shortcut/beta :: layer4.0.downsample.2.beta\nC5/bottleneck_0/shortcut/gamma :: layer4.0.downsample.2.gamma\nC5/bottleneck_0/shortcut/moving_mean :: layer4.0.downsample.2.running_mean\nC5/bottleneck_0/shortcut/moving_variance :: layer4.0.downsample.2.running_var\nC5/bottleneck_0/shortcut/weights :: layer4.0.downsample.1.weight\nC5/bottleneck_1/conv0/beta :: layer4.1.bn1.beta\nC5/bottleneck_1/conv0/gamma :: layer4.1.bn1.gamma\nC5/bottleneck_1/conv0/moving_mean :: layer4.1.bn1.running_mean\nC5/bottleneck_1/conv0/moving_variance :: layer4.1.bn1.running_var\nC5/bottleneck_1/conv0/weights :: layer4.1.conv1.weight\nC5/bottleneck_1/conv1/beta :: layer4.1.bn2.beta\nC5/bottleneck_1/conv1/gamma :: layer4.1.bn2.gamma\nC5/bottleneck_1/conv1/moving_mean :: layer4.1.bn2.running_mean\nC5/bottleneck_1/conv1/moving_variance :: layer4.1.bn2.running_var\nC5/bottleneck_1/conv1/weights :: layer4.1.conv2.weight\nC5/bottleneck_1/conv2/beta :: layer4.1.bn3.beta\nC5/bottleneck_1/conv2/gamma :: layer4.1.bn3.gamma\nC5/bottleneck_1/conv2/moving_mean :: layer4.1.bn3.running_mean\nC5/bottleneck_1/conv2/moving_variance :: layer4.1.bn3.running_var\nC5/bottleneck_1/conv2/weights :: layer4.1.conv3.weight\nC5/bottleneck_2/conv0/beta :: layer4.2.bn1.beta\nC5/bottleneck_2/conv0/gamma :: layer4.2.bn1.gamma\nC5/bottleneck_2/conv0/moving_mean :: layer4.2.bn1.running_mean\nC5/bottleneck_2/conv0/moving_variance :: layer4.2.bn1.running_var\nC5/bottleneck_2/conv0/weights :: layer4.2.conv1.weight\nC5/bottleneck_2/conv1/beta :: layer4.2.bn2.beta\nC5/bottleneck_2/conv1/gamma :: layer4.2.bn2.gamma\nC5/bottleneck_2/conv1/moving_mean :: layer4.2.bn2.running_mean\nC5/bottleneck_2/conv1/moving_variance :: layer4.2.bn2.running_var\nC5/bottleneck_2/conv1/weights :: layer4.2.conv2.weight\nC5/bottleneck_2/conv2/beta :: layer4.2.bn3.beta\nC5/bottleneck_2/conv2/gamma :: layer4.2.bn3.gamma\nC5/bottleneck_2/conv2/moving_mean :: layer4.2.bn3.running_mean\nC5/bottleneck_2/conv2/moving_variance :: layer4.2.bn3.running_var\nC5/bottleneck_2/conv2/weights :: layer4.2.conv3.weight\nlogits/biases :: fc.bias\nlogits/weights :: fc.weight\n'''"""
libs/gluon2TF/resnet/__init__.py,0,b' \n'
libs/gluon2TF/resnet/download_mxnet_resnet_weights.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport requests\nimport hashlib\nimport zipfile\nfrom tqdm import tqdm\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    sha1_file = sha1.hexdigest()\n    l = min(len(sha1_file), len(sha1_hash))\n    return sha1.hexdigest()[0:l] == sha1_hash[0:l]\n\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download an given URL\n    Parameters\n    ----------\n    url : str\n        URL to download\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if already exists.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n        but doesn\'t match.\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        print(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname\n\n\ndef download_mxnet_weights(name, tag=None, root=\'../mxnet_weights\'):\n    _model_sha1 = {name: checksum for checksum, name in [\n        (\'2d9d980c990442f826f20781ed039851e78dabe3\', \'resnet18_v1b\'),\n        (\'8e16b84814e84f64d897854003f049872991eaa6\', \'resnet34_v1b\'),\n        (\'0ecdba34691be172036ddf244ff1b2eade75ffde\', \'resnet50_v1b\'),\n        (\'a455932aa95cb7dcfa05fd040b9b5a5660733c39\', \'resnet101_v1b\'),\n        (\'a5a61ee1ce5ab7c09720775b223360f3c60e211d\', \'resnet152_v1b\'),\n        (\'2a4e070854db538595cc7ee02e1a914bdd49ca02\', \'resnet50_v1c\'),\n        (\'064858f23f9878bfbbe378a88ccb25d612b149a1\', \'resnet101_v1c\'),\n        (\'75babab699e1c93f5da3c1ce4fd0092d1075f9a0\', \'resnet152_v1c\'),\n        (\'117a384ecf61490eb31ea147eb0e61e6d2b8a449\', \'resnet50_v1d\'),\n        (\'1b2b825feff86b0354642a4ab59f9b6e35e47338\', \'resnet101_v1d\'),\n        (\'cddbc86ff24a5544f57242ded0acb14ef1fbd437\', \'resnet152_v1d\')\n    ]}\n\n    apache_repo_url = \'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/\'\n    _url_format = \'{repo_url}gluon/models/{file_name}.zip\'\n\n    def short_hash(name):\n        if name not in _model_sha1:\n            raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n        return _model_sha1[name][:8]\n\n    use_tag = isinstance(tag, str)\n    if use_tag:\n        file_name = \'{name}-{short_hash}\'.format(name=name,\n                                                 short_hash=tag)\n    else:\n        file_name = \'{name}-{short_hash}\'.format(name=name,\n                                                 short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, file_name + \'.params\')\n    if use_tag:\n        sha1_hash = tag\n    else:\n        sha1_hash = _model_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            return file_path\n        else:\n            print(\'Mismatch in the content of model file detected. Downloading again.\')\n    else:\n        print(\'Model file is not found. Downloading.\')\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name + \'.zip\')\n    repo_url = os.environ.get(\'MXNET_GLUON_REPO\', apache_repo_url)\n    if repo_url[-1] != \'/\':\n        repo_url = repo_url + \'/\'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return file_path\n    else:\n        raise ValueError(\'Downloaded file has different hash. Please try again.\')\n\n\nif __name__ == \'__main__\':\n    download_mxnet_weights(\'resnet%d_v%db\' % (101, 1),\n                           tag=True)'"
libs/gluon2TF/resnet/parse_mxnet_weights.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport mxnet.ndarray as nd\nimport numpy as np\n\n\ndef read_mxnet_weights(path, show=False):\n\n    # assert os.path.exists(path), ""path erro: {}"".format(path)\n\n    name_MxnetArray_dict = nd.load(path)\n\n    name_array_dict = {}\n    for name in sorted(name_MxnetArray_dict.keys()):\n        mxnet_array = name_MxnetArray_dict[name]\n        array = mxnet_array.asnumpy()\n\n        if show:\n            print (""name: {} || shape: {} || dtype: {}"".format(name, array.shape, array.dtype))\n\n        if name.endswith(""weight""):\n            if name.endswith(""fc.weight""):\n                array = np.transpose(array, [1, 0])\n            else:\n                array = np.transpose(array, [2, 3, 1, 0])\n            # (out_channel, in_channel, k, k)(mxnet) --> (k, k, in_channel, out_channel)(tf)\n            # (32, 3, 3, 3)-->(3, 3, 3, 32)\n        name_array_dict[name] = array\n\n    return name_array_dict\n\n\ndef check_mxnet_names(mxnet_tf_map, mxnetName_array_dict):\n\n    for key1, key2 in zip(sorted(mxnet_tf_map.keys()), sorted(mxnetName_array_dict.keys())):\n        assert key1 == key2, ""key in mxnet_array_dict and mxnet_tf_map do not equal, details are :\\n"" \\\n                             ""key1 in mxnet_tf_map: {}\\n""\\\n                             ""key2 in mxnet_array dict: {}"".format(key1, key2)\n    if len(mxnetName_array_dict) == len(mxnet_tf_map):\n        print(""all mxnet names are mapped"")\n\n\ndef check_tf_vars(tf_mxnet_map, mxnetName_array_dict, tf_model_vars, scope=\'resnet50_v1_d\'):\n\n    tf_nake_names = sorted([var.op.name.split(""%s/"" % scope)[1] for var in tf_model_vars])\n    # check_name\n    for tf_name, name2 in zip(tf_nake_names, sorted(tf_mxnet_map.keys())):\n        assert tf_name == name2, ""key in tf_model_vars and tf_mxnet_map do not equal, details are :\\n"" \\\n                                 ""tf_name in tf_model_vars: {}\\n"" \\\n                                 ""name2 in tf_mxnet_maps: {}"".format(tf_name, name2)\n    print(""all tf_model_var can find matched name in tf_mxnet_map"")\n\n    # check shape\n    for var in tf_model_vars:\n        name = var.op.name.split(""%s/""%scope)[1]\n        array = mxnetName_array_dict[tf_mxnet_map[name]]\n\n        assert var.shape == array.shape,  ""var in tf_model_vars and mxnet_arrays shape do not equal, details are :\\n"" \\\n                                          ""tf_var in tf_model_vars: {}\\n"" \\\n                                          ""name in tf_mxnet_maps: {}, shape is : {}"".format(var, tf_mxnet_map[name],\n                                                                                            array.shape)\n    print(""All tf_model_var shapes match the shape of arrays in mxnet_array_dict..."")\n\n\n'"
libs/gluon2TF/resnet/resnet.py,4,"b'# -*- coding: utf-8 -*-\n\n\nfrom __future__ import absolute_import, print_function, division\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom resnet_utils import get_resnet_v1_d, get_resnet_v1_b\nfrom parse_mxnet_weights import read_mxnet_weights, check_mxnet_names, check_tf_vars\nimport weights_map\nimport os\n\nBottleNeck_NUM_DICT = {\n    \'resnet50_v1b\': [3, 4, 6, 3],\n    \'resnet101_v1b\': [3, 4, 23, 3],\n    \'resnet50_v1d\': [3, 4, 6, 3],\n    \'resnet101_v1d\': [3, 4, 23, 3]\n}\n\nBASE_CHANNELS_DICT = {\n    \'resnet50_v1b\': [64, 128, 256, 512],\n    \'resnet101_v1b\': [64, 128, 256, 512],\n    \'resnet50_v1d\': [64, 128, 256, 512],\n    \'resnet101_v1d\': [64, 128, 256, 512]\n}\n\n\ndef create_resotre_op(scope, mxnet_weights_path):\n\n    mxnetName_array_dict = read_mxnet_weights(mxnet_weights_path, show=False)\n\n    tf_mxnet_map, mxnet_tf_map = \\\n        weights_map.get_map(scope=scope,\n                            bottleneck_nums=BottleNeck_NUM_DICT[scope], show_mxnettf=False, show_tfmxnet=False)\n\n    tf_model_vars = slim.get_model_variables(scope)\n\n    # # check name and var\n    check_mxnet_names(mxnet_tf_map, mxnetName_array_dict=mxnetName_array_dict)\n    check_tf_vars(tf_mxnet_map, mxnetName_array_dict, tf_model_vars, scope=scope)\n    # #\n\n    assign_ops = []\n\n    for var in tf_model_vars:\n        name = var.op.name.split(\'%s/\' % scope)[1]\n        new_val = tf.constant(mxnetName_array_dict[tf_mxnet_map[name]])\n        sub_assign_op = tf.assign(var, value=new_val)\n\n        assign_ops.append(sub_assign_op)\n\n    assign_op = tf.group(*assign_ops)\n\n    return assign_op\n\n\ndef build_resnet(img_batch=None, scope=\'resnet50_v1d\', is_training=True, freeze_norm=False, num_cls=1000):\n    if img_batch is None:\n        np.random.seed(30)\n        img_batch = np.random.rand(1, 224, 224, 3)  # H, W, C\n        img_batch = tf.constant(img_batch, dtype=tf.float32)\n\n    print(""Please Ensure the img is in NHWC"")\n\n    if scope.endswith(\'b\'):\n        get_resnet_fn = get_resnet_v1_b\n    elif scope.endswith(\'d\'):\n        get_resnet_fn = get_resnet_v1_d\n\n    logits = get_resnet_fn(input_x=img_batch, scope=scope,\n                           bottleneck_nums=BottleNeck_NUM_DICT[scope],\n                           base_channels=BASE_CHANNELS_DICT[scope],\n                           is_training=is_training, freeze_norm=freeze_norm, num_cls=num_cls)\n\n    return logits\n\n\nif __name__ == ""__main__"":\n    build_resnet()\n    create_resotre_op()\n'"
libs/gluon2TF/resnet/resnet_utils.py,35,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport math\n\nDATA_FORMAT = ""NHWC""\nDEBUG = False\ndebug_dict = {}\n\n\ndef resnet_arg_scope(freeze_norm, is_training=True, weight_decay=0.0001,\n                     batch_norm_decay=0.9, batch_norm_epsilon=1e-5, batch_norm_scale=True):\n\n    batch_norm_params = {\n        \'is_training\': (not freeze_norm) and is_training, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': (not freeze_norm) and is_training,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n        \'data_format\': DATA_FORMAT\n    }\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef stem_7x7(net, scope=""C1""):\n\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [3, 3], [3, 3], [0, 0]])  # pad for data\n        net = slim.conv2d(net, num_outputs=64, kernel_size=[7, 7], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=""conv0"")\n        if DEBUG:\n            debug_dict[\'conv_7x7_bn_relu\'] = tf.transpose(net, [0, 3, 1, 2])  # NHWC --> NCHW\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef stem_stack_3x3(net, input_channel=32, scope=""C1""):\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel*2, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv2\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef bottleneck_v1b(input_x, base_channel, scope, stride=1, projection=False, avg_down=True):\n    \'\'\'\n    for bottleneck_v1b: reduce spatial dim in conv_3x3 with stride 2.\n    \'\'\'\n    with tf.variable_scope(scope):\n        if DEBUG:\n            debug_dict[input_x.op.name] = tf.transpose(input_x, [0, 3, 1, 2])\n        net = slim.conv2d(input_x, num_outputs=base_channel, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = slim.conv2d(net, num_outputs=base_channel, kernel_size=[3, 3], stride=stride,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = slim.conv2d(net, num_outputs=base_channel * 4, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          activation_fn=None, scope=\'conv2\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        # Note that : gamma in the last conv should be init with 0.\n        # But we just reload params from mxnet, so don\'t specific batch norm initializer\n        if projection:\n\n            if avg_down:  # design for resnet_v1d\n                \'\'\'\n                In GluonCV, padding is ""ceil mode"". Here we use ""SAME"" to replace it, which may cause Erros.\n                And the erro will grow with depth of resnet. e.g. res101 erro > res50 erro\n                \'\'\'\n                shortcut = slim.avg_pool2d(input_x, kernel_size=[stride, stride], stride=stride, padding=""SAME"",\n                                           data_format=DATA_FORMAT)\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n                shortcut = slim.conv2d(shortcut, num_outputs=base_channel*4, kernel_size=[1, 1],\n                                       stride=1, padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                                       activation_fn=None,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n                # shortcut should have batch norm.\n            else:\n                shortcut = slim.conv2d(input_x, num_outputs=base_channel * 4, kernel_size=[1, 1],\n                                       stride=stride, padding=""VALID"", biases_initializer=None, activation_fn=None,\n                                       data_format=DATA_FORMAT,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n        else:\n            shortcut = tf.identity(input_x, name=\'shortcut/Identity\')\n            if DEBUG:\n                debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n        net = net + shortcut\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = tf.nn.relu(net)\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        return net\n\n\ndef make_block(net, base_channel, bottleneck_nums, scope, avg_down=True, spatial_downsample=False):\n    with tf.variable_scope(scope):\n        first_stride = 2 if spatial_downsample else 1\n\n        net = bottleneck_v1b(input_x=net, base_channel=base_channel,scope=\'bottleneck_0\',\n                             stride=first_stride, avg_down=avg_down, projection=True)\n        for i in range(1, bottleneck_nums):\n            net = bottleneck_v1b(input_x=net, base_channel=base_channel, scope=""bottleneck_%d"" % i,\n                                 stride=1, avg_down=avg_down, projection=False)\n        return net\n\n\ndef get_resnet_v1_b_base(input_x, freeze_norm, scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_7x7(net=input_x, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-1]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=False, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_b(input_x,\n                    scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3],\n                    base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False],\n                    is_training=True, freeze_norm=False,\n                    num_cls=1000, dropout=False):\n\n    net, fet_dict = get_resnet_v1_b_base(input_x=input_x, scope=scope, bottleneck_nums=bottleneck_nums, base_channels=base_channels,\n                                         freeze=freeze, is_training=is_training, freeze_norm=freeze_norm)\n    with tf.variable_scope(scope):\n        # net shape : [B, H, W, C]\n        if DATA_FORMAT.strip() == ""NCHW"":\n            net = tf.reduce_mean(net, axis=[2, 3], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, C, 1, 1]\n        elif DATA_FORMAT.strip() == ""NHWC"":\n            net = tf.reduce_mean(net, axis=[1, 2], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, 1, 1, C]\n        else:\n            raise ValueError(""Data Format Erro..."")\n\n        net = slim.flatten(net, scope=\'flatten\')\n        if dropout:\n            net = slim.dropout(net, keep_prob=0.5, is_training=is_training)\n        logits = slim.fully_connected(net, num_outputs=num_cls, activation_fn=None, scope=\'logits\')\n        return logits\n\n\ndef get_resnet_v1_d_base(input_x, freeze_norm, scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_stack_3x3(net=input_x, input_channel=32, scope=""C1"")\n            feature_dict[""C1""] = net\n            # print (net)\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-1]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=True, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_d(input_x,\n                    scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3],\n                    base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False],\n                    is_training=True, freeze_norm=False,\n                    num_cls=1000, dropout=False):\n\n    net, fet_dict = get_resnet_v1_d_base(input_x=input_x, scope=scope, bottleneck_nums=bottleneck_nums, base_channels=base_channels,\n                                         freeze=freeze, is_training=is_training, freeze_norm=freeze_norm)\n    with tf.variable_scope(scope):\n        # net shape : [B, H, W, C]\n        if DATA_FORMAT.strip() == ""NCHW"":\n            net = tf.reduce_mean(net, axis=[2, 3], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, C, 1, 1]\n        elif DATA_FORMAT.strip() == ""NHWC"":\n            net = tf.reduce_mean(net, axis=[1, 2], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, 1, 1, C]\n        else:\n            raise ValueError(""Data Format Erro..."")\n\n        net = slim.flatten(net, scope=\'flatten\')\n        if dropout:\n            net = slim.dropout(net, keep_prob=0.5, is_training=is_training)\n        logits = slim.fully_connected(net, num_outputs=num_cls, activation_fn=None, scope=\'logits\')\n        return logits\n\n\n\n\n\n\n'"
libs/gluon2TF/resnet/resnet_utils_NCHW.py,26,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport math\n\nDATA_FORMAT = ""NCHW""  # to match data format for mxnet\ndebug_dict = {}\ndef resnet_arg_scope(freeze_norm, is_training=True, weight_decay=0.0001,\n                     batch_norm_decay=0.9, batch_norm_epsilon=1e-5, batch_norm_scale=True):\n\n    batch_norm_params = {\n        \'is_training\': (not freeze_norm) and is_training, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': (not freeze_norm) and is_training,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n        \'data_format\': DATA_FORMAT\n    }\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef stem_7x7(net, scope=""C1""):\n\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [0, 0], [3, 3], [3, 3]])  # pad for data\n        net = slim.conv2d(net, num_outputs=64, kernel_size=[7, 7], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=""conv0"", normalizer_fn=None, activation_fn=None)\n        debug_dict[\'conv_7x7\'] = net\n        with tf.variable_scope(\'conv0\') as scope:\n            net = slim.batch_norm(net)\n            debug_dict[\'conv_7x7_bn\'] = net\n            net = tf.nn.relu(net)\n            debug_dict[\'conv_7x7_bn_relu\'] = net\n\n        net = tf.pad(net, paddings=[[0, 0], [0, 0], [1, 1], [1, 1]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\ndef stem_stack_3x3(net, input_channel=32, scope=""C1""):\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [0, 0], [1, 1], [1, 1]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        net = tf.pad(net, paddings=[[0, 0], [0, 0], [1, 1], [1, 1]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        net = tf.pad(net, paddings=[[0, 0], [0, 0], [1, 1], [1, 1]])\n        net = slim.conv2d(net, num_outputs=input_channel*2, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv2\')\n        net = tf.pad(net, paddings=[[0, 0], [0, 0], [1, 1], [1, 1]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef bottleneck_v1b(input_x, base_channel, scope, stride=1, projection=False, avg_down=True):\n    \'\'\'\n    for bottleneck_v1b: reduce spatial dim in conv_3x3 with stride 2.\n    \'\'\'\n    with tf.variable_scope(scope):\n        debug_dict[input_x.op.name] = input_x\n        net = slim.conv2d(input_x, num_outputs=base_channel, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        debug_dict[net.op.name] = net\n        net = tf.pad(net, paddings=[[0, 0], [0, 0], [1, 1], [1, 1]])\n        debug_dict[net.op.name] = net\n        net = slim.conv2d(net, num_outputs=base_channel, kernel_size=[3, 3], stride=stride,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        debug_dict[net.op.name] = net\n        net = slim.conv2d(net, num_outputs=base_channel * 4, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          activation_fn=None, scope=\'conv2\')\n        debug_dict[net.op.name] = net\n        # Note that : gamma in the last conv should be init with 0.\n        # But we just reload params from mxnet, so don\'t specific batch norm initializer\n        if projection:\n\n            if avg_down:  # design for resnet_v1d\n                # pad = 1  # int(math.floor((stride - 1)/2.0))\n                # input_x = tf.pad(input_x, paddings=[[0, 0], [0, 0], [pad, pad], [pad, pad]])\n                shortcut = slim.avg_pool2d(input_x, kernel_size=[stride, stride], stride=stride, padding=""SAME"",\n                                           data_format=DATA_FORMAT)\n                debug_dict[shortcut.op.name] = shortcut\n                shortcut = slim.conv2d(shortcut, num_outputs=base_channel*4, kernel_size=[1, 1],\n                                       stride=1, padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                                       activation_fn=None,\n                                       scope=\'shortcut\')\n                debug_dict[shortcut.op.name] = shortcut\n                # shortcut should have batch norm.\n            else:\n                shortcut = slim.conv2d(input_x, num_outputs=base_channel * 4, kernel_size=[1, 1],\n                                       stride=stride, padding=""VALID"", biases_initializer=None, activation_fn=None,\n                                       data_format=DATA_FORMAT,\n                                       scope=\'shortcut\')\n                debug_dict[shortcut.op.name] = shortcut\n        else:\n            shortcut = tf.identity(input_x, name=\'shortcut/Relu\')\n            debug_dict[shortcut.op.name] = shortcut\n\n        net = net + shortcut\n        debug_dict[net.op.name] = net\n        net = tf.nn.relu(net)\n        debug_dict[net.op.name] = net\n        return net\n\n\ndef make_block(net, base_channel, bottleneck_nums, scope, avg_down=True, spatial_downsample=False):\n    with tf.variable_scope(scope):\n        first_stride = 2 if spatial_downsample else 1\n\n        net = bottleneck_v1b(input_x=net, base_channel=base_channel,scope=\'bottleneck_0\',\n                             stride=first_stride, avg_down=avg_down, projection=True)\n        for i in range(1, bottleneck_nums):\n            net = bottleneck_v1b(input_x=net, base_channel=base_channel, scope=""bottleneck_%d"" % i,\n                                 stride=1, avg_down=avg_down, projection=False)\n        return net\n\n\ndef get_resnet_v1_b_base(input_x, freeze_norm, scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_7x7(net=input_x, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-2]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=False, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_b(input_x,\n                    scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3],\n                    base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False],\n                    is_training=True, freeze_norm=False,\n                    num_cls=1000, dropout=False):\n\n    net, fet_dict = get_resnet_v1_b_base(input_x=input_x, scope=scope, bottleneck_nums=bottleneck_nums, base_channels=base_channels,\n                                         freeze=freeze, is_training=is_training, freeze_norm=freeze_norm)\n    with tf.variable_scope(scope):\n        # net shape : [B, C, H, W]\n        if DATA_FORMAT.strip() == ""NCHW"":\n            net = tf.reduce_mean(net, axis=[2, 3], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, C, 1, 1]\n        elif DATA_FORMAT.strip() == ""NHWC"":\n            net = tf.reduce_mean(net, axis=[1, 2], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, 1, 1, C]\n        else:\n            raise ValueError(""Data Format Erro..."")\n\n        net = slim.flatten(net, scope=\'flatten\')\n        if dropout:\n            net = slim.dropout(net, keep_prob=0.5, is_training=is_training)\n        logits = slim.fully_connected(net, num_outputs=num_cls, activation_fn=None, scope=\'logits\')\n        return logits\n\n\ndef get_resnet_v1_d_base(input_x, freeze_norm, scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_stack_3x3(net=input_x, input_channel=32, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-2]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=True, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\ndef get_resnet_v1_d(input_x,\n                    scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3],\n                    base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False],\n                    is_training=True, freeze_norm=False,\n                    num_cls=1000, dropout=False):\n\n    net, fet_dict = get_resnet_v1_d_base(input_x=input_x, scope=scope, bottleneck_nums=bottleneck_nums, base_channels=base_channels,\n                                         freeze=freeze, is_training=is_training, freeze_norm=freeze_norm)\n    with tf.variable_scope(scope):\n        # net shape : [B, C, H, W]\n        if DATA_FORMAT.strip() == ""NCHW"":\n            net = tf.reduce_mean(net, axis=[2, 3], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, C, 1, 1]\n        elif DATA_FORMAT.strip() == ""NHWC"":\n            net = tf.reduce_mean(net, axis=[1, 2], name=""global_avg_pooling"",\n                                 keep_dims=True)  # [B, 1, 1, C]\n        else:\n            raise ValueError(""Data Format Erro..."")\n\n        net = slim.flatten(net, scope=\'flatten\')\n        if dropout:\n            net = slim.dropout(net, keep_prob=0.5, is_training=is_training)\n        logits = slim.fully_connected(net, num_outputs=num_cls, activation_fn=None, scope=\'logits\')\n        return logits\n\n\n\n\n\n\n'"
libs/gluon2TF/resnet/some_test.py,5,"b'# -*- coding: utf-8 -*-\n\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom mxnet import ndarray as nd\nimport tensorflow.contrib.slim as slim\nimport tensorflow as tf\nimport numpy as np\nfrom test_resnet import mxnet_process_img\n# \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n# \xe8\xbe\x93\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x98\xaf\xef\xbc\x9a batch * channel * height * width\n# \xe6\x9d\x83\xe9\x87\x8d\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x9aoutput_channels * in_channels * height * width\nnp.random.seed(30)\n\n# w = nd.array(np.random.rand(2, 3, 3, 3))\nw = nd.load(\'/home/yjr/MxNet_Codes/gluon-cv/scripts/gloun2TF/mxnet_weights/resnet50_v1b-0ecdba34.params\')[\'conv1.weight\']  # [64, 3, 7, 7]\n# w = nd.arange(9*2).reshape((2, 1, 3, 3))\ndata = nd.array(np.random.rand(1, 3, 224, 224))\n# data, _ = mxnet_process_img(\'../demo_img/person.jpg\')\n# data = nd.arange(6*6).reshape((1, 1, 6, 6))\n\n# \xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbf\x90\xe7\xae\x97\nout = nd.Convolution(data, w, no_bias=True,\n                     kernel=(7, 7),\n                     stride=(2, 2),\n                     num_filter=64,\n                     pad=(3, 3))\n\n\n\ndef tf_conv(data, w):\n\n    data = tf.constant(data.asnumpy())\n    data = tf.pad(data, paddings=[[0, 0], [0, 0], [3, 3], [3, 3]])\n    tf_out = slim.conv2d(data, num_outputs=64, kernel_size=[7, 7], padding=\'VALID\', stride=2,\n                         biases_initializer=None, data_format=\'NCHW\', normalizer_fn=None, activation_fn=None)\n    tf_w = tf.constant(np.transpose(w.asnumpy(), [2, 3, 1, 0]))\n    # tf_w =\n    model_vars = slim.get_model_variables()\n    assign_op = tf.assign(model_vars[0], tf_w)\n\n    with tf.Session() as sess:\n        sess.run(assign_op)\n        print(sess.run(tf_out))\n\n\nif __name__ == \'__main__\':\n    tf_conv(data, w=w)\n    print ""mxnet_out: "", out\n    print 20 * ""+""'"
libs/gluon2TF/resnet/test_resnet.py,6,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom mxnet import nd, image\nimport numpy as np\nfrom mxnet.gluon.data.vision import transforms\nfrom gluoncv.model_zoo import get_model\nfrom gluoncv.data.transforms.presets.imagenet import transform_eval\nfrom resnet import build_resnet, create_resotre_op\nfrom resnet_utils import DEBUG\nimport os\n\n\n# MODEL_NAME = \'resnet50_v1d\'\n# Mxnet_Weights_PATH = \'../mxnet_weights/resnet50_v1d-117a384e.params\'\n\n\nMODEL_NAME = \'resnet101_v1b\'\n# Mxnet_Weights_PATH = \'../mxnet_weights/resnet101_v1d-1b2b825f.params\'\nMxnet_Weights_PATH = \'../mxnet_weights/resnet101_v1b-a455932a.params\'\n\ndef mxnet_process_img(path):\n    # Load Images\n    img = image.imread(path)\n\n    # Transform\n    img = transform_eval(img)\n    img_arr = img.asnumpy()\n    if len(img_arr) == 3:\n        img_arr = np.expand_dims(img_arr, axis=0)\n    img_tf_tensor = tf.constant(img_arr)\n\n    # np.random.seed(30)\n    # img = nd.array(np.random.randn(1, 3, 600, 800))\n    # img_tf_tensor = tf.constant(img.asnumpy())\n    img_tf_tensor = tf.transpose(img_tf_tensor, [0, 2, 3, 1])\n    return img, img_tf_tensor\n\n#\ndef mxnet_infer(img):\n\n    model_name = MODEL_NAME\n    net = get_model(model_name, pretrained=False)\n    net.load_parameters(Mxnet_Weights_PATH)\n    pred = net(img)\n\n    # print (pred.shape, pred.dtype)\n    pred = pred.asnumpy()\n    return pred\n\n\ndef tf_infer(img, save_ckpt=True, restore_from_tfckpt=False, ckpt_path=None):\n\n    pred_tensor = build_resnet(img_batch=img, scope=MODEL_NAME,\n                               is_training=False, freeze_norm=True, num_cls=1000)\n\n    if restore_from_tfckpt:\n        print(""restore weights from tf_CKPT"")\n        assert not ckpt_path is None, ""ckpt_path is None, Erro""\n        restore_op = tf.train.Saver()\n    else:\n        print(\'restore weights from MxnetWeights\')\n        restore_op = create_resotre_op(MODEL_NAME, Mxnet_Weights_PATH)\n\n    if DEBUG:\n        from resnet_utils import debug_dict\n        print (debug_dict)\n        assert len(debug_dict) >=3, ""debug_dict size erro, len is :{}"".format(len(debug_dict))\n\n    if save_ckpt:\n        save_dir = \'../tf_ckpts\'\n        if not os.path.exists(save_dir):\n            os.mkdir(save_dir)\n        saver = tf.train.Saver(max_to_keep=30)\n        save_ckpt = os.path.join(save_dir, \'%s.ckpt\' % MODEL_NAME)\n\n    with tf.Session() as sess:\n        if restore_from_tfckpt:\n            restore_op.restore(sess, ckpt_path)\n        else:\n            sess.run(restore_op)\n        if DEBUG:\n            name_val = {}\n            for name in debug_dict.keys():\n                name_val[name] = sess.run(debug_dict[name])\n        pred = sess.run(pred_tensor)\n        if save_ckpt:\n            saver.save(sess, save_ckpt)\n\n    return pred\n\n\ndef cal_erro(img_path, use_tf_ckpt=False, ckpt_path=None, save_ckpt=False):\n\n    mxnet_img, tf_img = mxnet_process_img(img_path)\n\n    mxnet_pred = mxnet_infer(mxnet_img)\n\n    mxnet_pred = np.squeeze(mxnet_pred, axis=0)\n    tf_pred = tf_infer(tf_img, restore_from_tfckpt=use_tf_ckpt, ckpt_path=ckpt_path, save_ckpt=save_ckpt)\n    tf_pred = np.squeeze(tf_pred, axis=0)\n    assert mxnet_pred.shape == tf_pred.shape, ""mxnet_pred shape Do Not equal with tf_pred shape""\n\n    argmax_mxnet = np.argmax(mxnet_pred)\n    argmax_tf = np.argmax(tf_pred)\n\n    erro = np.linalg.norm(tf_pred-mxnet_pred)\n    for i, (m, t) in enumerate(zip(mxnet_pred, tf_pred)):\n        if i == 5:\n            break\n        print (""mxnet|tf==>>{} | {} "".format(m, t))\n\n    print (\'total_erro-->\', erro)\n    print (\'erro_rate-->\', erro/np.linalg.norm(mxnet_pred))\n    print (""argmax_mxnet: {} || tf_argmx: {}"".format(argmax_mxnet, argmax_tf))\n\n\nif __name__ == \'__main__\':\n\n    # cal_erro(img_path=\'../demo_img/person.jpg\',\n    #          use_tf_ckpt=False,\n    #          ckpt_path=None,\n    #          save_ckpt=True)\n    cal_erro(img_path=\'../demo_img/person.jpg\',\n             use_tf_ckpt=True,\n             ckpt_path=\'../tf_ckpts/%s.ckpt\' % MODEL_NAME,\n             save_ckpt=False)\n    print (20*""++"")\n'"
libs/gluon2TF/resnet/weights_map.py,0,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n# from tensorflow.python.tools import inspect_checkpoint as chkp\n#\n# # print all tensors in checkpoint file\n# chkp.print_tensors_in_checkpoint_file(""/tmp/model.ckpt"", tensor_name=\'\', all_tensors=True)\n\n\ntf_mxnet_map = {}\n\ntf_mxnet_prefix_map = {""weights"": ""weight"",\n                       ""moving_mean"": ""running_mean"",\n                       ""moving_variance"": ""running_var"",\n                       ""beta"": ""beta"",\n                       ""gamma"": ""gamma""}\ndef update_logitis():\n    tf_mxnet_map[""logits/weights""] = \'fc.weight\'\n    tf_mxnet_map[""logits/biases""] = \'fc.bias\'\n\n\ndef update_C1_resnet_v1_b():\n    tmp_map = {""C1/conv0/weights"": ""conv1.weight"",\n               ""C1/conv0/BatchNorm/beta"": ""bn1.beta"",\n               ""C1/conv0/BatchNorm/gamma"": ""bn1.gamma"",\n               ""C1/conv0/BatchNorm/moving_mean"": ""bn1.running_mean"",\n               ""C1/conv0/BatchNorm/moving_variance"": ""bn1.running_var""}\n    tf_mxnet_map.update(tmp_map)\n\n\ndef update_C1_resnet_v1_d():\n    tmp_map = {""C1/conv0/weights"": ""conv1.0.weight"",\n               ""C1/conv0/BatchNorm/beta"": ""conv1.1.beta"",\n               ""C1/conv0/BatchNorm/gamma"": ""conv1.1.gamma"",\n               ""C1/conv0/BatchNorm/moving_mean"": ""conv1.1.running_mean"",\n               ""C1/conv0/BatchNorm/moving_variance"": ""conv1.1.running_var"",\n               ""C1/conv1/weights"": ""conv1.3.weight"",\n               ""C1/conv2/weights"": ""conv1.6.weight""}\n    tf_mxnet_map.update(tmp_map)\n\n    tf_prefix = ""C1/conv1/BatchNorm/""\n    for key in tf_mxnet_prefix_map.keys():\n        if key != \'weights\':\n            tf_mxnet_map[tf_prefix+key] = ""conv1.4."" + tf_mxnet_prefix_map[key]\n\n    tf_prefix = ""C1/conv2/BatchNorm/""\n    for key in tf_mxnet_prefix_map.keys():\n        if key != \'weights\':\n            tf_mxnet_map[tf_prefix+key] = ""bn1."" + tf_mxnet_prefix_map[key]\n\n\ndef update_C2345(scope, bottleneck_nums):\n\n    \'\'\'\n    bottleneck nums :[3, 4, 6, 3] for res 50\n    \'\'\'\n    for layer, num in enumerate(bottleneck_nums):\n\n        layer += 2  # 0->C2; 1->C3...3->C5\n        for i in range(num):\n            for j in range(3):\n                tf_prefix = ""C%d/bottleneck_%d/conv%d/"" % (layer, i, j)\n                for key in tf_mxnet_prefix_map.keys():\n                    if key == \'weights\':\n                        tf_mxnet_map[tf_prefix + key] = ""layer%d.%d.conv%d."" % (layer-1, i, j + 1) + tf_mxnet_prefix_map[key]\n                    else:\n                        tf_mxnet_map[tf_prefix + ""BatchNorm/"" + key] = ""layer%d.%d.bn%d."" % (layer-1, i, j + 1) + tf_mxnet_prefix_map[key]\n            if i == 0:\n                tf_prefix = ""C%d/bottleneck_%d/shortcut/"" % (layer, i)\n                for key in tf_mxnet_prefix_map.keys():\n                    index = 1\n                    if scope.endswith(\'b\'):\n                        index = 0\n                    if key == \'weights\':\n                        tf_mxnet_map[tf_prefix + key] = ""layer%d.%d.downsample.%d."" % (layer-1, i, index) + tf_mxnet_prefix_map[key]\n                    else:\n                        tf_mxnet_map[tf_prefix + ""BatchNorm/"" + key] = ""layer%d.%d.downsample.%d."" % (layer-1, i, index+1) + tf_mxnet_prefix_map[key]\n\n\n\n# def update_C2(bottleneck_num):\n#     for i in range(bottleneck_num):\n#         for j in range(3):\n#             tf_prefix = ""C2/bottleneck_%d/conv%d/"" % (i, j)\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix+key] = ""layer1.%d.conv%d."" % (i, j+1) + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer1.%d.bn%d."" % (i, j + 1) + tf_mxnet_prefix_map[key]\n#         if i==0:\n#             tf_prefix = ""C2/bottleneck_%d/shortcut/"" % i\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix + key] = ""layer1.%d.downsample.1."" % i + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer1.%d.downsample.2."" % i + tf_mxnet_prefix_map[key]\n#\n# def update_C3(bottleneck_num):\n#\n#     for i in range(bottleneck_num):\n#         for j in range(3):\n#             tf_prefix = ""C3/bottleneck_%d/conv%d/"" % (i, j)\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix+key] = ""layer2.%d.conv%d."" % (i, j+1) + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer2.%d.bn%d."" % (i, j + 1) + tf_mxnet_prefix_map[key]\n#         if i == 0:\n#             tf_prefix = ""C3/bottleneck_%d/shortcut/"" % i\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix + key] = ""layer2.%d.downsample.1."" % i + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer2.%d.downsample.2."" % i + tf_mxnet_prefix_map[key]\n#\n# def update_C4(bottleneck_num):\n#     for i in range(bottleneck_num):\n#         for j in range(3):\n#             tf_prefix = ""C4/bottleneck_%d/conv%d/"" % (i, j)\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix+key] = ""layer3.%d.conv%d."" % (i, j+1) + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer3.%d.bn%d."" % (i, j + 1) + tf_mxnet_prefix_map[key]\n#         if i == 0:\n#             tf_prefix = ""C4/bottleneck_%d/shortcut/"" % i\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix + key] = ""layer3.%d.downsample.1."" % i + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer3.%d.downsample.2."" % i + tf_mxnet_prefix_map[key]\n#\n# def update_C5(bottleneck_num):\n#     for i in range(bottleneck_num):\n#         for j in range(3):\n#             tf_prefix = ""C5/bottleneck_%d/conv%d/"" % (i, j)\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix+key] = ""layer4.%d.conv%d."" % (i, j+1) + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer4.%d.bn%d."" % (i, j + 1) + tf_mxnet_prefix_map[key]\n#         if i == 0:\n#             tf_prefix = ""C5/bottleneck_%d/shortcut/"" % i\n#             for key in tf_mxnet_prefix_map.keys():\n#                 if key == \'weights\':\n#                     tf_mxnet_map[tf_prefix + key] = ""layer4.%d.downsample.1."" % i + tf_mxnet_prefix_map[key]\n#                 else:\n#                     tf_mxnet_map[tf_prefix + key] = ""layer4.%d.downsample.2."" % i + tf_mxnet_prefix_map[key]\n\n\ndef get_map(scope, bottleneck_nums, show_mxnettf=True, show_tfmxnet=True):\n\n    if scope.endswith(\'b\'):\n        update_C1_resnet_v1_b()\n    elif scope.endswith(\'d\'):\n        update_C1_resnet_v1_d()\n\n    update_C2345(scope, bottleneck_nums)\n    update_logitis()\n\n    mxnet_tf_map = {}\n    for tf_name, mxnet_name in tf_mxnet_map.items():\n        mxnet_tf_map[mxnet_name] = tf_name\n\n    if show_mxnettf:\n        for key in sorted(mxnet_tf_map.keys()):\n            print (""{} :: {}"".format(key, mxnet_tf_map[key]))\n        print(20*""==="")\n\n    if show_tfmxnet:\n        for key in sorted(tf_mxnet_map.keys()):\n            print(""{} :: {}"".format(key, tf_mxnet_map[key]))\n        print(20 * ""==="")\n\n    return tf_mxnet_map, mxnet_tf_map\n\n\nif __name__ == ""__main__"":\n    get_map(bottleneck_nums=[3, 4, 6, 3])\n'"
libs/networks/mobilenet/__init__.py,0,b''
libs/networks/mobilenet/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional scope.\n\n  Returns:\n    Tensor of depth num_outputs\n\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=tf.identity)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')\n'"
libs/networks/mobilenet/mobilenet.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = defaults.items()\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode).\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      slim.arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n    set to non-training mode. This might be helpful for code that is reused\n    across both training/evaluation, but most of the time training_scope with\n    value False is not needed.\n\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability\n    bn_decay: decay for the batch norm moving averages.\n\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'decay\': bn_decay,\n  }\n\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      slim.arg_scope([slim.batch_norm], **batch_norm_params), \\\n      slim.arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s\n'"
libs/networks/mobilenet/mobilenet_v2.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\n\nArchitecture: https://arxiv.org/abs/1801.04381\n\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import conv_blocks as ops\nfrom libs.networks.mobilenet import mobilenet as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\nV2_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24),\n        op(ops.expanded_conv, stride=1, num_outputs=24),\n        op(ops.expanded_conv, stride=2, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=2, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=2, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=320),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n    ],\n)\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediciton_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_classes=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  with slim.\n\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']\n'"
libs/networks/mobilenet/mobilenet_v2_test.py,25,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for mobilenet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport copy\nimport tensorflow as tf\nfrom nets.mobilenet import conv_blocks as ops\nfrom nets.mobilenet import mobilenet\nfrom nets.mobilenet import mobilenet_v2\n\n\nslim = tf.contrib.slim\n\n\ndef find_ops(optype):\n  """"""Find ops of a given type in graphdef or a graph.\n\n  Args:\n    optype: operation type (e.g. Conv2D)\n  Returns:\n     List of operations.\n  """"""\n  gd = tf.get_default_graph()\n  return [var for var in gd.get_operations() if var.type == optype]\n\n\nclass MobilenetV2Test(tf.test.TestCase):\n\n  def setUp(self):\n    tf.reset_default_graph()\n\n  def testCreation(self):\n    spec = dict(mobilenet_v2.V2_DEF)\n    _, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n\n    # This is mostly a sanity test. No deep reason for these particular\n    # constants.\n    #\n    # All but first 2 and last one have  two convolutions, and there is one\n    # extra conv that is not in the spec. (logits)\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 2 - 2)\n    # Check that depthwise are exposed.\n    for i in range(2, 17):\n      self.assertIn(\'layer_%d/depthwise_output\' % i, ep)\n\n  def testCreationNoClasses(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    net, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec,\n        num_classes=None)\n    self.assertIs(net, ep[\'global_pool\'])\n\n  def testImageSizes(self):\n    for input_size, output_size in [(224, 7), (192, 6), (160, 5),\n                                    (128, 4), (96, 3)]:\n      tf.reset_default_graph()\n      _, ep = mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, input_size, input_size, 3)))\n\n      self.assertEqual(ep[\'layer_18/output\'].get_shape().as_list()[1:3],\n                       [output_size] * 2)\n\n  def testWithSplits(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    spec[\'overrides\'] = {\n        (ops.expanded_conv,): dict(split_expansion=2),\n    }\n    _, _ = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n    # All but 3 op has 3 conv operatore, the remainign 3 have one\n    # and there is one unaccounted.\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 3 - 5)\n\n  def testWithOutputStride8(self):\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testDivisibleBy(self):\n    tf.reset_default_graph()\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        divisible_by=16,\n        min_depth=32)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    self.assertSameElements([32, 64, 96, 160, 192, 320, 384, 576, 960, 1280,\n                             1001], s)\n\n  def testDivisibleByWithArgScope(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, 224, 224, 2)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n      s = set(s)\n      self.assertSameElements(s, [32, 192, 128, 1001])\n\n  def testFineGrained(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 2)),\n        conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.01,\n        finegrain_classification_mode=True)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    # All convolutions will be 8->48, except for the last one.\n    self.assertSameElements(s, [8, 48, 1001, 1280])\n\n  def testMobilenetBase(self):\n    tf.reset_default_graph()\n    # Verifies that mobilenet_base returns pre-pooling layer.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      net, _ = mobilenet_v2.mobilenet_base(\n          tf.placeholder(tf.float32, (10, 224, 224, 16)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      self.assertEqual(net.get_shape().as_list(), [10, 7, 7, 128])\n\n  def testWithOutputStride16(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n  def testWithOutputStride8AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        use_explicit_padding=True,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testWithOutputStride16AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16,\n        use_explicit_padding=True)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/__init__.py,0,b'\n'
libs/networks/slim_nets/alexnet.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
libs/networks/slim_nets/alexnet_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_resnet_v2 import inception_resnet_v2_base\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
libs/networks/slim_nets/inception_resnet_v2.py,41,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint=\'Conv2d_7b_1x1\',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None):\n  """"""Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_6a\', \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after \'PreAuxLogits\'.\n  """"""\n  if output_stride != 8 and output_stride != 16:\n    raise ValueError(\'output_stride must be 8 or 16.\')\n\n  padding = \'SAME\' if align_feature_maps else \'VALID\'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_3a_3x3\')\n      if add_and_check_final(\'MaxPool_3a_3x3\', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope=\'Conv2d_3b_1x1\')\n      if add_and_check_final(\'Conv2d_3b_1x1\', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope=\'Conv2d_4a_3x3\')\n      if add_and_check_final(\'Conv2d_4a_3x3\', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_5a_3x3\')\n      if add_and_check_final(\'MaxPool_5a_3x3\', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope(\'Mixed_5b\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                       scope=\'AvgPool_0a_3x3\')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final(\'Mixed_5b\', net): return net, end_points\n      # TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17)\n\n      # 17 x 17 x 1088 if output_stride == 8,\n      # 33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope(\'Mixed_6a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10)\n      if add_and_check_final(\'PreAuxLogits\', net): return net, end_points\n\n      if output_stride == 8:\n        # TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError(\'output_stride==8 is only supported up to the \'\n                         \'PreAuxlogits end_point for now.\')\n\n      # 8 x 8 x 2080\n      with tf.variable_scope(\'Mixed_7a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20)\n      net = block8(net, activation_fn=None)\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n      if add_and_check_final(\'Conv2d_7b_1x1\', net): return net, end_points\n\n    raise ValueError(\'final_endpoint (%s) not recognized\', final_endpoint)\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,#0.8\n                        reuse=None,\n                        scope=\'InceptionResnetV2\',\n                        create_aux_logits=True):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n\n      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n\n      if create_aux_logits:\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = end_points[\'PreAuxLogits\']\n          aux = slim.avg_pool2d(aux, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                              scope=\'AvgPool_1a_8x8\')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'Dropout\')\n\n        end_points[\'PreLogitsFlatten\'] = net\n        # end_points[\'yjr_feature\'] = tf.squeeze(net, axis=0)\n\n        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                      scope=\'Logits\')\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
libs/networks/slim_nets/inception_resnet_v2_test.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'AuxLogits\' in endpoints)\n      auxlogits = endpoints[\'AuxLogits\']\n      self.assertTrue(\n          auxlogits.op.name.startswith(\'InceptionResnetV2/AuxLogits\'))\n      self.assertListEqual(auxlogits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes,\n                                                        create_aux_logits=False)\n      self.assertTrue(\'AuxLogits\' not in endpoints)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_resnet_v2_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'InceptionResnetV2/Conv2d_7b_1x1\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 8, 8, 1536])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                          \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                 \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_resnet_v2_base(\n            inputs, final_endpoint=endpoint)\n        if endpoint != \'PreAuxLogits\':\n          self.assertTrue(out_tensor.op.name.startswith(\n              \'InceptionResnetV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 17, 17, 1088],\n                        \'PreAuxLogits\': [5, 17, 17, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithAlignedFeatureMaps(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', align_feature_maps=True)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2b_3x3\': [5, 150, 150, 64],\n                        \'MaxPool_3a_3x3\': [5, 75, 75, 64],\n                        \'Conv2d_3b_1x1\': [5, 75, 75, 80],\n                        \'Conv2d_4a_3x3\': [5, 75, 75, 192],\n                        \'MaxPool_5a_3x3\': [5, 38, 38, 192],\n                        \'Mixed_5b\': [5, 38, 38, 320],\n                        \'Mixed_6a\': [5, 19, 19, 1088],\n                        \'PreAuxLogits\': [5, 19, 19, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithOutputStrideEight(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', output_stride=8)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 33, 33, 1088],\n                        \'PreAuxLogits\': [5, 33, 33, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v4_test.py,24,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
libs/networks/slim_nets/mobilenet_v1.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""MobileNet v1.\n\nMobileNet is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nhead (for example: embeddings, localization and classification).\n\nAs described in https://arxiv.org/abs/1704.04861.\n\n  MobileNets: Efficient Convolutional Neural Networks for\n    Mobile Vision Applications\n  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n    Tobias Weyand, Marco Andreetto, Hartwig Adam\n\n100% Mobilenet V1 (base) with input size 224x224:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 864      10,838,016\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    288       3,612,672\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     2,048      25,690,112\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    576       1,806,336\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     8,192      25,690,112\nMobilenetV1/Conv2d_3_depthwise/depthwise:                  1,152       3,612,672\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                    16,384      51,380,224\nMobilenetV1/Conv2d_4_depthwise/depthwise:                  1,152         903,168\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    32,768      25,690,112\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  2,304       1,806,336\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    65,536      51,380,224\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  2,304         451,584\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                   131,072      25,690,112\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 4,608         225,792\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  524,288      25,690,112\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 9,216         451,584\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                1,048,576      51,380,224\n--------------------------------------------------------------------------------\nTotal:                                                 3,185,088     567,716,352\n\n\n75% Mobilenet V1 (base) with input size 128x128:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 648       2,654,208\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    216         884,736\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     1,152       4,718,592\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    432         442,368\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     4,608       4,718,592\nMobilenetV1/Conv2d_3_depthwise/depthwise:                    864         884,736\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                     9,216       9,437,184\nMobilenetV1/Conv2d_4_depthwise/depthwise:                    864         221,184\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    18,432       4,718,592\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  1,728         442,368\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    36,864       9,437,184\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  1,728         110,592\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                    73,728       4,718,592\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 3,456          55,296\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  294,912       4,718,592\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 6,912         110,592\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                  589,824       9,437,184\n--------------------------------------------------------------------------------\nTotal:                                                 1,800,144     106,002,432\n\n""""""\n\n# Tensorflow mandates these.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# Conv and DepthSepConv namedtuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=[3, 3], stride=2, depth=32),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)\n]\n\n\ndef mobilenet_v1_base(inputs,\n                      final_endpoint=\'Conv2d_13_pointwise\',\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      conv_defs=None,\n                      output_stride=None,\n                      scope=None):\n  """"""Mobilenet v1.\n\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_0\', \'Conv2d_1_pointwise\', \'Conv2d_2_pointwise\',\n      \'Conv2d_3_pointwise\', \'Conv2d_4_pointwise\', \'Conv2d_5\'_pointwise,\n      \'Conv2d_6_pointwise\', \'Conv2d_7_pointwise\', \'Conv2d_8_pointwise\',\n      \'Conv2d_9_pointwise\', \'Conv2d_10_pointwise\', \'Conv2d_11_pointwise\',\n      \'Conv2d_12_pointwise\', \'Conv2d_13_pointwise\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 8 (accurate fully convolutional\n      mode), 16 (fast fully convolutional mode), 32 (classification mode).\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  if conv_defs is None:\n    conv_defs = _CONV_DEFS\n\n  if output_stride is not None and output_stride not in [8, 16, 32]:\n    raise ValueError(\'Only allowed output_stride values are 8, 16, 32.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n      # The current_stride variable keeps track of the output stride of the\n      # activations, i.e., the running product of convolution strides up to the\n      # current network layer. This allows us to invoke atrous convolution\n      # whenever applying the next convolution would result in the activations\n      # having output stride larger than the target output_stride.\n      current_stride = 1\n\n      # The atrous convolution rate parameter.\n      rate = 1\n\n      net = inputs\n      for i, conv_def in enumerate(conv_defs):\n        end_point_base = \'Conv2d_%d\' % i\n\n        if output_stride is not None and current_stride == output_stride:\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          layer_stride = 1\n          layer_rate = rate\n          rate *= conv_def.stride\n        else:\n          layer_stride = conv_def.stride\n          layer_rate = 1\n          current_stride *= conv_def.stride\n\n        if isinstance(conv_def, Conv):\n          end_point = end_point_base\n          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n                            stride=conv_def.stride,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        elif isinstance(conv_def, DepthSepConv):\n          end_point = end_point_base + \'_depthwise\'\n\n          # By passing filters=None\n          # separable_conv2d produces only a depthwise convolution layer\n          net = slim.separable_conv2d(net, None, conv_def.kernel,\n                                      depth_multiplier=1,\n                                      stride=layer_stride,\n                                      rate=layer_rate,\n                                      normalizer_fn=slim.batch_norm,\n                                      scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n          end_point = end_point_base + \'_pointwise\'\n\n          net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                            stride=1,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n        else:\n          raise ValueError(\'Unknown convolution type %s for layer %d\'\n                           % (conv_def.ltype, i))\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef mobilenet_v1(inputs,\n                 num_classes=1000,\n                 dropout_keep_prob=0.999,\n                 is_training=True,\n                 min_depth=8,\n                 depth_multiplier=1.0,\n                 conv_defs=None,\n                 prediction_fn=tf.contrib.layers.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'MobilenetV1\'):\n  """"""Mobilenet v1 model for classification.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    is_training: whether is training or not.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\n                     len(input_shape))\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = mobilenet_v1_base(inputs, scope=scope,\n                                          min_depth=min_depth,\n                                          depth_multiplier=depth_multiplier,\n                                          conv_defs=conv_defs)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a\')\n        end_points[\'AvgPool_1a\'] = net\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      if prediction_fn:\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\n\nmobilenet_v1.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef mobilenet_v1_arg_scope(is_training=True,\n                           weight_decay=0.00004,\n                           stddev=0.09,\n                           regularize_depthwise=False):\n  """"""Defines the default MobilenetV1 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    regularize_depthwise: Whether or not apply regularization on depthwise.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v1 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  if regularize_depthwise:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n'"
libs/networks/slim_nets/mobilenet_v1_test.py,32,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""Tests for MobileNet v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\nclass MobilenetV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_13\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                          \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                          \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                          \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                          \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                          \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                          \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                          \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                          \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                          \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                          \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_0\',\n                 \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                 \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                 \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                 \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                 \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                 \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                 \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                 \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                 \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                 \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                 \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                 \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                 \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = mobilenet_v1.mobilenet_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'MobilenetV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildCustomNetworkUsingConvDefs(self):\n    batch_size = 5\n    height, width = 224, 224\n    conv_defs = [\n        mobilenet_v1.Conv(kernel=[3, 3], stride=2, depth=32),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=512)\n    ]\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(\n        inputs, final_endpoint=\'Conv2d_3_pointwise\', conv_defs=conv_defs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_3\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 56, 56, 512])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 7, 7, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 7, 7, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride16BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 16\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 14, 14, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride8BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 8\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 28, 28, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsApproximateFaceNet(self):\n    batch_size = 5\n    height, width = 128, 128\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\', depth_multiplier=0.75)\n    # For the Conv2d_0 layer FaceNet has depth=16\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_depthwise\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_pointwise\': [batch_size, 64, 64, 48],\n                        \'Conv2d_2_depthwise\': [batch_size, 32, 32, 48],\n                        \'Conv2d_2_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_depthwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_4_depthwise\': [batch_size, 16, 16, 96],\n                        \'Conv2d_4_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_depthwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_6_depthwise\': [batch_size, 8, 8, 192],\n                        \'Conv2d_6_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_12_depthwise\': [batch_size, 4, 4, 384],\n                        \'Conv2d_12_pointwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_depthwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_pointwise\': [batch_size, 4, 4, 768]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      mobilenet_v1.mobilenet_v1_base(inputs)\n      total_params, _ = slim.model_analyzer.analyze_vars(\n          slim.get_model_variables())\n      self.assertAlmostEqual(3217920L, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys() if key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Conv2d_13_pointwise\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_13_pointwise\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    mobilenet_v1.mobilenet_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = mobilenet_v1.mobilenet_v1(images,\n                                          num_classes=num_classes,\n                                          spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import mobilenet_v1\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n                \'mobilenet_v1\': mobilenet_v1.mobilenet_v1,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                  \'mobilenet_v1\': mobilenet_v1.mobilenet_v1_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
libs/networks/slim_nets/nets_factory_test.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnArgScope(self):\n    batch_size = 5\n    num_classes = 10\n    net = \'cifarnet\'\n    with self.test_session(use_gpu=True):\n      net_fn = nets_factory.get_network_fn(net, num_classes)\n      image_size = getattr(net_fn, \'default_image_size\', 224)\n      with slim.arg_scope([slim.model_variable, slim.variable],\n                          device=\'/CPU:0\'):\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        net_fn(inputs)\n      weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \'CifarNet/conv1\')[0]\n      self.assertDeviceEqual(\'/CPU:0\', weights.device)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
libs/networks/slim_nets/overfeat_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997, #0.997\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
libs/networks/slim_nets/resnet_v1.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.networks.slim_nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          # yjr_feature = tf.squeeze(net, [0, 1, 2])\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n\n        ###\n        # end_points[\'yjr_feature\'] = yjr_feature\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
libs/networks/slim_nets/resnet_v1_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    blocks = [\n        resnet_v1.resnet_v1_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v1.resnet_v1_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v2 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v2 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=False,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
libs/networks/slim_nets/resnet_v2_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    blocks = [\n        resnet_v2.resnet_v2_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v2.resnet_v2_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/vgg.py,10,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      # yjr_feature = tf.squeeze(net)\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      # end_points[\'yjr_feature\'] = yjr_feature\n      end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
libs/networks/slim_nets/vgg_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
data/lib_coco/PythonAPI/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
data/lib_coco/PythonAPI/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
data/lib_coco/PythonAPI/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
data/lib_coco/PythonAPI/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
