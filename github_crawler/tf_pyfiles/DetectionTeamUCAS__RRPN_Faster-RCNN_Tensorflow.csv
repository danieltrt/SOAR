file_path,api_count,code
__init__.py,0,b''
data/__init__.py,0,b''
help_utils/__init__.py,0,b''
help_utils/tools.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport math\nimport sys\nimport os\n\n\ndef view_bar(message, num, total):\n    rate = num / total\n    rate_num = int(rate * 40)\n    rate_nums = math.ceil(rate * 100)\n    r = \'\\r%s:[%s%s]%d%%\\t%d/%d\' % (message, "">"" * rate_num, "" "" * (40 - rate_num), rate_nums, num, total,)\n    sys.stdout.write(r)\n    sys.stdout.flush()\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)'"
libs/__init__.py,0,b''
libs/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport subprocess\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    # Extension(\n    #     ""utils.cython_bbox"",\n    #     [""utils/bbox.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    # Extension(\n    #     ""nms.cpu_nms"",\n    #     [""nms/cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_cython_nms"",\n    #     [""rotation/rotate_cython_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_circle_nms"",\n    #     [""rotation/rotate_circle_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\'nms.gpu_nms\',\n    #     [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    # Extension(\'rotation.rotate_gpu_nms\',\n    #     [\'rotation/rotate_nms_kernel.cu\', \'rotation/rotate_gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc anrbd not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    Extension(\'rotation.rbbox_overlaps\',\n        [\'rotation/rbbox_overlaps_kernel.cu\', \'rotation/rbbox_overlaps.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n    # Extension(\'rotation.rotate_polygon_nms\',\n    #     [\'rotation/rotate_polygon_nms_kernel.cu\', \'rotation/rotate_polygon_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    #\n    # Extension(\n    #     \'pycocotools._mask\',\n    #     sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n    #     include_dirs = [numpy_include, \'pycocotools\'],\n    #     extra_compile_args={\n    #         \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    # ),\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
tools/__init__.py,0,b''
tools/demo.py,8,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nsys.path.append(""../"")\nimport tensorflow as tf\nimport time\nimport cv2\nimport numpy as np\nfrom timeit import default_timer as timer\nimport argparse\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom help_utils.tools import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils import coordinate_convert\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils import tools\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef get_file_paths_recursive(folder=None, file_ext=None):\n    """""" Get the absolute path of all files in given folder recursively\n    :param folder:\n    :param file_ext:\n    :return:\n    """"""\n    file_list = []\n    if folder is None:\n        return file_list\n\n    for dir_path, dir_names, file_names in os.walk(folder):\n        for file_name in file_names:\n            if file_ext is None:\n                file_list.append(os.path.join(dir_path, file_name))\n                continue\n            if file_name.endswith(file_ext):\n                file_list.append(os.path.join(dir_path, file_name))\n    return file_list\n\n\ndef inference(det_net, file_paths, des_folder, h_len, w_len, h_overlap, w_overlap, show_res=False):\n\n    if show_res:\n        assert cfgs.SHOW_SCORE_THRSHOLD >= 0.5, \\\n            \'please set score threshold (example: SHOW_SCORE_THRSHOLD = 0.5) in cfgs.py\'\n\n    else:\n        assert cfgs.SHOW_SCORE_THRSHOLD < 0.005, \\\n            \'please set score threshold (example: SHOW_SCORE_THRSHOLD = 0.00) in cfgs.py\'\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     is_resize=False)\n\n    det_boxes, det_scores, det_category = det_net.build_whole_detection_network(input_img_batch=img_batch,\n                                                                                gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        for count, img_path in enumerate(file_paths):\n            start = timer()\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < h_len:\n                temp = np.zeros([h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = h_len\n\n            if imgW < w_len:\n                temp = np.zeros([imgH, w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = w_len\n\n            for hh in range(0, imgH, h_len - h_overlap):\n                if imgH - hh - 1 < h_len:\n                    hh_ = imgH - h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, w_len - w_overlap):\n                    if imgW - ww - 1 < w_len:\n                        ww_ = imgW - w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + h_len), ww_:(ww_ + w_len), :]\n\n                    det_boxes_, det_scores_, det_category_ = \\\n                        sess.run(\n                            [det_boxes, det_scores, det_category],\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\n                        )\n\n                    if len(det_boxes_) > 0:\n                        for ii in range(len(det_boxes_)):\n                            box_rotate = det_boxes_[ii]\n                            box_rotate[0] = box_rotate[0] + ww_\n                            box_rotate[1] = box_rotate[1] + hh_\n                            box_res_rotate.append(box_rotate)\n                            label_res_rotate.append(det_category_[ii])\n                            score_res_rotate.append(det_scores_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.2,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.2, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.1, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM+1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEl_NAME_MAP[sub_class]],\n                                                    max_output_size=500)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEl_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n            time_elapsed = timer() - start\n\n            if show_res:\n                det_detections = draw_box_in_img.draw_rotate_box_cv(np.array(img, np.float32) - np.array(cfgs.PIXEL_MEAN),\n                                                                    boxes=np.array(box_res_rotate_),\n                                                                    labels=np.array(label_res_rotate_),\n                                                                    scores=np.array(score_res_rotate_))\n                save_dir = os.path.join(des_folder, cfgs.VERSION)\n                tools.mkdir(save_dir)\n                cv2.imwrite(save_dir + \'/\' + img_path.split(\'/\')[-1].split(\'.\')[0] + \'_r.jpg\',\n                            det_detections)\n            else:\n                # eval txt\n                CLASS_DOTA = NAME_LABEL_MAP.keys()\n                write_handle = {}\n                txt_dir = os.path.join(\'txt_output\', cfgs.VERSION)\n                tools.mkdir(txt_dir)\n                for sub_class in CLASS_DOTA:\n                    if sub_class == \'back_ground\':\n                        continue\n                    write_handle[sub_class] = open(os.path.join(txt_dir, \'Task1_%s.txt\' % sub_class), \'a+\')\n\n                rboxes = coordinate_convert.forward_convert(box_res_rotate_, with_label=False)\n\n                for i, rbox in enumerate(rboxes):\n                    command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (img_path.split(\'/\')[-1].split(\'.\')[0],\n                                                                                     score_res_rotate_[i],\n                                                                                     rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                     rbox[4], rbox[5], rbox[6], rbox[7],)\n                    write_handle[LABEl_NAME_MAP[label_res_rotate_[i]]].write(command)\n\n                for sub_class in CLASS_DOTA:\n                    if sub_class == \'back_ground\':\n                        continue\n                    write_handle[sub_class].close()\n\n                view_bar(\'{} cost {}s\'.format(img_path.split(\'/\')[-1].split(\'.\')[0],\n                                              time_elapsed), count + 1, len(file_paths))\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a RRPN network\')\n    parser.add_argument(\'--src_folder\', dest=\'src_folder\',\n                        help=\'images path\',\n                        default=None, type=str)\n    parser.add_argument(\'--des_folder\', dest=\'des_folder\',\n                        help=\'output path\',\n                        default=None, type=str)\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=800, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=800, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=0, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=0, type=int)\n    parser.add_argument(\'--image_ext\', dest=\'image_ext\',\n                        help=\'image format\',\n                        default=\'.png\', type=str)\n    parser.add_argument(\'--gpu\', dest=\'gpu\',\n                        help=\'gpu index\',\n                        default=\'0\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n\n    # file_paths = get_file_paths_recursive(\'/root/userfolder/DOTA/test/\', \'.png\')\n\n    # det_net = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n    #                                                is_training=False)\n\n    inference(det_net, args.des_folder, args.h_len, args.w_len,\n              args.h_overlap, args.w_overlap, False)\n    # inference(det_net, file_paths, \'/root/userfolder/yx/R2CNN_Attention/tools/demo/\', 800, 800,\n    #           200, 200, False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/eval.py,8,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nsys.path.append(""../"")\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport argparse\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom libs.val_libs import voc_eval_r\nfrom libs.box_utils import draw_box_in_img\nfrom libs.label_name_dict.pascal_dict import LABEl_NAME_MAP, NAME_LABEL_MAP\nfrom libs.box_utils.coordinate_convert import forward_convert, back_forward_convert\nfrom help_utils import tools\n\n\ndef eval_with_plac(img_dir, det_net, num_imgs, image_ext, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN)\n\n    det_boxes_r, det_scores_r, det_category_r = det_net.build_whole_detection_network(input_img_batch=img_batch,\n                                                                                      gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        all_boxes_r = []\n        imgs = os.listdir(img_dir)\n        for i, a_img_name in enumerate(imgs):\n            a_img_name = a_img_name.split(image_ext)[0]\n\n            raw_img = cv2.imread(os.path.join(img_dir,\n                                              a_img_name + image_ext))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                sess.run(\n                    [img_batch, det_boxes_r, det_scores_r, det_category_r],\n                    feed_dict={img_plac: raw_img})\n            end = time.time()\n            # print(""{} cost time : {} "".format(img_name, (end - start)))\n            if draw_imgs:\n\n                det_detections_r = draw_box_in_img.draw_rotate_box_cv(np.squeeze(resized_img, 0),\n                                                                      boxes=det_boxes_r_,\n                                                                      labels=det_category_r_,\n                                                                      scores=det_scores_r_)\n                save_dir = os.path.join(cfgs.TEST_SAVE_PATH, cfgs.VERSION)\n                tools.mkdir(save_dir)\n\n                cv2.imwrite(save_dir + \'/\' + a_img_name + \'_r.jpg\',\n                            det_detections_r[:, :, ::-1])\n\n            if det_boxes_r_.shape[0] != 0:\n                resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                det_boxes_r_[:, 0::2] *= (raw_w / resized_w)\n                det_boxes_r_[:, 1::2] *= (raw_h / resized_h)\n                det_boxes_r_ = back_forward_convert(det_boxes_r_, False)\n\n            x_c, y_c, w, h, theta = det_boxes_r_[:, 0], det_boxes_r_[:, 1], det_boxes_r_[:, 2], \\\n                                    det_boxes_r_[:, 3], det_boxes_r_[:, 4]\n\n            boxes_r = np.transpose(np.stack([x_c, y_c, w, h, theta]))\n\n            dets_r = np.hstack((det_category_r_.reshape(-1, 1),\n                                det_scores_r_.reshape(-1, 1),\n                                boxes_r))\n            all_boxes_r.append(dets_r)\n\n            tools.view_bar(\'{} image cost {}s\'.format(a_img_name, (end - start)), i + 1, len(imgs))\n\n        fw2 = open(cfgs.VERSION + \'_detections_r.pkl\', \'w\')\n        pickle.dump(all_boxes_r, fw2)\n\n\ndef eval(num_imgs, img_dir, image_ext, test_annotation_path):\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=False)\n    eval_with_plac(img_dir=img_dir, det_net=faster_rcnn, num_imgs=num_imgs, image_ext=image_ext, draw_imgs=False)\n\n    with open(cfgs.VERSION + \'_detections_r.pkl\') as f2:\n        all_boxes_r = pickle.load(f2)\n\n        print(len(all_boxes_r))\n\n    imgs = os.listdir(img_dir)\n    real_test_imgname_list = [i.split(image_ext)[0] for i in imgs]\n\n    print(10 * ""**"")\n    print(\'rotation eval:\')\n    voc_eval_r.voc_evaluate_detections(all_boxes=all_boxes_r,\n                                       test_imgid_list=real_test_imgname_list,\n                                       test_annotation_path=test_annotation_path)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a RRPN network\')\n    parser.add_argument(\'--img_dir\', dest=\'img_dir\',\n                        help=\'images path\',\n                        default=\'/mnt/USBB/gx/DOTA/DOTA_clip/val/images/\', type=str)\n    parser.add_argument(\'--image_ext\', dest=\'image_ext\',\n                        help=\'image format\',\n                        default=\'.png\', type=str)\n    parser.add_argument(\'--test_annotation_path\', dest=\'test_annotation_path\',\n                        help=\'test annotate path\',\n                        default=cfgs.TEST_ANNOTATION_PATH, type=str)\n    parser.add_argument(\'--gpu\', dest=\'gpu\',\n                        help=\'gpu index\',\n                        default=\'0\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    eval(np.inf, args.img_dir, args.image_ext, args.test_annotation_path)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/inference.py,8,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nsys.path.append(""../"")\nimport tensorflow as tf\nimport time\nimport cv2\nimport numpy as np\nimport argparse\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom help_utils.tools import *\nfrom libs.box_utils import draw_box_in_img\nfrom help_utils import tools\n\n\ndef inference(det_net, data_dir):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN)\n\n    det_boxes_r, det_scores_r, det_category_r = det_net.build_whole_detection_network(input_img_batch=img_batch,\n                                                                                      gtboxes_batch=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        imgs = os.listdir(data_dir)\n        for i, a_img_name in enumerate(imgs):\n\n            # f = open(\'./res_icdar_r/res_{}.txt\'.format(a_img_name.split(\'.jpg\')[0]), \'w\')\n\n            raw_img = cv2.imread(os.path.join(data_dir,\n                                              a_img_name))\n            # raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            start = time.time()\n            resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                sess.run(\n                    [img_batch, det_boxes_r, det_scores_r, det_category_r],\n                    feed_dict={img_plac: raw_img}\n                )\n            end = time.time()\n\n            # res_r = coordinate_convert.forward_convert(det_boxes_r_, False)\n            # res_r = np.array(res_r, np.int32)\n            # for r in res_r:\n            #     f.write(\'{},{},{},{},{},{},{},{}\\n\'.format(r[0], r[1], r[2], r[3],\n            #                                                r[4], r[5], r[6], r[7]))\n            # f.close()\n\n            det_detections_r = draw_box_in_img.draw_rotate_box_cv(np.squeeze(resized_img, 0),\n                                                                  boxes=det_boxes_r_,\n                                                                  labels=det_category_r_,\n                                                                  scores=det_scores_r_)\n            save_dir = os.path.join(cfgs.INFERENCE_SAVE_PATH, cfgs.VERSION)\n            tools.mkdir(save_dir)\n            cv2.imwrite(save_dir + \'/\' + a_img_name + \'_r.jpg\',\n                        det_detections_r)\n            view_bar(\'{} cost {}s\'.format(a_img_name, (end - start)), i + 1, len(imgs))\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a Fast RRPN network\')\n    parser.add_argument(\'--data_dir\', dest=\'data_dir\',\n                        help=\'data path\',\n                        default=\'/mnt/USBC/gx/Detection/icdar2015/ch4_test_images/\', type=str)\n    parser.add_argument(\'--gpu\', dest=\'gpu\',\n                        help=\'gpu index\',\n                        default=\'0\', type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    det_net = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                   is_training=False)\n\n    inference(det_net, data_dir=args.data_dir)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train.py,46,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nsys.path.append(""../"")\nimport numpy as np\nimport time\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord import next_batch\nfrom libs.box_utils.show_box_in_tensor import *\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import back_forward_convert\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef tower_loss(scope, img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch):\n    with tf.name_scope(scope):\n        faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                           is_training=True)\n\n        gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 5])\n\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        # list as many types of layers as possible, even if they are not used now\n        with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                             slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                            weights_regularizer=weights_regularizer,\n                            biases_regularizer=biases_regularizer,\n                            biases_initializer=tf.constant_initializer(0.0)):\n            final_bbox, final_scores, final_category, loss_dict = faster_rcnn.build_whole_detection_network(\n                input_img_batch=img_batch,\n                gtboxes_batch=gtboxes_and_label)\n\n        # ----------------------------------------------------------------------------------------------------build loss\n        weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n        rpn_location_loss = loss_dict[\'rpn_loc_loss\']\n        rpn_cls_loss = loss_dict[\'rpn_cls_loss\']\n        rpn_total_loss = rpn_location_loss + rpn_cls_loss\n\n        fastrcnn_cls_loss = loss_dict[\'fastrcnn_cls_loss\']\n        fastrcnn_loc_loss = loss_dict[\'fastrcnn_loc_loss\']\n        fastrcnn_total_loss = fastrcnn_cls_loss + fastrcnn_loc_loss\n\n        total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss\n        # ____________________________________________________________________________________________________build loss\n\n        # ---------------------------------------------------------------------------------------------------add summary\n        tf.summary.scalar(scope, \'/RPN_LOSS/cls_loss\', rpn_cls_loss)\n        tf.summary.scalar(scope, \'/RPN_LOSS/location_loss\', rpn_location_loss)\n        tf.summary.scalar(scope, \'/RPN_LOSS/rpn_total_loss\', rpn_total_loss)\n\n        tf.summary.scalar(scope, \'/FAST_LOSS/fastrcnn_cls_loss\', fastrcnn_cls_loss)\n        tf.summary.scalar(scope, \'/FAST_LOSS/fastrcnn_location_loss\', fastrcnn_loc_loss)\n        tf.summary.scalar(scope, \'/FAST_LOSS/fastrcnn_total_loss\', fastrcnn_total_loss)\n\n        tf.summary.scalar(scope, \'/LOSS/total_loss\', total_loss)\n        tf.summary.scalar(scope, \'/LOSS/regular_weights\', weight_decay_loss)\n\n        with tf.name_scope(\'draw_gtboxes\'):\n            gtboxes_in_img = draw_box_with_color_rotate(img_batch, tf.reshape(gtboxes_and_label, [-1, 6])[:, :-1],\n                                                        text=tf.shape(gtboxes_and_label)[0])\n        if cfgs.ADD_BOX_IN_TENSORBOARD:\n            detections_in_img = draw_boxes_with_categories_and_scores(img_batch=img_batch,\n                                                                      boxes=final_bbox,\n                                                                      labels=final_category,\n                                                                      scores=final_scores)\n            tf.summary.image(scope, \'/Compare/final_detection\', detections_in_img)\n        tf.summary.image(scope, \'/Compare/gtboxes\', gtboxes_in_img)\n\n        return total_loss, faster_rcnn, img_name_batch, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\\\n               fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss\n\n\ndef average_gradients(tower_grads):\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        grads = []\n        for g, _ in grad_and_vars:\n            expended_g = tf.expand_dims(g, 0)\n            grads.append(expended_g)\n\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n        global_step = slim.get_or_create_global_step()\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1])],\n                                         values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100.])\n        tf.summary.scalar(\'lr\', lr)\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                       is_training=True)\n\n        gtboxes_and_label = tf.py_func(back_forward_convert,\n                                       inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                       Tout=tf.float32)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n        img_batch = tf.reshape(img_batch, [1, 720, 1280, 3])\n\n        batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n            [img_name_batch, img_batch, gtboxes_and_label, num_objects_batch], capacity=2 * len(cfgs.GPU_GROUP))\n\n        tower_grads = []\n        for i in range(len(cfgs.GPU_GROUP)):\n            with tf.device(\'/gpu:%d\' % i):\n                with tf.name_scope(\'Gpu%d\' % i) as scope:\n\n                    img_name_, img_, gtboxes_and_label_, num_objects_ = batch_queue.dequeue()\n\n                    loss, faster_rcnn, img_name_batch, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\\\n                    fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss = \\\n                        tower_loss(scope, img_name_, img_, gtboxes_and_label_, num_objects_)\n                    tf.get_variable_scope().reuse_variables()\n                    grads = optimizer.compute_gradients(loss)\n\n                    if cfgs.MUTILPY_BIAS_GRADIENT:\n                        grads = faster_rcnn.enlarge_gradients_for_bias(grads)\n\n                    if cfgs.GRADIENT_CLIPPING_BY_NORM:\n                        with tf.name_scope(\'clip_gradients\'):\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                    tower_grads.append(grads)\n        grads = average_gradients(tower_grads)\n        train_op = optimizer.apply_gradients(grads, global_step)\n\n        summary_op = tf.summary.merge_all()\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        restorer, restore_ckpt = faster_rcnn.get_restorer()\n        saver = tf.train.Saver(max_to_keep=10)\n\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess, coord)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            for step in range(cfgs.MAX_ITERATION):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, img_name, rpnLocLoss, rpnClsLoss, rpnTotalLoss, \\\n                        fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss = \\\n                            sess.run(\n                                [train_op, global_step, img_name_batch, rpn_location_loss, rpn_cls_loss, rpn_total_loss,\n                                 fastrcnn_loc_loss, fastrcnn_cls_loss, fastrcnn_total_loss, loss])\n\n                        end = time.time()\n                        print("""""" {}: step{}    image_name:{} |\\t\n                                  rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t rpn_total_loss:{} |\n                                  fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t fast_rcnn_total_loss:{} |\n                                  total_loss:{} |\\t per_cost_time:{}s"""""" \\\n                              .format(training_time, global_stepnp, str(img_name[0]), rpnLocLoss, rpnClsLoss,\n                                      rpnTotalLoss, fastrcnnLocLoss, fastrcnnClsLoss, fastrcnnTotalLoss, totalLoss,\n                                      (end - start)))\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, global_stepnp)\n                            summary_writer.flush()\n\n                if (step > 0 and step % cfgs.SAVE_WEIGHTS_INTE == 0) or (step == cfgs.MAX_ITERATION - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'voc_\' + str(global_stepnp) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/train.py,36,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os, sys\nsys.path.append(""../"")\nimport time\n\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord import next_batch\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import back_forward_convert\nfrom libs.box_utils.show_box_in_tensor import *\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef train():\n\n    faster_rcnn = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                       is_training=True)\n\n    with tf.name_scope(\'get_batch\'):\n        img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n            next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                       batch_size=cfgs.BATCH_SIZE,\n                       shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                       is_training=True)\n\n        gtboxes_and_label = tf.py_func(back_forward_convert,\n                                       inp=[tf.squeeze(gtboxes_and_label_batch, 0)],\n                                       Tout=tf.float32)\n        gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 6])\n\n    with tf.name_scope(\'draw_gtboxes\'):\n\n        gtboxes_in_img = draw_box_with_color_rotate(img_batch, tf.reshape(gtboxes_and_label, [-1, 6])[:, :-1],\n                                                    text=tf.shape(gtboxes_and_label)[0])\n\n    biases_regularizer = tf.no_regularizer\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                         slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                        weights_regularizer=weights_regularizer,\n                        biases_regularizer=biases_regularizer,\n                        biases_initializer=tf.constant_initializer(0.0)):\n        final_boxes, final_scores, final_category, loss_dict = \\\n            faster_rcnn.build_whole_detection_network(input_img_batch=img_batch,\n                                                      gtboxes_batch=gtboxes_and_label)\n\n    dets_in_img = draw_boxes_with_categories_and_scores_rotate(img_batch=img_batch,\n                                                               boxes=final_boxes,\n                                                               labels=final_category,\n                                                               scores=final_scores)\n\n    # ----------------------------------------------------------------------------------------------------build loss\n    weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n    rpn_location_loss = loss_dict[\'rpn_loc_loss\']\n    rpn_cls_loss = loss_dict[\'rpn_cls_loss\']\n    rpn_total_loss = rpn_location_loss + rpn_cls_loss\n\n    fastrcnn_cls_loss = loss_dict[\'fastrcnn_cls_loss\']\n    fastrcnn_loc_loss = loss_dict[\'fastrcnn_loc_loss\']\n    fastrcnn_total_loss = fastrcnn_cls_loss + fastrcnn_loc_loss\n\n    total_loss = rpn_total_loss + fastrcnn_total_loss + weight_decay_loss\n    # ____________________________________________________________________________________________________build loss\n\n    # ---------------------------------------------------------------------------------------------------add summary\n    tf.summary.scalar(\'RPN_LOSS/cls_loss\', rpn_cls_loss)\n    tf.summary.scalar(\'RPN_LOSS/location_loss\', rpn_location_loss)\n    tf.summary.scalar(\'RPN_LOSS/rpn_total_loss\', rpn_total_loss)\n\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_cls_loss\', fastrcnn_cls_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_location_loss\', fastrcnn_loc_loss)\n    tf.summary.scalar(\'FAST_LOSS/fastrcnn_total_loss\', fastrcnn_total_loss)\n\n    tf.summary.scalar(\'LOSS/total_loss\', total_loss)\n    tf.summary.scalar(\'LOSS/regular_weights\', weight_decay_loss)\n\n    tf.summary.image(\'img/gtboxes\', gtboxes_in_img)\n    tf.summary.image(\'img/dets\', dets_in_img)\n\n    # ___________________________________________________________________________________________________add summary\n\n    global_step = slim.get_or_create_global_step()\n    lr = tf.train.piecewise_constant(global_step,\n                                     boundaries=[np.int64(cfgs.DECAY_STEP[0]), np.int64(cfgs.DECAY_STEP[1])],\n                                     values=[cfgs.LR, cfgs.LR / 10., cfgs.LR / 100.])\n    tf.summary.scalar(\'lr\', lr)\n    optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n\n    # ---------------------------------------------------------------------------------------------compute gradients\n    gradients = faster_rcnn.get_gradients(optimizer, total_loss)\n\n    # enlarge_gradients for bias\n    if cfgs.MUTILPY_BIAS_GRADIENT:\n        gradients = faster_rcnn.enlarge_gradients_for_bias(gradients)\n\n    if cfgs.GRADIENT_CLIPPING_BY_NORM:\n        with tf.name_scope(\'clip_gradients\'):\n            gradients = slim.learning.clip_gradient_norms(gradients,\n                                                          cfgs.GRADIENT_CLIPPING_BY_NORM)\n    # _____________________________________________________________________________________________compute gradients\n\n    # train_op\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients,\n                                         global_step=global_step)\n    summary_op = tf.summary.merge_all()\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = faster_rcnn.get_restorer()\n    saver = tf.train.Saver(max_to_keep=10)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n        tools.mkdir(summary_path)\n        summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n        for step in range(cfgs.MAX_ITERATION):\n            training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n            if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                _, global_stepnp = sess.run([train_op, global_step])\n\n            else:\n                if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                    start = time.time()\n\n                    _global_step, _img_name_batch, _rpn_location_loss, _rpn_classification_loss, \\\n                    _rpn_total_loss, _fast_rcnn_location_loss, _fast_rcnn_classification_loss, \\\n                    _fast_rcnn_total_loss, _total_loss, _ = \\\n                        sess.run([global_step, img_name_batch, rpn_location_loss, rpn_cls_loss,\n                                  rpn_total_loss, fastrcnn_loc_loss, fastrcnn_cls_loss,\n                                  fastrcnn_total_loss, total_loss, train_op])\n\n                    # final_boxes_r, _final_scores_r, _final_category_r = sess.run([final_boxes_r, final_scores_r, final_category_r])\n                    # print(\'*\'*100)\n                    # print(_final_boxes_r)\n                    # print(_final_scores_r)\n                    # print(_final_category_r)\n\n                    end = time.time()\n                    print("""""" {}: step{}    image_name:{} |\\t\n                                                    rpn_loc_loss:{} |\\t rpn_cla_loss:{} |\\t\n                                                    rpn_total_loss:{} |\n                                                    fast_rcnn_loc_loss:{} |\\t fast_rcnn_cla_loss:{} |\\t\n                                                    fast_rcnn_total_loss:{} |\\t\n                                                    total_loss:{} |\\t pre_cost_time:{}s"""""" \\\n                          .format(training_time, _global_step, str(_img_name_batch[0]), _rpn_location_loss,\n                                  _rpn_classification_loss, _rpn_total_loss, _fast_rcnn_location_loss,\n                                  _fast_rcnn_classification_loss, _fast_rcnn_total_loss, _total_loss,\n                                  (end - start)))\n                else:\n                    if step % cfgs.SMRY_ITER == 0:\n                        _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                        summary_writer.add_summary(summary_str, global_stepnp)\n                        summary_writer.flush()\n\n            if (step > 0 and step % cfgs.SAVE_WEIGHTS_INTE == 0) or (step == cfgs.MAX_ITERATION - 1):\n\n                save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                if not os.path.exists(save_dir):\n                    os.makedirs(save_dir)\n\n                save_ckpt = os.path.join(save_dir, \'voc_\' + str(global_stepnp) + \'model.ckpt\')\n                saver.save(sess, save_ckpt)\n                print(\' weights had been saved\')\n\n        coord.request_stop()\n        coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
data/io/__init__.py,0,b''
data/io/convert_data_to_tfrecord.py,15,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport xml.etree.cElementTree as ET\nimport numpy as np\nimport tensorflow as tf\nimport glob\nimport cv2\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils.tools import *\n\ntf.app.flags.DEFINE_string(\'VOC_dir\', \'/mnt/USBB/gx/DOTA/DOTA_TOTAL/\', \'Voc dir\')\ntf.app.flags.DEFINE_string(\'xml_dir\', \'XML\', \'xml dir\')\ntf.app.flags.DEFINE_string(\'image_dir\', \'IMG\', \'image dir\')\ntf.app.flags.DEFINE_string(\'save_name\', \'train\', \'save name\')\ntf.app.flags.DEFINE_string(\'save_dir\', \'../tfrecord/\', \'save name\')\ntf.app.flags.DEFINE_string(\'img_format\', \'.png\', \'format of image\')\ntf.app.flags.DEFINE_string(\'dataset\', \'DOTA_TOTAL\', \'dataset\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 9],\n           and has [x1, y1, x2, y2, x3, y3, x4, y4, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef convert_pascal_to_tfrecord():\n    xml_path = FLAGS.VOC_dir + FLAGS.xml_dir\n    image_path = FLAGS.VOC_dir + FLAGS.image_dir\n    save_path = FLAGS.save_dir + FLAGS.dataset + \'_\' + FLAGS.save_name + \'.tfrecord\'\n    mkdir(FLAGS.save_dir)\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=save_path, options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=save_path)\n    for count, xml in enumerate(glob.glob(xml_path + \'/*.xml\')):\n        # to avoid path error in different development platform\n        xml = xml.replace(\'\\\\\', \'/\')\n\n        img_name = xml.split(\'/\')[-1].split(\'.\')[0] + FLAGS.img_format\n        img_path = image_path + \'/\' + img_name\n\n        if not os.path.exists(img_path):\n            print(\'{} is not exist!\'.format(img_path))\n            continue\n\n        img_height, img_width, gtbox_label = read_xml_gtbox_and_label(xml)\n\n        # img = np.array(Image.open(img_path))\n        img = cv2.imread(img_path)\n\n        feature = tf.train.Features(feature={\n            # do not need encode() in linux\n            # \'img_name\': _bytes_feature(img_name.encode()),\n            \'img_name\': _bytes_feature(img_name),\n            \'img_height\': _int64_feature(img_height),\n            \'img_width\': _int64_feature(img_width),\n            \'img\': _bytes_feature(img.tostring()),\n            \'gtboxes_and_label\': _bytes_feature(gtbox_label.tostring()),\n            \'num_objects\': _int64_feature(gtbox_label.shape[0])\n        })\n\n        example = tf.train.Example(features=feature)\n\n        writer.write(example.SerializeToString())\n\n        view_bar(\'Conversion progress\', count + 1, len(glob.glob(xml_path + \'/*.xml\')))\n\n    print(\'\\nConversion is complete!\')\n\n\nif __name__ == \'__main__\':\n    # xml_path = \'../data/dataset/VOCdevkit/VOC2007/Annotations/000005.xml\'\n    # read_xml_gtbox_and_label(xml_path)\n\n    convert_pascal_to_tfrecord()\n'"
data/io/image_preprocess.py,16,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 9]\n    :param target_shortside_len:\n    :return:\n    '''\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    new_h, new_w = tf.cond(tf.less(h, w),\n                           true_fn=lambda: (target_shortside_len, target_shortside_len * w//h),\n                           false_fn=lambda: (target_shortside_len * h//w,  target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    x1, x2, x3, x4 = x1 * new_w//w, x2 * new_w//w, x3 * new_w//w, x4 * new_w//w\n    y1, y2, y3, y4 = y1 * new_h//h, y2 * new_h//h, y3 * new_h//h, y4 * new_h//h\n\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor, tf.transpose(tf.stack([x1, y1, x2, y2, x3, y3, x4, y4, label], axis=0))\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, is_resize=True):\n    h, w, = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n\n    if is_resize:\n        new_h, new_w = tf.cond(tf.less(h, w),\n                               true_fn=lambda: (target_shortside_len, target_shortside_len*w//h),\n                               false_fn=lambda: (target_shortside_len*h//w, target_shortside_len))\n        img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    return img_tensor  # [1, h, w, c]\n\n\ndef flip_left_right(img_tensor, gtboxes_and_label):\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_x1 = w - x1\n    new_x2 = w - x2\n    new_x3 = w - x3\n    new_x4 = w - x4\n    return img_tensor, tf.transpose(tf.stack([new_x1, y1, new_x2, y2, new_x3, y3, new_x4, y4, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\n"""
data/io/read_tfrecord.py,30,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom data.io import image_preprocess\nfrom libs.configs import cfgs\n\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 9])\n\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n    img = img - tf.constant(cfgs.PIXEL_MEAN)\n    if is_training:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len)\n        img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                         gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len)\n\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n    assert batch_size == 1, ""we only support batch_size is 1.We may support large batch_size in the future""\n\n    if dataset_name not in [\'DOTA\', \'ship\', \'icdar\', \'pascal\', \'coco\', \'DOTA_TOTAL\', \'FDDB\']:\n        raise ValueError(\'dataSet name must be in pascal, coco spacenet and ship\')\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    img_name, img, gtboxes_and_label, num_obs = read_and_prepocess_single_img(filename_queue, shortside_len,\n                                                                              is_training=is_training)\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs],\n                       batch_size=batch_size,\n                       capacity=1,\n                       num_threads=1,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'1\'\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch = \\\n        next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                   batch_size=cfgs.BATCH_SIZE,\n                   shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                   is_training=True)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 9])\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        img_name_batch_, img_batch_, gtboxes_and_label_batch_, num_objects_batch_ \\\n            = sess.run([img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch])\n\n        print(\'debug\')\n\n        coord.request_stop()\n        coord.join(threads)'"
data/io/txt2xml.py,0,"b'import os\nfrom xml.dom.minidom import Document\nfrom xml.dom.minidom import parse\nimport xml.dom.minidom\nimport numpy as np\nimport csv\nimport cv2\n\n\ndef WriterXMLFiles(filename, path, box_list, label_list, w, h, d):\n\n    # dict_box[filename]=json_dict[filename]\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(\'annotation\')\n    doc.appendChild(root)\n\n    foldername = doc.createElement(""folder"")\n    foldername.appendChild(doc.createTextNode(""JPEGImages""))\n    root.appendChild(foldername)\n\n    nodeFilename = doc.createElement(\'filename\')\n    nodeFilename.appendChild(doc.createTextNode(filename))\n    root.appendChild(nodeFilename)\n\n    pathname = doc.createElement(""path"")\n    pathname.appendChild(doc.createTextNode(""xxxx""))\n    root.appendChild(pathname)\n\n    sourcename=doc.createElement(""source"")\n\n    databasename = doc.createElement(""database"")\n    databasename.appendChild(doc.createTextNode(""Unknown""))\n    sourcename.appendChild(databasename)\n\n    annotationname = doc.createElement(""annotation"")\n    annotationname.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(annotationname)\n\n    imagename = doc.createElement(""image"")\n    imagename.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(imagename)\n\n    flickridname = doc.createElement(""flickrid"")\n    flickridname.appendChild(doc.createTextNode(""0""))\n    sourcename.appendChild(flickridname)\n\n    root.appendChild(sourcename)\n\n    nodesize = doc.createElement(\'size\')\n    nodewidth = doc.createElement(\'width\')\n    nodewidth.appendChild(doc.createTextNode(str(w)))\n    nodesize.appendChild(nodewidth)\n    nodeheight = doc.createElement(\'height\')\n    nodeheight.appendChild(doc.createTextNode(str(h)))\n    nodesize.appendChild(nodeheight)\n    nodedepth = doc.createElement(\'depth\')\n    nodedepth.appendChild(doc.createTextNode(str(d)))\n    nodesize.appendChild(nodedepth)\n    root.appendChild(nodesize)\n\n    segname = doc.createElement(""segmented"")\n    segname.appendChild(doc.createTextNode(""0""))\n    root.appendChild(segname)\n\n    for (box, label) in zip(box_list, label_list):\n\n        nodeobject = doc.createElement(\'object\')\n        nodename = doc.createElement(\'name\')\n        nodename.appendChild(doc.createTextNode(str(label)))\n        nodeobject.appendChild(nodename)\n        nodebndbox = doc.createElement(\'bndbox\')\n        nodex1 = doc.createElement(\'x1\')\n        nodex1.appendChild(doc.createTextNode(str(box[0])))\n        nodebndbox.appendChild(nodex1)\n        nodey1 = doc.createElement(\'y1\')\n        nodey1.appendChild(doc.createTextNode(str(box[1])))\n        nodebndbox.appendChild(nodey1)\n        nodex2 = doc.createElement(\'x2\')\n        nodex2.appendChild(doc.createTextNode(str(box[2])))\n        nodebndbox.appendChild(nodex2)\n        nodey2 = doc.createElement(\'y2\')\n        nodey2.appendChild(doc.createTextNode(str(box[3])))\n        nodebndbox.appendChild(nodey2)\n        nodex3 = doc.createElement(\'x3\')\n        nodex3.appendChild(doc.createTextNode(str(box[4])))\n        nodebndbox.appendChild(nodex3)\n        nodey3 = doc.createElement(\'y3\')\n        nodey3.appendChild(doc.createTextNode(str(box[5])))\n        nodebndbox.appendChild(nodey3)\n        nodex4 = doc.createElement(\'x4\')\n        nodex4.appendChild(doc.createTextNode(str(box[6])))\n        nodebndbox.appendChild(nodex4)\n        nodey4 = doc.createElement(\'y4\')\n        nodey4.appendChild(doc.createTextNode(str(box[7])))\n        nodebndbox.appendChild(nodey4)\n\n        # ang = doc.createElement(\'angle\')\n        # ang.appendChild(doc.createTextNode(str(angle)))\n        # nodebndbox.appendChild(ang)\n        nodeobject.appendChild(nodebndbox)\n        root.appendChild(nodeobject)\n    fp = open(path + filename, \'w\')\n    doc.writexml(fp, indent=\'\\n\')\n    fp.close()\n\n\ndef load_annoataion(p):\n    \'\'\'\n    load annotation from the text file\n    :param p:\n    :return:\n    \'\'\'\n    text_polys = []\n    text_tags = []\n    if not os.path.exists(p):\n        return np.array(text_polys, dtype=np.float32)\n    with open(p, \'r\') as f:\n        reader = csv.reader(f)\n        for line in reader:\n            label = \'text\'\n            # strip BOM. \\ufeff for python3,  \\xef\\xbb\\bf for python2\n            line = [i.strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\') for i in line]\n\n            x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))\n            text_polys.append([x1, y1, x2, y2, x3, y3, x4, y4])\n            text_tags.append(label)\n\n        return np.array(text_polys, dtype=np.int32), np.array(text_tags, dtype=np.str)\n\nif __name__ == ""__main__"":\n    txt_path = \'/mnt/USBC/yx/icdar/txts/\'\n    xml_path = \'/mnt/USBC/yx/icdar/Annotations/\'\n    img_path = \'/mnt/USBC/yx/icdar/JPEGImages/\'\n    print(os.path.exists(txt_path))\n    txts = os.listdir(txt_path)\n    for count, t in enumerate(txts):\n        boxes, labels = load_annoataion(os.path.join(txt_path, t))\n        xml_name = t.replace(\'.txt\', \'.xml\')\n        img_name = t.replace(\'.txt\', \'.jpg\')\n        img = cv2.imread(os.path.join(img_path, img_name.split(\'gt_\')[-1]))\n        if img != None:\n            h, w, d = img.shape\n            WriterXMLFiles(xml_name.split(\'gt_\')[-1], xml_path, boxes, labels, w, h, d)\n\n        if count % 1000 == 0:\n            print(count)'"
libs/box_utils/__init__.py,0,b''
libs/box_utils/anchor_utils.py,29,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\nimport numpy as np\n\n# def generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n#                      scales=2 ** np.arange(3, 6)):\n#   """"""\n#   Generate anchor (reference) windows by enumerating aspect ratios X\n#   scales wrt a reference (0, 0, 15, 15) window.\n#   """"""\n#\n#   base_anchor = np.array([1, 1, base_size, base_size]) - 1\n#   ratio_anchors = _ratio_enum(base_anchor, ratios)\n#   anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n#                        for i in range(ratio_anchors.shape[0])])\n#   return anchors\n#\n# def _whctrs(anchor):\n#   """"""\n#   Return width, height, x center, and y center for an anchor (window).\n#   """"""\n#\n#   w = anchor[2] - anchor[0] + 1\n#   h = anchor[3] - anchor[1] + 1\n#   x_ctr = anchor[0] + 0.5 * (w - 1)\n#   y_ctr = anchor[1] + 0.5 * (h - 1)\n#   return w, h, x_ctr, y_ctr\n#\n#\n# def _mkanchors(ws, hs, x_ctr, y_ctr):\n#   """"""\n#   Given a vector of widths (ws) and heights (hs) around a center\n#   (x_ctr, y_ctr), output a set of anchors (windows).\n#   """"""\n#\n#   ws = ws[:, np.newaxis]\n#   hs = hs[:, np.newaxis]\n#   anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n#                        y_ctr - 0.5 * (hs - 1),\n#                        x_ctr + 0.5 * (ws - 1),\n#                        y_ctr + 0.5 * (hs - 1)))\n#   return anchors\n#\n#\n# def _ratio_enum(anchor, ratios):\n#   """"""\n#   Enumerate a set of anchors for each aspect ratio wrt an anchor.\n#   """"""\n#\n#   w, h, x_ctr, y_ctr = _whctrs(anchor)\n#   size = w * h\n#   size_ratios = size / ratios\n#   ws = np.round(np.sqrt(size_ratios))\n#   hs = np.round(ws * ratios)\n#   anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n#   return anchors\n#\n#\n# def _scale_enum(anchor, scales):\n#   """"""\n#   Enumerate a set of anchors for each scale wrt an anchor.\n#   """"""\n#\n#   w, h, x_ctr, y_ctr = _whctrs(anchor)\n#   ws = w * scales\n#   hs = h * scales\n#   anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n#   return anchors\n#\n#\n# def make_anchors(\n#         height, width, feat_stride, anchor_scales=(8, 16, 32),\n#         anchor_ratios=(0.5, 1., 2.), base_size=16):\n#\n#     anchors = generate_anchors(\n#         ratios=np.array(anchor_ratios), scales=np.array(anchor_scales),\n#         base_size=base_size)\n#     shift_x = tf.range(width, dtype=np.float32) * feat_stride\n#     shift_y = tf.range(height, dtype=np.float32) * feat_stride\n#     shift_x, shift_y = tf.meshgrid(shift_x, shift_y)\n#     shifts = tf.stack(\n#         (tf.reshape(shift_x, (-1, 1)), tf.reshape(shift_y, (-1, 1)),\n#          tf.reshape(shift_x, (-1, 1)), tf.reshape(shift_y, (-1, 1))))\n#     shifts = tf.transpose(shifts, [1, 0, 2])\n#     final_anc = tf.constant(anchors.reshape((1, -1, 4)), dtype=np.float32) + \\\n#           tf.transpose(tf.reshape(shifts, (1, -1, 4)), (1, 0, 2))\n#     return tf.reshape(final_anc, (-1, 4))\n\n#\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios,\n                 featuremap_height, featuremap_width,\n                 stride, name=\'make_anchors\'):\n    \'\'\'\n    :param base_anchor_size:256\n    :param anchor_scales:\n    :param anchor_ratios:\n    :param featuremap_height:\n    :param featuremap_width:\n    :param stride:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)  # [x_center, y_center, w, h]\n\n        ws, hs = enum_ratios(enum_scales(base_anchor, anchor_scales),\n                             anchor_ratios)  # per locations ws and hs\n\n        x_centers = tf.range(featuremap_width, dtype=tf.float32) * stride\n        y_centers = tf.range(featuremap_height, dtype=tf.float32) * stride\n\n        x_centers, y_centers = tf.meshgrid(x_centers, y_centers)\n\n        ws, x_centers = tf.meshgrid(ws, x_centers)\n        hs, y_centers = tf.meshgrid(hs, y_centers)\n\n        anchor_centers = tf.stack([x_centers, y_centers], 2)\n        anchor_centers = tf.reshape(anchor_centers, [-1, 2])\n\n        box_sizes = tf.stack([ws, hs], axis=2)\n        box_sizes = tf.reshape(box_sizes, [-1, 2])\n        # anchors = tf.concat([anchor_centers, box_sizes], axis=1)\n        anchors = tf.concat([anchor_centers - 0.5*box_sizes,\n                             anchor_centers + 0.5*box_sizes], axis=1)\n        return anchors\n\n\ndef enum_scales(base_anchor, anchor_scales):\n\n    anchor_scales = base_anchor * tf.constant(anchor_scales, dtype=tf.float32, shape=(len(anchor_scales), 1))\n\n    return anchor_scales\n\n\ndef enum_ratios(anchors, anchor_ratios):\n    \'\'\'\n    ratio = h /w\n    :param anchors:\n    :param anchor_ratios:\n    :return:\n    \'\'\'\n    ws = anchors[:, 2]  # for base anchor: w == h\n    hs = anchors[:, 3]\n    sqrt_ratios = tf.sqrt(tf.constant(anchor_ratios))\n\n    ws = tf.reshape(ws / sqrt_ratios[:, tf.newaxis], [-1, 1])\n    hs = tf.reshape(hs * sqrt_ratios[:, tf.newaxis], [-1, 1])\n\n    return ws, hs\n\n\n# if __name__ == \'__main__\':\n#     import os\n#     os.environ[""CUDA_VISIBLE_DEVICES""] = \'14\'\n#     base_anchor_size = 256\n#     anchor_scales = [0.5, 1., 2.0]\n#     anchor_ratios = [0.5, 2.0, 1.0]\n#     anchors = make_anchors(base_anchor_size=base_anchor_size, anchor_ratios=anchor_ratios,\n#                            anchor_scales=anchor_scales,\n#                            featuremap_width=32,\n#                            featuremap_height=63,\n#                            stride=16)\n#     init = tf.global_variables_initializer()\n#     with tf.Session() as sess:\n#         sess.run(init)\n#         anchor_result = sess.run(anchors)\n#         print (anchor_result.shape)\n'"
libs/box_utils/boxes_utils.py,56,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom libs.box_utils.coordinate_convert import forward_convert\n\n\ndef ious_calu(boxes_1, boxes_2):\n    '''\n\n    :param boxes_1: [N, 4] [xmin, ymin, xmax, ymax]\n    :param boxes_2: [M, 4] [xmin, ymin. xmax, ymax]\n    :return:\n    '''\n    boxes_1 = tf.cast(boxes_1, tf.float32)\n    boxes_2 = tf.cast(boxes_2, tf.float32)\n    xmin_1, ymin_1, xmax_1, ymax_1 = tf.split(boxes_1, 4, axis=1)  # xmin_1 shape is [N, 1]..\n    xmin_2, ymin_2, xmax_2, ymax_2 = tf.unstack(boxes_2, axis=1)  # xmin_2 shape is [M, ]..\n\n    max_xmin = tf.maximum(xmin_1, xmin_2)\n    min_xmax = tf.minimum(xmax_1, xmax_2)\n\n    max_ymin = tf.maximum(ymin_1, ymin_2)\n    min_ymax = tf.minimum(ymax_1, ymax_2)\n\n    overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n    overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n    overlaps = overlap_h * overlap_w\n\n    area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n    area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n    ious = overlaps / (area_1 + area_2 - overlaps)\n\n    return ious\n\n\ndef clip_boxes_to_img_boundaries(decode_boxes, img_shape):\n    '''\n\n    :param decode_boxes:\n    :return: decode boxes, and already clip to boundaries\n    '''\n\n    with tf.name_scope('clip_boxes_to_img_boundaries'):\n\n        # xmin, ymin, xmax, ymax = tf.unstack(decode_boxes, axis=1)\n        xmin = decode_boxes[:, 0]\n        ymin = decode_boxes[:, 1]\n        xmax = decode_boxes[:, 2]\n        ymax = decode_boxes[:, 3]\n        img_h, img_w = img_shape[1], img_shape[2]\n\n        img_h, img_w = tf.cast(img_h, tf.float32), tf.cast(img_w, tf.float32)\n\n        xmin = tf.maximum(tf.minimum(xmin, img_w-1.), 0.)\n        ymin = tf.maximum(tf.minimum(ymin, img_h-1.), 0.)\n\n        xmax = tf.maximum(tf.minimum(xmax, img_w-1.), 0.)\n        ymax = tf.maximum(tf.minimum(ymax, img_h-1.), 0.)\n\n        return tf.transpose(tf.stack([xmin, ymin, xmax, ymax]))\n\n\ndef filter_outside_boxes(boxes, img_h, img_w):\n    '''\n    :param anchors:boxes with format [xmin, ymin, xmax, ymax]\n    :param img_h: height of image\n    :param img_w: width of image\n    :return: indices of anchors that inside the image boundary\n    '''\n\n    with tf.name_scope('filter_outside_boxes'):\n        xmin, ymin, xmax, ymax = tf.unstack(boxes, axis=1)\n\n        xmin_index = tf.greater_equal(xmin, 0)\n        ymin_index = tf.greater_equal(ymin, 0)\n        xmax_index = tf.less_equal(xmax, tf.cast(img_w, tf.float32))\n        ymax_index = tf.less_equal(ymax, tf.cast(img_h, tf.float32))\n\n        indices = tf.transpose(tf.stack([xmin_index, ymin_index, xmax_index, ymax_index]))\n        indices = tf.cast(indices, dtype=tf.int32)\n        indices = tf.reduce_sum(indices, axis=1)\n        indices = tf.where(tf.equal(indices, 4))\n        # indices = tf.equal(indices, 4)\n        return tf.reshape(indices, [-1])\n\n\ndef padd_boxes_with_zeros(boxes, scores, max_num_of_boxes):\n\n    '''\n    num of boxes less than max num of boxes, so it need to pad with zeros[0, 0, 0, 0]\n    :param boxes:\n    :param scores: [-1]\n    :param max_num_of_boxes:\n    :return:\n    '''\n\n    pad_num = tf.cast(max_num_of_boxes, tf.int32) - tf.shape(boxes)[0]\n\n    zero_boxes = tf.zeros(shape=[pad_num, 4], dtype=boxes.dtype)\n    zero_scores = tf.zeros(shape=[pad_num], dtype=scores.dtype)\n\n    final_boxes = tf.concat([boxes, zero_boxes], axis=0)\n\n    final_scores = tf.concat([scores, zero_scores], axis=0)\n\n    return final_boxes, final_scores\n\n\ndef get_horizen_minAreaRectangle(boxs, with_label=True):\n\n    rpn_proposals_boxes_convert = tf.py_func(forward_convert,\n                                             inp=[boxs, with_label],\n                                             Tout=tf.float32)\n    if with_label:\n        rpn_proposals_boxes_convert = tf.reshape(rpn_proposals_boxes_convert, [-1, 9])\n\n        boxes_shape = tf.shape(rpn_proposals_boxes_convert)\n        x_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n        y_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n\n        label = tf.unstack(rpn_proposals_boxes_convert, axis=1)[-1]\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n        return tf.transpose(tf.stack([x_min, y_min, x_max, y_max, label], axis=0))\n    else:\n        rpn_proposals_boxes_convert = tf.reshape(rpn_proposals_boxes_convert, [-1, 8])\n\n        boxes_shape = tf.shape(rpn_proposals_boxes_convert)\n        x_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n        y_list = tf.strided_slice(rpn_proposals_boxes_convert, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n\n    return tf.transpose(tf.stack([x_min, y_min, x_max, y_max], axis=0))"""
libs/box_utils/coordinate_convert.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\n\n\ndef forward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [x_c, y_c, w, h, theta]\n    :return: format [x1, y1, x2, y2, x3, y3, x4, y4]\n    """"""\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[0], rect[1]), (rect[2], rect[3]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7], rect[5]])\n    else:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[0], rect[1]), (rect[2], rect[3]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]])\n\n    return np.array(boxes, dtype=np.float32)\n\n\ndef back_forward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [x1, y1, x2, y2, x3, y3, x4, y4, (label)] \n    :param with_label: default True\n    :return: format [x_c, y_c, w, h, theta, (label)]\n    """"""\n\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = np.int0(rect[:-1])\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([x, y, w, h, theta, rect[-1]])\n\n    else:\n        for rect in coordinate:\n            box = np.int0(rect)\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([x, y, w, h, theta])\n\n    return np.array(boxes, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    coord = np.array([[150, 150, 50, 100, -90, 1],\n                      [150, 150, 100, 50, -90, 1],\n                      [150, 150, 50, 100, -45, 1],\n                      [150, 150, 100, 50, -45, 1]])\n\n    coord1 = np.array([[150, 150, 100, 50, 0],\n                      [150, 150, 100, 50, -90],\n                      [150, 150, 100, 50, 45],\n                      [150, 150, 100, 50, -45]])\n\n    coord2 = forward_convert(coord)\n    # coord3 = forward_convert(coord1, mode=-1)\n    print(coord2)\n    # print(coord3-coord2)\n    # coord_label = np.array([[167., 203., 96., 132., 132., 96., 203., 167., 1.]])\n    #\n    # coord4 = back_forward_convert(coord_label, mode=1)\n    # coord5 = back_forward_convert(coord_label)\n\n    # print(coord4)\n    # print(coord5)\n\n    # coord3 = coordinate_present_convert(coord, -1)\n    # print(coord3)\n    # coord4 = coordinate_present_convert(coord3, mode=1)\n    # print(coord4)\n\n'"
libs/box_utils/draw_box_in_img.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\n\nfrom libs.configs import cfgs\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\nNOT_DRAW_BOXES = 0\nONLY_DRAW_BOXES = -1\nONLY_DRAW_BOXES_WITH_SCORES = -2\n\nSTANDARD_COLORS = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\', \'LightBlue\', \'LightGreen\'\n]\nFONT = ImageFont.load_default()\n\n\ndef draw_a_rectangel_in_img(draw_obj, box, color, width):\n    \'\'\'\n    use draw lines to draw rectangle. since the draw_rectangle func can not modify the width of rectangle\n    :param draw_obj:\n    :param box: [x1, y1, x2, y2]\n    :return:\n    \'\'\'\n    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n    top_left, top_right = (x1, y1), (x2, y1)\n    bottom_left, bottom_right = (x1, y2), (x2, y2)\n\n    draw_obj.line(xy=[top_left, top_right],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[top_left, bottom_left],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[bottom_left, bottom_right],\n                  fill=color,\n                  width=width)\n    draw_obj.line(xy=[top_right, bottom_right],\n                  fill=color,\n                  width=width)\n\n\ndef only_draw_scores(draw_obj, box, score, color):\n\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x+60, y+10],\n                       fill=color)\n    draw_obj.text(xy=(x, y),\n                  text=""obj:"" +str(round(score, 2)),\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_label_with_scores(draw_obj, box, label, score, color):\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x + 60, y + 10],\n                       fill=color)\n\n    txt = LABEl_NAME_MAP[label] + \':\' + str(round(score, 2))\n    draw_obj.text(xy=(x, y),\n                  text=txt,\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_boxes_with_label_and_scores(img_array, boxes, labels, scores):\n\n    img_array = img_array + np.array(cfgs.PIXEL_MEAN)\n    img_array.astype(np.float32)\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img_array = np.array(img_array * 255 / np.max(img_array), dtype=np.uint8)\n\n    img_obj = Image.fromarray(img_array)\n    raw_img_obj = img_obj.copy()\n\n    draw_obj = ImageDraw.Draw(img_obj)\n    num_of_objs = 0\n    for box, a_label, a_score in zip(boxes, labels, scores):\n\n        if a_label != NOT_DRAW_BOXES:\n            num_of_objs += 1\n            draw_a_rectangel_in_img(draw_obj, box, color=STANDARD_COLORS[a_label], width=3)\n            if a_label == ONLY_DRAW_BOXES:  # -1\n                continue\n            elif a_label == ONLY_DRAW_BOXES_WITH_SCORES:  # -2\n                 only_draw_scores(draw_obj, box, a_score, color=\'White\')\n                 continue\n            else:\n                draw_label_with_scores(draw_obj, box, a_label, a_score, color=\'White\')\n\n    out_img_obj = Image.blend(raw_img_obj, img_obj, alpha=0.7)\n\n    return np.array(out_img_obj)\n\n\ndef draw_box_cv(img, boxes, labels, scores):\n    img = img + np.array(cfgs.PIXEL_MEAN)\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img = np.array(img, np.float32)\n    img = np.array(img*255/np.max(img), np.uint8)\n\n    num_of_object = 0\n    for i, box in enumerate(boxes):\n        xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n\n        label = labels[i]\n        if label != 0:\n            num_of_object += 1\n            # color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            color = (0, 255, 0)\n            cv2.rectangle(img,\n                          pt1=(xmin, ymin),\n                          pt2=(xmax, ymax),\n                          color=color,\n                          thickness=2)\n\n            category = LABEl_NAME_MAP[label]\n\n            # if scores is not None:\n            #     cv2.rectangle(img,\n            #                   pt1=(xmin, ymin),\n            #                   pt2=(xmin+150, ymin+15),\n            #                   color=color,\n            #                   thickness=-1)\n            #     cv2.putText(img,\n            #                 text=category+"": ""+str(scores[i]),\n            #                 org=(xmin, ymin+10),\n            #                 fontFace=1,\n            #                 fontScale=1,\n            #                 thickness=2,\n            #                 color=(color[1], color[2], color[0]))\n            # else:\n            #     cv2.rectangle(img,\n            #                   pt1=(xmin, ymin),\n            #                   pt2=(xmin + 40, ymin + 15),\n            #                   color=color,\n            #                   thickness=-1)\n            #     cv2.putText(img,\n            #                 text=category,\n            #                 org=(xmin, ymin + 10),\n            #                 fontFace=1,\n            #                 fontScale=1,\n            #                 thickness=2,\n            #                 color=(color[1], color[2], color[0]))\n    cv2.putText(img,\n                text=str(num_of_object),\n                org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                fontFace=3,\n                fontScale=1,\n                color=(255, 0, 0))\n    return img\n\n\ndef draw_rotate_box_cv(img, boxes, labels, scores):\n    img = img + np.array(cfgs.PIXEL_MEAN)\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img = np.array(img, np.float32)\n    img = np.array(img*255/np.max(img), np.uint8)\n\n    num_of_object = 0\n    for i, box in enumerate(boxes):\n        x_c, y_c, w, h, theta = box[0], box[1], box[2], box[3], box[4]\n\n        label = labels[i]\n        if label != 0:\n            num_of_object += 1\n            # color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            color = (0, 255, 0)\n            rect = ((x_c, y_c), (w, h), theta)\n            rect = cv2.boxPoints(rect)\n            rect = np.int0(rect)\n            cv2.drawContours(img, [rect], -1, color, 2)\n\n            category = LABEl_NAME_MAP[label]\n\n            if scores is not None:\n                cv2.rectangle(img,\n                              pt1=(x_c, y_c),\n                              pt2=(x_c + 120, y_c + 15),\n                              color=color,\n                              thickness=-1)\n                cv2.putText(img,\n                            text=category+"": ""+str(scores[i]),\n                            org=(x_c, y_c+10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n            else:\n                cv2.rectangle(img,\n                              pt1=(x_c, y_c),\n                              pt2=(x_c + 40, y_c + 15),\n                              color=color,\n                              thickness=-1)\n                cv2.putText(img,\n                            text=category,\n                            org=(x_c, y_c + 10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n    cv2.putText(img,\n                text=str(num_of_object),\n                org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                fontFace=3,\n                fontScale=1,\n                color=(255, 0, 0))\n    return img\n\n\nif __name__ == \'__main__\':\n    img_array = cv2.imread(""/home/yjr/PycharmProjects/FPN_TF/tools/inference_image/2.jpg"")\n    img_array = np.array(img_array, np.float32) - np.array(cfgs.PIXEL_MEAN)\n    boxes = np.array(\n        [[200, 200, 500, 500],\n         [300, 300, 400, 400],\n         [200, 200, 400, 400]]\n    )\n\n    # test only draw boxes\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES\n    scores = np.zeros_like(labes)\n    imm = draw_boxes_with_label_and_scores(img_array, boxes, labes ,scores)\n    # imm = np.array(imm)\n\n    cv2.imshow(""te"", imm)\n\n    # test only draw scores\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES_WITH_SCORES\n    scores = np.random.rand((len(boxes))) * 10\n    imm2 = draw_boxes_with_label_and_scores(img_array, boxes, labes, scores)\n\n    cv2.imshow(""te2"", imm2)\n    # test draw label and scores\n\n    labels = np.arange(1, 4)\n    imm3 = draw_boxes_with_label_and_scores(img_array, boxes, labels, scores)\n    cv2.imshow(""te3"", imm3)\n\n    cv2.waitKey(0)\n\n\n\n\n\n\n\n'"
libs/box_utils/encode_and_decode.py,30,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\nimport math\n\n\ndef decode_boxes_rotate(encode_boxes, reference_boxes, scale_factors=None):\n    '''\n\n    :param encode_boxes:[N, 5]\n    :param reference_boxes: [N, 5] .\n    :param scale_factors: use for scale\n    in the rpn stage, reference_boxes are anchors\n    in the fast_rcnn stage, reference boxes are proposals(decode) produced by rpn stage\n    :return:decode boxes [N, 5]\n    '''\n\n    t_xcenter, t_ycenter, t_w, t_h, t_theta = tf.unstack(encode_boxes, axis=1)\n    if scale_factors:\n        t_xcenter /= scale_factors[0]\n        t_ycenter /= scale_factors[1]\n        t_w /= scale_factors[2]\n        t_h /= scale_factors[3]\n        t_theta /= scale_factors[4]\n    reference_x_center, reference_y_center, reference_w, reference_h, reference_theta = \\\n        tf.unstack(reference_boxes, axis=1)\n    predict_x_center = t_xcenter * reference_w + reference_x_center\n    predict_y_center = t_ycenter * reference_h + reference_y_center\n    predict_w = tf.exp(t_w) * reference_w\n    predict_h = tf.exp(t_h) * reference_h\n    predict_theta = t_theta * 180 / math.pi + reference_theta\n    # mask1 = tf.less(predict_theta, -90)\n    # mask2 = tf.greater_equal(predict_theta, -180)\n    # mask7 = tf.less(predict_theta, -180)\n    # mask8 = tf.greater_equal(predict_theta, -270)\n    #\n    # mask3 = tf.greater_equal(predict_theta, 0)\n    # mask4 = tf.less(predict_theta, 90)\n    # mask5 = tf.greater_equal(predict_theta, 90)\n    # mask6 = tf.less(predict_theta, 180)\n    #\n    # # to keep range in [-90, 0)\n    # # [-180, -90)\n    # convert_mask = tf.logical_and(mask1, mask2)\n    # remain_mask = tf.logical_not(convert_mask)\n    # predict_theta += tf.cast(convert_mask, tf.float32) * 90.\n    #\n    # remain_h = tf.cast(remain_mask, tf.float32) * predict_h\n    # remain_w = tf.cast(remain_mask, tf.float32) * predict_w\n    # convert_h = tf.cast(convert_mask, tf.float32) * predict_h\n    # convert_w = tf.cast(convert_mask, tf.float32) * predict_w\n    #\n    # predict_h = remain_h + convert_w\n    # predict_w = remain_w + convert_h\n    #\n    # # [-270, -180)\n    # cond4 = tf.cast(tf.logical_and(mask7, mask8), tf.float32) * 180.\n    # predict_theta += cond4\n    #\n    # # [0, 90)\n    # # cond2 = tf.cast(tf.logical_and(mask3, mask4), tf.float32) * 90.\n    # # predict_theta -= cond2\n    #\n    # convert_mask1 = tf.logical_and(mask3, mask4)\n    # remain_mask1 = tf.logical_not(convert_mask1)\n    # predict_theta -= tf.cast(convert_mask1, tf.float32) * 90.\n    #\n    # remain_h = tf.cast(remain_mask1, tf.float32) * predict_h\n    # remain_w = tf.cast(remain_mask1, tf.float32) * predict_w\n    # convert_h = tf.cast(convert_mask1, tf.float32) * predict_h\n    # convert_w = tf.cast(convert_mask1, tf.float32) * predict_w\n    #\n    # predict_h = remain_h + convert_w\n    # predict_w = remain_w + convert_h\n    #\n    # # [90, 180)\n    # cond3 = tf.cast(tf.logical_and(mask5, mask6), tf.float32) * 180.\n    # predict_theta -= cond3\n    decode_boxes = tf.transpose(tf.stack([predict_x_center, predict_y_center,\n                                          predict_w, predict_h, predict_theta]))\n    return decode_boxes\n\n\ndef encode_boxes_rotate(unencode_boxes, reference_boxes, scale_factors=None):\n    '''\n    :param unencode_boxes: [batch_size*H*W*num_anchors_per_location, 5]\n    :param reference_boxes: [H*W*num_anchors_per_location, 5]\n    :return: encode_boxes [-1, 5]\n    '''\n    x_center, y_center, w, h, theta = \\\n        unencode_boxes[:, 0], unencode_boxes[:, 1], unencode_boxes[:, 2], unencode_boxes[:, 3], unencode_boxes[:, 4]\n    reference_x_center, reference_y_center, reference_w, reference_h, reference_theta = \\\n        reference_boxes[:, 0], reference_boxes[:, 1], reference_boxes[:, 2], reference_boxes[:, 3], reference_boxes[:, 4]\n\n    reference_w += 1e-8\n    reference_h += 1e-8\n    w += 1e-8\n    h += 1e-8  # to avoid NaN in division and log below\n    t_xcenter = (x_center - reference_x_center) / reference_w\n    t_ycenter = (y_center - reference_y_center) / reference_h\n    t_w = np.log(w / reference_w)\n    t_h = np.log(h / reference_h)\n    t_theta = (theta - reference_theta) * math.pi / 180\n    if scale_factors:\n        t_xcenter *= scale_factors[0]\n        t_ycenter *= scale_factors[1]\n        t_w *= scale_factors[2]\n        t_h *= scale_factors[3]\n        t_theta *= scale_factors[4]\n    return np.transpose(np.stack([t_xcenter, t_ycenter, t_w, t_h, t_theta]))"""
libs/box_utils/iou.py,9,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\n\ndef iou_calculate(boxes_1, boxes_2):\n    '''\n\n    :param boxes_1: [N, 4] [ymin, xmin, ymax, xmax]\n    :param boxes_2: [M, 4] [ymin, xmin. ymax, xmax]\n    :return:\n    '''\n    with tf.name_scope('iou_caculate'):\n\n        ymin_1, xmin_1, ymax_1, xmax_1 = tf.split(boxes_1, 4, axis=1)  # ymin_1 shape is [N, 1]..\n\n        ymin_2, xmin_2, ymax_2, xmax_2 = tf.unstack(boxes_2, axis=1)  # ymin_2 shape is [M, ]..\n\n        max_xmin = tf.maximum(xmin_1, xmin_2)\n        min_xmax = tf.minimum(xmax_1, xmax_2)\n\n        max_ymin = tf.maximum(ymin_1, ymin_2)\n        min_ymax = tf.minimum(ymax_1, ymax_2)\n\n        overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n        overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n        overlaps = overlap_h * overlap_w\n\n        area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n        area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n        iou = overlaps / (area_1 + area_2 - overlaps)\n\n        return iou\n"""
libs/box_utils/iou_rotate.py,8,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\nfrom libs.box_utils.coordinate_convert import *\nfrom libs.box_utils.rbbox_overlaps import rbbx_overlaps\nfrom libs.box_utils.iou_cpu import get_iou_matrix\n\n\ndef iou_rotate_calculate(boxes1, boxes2, use_gpu=True, gpu_id=0):\n    \'\'\'\n\n    :param boxes_list1:[N, 8] tensor\n    :param boxes_list2: [M, 8] tensor\n    :return:\n    \'\'\'\n\n    boxes1 = tf.cast(boxes1, tf.float32)\n    boxes2 = tf.cast(boxes2, tf.float32)\n    if use_gpu:\n\n        iou_matrix = tf.py_func(rbbx_overlaps,\n                                inp=[boxes1, boxes2, gpu_id],\n                                Tout=tf.float32)\n    else:\n        iou_matrix = tf.py_func(get_iou_matrix, inp=[boxes1, boxes2],\n                                Tout=tf.float32)\n\n    iou_matrix = tf.reshape(iou_matrix, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\n\n    return iou_matrix\n\n\ndef iou_rotate_calculate1(boxes1, boxes2, use_gpu=True, gpu_id=0):\n\n    # start = time.time()\n    if use_gpu:\n        ious = rbbx_overlaps(boxes1, boxes2, gpu_id)\n    else:\n        area1 = boxes1[:, 2] * boxes1[:, 3]\n        area2 = boxes2[:, 2] * boxes2[:, 3]\n        ious = []\n        for i, box1 in enumerate(boxes1):\n            temp_ious = []\n            r1 = ((box1[0], box1[1]), (box1[2], box1[3]), box1[4])\n            for j, box2 in enumerate(boxes2):\n                r2 = ((box2[0], box2[1]), (box2[2], box2[3]), box2[4])\n\n                int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n                if int_pts is not None:\n                    order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                    int_area = cv2.contourArea(order_pts)\n\n                    inter = int_area * 1.0 / (area1[i] + area2[j] - int_area)\n                    temp_ious.append(inter)\n                else:\n                    temp_ious.append(0.0)\n            ious.append(temp_ious)\n\n    # print(\'{}s\'.format(time.time() - start))\n\n    return np.array(ious, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'13\'\n    boxes1 = np.array([[50, 50, 100, 300, 0],\n                       [60, 60, 100, 200, 0]], np.float32)\n\n    boxes2 = np.array([[50, 50, 100, 300, -45.],\n                       [200, 200, 100, 200, 0.]], np.float32)\n\n    start = time.time()\n    with tf.Session() as sess:\n        ious = iou_rotate_calculate1(boxes1, boxes2, use_gpu=False)\n        print(sess.run(ious))\n        print(\'{}s\'.format(time.time() - start))\n\n    # start = time.time()\n    # for _ in range(10):\n    #     ious = rbbox_overlaps.rbbx_overlaps(boxes1, boxes2)\n    # print(\'{}s\'.format(time.time() - start))\n    # print(ious)\n\n    # print(ovr)\n\n\n\n'"
libs/box_utils/make_rotate_anchors.py,28,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport time\nfrom libs.box_utils.show_box_in_tensor import *\n\n\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios, anchor_angles,\n                 featuremap_height, featuremap_width, stride, name=\'make_ratate_anchors\'):\n\n\n    \'\'\'\n    :param base_anchor_size:\n    :param anchor_scales:\n    :param anchor_ratios:\n    :param anchor_thetas:\n    :param featuremap_height:\n    :param featuremap_width:\n    :param stride:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)  # [y_center, x_center, h, w]\n        ws, hs, angles = enum_ratios_and_thetas(enum_scales(base_anchor, anchor_scales),\n                                                anchor_ratios, anchor_angles)  # per locations ws and hs and thetas\n\n        x_centers = tf.range(featuremap_width, dtype=tf.float32) * stride + stride // 2\n        y_centers = tf.range(featuremap_height, dtype=tf.float32) * stride + stride // 2\n\n        x_centers, y_centers = tf.meshgrid(x_centers, y_centers)\n\n        angles, _ = tf.meshgrid(angles, x_centers)\n        ws, x_centers = tf.meshgrid(ws, x_centers)\n        hs, y_centers = tf.meshgrid(hs, y_centers)\n\n        anchor_centers = tf.stack([x_centers, y_centers], 2)\n        anchor_centers = tf.reshape(anchor_centers, [-1, 2])\n\n        box_parameters = tf.stack([ws, hs, angles], axis=2)\n        box_parameters = tf.reshape(box_parameters, [-1, 3])\n        anchors = tf.concat([anchor_centers, box_parameters], axis=1)\n\n        return anchors\n\n\ndef enum_scales(base_anchor, anchor_scales):\n    anchor_scales = base_anchor * tf.constant(anchor_scales, dtype=tf.float32, shape=(len(anchor_scales), 1))\n\n    return anchor_scales\n\n\ndef enum_ratios_and_thetas(anchors, anchor_ratios, anchor_angles):\n    \'\'\'\n    ratio = h /w\n    :param anchors:\n    :param anchor_ratios:\n    :return:\n    \'\'\'\n    ws = anchors[:, 2]  # for base anchor: w == h\n    hs = anchors[:, 3]\n    anchor_angles = tf.constant(anchor_angles, tf.float32)\n    sqrt_ratios = tf.sqrt(tf.constant(anchor_ratios))\n\n    ws = tf.reshape(ws / sqrt_ratios[:, tf.newaxis], [-1])\n    hs = tf.reshape(hs * sqrt_ratios[:, tf.newaxis], [-1])\n\n    ws, _ = tf.meshgrid(ws, anchor_angles)\n    hs, anchor_angles = tf.meshgrid(hs, anchor_angles)\n\n    anchor_angles = tf.reshape(anchor_angles, [-1, 1])\n    ws = tf.reshape(ws, [-1, 1])\n    hs = tf.reshape(hs, [-1, 1])\n\n    return ws, hs, anchor_angles\n\n\nif __name__ == \'__main__\':\n    import os\n    from libs.configs import cfgs\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'2\'\n    base_anchor_size = 256\n    anchor_scales = [1.]\n    anchor_ratios = [0.5, 2.0, 1/3, 3, 1/5, 5, 1/8, 8]\n    anchor_angless = [-90, -75, -60, -45, -30, -15]\n    base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)\n    tmp1 = enum_ratios_and_thetas(enum_scales(base_anchor, anchor_scales), anchor_ratios, anchor_angless)\n    anchors = make_anchors(32,\n                   [2.], [2.0, 1/2], anchor_angless,\n                   featuremap_height=800 // 16 * 2,\n                   featuremap_width=800 // 16 * 2,\n                   stride=8)\n\n    # anchors = make_anchors(base_anchor_size=cfgs.BASE_ANCHOR_SIZE_LIST[0],\n    #                        anchor_scales=cfgs.ANCHOR_SCALES,\n    #                        anchor_ratios=cfgs.ANCHOR_RATIOS,\n    #                        anchor_angles=cfgs.ANCHOR_ANGLES,\n    #                        featuremap_height=800 // 16,\n    #                        featuremap_width=800 // 16,\n    #                        stride=cfgs.ANCHOR_STRIDE[0],\n    #                        name=""make_anchors_forRPN"")\n\n    img = tf.ones([800, 800, 3])\n    img = tf.expand_dims(img, axis=0)\n\n    img1 = draw_box_with_color_rotate(img, anchors[9100:9110], text=tf.shape(anchors)[1])\n\n    with tf.Session() as sess:\n        temp1, _img1 = sess.run([anchors, img1])\n\n        _img1 = _img1[0]\n\n        cv2.imwrite(\'rotate_anchors.jpg\', _img1)\n        cv2.waitKey(0)\n\n        print(temp1)\n        print(\'debug\')\n'"
libs/box_utils/nms_rotate.py,18,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nfrom libs.configs import cfgs\nimport tensorflow as tf\nif cfgs.ROTATE_NMS_USE_GPU:\n    from libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef nms_rotate(decode_boxes, scores, iou_threshold, max_output_size,\n               use_angle_condition=False, angle_threshold=0, use_gpu=True, gpu_id=0):\n    """"""\n    :param boxes: format [x_c, y_c, w, h, theta]\n    :param scores: scores of boxes\n    :param threshold: iou threshold (0.7 or 0.5)\n    :param max_output_size: max number of output\n    :return: the remaining index of boxes\n    """"""\n\n    if use_gpu:\n        keep = nms_rotate_gpu(boxes_list=decode_boxes,\n                              scores=scores,\n                              iou_threshold=iou_threshold,\n                              angle_gap_threshold=angle_threshold,\n                              use_angle_condition=use_angle_condition,\n                              device_id=gpu_id)\n\n        keep = tf.cond(\n            tf.greater(tf.shape(keep)[0], max_output_size),\n            true_fn=lambda: tf.slice(keep, [0], [max_output_size]),\n            false_fn=lambda: keep)\n\n    else:\n        keep = tf.py_func(nms_rotate_cpu,\n                          inp=[decode_boxes, scores, iou_threshold, max_output_size],\n                          Tout=tf.int64)\n    return keep\n\n\ndef nms_rotate_cpu(boxes, scores, iou_threshold, max_output_size):\n\n    keep = []\n\n    order = scores.argsort()[::-1]\n    num = boxes.shape[0]\n\n    suppressed = np.zeros((num), dtype=np.int)\n\n    for _i in range(num):\n        if len(keep) >= max_output_size:\n            break\n\n        i = order[_i]\n        if suppressed[i] == 1:\n            continue\n        keep.append(i)\n        r1 = ((boxes[i, 0], boxes[i, 1]), (boxes[i, 2], boxes[i, 3]), boxes[i, 4])\n        area_r1 = boxes[i, 2] * boxes[i, 3]\n        for _j in range(_i + 1, num):\n            j = order[_j]\n            if suppressed[i] == 1:\n                continue\n            r2 = ((boxes[j, 0], boxes[j, 1]), (boxes[j, 2], boxes[j, 3]), boxes[j, 4])\n            area_r2 = boxes[j, 2] * boxes[j, 3]\n            inter = 0.0\n\n            int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n            if int_pts is not None:\n                order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                int_area = cv2.contourArea(order_pts)\n\n                inter = int_area * 1.0 / (area_r1 + area_r2 - int_area + cfgs.EPSILON)\n\n            if inter >= iou_threshold:\n                suppressed[j] = 1\n\n    return np.array(keep, np.int64)\n\n\ndef nms_rotate_gpu(boxes_list, scores, iou_threshold, use_angle_condition=False, angle_gap_threshold=0, device_id=0):\n    if use_angle_condition:\n        x_c, y_c, w, h, theta = tf.unstack(boxes_list, axis=1)\n        boxes_list = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n        det_tensor = tf.concat([boxes_list, tf.expand_dims(scores, axis=1)], axis=1)\n        keep = tf.py_func(rotate_gpu_nms,\n                          inp=[det_tensor, iou_threshold, device_id],\n                          Tout=tf.int64)\n        return keep\n    else:\n        x_c, y_c, w, h, theta = tf.unstack(boxes_list, axis=1)\n        boxes_list = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n        det_tensor = tf.concat([boxes_list, tf.expand_dims(scores, axis=1)], axis=1)\n        keep = tf.py_func(rotate_gpu_nms,\n                          inp=[det_tensor, iou_threshold, device_id],\n                          Tout=tf.int64)\n        keep = tf.reshape(keep, [-1])\n        return keep\n\n\nif __name__ == \'__main__\':\n    boxes = np.array([[50, 50, 100, 100, 0],\n                      [60, 60, 100, 100, 0],\n                      [50, 50, 100, 100, -45.],\n                      [200, 200, 100, 100, 0.]])\n\n    scores = np.array([0.99, 0.88, 0.66, 0.77])\n\n    keep = nms_rotate(tf.convert_to_tensor(boxes, dtype=tf.float32), tf.convert_to_tensor(scores, dtype=tf.float32),\n                      0.7, 5)\n\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    with tf.Session() as sess:\n        print(sess.run(keep))\n'"
libs/box_utils/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport subprocess\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\'rbbox_overlaps\',\n              [\'rbbox_overlaps_kernel.cu\', \'rbbox_overlaps.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with\n              # gcc the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n    Extension(\'rotate_polygon_nms\',\n        [\'rotate_polygon_nms_kernel.cu\', \'rotate_polygon_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs=[numpy_include, CUDA[\'include\']]\n    ),\n    Extension(\'iou_cpu\',\n              [\'iou_cpu.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with\n              # gcc the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']])\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
libs/box_utils/show_box_in_tensor.py,21,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom libs.label_name_dict.label_dict import LABEl_NAME_MAP\n\nfrom libs.configs import cfgs\n\n\ndef draw_box_with_color(img_batch, boxes, text):\n\n    def draw_box_cv(img, boxes, text):\n        img = img + np.array(cfgs.PIXEL_MEAN)\n        boxes = boxes.astype(np.int64)\n        img = np.array(img * 255 / np.max(img), np.uint8)\n        for box in boxes:\n            xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n\n            color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            cv2.rectangle(img,\n                          pt1=(xmin, ymin),\n                          pt2=(xmax, ymax),\n                          color=color,\n                          thickness=2)\n\n        text = str(text)\n        cv2.putText(img,\n                    text=text,\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n\n        # img = np.transpose(img, [2, 1, 0])\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    # color = tf.constant([0, 0, 255])\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, text],\n                                       Tout=[tf.uint8])\n\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n\n    return img_tensor_with_boxes\n\n\ndef draw_box_with_color_rotate(img_batch, boxes, text):\n\n    def draw_box_cv(img, boxes, text):\n        img = img + np.array(cfgs.PIXEL_MEAN)\n        boxes = boxes.astype(np.int64)\n        img = np.array(img * 255 / np.max(img), np.uint8)\n        for box in boxes:\n            x_c, y_c, w, h, theta = box[0], box[1], box[2], box[3], box[4]\n            rect = ((x_c, y_c), (w, h), theta)\n            rect = cv2.boxPoints(rect)\n            rect = np.int0(rect)\n            color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            cv2.drawContours(img, [rect], -1, color, 3)\n\n        text = str(text)\n        cv2.putText(img,\n                    text=text,\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, text],\n                                       Tout=[tf.uint8])\n\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_categories(img_batch, boxes, scores):\n\n    def draw_box_cv(img, boxes, scores):\n        img = img + np.array(cfgs.PIXEL_MEAN)\n        boxes = boxes.astype(np.int64)\n        img = np.array(img*255/np.max(img), np.uint8)\n\n        num_of_object = 0\n        for i, box in enumerate(boxes):\n            xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n\n            score = scores[i]\n\n            num_of_object += 1\n            color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n            cv2.rectangle(img,\n                          pt1=(xmin, ymin),\n                          pt2=(xmax, ymax),\n                          color=color,\n                          thickness=2)\n            cv2.rectangle(img,\n                          pt1=(xmin, ymin),\n                          pt2=(xmin+120, ymin+15),\n                          color=color,\n                          thickness=-1)\n            cv2.putText(img,\n                        text=str(score),\n                        org=(xmin, ymin+10),\n                        fontFace=1,\n                        fontScale=1,\n                        thickness=2,\n                        color=(color[1], color[2], color[0]))\n        cv2.putText(img,\n                    text=str(num_of_object),\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_categories_and_scores(img_batch, boxes, labels, scores):\n\n    def draw_box_cv(img, boxes, labels, scores):\n        img = img + np.array(cfgs.PIXEL_MEAN)\n        boxes = boxes.astype(np.int64)\n        labels = labels.astype(np.int32)\n        img = np.array(img*255/np.max(img), np.uint8)\n\n        num_of_object = 0\n        for i, box in enumerate(boxes):\n            xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n\n            label = labels[i]\n            score = scores[i]\n            if label != 0:\n                num_of_object += 1\n                color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n                cv2.rectangle(img,\n                              pt1=(xmin, ymin),\n                              pt2=(xmax, ymax),\n                              color=color,\n                              thickness=2)\n                cv2.rectangle(img,\n                              pt1=(xmin, ymin),\n                              pt2=(xmin+120, ymin+15),\n                              color=color,\n                              thickness=-1)\n                category = LABEl_NAME_MAP[label]\n                cv2.putText(img,\n                            text=category+"": ""+str(score),\n                            org=(xmin, ymin+10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n        cv2.putText(img,\n                    text=str(num_of_object),\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_categories_and_scores_rotate(img_batch, boxes, labels, scores):\n\n    def draw_box_cv(img, boxes, labels, scores):\n        img = img + np.array(cfgs.PIXEL_MEAN)\n        boxes = boxes.astype(np.int64)\n        labels = labels.astype(np.int32)\n        img = np.array(img*255/np.max(img), np.uint8)\n\n        num_of_object = 0\n        for i, box in enumerate(boxes):\n\n            x_c, y_c, w, h, theta = box[0], box[1], box[2], box[3], box[4]\n            label = labels[i]\n            score = scores[i]\n            if label != 0:\n                num_of_object += 1\n\n                rect = ((x_c, y_c), (w, h), theta)\n                rect = cv2.boxPoints(rect)\n                rect = np.int0(rect)\n                color = (np.random.randint(255), np.random.randint(255), np.random.randint(255))\n                cv2.drawContours(img, [rect], -1, color, 3)\n\n                cv2.rectangle(img,\n                              pt1=(x_c, y_c),\n                              pt2=(x_c+120, y_c+15),\n                              color=color,\n                              thickness=-1)\n                category = LABEl_NAME_MAP[label]\n                cv2.putText(img,\n                            text=category+"": ""+str(score),\n                            org=(x_c, y_c+10),\n                            fontFace=1,\n                            fontScale=1,\n                            thickness=2,\n                            color=(color[1], color[2], color[0]))\n        cv2.putText(img,\n                    text=str(num_of_object),\n                    org=((img.shape[1]) // 2, (img.shape[0]) // 2),\n                    fontFace=3,\n                    fontScale=1,\n                    color=(255, 0, 0))\n        img = img[:, :, ::-1]\n        return img\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor_with_boxes = tf.py_func(draw_box_cv,\n                                       inp=[img_tensor, boxes, labels, scores],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\nif __name__ == ""__main__"":\n    print (1)\n\n'"
libs/box_utils/tf_ops.py,7,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\n\n\'\'\'\nall of these ops are derived from tenosrflow Object Detection API\n\'\'\'\ndef indices_to_dense_vector(indices,\n                            size,\n                            indices_value=1.,\n                            default_value=0,\n                            dtype=tf.float32):\n  """"""Creates dense vector with indices set to specific (the para ""indices_value"" ) and rest to zeros.\n\n  This function exists because it is unclear if it is safe to use\n    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)\n  with indices which are not ordered.\n  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])\n\n  Args:\n    indices: 1d Tensor with integer indices which are to be set to\n        indices_values.\n    size: scalar with size (integer) of output Tensor.\n    indices_value: values of elements specified by indices in the output vector\n    default_value: values of other elements in the output vector.\n    dtype: data type.\n\n  Returns:\n    dense 1D Tensor of shape [size] with indices set to indices_values and the\n        rest set to default_value.\n  """"""\n  size = tf.to_int32(size)\n  zeros = tf.ones([size], dtype=dtype) * default_value\n  values = tf.ones_like(indices, dtype=dtype) * indices_value\n\n  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],\n                           [zeros, values])'"
libs/configs/__init__.py,0,b''
libs/configs/cfgs.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\n\n\n# ------------------------------------------------\nVERSION = \'RRPN_20180901_DOTA_v1\'\nNET_NAME = \'resnet_v1_101\'\nADD_BOX_IN_TENSORBOARD = True\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""2""\nSHOW_TRAIN_INFO_INTE = 10\nSMRY_ITER = 100\nSAVE_WEIGHTS_INTE = 2000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\nINFERENCE_IMAGE_PATH = ROOT_PATH + \'/tools/inference_image\'\nINFERENCE_SAVE_PATH = ROOT_PATH + \'/tools/inference_results\'\n\nif NET_NAME.startswith(\'resnet\'):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(\'MobilenetV2\'):\n    weights_name = \'mobilenet/mobilenet_v2_1.0_224\'\nelse:\n    raise NotImplementedError\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\n\nEVALUATE_H_DIR = ROOT_PATH + \'/output\' + \'/evaluate_h_result_pickle/\' + VERSION\nEVALUATE_R_DIR = ROOT_PATH + \'/output\' + \'/evaluate_r_result_pickle/\' + VERSION\nTEST_ANNOTATION_PATH = \'/mnt/USBB/gx/DOTA/DOTA_clip/val/labeltxt\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nIS_FILTER_OUTSIDE_BOXES = True\nROTATE_NMS_USE_GPU = True\nFIXED_BLOCKS = 2  # allow 0~3\n\nRPN_LOCATION_LOSS_WEIGHT = 1 / 7\nRPN_CLASSIFICATION_LOSS_WEIGHT = 2.0\n\nFAST_RCNN_LOCATION_LOSS_WEIGHT = 4.0\nFAST_RCNN_CLASSIFICATION_LOSS_WEIGHT = 2.0\nRPN_SIGMA = 3.0\nFASTRCNN_SIGMA = 1.0\n\n\nMUTILPY_BIAS_GRADIENT = None  # 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None   # 10.0  if None, will not clip\n\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 0.0003  # 0.0003\nDECAY_STEP = [100000, 200000]  # 90000, 120000\nMAX_ITERATION = 300000\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA_TOTAL\'  # \'ship\', \'spacenet\', \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 1000\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nBATCH_SIZE = 1\nINITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01)\nBBOX_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.001)\nWEIGHT_DECAY = 0.0001\n\n\n# ---------------------------------------------Anchor config\nBASE_ANCHOR_SIZE_LIST = [256]  # can be modified\nANCHOR_STRIDE = [16]  # can not be modified in most situations\nANCHOR_SCALES = [0.0625, 0.125, 0.25, 0.5, 1., 2.0]  # [4, 8, 16, 32]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 4., 4., 1 / 5., 6., 1 / 6., 7., 1 / 7.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nROI_SCALE_FACTORS = [10., 10., 5.0, 5.0, 5.0]\nANCHOR_SCALE_FACTORS = None\n\n\n# --------------------------------------------RPN config\nKERNEL_SIZE = 3\nRPN_IOU_POSITIVE_THRESHOLD = 0.7\nRPN_IOU_NEGATIVE_THRESHOLD = 0.3\nTRAIN_RPN_CLOOBER_POSITIVES = False\n\nRPN_MINIBATCH_SIZE = 512\nRPN_POSITIVE_RATE = 0.5\nRPN_NMS_IOU_THRESHOLD = 0.7\nRPN_TOP_K_NMS_TRAIN = 12000\nRPN_MAXIMUM_PROPOSAL_TARIN = 2000\n\nRPN_TOP_K_NMS_TEST = 10000  # 5000\nRPN_MAXIMUM_PROPOSAL_TEST = 300  # 300\n\n\n# -------------------------------------------Fast-RCNN config\nROI_SIZE = 14\nROI_POOL_KERNEL_SIZE = 2\nUSE_DROPOUT = False\nKEEP_PROB = 1.0\nSHOW_SCORE_THRSHOLD = 0.001  # only show in tensorboard\n\nFAST_RCNN_NMS_IOU_THRESHOLD = 0.2  # 0.6\nFAST_RCNN_NMS_MAX_BOXES_PER_CLASS = 150\nFAST_RCNN_IOU_POSITIVE_THRESHOLD = 0.4\nFAST_RCNN_IOU_NEGATIVE_THRESHOLD = 0.0   # 0.1 < IOU < 0.5 is negative\nFAST_RCNN_MINIBATCH_SIZE = 512  # if is -1, that is train with OHEM\nFAST_RCNN_POSITIVE_RATE = 0.35\n\nADD_GTBOXES_TO_TRAIN = False\n\n\n\n'"
libs/detection_oprations/__init__.py,0,b''
libs/detection_oprations/anchor_target_layer_without_boxweight.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom libs.configs import cfgs\nimport numpy as np\nimport numpy.random as npr\nfrom libs.box_utils import iou_rotate\nfrom libs.box_utils import encode_and_decode\nimport tensorflow as tf\n\n\ndef anchor_target_layer(\n        gt_boxes, anchors, overlaps, is_restrict_bg=False):\n    """"""Same as the anchor target layer in original Fast/er RCNN """"""\n\n    total_anchors = anchors.shape[0]\n    gt_boxes = gt_boxes[:, :-1]  # remove class label\n\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    labels = np.empty((total_anchors,), dtype=np.float32)\n    labels.fill(-1)\n\n    # overlaps between the anchors and the gt boxes\n\n    argmax_overlaps = overlaps.argmax(axis=1)\n    max_overlaps = overlaps[np.arange(total_anchors), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0)\n    gt_max_overlaps = overlaps[\n        gt_argmax_overlaps, np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n    if not cfgs.TRAIN_RPN_CLOOBER_POSITIVES:\n        labels[max_overlaps < cfgs.RPN_IOU_NEGATIVE_THRESHOLD] = 0\n\n    labels[gt_argmax_overlaps] = 1\n    labels[max_overlaps >= cfgs.RPN_IOU_POSITIVE_THRESHOLD] = 1\n\n    if cfgs.TRAIN_RPN_CLOOBER_POSITIVES:\n        labels[max_overlaps < cfgs.RPN_IOU_NEGATIVE_THRESHOLD] = 0\n\n    num_fg = int(cfgs.RPN_MINIBATCH_SIZE * cfgs.RPN_POSITIVE_RATE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n\n    num_bg = cfgs.RPN_MINIBATCH_SIZE - np.sum(labels == 1)\n    if is_restrict_bg:\n        num_bg = max(num_bg, num_fg * 1.5)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n    # labels = labels.reshape((1, height, width, A))\n    rpn_labels = labels.reshape((-1, 1))\n\n    # bbox_targets\n    bbox_targets = bbox_targets.reshape((-1, 5))\n    rpn_bbox_targets = bbox_targets\n\n    return rpn_labels, rpn_bbox_targets\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count,), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # targets = bbox_transform(ex_rois, gt_rois[:, :5]).astype(\n    #     np.float32, copy=False)\n    targets = encode_and_decode.encode_boxes_rotate(unencode_boxes=gt_rois,\n                                                    reference_boxes=ex_rois,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n    # targets = encode_and_decode.encode_boxes(ex_rois=ex_rois,\n    #                                          gt_rois=gt_rois,\n    #                                          scale_factor=None)\n    return targets\n'"
libs/detection_oprations/proposal_opr.py,6,"b'# encoding: utf-8\n""""""\n@author: zeming li\n@contact: zengarden2009@gmail.com\n""""""\n\nfrom libs.configs import cfgs\nfrom libs.box_utils import encode_and_decode, nms_rotate\nfrom libs.box_utils import boxes_utils\nimport tensorflow as tf\nimport numpy as np\n\n\ndef postprocess_rpn_proposals(rpn_bbox_pred, rpn_cls_prob, img_shape, anchors, is_training):\n    \'\'\'\n\n    :param rpn_bbox_pred: [-1, 4]\n    :param rpn_cls_prob: [-1, 2]\n    :param img_shape:\n    :param anchors:[-1, 4]\n    :param is_training:\n    :return:\n    \'\'\'\n\n    if is_training:\n        pre_nms_topN = cfgs.RPN_TOP_K_NMS_TRAIN\n        post_nms_topN = cfgs.RPN_MAXIMUM_PROPOSAL_TARIN\n        nms_thresh = cfgs.RPN_NMS_IOU_THRESHOLD\n    else:\n        pre_nms_topN = cfgs.RPN_TOP_K_NMS_TEST\n        post_nms_topN = cfgs.RPN_MAXIMUM_PROPOSAL_TEST\n        nms_thresh = cfgs.RPN_NMS_IOU_THRESHOLD\n\n    cls_prob = rpn_cls_prob[:, 1]\n\n    # 1. decode boxes\n    decode_boxes = encode_and_decode.decode_boxes_rotate(encode_boxes=rpn_bbox_pred,\n                                                         reference_boxes=anchors,\n                                                         scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # decode_boxes = encode_and_decode.decode_boxes(boxes=anchors,\n    #                                               deltas=rpn_bbox_pred,\n    #                                               scale_factor=None)\n\n    # 2. clip to img boundaries\n    # decode_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=decode_boxes,\n    #                                                         img_shape=img_shape)\n\n    # 3. get top N to NMS\n    if pre_nms_topN > 0:\n        pre_nms_topN = tf.minimum(pre_nms_topN, tf.shape(decode_boxes)[0], name=\'avoid_unenough_boxes\')\n        cls_prob, top_k_indices = tf.nn.top_k(cls_prob, k=pre_nms_topN)\n        decode_boxes = tf.gather(decode_boxes, top_k_indices)\n\n    # 4. NMS\n    # keep = tf.image.non_max_suppression(boxes=decode_boxes,\n    #                                     scores=cls_prob,\n    #                                     max_output_size=post_nms_topN,\n    #                                     iou_threshold=nms_thresh)\n    keep = nms_rotate.nms_rotate(decode_boxes=decode_boxes,\n                                 scores=cls_prob,\n                                 iou_threshold=nms_thresh,\n                                 max_output_size=post_nms_topN,\n                                 use_angle_condition=False,\n                                 angle_threshold=15,\n                                 use_gpu=True)\n\n    final_boxes = tf.gather(decode_boxes, keep)\n    final_probs = tf.gather(cls_prob, keep)\n\n    return final_boxes, final_probs\n\n'"
libs/detection_oprations/proposal_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom libs.configs import cfgs\nimport numpy as np\nimport numpy.random as npr\n\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import iou_rotate\n\n\ndef proposal_target_layer(rpn_rois, gt_boxes, overlaps):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    """"""\n\n    rois_per_image = np.inf if cfgs.FAST_RCNN_MINIBATCH_SIZE == -1 else cfgs.FAST_RCNN_MINIBATCH_SIZE\n\n    fg_rois_per_image = np.round(cfgs.FAST_RCNN_POSITIVE_RATE * rois_per_image)\n    # Sample rois with classification labels and bounding box regression\n    labels, rois, bbox_targets = _sample_rois(rpn_rois, gt_boxes, overlaps,\n                                              fg_rois_per_image, rois_per_image, cfgs.CLASS_NUM+1)\n    rois = rois.reshape(-1, 5)\n    labels = labels.reshape(-1)\n    bbox_targets = bbox_targets.reshape(-1, (cfgs.CLASS_NUM+1) * 5)\n\n    return rois, labels, bbox_targets\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets (bbox_target_data) are stored in a\n    compact form N x (class, tx, ty, tw, th, ttheta)\n\n    This function expands those targets into the 5-of-5*K representation used\n    by the network (i.e. only one class has non-zero targets).\n\n    Returns:\n        bbox_target (ndarray): N x 5K blob of regression targets\n    """"""\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 5 * num_classes), dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = int(5 * cls)\n        end = start + 5\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n\n    return bbox_targets\n\n\ndef _compute_targets(ex_rois, gt_rois_r, labels):\n    """"""Compute bounding-box regression targets for an image.\n    that is : [label, tx, ty, tw, th]\n    """"""\n\n    assert ex_rois.shape[0] == gt_rois_r.shape[0]\n    assert ex_rois.shape[1] == 5\n    assert gt_rois_r.shape[1] == 5\n\n    targets_r = encode_and_decode.encode_boxes_rotate(unencode_boxes=gt_rois_r,\n                                                      reference_boxes=ex_rois,\n                                                      scale_factors=cfgs.ROI_SCALE_FACTORS)\n    # targets = encode_and_decode.encode_boxes(ex_rois=ex_rois,\n    #                                          gt_rois=gt_rois,\n    #                                          scale_factor=cfgs.ROI_SCALE_FACTORS)\n    return np.hstack((labels[:, np.newaxis], targets_r)).astype(np.float32, copy=False)\n\n\ndef _sample_rois(all_rois,  gt_boxes, overlaps, fg_rois_per_image,\n                 rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n\n    all_rois shape is [-1, 5]\n    gt_boxes shape is [-1, 6]. that is [x_c, y_c, w, h, theta, label]\n    """"""\n    # overlaps: (rois x gt_boxes)\n\n    gt_assignment = overlaps.argmax(axis=1)\n    max_overlaps = overlaps.max(axis=1)\n    labels = gt_boxes[gt_assignment, -1]\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfgs.FAST_RCNN_IOU_POSITIVE_THRESHOLD) &\n                       (max_overlaps >= cfgs.FAST_RCNN_IOU_NEGATIVE_THRESHOLD))[0]\n    # print(""first fileter, fg_size: {} || bg_size: {}"".format(fg_inds.shape, bg_inds.shape))\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = min(fg_rois_per_image, fg_inds.size)\n\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=int(fg_rois_per_this_image), replace=False)\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=int(bg_rois_per_this_image), replace=False)\n\n    # print(""second fileter, fg_size: {} || bg_size: {}"".format(fg_inds.shape, bg_inds.shape))\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n\n    # Clamp labels for the background RoIs to 0\n    labels[int(fg_rois_per_this_image):] = 0\n    rois = all_rois[keep_inds]\n    bbox_target_data = _compute_targets(rois, gt_boxes[gt_assignment[keep_inds], :-1], labels)\n    bbox_targets = \\\n        _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n    return labels, rois, bbox_targets\n'"
libs/label_name_dict/__init__.py,0,b''
libs/label_name_dict/label_dict.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nfrom libs.configs import cfgs\n\nif cfgs.DATASET_NAME == 'ship':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'ship': 1\n    }\nelif cfgs.DATASET_NAME == 'FDDB':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'face': 1\n    }\nelif cfgs.DATASET_NAME == 'icdar':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'text': 1\n    }\nelif cfgs.DATASET_NAME.startswith('DOTA'):\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'roundabout': 1,\n        'tennis-court': 2,\n        'swimming-pool': 3,\n        'storage-tank': 4,\n        'soccer-ball-field': 5,\n        'small-vehicle': 6,\n        'ship': 7,\n        'plane': 8,\n        'large-vehicle': 9,\n        'helicopter': 10,\n        'harbor': 11,\n        'ground-track-field': 12,\n        'bridge': 13,\n        'basketball-court': 14,\n        'baseball-diamond': 15\n    }\nelif cfgs.DATASET_NAME == 'pascal':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1,\n        'bicycle': 2,\n        'bird': 3,\n        'boat': 4,\n        'bottle': 5,\n        'bus': 6,\n        'car': 7,\n        'cat': 8,\n        'chair': 9,\n        'cow': 10,\n        'diningtable': 11,\n        'dog': 12,\n        'horse': 13,\n        'motorbike': 14,\n        'person': 15,\n        'pottedplant': 16,\n        'sheep': 17,\n        'sofa': 18,\n        'train': 19,\n        'tvmonitor': 20\n    }\nelse:\n    assert 'please set label dict!'\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\nLABEl_NAME_MAP = get_label_name_map()"""
libs/label_name_dict/pascal_dict.py,0,"b""# -*- coding: utf-8 -*-\n\nNAME_LABEL_MAP = {\n    'back_ground': 0,\n    'aeroplane': 1,\n    'bicycle': 2,\n    'bird': 3,\n    'boat': 4,\n    'bottle': 5,\n    'bus': 6,\n    'car': 7,\n    'cat': 8,\n    'chair': 9,\n    'cow': 10,\n    'diningtable': 11,\n    'dog': 12,\n    'horse': 13,\n    'motorbike': 14,\n    'person': 15,\n    'pottedplant': 16,\n    'sheep': 17,\n    'sofa': 18,\n    'train': 19,\n    'tvmonitor': 20\n}\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\nLABEl_NAME_MAP = get_label_name_map()"""
libs/label_name_dict/remote_sensing_dict.py,0,"b""# -*- coding: utf-8 -*-\n\nNAME_LABEL_MAP = {\n    'back_ground': 0,\n    'building': 1\n}\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\nLABEl_NAME_MAP = get_label_name_map()"""
libs/losses/__init__.py,0,b''
libs/losses/losses.py,22,"b'# -*- coding: utf-8 -*-\n""""""\n@author: jemmy li\n@contact: zengarden2009@gmail.com\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4] in RPN. [-1, cls_num+1, 4] or [-1, cls_num+1, 5] in Fast-rcnn\n    :param bbox_targets: shape is same as bbox_pred\n    :param sigma:\n    :return:\n    \'\'\'\n    sigma_2 = sigma**2\n\n    box_diff = bbox_pred - bbox_targets\n\n    abs_box_diff = tf.abs(box_diff)\n\n    smoothL1_sign = tf.stop_gradient(\n        tf.to_float(tf.less(abs_box_diff, 1. / sigma_2)))\n    loss_box = tf.pow(box_diff, 2) * (sigma_2 / 2.0) * smoothL1_sign \\\n               + (abs_box_diff - (0.5 / sigma_2)) * (1.0 - smoothL1_sign)\n    return loss_box\n\n\ndef smooth_l1_loss_rpn(bbox_pred, bbox_targets, label, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4]\n    :param bbox_targets: [-1, 4]\n    :param label: [-1]\n    :param sigma:\n    :return:\n    \'\'\'\n    value = _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=sigma)\n\n    value = tf.reduce_sum(value, axis=1)  # to sum in axis 1\n\n    rpn_select = tf.reshape(tf.where(tf.greater_equal(label, 0)), [-1])\n\n    # rpn_select = tf.stop_gradient(rpn_select) # to avoid\n    selected_value = tf.gather(value, rpn_select)\n\n    non_ignored_mask = tf.stop_gradient(\n        1.0 - tf.to_float(tf.equal(label, -1)))  # positve is 1.0 others is 0.0\n\n    bbox_loss = tf.reduce_sum(selected_value) / tf.maximum(1.0, tf.reduce_sum(non_ignored_mask))\n\n    return bbox_loss\n\n\ndef smooth_l1_loss_rcnn(bbox_pred, bbox_targets, label, num_classes, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, (cfgs.CLS_NUM +1) * 5]\n    :param bbox_targets:[-1, (cfgs.CLS_NUM +1) * 5]\n    :param label:[-1]\n    :param num_classes:\n    :param sigma:\n    :return:\n    \'\'\'\n\n    outside_mask = tf.stop_gradient(tf.to_float(tf.greater(label, 0)))\n\n    bbox_pred = tf.reshape(bbox_pred, [-1, num_classes, 5])\n    bbox_targets = tf.reshape(bbox_targets, [-1, num_classes, 5])\n\n    value = _smooth_l1_loss_base(bbox_pred,\n                                 bbox_targets,\n                                 sigma=sigma)\n    value = tf.reduce_sum(value, 2)\n    value = tf.reshape(value, [-1, num_classes])\n\n    inside_mask = tf.one_hot(tf.reshape(label, [-1, 1]),\n                             depth=num_classes, axis=1)\n\n    inside_mask = tf.stop_gradient(\n        tf.to_float(tf.reshape(inside_mask, [-1, num_classes])))\n\n    normalizer = tf.to_float(tf.shape(bbox_pred)[0])\n    bbox_loss = tf.reduce_sum(\n        tf.reduce_sum(value * inside_mask, 1)*outside_mask) / normalizer\n\n    return bbox_loss\n\n\ndef sum_ohem_loss(cls_score, label, bbox_pred, bbox_targets,\n                  nr_ohem_sampling, nr_classes, sigma=1.0):\n\n    raise NotImplementedError(\'not implement Now. YJR will implemetn in the future\')'"
libs/networks/__init__.py,0,b''
libs/networks/build_whole_network.py,113,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet\nfrom libs.networks import mobilenet_v2\nfrom libs.box_utils import encode_and_decode\nfrom libs.box_utils import boxes_utils, iou_rotate\nfrom libs.box_utils import make_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.proposal_opr import postprocess_rpn_proposals\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.proposal_target_layer import proposal_target_layer\nfrom libs.box_utils import nms_rotate\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name == \'resnet_v1_50\':\n            return resnet.resnet_50_base(input_img_batch, is_training=self.is_training)\n        elif self.base_network_name == \'resnet_v1_101\':\n            return resnet.resnet_101_base(input_img_batch, is_training=self.is_training)\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n        else:\n            raise ValueError(\'net name error\')\n\n    def postprocess_fastrcnn(self, rois, bbox_ppred, scores, img_shape):\n        \'\'\'\n\n        :param rois:[-1, 4]\n        :param bbox_ppred: [-1, (cfgs.Class_num+1) * 5]\n        :param scores: [-1, cfgs.Class_num + 1]\n        :return:\n        \'\'\'\n\n        with tf.name_scope(\'postprocess_fastrcnn\'):\n            rois = tf.stop_gradient(rois)\n            scores = tf.stop_gradient(scores)\n            bbox_ppred = tf.reshape(bbox_ppred, [-1, cfgs.CLASS_NUM + 1, 5])\n            bbox_ppred = tf.stop_gradient(bbox_ppred)\n\n            bbox_pred_list = tf.unstack(bbox_ppred, axis=1)\n            score_list = tf.unstack(scores, axis=1)\n\n            allclasses_boxes = []\n            allclasses_scores = []\n            categories = []\n            for i in range(1, cfgs.CLASS_NUM+1):\n\n                # 1. decode boxes in each class\n                tmp_encoded_box = bbox_pred_list[i]\n                tmp_score = score_list[i]\n                tmp_decoded_boxes = encode_and_decode.decode_boxes_rotate(encode_boxes=tmp_encoded_box,\n                                                                          reference_boxes=rois,\n                                                                          scale_factors=cfgs.ROI_SCALE_FACTORS)\n                # tmp_decoded_boxes = encode_and_decode.decode_boxes(boxes=rois,\n                #                                                    deltas=tmp_encoded_box,\n                #                                                    scale_factor=cfgs.ROI_SCALE_FACTORS)\n\n                # 2. clip to img boundaries\n                # tmp_decoded_boxes = boxes_utils.clip_boxes_to_img_boundaries(decode_boxes=tmp_decoded_boxes,\n                #                                                              img_shape=img_shape)\n\n                # 3. NMS\n                keep = nms_rotate.nms_rotate(decode_boxes=tmp_decoded_boxes,\n                                             scores=tmp_score,\n                                             iou_threshold=cfgs.FAST_RCNN_NMS_IOU_THRESHOLD,\n                                             max_output_size=cfgs.FAST_RCNN_NMS_MAX_BOXES_PER_CLASS,\n                                             use_angle_condition=False,\n                                             angle_threshold=15,\n                                             use_gpu=True)\n\n                perclass_boxes = tf.gather(tmp_decoded_boxes, keep)\n                perclass_scores = tf.gather(tmp_score, keep)\n\n                allclasses_boxes.append(perclass_boxes)\n                allclasses_scores.append(perclass_scores)\n                categories.append(tf.ones_like(perclass_scores) * i)\n\n            final_boxes = tf.concat(allclasses_boxes, axis=0)\n            final_scores = tf.concat(allclasses_scores, axis=0)\n            final_category = tf.concat(categories, axis=0)\n\n            # if self.is_training:\n            \'\'\'\n            in training. We should show the detecitons in the tensorboard. So we add this.\n            \'\'\'\n            kept_indices = tf.reshape(tf.where(tf.greater_equal(final_scores, cfgs.SHOW_SCORE_THRSHOLD)), [-1])\n            final_boxes = tf.gather(final_boxes, kept_indices)\n            final_scores = tf.gather(final_scores, kept_indices)\n            final_category = tf.gather(final_category, kept_indices)\n\n        return final_boxes, final_scores, final_category\n\n    def roi_pooling(self, feature_maps, rois, img_shape):\n        \'\'\'\n        Here use roi warping as roi_pooling\n\n        :param featuremaps_dict: feature map to crop\n        :param rois: shape is [-1, 4]. [x1, y1, x2, y2]\n        :return:\n        \'\'\'\n\n        with tf.variable_scope(\'ROI_Warping\'):\n\n            img_h, img_w = tf.cast(img_shape[1], tf.float32), tf.cast(img_shape[2], tf.float32)\n            N = tf.shape(rois)[0]\n            x1, y1, x2, y2 = tf.unstack(rois, axis=1)\n\n            normalized_x1 = x1 / img_w\n            normalized_x2 = x2 / img_w\n            normalized_y1 = y1 / img_h\n            normalized_y2 = y2 / img_h\n\n            normalized_rois = tf.transpose(\n                tf.stack([normalized_y1, normalized_x1, normalized_y2, normalized_x2]), name=\'get_normalized_rois\')\n\n            normalized_rois = tf.stop_gradient(normalized_rois)\n\n            cropped_roi_features = tf.image.crop_and_resize(feature_maps, normalized_rois,\n                                                            box_ind=tf.zeros(shape=[N, ],\n                                                                             dtype=tf.int32),\n                                                            crop_size=[cfgs.ROI_SIZE, cfgs.ROI_SIZE],\n                                                            name=\'CROP_AND_RESIZE\'\n                                                            )\n            roi_features = slim.max_pool2d(cropped_roi_features,\n                                           [cfgs.ROI_POOL_KERNEL_SIZE, cfgs.ROI_POOL_KERNEL_SIZE],\n                                           stride=cfgs.ROI_POOL_KERNEL_SIZE)\n\n        return roi_features\n\n    def build_fastrcnn(self, feature_to_cropped, rois, img_shape):\n\n        with tf.variable_scope(\'Fast-RCNN\'):\n            # 5. ROI Pooling\n            with tf.variable_scope(\'rois_pooling\'):\n\n                rois = boxes_utils.get_horizen_minAreaRectangle(rois, False)\n\n                pooled_features = self.roi_pooling(feature_maps=feature_to_cropped, rois=rois, img_shape=img_shape)\n\n                # xmin, ymin, xmax, ymax = tf.unstack(rois, axis=1)\n                #\n                # h = tf.maximum(ymax - ymin, 0)\n                # w = tf.maximum(xmax - xmin, 0)\n                # x_c = (xmax + xmin) // 2\n                # y_c = (ymax + ymin) // 2\n                # theta = tf.ones_like(h) * -90\n                # rois = tf.transpose(tf.stack([x_c, y_c, h, w, theta]))\n\n            # 6. inferecne rois in Fast-RCNN to obtain fc_flatten features\n            if self.base_network_name.startswith(\'resnet\'):\n                fc_flatten = resnet.restnet_head(input=pooled_features,\n                                                 is_training=self.is_training,\n                                                 scope=self.base_network_name)\n            elif self.base_network_name.startswith(\'MobilenetV2\'):\n                fc_flatten = mobilenet_v2.mobilenetv2_head(inputs=pooled_features,\n                                                           is_training=self.is_training)\n            else:\n                raise NotImplementedError(\'only support resnet and mobilenet\')\n\n            # 7. cls and reg in Fast-RCNN\n            with slim.arg_scope([slim.fully_connected], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                cls_score = slim.fully_connected(fc_flatten,\n                                                 num_outputs=cfgs.CLASS_NUM + 1,\n                                                 weights_initializer=cfgs.INITIALIZER,\n                                                 activation_fn=None, trainable=self.is_training,\n                                                 scope=\'cls_fc\')\n                bbox_pred = slim.fully_connected(fc_flatten,\n                                                 num_outputs=(cfgs.CLASS_NUM + 1) * 5,\n                                                 weights_initializer=cfgs.BBOX_INITIALIZER,\n                                                 activation_fn=None, trainable=self.is_training,\n                                                 scope=\'reg_fc\')\n                # for convient. It also produce (cls_num +1) bboxes\n                cls_score = tf.reshape(cls_score, [-1, cfgs.CLASS_NUM + 1])\n                bbox_pred = tf.reshape(bbox_pred, [-1, 5 * (cfgs.CLASS_NUM + 1)])\n\n            return bbox_pred, cls_score\n\n    def add_anchor_img_smry(self, img, anchors, labels):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.draw_box_with_color_rotate(img, positive_anchor, tf.shape(positive_anchor)[0])\n        neg_in_img = show_box_in_tensor.draw_box_with_color_rotate(img, negative_anchor, tf.shape(positive_anchor)[0])\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def add_roi_batch_img_smry(self, img, rois, labels):\n        positive_roi_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n\n        negative_roi_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        pos_roi = tf.gather(rois, positive_roi_indices)\n        neg_roi = tf.gather(rois, negative_roi_indices)\n\n        pos_in_img = show_box_in_tensor.draw_box_with_color_rotate(img, pos_roi, tf.shape(pos_roi)[0])\n        neg_in_img = show_box_in_tensor.draw_box_with_color_rotate(img, neg_roi, tf.shape(neg_roi)[0])\n\n        tf.summary.image(\'pos_rois\', pos_in_img)\n        tf.summary.image(\'neg_rois\', neg_in_img)\n\n    def build_loss(self, rpn_box_pred, rpn_bbox_targets, rpn_cls_score, rpn_labels,\n                   bbox_pred, bbox_targets, cls_score, labels):\n        \'\'\'\n\n        :param rpn_box_pred: [-1, 4]\n        :param rpn_bbox_targets: [-1, 4]\n        :param rpn_cls_score: [-1]\n        :param rpn_labels: [-1]\n        :param bbox_pred: [-1, 5*(cls_num+1)]\n        :param bbox_targets: [-1, 5*(cls_num+1)]\n        :param cls_score: [-1, cls_num+1]\n        :param labels: [-1]\n        :return:\n        \'\'\'\n        with tf.variable_scope(\'build_loss\') as sc:\n            with tf.variable_scope(\'rpn_loss\'):\n\n                rpn_bbox_loss = losses.smooth_l1_loss_rpn(bbox_pred=rpn_box_pred,\n                                                          bbox_targets=rpn_bbox_targets,\n                                                          label=rpn_labels,\n                                                          sigma=cfgs.RPN_SIGMA)\n                # rpn_cls_loss:\n                # rpn_cls_score = tf.reshape(rpn_cls_score, [-1, 2])\n                # rpn_labels = tf.reshape(rpn_labels, [-1])\n                # ensure rpn_labels shape is [-1]\n                rpn_select = tf.reshape(tf.where(tf.not_equal(rpn_labels, -1)), [-1])\n                rpn_cls_score = tf.reshape(tf.gather(rpn_cls_score, rpn_select), [-1, 2])\n                rpn_labels = tf.reshape(tf.gather(rpn_labels, rpn_select), [-1])\n                rpn_cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rpn_cls_score,\n                                                                                             labels=rpn_labels))\n\n                rpn_cls_loss = rpn_cls_loss * cfgs.RPN_CLASSIFICATION_LOSS_WEIGHT\n                rpn_bbox_loss = rpn_bbox_loss * cfgs.RPN_LOCATION_LOSS_WEIGHT\n\n            with tf.variable_scope(\'FastRCNN_loss\'):\n                if not cfgs.FAST_RCNN_MINIBATCH_SIZE == -1:\n\n                    bbox_loss = losses.smooth_l1_loss_rcnn(bbox_pred=bbox_pred,\n                                                           bbox_targets=bbox_targets,\n                                                           label=labels,\n                                                           num_classes=cfgs.CLASS_NUM + 1,\n                                                           sigma=cfgs.FASTRCNN_SIGMA)\n\n                    cls_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score,\n                                                                                             labels=labels))\n                else:\n                    \'\'\' \n                    applying OHEM here\n                    \'\'\'\n                    print(20 * ""@@"")\n                    print(""@@"" + 10 * "" "" + ""TRAIN WITH OHEM ..."")\n                    print(20 * ""@@"")\n                    cls_loss = bbox_loss = losses.sum_ohem_loss(\n                        cls_score=cls_score,\n                        label=labels,\n                        bbox_targets=bbox_targets,\n                        nr_ohem_sampling=128,\n                        nr_classes=cfgs.CLASS_NUM + 1)\n\n                cls_loss = cls_loss * cfgs.FAST_RCNN_CLASSIFICATION_LOSS_WEIGHT\n                bbox_loss = bbox_loss * cfgs.FAST_RCNN_LOCATION_LOSS_WEIGHT\n            loss_dict = {\n                \'rpn_cls_loss\': rpn_cls_loss,\n                \'rpn_loc_loss\': rpn_bbox_loss,\n                \'fastrcnn_cls_loss\': cls_loss,\n                \'fastrcnn_loc_loss\': bbox_loss,\n            }\n        return loss_dict\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch):\n\n        if self.is_training:\n            # ensure shape is [M, 5]\n            gtboxes_batch = tf.reshape(gtboxes_batch, [-1, 6])\n            gtboxes_batch = tf.cast(gtboxes_batch, tf.float32)\n\n        img_shape = tf.shape(input_img_batch)\n\n        # 1. build base network\n        feature_to_cropped = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        with tf.variable_scope(\'build_rpn\',\n                               regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n\n            rpn_conv3x3 = slim.conv2d(\n                feature_to_cropped, 512, [3, 3],\n                trainable=self.is_training, weights_initializer=cfgs.INITIALIZER,\n                activation_fn=tf.nn.relu,\n                scope=\'rpn_conv/3x3\')\n            rpn_cls_score = slim.conv2d(rpn_conv3x3, self.num_anchors_per_location*2, [1, 1], stride=1,\n                                        trainable=self.is_training, weights_initializer=cfgs.INITIALIZER,\n                                        activation_fn=None,\n                                        scope=\'rpn_cls_score\')\n            rpn_box_pred = slim.conv2d(rpn_conv3x3, self.num_anchors_per_location*5, [1, 1], stride=1,\n                                       trainable=self.is_training, weights_initializer=cfgs.BBOX_INITIALIZER,\n                                       activation_fn=None,\n                                       scope=\'rpn_bbox_pred\')\n            rpn_box_pred = tf.reshape(rpn_box_pred, [-1, 5])\n            rpn_cls_score = tf.reshape(rpn_cls_score, [-1, 2])\n            rpn_cls_prob = slim.softmax(rpn_cls_score, scope=\'rpn_cls_prob\')\n\n        # 3. generate_anchors\n        featuremap_height, featuremap_width = tf.shape(feature_to_cropped)[1], tf.shape(feature_to_cropped)[2]\n        featuremap_height = tf.cast(featuremap_height, tf.float32)\n        featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n        anchors = make_rotate_anchors.make_anchors(base_anchor_size=cfgs.BASE_ANCHOR_SIZE_LIST[0],\n                                                   anchor_scales=cfgs.ANCHOR_SCALES,\n                                                   anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                   anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                   featuremap_height=featuremap_height,\n                                                   featuremap_width=featuremap_width,\n                                                   stride=cfgs.ANCHOR_STRIDE[0],\n                                                   name=""make_anchors_forRPN"")\n\n        # with tf.variable_scope(\'make_anchors\'):\n        #     anchors = anchor_utils.make_anchors(height=featuremap_height,\n        #                                         width=featuremap_width,\n        #                                         feat_stride=cfgs.ANCHOR_STRIDE[0],\n        #                                         anchor_scales=cfgs.ANCHOR_SCALES,\n        #                                         anchor_ratios=cfgs.ANCHOR_RATIOS, base_size=16\n        #                                         )\n\n        # 4. postprocess rpn proposals. such as: decode, clip, NMS\n        with tf.variable_scope(\'postprocess_RPN\'):\n            # rpn_cls_prob = tf.reshape(rpn_cls_score, [-1, 2])\n            # rpn_cls_prob = slim.softmax(rpn_cls_prob, scope=\'rpn_cls_prob\')\n            # rpn_box_pred = tf.reshape(rpn_box_pred, [-1, 4])\n            rois, roi_scores = postprocess_rpn_proposals(rpn_bbox_pred=rpn_box_pred,\n                                                         rpn_cls_prob=rpn_cls_prob,\n                                                         img_shape=img_shape,\n                                                         anchors=anchors,\n                                                         is_training=self.is_training)\n            # rois shape [-1, 4]\n            # +++++++++++++++++++++++++++++++++++++add img smry+++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n            if self.is_training:\n                rois_in_img = show_box_in_tensor.draw_box_with_color_rotate(img_batch=input_img_batch,\n                                                                            boxes=rois,\n                                                                            text=tf.shape(rois)[0])\n                tf.summary.image(\'all_rpn_rois\', rois_in_img)\n\n                score_gre_05 = tf.reshape(tf.where(tf.greater_equal(roi_scores, 0.5)), [-1])\n                score_gre_05_rois = tf.gather(rois, score_gre_05)\n                score_gre_05_score = tf.gather(roi_scores, score_gre_05)\n                score_gre_05_in_img = show_box_in_tensor.draw_box_with_color_rotate(img_batch=input_img_batch,\n                                                                                    boxes=score_gre_05_rois,\n                                                                                    text=tf.shape(score_gre_05_rois)[0])\n                tf.summary.image(\'score_greater_05_rois\', score_gre_05_in_img)\n            # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n        if self.is_training:\n            with tf.variable_scope(\'sample_anchors_minibatch\'):\n\n                # overlaps between the anchors and the gt boxes\n                overlaps = iou_rotate.iou_rotate_calculate(anchors, gtboxes_batch[:, :-1], use_gpu=True, gpu_id=0)\n\n                rpn_labels, rpn_bbox_targets = \\\n                    tf.py_func(\n                        anchor_target_layer,\n                        [gtboxes_batch, anchors, overlaps],\n                        [tf.float32, tf.float32])\n                rpn_bbox_targets = tf.reshape(rpn_bbox_targets, [-1, 5])\n                rpn_labels = tf.to_int32(rpn_labels, name=""to_int32"")\n                rpn_labels = tf.reshape(rpn_labels, [-1])\n                self.add_anchor_img_smry(input_img_batch, anchors, rpn_labels)\n\n            # --------------------------------------add smry-----------------------------------------------------------\n\n            rpn_cls_category = tf.argmax(rpn_cls_prob, axis=1)\n            kept_rpppn = tf.reshape(tf.where(tf.not_equal(rpn_labels, -1)), [-1])\n            rpn_cls_category = tf.gather(rpn_cls_category, kept_rpppn)\n            acc = tf.reduce_mean(tf.to_float(tf.equal(rpn_cls_category, tf.to_int64(tf.gather(rpn_labels, kept_rpppn)))))\n            tf.summary.scalar(\'ACC/rpn_accuracy\', acc)\n\n            with tf.control_dependencies([rpn_labels]):\n                with tf.variable_scope(\'sample_RCNN_minibatch\'):\n\n                    overlaps = iou_rotate.iou_rotate_calculate(rois, gtboxes_batch[:, :-1], use_gpu=True, gpu_id=0)\n\n                    rois, labels, bbox_targets = \\\n                    tf.py_func(proposal_target_layer,\n                               [rois, gtboxes_batch, overlaps],\n                               [tf.float32, tf.float32, tf.float32])\n\n                    rois = tf.reshape(rois, [-1, 5])\n                    labels = tf.to_int32(labels)\n                    labels = tf.reshape(labels, [-1])\n                    bbox_targets = tf.reshape(bbox_targets, [-1, 5*(cfgs.CLASS_NUM+1)])\n                    self.add_roi_batch_img_smry(input_img_batch, rois, labels)\n\n        # -------------------------------------------------------------------------------------------------------------#\n        #                                            Fast-RCNN                                                         #\n        # -------------------------------------------------------------------------------------------------------------#\n\n        # 5. build Fast-RCNN\n        # rois = tf.Print(rois, [tf.shape(rois)], \'rois shape\', summarize=10)\n        bbox_pred, cls_score = self.build_fastrcnn(feature_to_cropped=feature_to_cropped,\n                                                   rois=rois,\n                                                   img_shape=img_shape)\n\n        # bbox_pred shape: [-1, 4*(cls_num+1)].\n        # cls_score shape\xef\xbc\x9a [-1, cls_num+1]\n\n        cls_prob = slim.softmax(cls_score, \'cls_prob\')\n\n        # ----------------------------------------------add smry-------------------------------------------------------\n        if self.is_training:\n            cls_category = tf.argmax(cls_prob, axis=1)\n            fast_acc = tf.reduce_mean(tf.to_float(tf.equal(cls_category, tf.to_int64(labels))))\n            tf.summary.scalar(\'ACC/fast_acc\', fast_acc)\n\n        #  6. postprocess_fastrcnn\n        if not self.is_training:\n            final_boxes, final_scores, final_category = self.postprocess_fastrcnn(rois=rois,\n                                                                                  bbox_ppred=bbox_pred,\n                                                                                  scores=cls_prob,\n                                                                                  img_shape=img_shape)\n            return final_boxes, final_scores, final_category\n        else:\n            \'\'\'\n            when trian. We need build Loss\n            \'\'\'\n            loss_dict = self.build_loss(rpn_box_pred=rpn_box_pred,\n                                        rpn_bbox_targets=rpn_bbox_targets,\n                                        rpn_cls_score=rpn_cls_score,\n                                        rpn_labels=rpn_labels,\n                                        bbox_pred=bbox_pred,\n                                        bbox_targets=bbox_targets,\n                                        cls_score=cls_score,\n                                        labels=labels)\n\n            final_boxes, final_scores, final_category = self.postprocess_fastrcnn(rois=rois,\n                                                                                  bbox_ppred=bbox_pred,\n                                                                                  scores=cls_prob,\n                                                                                  img_shape=img_shape)\n\n            return final_boxes, final_scores, final_category, loss_dict\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n            # print(model_variables)\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name+\'/block4\'):\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""---"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
libs/networks/mobilenet_v2.py,4,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport tensorflow.contrib.slim as slim\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import mobilenet_v2\nfrom libs.networks.mobilenet.mobilenet import training_scope\nfrom libs.networks.mobilenet.mobilenet_v2 import op\nfrom libs.networks.mobilenet.mobilenet_v2 import ops\nexpand_input = ops.expand_input_by_factor\n\nV2_BASE_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16, scope=\'expanded_conv\'),\n        op(ops.expanded_conv, stride=2, num_outputs=24, scope=\'expanded_conv_1\'),\n        op(ops.expanded_conv, stride=1, num_outputs=24, scope=\'expanded_conv_2\'),\n        op(ops.expanded_conv, stride=2, num_outputs=32, scope=\'expanded_conv_3\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_4\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_5\'),\n        op(ops.expanded_conv, stride=2, num_outputs=64, scope=\'expanded_conv_6\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_7\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_8\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_9\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_10\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_11\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_12\')\n    ],\n)\n\n\nV2_HEAD_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(ops.expanded_conv, stride=2, num_outputs=160, scope=\'expanded_conv_13\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_14\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_15\'),\n        op(ops.expanded_conv, stride=1, num_outputs=320, scope=\'expanded_conv_16\'),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280, scope=\'Conv_1\')\n    ],\n)\n\ndef mobilenetv2_scope(is_training=True,\n                      trainable=True,\n                      weight_decay=0.00004,\n                      stddev=0.09,\n                      dropout_keep_prob=0.8,\n                      bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n  In default. We do not use BN\n\n  ReWrite the scope.\n  """"""\n  batch_norm_params = {\n      \'is_training\': False,\n      \'trainable\': False,\n      \'decay\': bn_decay,\n  }\n  with slim.arg_scope(training_scope(is_training=is_training, weight_decay=weight_decay)):\n      with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n                          trainable=trainable):\n          with slim.arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n              return sc\n\n\n\ndef mobilenetv2_base(img_batch, is_training=True):\n\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n\n        feature_to_crop, endpoints = mobilenet_v2.mobilenet_base(input_tensor=img_batch,\n                                                      num_classes=None,\n                                                      is_training=False,\n                                                      depth_multiplier=1.0,\n                                                      scope=\'MobilenetV2\',\n                                                      conv_defs=V2_BASE_DEF,\n                                                      finegrain_classification_mode=False)\n\n        # feature_to_crop = tf.Print(feature_to_crop, [tf.shape(feature_to_crop)], summarize=10, message=\'rpn_shape\')\n        return feature_to_crop\n\n\ndef mobilenetv2_head(inputs, is_training=True):\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n        net, _ = mobilenet_v2.mobilenet(input_tensor=inputs,\n                                        num_classes=None,\n                                        is_training=False,\n                                        depth_multiplier=1.0,\n                                        scope=\'MobilenetV2\',\n                                        conv_defs=V2_HEAD_DEF,\n                                        finegrain_classification_mode=False)\n\n        net = tf.squeeze(net, [1, 2])\n\n        return net'"
libs/networks/resnet.py,10,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.configs import cfgs\n# from slim.nets import resnet_v1, resnet_utils\n# from slim.nets.resnet_v1 import resnet_v1_block\nfrom libs.networks.slim_nets import resnet_v1, resnet_utils\nfrom libs.networks.slim_nets.resnet_v1 import resnet_v1_block\n\n\ndef resnet_arg_scope(\n        is_training=True, weight_decay=cfgs.WEIGHT_DECAY, batch_norm_decay=0.997,\n        batch_norm_epsilon=1e-5, batch_norm_scale=True):\n    \'\'\'\n\n    In Default, we do not use BN to train resnet, since batch_size is too small.\n    So is_training is False and trainable is False in the batch_norm params.\n\n    \'\'\'\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS\n    }\n\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef modified_resnet_v1_block(scope, bottleneck, parameter_tuple_list):\n  """"""\n  Different from The Impletation of Slim. We changed it to same as KaiMing\'s Paper.\n  Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  parameter_list = []\n  for a_tuple in parameter_tuple_list:\n      parameter_list.append(\n          {\n              \'depth\': a_tuple[0],\n              \'depth_bottleneck\': a_tuple[1],\n              \'stride\': a_tuple[2]\n          }\n      )\n  return resnet_utils.Block(scope, bottleneck, parameter_list)\n\n\ndef resnet_50_base(img_batch, is_training=True):\n    \'\'\'\n    this code is from light-head rcnn.\n\n    It is convenient to freeze blocks.\n    @author: jemmy li\n    @contact: zengarden2009@gmail.com\n    \'\'\'\n\n    bottleneck = resnet_v1.bottleneck\n    #\n    blocks = [\n        modified_resnet_v1_block(\'block1\', bottleneck,\n                           [(256, 64, 1, 1)] * 2 + [(256, 64, 1, 1)]),\n        modified_resnet_v1_block(\'block2\', bottleneck,\n                           [(512, 128, 2, 1)] + [(512, 128, 1, 1)] * 3),\n        modified_resnet_v1_block(\'block3\', bottleneck,\n                           [(1024, 256, 2, 1)] + [(1024, 256, 1, 1)] * 5)\n    ]\n\n    with slim.arg_scope(resnet_arg_scope(is_training=False)):  # freeze the base conv_net\n        with tf.variable_scope(\'resnet_v1_50\', \'resnet_v1_50\'):\n            net = resnet_utils.conv2d_same(\n                img_batch, 64, 7, stride=2, scope=\'conv1\')\n            net = slim.max_pool2d(\n                net, [3, 3], stride=2, padding=\'SAME\', scope=\'pool1\')\n\n    not_freezed = [False] * cfgs.FIXED_BLOCKS + (4-cfgs.FIXED_BLOCKS)*[True]\n    # Fixed_Blocks can be 1~3\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[0]))):\n        C2, _ = resnet_v1.resnet_v1(\n            net, blocks[0:1], global_pool=False, include_root_block=False,\n            scope=\'resnet_v1_50\')\n\n    # C2 = tf.Print(C2, [tf.shape(C2)], summarize=10, message=\'C2_shape\')\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[1]))):\n        C3, _ = resnet_v1.resnet_v1(\n            C2, blocks[1:2], global_pool=False, include_root_block=False,\n            scope=\'resnet_v1_50\')\n\n    # C3 = tf.Print(C3, [tf.shape(C3)], summarize=10, message=\'C3_shape\')\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[2]))):\n        C4, _ = resnet_v1.resnet_v1(\n            C3, blocks[2:3], global_pool=False,\n            include_root_block=False, scope=\'resnet_v1_50\')\n\n    # C4 = tf.Print(C4, [tf.shape(C4)], summarize=10, message=\'C4_shape\')\n    return C4\n\n\ndef resnet_101_base(img_batch, is_training):\n    \'\'\'\n        this code is from light-head rcnn.\n\n        It is convenient to freeze blocks.\n        @author: jemmy li\n        @contact: zengarden2009@gmail.com\n    \'\'\'\n\n    bottleneck = resnet_v1.bottleneck\n    blocks = [\n        modified_resnet_v1_block(\'block1\', bottleneck,\n                                 [(256, 64, 1, 1)] * 2 + [(256, 64, 1, 1)]),\n        modified_resnet_v1_block(\'block2\', bottleneck,\n                                 [(512, 128, 2, 1)] + [(512, 128, 1, 1)] * 3),\n        modified_resnet_v1_block(\'block3\', bottleneck,\n                                 [(1024, 256, 2, 1)] + [(1024, 256, 1, 1)] * 22)\n    ]\n    with slim.arg_scope(resnet_arg_scope(is_training=False)):  # freeze the base conv_net\n        with tf.variable_scope(\'resnet_v1_101\', \'resnet_v1_101\'):\n            net = resnet_utils.conv2d_same(\n                img_batch, 64, 7, stride=2, scope=\'conv1\')\n            net = slim.max_pool2d(\n                net, [3, 3], stride=2, padding=\'SAME\', scope=\'pool1\')\n\n    not_freezed = [False] * cfgs.FIXED_BLOCKS + (4 - cfgs.FIXED_BLOCKS) * [True]\n    # Fixed_Blocks can be 1~3\n\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training and not_freezed[0])):\n        C2, _ = resnet_v1.resnet_v1(\n            net, blocks[0:1], global_pool=False, include_root_block=False,\n            scope=\'resnet_v1_101\')\n\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training and not_freezed[1])):\n        C3, _ = resnet_v1.resnet_v1(\n            C2, blocks[1:2], global_pool=False, include_root_block=False,\n            scope=\'resnet_v1_101\')\n\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training and not_freezed[2])):\n        C4, _ = resnet_v1.resnet_v1(\n            C3, blocks[2:3], global_pool=False,\n            include_root_block=False, scope=\'resnet_v1_101\')\n\n    return C4\n\n\ndef restnet_head(input, is_training, scope):\n\n    block4 = [modified_resnet_v1_block(\'block4\', resnet_v1.bottleneck,\n                           [(2048, 512, 1, 2)] + [(2048, 512, 1, 2)] * 2)]\n\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n        C5_flatten, _ = resnet_v1.resnet_v1(\n            input, block4, global_pool=True,\n            spatial_squeeze=True,\n            include_root_block=False, scope=scope)\n    # global average pooling C5 to obtain fc layers\n    return C5_flatten\n\n\ndef build_feature_pyramid(feature_maps_dict):\n\n    \'\'\'\n    reference: https://github.com/CharlesShang/FastMaskRCNN\n    build P2, P3, P4, P5\n    :return: multi-scale feature map\n    \'\'\'\n\n    feature_pyramid = {}\n    with tf.variable_scope(\'build_feature_pyramid\'):\n        feature_pyramid[\'P4\'] = slim.conv2d(feature_maps_dict[\'C4\'],\n                                            num_outputs=512,\n                                            kernel_size=[1, 1],\n                                            stride=1,\n                                            scope=\'build_P4\')\n\n        feature_pyramid[\'P5\'] = slim.max_pool2d(feature_pyramid[\'P4\'],\n                                                kernel_size=[2, 2], stride=2, scope=\'build_P5\')\n        # P6 is down sample of P5\n\n        for layer in range(3, 1, -1):\n            p, c = feature_pyramid[\'P\' + str(layer + 1)], feature_maps_dict[\'C\' + str(layer)]\n            up_sample_shape = tf.shape(c)\n            up_sample = tf.image.resize_nearest_neighbor(p, [up_sample_shape[1], up_sample_shape[2]],\n                                                         name=\'build_P%d/up_sample_nearest_neighbor\' % layer)\n\n            c = slim.conv2d(c, num_outputs=512, kernel_size=[1, 1], stride=1,\n                            scope=\'build_P%d/reduce_dimension\' % layer)\n            p = up_sample + c\n            p = slim.conv2d(p, 512, kernel_size=[3, 3], stride=1,\n                            padding=\'SAME\', scope=\'build_P%d/avoid_aliasing\' % layer)\n            feature_pyramid[\'P\' + str(layer)] = p\n\n    return feature_pyramid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
libs/val_libs/__init__.py,0,b''
libs/val_libs/voc_eval_r.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\nfrom libs.configs import cfgs\nfrom libs.box_utils import iou_rotate\nfrom libs.box_utils import coordinate_convert\nfrom help_utils import tools\n\n\ndef _write_voc_results_file(all_boxes, test_imgid_list, det_save_path):\n  for cls, cls_ind in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    print(\'Writing {} VOC results file\'.format(cls))\n\n    with open(det_save_path, \'wt\') as f:\n      for im_ind, index in enumerate(test_imgid_list):\n        dets = all_boxes[cls_ind][im_ind]\n        if dets == []:\n          continue\n        # the VOCdevkit expects 1-based indices\n        for k in range(dets.shape[0]):\n          f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                  format(index, dets[k, -1],\n                         dets[k, 0] + 1, dets[k, 1] + 1,\n                         dets[k, 2] + 1, dets[k, 3] + 1, dets[k, 4] + 1))\n\n\ndef write_voc_results_file(all_boxes, test_imgid_list, det_save_dir):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n  the detections is a array. shape is [-1, 7]. [category, score, x, y, w, h, theta]\n  Note that: if none detections in this img. that the detetions is : []\n\n  :param test_imgid_list:\n  :param det_save_path:\n  :return:\n  \'\'\'\n  for cls, cls_id in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    print(""Writing {} VOC resutls file"".format(cls))\n\n    tools.mkdir(det_save_dir)\n    det_save_path = os.path.join(det_save_dir, ""det_""+cls+"".txt"")\n    with open(det_save_path, \'wt\') as f:\n      for index, img_name in enumerate(test_imgid_list):\n        this_img_detections = all_boxes[index]\n\n        this_cls_detections = this_img_detections[this_img_detections[:, 0] == cls_id]\n        if this_cls_detections.shape[0] == 0:\n          continue # this cls has none detections in this img\n        for a_det in this_cls_detections:\n          f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                  format(img_name, a_det[1],\n                         a_det[2], a_det[3],\n                         a_det[4], a_det[5], a_det[6]))  # that is [img_name, score, x, y, w, h, theta]\n\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    bbox = obj.find(\'bndbox\')\n    rbox = [int(bbox.find(\'x0\').text), int(bbox.find(\'y0\').text), int(bbox.find(\'x1\').text),\n            int(bbox.find(\'y1\').text), int(bbox.find(\'x2\').text), int(bbox.find(\'y2\').text),\n            int(bbox.find(\'x3\').text), int(bbox.find(\'y3\').text)]\n    rbox = np.array([rbox], np.float32)\n    rbox = coordinate_convert.back_forward_convert(rbox, with_label=False)\n    obj_struct[\'bbox\'] = rbox\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath, annopath, test_imgid_list, cls_name, ovthresh=0.5,\n                 use_07_metric=False, use_diff=False):\n  \'\'\'\n\n  :param detpath:\n  :param annopath:\n  :param test_imgid_list: it \'s a list that contains the img_name of test_imgs\n  :param cls_name:\n  :param ovthresh:\n  :param use_07_metric:\n  :param use_diff:\n  :return:\n  \'\'\'\n  # 1. parse xml to get gtboxes\n\n  # read list of images\n  imagenames = test_imgid_list\n\n  recs = {}\n  for i, imagename in enumerate(imagenames):\n    recs[imagename] = parse_rec(os.path.join(annopath, imagename+\'.xml\'))\n    # if i % 100 == 0:\n    #   print(\'Reading annotation for {:d}/{:d}\'.format(\n    #     i + 1, len(imagenames)))\n\n  # 2. get gtboxes for this class.\n  class_recs = {}\n  num_pos = 0\n  # if cls_name == \'person\':\n  #   print (""aaa"")\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == cls_name]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    if use_diff:\n      difficult = np.array([False for x in R]).astype(np.bool)\n    else:\n      difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    num_pos = num_pos + sum(~difficult)  # ignored the diffcult boxes\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det} # det means that gtboxes has already been detected\n\n  # 3. read the detection file\n  detfile = os.path.join(detpath, ""det_""+cls_name+"".txt"")\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  # for a line. that is [img_name, confidence, xmin, ymin, xmax, ymax]\n  splitlines = [x.strip().split(\' \') for x in lines]  # a list that include a list\n  image_ids = [x[0] for x in splitlines]  # img_id is img_name\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids) # num of detections. That, a line is a det_box.\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]  #reorder the img_name\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]  # img_id is img_name\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        # ixmin = np.maximum(BBGT[:, 0], bb[0])\n        # iymin = np.maximum(BBGT[:, 1], bb[1])\n        # ixmax = np.minimum(BBGT[:, 2], bb[2])\n        # iymax = np.minimum(BBGT[:, 3], bb[3])\n        # iw = np.maximum(ixmax - ixmin + 1., 0.)\n        # ih = np.maximum(iymax - iymin + 1., 0.)\n        # inters = iw * ih\n        #\n        # # union\n        # uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n        #        (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n        #        (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n        #\n        # overlaps = inters / uni\n        overlaps = []\n        for i in range(len(BBGT)):\n          overlap = iou_rotate.iou_rotate_calculate1(np.array([bb]),\n                                                      BBGT[i],\n                                                      use_gpu=False)[0]\n          overlaps.append(overlap)\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # 4. get recall, precison and AP\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(num_pos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric)\n\n  return rec, prec, ap\n\n\ndef do_python_eval(test_imgid_list, test_annotation_path):\n  import matplotlib.colors as colors\n  import matplotlib.pyplot as plt\n\n  AP_list = []\n  for cls, index in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    recall, precision, AP = voc_eval(detpath=cfgs.EVALUATE_R_DIR,\n                                     test_imgid_list=test_imgid_list,\n                                     cls_name=cls,\n                                     annopath=test_annotation_path)\n    AP_list += [AP]\n    print(""cls : {}|| Recall: {} || Precison: {}|| AP: {}"".format(cls, recall[-1], precision[-1], AP))\n    # print(""{}_ap: {}"".format(cls, AP))\n    # print(""{}_recall: {}"".format(cls, recall[-1]))\n    # print(""{}_precision: {}"".format(cls, precision[-1]))\n\n    c = colors.cnames.keys()\n    c_dark = list(filter(lambda x: x.startswith(\'dark\'), c))\n    c = [\'red\', \'orange\']\n    plt.axis([0, 1.2, 0, 1])\n    plt.plot(recall, precision, color=c_dark[index], label=cls)\n\n  plt.legend(loc=\'upper right\')\n  plt.xlabel(\'R\')\n  plt.ylabel(\'P\')\n  plt.savefig(\'./PR_R.png\')\n\n  print(""mAP is : {}"".format(np.mean(AP_list)))\n\n\ndef voc_evaluate_detections(all_boxes, test_imgid_list, test_annotation_path):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n\n  The detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n  :return:\n  \'\'\'\n\n  write_voc_results_file(all_boxes, test_imgid_list=test_imgid_list,\n                         det_save_dir=cfgs.EVALUATE_R_DIR)\n  do_python_eval(test_imgid_list, test_annotation_path)\n\n\n\n\n\n\n\n\n# def voc_eval(detpath,\n#              annopath,\n#              imagesetfile,\n#              classname,\n#              cachedir,\n#              ovthresh=0.5,\n#              use_07_metric=False,\n#              use_diff=False):\n#   """"""rec, prec, ap = voc_eval(detpath,\n#                               annopath,\n#                               imagesetfile,\n#                               classname,\n#                               [ovthresh],\n#                               [use_07_metric])\n#\n#   Top level function that does the PASCAL VOC evaluation.\n#\n#   detpath: Path to detections\n#       detpath.format(classname) should produce the detection results file.\n#   annopath: Path to annotations\n#       annopath.format(imagename) should be the xml annotations file.\n#   imagesetfile: Text file containing the list of images, one image per line.\n#   classname: Category name (duh)\n#   cachedir: Directory for caching the annotations\n#   [ovthresh]: Overlap threshold (default = 0.5)\n#   [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n#       (default False)\n#   """"""\n#   # assumes detections are in detpath.format(classname)\n#   # assumes annotations are in annopath.format(imagename)\n#   # assumes imagesetfile is a text file with each line an image name\n#   # cachedir caches the annotations in a pickle file\n#\n#   # first load gt\n#   if not os.path.isdir(cachedir):\n#     os.mkdir(cachedir)\n#   cachefile = os.path.join(cachedir, \'%s_annots.pkl\' % imagesetfile)\n#   # read list of images\n#   with open(imagesetfile, \'r\') as f:\n#     lines = f.readlines()\n#   imagenames = [x.strip() for x in lines]\n#\n#   if not os.path.isfile(cachefile):\n#     # load annotations\n#     recs = {}\n#     for i, imagename in enumerate(imagenames):\n#       recs[imagename] = parse_rec(annopath.format(imagename))\n#       if i % 100 == 0:\n#         print(\'Reading annotation for {:d}/{:d}\'.format(\n#           i + 1, len(imagenames)))\n#     # save\n#     print(\'Saving cached annotations to {:s}\'.format(cachefile))\n#     with open(cachefile, \'w\') as f:\n#       pickle.dump(recs, f)\n#   else:\n#     # load\n#     with open(cachefile, \'rb\') as f:\n#       try:\n#         recs = pickle.load(f)\n#       except:\n#         recs = pickle.load(f, encoding=\'bytes\')\n#\n#   # extract gt objects for this class\n#   class_recs = {}\n#   npos = 0\n#   for imagename in imagenames:\n#     R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n#     bbox = np.array([x[\'bbox\'] for x in R])\n#     if use_diff:\n#       difficult = np.array([False for x in R]).astype(np.bool)\n#     else:\n#       difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n#     det = [False] * len(R)\n#     npos = npos + sum(~difficult)\n#     class_recs[imagename] = {\'bbox\': bbox,\n#                              \'difficult\': difficult,\n#                              \'det\': det}\n#\n#   # read dets\n#   detfile = detpath.format(classname)\n#   with open(detfile, \'r\') as f:\n#     lines = f.readlines()\n#\n#   splitlines = [x.strip().split(\' \') for x in lines]\n#   image_ids = [x[0] for x in splitlines]\n#   confidence = np.array([float(x[1]) for x in splitlines])\n#   BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n#\n#   nd = len(image_ids)\n#   tp = np.zeros(nd)\n#   fp = np.zeros(nd)\n#\n#   if BB.shape[0] > 0:\n#     # sort by confidence\n#     sorted_ind = np.argsort(-confidence)\n#     sorted_scores = np.sort(-confidence)\n#     BB = BB[sorted_ind, :]\n#     image_ids = [image_ids[x] for x in sorted_ind]\n#\n#     # go down dets and mark TPs and FPs\n#     for d in range(nd):\n#       R = class_recs[image_ids[d]]\n#       bb = BB[d, :].astype(float)\n#       ovmax = -np.inf\n#       BBGT = R[\'bbox\'].astype(float)\n#\n#       if BBGT.size > 0:\n#         # compute overlaps\n#         # intersection\n#         ixmin = np.maximum(BBGT[:, 0], bb[0])\n#         iymin = np.maximum(BBGT[:, 1], bb[1])\n#         ixmax = np.minimum(BBGT[:, 2], bb[2])\n#         iymax = np.minimum(BBGT[:, 3], bb[3])\n#         iw = np.maximum(ixmax - ixmin + 1., 0.)\n#         ih = np.maximum(iymax - iymin + 1., 0.)\n#         inters = iw * ih\n#\n#         # union\n#         uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n#                (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n#                (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n#\n#         overlaps = inters / uni\n#         ovmax = np.max(overlaps)\n#         jmax = np.argmax(overlaps)\n#\n#       if ovmax > ovthresh:\n#         if not R[\'difficult\'][jmax]:\n#           if not R[\'det\'][jmax]:\n#             tp[d] = 1.\n#             R[\'det\'][jmax] = 1\n#           else:\n#             fp[d] = 1.\n#       else:\n#         fp[d] = 1.\n#\n#   # compute precision recall\n#   fp = np.cumsum(fp)\n#   tp = np.cumsum(tp)\n#   rec = tp / float(npos)\n#   # avoid divide by zero in case the first detection matches a difficult\n#   # ground truth\n#   prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n#   ap = voc_ap(rec, prec, use_07_metric)\n#\n#   return rec, prec, ap\n'"
data/io/DOTA/train_crop.py,0,"b'import os\nimport scipy.misc as misc\nfrom xml.dom.minidom import Document\nimport numpy as np\nimport copy, cv2\n\ndef save_to_xml(save_path, im_height, im_width, objects_axis, label_name):\n    im_depth = 0\n    object_num = len(objects_axis)\n    doc = Document()\n\n    annotation = doc.createElement(\'annotation\')\n    doc.appendChild(annotation)\n\n    folder = doc.createElement(\'folder\')\n    folder_name = doc.createTextNode(\'VOC2007\')\n    folder.appendChild(folder_name)\n    annotation.appendChild(folder)\n\n    filename = doc.createElement(\'filename\')\n    filename_name = doc.createTextNode(\'000024.jpg\')\n    filename.appendChild(filename_name)\n    annotation.appendChild(filename)\n\n    source = doc.createElement(\'source\')\n    annotation.appendChild(source)\n\n    database = doc.createElement(\'database\')\n    database.appendChild(doc.createTextNode(\'The VOC2007 Database\'))\n    source.appendChild(database)\n\n    annotation_s = doc.createElement(\'annotation\')\n    annotation_s.appendChild(doc.createTextNode(\'PASCAL VOC2007\'))\n    source.appendChild(annotation_s)\n\n    image = doc.createElement(\'image\')\n    image.appendChild(doc.createTextNode(\'flickr\'))\n    source.appendChild(image)\n\n    flickrid = doc.createElement(\'flickrid\')\n    flickrid.appendChild(doc.createTextNode(\'322409915\'))\n    source.appendChild(flickrid)\n\n    owner = doc.createElement(\'owner\')\n    annotation.appendChild(owner)\n\n    flickrid_o = doc.createElement(\'flickrid\')\n    flickrid_o.appendChild(doc.createTextNode(\'knautia\'))\n    owner.appendChild(flickrid_o)\n\n    name_o = doc.createElement(\'name\')\n    name_o.appendChild(doc.createTextNode(\'yang\'))\n    owner.appendChild(name_o)\n\n\n    size = doc.createElement(\'size\')\n    annotation.appendChild(size)\n    width = doc.createElement(\'width\')\n    width.appendChild(doc.createTextNode(str(im_width)))\n    height = doc.createElement(\'height\')\n    height.appendChild(doc.createTextNode(str(im_height)))\n    depth = doc.createElement(\'depth\')\n    depth.appendChild(doc.createTextNode(str(im_depth)))\n    size.appendChild(width)\n    size.appendChild(height)\n    size.appendChild(depth)\n    segmented = doc.createElement(\'segmented\')\n    segmented.appendChild(doc.createTextNode(\'0\'))\n    annotation.appendChild(segmented)\n    for i in range(object_num):\n        objects = doc.createElement(\'object\')\n        annotation.appendChild(objects)\n        object_name = doc.createElement(\'name\')\n        object_name.appendChild(doc.createTextNode(label_name[int(objects_axis[i][-1])]))\n        objects.appendChild(object_name)\n        pose = doc.createElement(\'pose\')\n        pose.appendChild(doc.createTextNode(\'Unspecified\'))\n        objects.appendChild(pose)\n        truncated = doc.createElement(\'truncated\')\n        truncated.appendChild(doc.createTextNode(\'1\'))\n        objects.appendChild(truncated)\n        difficult = doc.createElement(\'difficult\')\n        difficult.appendChild(doc.createTextNode(\'0\'))\n        objects.appendChild(difficult)\n        bndbox = doc.createElement(\'bndbox\')\n        objects.appendChild(bndbox)\n        \n        x0 = doc.createElement(\'x0\')\n        x0.appendChild(doc.createTextNode(str((objects_axis[i][0]))))\n        bndbox.appendChild(x0)\n        y0 = doc.createElement(\'y0\')\n        y0.appendChild(doc.createTextNode(str((objects_axis[i][1]))))\n        bndbox.appendChild(y0)\n\n        x1 = doc.createElement(\'x1\')\n        x1.appendChild(doc.createTextNode(str((objects_axis[i][2]))))\n        bndbox.appendChild(x1)\n        y1 = doc.createElement(\'y1\')\n        y1.appendChild(doc.createTextNode(str((objects_axis[i][3]))))\n        bndbox.appendChild(y1)\n        \n        x2 = doc.createElement(\'x2\')\n        x2.appendChild(doc.createTextNode(str((objects_axis[i][4]))))\n        bndbox.appendChild(x2)\n        y2 = doc.createElement(\'y2\')\n        y2.appendChild(doc.createTextNode(str((objects_axis[i][5]))))\n        bndbox.appendChild(y2)\n\n        x3 = doc.createElement(\'x3\')\n        x3.appendChild(doc.createTextNode(str((objects_axis[i][6]))))\n        bndbox.appendChild(x3)\n        y3 = doc.createElement(\'y3\')\n        y3.appendChild(doc.createTextNode(str((objects_axis[i][7]))))\n        bndbox.appendChild(y3)\n        \n    f = open(save_path,\'w\')\n    f.write(doc.toprettyxml(indent = \'\'))\n    f.close() \n\nclass_list = [\'plane\', \'baseball-diamond\', \'bridge\', \'ground-track-field\', \n\'small-vehicle\', \'large-vehicle\', \'ship\', \n\'tennis-court\', \'basketball-court\',  \n\'storage-tank\', \'soccer-ball-field\', \n\'roundabout\', \'harbor\', \n\'swimming-pool\', \'helicopter\']\n\n\n\n\ndef format_label(txt_list):\n    format_data = []\n    for i in txt_list[2:]:\n        format_data.append(\n        [int(xy) for xy in i.split(\' \')[:8]] + [class_list.index(i.split(\' \')[8])]\n        # {\'x0\': int(i.split(\' \')[0]),\n        # \'x1\': int(i.split(\' \')[2]),\n        # \'x2\': int(i.split(\' \')[4]),\n        # \'x3\': int(i.split(\' \')[6]),\n        # \'y1\': int(i.split(\' \')[1]),\n        # \'y2\': int(i.split(\' \')[3]),\n        # \'y3\': int(i.split(\' \')[5]),\n        # \'y4\': int(i.split(\' \')[7]),\n        # \'class\': class_list.index(i.split(\' \')[8]) if i.split(\' \')[8] in class_list else 0, \n        # \'difficulty\': int(i.split(\' \')[9])}\n        )\n        if i.split(\' \')[8] not in class_list :\n            print (\'warning found a new label :\', i.split(\' \')[8])\n            exit()\n    return np.array(format_data)\n\ndef clip_image(file_idx, image, boxes_all, width, height):\n    # print (\'image shape\', image.shape)\n    if len(boxes_all) > 0:\n        shape = image.shape\n        for start_h in range(0, shape[0], 256):\n            for start_w in range(0, shape[1], 256):\n                boxes = copy.deepcopy(boxes_all)\n                box = np.zeros_like(boxes_all)\n                start_h_new = start_h\n                start_w_new = start_w\n                if start_h + height > shape[0]:\n                  start_h_new = shape[0] - height\n                if start_w + width > shape[1]:\n                  start_w_new = shape[1] - width\n                top_left_row = max(start_h_new, 0)\n                top_left_col = max(start_w_new, 0)\n                bottom_right_row = min(start_h + height, shape[0])\n                bottom_right_col = min(start_w + width, shape[1])\n\n\n                subImage = image[top_left_row:bottom_right_row, top_left_col: bottom_right_col]\n\n                box[:, 0] = boxes[:, 0] - top_left_col\n                box[:, 2] = boxes[:, 2] - top_left_col\n                box[:, 4] = boxes[:, 4] - top_left_col\n                box[:, 6] = boxes[:, 6] - top_left_col\n\n                box[:, 1] = boxes[:, 1] - top_left_row\n                box[:, 3] = boxes[:, 3] - top_left_row\n                box[:, 5] = boxes[:, 5] - top_left_row\n                box[:, 7] = boxes[:, 7] - top_left_row\n                box[:, 8] = boxes[:, 8]\n                center_y = 0.25*(box[:, 1] + box[:, 3] + box[:, 5] + box[:, 7])\n                center_x = 0.25*(box[:, 0] + box[:, 2] + box[:, 4] + box[:, 6])\n                # print(\'center_y\', center_y)\n                # print(\'center_x\', center_x)\n                # print (\'boxes\', boxes)\n                # print (\'boxes_all\', boxes_all)\n                # print (\'top_left_col\', top_left_col, \'top_left_row\', top_left_row)\n\n                cond1 = np.intersect1d(np.where(center_y[:] >=0 )[0], np.where(center_x[:] >=0 )[0])\n                cond2 = np.intersect1d(np.where(center_y[:] <= (bottom_right_row - top_left_row))[0],\n                                        np.where(center_x[:] <= (bottom_right_col - top_left_col))[0])\n                idx = np.intersect1d(cond1, cond2)\n                # idx = np.where(center_y[:]>=0 and center_x[:]>=0 and center_y[:] <= (bottom_right_row - top_left_row) and center_x[:] <= (bottom_right_col - top_left_col))[0]\n                # save_path, im_width, im_height, objects_axis, label_name\n                if len(idx) > 0:\n                    xml = os.path.join(save_dir, \'labeltxt\', ""%s_%04d_%04d.xml"" % (file_idx, top_left_row, top_left_col))\n                    save_to_xml(xml, subImage.shape[0], subImage.shape[1], box[idx, :], class_list)\n                    # print (\'save xml : \', xml)\n                    if subImage.shape[0] > 5 and subImage.shape[1] >5:\n                        img = os.path.join(save_dir, \'images\', ""%s_%04d_%04d.png"" % (file_idx, top_left_row, top_left_col))\n                        cv2.imwrite(img, subImage)\n        \n    \n    \n\nprint (\'class_list\', len(class_list))\nraw_data = \'/dataset/DOTA/train/\'\nraw_images_dir = os.path.join(raw_data, \'images\')\nraw_label_dir = os.path.join(raw_data, \'labelTxt\')\n\nsave_dir = \'/dataset/DOTA_clip/train/\'\n\nimages = [i for i in os.listdir(raw_images_dir) if \'png\' in i]\nlabels = [i for i in os.listdir(raw_label_dir) if \'txt\' in i]\n\nprint (\'find image\', len(images))\nprint (\'find label\', len(labels))\n\nmin_length = 1e10\nmax_length = 1\n\nfor idx, img in enumerate(images):\n# img = \'P1524.png\'\n    print (idx, \'read image\', img)\n    img_data = misc.imread(os.path.join(raw_images_dir, img))\n    # img_data = cv2.imread(os.path.join(raw_images_dir, img))\n    # if len(img_data.shape) == 2:\n        # img_data = img_data[:, :, np.newaxis]\n        # print (\'find gray image\')\n\n    txt_data = open(os.path.join(raw_label_dir, img.replace(\'png\', \'txt\')), \'r\').readlines()\n    # print (idx, len(format_label(txt_data)), img_data.shape)\n    # if max(img_data.shape[:2]) > max_length:\n        # max_length = max(img_data.shape[:2])\n    # if min(img_data.shape[:2]) < min_length:\n        # min_length = min(img_data.shape[:2])\n    # if idx % 50 ==0:\n        # print (idx, len(format_label(txt_data)), img_data.shape)\n        # print (idx, \'min_length\', min_length, \'max_length\', max_length)\n    box = format_label(txt_data)\n    clip_image(img.strip(\'.png\'), img_data, box, 800, 800)\n    \n    \n    \n    \n    \n#     rm train/images/*   &&   rm train/labeltxt/*\n\n    \n    \n    \n\n\n\n\n\n\n\n\n'"
data/io/DOTA/val_crop.py,0,"b'import os\nimport scipy.misc as misc\nfrom xml.dom.minidom import Document\nimport numpy as np\nimport copy, cv2\n\ndef save_to_xml(save_path, im_height, im_width, objects_axis, label_name):\n    im_depth = 0\n    object_num = len(objects_axis)\n    doc = Document()\n\n    annotation = doc.createElement(\'annotation\')\n    doc.appendChild(annotation)\n\n    folder = doc.createElement(\'folder\')\n    folder_name = doc.createTextNode(\'VOC2007\')\n    folder.appendChild(folder_name)\n    annotation.appendChild(folder)\n\n    filename = doc.createElement(\'filename\')\n    filename_name = doc.createTextNode(\'000024.jpg\')\n    filename.appendChild(filename_name)\n    annotation.appendChild(filename)\n\n    source = doc.createElement(\'source\')\n    annotation.appendChild(source)\n\n    database = doc.createElement(\'database\')\n    database.appendChild(doc.createTextNode(\'The VOC2007 Database\'))\n    source.appendChild(database)\n\n    annotation_s = doc.createElement(\'annotation\')\n    annotation_s.appendChild(doc.createTextNode(\'PASCAL VOC2007\'))\n    source.appendChild(annotation_s)\n\n    image = doc.createElement(\'image\')\n    image.appendChild(doc.createTextNode(\'flickr\'))\n    source.appendChild(image)\n\n    flickrid = doc.createElement(\'flickrid\')\n    flickrid.appendChild(doc.createTextNode(\'322409915\'))\n    source.appendChild(flickrid)\n\n    owner = doc.createElement(\'owner\')\n    annotation.appendChild(owner)\n\n    flickrid_o = doc.createElement(\'flickrid\')\n    flickrid_o.appendChild(doc.createTextNode(\'knautia\'))\n    owner.appendChild(flickrid_o)\n\n    name_o = doc.createElement(\'name\')\n    name_o.appendChild(doc.createTextNode(\'yang\'))\n    owner.appendChild(name_o)\n\n\n    size = doc.createElement(\'size\')\n    annotation.appendChild(size)\n    width = doc.createElement(\'width\')\n    width.appendChild(doc.createTextNode(str(im_width)))\n    height = doc.createElement(\'height\')\n    height.appendChild(doc.createTextNode(str(im_height)))\n    depth = doc.createElement(\'depth\')\n    depth.appendChild(doc.createTextNode(str(im_depth)))\n    size.appendChild(width)\n    size.appendChild(height)\n    size.appendChild(depth)\n    segmented = doc.createElement(\'segmented\')\n    segmented.appendChild(doc.createTextNode(\'0\'))\n    annotation.appendChild(segmented)\n    for i in range(object_num):\n        objects = doc.createElement(\'object\')\n        annotation.appendChild(objects)\n        object_name = doc.createElement(\'name\')\n        object_name.appendChild(doc.createTextNode(label_name[int(objects_axis[i][-1])]))\n        objects.appendChild(object_name)\n        pose = doc.createElement(\'pose\')\n        pose.appendChild(doc.createTextNode(\'Unspecified\'))\n        objects.appendChild(pose)\n        truncated = doc.createElement(\'truncated\')\n        truncated.appendChild(doc.createTextNode(\'1\'))\n        objects.appendChild(truncated)\n        difficult = doc.createElement(\'difficult\')\n        difficult.appendChild(doc.createTextNode(\'0\'))\n        objects.appendChild(difficult)\n        bndbox = doc.createElement(\'bndbox\')\n        objects.appendChild(bndbox)\n        \n        x0 = doc.createElement(\'x0\')\n        x0.appendChild(doc.createTextNode(str((objects_axis[i][0]))))\n        bndbox.appendChild(x0)\n        y0 = doc.createElement(\'y0\')\n        y0.appendChild(doc.createTextNode(str((objects_axis[i][1]))))\n        bndbox.appendChild(y0)\n\n        x1 = doc.createElement(\'x1\')\n        x1.appendChild(doc.createTextNode(str((objects_axis[i][2]))))\n        bndbox.appendChild(x1)\n        y1 = doc.createElement(\'y1\')\n        y1.appendChild(doc.createTextNode(str((objects_axis[i][3]))))\n        bndbox.appendChild(y1)\n        \n        x2 = doc.createElement(\'x2\')\n        x2.appendChild(doc.createTextNode(str((objects_axis[i][4]))))\n        bndbox.appendChild(x2)\n        y2 = doc.createElement(\'y2\')\n        y2.appendChild(doc.createTextNode(str((objects_axis[i][5]))))\n        bndbox.appendChild(y2)\n\n        x3 = doc.createElement(\'x3\')\n        x3.appendChild(doc.createTextNode(str((objects_axis[i][6]))))\n        bndbox.appendChild(x3)\n        y3 = doc.createElement(\'y3\')\n        y3.appendChild(doc.createTextNode(str((objects_axis[i][7]))))\n        bndbox.appendChild(y3)\n        \n    f = open(save_path,\'w\')\n    f.write(doc.toprettyxml(indent = \'\'))\n    f.close() \n\nclass_list = [\'plane\', \'baseball-diamond\', \'bridge\', \'ground-track-field\', \n\'small-vehicle\', \'large-vehicle\', \'ship\', \n\'tennis-court\', \'basketball-court\',  \n\'storage-tank\', \'soccer-ball-field\', \n\'roundabout\', \'harbor\', \n\'swimming-pool\', \'helicopter\']\n\n\n\n\ndef format_label(txt_list):\n    format_data = []\n    for i in txt_list[2:]:\n        format_data.append(\n        [int(xy) for xy in i.split(\' \')[:8]] + [class_list.index(i.split(\' \')[8])]\n        # {\'x0\': int(i.split(\' \')[0]),\n        # \'x1\': int(i.split(\' \')[2]),\n        # \'x2\': int(i.split(\' \')[4]),\n        # \'x3\': int(i.split(\' \')[6]),\n        # \'y1\': int(i.split(\' \')[1]),\n        # \'y2\': int(i.split(\' \')[3]),\n        # \'y3\': int(i.split(\' \')[5]),\n        # \'y4\': int(i.split(\' \')[7]),\n        # \'class\': class_list.index(i.split(\' \')[8]) if i.split(\' \')[8] in class_list else 0, \n        # \'difficulty\': int(i.split(\' \')[9])}\n        )\n        if i.split(\' \')[8] not in class_list :\n            print (\'warning found a new label :\', i.split(\' \')[8])\n            exit()\n    return np.array(format_data)\n\ndef clip_image(file_idx, image, boxes_all, width, height):\n    if len(boxes_all) > 0:\n    # print (\'image shape\', image.shape)\n        shape = image.shape\n        for start_h in range(0, shape[0], 512):\n            for start_w in range(0, shape[1], 512):\n                boxes = copy.deepcopy(boxes_all)\n                box = np.zeros_like(boxes_all)\n                start_h_new = start_h\n                start_w_new = start_w\n                if start_h + height > shape[0]:\n                  start_h_new = shape[0] - height\n                if start_w + width > shape[1]:\n                  start_w_new = shape[1] - width\n                top_left_row = max(start_h_new, 0)\n                top_left_col = max(start_w_new, 0)\n                bottom_right_row = min(start_h + height, shape[0])\n                bottom_right_col = min(start_w + width, shape[1])\n\n\n                subImage = image[top_left_row:bottom_right_row, top_left_col: bottom_right_col]\n                box[:, 0] = boxes[:, 0] - top_left_col\n                box[:, 2] = boxes[:, 2] - top_left_col\n                box[:, 4] = boxes[:, 4] - top_left_col\n                box[:, 6] = boxes[:, 6] - top_left_col\n\n                box[:, 1] = boxes[:, 1] - top_left_row\n                box[:, 3] = boxes[:, 3] - top_left_row\n                box[:, 5] = boxes[:, 5] - top_left_row\n                box[:, 7] = boxes[:, 7] - top_left_row\n                box[:, 8] = boxes[:, 8]\n                center_y = 0.25*(box[:, 1] + box[:, 3] + box[:, 5] + box[:, 7])\n                center_x = 0.25*(box[:, 0] + box[:, 2] + box[:, 4] + box[:, 6])\n                # print(\'center_y\', center_y)\n                # print(\'center_x\', center_x)\n                # print (\'boxes\', boxes)\n                # print (\'boxes_all\', boxes_all)\n                # print (\'top_left_col\', top_left_col, \'top_left_row\', top_left_row)\n\n                cond1 = np.intersect1d(np.where(center_y[:]>=0 )[0], np.where(center_x[:]>=0 )[0])\n                cond2 = np.intersect1d(np.where(center_y[:] <= (bottom_right_row - top_left_row))[0],\n                                        np.where(center_x[:] <= (bottom_right_col - top_left_col))[0])\n                idx = np.intersect1d(cond1, cond2)\n                # idx = np.where(center_y[:]>=0 and center_x[:]>=0 and center_y[:] <= (bottom_right_row - top_left_row) and center_x[:] <= (bottom_right_col - top_left_col))[0]\n                # save_path, im_width, im_height, objects_axis, label_name\n                if len(idx) > 0:\n                    xml = os.path.join(save_dir, \'labeltxt\', ""%s_%04d_%04d.xml"" % (file_idx, top_left_row, top_left_col))\n                    save_to_xml(xml, subImage.shape[0], subImage.shape[1], box[idx, :], class_list)\n                    # print (\'save xml : \', xml)\n                    if subImage.shape[0] > 5 and subImage.shape[1] >5:\n                        img = os.path.join(save_dir, \'images\', ""%s_%04d_%04d.png"" % (file_idx, top_left_row, top_left_col))\n                        cv2.imwrite(img, subImage)\n        \n    \n    \n\nprint (\'class_list\', len(class_list))\nraw_data = \'/dataset/DOTA/val/\'\nraw_images_dir = os.path.join(raw_data, \'images\')\nraw_label_dir = os.path.join(raw_data, \'labelTxt\')\n\nsave_dir = \'/dataset/DOTA_clip/val/\'\n\nimages = [i for i in os.listdir(raw_images_dir) if \'png\' in i]\nlabels = [i for i in os.listdir(raw_label_dir) if \'txt\' in i]\n\nprint (\'find image\', len(images))\nprint (\'find label\', len(labels))\n\nmin_length = 1e10\nmax_length = 1\n\n\nfor idx, img in enumerate(images):\n    # img = \'P2330.png\'\n    print (idx, \'read image\', img)\n    img_data = misc.imread(os.path.join(raw_images_dir, img))\n    # img_data = cv2.imread(os.path.join(raw_images_dir, img))\n    # if len(img_data.shape) == 2:\n        # img_data = img_data[:, :, np.newaxis]\n        # print (\'find gray image\')\n\n    txt_data = open(os.path.join(raw_label_dir, img.replace(\'png\', \'txt\')), \'r\').readlines()\n    # print (idx, len(format_label(txt_data)), img_data.shape)\n    # if max(img_data.shape[:2]) > max_length:\n        # max_length = max(img_data.shape[:2])\n    # if min(img_data.shape[:2]) < min_length:\n        # min_length = min(img_data.shape[:2])\n    # if idx % 50 ==0:\n        # print (idx, len(format_label(txt_data)), img_data.shape)\n        # print (idx, \'min_length\', min_length, \'max_length\', max_length)\n    box = format_label(txt_data)\n    clip_image(img.strip(\'.png\'), img_data, box, 800, 800)\n    \n    \n    \n    \n\n\n'"
libs/box_utils/cython_utils/__init__.py,0,b''
libs/box_utils/cython_utils/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""cython_bbox"",\n        [""bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""cython_nms"",\n        [""nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    )\n    # Extension(\n    #     ""cpu_nms"",\n    #     [""cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # )\n]\n\nsetup(\n    name=\'tf_faster_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
libs/networks/mobilenet/__init__.py,0,b''
libs/networks/mobilenet/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional scope.\n\n  Returns:\n    Tensor of depth num_outputs\n\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=tf.identity)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')\n'"
libs/networks/mobilenet/mobilenet.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = defaults.items()\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode).\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      slim.arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n    set to non-training mode. This might be helpful for code that is reused\n    across both training/evaluation, but most of the time training_scope with\n    value False is not needed.\n\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability\n    bn_decay: decay for the batch norm moving averages.\n\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'decay\': bn_decay,\n  }\n\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      slim.arg_scope([slim.batch_norm], **batch_norm_params), \\\n      slim.arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s\n'"
libs/networks/mobilenet/mobilenet_v2.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\n\nArchitecture: https://arxiv.org/abs/1801.04381\n\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import conv_blocks as ops\nfrom libs.networks.mobilenet import mobilenet as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\nV2_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24),\n        op(ops.expanded_conv, stride=1, num_outputs=24),\n        op(ops.expanded_conv, stride=2, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=2, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=2, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=320),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n    ],\n)\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediciton_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_classes=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  with slim.\n\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']\n'"
libs/networks/mobilenet/mobilenet_v2_test.py,25,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for mobilenet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport copy\nimport tensorflow as tf\nfrom nets.mobilenet import conv_blocks as ops\nfrom nets.mobilenet import mobilenet\nfrom nets.mobilenet import mobilenet_v2\n\n\nslim = tf.contrib.slim\n\n\ndef find_ops(optype):\n  """"""Find ops of a given type in graphdef or a graph.\n\n  Args:\n    optype: operation type (e.g. Conv2D)\n  Returns:\n     List of operations.\n  """"""\n  gd = tf.get_default_graph()\n  return [var for var in gd.get_operations() if var.type == optype]\n\n\nclass MobilenetV2Test(tf.test.TestCase):\n\n  def setUp(self):\n    tf.reset_default_graph()\n\n  def testCreation(self):\n    spec = dict(mobilenet_v2.V2_DEF)\n    _, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n\n    # This is mostly a sanity test. No deep reason for these particular\n    # constants.\n    #\n    # All but first 2 and last one have  two convolutions, and there is one\n    # extra conv that is not in the spec. (logits)\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 2 - 2)\n    # Check that depthwise are exposed.\n    for i in range(2, 17):\n      self.assertIn(\'layer_%d/depthwise_output\' % i, ep)\n\n  def testCreationNoClasses(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    net, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec,\n        num_classes=None)\n    self.assertIs(net, ep[\'global_pool\'])\n\n  def testImageSizes(self):\n    for input_size, output_size in [(224, 7), (192, 6), (160, 5),\n                                    (128, 4), (96, 3)]:\n      tf.reset_default_graph()\n      _, ep = mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, input_size, input_size, 3)))\n\n      self.assertEqual(ep[\'layer_18/output\'].get_shape().as_list()[1:3],\n                       [output_size] * 2)\n\n  def testWithSplits(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    spec[\'overrides\'] = {\n        (ops.expanded_conv,): dict(split_expansion=2),\n    }\n    _, _ = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n    # All but 3 op has 3 conv operatore, the remainign 3 have one\n    # and there is one unaccounted.\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 3 - 5)\n\n  def testWithOutputStride8(self):\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testDivisibleBy(self):\n    tf.reset_default_graph()\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        divisible_by=16,\n        min_depth=32)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    self.assertSameElements([32, 64, 96, 160, 192, 320, 384, 576, 960, 1280,\n                             1001], s)\n\n  def testDivisibleByWithArgScope(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, 224, 224, 2)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n      s = set(s)\n      self.assertSameElements(s, [32, 192, 128, 1001])\n\n  def testFineGrained(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 2)),\n        conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.01,\n        finegrain_classification_mode=True)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    # All convolutions will be 8->48, except for the last one.\n    self.assertSameElements(s, [8, 48, 1001, 1280])\n\n  def testMobilenetBase(self):\n    tf.reset_default_graph()\n    # Verifies that mobilenet_base returns pre-pooling layer.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      net, _ = mobilenet_v2.mobilenet_base(\n          tf.placeholder(tf.float32, (10, 224, 224, 16)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      self.assertEqual(net.get_shape().as_list(), [10, 7, 7, 128])\n\n  def testWithOutputStride16(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n  def testWithOutputStride8AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        use_explicit_padding=True,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testWithOutputStride16AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16,\n        use_explicit_padding=True)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/__init__.py,0,b'\n'
libs/networks/slim_nets/alexnet.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
libs/networks/slim_nets/alexnet_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_resnet_v2 import inception_resnet_v2_base\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
libs/networks/slim_nets/inception_resnet_v2.py,41,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint=\'Conv2d_7b_1x1\',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None):\n  """"""Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_6a\', \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after \'PreAuxLogits\'.\n  """"""\n  if output_stride != 8 and output_stride != 16:\n    raise ValueError(\'output_stride must be 8 or 16.\')\n\n  padding = \'SAME\' if align_feature_maps else \'VALID\'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_3a_3x3\')\n      if add_and_check_final(\'MaxPool_3a_3x3\', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope=\'Conv2d_3b_1x1\')\n      if add_and_check_final(\'Conv2d_3b_1x1\', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope=\'Conv2d_4a_3x3\')\n      if add_and_check_final(\'Conv2d_4a_3x3\', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_5a_3x3\')\n      if add_and_check_final(\'MaxPool_5a_3x3\', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope(\'Mixed_5b\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                       scope=\'AvgPool_0a_3x3\')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final(\'Mixed_5b\', net): return net, end_points\n      # TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17)\n\n      # 17 x 17 x 1088 if output_stride == 8,\n      # 33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope(\'Mixed_6a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10)\n      if add_and_check_final(\'PreAuxLogits\', net): return net, end_points\n\n      if output_stride == 8:\n        # TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError(\'output_stride==8 is only supported up to the \'\n                         \'PreAuxlogits end_point for now.\')\n\n      # 8 x 8 x 2080\n      with tf.variable_scope(\'Mixed_7a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20)\n      net = block8(net, activation_fn=None)\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n      if add_and_check_final(\'Conv2d_7b_1x1\', net): return net, end_points\n\n    raise ValueError(\'final_endpoint (%s) not recognized\', final_endpoint)\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,#0.8\n                        reuse=None,\n                        scope=\'InceptionResnetV2\',\n                        create_aux_logits=True):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n\n      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n\n      if create_aux_logits:\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = end_points[\'PreAuxLogits\']\n          aux = slim.avg_pool2d(aux, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                              scope=\'AvgPool_1a_8x8\')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'Dropout\')\n\n        end_points[\'PreLogitsFlatten\'] = net\n        # end_points[\'yjr_feature\'] = tf.squeeze(net, axis=0)\n\n        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                      scope=\'Logits\')\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
libs/networks/slim_nets/inception_resnet_v2_test.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'AuxLogits\' in endpoints)\n      auxlogits = endpoints[\'AuxLogits\']\n      self.assertTrue(\n          auxlogits.op.name.startswith(\'InceptionResnetV2/AuxLogits\'))\n      self.assertListEqual(auxlogits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes,\n                                                        create_aux_logits=False)\n      self.assertTrue(\'AuxLogits\' not in endpoints)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_resnet_v2_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'InceptionResnetV2/Conv2d_7b_1x1\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 8, 8, 1536])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                          \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                 \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_resnet_v2_base(\n            inputs, final_endpoint=endpoint)\n        if endpoint != \'PreAuxLogits\':\n          self.assertTrue(out_tensor.op.name.startswith(\n              \'InceptionResnetV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 17, 17, 1088],\n                        \'PreAuxLogits\': [5, 17, 17, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithAlignedFeatureMaps(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', align_feature_maps=True)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2b_3x3\': [5, 150, 150, 64],\n                        \'MaxPool_3a_3x3\': [5, 75, 75, 64],\n                        \'Conv2d_3b_1x1\': [5, 75, 75, 80],\n                        \'Conv2d_4a_3x3\': [5, 75, 75, 192],\n                        \'MaxPool_5a_3x3\': [5, 38, 38, 192],\n                        \'Mixed_5b\': [5, 38, 38, 320],\n                        \'Mixed_6a\': [5, 19, 19, 1088],\n                        \'PreAuxLogits\': [5, 19, 19, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithOutputStrideEight(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', output_stride=8)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 33, 33, 1088],\n                        \'PreAuxLogits\': [5, 33, 33, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v4_test.py,24,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
libs/networks/slim_nets/mobilenet_v1.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""MobileNet v1.\n\nMobileNet is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nhead (for example: embeddings, localization and classification).\n\nAs described in https://arxiv.org/abs/1704.04861.\n\n  MobileNets: Efficient Convolutional Neural Networks for\n    Mobile Vision Applications\n  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n    Tobias Weyand, Marco Andreetto, Hartwig Adam\n\n100% Mobilenet V1 (base) with input size 224x224:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 864      10,838,016\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    288       3,612,672\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     2,048      25,690,112\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    576       1,806,336\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     8,192      25,690,112\nMobilenetV1/Conv2d_3_depthwise/depthwise:                  1,152       3,612,672\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                    16,384      51,380,224\nMobilenetV1/Conv2d_4_depthwise/depthwise:                  1,152         903,168\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    32,768      25,690,112\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  2,304       1,806,336\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    65,536      51,380,224\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  2,304         451,584\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                   131,072      25,690,112\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 4,608         225,792\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  524,288      25,690,112\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 9,216         451,584\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                1,048,576      51,380,224\n--------------------------------------------------------------------------------\nTotal:                                                 3,185,088     567,716,352\n\n\n75% Mobilenet V1 (base) with input size 128x128:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 648       2,654,208\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    216         884,736\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     1,152       4,718,592\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    432         442,368\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     4,608       4,718,592\nMobilenetV1/Conv2d_3_depthwise/depthwise:                    864         884,736\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                     9,216       9,437,184\nMobilenetV1/Conv2d_4_depthwise/depthwise:                    864         221,184\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    18,432       4,718,592\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  1,728         442,368\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    36,864       9,437,184\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  1,728         110,592\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                    73,728       4,718,592\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 3,456          55,296\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  294,912       4,718,592\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 6,912         110,592\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                  589,824       9,437,184\n--------------------------------------------------------------------------------\nTotal:                                                 1,800,144     106,002,432\n\n""""""\n\n# Tensorflow mandates these.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# Conv and DepthSepConv namedtuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=[3, 3], stride=2, depth=32),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)\n]\n\n\ndef mobilenet_v1_base(inputs,\n                      final_endpoint=\'Conv2d_13_pointwise\',\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      conv_defs=None,\n                      output_stride=None,\n                      scope=None):\n  """"""Mobilenet v1.\n\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_0\', \'Conv2d_1_pointwise\', \'Conv2d_2_pointwise\',\n      \'Conv2d_3_pointwise\', \'Conv2d_4_pointwise\', \'Conv2d_5\'_pointwise,\n      \'Conv2d_6_pointwise\', \'Conv2d_7_pointwise\', \'Conv2d_8_pointwise\',\n      \'Conv2d_9_pointwise\', \'Conv2d_10_pointwise\', \'Conv2d_11_pointwise\',\n      \'Conv2d_12_pointwise\', \'Conv2d_13_pointwise\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 8 (accurate fully convolutional\n      mode), 16 (fast fully convolutional mode), 32 (classification mode).\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  if conv_defs is None:\n    conv_defs = _CONV_DEFS\n\n  if output_stride is not None and output_stride not in [8, 16, 32]:\n    raise ValueError(\'Only allowed output_stride values are 8, 16, 32.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n      # The current_stride variable keeps track of the output stride of the\n      # activations, i.e., the running product of convolution strides up to the\n      # current network layer. This allows us to invoke atrous convolution\n      # whenever applying the next convolution would result in the activations\n      # having output stride larger than the target output_stride.\n      current_stride = 1\n\n      # The atrous convolution rate parameter.\n      rate = 1\n\n      net = inputs\n      for i, conv_def in enumerate(conv_defs):\n        end_point_base = \'Conv2d_%d\' % i\n\n        if output_stride is not None and current_stride == output_stride:\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          layer_stride = 1\n          layer_rate = rate\n          rate *= conv_def.stride\n        else:\n          layer_stride = conv_def.stride\n          layer_rate = 1\n          current_stride *= conv_def.stride\n\n        if isinstance(conv_def, Conv):\n          end_point = end_point_base\n          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n                            stride=conv_def.stride,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        elif isinstance(conv_def, DepthSepConv):\n          end_point = end_point_base + \'_depthwise\'\n\n          # By passing filters=None\n          # separable_conv2d produces only a depthwise convolution layer\n          net = slim.separable_conv2d(net, None, conv_def.kernel,\n                                      depth_multiplier=1,\n                                      stride=layer_stride,\n                                      rate=layer_rate,\n                                      normalizer_fn=slim.batch_norm,\n                                      scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n          end_point = end_point_base + \'_pointwise\'\n\n          net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                            stride=1,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n        else:\n          raise ValueError(\'Unknown convolution type %s for layer %d\'\n                           % (conv_def.ltype, i))\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef mobilenet_v1(inputs,\n                 num_classes=1000,\n                 dropout_keep_prob=0.999,\n                 is_training=True,\n                 min_depth=8,\n                 depth_multiplier=1.0,\n                 conv_defs=None,\n                 prediction_fn=tf.contrib.layers.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'MobilenetV1\'):\n  """"""Mobilenet v1 model for classification.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    is_training: whether is training or not.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\n                     len(input_shape))\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = mobilenet_v1_base(inputs, scope=scope,\n                                          min_depth=min_depth,\n                                          depth_multiplier=depth_multiplier,\n                                          conv_defs=conv_defs)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a\')\n        end_points[\'AvgPool_1a\'] = net\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      if prediction_fn:\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\n\nmobilenet_v1.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef mobilenet_v1_arg_scope(is_training=True,\n                           weight_decay=0.00004,\n                           stddev=0.09,\n                           regularize_depthwise=False):\n  """"""Defines the default MobilenetV1 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    regularize_depthwise: Whether or not apply regularization on depthwise.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v1 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  if regularize_depthwise:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n'"
libs/networks/slim_nets/mobilenet_v1_test.py,32,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""Tests for MobileNet v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\nclass MobilenetV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_13\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                          \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                          \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                          \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                          \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                          \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                          \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                          \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                          \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                          \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                          \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_0\',\n                 \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                 \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                 \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                 \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                 \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                 \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                 \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                 \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                 \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                 \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                 \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                 \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                 \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = mobilenet_v1.mobilenet_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'MobilenetV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildCustomNetworkUsingConvDefs(self):\n    batch_size = 5\n    height, width = 224, 224\n    conv_defs = [\n        mobilenet_v1.Conv(kernel=[3, 3], stride=2, depth=32),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=512)\n    ]\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(\n        inputs, final_endpoint=\'Conv2d_3_pointwise\', conv_defs=conv_defs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_3\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 56, 56, 512])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 7, 7, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 7, 7, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride16BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 16\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 14, 14, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride8BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 8\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 28, 28, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsApproximateFaceNet(self):\n    batch_size = 5\n    height, width = 128, 128\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\', depth_multiplier=0.75)\n    # For the Conv2d_0 layer FaceNet has depth=16\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_depthwise\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_pointwise\': [batch_size, 64, 64, 48],\n                        \'Conv2d_2_depthwise\': [batch_size, 32, 32, 48],\n                        \'Conv2d_2_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_depthwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_4_depthwise\': [batch_size, 16, 16, 96],\n                        \'Conv2d_4_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_depthwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_6_depthwise\': [batch_size, 8, 8, 192],\n                        \'Conv2d_6_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_12_depthwise\': [batch_size, 4, 4, 384],\n                        \'Conv2d_12_pointwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_depthwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_pointwise\': [batch_size, 4, 4, 768]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      mobilenet_v1.mobilenet_v1_base(inputs)\n      total_params, _ = slim.model_analyzer.analyze_vars(\n          slim.get_model_variables())\n      self.assertAlmostEqual(3217920L, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys() if key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Conv2d_13_pointwise\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_13_pointwise\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    mobilenet_v1.mobilenet_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = mobilenet_v1.mobilenet_v1(images,\n                                          num_classes=num_classes,\n                                          spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import mobilenet_v1\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n                \'mobilenet_v1\': mobilenet_v1.mobilenet_v1,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                  \'mobilenet_v1\': mobilenet_v1.mobilenet_v1_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
libs/networks/slim_nets/nets_factory_test.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnArgScope(self):\n    batch_size = 5\n    num_classes = 10\n    net = \'cifarnet\'\n    with self.test_session(use_gpu=True):\n      net_fn = nets_factory.get_network_fn(net, num_classes)\n      image_size = getattr(net_fn, \'default_image_size\', 224)\n      with slim.arg_scope([slim.model_variable, slim.variable],\n                          device=\'/CPU:0\'):\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        net_fn(inputs)\n      weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \'CifarNet/conv1\')[0]\n      self.assertDeviceEqual(\'/CPU:0\', weights.device)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
libs/networks/slim_nets/overfeat_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997, #0.997\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
libs/networks/slim_nets/resnet_v1.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.networks.slim_nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          # yjr_feature = tf.squeeze(net, [0, 1, 2])\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n\n        ###\n        # end_points[\'yjr_feature\'] = yjr_feature\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
libs/networks/slim_nets/resnet_v1_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    blocks = [\n        resnet_v1.resnet_v1_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v1.resnet_v1_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v2 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v2 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=False,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
libs/networks/slim_nets/resnet_v2_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    blocks = [\n        resnet_v2.resnet_v2_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v2.resnet_v2_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/vgg.py,10,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      # yjr_feature = tf.squeeze(net)\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      # end_points[\'yjr_feature\'] = yjr_feature\n      end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
libs/networks/slim_nets/vgg_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
