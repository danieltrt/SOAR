file_path,api_count,code
data.py,0,"b'import sys, pickle, os, random\nimport numpy as np\n\n## tags, BIO\ntag2label = {""O"": 0,\n             ""B-PER"": 1, ""I-PER"": 2,\n             ""B-LOC"": 3, ""I-LOC"": 4,\n             ""B-ORG"": 5, ""I-ORG"": 6\n             }\n\n\ndef read_corpus(corpus_path):\n    """"""\n    read corpus and return the list of samples\n    :param corpus_path:\n    :return: data\n    """"""\n    data = []\n    with open(corpus_path, encoding=\'utf-8\') as fr:\n        lines = fr.readlines()\n    sent_, tag_ = [], []\n    for line in lines:\n        if line != \'\\n\':\n            [char, label] = line.strip().split()\n            sent_.append(char)\n            tag_.append(label)\n        else:\n            data.append((sent_, tag_))\n            sent_, tag_ = [], []\n\n    return data\n\n\ndef vocab_build(vocab_path, corpus_path, min_count):\n    """"""\n\n    :param vocab_path:\n    :param corpus_path:\n    :param min_count:\n    :return:\n    """"""\n    data = read_corpus(corpus_path)\n    word2id = {}\n    for sent_, tag_ in data:\n        for word in sent_:\n            if word.isdigit():\n                word = \'<NUM>\'\n            elif (\'\\u0041\' <= word <=\'\\u005a\') or (\'\\u0061\' <= word <=\'\\u007a\'):\n                word = \'<ENG>\'\n            if word not in word2id:\n                word2id[word] = [len(word2id)+1, 1]\n            else:\n                word2id[word][1] += 1\n    low_freq_words = []\n    for word, [word_id, word_freq] in word2id.items():\n        if word_freq < min_count and word != \'<NUM>\' and word != \'<ENG>\':\n            low_freq_words.append(word)\n    for word in low_freq_words:\n        del word2id[word]\n\n    new_id = 1\n    for word in word2id.keys():\n        word2id[word] = new_id\n        new_id += 1\n    word2id[\'<UNK>\'] = new_id\n    word2id[\'<PAD>\'] = 0\n\n    print(len(word2id))\n    with open(vocab_path, \'wb\') as fw:\n        pickle.dump(word2id, fw)\n\n\ndef sentence2id(sent, word2id):\n    """"""\n\n    :param sent:\n    :param word2id:\n    :return:\n    """"""\n    sentence_id = []\n    for word in sent:\n        if word.isdigit():\n            word = \'<NUM>\'\n        elif (\'\\u0041\' <= word <= \'\\u005a\') or (\'\\u0061\' <= word <= \'\\u007a\'):\n            word = \'<ENG>\'\n        if word not in word2id:\n            word = \'<UNK>\'\n        sentence_id.append(word2id[word])\n    return sentence_id\n\n\ndef read_dictionary(vocab_path):\n    """"""\n\n    :param vocab_path:\n    :return:\n    """"""\n    vocab_path = os.path.join(vocab_path)\n    with open(vocab_path, \'rb\') as fr:\n        word2id = pickle.load(fr)\n    print(\'vocab_size:\', len(word2id))\n    return word2id\n\n\ndef random_embedding(vocab, embedding_dim):\n    """"""\n\n    :param vocab:\n    :param embedding_dim:\n    :return:\n    """"""\n    embedding_mat = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n    embedding_mat = np.float32(embedding_mat)\n    return embedding_mat\n\n\ndef pad_sequences(sequences, pad_mark=0):\n    """"""\n\n    :param sequences:\n    :param pad_mark:\n    :return:\n    """"""\n    max_len = max(map(lambda x : len(x), sequences))\n    seq_list, seq_len_list = [], []\n    for seq in sequences:\n        seq = list(seq)\n        seq_ = seq[:max_len] + [pad_mark] * max(max_len - len(seq), 0)\n        seq_list.append(seq_)\n        seq_len_list.append(min(len(seq), max_len))\n    return seq_list, seq_len_list\n\n\ndef batch_yield(data, batch_size, vocab, tag2label, shuffle=False):\n    """"""\n\n    :param data:\n    :param batch_size:\n    :param vocab:\n    :param tag2label:\n    :param shuffle:\n    :return:\n    """"""\n    if shuffle:\n        random.shuffle(data)\n\n    seqs, labels = [], []\n    for (sent_, tag_) in data:\n        sent_ = sentence2id(sent_, vocab)\n        label_ = [tag2label[tag] for tag in tag_]\n\n        if len(seqs) == batch_size:\n            yield seqs, labels\n            seqs, labels = [], []\n\n        seqs.append(sent_)\n        labels.append(label_)\n\n    if len(seqs) != 0:\n        yield seqs, labels\n\n'"
eval.py,0,"b'import os\r\n\r\n\r\ndef conlleval(label_predict, label_path, metric_path):\r\n    """"""\r\n\r\n    :param label_predict:\r\n    :param label_path:\r\n    :param metric_path:\r\n    :return:\r\n    """"""\r\n    eval_perl = ""./conlleval_rev.pl""\r\n    with open(label_path, ""w"") as fw:\r\n        line = []\r\n        for sent_result in label_predict:\r\n            for char, tag, tag_ in sent_result:\r\n                tag = \'0\' if tag == \'O\' else tag\r\n                char = char.encode(""utf-8"")\r\n                line.append(""{} {} {}\\n"".format(char, tag, tag_))\r\n            line.append(""\\n"")\r\n        fw.writelines(line)\r\n    os.system(""perl {} < {} > {}"".format(eval_perl, label_path, metric_path))\r\n    with open(metric_path) as fr:\r\n        metrics = [line.strip() for line in fr]\r\n    return metrics\r\n    '"
main.py,5,"b'import tensorflow as tf\nimport numpy as np\nimport os, argparse, time, random\nfrom model import BiLSTM_CRF\nfrom utils import str2bool, get_logger, get_entity\nfrom data import read_corpus, read_dictionary, tag2label, random_embedding\n\n\n## Session configuration\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'  # default: 0\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.2  # need ~700MB GPU memory\n\n\n## hyperparameters\nparser = argparse.ArgumentParser(description=\'BiLSTM-CRF for Chinese NER task\')\nparser.add_argument(\'--train_data\', type=str, default=\'data_path\', help=\'train data source\')\nparser.add_argument(\'--test_data\', type=str, default=\'data_path\', help=\'test data source\')\nparser.add_argument(\'--batch_size\', type=int, default=64, help=\'#sample of each minibatch\')\nparser.add_argument(\'--epoch\', type=int, default=40, help=\'#epoch of training\')\nparser.add_argument(\'--hidden_dim\', type=int, default=300, help=\'#dim of hidden state\')\nparser.add_argument(\'--optimizer\', type=str, default=\'Adam\', help=\'Adam/Adadelta/Adagrad/RMSProp/Momentum/SGD\')\nparser.add_argument(\'--CRF\', type=str2bool, default=True, help=\'use CRF at the top layer. if False, use Softmax\')\nparser.add_argument(\'--lr\', type=float, default=0.001, help=\'learning rate\')\nparser.add_argument(\'--clip\', type=float, default=5.0, help=\'gradient clipping\')\nparser.add_argument(\'--dropout\', type=float, default=0.5, help=\'dropout keep_prob\')\nparser.add_argument(\'--update_embedding\', type=str2bool, default=True, help=\'update embedding during training\')\nparser.add_argument(\'--pretrain_embedding\', type=str, default=\'random\', help=\'use pretrained char embedding or init it randomly\')\nparser.add_argument(\'--embedding_dim\', type=int, default=300, help=\'random init char embedding_dim\')\nparser.add_argument(\'--shuffle\', type=str2bool, default=True, help=\'shuffle training data before each epoch\')\nparser.add_argument(\'--mode\', type=str, default=\'demo\', help=\'train/test/demo\')\nparser.add_argument(\'--demo_model\', type=str, default=\'1521112368\', help=\'model for test and demo\')\nargs = parser.parse_args()\n\n\n## get char embeddings\nword2id = read_dictionary(os.path.join(\'.\', args.train_data, \'word2id.pkl\'))\nif args.pretrain_embedding == \'random\':\n    embeddings = random_embedding(word2id, args.embedding_dim)\nelse:\n    embedding_path = \'pretrain_embedding.npy\'\n    embeddings = np.array(np.load(embedding_path), dtype=\'float32\')\n\n\n## read corpus and get training data\nif args.mode != \'demo\':\n    train_path = os.path.join(\'.\', args.train_data, \'train_data\')\n    test_path = os.path.join(\'.\', args.test_data, \'test_data\')\n    train_data = read_corpus(train_path)\n    test_data = read_corpus(test_path); test_size = len(test_data)\n\n\n## paths setting\npaths = {}\ntimestamp = str(int(time.time())) if args.mode == \'train\' else args.demo_model\noutput_path = os.path.join(\'.\', args.train_data+""_save"", timestamp)\nif not os.path.exists(output_path): os.makedirs(output_path)\nsummary_path = os.path.join(output_path, ""summaries"")\npaths[\'summary_path\'] = summary_path\nif not os.path.exists(summary_path): os.makedirs(summary_path)\nmodel_path = os.path.join(output_path, ""checkpoints/"")\nif not os.path.exists(model_path): os.makedirs(model_path)\nckpt_prefix = os.path.join(model_path, ""model"")\npaths[\'model_path\'] = ckpt_prefix\nresult_path = os.path.join(output_path, ""results"")\npaths[\'result_path\'] = result_path\nif not os.path.exists(result_path): os.makedirs(result_path)\nlog_path = os.path.join(result_path, ""log.txt"")\npaths[\'log_path\'] = log_path\nget_logger(log_path).info(str(args))\n\n\n## training model\nif args.mode == \'train\':\n    model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n    model.build_graph()\n\n    ## hyperparameters-tuning, split train/dev\n    # dev_data = train_data[:5000]; dev_size = len(dev_data)\n    # train_data = train_data[5000:]; train_size = len(train_data)\n    # print(""train data: {0}\\ndev data: {1}"".format(train_size, dev_size))\n    # model.train(train=train_data, dev=dev_data)\n\n    ## train model on the whole training data\n    print(""train data: {}"".format(len(train_data)))\n    model.train(train=train_data, dev=test_data)  # use test_data as the dev_data to see overfitting phenomena\n\n## testing model\nelif args.mode == \'test\':\n    ckpt_file = tf.train.latest_checkpoint(model_path)\n    print(ckpt_file)\n    paths[\'model_path\'] = ckpt_file\n    model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n    model.build_graph()\n    print(""test data: {}"".format(test_size))\n    model.test(test_data)\n\n## demo\nelif args.mode == \'demo\':\n    ckpt_file = tf.train.latest_checkpoint(model_path)\n    print(ckpt_file)\n    paths[\'model_path\'] = ckpt_file\n    model = BiLSTM_CRF(args, embeddings, tag2label, word2id, paths, config=config)\n    model.build_graph()\n    saver = tf.train.Saver()\n    with tf.Session(config=config) as sess:\n        print(\'============= demo =============\')\n        saver.restore(sess, ckpt_file)\n        while(1):\n            print(\'Please input your sentence:\')\n            demo_sent = input()\n            if demo_sent == \'\' or demo_sent.isspace():\n                print(\'See you next time!\')\n                break\n            else:\n                demo_sent = list(demo_sent.strip())\n                demo_data = [(demo_sent, [\'O\'] * len(demo_sent))]\n                tag = model.demo_one(sess, demo_data)\n                PER, LOC, ORG = get_entity(tag, demo_sent)\n                print(\'PER: {}\\nLOC: {}\\nORG: {}\'.format(PER, LOC, ORG))\n'"
model.py,51,"b'import numpy as np\nimport os, time, sys\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\nfrom tensorflow.contrib.crf import crf_log_likelihood\nfrom tensorflow.contrib.crf import viterbi_decode\nfrom data import pad_sequences, batch_yield\nfrom utils import get_logger\nfrom eval import conlleval\n\n\nclass BiLSTM_CRF(object):\n    def __init__(self, args, embeddings, tag2label, vocab, paths, config):\n        self.batch_size = args.batch_size\n        self.epoch_num = args.epoch\n        self.hidden_dim = args.hidden_dim\n        self.embeddings = embeddings\n        self.CRF = args.CRF\n        self.update_embedding = args.update_embedding\n        self.dropout_keep_prob = args.dropout\n        self.optimizer = args.optimizer\n        self.lr = args.lr\n        self.clip_grad = args.clip\n        self.tag2label = tag2label\n        self.num_tags = len(tag2label)\n        self.vocab = vocab\n        self.shuffle = args.shuffle\n        self.model_path = paths[\'model_path\']\n        self.summary_path = paths[\'summary_path\']\n        self.logger = get_logger(paths[\'log_path\'])\n        self.result_path = paths[\'result_path\']\n        self.config = config\n\n    def build_graph(self):\n        self.add_placeholders()\n        self.lookup_layer_op()\n        self.biLSTM_layer_op()\n        self.softmax_pred_op()\n        self.loss_op()\n        self.trainstep_op()\n        self.init_op()\n\n    def add_placeholders(self):\n        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name=""word_ids"")\n        self.labels = tf.placeholder(tf.int32, shape=[None, None], name=""labels"")\n        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=""sequence_lengths"")\n\n        self.dropout_pl = tf.placeholder(dtype=tf.float32, shape=[], name=""dropout"")\n        self.lr_pl = tf.placeholder(dtype=tf.float32, shape=[], name=""lr"")\n\n    def lookup_layer_op(self):\n        with tf.variable_scope(""words""):\n            _word_embeddings = tf.Variable(self.embeddings,\n                                           dtype=tf.float32,\n                                           trainable=self.update_embedding,\n                                           name=""_word_embeddings"")\n            word_embeddings = tf.nn.embedding_lookup(params=_word_embeddings,\n                                                     ids=self.word_ids,\n                                                     name=""word_embeddings"")\n        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout_pl)\n\n    def biLSTM_layer_op(self):\n        with tf.variable_scope(""bi-lstm""):\n            cell_fw = LSTMCell(self.hidden_dim)\n            cell_bw = LSTMCell(self.hidden_dim)\n            (output_fw_seq, output_bw_seq), _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=cell_fw,\n                cell_bw=cell_bw,\n                inputs=self.word_embeddings,\n                sequence_length=self.sequence_lengths,\n                dtype=tf.float32)\n            output = tf.concat([output_fw_seq, output_bw_seq], axis=-1)\n            output = tf.nn.dropout(output, self.dropout_pl)\n\n        with tf.variable_scope(""proj""):\n            W = tf.get_variable(name=""W"",\n                                shape=[2 * self.hidden_dim, self.num_tags],\n                                initializer=tf.contrib.layers.xavier_initializer(),\n                                dtype=tf.float32)\n\n            b = tf.get_variable(name=""b"",\n                                shape=[self.num_tags],\n                                initializer=tf.zeros_initializer(),\n                                dtype=tf.float32)\n\n            s = tf.shape(output)\n            output = tf.reshape(output, [-1, 2*self.hidden_dim])\n            pred = tf.matmul(output, W) + b\n\n            self.logits = tf.reshape(pred, [-1, s[1], self.num_tags])\n\n    def loss_op(self):\n        if self.CRF:\n            log_likelihood, self.transition_params = crf_log_likelihood(inputs=self.logits,\n                                                                   tag_indices=self.labels,\n                                                                   sequence_lengths=self.sequence_lengths)\n            self.loss = -tf.reduce_mean(log_likelihood)\n\n        else:\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n                                                                    labels=self.labels)\n            mask = tf.sequence_mask(self.sequence_lengths)\n            losses = tf.boolean_mask(losses, mask)\n            self.loss = tf.reduce_mean(losses)\n\n        tf.summary.scalar(""loss"", self.loss)\n\n    def softmax_pred_op(self):\n        if not self.CRF:\n            self.labels_softmax_ = tf.argmax(self.logits, axis=-1)\n            self.labels_softmax_ = tf.cast(self.labels_softmax_, tf.int32)\n\n    def trainstep_op(self):\n        with tf.variable_scope(""train_step""):\n            self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\n            if self.optimizer == \'Adam\':\n                optim = tf.train.AdamOptimizer(learning_rate=self.lr_pl)\n            elif self.optimizer == \'Adadelta\':\n                optim = tf.train.AdadeltaOptimizer(learning_rate=self.lr_pl)\n            elif self.optimizer == \'Adagrad\':\n                optim = tf.train.AdagradOptimizer(learning_rate=self.lr_pl)\n            elif self.optimizer == \'RMSProp\':\n                optim = tf.train.RMSPropOptimizer(learning_rate=self.lr_pl)\n            elif self.optimizer == \'Momentum\':\n                optim = tf.train.MomentumOptimizer(learning_rate=self.lr_pl, momentum=0.9)\n            elif self.optimizer == \'SGD\':\n                optim = tf.train.GradientDescentOptimizer(learning_rate=self.lr_pl)\n            else:\n                optim = tf.train.GradientDescentOptimizer(learning_rate=self.lr_pl)\n\n            grads_and_vars = optim.compute_gradients(self.loss)\n            grads_and_vars_clip = [[tf.clip_by_value(g, -self.clip_grad, self.clip_grad), v] for g, v in grads_and_vars]\n            self.train_op = optim.apply_gradients(grads_and_vars_clip, global_step=self.global_step)\n\n    def init_op(self):\n        self.init_op = tf.global_variables_initializer()\n\n    def add_summary(self, sess):\n        """"""\n\n        :param sess:\n        :return:\n        """"""\n        self.merged = tf.summary.merge_all()\n        self.file_writer = tf.summary.FileWriter(self.summary_path, sess.graph)\n\n    def train(self, train, dev):\n        """"""\n\n        :param train:\n        :param dev:\n        :return:\n        """"""\n        saver = tf.train.Saver(tf.global_variables())\n\n        with tf.Session(config=self.config) as sess:\n            sess.run(self.init_op)\n            self.add_summary(sess)\n\n            for epoch in range(self.epoch_num):\n                self.run_one_epoch(sess, train, dev, self.tag2label, epoch, saver)\n\n    def test(self, test):\n        saver = tf.train.Saver()\n        with tf.Session(config=self.config) as sess:\n            self.logger.info(\'=========== testing ===========\')\n            saver.restore(sess, self.model_path)\n            label_list, seq_len_list = self.dev_one_epoch(sess, test)\n            self.evaluate(label_list, seq_len_list, test)\n\n    def demo_one(self, sess, sent):\n        """"""\n\n        :param sess:\n        :param sent: \n        :return:\n        """"""\n        label_list = []\n        for seqs, labels in batch_yield(sent, self.batch_size, self.vocab, self.tag2label, shuffle=False):\n            label_list_, _ = self.predict_one_batch(sess, seqs)\n            label_list.extend(label_list_)\n        label2tag = {}\n        for tag, label in self.tag2label.items():\n            label2tag[label] = tag if label != 0 else label\n        tag = [label2tag[label] for label in label_list[0]]\n        return tag\n\n    def run_one_epoch(self, sess, train, dev, tag2label, epoch, saver):\n        """"""\n\n        :param sess:\n        :param train:\n        :param dev:\n        :param tag2label:\n        :param epoch:\n        :param saver:\n        :return:\n        """"""\n        num_batches = (len(train) + self.batch_size - 1) // self.batch_size\n\n        start_time = time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime())\n        batches = batch_yield(train, self.batch_size, self.vocab, self.tag2label, shuffle=self.shuffle)\n        for step, (seqs, labels) in enumerate(batches):\n\n            sys.stdout.write(\' processing: {} batch / {} batches.\'.format(step + 1, num_batches) + \'\\r\')\n            step_num = epoch * num_batches + step + 1\n            feed_dict, _ = self.get_feed_dict(seqs, labels, self.lr, self.dropout_keep_prob)\n            _, loss_train, summary, step_num_ = sess.run([self.train_op, self.loss, self.merged, self.global_step],\n                                                         feed_dict=feed_dict)\n            if step + 1 == 1 or (step + 1) % 300 == 0 or step + 1 == num_batches:\n                self.logger.info(\n                    \'{} epoch {}, step {}, loss: {:.4}, global_step: {}\'.format(start_time, epoch + 1, step + 1,\n                                                                                loss_train, step_num))\n\n            self.file_writer.add_summary(summary, step_num)\n\n            if step + 1 == num_batches:\n                saver.save(sess, self.model_path, global_step=step_num)\n\n        self.logger.info(\'===========validation / test===========\')\n        label_list_dev, seq_len_list_dev = self.dev_one_epoch(sess, dev)\n        self.evaluate(label_list_dev, seq_len_list_dev, dev, epoch)\n\n    def get_feed_dict(self, seqs, labels=None, lr=None, dropout=None):\n        """"""\n\n        :param seqs:\n        :param labels:\n        :param lr:\n        :param dropout:\n        :return: feed_dict\n        """"""\n        word_ids, seq_len_list = pad_sequences(seqs, pad_mark=0)\n\n        feed_dict = {self.word_ids: word_ids,\n                     self.sequence_lengths: seq_len_list}\n        if labels is not None:\n            labels_, _ = pad_sequences(labels, pad_mark=0)\n            feed_dict[self.labels] = labels_\n        if lr is not None:\n            feed_dict[self.lr_pl] = lr\n        if dropout is not None:\n            feed_dict[self.dropout_pl] = dropout\n\n        return feed_dict, seq_len_list\n\n    def dev_one_epoch(self, sess, dev):\n        """"""\n\n        :param sess:\n        :param dev:\n        :return:\n        """"""\n        label_list, seq_len_list = [], []\n        for seqs, labels in batch_yield(dev, self.batch_size, self.vocab, self.tag2label, shuffle=False):\n            label_list_, seq_len_list_ = self.predict_one_batch(sess, seqs)\n            label_list.extend(label_list_)\n            seq_len_list.extend(seq_len_list_)\n        return label_list, seq_len_list\n\n    def predict_one_batch(self, sess, seqs):\n        """"""\n\n        :param sess:\n        :param seqs:\n        :return: label_list\n                 seq_len_list\n        """"""\n        feed_dict, seq_len_list = self.get_feed_dict(seqs, dropout=1.0)\n\n        if self.CRF:\n            logits, transition_params = sess.run([self.logits, self.transition_params],\n                                                 feed_dict=feed_dict)\n            label_list = []\n            for logit, seq_len in zip(logits, seq_len_list):\n                viterbi_seq, _ = viterbi_decode(logit[:seq_len], transition_params)\n                label_list.append(viterbi_seq)\n            return label_list, seq_len_list\n\n        else:\n            label_list = sess.run(self.labels_softmax_, feed_dict=feed_dict)\n            return label_list, seq_len_list\n\n    def evaluate(self, label_list, seq_len_list, data, epoch=None):\n        """"""\n\n        :param label_list:\n        :param seq_len_list:\n        :param data:\n        :param epoch:\n        :return:\n        """"""\n        label2tag = {}\n        for tag, label in self.tag2label.items():\n            label2tag[label] = tag if label != 0 else label\n\n        model_predict = []\n        for label_, (sent, tag) in zip(label_list, data):\n            tag_ = [label2tag[label__] for label__ in label_]\n            sent_res = []\n            if  len(label_) != len(sent):\n                print(sent)\n                print(len(label_))\n                print(tag)\n            for i in range(len(sent)):\n                sent_res.append([sent[i], tag[i], tag_[i]])\n            model_predict.append(sent_res)\n        epoch_num = str(epoch+1) if epoch != None else \'test\'\n        label_path = os.path.join(self.result_path, \'label_\' + epoch_num)\n        metric_path = os.path.join(self.result_path, \'result_metric_\' + epoch_num)\n        for _ in conlleval(model_predict, label_path, metric_path):\n            self.logger.info(_)\n\n'"
utils.py,0,"b""import logging, sys, argparse\n\n\ndef str2bool(v):\n    # copy from StackOverflow\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n\ndef get_entity(tag_seq, char_seq):\n    PER = get_PER_entity(tag_seq, char_seq)\n    LOC = get_LOC_entity(tag_seq, char_seq)\n    ORG = get_ORG_entity(tag_seq, char_seq)\n    return PER, LOC, ORG\n\n\ndef get_PER_entity(tag_seq, char_seq):\n    length = len(char_seq)\n    PER = []\n    for i, (char, tag) in enumerate(zip(char_seq, tag_seq)):\n        if tag == 'B-PER':\n            if 'per' in locals().keys():\n                PER.append(per)\n                del per\n            per = char\n            if i+1 == length:\n                PER.append(per)\n        if tag == 'I-PER':\n            per += char\n            if i+1 == length:\n                PER.append(per)\n        if tag not in ['I-PER', 'B-PER']:\n            if 'per' in locals().keys():\n                PER.append(per)\n                del per\n            continue\n    return PER\n\n\ndef get_LOC_entity(tag_seq, char_seq):\n    length = len(char_seq)\n    LOC = []\n    for i, (char, tag) in enumerate(zip(char_seq, tag_seq)):\n        if tag == 'B-LOC':\n            if 'loc' in locals().keys():\n                LOC.append(loc)\n                del loc\n            loc = char\n            if i+1 == length:\n                LOC.append(loc)\n        if tag == 'I-LOC':\n            loc += char\n            if i+1 == length:\n                LOC.append(loc)\n        if tag not in ['I-LOC', 'B-LOC']:\n            if 'loc' in locals().keys():\n                LOC.append(loc)\n                del loc\n            continue\n    return LOC\n\n\ndef get_ORG_entity(tag_seq, char_seq):\n    length = len(char_seq)\n    ORG = []\n    for i, (char, tag) in enumerate(zip(char_seq, tag_seq)):\n        if tag == 'B-ORG':\n            if 'org' in locals().keys():\n                ORG.append(org)\n                del org\n            org = char\n            if i+1 == length:\n                ORG.append(org)\n        if tag == 'I-ORG':\n            org += char\n            if i+1 == length:\n                ORG.append(org)\n        if tag not in ['I-ORG', 'B-ORG']:\n            if 'org' in locals().keys():\n                ORG.append(org)\n                del org\n            continue\n    return ORG\n\n\ndef get_logger(filename):\n    logger = logging.getLogger('logger')\n    logger.setLevel(logging.DEBUG)\n    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n    handler = logging.FileHandler(filename)\n    handler.setLevel(logging.DEBUG)\n    handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n    logging.getLogger().addHandler(handler)\n    return logger\n"""
