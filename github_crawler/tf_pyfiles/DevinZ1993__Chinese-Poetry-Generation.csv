file_path,api_count,code
char2vec.py,0,"b'#! /usr/bin/env python3\n# -*- coding:utf-8 -*-\n\nfrom char_dict import CharDict\nfrom gensim import models\nfrom numpy.random import uniform\nfrom paths import char2vec_path, check_uptodate\nfrom poems import Poems\nfrom singleton import Singleton\nfrom utils import CHAR_VEC_DIM\nimport numpy as np\nimport os\n\n\ndef _gen_char2vec():\n    print(""Generating char2vec model ..."")\n    char_dict = CharDict()\n    poems = Poems()\n    model = models.Word2Vec(poems, size = CHAR_VEC_DIM, min_count = 5)\n    embedding = uniform(-1.0, 1.0, [len(char_dict), CHAR_VEC_DIM])\n    for i, ch in enumerate(char_dict):\n        if ch in model.wv:\n            embedding[i, :] = model.wv[ch]\n    np.save(char2vec_path, embedding)\n\n\nclass Char2Vec(Singleton):\n\n    def __init__(self):\n        if not check_uptodate(char2vec_path):\n            _gen_char2vec()\n        self.embedding = np.load(char2vec_path)\n        self.char_dict = CharDict()\n\n    def get_embedding(self):\n        return self.embedding\n\n    def get_vect(self, ch):\n        return self.embedding[self.char_dict.char2int(ch)]\n\n    def get_vects(self, text):\n        return np.stack(map(self.get_vect, text)) if len(text) > 0 \\\n                else np.reshape(np.array([[]]), [0, CHAR_VEC_DIM])\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    char2vec = Char2Vec()\n\n'"
char_dict.py,0,"b'#! /usr/bin/env python3\n#-*- coding:utf-8 -*-\n\nfrom paths import raw_dir, char_dict_path, check_uptodate\nfrom singleton import Singleton\nfrom utils import is_cn_char\nimport os\n\n\nMAX_DICT_SIZE = 6000\n\n_corpus_list = [\'qts_tab.txt\', \'qss_tab.txt\', \'qsc_tab.txt\', \'qtais_tab.txt\',\n        \'yuan.all\', \'ming.all\', \'qing.all\']\n\n\ndef start_of_sentence():\n    return \'^\'\n\ndef end_of_sentence():\n    return \'$\'\n\ndef _gen_char_dict():\n    print(""Generating dictionary from corpus ..."")\n    \n    # Count char frequencies.\n    char_cnts = dict()\n    for corpus in _corpus_list:\n        with open(os.path.join(raw_dir, corpus), \'r\') as fin:\n            for ch in filter(is_cn_char, fin.read()):\n                if ch not in char_cnts:\n                    char_cnts[ch] = 1\n                else:\n                    char_cnts[ch] += 1\n    \n    # Sort in decreasing order of frequency.\n    cnt2char = sorted(char_cnts.items(), key = lambda x: -x[1])\n\n    # Store most popular chars into the file.\n    with open(char_dict_path, \'w\') as fout:\n        for i in range(min(MAX_DICT_SIZE - 2, len(cnt2char))):\n            fout.write(cnt2char[i][0])\n\n\nclass CharDict(Singleton):\n\n    def __init__(self):\n        if not check_uptodate(char_dict_path):\n            _gen_char_dict()\n        self._int2char = []\n        self._char2int = dict()\n        # Add start-of-sentence symbol.\n        self._int2char.append(start_of_sentence())\n        self._char2int[start_of_sentence()] = 0\n        with open(char_dict_path, \'r\') as fin:\n            idx = 1\n            for ch in fin.read():\n                self._int2char.append(ch)\n                self._char2int[ch] = idx\n                idx += 1\n        # Add end-of-sentence symbol.\n        self._int2char.append(end_of_sentence())\n        self._char2int[end_of_sentence()] = len(self._int2char) - 1\n\n    def char2int(self, ch):\n        if ch not in self._char2int:\n            return -1\n        return self._char2int[ch]\n\n    def int2char(self, idx):\n        return self._int2char[idx]\n\n    def __len__(self):\n        return len(self._int2char)\n\n    def __iter__(self):\n        return iter(self._int2char)\n\n    def __contains__(self, ch):\n        return ch in self._char2int\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    char_dict = CharDict()\n    for i in range(10):\n        ch = char_dict.int2char(i)\n        print(ch)\n        assert i == char_dict.char2int(ch)\n\n'"
data_utils.py,0,"b'#! /usr/bin/env python3\n#-*- coding:utf-8 -*-\n\nfrom char_dict import end_of_sentence, start_of_sentence\nfrom paths import gen_data_path, plan_data_path, check_uptodate\nfrom poems import Poems\nfrom rank_words import RankedWords\nfrom segment import Segmenter\nimport re\nimport subprocess\n\n\ndef gen_train_data():\n    print(""Generating training data ..."")\n    segmenter = Segmenter()\n    poems = Poems()\n    poems.shuffle()\n    ranked_words = RankedWords()\n    plan_data = []\n    gen_data = []\n    for poem in poems:\n        if len(poem) != 4:\n            continue # Only consider quatrains.\n        valid = True\n        context = start_of_sentence()\n        gen_lines = []\n        keywords = []\n        for sentence in poem:\n            if len(sentence) != 7:\n                valid = False\n                break\n            words = list(filter(lambda seg: seg in ranked_words, \n                    segmenter.segment(sentence)))\n            if len(words) == 0:\n                valid = False\n                break\n            keyword = words[0]\n            for word in words[1 : ]:\n                if ranked_words.get_rank(word) < ranked_words.get_rank(keyword):\n                    keyword = word\n            gen_line = sentence + end_of_sentence() + \\\n                    \'\\t\' + keyword + \'\\t\' + context + \'\\n\'\n            gen_lines.append(gen_line)\n            keywords.append(keyword)\n            context += sentence + end_of_sentence()\n        if valid:\n            plan_data.append(\'\\t\'.join(keywords) + \'\\n\')\n            gen_data.extend(gen_lines)\n    with open(plan_data_path, \'w\') as fout:\n        for line in plan_data:\n            fout.write(line)\n    with open(gen_data_path, \'w\') as fout:\n        for line in gen_data:\n            fout.write(line)\n\n\ndef batch_train_data(batch_size):\n    """""" Training data generator for the poem generator.""""""\n    gen_train_data() # Shuffle data order and cool down CPU.\n    keywords = []\n    contexts = []\n    sentences = []\n    with open(gen_data_path, \'r\') as fin:\n        for line in fin.readlines():\n            toks = line.strip().split(\'\\t\')\n            sentences.append(toks[0])\n            keywords.append(toks[1])\n            contexts.append(toks[2])\n            if len(keywords) == batch_size:\n                yield keywords, contexts, sentences\n                keywords.clear()\n                contexts.clear()\n                sentences.clear()\n        # For simplicity, only return full batches for now.\n\n\nif __name__ == \'__main__\':\n    if not check_uptodate(plan_data_path) or \\\n            not check_uptodate(gen_data_path):\n        gen_train_data()\n\n'"
generate.py,61,"b'#! /usr/bin/env python3\n# -*- coding:utf-8 -*-\n\nfrom char2vec import Char2Vec\nfrom char_dict import CharDict, end_of_sentence, start_of_sentence\nfrom data_utils import batch_train_data\nfrom paths import save_dir\nfrom pron_dict import PronDict\nfrom random import random\nfrom singleton import Singleton\nfrom utils import CHAR_VEC_DIM, NUM_OF_SENTENCES\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\n\n\n_BATCH_SIZE = 64\n_NUM_UNITS = 512\n\n_model_path = os.path.join(save_dir, \'model\')\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\nclass Generator(Singleton):\n\n    def _build_keyword_encoder(self):\n        """""" Encode keyword into a vector.""""""\n        self.keyword = tf.placeholder(\n                shape = [_BATCH_SIZE, None, CHAR_VEC_DIM],\n                dtype = tf.float32, \n                name = ""keyword"")\n        self.keyword_length = tf.placeholder(\n                shape = [_BATCH_SIZE],\n                dtype = tf.int32,\n                name = ""keyword_length"")\n        _, bi_states = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = tf.contrib.rnn.GRUCell(_NUM_UNITS / 2),\n                cell_bw = tf.contrib.rnn.GRUCell(_NUM_UNITS / 2),\n                inputs = self.keyword,\n                sequence_length = self.keyword_length,\n                dtype = tf.float32, \n                time_major = False,\n                scope = ""keyword_encoder"")\n        self.keyword_state = tf.concat(bi_states, axis = 1)\n        tf.TensorShape([_BATCH_SIZE, _NUM_UNITS]).\\\n                assert_same_rank(self.keyword_state.shape)\n\n    def _build_context_encoder(self):\n        """""" Encode context into a list of vectors. """"""\n        self.context = tf.placeholder(\n                shape = [_BATCH_SIZE, None, CHAR_VEC_DIM],\n                dtype = tf.float32, \n                name = ""context"")\n        self.context_length = tf.placeholder(\n                shape = [_BATCH_SIZE],\n                dtype = tf.int32,\n                name = ""context_length"")\n        bi_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = tf.contrib.rnn.GRUCell(_NUM_UNITS / 2),\n                cell_bw = tf.contrib.rnn.GRUCell(_NUM_UNITS / 2),\n                inputs = self.context,\n                sequence_length = self.context_length,\n                dtype = tf.float32, \n                time_major = False,\n                scope = ""context_encoder"")\n        self.context_outputs = tf.concat(bi_outputs, axis = 2)\n        tf.TensorShape([_BATCH_SIZE, None, _NUM_UNITS]).\\\n                assert_same_rank(self.context_outputs.shape)\n\n    def _build_decoder(self):\n        """""" Decode keyword and context into a sequence of vectors. """"""\n        attention = tf.contrib.seq2seq.BahdanauAttention(\n                num_units = _NUM_UNITS, \n                memory = self.context_outputs,\n                memory_sequence_length = self.context_length)\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n                cell = tf.contrib.rnn.GRUCell(_NUM_UNITS),\n                attention_mechanism = attention)\n        self.decoder_init_state = decoder_cell.zero_state(\n                batch_size = _BATCH_SIZE, dtype = tf.float32).\\\n                        clone(cell_state = self.keyword_state)\n        self.decoder_inputs = tf.placeholder(\n                shape = [_BATCH_SIZE, None, CHAR_VEC_DIM],\n                dtype = tf.float32, \n                name = ""decoder_inputs"")\n        self.decoder_input_length = tf.placeholder(\n                shape = [_BATCH_SIZE],\n                dtype = tf.int32,\n                name = ""decoder_input_length"")\n        self.decoder_outputs, self.decoder_final_state = tf.nn.dynamic_rnn(\n                cell = decoder_cell,\n                inputs = self.decoder_inputs,\n                sequence_length = self.decoder_input_length,\n                initial_state = self.decoder_init_state,\n                dtype = tf.float32, \n                time_major = False,\n                scope = ""training_decoder"")\n        tf.TensorShape([_BATCH_SIZE, None, _NUM_UNITS]).\\\n                assert_same_rank(self.decoder_outputs.shape)\n\n    def _build_projector(self):\n        """""" Project decoder_outputs into character space. """"""\n        softmax_w = tf.Variable(\n                tf.random_normal(shape = [_NUM_UNITS, len(self.char_dict)],\n                    mean = 0.0, stddev = 0.08), \n                trainable = True)\n        softmax_b = tf.Variable(\n                tf.random_normal(shape = [len(self.char_dict)],\n                    mean = 0.0, stddev = 0.08),\n                trainable = True)\n        reshaped_outputs = self._reshape_decoder_outputs()\n        self.logits = tf.nn.bias_add(\n                tf.matmul(reshaped_outputs, softmax_w),\n                bias = softmax_b)\n        self.probs = tf.nn.softmax(self.logits)\n\n    def _reshape_decoder_outputs(self):\n        """""" Reshape decoder_outputs into shape [?, _NUM_UNITS]. """"""\n        def concat_output_slices(idx, val):\n            output_slice = tf.slice(\n                    input_ = self.decoder_outputs,\n                    begin = [idx, 0, 0],\n                    size = [1, self.decoder_input_length[idx],  _NUM_UNITS])\n            return tf.add(idx, 1),\\\n                    tf.concat([val, tf.squeeze(output_slice, axis = 0)], \n                            axis = 0)\n        tf_i = tf.constant(0)\n        tf_v = tf.zeros(shape = [0, _NUM_UNITS], dtype = tf.float32)\n        _, reshaped_outputs = tf.while_loop(\n                cond = lambda i, v: i < _BATCH_SIZE,\n                body = concat_output_slices,\n                loop_vars = [tf_i, tf_v],\n                shape_invariants = [tf.TensorShape([]),\n                    tf.TensorShape([None, _NUM_UNITS])])\n        tf.TensorShape([None, _NUM_UNITS]).\\\n                assert_same_rank(reshaped_outputs.shape)\n        return reshaped_outputs\n\n    def _build_optimizer(self):\n        """""" Define cross-entropy loss and minimize it. """"""\n        self.targets = tf.placeholder(\n                shape = [None],\n                dtype = tf.int32, \n                name = ""targets"")\n        labels = tf.one_hot(self.targets, depth = len(self.char_dict))\n        cross_entropy = tf.losses.softmax_cross_entropy(\n                onehot_labels = labels,\n                logits = self.logits)\n        self.loss = tf.reduce_mean(cross_entropy)\n\n        self.learning_rate = tf.clip_by_value(\n                tf.multiply(1.6e-5, tf.pow(2.1, self.loss)),\n                clip_value_min = 0.0002,\n                clip_value_max = 0.02)\n        self.opt_step = tf.train.AdamOptimizer(\n                learning_rate = self.learning_rate).\\\n                        minimize(loss = self.loss)\n\n    def _build_graph(self):\n        self._build_keyword_encoder()\n        self._build_context_encoder()\n        self._build_decoder()\n        self._build_projector()\n        self._build_optimizer()\n\n    def __init__(self):\n        self.char_dict = CharDict()\n        self.char2vec = Char2Vec()\n        self._build_graph()\n        if not os.path.exists(save_dir):\n            os.mkdir(save_dir)\n        self.saver = tf.train.Saver(tf.global_variables())\n        self.trained = False\n        \n    def _initialize_session(self, session):\n        checkpoint = tf.train.get_checkpoint_state(save_dir)\n        if not checkpoint or not checkpoint.model_checkpoint_path:\n            init_op = tf.group(tf.global_variables_initializer(),\n                    tf.local_variables_initializer())\n            session.run(init_op)\n        else:\n            self.saver.restore(session, checkpoint.model_checkpoint_path)\n            self.trained = True\n\n    def generate(self, keywords):\n        assert NUM_OF_SENTENCES == len(keywords)\n        pron_dict = PronDict()\n        context = start_of_sentence()\n        with tf.Session() as session:\n            self._initialize_session(session)\n            if not self.trained:\n                print(""Please train the model first! (./train.py -g)"")\n                sys.exit(1)\n            for keyword in keywords:\n                keyword_data, keyword_length = self._fill_np_matrix(\n                        [keyword] * _BATCH_SIZE)\n                context_data, context_length = self._fill_np_matrix(\n                        [context] * _BATCH_SIZE)\n                char = start_of_sentence()\n                for _ in range(7):\n                    decoder_input, decoder_input_length = \\\n                            self._fill_np_matrix([char])\n                    encoder_feed_dict = {\n                            self.keyword : keyword_data,\n                            self.keyword_length : keyword_length,\n                            self.context : context_data,\n                            self.context_length : context_length,\n                            self.decoder_inputs : decoder_input,\n                            self.decoder_input_length : decoder_input_length\n                            }\n                    if char == start_of_sentence():\n                        pass\n                    else:\n                        encoder_feed_dict[self.decoder_init_state] = state\n                    probs, state = session.run(\n                            [self.probs, self.decoder_final_state], \n                            feed_dict = encoder_feed_dict)\n                    prob_list = self._gen_prob_list(probs, context, pron_dict)\n                    prob_sums = np.cumsum(prob_list)\n                    rand_val = prob_sums[-1] * random()\n                    for i, prob_sum in enumerate(prob_sums):\n                        if rand_val < prob_sum:\n                            char = self.char_dict.int2char(i)\n                            break\n                    context += char\n                context += end_of_sentence()\n        return context[1:].split(end_of_sentence())\n\n    def _gen_prob_list(self, probs, context, pron_dict):\n        prob_list = probs.tolist()[0]\n        prob_list[0] = 0\n        prob_list[-1] = 0\n        idx = len(context)\n        used_chars = set(ch for ch in context)\n        for i in range(1, len(prob_list) - 1):\n            ch = self.char_dict.int2char(i)\n            # Penalize used characters.\n            if ch in used_chars:\n                prob_list[i] *= 0.6\n            # Penalize rhyming violations.\n            if (idx == 15 or idx == 31) and \\\n                    not pron_dict.co_rhyme(ch, context[7]):\n                prob_list[i] *= 0.2\n            # Penalize tonal violations.\n            if idx > 2 and 2 == idx % 8 and \\\n                    not pron_dict.counter_tone(context[2], ch):\n                prob_list[i] *= 0.4\n            if (4 == idx % 8 or 6 == idx % 8) and \\\n                    not pron_dict.counter_tone(context[idx - 2], ch):\n                prob_list[i] *= 0.4\n        return prob_list\n\n    def train(self, n_epochs = 6):\n        print(""Training RNN-based generator ..."")\n        with tf.Session() as session:\n            self._initialize_session(session)\n            try:\n                for epoch in range(n_epochs):\n                    batch_no = 0\n                    for keywords, contexts, sentences \\\n                            in batch_train_data(_BATCH_SIZE):\n                        sys.stdout.write(""[Seq2Seq Training] epoch = %d, "" \\\n                                ""line %d to %d ..."" % \n                                (epoch, batch_no * _BATCH_SIZE,\n                                (batch_no + 1) * _BATCH_SIZE))\n                        sys.stdout.flush()\n                        self._train_a_batch(session, epoch,\n                                keywords, contexts, sentences)\n                        batch_no += 1\n                        if 0 == batch_no % 32:\n                            self.saver.save(session, _model_path)\n                    self.saver.save(session, _model_path)\n                print(""Training is done."")\n            except KeyboardInterrupt:\n                print(""Training is interrupted."")\n\n    def _train_a_batch(self, session, epoch, keywords, contexts, sentences):\n        keyword_data, keyword_length = self._fill_np_matrix(keywords)\n        context_data, context_length = self._fill_np_matrix(contexts)\n        decoder_inputs, decoder_input_length  = self._fill_np_matrix(\n                [start_of_sentence() + sentence[:-1] \\\n                        for sentence in sentences])\n        targets = self._fill_targets(sentences)\n        feed_dict = {\n                self.keyword : keyword_data,\n                self.keyword_length : keyword_length,\n                self.context : context_data,\n                self.context_length : context_length,\n                self.decoder_inputs : decoder_inputs,\n                self.decoder_input_length : decoder_input_length,\n                self.targets : targets\n                }\n        loss, learning_rate, _ = session.run(\n                [self.loss, self.learning_rate, self.opt_step],\n                feed_dict = feed_dict)\n        print("" loss =  %f, learning_rate = %f"" % (loss, learning_rate))\n\n    def _fill_np_matrix(self, texts):\n        max_time = max(map(len, texts))\n        matrix = np.zeros([_BATCH_SIZE, max_time, CHAR_VEC_DIM], \n                dtype = np.float32)\n        for i in range(_BATCH_SIZE):\n            for j in range(max_time):\n                matrix[i, j, :] = self.char2vec.get_vect(end_of_sentence())\n        for i, text in enumerate(texts):\n            matrix[i, : len(text)] = self.char2vec.get_vects(text)\n        seq_length = [len(texts[i]) if i < len(texts) else 0 \\\n                for i in range(_BATCH_SIZE)]\n        return matrix, seq_length\n\n    def _fill_targets(self, sentences):\n        targets = []\n        for sentence in sentences:\n            targets.extend(map(self.char_dict.char2int, sentence))\n        return targets\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    generator = Generator()\n    keywords = [\'\xe5\x9b\x9b\xe6\x97\xb6\', \'\xe5\x8f\x98\', \'\xe9\x9b\xaa\', \'\xe6\x96\xb0\']\n    poem = generator.generate(keywords)\n    for sentence in poem:\n        print(sentence)\n\n'"
main.py,0,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nfrom generate import Generator\nfrom plan import Planner\n\n\nif __name__ == \'__main__\':\n    planner = Planner()\n    generator = Generator()\n    while True:\n        hints = input(""Type in hints >> "")\n        keywords = planner.plan(hints)\n        print(""Keywords: "" + \' \'.join(keywords))\n        poem = generator.generate(keywords)\n        print(""Poem generated:"")\n        for sentence in poem:\n            print(sentence)\n\n'"
paths.py,0,"b'#! /usr/bin/env python3\n# -*- coding:utf-8 -*-\n\nimport os\n\n\nroot_dir = os.path.dirname(__file__)\ndata_dir = os.path.join(root_dir, \'data\')\nraw_dir = os.path.join(root_dir, \'raw\')\nsave_dir = os.path.join(root_dir, \'save\')\n\nsxhy_path = os.path.join(data_dir, \'sxhy_dict.txt\')\nchar_dict_path = os.path.join(data_dir, \'char_dict.txt\')\npoems_path = os.path.join(data_dir, \'poem.txt\')\nchar2vec_path = os.path.join(data_dir, \'char2vec.npy\')\nwordrank_path = os.path.join(data_dir, \'wordrank.txt\')\nplan_data_path = os.path.join(data_dir, \'plan_data.txt\')\ngen_data_path = os.path.join(data_dir, \'gen_data.txt\')\n\n\n# TODO: configure dependencies in another file.\n_dependency_dict = {\n        poems_path : [char_dict_path],\n        char2vec_path : [char_dict_path, poems_path],\n        wordrank_path : [sxhy_path, poems_path],\n        gen_data_path : [char_dict_path, poems_path, sxhy_path, char2vec_path],\n        plan_data_path : [char_dict_path, poems_path, sxhy_path, char2vec_path],\n        }\n\ndef check_uptodate(path):\n    """""" Return true iff the file exists and up-to-date with dependencies.""""""\n    if not os.path.exists(path):\n        # File not found.\n        return False\n    timestamp = os.path.getmtime(path)\n    if path in _dependency_dict:\n        for dependency in _dependency_dict[path]:\n            if not os.path.exists(dependency) or \\\n                    os.path.getmtime(dependency) > timestamp:\n                # File stale.\n                return False\n    return True\n\n'"
plan.py,0,"b'#! /usr/bin/env python3\n#-*- coding:utf-8 -*-\n\nfrom data_utils import gen_train_data\nfrom gensim import models\nfrom paths import save_dir, plan_data_path, check_uptodate\nfrom random import random, shuffle\nfrom rank_words import RankedWords\nfrom singleton import Singleton\nfrom utils import split_sentences, NUM_OF_SENTENCES\nimport jieba\nimport os\n\n\n_plan_model_path = os.path.join(save_dir, \'plan_model.bin\')\n\n\ndef train_planner():\n    # TODO: try other keyword-expansion models.\n    print(""Training Word2Vec-based planner ..."")\n    if not os.path.exists(save_dir):\n        os.mkdir(save_dir)\n    if not check_uptodate(plan_data_path):\n        gen_train_data()\n    word_lists = []\n    with open(plan_data_path, \'r\') as fin:\n        for line in fin.readlines():\n            word_lists.append(line.strip().split(\'\\t\'))\n    model = models.Word2Vec(word_lists, size = 512, min_count = 5)\n    model.save(_plan_model_path)\n\n\nclass Planner(Singleton):\n\n    def __init__(self):\n        self.ranked_words = RankedWords()\n        if not os.path.exists(_plan_model_path):\n            train_planner()\n        self.model = models.Word2Vec.load(_plan_model_path)\n\n    def plan(self, text):\n        return self._expand(self._extract(text))\n\n    def _expand(self, keywords):\n        if len(keywords) < NUM_OF_SENTENCES:\n            filtered_keywords = list(filter(lambda w : w in \\\n                    self.model.wv,  keywords))\n            if len(filtered_keywords) > 0:\n                similars = self.model.wv.most_similar(\n                        positive = filtered_keywords)\n                # Sort similar words in decreasing similarity with randomness.\n                similars = sorted(similars, key = lambda x: x[1] * random())\n                for similar in similars:\n                    keywords.add(similar[0])\n                    if len(keywords) == NUM_OF_SENTENCES:\n                        break\n            prob_sum = sum(1. / (i + 1) \\\n                    for i, word in enumerate(self.ranked_words) \\\n                    if word not in keywords)\n            rand_val = prob_sum * random()\n            word_idx = 0\n            s = 0\n            while len(keywords) < NUM_OF_SENTENCES \\\n                    and word_idx < len(self.ranked_words):\n                word = self.ranked_words[word_idx]\n                s += 1.0 / (word_idx + 1)\n                if word not in keywords and rand_val < s:\n                    keywords.add(word)\n                word_idx += 1\n        results = list(keywords)\n        shuffle(results)\n        return results\n\n    def _extract(self, text):\n        def extract_from_sentence(sentence):\n            return filter(lambda w : w in self.ranked_words,\n                jieba.lcut(sentence))\n        keywords = set()\n        for sentence in split_sentences(text):\n            keywords.update(extract_from_sentence(sentence))\n        return keywords\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    planner = Planner()\n    keywords = planner.plan(""\xe6\x98\xa5\xe5\xa4\xa9\xe5\x88\xb0\xe4\xba\x86\xef\xbc\x8c\xe6\xa1\x83\xe8\x8a\xb1\xe5\xbc\x80\xe4\xba\x86\xe3\x80\x82"")\n    print(keywords)\n\n'"
poems.py,0,"b'#! /usr/bin/env python3\n# -*- coding:utf-8 -*-\n\nfrom char_dict import CharDict\nfrom paths import raw_dir, poems_path, check_uptodate\nfrom random import shuffle\nfrom singleton import Singleton\nfrom utils import split_sentences\nimport os\n\n_corpus_list = [\'qts_tab.txt\']\n\n\ndef _gen_poems():\n    print(""Parsing poems ..."")\n    char_dict = CharDict()\n    with open(poems_path, \'w\') as fout:\n        for corpus in _corpus_list:\n            with open(os.path.join(raw_dir, corpus), \'r\') as fin:\n                for line in fin.readlines()[1 : ]:\n                    sentences = split_sentences(line.strip().split()[-1])\n                    all_char_in_dict = True\n                    for sentence in sentences:\n                        for ch in sentence:\n                            if char_dict.char2int(ch) < 0:\n                                all_char_in_dict = False\n                                break\n                        if not all_char_in_dict:\n                            break\n                    if all_char_in_dict:\n                        fout.write(\' \'.join(sentences) + \'\\n\')\n            print(""Finished parsing %s."" % corpus)\n\n\nclass Poems(Singleton):\n\n    def __init__(self):\n        if not check_uptodate(poems_path):\n            _gen_poems()\n        self.poems = []\n        with open(poems_path, \'r\') as fin:\n            for line in fin.readlines():\n                self.poems.append(line.strip().split())\n\n    def __getitem__(self, index):\n        if index < 0 or index >= len(self.poems):\n            return None\n        return self.poems[index]\n\n    def __len__(self):\n        return len(self.poems)\n\n    def __iter__(self):\n        return iter(self.poems)\n\n    def shuffle(self):\n        shuffle(self.poems)\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    poems = Poems()\n    for i in range(10):\n        print(\' \'.join(poems[i]))\n\n'"
pron_dict.py,0,"b'#! /usr/bin/env python3\n# -*- coding:utf-8 -*-\n\nfrom char_dict import CharDict\nfrom paths import raw_dir\nfrom singleton import Singleton\nimport os\n\n_pinyin_path = os.path.join(raw_dir, \'pinyin.txt\')\n\n\ndef _get_vowel(pinyin):\n    i = len(pinyin) - 1\n    while i >= 0 and \\\n            pinyin[i] in [\'A\', \'O\', \'E\', \'I\', \'U\', \'V\']:\n        i -= 1\n    return pinyin[i+1 : ]\n\ndef _get_rhyme(pinyin):\n    vowel = _get_vowel(pinyin)\n    if vowel in [\'A\', \'IA\', \'UA\']:\n        return 1\n    elif vowel in [\'O\', \'E\', \'UO\']:\n        return 2\n    elif vowel in [\'IE\', \'VE\']:\n        return 3\n    elif vowel in [\'AI\', \'UAI\']:\n        return 4\n    elif vowel in [\'EI\', \'UI\']:\n        return 5\n    elif vowel in [\'AO\', \'IAO\']:\n        return 6\n    elif vowel in [\'OU\', \'IU\']:\n        return 7\n    elif vowel in [\'AN\', \'IAN\', \'UAN\', \'VAN\']:\n        return 8\n    elif vowel in [\'EN\', \'IN\', \'UN\', \'VN\']:\n        return 9\n    elif vowel in [\'ANG\', \'IANG\', \'UANG\']:\n        return 10\n    elif vowel in [\'ENG\', \'ING\']:\n        return 11\n    elif vowel in [\'ONG\', \'IONG\']:\n        return 12\n    elif (vowel == \'I\' and not pinyin[0] in [\'Z\', \'C\', \'S\', \'R\']) \\\n            or vowel == \'V\':\n        return 13\n    elif vowel == \'I\':\n        return 14\n    elif vowel == \'U\':\n        return 15\n    return 0\n\n\nclass PronDict(Singleton):\n\n    def __init__(self):\n        self.char_dict = CharDict()\n        self._pron_dict = dict()\n        with open(_pinyin_path, \'r\') as fin:\n            for line in fin.readlines():\n                toks = line.strip().split()\n                ch = chr(int(toks[0], 16))\n                if ch not in self.char_dict:\n                    continue\n                self._pron_dict[ch] = []\n                for tok in toks[1 : ]:\n                    self._pron_dict[ch].append((tok[:-1], int(tok[-1])))\n\n    def co_rhyme(self, a, b):\n        """""" Return True if two pinyins may have the same rhyme. """"""\n        if a in self._pron_dict and b in self._pron_dict:\n            a_rhymes = map(lambda x : _get_rhyme(x[0]), self._pron_dict[a])\n            b_rhymes = map(lambda x : _get_rhyme(x[0]), self._pron_dict[b])\n            for a_rhyme in a_rhymes:\n                if a_rhyme in b_rhymes:\n                    return True\n        return False\n\n    def counter_tone(self, a, b):\n        """""" Return True if two pinyins may have opposite tones. """"""\n        if a in self._pron_dict and b in self._pron_dict:\n            level_tone = lambda x : x == 1 or x == 2\n            a_tones = map(lambda x : level_tone(x[1]), self._pron_dict[a])\n            b_tones = map(lambda x : level_tone(x[1]), self._pron_dict[b])\n            for a_tone in a_tones:\n                if (not a_tone) in b_tones:\n                    return True\n        return False\n\n    def __iter__(self):\n        return iter(self._pron_dict)\n\n    def __getitem__(self, ch):\n        return self._pron_dict[ch]\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    pron_dict = PronDict()\n    assert pron_dict.co_rhyme(\'\xe7\x94\x9f\', \'\xe6\x83\x85\')\n    assert not pron_dict.co_rhyme(\'\xe8\x9b\xa4\', \'\xe4\xba\xba\')\n    assert pron_dict.counter_tone(\'\xe5\xb9\xb3\', \'\xe4\xbb\x84\')\n    assert not pron_dict.counter_tone(\'\xe8\xb5\xb7\', \'\xe5\xbc\x83\')\n    cnt = 0\n    for ch in pron_dict:\n        print(ch + "": ""+str(pron_dict[ch]))\n        cnt += 1\n        if cnt > 20:\n            break\n\n'"
rank_words.py,0,"b'#! /usr/bin/env python3\n#-*- coding:utf-8 -*-\n\nfrom paths import raw_dir, wordrank_path, check_uptodate\nfrom poems import Poems\nfrom segment import Segmenter\nfrom singleton import Singleton\nimport json\nimport os\nimport sys\n\n\n_stopwords_path = os.path.join(raw_dir, \'stopwords.txt\')\n\n\n_damp = 0.85\n\n\ndef _get_stopwords():\n    stopwords = set()\n    with open(_stopwords_path, \'r\') as fin:\n        for line in fin.readlines():\n            stopwords.add(line.strip())\n    return stopwords\n\n\n# TODO: try other keyword-extraction algorithms. This doesn\'t work well.\n\nclass RankedWords(Singleton):\n\n    def __init__(self):\n        self.stopwords = _get_stopwords()\n        if not check_uptodate(wordrank_path):\n            self._do_text_rank()\n        with open(wordrank_path, \'r\') as fin:\n            self.word_scores = json.load(fin)\n        self.word2rank = dict((word_score[0], rank) \n                for rank, word_score in enumerate(self.word_scores))\n\n    def _do_text_rank(self):\n        print(""Do text ranking ..."")\n        adjlists = self._get_adjlists()\n        print(""[TextRank] Total words: %d"" % len(adjlists))\n\n        # Value initialization.\n        scores = dict()\n        for word in adjlists:\n            scores[word] = [1.0, 1.0]\n\n        # Synchronous value iterations.\n        itr = 0\n        while True:\n            sys.stdout.write(""[TextRank] Iteration %d ..."" % itr)\n            sys.stdout.flush()\n            for word, adjlist in adjlists.items():\n                scores[word][1] = (1.0 - _damp) + _damp * \\\n                        sum(adjlists[other][word] * scores[other][0] \n                                for other in adjlist)\n            eps = 0\n            for word in scores:\n                eps = max(eps, abs(scores[word][0] - scores[word][1]))\n                scores[word][0] = scores[word][1]\n            print("" eps = %f"" % eps)\n            if eps <= 1e-6:\n                break\n            itr += 1\n\n        # Dictionary-based comparison with TextRank score as a tie-breaker.\n        segmenter = Segmenter()\n        def cmp_key(x):\n            word, score = x\n            return (0 if word in segmenter.sxhy_dict else 1, -score)\n        words = sorted([(word, score[0]) for word, score in scores.items()], \n                key = cmp_key)\n\n        # Store ranked words and scores.\n        with open(wordrank_path, \'w\') as fout:\n            json.dump(words, fout)\n\n    def _get_adjlists(self):\n        print(""[TextRank] Generating word graph ..."")\n        segmenter = Segmenter()\n        poems = Poems()\n        adjlists = dict()\n        # Count number of co-occurrence.\n        for poem in poems:\n            for sentence in poem:\n                words = []\n                for word in segmenter.segment(sentence):\n                    if word not in self.stopwords:\n                        words.append(word)\n                for word in words:\n                    if word not in adjlists:\n                        adjlists[word] = dict()\n                for i in range(len(words)):\n                    for j in range(i + 1, len(words)):\n                        if words[j] not in adjlists[words[i]]:\n                            adjlists[words[i]][words[j]] = 1.0\n                        else:\n                            adjlists[words[i]][words[j]] += 1.0\n                        if words[i] not in adjlists[words[j]]:\n                            adjlists[words[j]][words[i]] = 1.0\n                        else:\n                            adjlists[words[j]][words[i]] += 1.0\n        # Normalize weights.\n        for a in adjlists:\n            sum_w = sum(w for _, w in adjlists[a].items())\n            for b in adjlists[a]:\n                adjlists[a][b] /= sum_w\n        return adjlists\n\n    def __getitem__(self, index):\n        if index < 0 or index >= len(self.word_scores):\n            return None\n        return self.word_scores[index][0]\n\n    def __len__(self):\n        return len(self.word_scores)\n\n    def __iter__(self):\n        return map(lambda x: x[0], self.word_scores)\n\n    def __contains__(self, word):\n        return word in self.word2rank\n\n    def get_rank(self, word):\n        if word not in self.word2rank:\n            return len(self.word2rank)\n        return self.word2rank[word]\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    ranked_words = RankedWords()\n    for i in range(100):\n        print(ranked_words[i])\n\n'"
segment.py,0,"b'#! /usr/bin/env python3\n#-*- coding:utf-8 -*-\n\nfrom paths import raw_dir, sxhy_path, check_uptodate\nfrom singleton import Singleton\nfrom utils import is_cn_sentence, split_sentences\nimport jieba\nimport os\n\n\n_rawsxhy_path = os.path.join(raw_dir, \'shixuehanying.txt\')\n\n\ndef _gen_sxhy_dict():\n    print(""Parsing shixuehanying dictionary ..."")\n    words = set()\n    with open(_rawsxhy_path, \'r\') as fin:\n        for line in fin.readlines():\n            if line[0] == \'<\':\n                continue\n            for phrase in line.strip().split()[1:]:\n                if not is_cn_sentence(phrase):\n                    continue\n                idx = 0\n                while idx + 4 <= len(phrase):\n                    # Cut 2 chars each time.\n                    words.add(phrase[idx : idx + 2])\n                    idx += 2\n                # Use jieba to cut the last 3 chars.\n                if idx < len(phrase):\n                    for word in jieba.lcut(phrase[idx:]):\n                        words.add(word)\n    with open(sxhy_path, \'w\') as fout:\n        fout.write(\' \'.join(words))\n\n\nclass Segmenter(Singleton):\n\n    def __init__(self):\n        if not check_uptodate(sxhy_path):\n            _gen_sxhy_dict()\n        with open(sxhy_path, \'r\') as fin:\n            self.sxhy_dict = set(fin.read().split())\n\n    def segment(self, sentence):\n        # TODO: try CRF-based segmentation.\n        toks = []\n        idx = 0\n        while idx + 4 <= len(sentence):\n            # Cut 2 chars each time.\n            if sentence[idx : idx + 2] in self.sxhy_dict:\n                toks.append(sentence[idx : idx + 2])\n            else:\n                for tok in jieba.lcut(sentence[idx : idx + 2]):\n                    toks.append(tok)\n            idx += 2\n        # Cut last 3 chars.\n        if idx < len(sentence):\n            if sentence[idx : ] in self.sxhy_dict:\n                toks.append(sentence[idx : ])\n            else:\n                for tok in jieba.lcut(sentence[idx : ]):\n                    toks.append(tok)\n        return toks\n\n\n# For testing purpose.\nif __name__ == \'__main__\':\n    segmenter = Segmenter()\n    with open(os.path.join(raw_dir, \'qts_tab.txt\'), \'r\') as fin:\n        for line in fin.readlines()[1 : 6]:\n            for sentence in split_sentences(line.strip().split()[3]):\n                print(\' \'.join(segmenter.segment(sentence)))\n\n'"
singleton.py,0,"b'#! /usr/bin/env python3\n#-*- coding:utf-8 -*-\n\n# Reference: https://stackoverflow.com/questions/6760685/creating-a-singleton-in-python?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa.\n\nclass Singleton(object):\n    \n    _instance = None\n\n    def __new__(class_, *args, **kwargs):\n        if not class_._instance:\n            class_._instance = object.__new__(class_, *args, **kwargs)\n        return class_._instance\n\n'"
train.py,0,"b'#! /usr/bin/env python3\n#-*- coding:utf-8 -*-\n\nfrom generate import Generator\nfrom gensim import models\nfrom plan import train_planner\nfrom paths import save_dir\nimport argparse\nimport os\nimport sys\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = \'Chinese poem generation.\')\n    parser.add_argument(\'-p\', dest = \'planner\', default = False, \n            action = \'store_true\', help = \'train planning model\')\n    parser.add_argument(\'-g\', dest = \'generator\', default = False, \n            action = \'store_true\', help = \'train generation model\')\n    parser.add_argument(\'-a\', dest = \'all\', default = False,\n            action = \'store_true\', help = \'train both models\')\n    parser.add_argument(\'--clean\', dest = \'clean\', default = False,\n            action = \'store_true\', help = \'delete all models\')\n    args = parser.parse_args()\n    if args.clean:\n        for f in os.listdir(save_dir):\n            os.remove(os.path.join(save_dir, f))\n    else:\n        if args.all or args.planner:\n            train_planner()\n        if args.all or args.generator:\n            generator = Generator()\n            generator.train(n_epochs = 1000)\n        print(""All training is done!"")\n\n'"
utils.py,0,"b'#! /usr/bin/env python3\n# -*- coding:utf-8 -*-\n\n\ndef is_cn_char(ch):\n    """""" Test if a char is a Chinese character. """"""\n    return ch >= u\'\\u4e00\' and ch <= u\'\\u9fa5\'\n\ndef is_cn_sentence(sentence):\n    """""" Test if a sentence is made of Chinese characters. """"""\n    for ch in sentence:\n        if not is_cn_char(ch):\n            return False\n    return True\n\ndef split_sentences(text):\n    """""" Split a piece of text into a list of sentences. """"""\n    sentences = []\n    i = 0\n    for j in range(len(text) + 1):\n        if j == len(text) or \\\n                text[j] in [u\'\xef\xbc\x8c\', u\'\xe3\x80\x82\', u\'\xef\xbc\x81\', u\'\xef\xbc\x9f\', u\'\xe3\x80\x81\', u\'\\n\']:\n            if i < j:\n                sentence = u\'\'.join(filter(is_cn_char, text[i:j]))\n                sentences.append(sentence)\n            i = j + 1\n    return sentences\n\nNUM_OF_SENTENCES = 4\nCHAR_VEC_DIM = 512\n\n'"
