file_path,api_count,code
numplate_recognition_detection.py,6,"b'import numpy as np\nimport os\nimport sys\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nimport cv2\nimport pytesseract\n\nfrom custom_plate import allow_needed_values as anv \nfrom custom_plate import do_image_conversion as dic\n\nget_ipython().magic(\'matplotlib inline\')\nsys.path.append("".."")\n\nfrom utils import label_map_util\nfrom utils import visualization_utils as vis_util\n\nMODEL_NAME = \'numplate\'\nPATH_TO_CKPT = MODEL_NAME + \'/graph-200000/frozen_inference_graph.pb\'\nPATH_TO_LABELS = os.path.join(\'training\', \'object-detection.pbtxt\')\nNUM_CLASSES = 1\n\n\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n  od_graph_def = tf.GraphDef()\n  with tf.gfile.GFile(PATH_TO_CKPT, \'rb\') as fid:\n    serialized_graph = fid.read()\n    od_graph_def.ParseFromString(serialized_graph)\n    tf.import_graph_def(od_graph_def, name=\'\')\n\n\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\n\ndef load_image_into_numpy_array(image):\n  (im_width, im_height) = image.size\n  return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\n\nPATH_TO_TEST_IMAGES_DIR = \'png_tesseract/test_ram\'\nTEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, \'image{}.jpg\'.format(i)) for i in range(5, 6) ]\nIMAGE_SIZE = (12, 8)\nTEST_DHARUN=os.path.join(\'numplate\')\ncount = 0\n\n\nwith detection_graph.as_default():\n  with tf.Session(graph=detection_graph) as sess:\n    image_tensor = detection_graph.get_tensor_by_name(\'image_tensor:0\')\n    detection_boxes = detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n    detection_scores = detection_graph.get_tensor_by_name(\'detection_scores:0\')\n    detection_classes = detection_graph.get_tensor_by_name(\'detection_classes:0\')\n    num_detections = detection_graph.get_tensor_by_name(\'num_detections:0\')\n    for image_path in TEST_IMAGE_PATHS:\n      image = Image.open(image_path) \n      image_np = load_image_into_numpy_array(image)\n      image_np_expanded = np.expand_dims(image_np, axis=0)\n      (boxes, scores, classes, num) = sess.run(\n          [detection_boxes, detection_scores, detection_classes, num_detections],\n          feed_dict={image_tensor: image_np_expanded})\n      ymin = boxes[0,0,0]\n      xmin = boxes[0,0,1]\n      ymax = boxes[0,0,2]\n      xmax = boxes[0,0,3]\n      (im_width, im_height) = image.size\n      (xminn, xmaxx, yminn, ymaxx) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n      cropped_image = tf.image.crop_to_bounding_box(image_np, int(yminn), int(xminn),int(ymaxx - yminn), int(xmaxx - xminn))\n      img_data = sess.run(cropped_image)\n      count = 0\n      filename = dic.yo_make_the_conversion(img_data, count)\n      pytesseract.tesseract_cmd = \'/home/tensorflow-cuda/dharun_custom/models/research/object_detection/tessdata/\'\n      text = pytesseract.image_to_string(Image.open(filename),lang=None) \n      print(\'CHARCTER RECOGNITION : \',anv.catch_rectify_plate_characters(text))\n      vis_util.visualize_boxes_and_labels_on_image_array(\n          image_np,\n          np.squeeze(boxes),\n          np.squeeze(classes).astype(np.int32),\n          np.squeeze(scores),\n          category_index,\n          use_normalized_coordinates=True,\n          line_thickness=5)\n      \n      \n      plt.figure(figsize=IMAGE_SIZE)\n#      plt.imshow(img_data)\n#      plt.show()\n'"
custom_plate/__init__.py,0,b''
custom_plate/allow_needed_values.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Feb 26 15:27:59 2018\n    Script filters out the unwanted values and prints\n    values needed for License plate \n    [DESIGNED FOR TESSERACT]\n@author: tensorflow-cuda\n""""""\n\ndef check_array(tex):\n    side=tex\n    az=\'ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\'\n    i=0\n    yo=len(az)\n    txt=\'\'\n    for i in range(0,yo):\n        if side==az[i]:\n            txt=az[i]\n            break\n    return txt   \ndef catch_rectify_plate_characters(text):\n    tex = text\n    out1=[]\n    size=len(tex)\n    for i in range(0,size):\n      if tex[i]==check_array(tex[i]):\n        out1.append(tex[i])\n    yup=\'\'.join(str(e) for e in out1)    \n    return yup\n\n\n'"
custom_plate/do_image_conversion.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Mar  5 15:29:57 2018\n\n@author: tensorflow-cuda\n""""""\nimport cv2\nimport os\n\ndef yo_make_the_conversion(img_data, count):\n    img = img_data\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    gray = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n    gray = cv2.medianBlur(gray, 3)\n    path_png=\'png_tesseract\'\n    count += 1\n    filename = os.path.join(path_png,\'image{}.png\'.format(count))\n#    yo = tester.process_image_for_ocr(file)\n    cv2.imwrite(filename, gray)\n    return filename\n'"
custom_plate/jpg2png.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Apr  2 15:53:30 2018\nimport os\nos.remove(file) for file in os.listdir(\'path/to/directory\') if file.endswith(\'.png\')\n@author: tensorflow-cuda\n""""""\n#import os\n#from PIL import Image\n#i=1\n#\n#\n#os.remove(file) for file in os.listdir() if file.endswith(\'.jpeg\')\n#\n#\n#\n#\n#for i in range(1,162):\n#   im = Image.open(\'image{}.jpg\'.format(i))\n#   im.save(\'image{}.png\'.format(i))\n\n\n\n\nfrom glob import glob                                                           \nimport cv2 \njpgs = glob(\'./*.JPG\')\n\nfor j in jpgs:\n    img = cv2.imread(j)\n    cv2.imwrite(j[:-3] + \'png\', img)'"
custom_plate/test_db.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Feb 26 15:27:59 2018\n    Script created to support tensorflow\n    It pushes License plate value along with the timestamp\n    in POSTGRESSQL\n@author: tensorflow-cuda\n""""""\nimport psycopg2\ndef psyco_insert_plate(plate):\n    plate_char = str(plate)\n    try:\n        connect_str = ""dbname=\'testpython\' user=\'postgres\' host=\'localhost\' "" + \\\n                  ""password=\'postgres\'""\n        conn = psycopg2.connect(connect_str)\n        print(\'ok\')\n        cursor = conn.cursor()\n        cursor.execute(\'INSERT INTO testing (numplate) VALUES (%s)\',(plate_char))\n        conn.commit()\n        print(\'ok\')\n        cursor.execute(""""""SELECT * from testing"""""")\n        rows = cursor.fetchall()\n#        print(rows)\n        print(\'Plate inserted\')\n        return rows\n    except Exception as e:\n        print(""Big problem. Invalid dbname, user or password?"")\n        print(e)\n    \nyo = \'sdffv\'\nprint(psyco_insert_plate(yo))'"
custom_plate/tester.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Mar  1 15:23:02 2018\n\n@author: tensorflow-cuda\n""""""\nimport tempfile\nimport cv2\nimport logging\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom matplotlib import pyplot as plt\nget_ipython().magic(\'matplotlib inline\')\nimport pytesseract\nIMAGE_SIZE = 1800\nBINARY_THREHOLD = 180\n\nsize = None\n\n\ndef get_size_of_scaled_image(im):\n    global size\n    if size is None:\n        length_x, width_y = im.size\n        factor = max(1, int(IMAGE_SIZE / length_x))\n        size = factor * length_x, factor * width_y\n    return size\n\n\ndef process_image_for_ocr(file_path):\n    logging.info(\'Processing image for text Extraction\')\n    temp_filename = set_image_dpi(file_path)\n    im_new = remove_noise_and_smooth(temp_filename)\n    return im_new\n\n\ndef set_image_dpi(file_path):\n    im = Image.open(file_path)\n    # size = (1800, 1800)\n    size = get_size_of_scaled_image(im)\n    im_resized = im.resize(size, Image.ANTIALIAS)\n    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\'.jpg\')\n    temp_filename = temp_file.name\n    im_resized.save(temp_filename, dpi=(300, 300))  # best for OCR\n    return temp_filename\n\n\ndef image_smoothening(img):\n    ret1, th1 = cv2.threshold(img, BINARY_THREHOLD, 255, cv2.THRESH_BINARY)\n    ret2, th2 = cv2.threshold(th1, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    blur = cv2.GaussianBlur(th2, (1, 1), 0)\n    ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return th3\n\n\ndef remove_noise_and_smooth(file_name):\n    logging.info(\'Removing noise and smoothening image\')\n    img = cv2.imread(file_name, 0)\n    filtered = cv2.adaptiveThreshold(img.astype(np.uint8), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 41, 3)\n    kernel = np.ones((1, 1), np.uint8)\n    opening = cv2.morphologyEx(filtered, cv2.MORPH_OPEN, kernel)\n    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n    img = image_smoothening(img)\n    or_image = cv2.bitwise_or(img, closing)\n    return or_image\nimg = os.path.join(\'image13.png\')\nyo=process_image_for_ocr(img)\nplt.imshow(yo)\npytesseract.tesseract_cmd = \'/home/tensorflow-cuda/tesseract-master/tessdata/\'\ntext = pytesseract.image_to_string(Image.open(img),lang=None)\nprint(text)'"
custom_plate/Extra_Work/MNIST_data_to_TF_record.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\nr""""""Downloads and converts MNIST data to TFRecords of TF-Example protos.\r\nThis module downloads the MNIST data, uncompresses it, reads the files\r\nthat make up the MNIST data and creates two TFRecord datasets: one for train\r\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\r\nprotocol buffers, each of which contain a single image and label.\r\nThe script should take about a minute to run.\r\n""""""\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport gzip\r\nimport os\r\nimport sys\r\n\r\nimport numpy as np\r\nfrom six.moves import urllib\r\nimport tensorflow as tf\r\n\r\nfrom utils import dataset_util\r\n\r\n# The URLs where the MNIST data can be downloaded.\r\n_DATA_URL = \'http://yann.lecun.com/exdb/mnist/\'\r\n_TRAIN_DATA_FILENAME = \'train-images-idx3-ubyte.gz\'\r\n_TRAIN_LABELS_FILENAME = \'train-labels-idx1-ubyte.gz\'\r\n_TEST_DATA_FILENAME = \'t10k-images-idx3-ubyte.gz\'\r\n_TEST_LABELS_FILENAME = \'t10k-labels-idx1-ubyte.gz\'\r\n\r\n_IMAGE_SIZE = 28\r\n_NUM_CHANNELS = 1\r\n\r\n# The names of the classes.\r\n_CLASS_NAMES = [\r\n    \'zero\',\r\n    \'one\',\r\n    \'two\',\r\n    \'three\',\r\n    \'four\',\r\n    \'five\',\r\n    \'size\',\r\n    \'seven\',\r\n    \'eight\',\r\n    \'nine\',\r\n]\r\n\r\n\r\ndef _extract_images(filename, num_images):\r\n  """"""Extract the images into a numpy array.\r\n  Args:\r\n    filename: The path to an MNIST images file.\r\n    num_images: The number of images in the file.\r\n  Returns:\r\n    A numpy array of shape [number_of_images, height, width, channels].\r\n  """"""\r\n  print(\'Extracting images from: \', filename)\r\n  with gzip.open(filename) as bytestream:\r\n    bytestream.read(16)\r\n    buf = bytestream.read(\r\n        _IMAGE_SIZE * _IMAGE_SIZE * num_images * _NUM_CHANNELS)\r\n    data = np.frombuffer(buf, dtype=np.uint8)\r\n    data = data.reshape(num_images, _IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\r\n  return data\r\n\r\n\r\ndef _extract_labels(filename, num_labels):\r\n  """"""Extract the labels into a vector of int64 label IDs.\r\n  Args:\r\n    filename: The path to an MNIST labels file.\r\n    num_labels: The number of labels in the file.\r\n  Returns:\r\n    A numpy array of shape [number_of_labels]\r\n  """"""\r\n  print(\'Extracting labels from: \', filename)\r\n  with gzip.open(filename) as bytestream:\r\n    bytestream.read(8)\r\n    buf = bytestream.read(1 * num_labels)\r\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\r\n  return labels\r\n\r\n\r\ndef _add_to_tfrecord(data_filename, labels_filename, num_images,\r\n                     tfrecord_writer):\r\n  """"""Loads data from the binary MNIST files and writes files to a TFRecord.\r\n  Args:\r\n    data_filename: The filename of the MNIST images.\r\n    labels_filename: The filename of the MNIST labels.\r\n    num_images: The number of images in the dataset.\r\n    tfrecord_writer: The TFRecord writer to use for writing.\r\n  """"""\r\n  images = _extract_images(data_filename, num_images)\r\n  labels = _extract_labels(labels_filename, num_images)\r\n\r\n  shape = (_IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\r\n  with tf.Graph().as_default():\r\n    image = tf.placeholder(dtype=tf.uint8, shape=shape)\r\n    encoded_png = tf.image.encode_png(image)\r\n\r\n    with tf.Session(\'\') as sess:\r\n      for j in range(num_images):\r\n        sys.stdout.write(\'\\r>> Converting image %d/%d\' % (j + 1, num_images))\r\n        sys.stdout.flush()\r\n\r\n        png_string = sess.run(encoded_png, feed_dict={image: images[j]})\r\n\r\n        example = dataset_util.image_to_tfexample(\r\n            png_string, \'png\'.encode(), _IMAGE_SIZE, _IMAGE_SIZE, labels[j])\r\n        tfrecord_writer.write(example.SerializeToString())\r\n\r\n\r\ndef _get_output_filename(dataset_dir, split_name):\r\n  """"""Creates the output filename.\r\n  Args:\r\n    dataset_dir: The directory where the temporary files are stored.\r\n    split_name: The name of the train/test split.\r\n  Returns:\r\n    An absolute file path.\r\n  """"""\r\n  return \'%s/mnist_%s.tfrecord\' % (dataset_dir, split_name)\r\n\r\n\r\ndef _download_dataset(dataset_dir):\r\n  """"""Downloads MNIST locally.\r\n  Args:\r\n    dataset_dir: The directory where the temporary files are stored.\r\n  """"""\r\n  for filename in [_TRAIN_DATA_FILENAME,\r\n                   _TRAIN_LABELS_FILENAME,\r\n                   _TEST_DATA_FILENAME,\r\n                   _TEST_LABELS_FILENAME]:\r\n    filepath = os.path.join(dataset_dir, filename)\r\n\r\n    if not os.path.exists(filepath):\r\n      print(\'Downloading file %s...\' % filename)\r\n      def _progress(count, block_size, total_size):\r\n        sys.stdout.write(\'\\r>> Downloading %.1f%%\' % (\r\n            float(count * block_size) / float(total_size) * 100.0))\r\n        sys.stdout.flush()\r\n      filepath, _ = urllib.request.urlretrieve(_DATA_URL + filename,\r\n                                               filepath,\r\n                                               _progress)\r\n      print()\r\n      with tf.gfile.GFile(filepath) as f:\r\n        size = f.size()\r\n      print(\'Successfully downloaded\', filename, size, \'bytes.\')\r\n\r\n\r\ndef _clean_up_temporary_files(dataset_dir):\r\n  """"""Removes temporary files used to create the dataset.\r\n  Args:\r\n    dataset_dir: The directory where the temporary files are stored.\r\n  """"""\r\n  for filename in [_TRAIN_DATA_FILENAME,\r\n                   _TRAIN_LABELS_FILENAME,\r\n                   _TEST_DATA_FILENAME,\r\n                   _TEST_LABELS_FILENAME]:\r\n    filepath = os.path.join(dataset_dir, filename)\r\n    tf.gfile.Remove(filepath)\r\n\r\n\r\ndef run(dataset_dir):\r\n  """"""Runs the download and conversion operation.\r\n  Args:\r\n    dataset_dir: The dataset directory where the dataset is stored.\r\n  """"""\r\n  if not tf.gfile.Exists(dataset_dir):\r\n    tf.gfile.MakeDirs(dataset_dir)\r\n\r\n  training_filename = _get_output_filename(dataset_dir, \'train\')\r\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\r\n\r\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\r\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\r\n    return\r\n\r\n  _download_dataset(dataset_dir)\r\n\r\n  # First, process the training data:\r\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\r\n    data_filename = os.path.join(dataset_dir, _TRAIN_DATA_FILENAME)\r\n    labels_filename = os.path.join(dataset_dir, _TRAIN_LABELS_FILENAME)\r\n    _add_to_tfrecord(data_filename, labels_filename, 60000, tfrecord_writer)\r\n\r\n  # Next, process the testing data:\r\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\r\n    data_filename = os.path.join(dataset_dir, _TEST_DATA_FILENAME)\r\n    labels_filename = os.path.join(dataset_dir, _TEST_LABELS_FILENAME)\r\n    _add_to_tfrecord(data_filename, labels_filename, 10000, tfrecord_writer)\r\n\r\n  # Finally, write the labels file:\r\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\r\n  dataset_util.write_label_file(labels_to_class_names, dataset_dir)\r\n\r\n  _clean_up_temporary_files(dataset_dir)\r\n  print(\'\\nFinished converting the MNIST dataset!\')'"
custom_plate/Extra_Work/cv_frame_MJPG.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Tue Feb  6 15:55:28 2018\r\n\r\n@author: Vasantha kumar\r\n""""""\r\n\r\nimport cv2\r\nimport numpy as cv\r\n#from cv2.cv import *\r\n#import cv\r\ncap=cv2.VideoCapture(\'video123.mp4\')\r\nwidth=cap.get(cv2.CAP_PROP_FRAME_WIDTH)\r\nheight=cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\r\nFPS=10\r\nvideo=cv2.VideoWriter(\'bro_peace.avi\',cv2.VideoWriter_fourcc(*\'MJPG\'),10,(640,360),True)\r\nret = True\r\nwhile (ret):\r\n    ret,img=cap.read()\r\n    cv2.imshow(\'image\',img) \r\n#    cv2.resize(img,(1280,960))\r\n    video.write(img)\r\n#    k=cv2.waitKey(10)&0xff \r\n#    if k==27:\r\n#        break\r\n#    cap.release()\r\n#    video.release()\r\n#    cv2.destroyAllWindows()\r\n    if cv2.waitKey(100) & 0xFF == ord(\'q\'):\r\n        break\r\n        cap.release()\r\n        cv2.destroyAllWindows\r\n        video.release'"
custom_plate/Extra_Work/cv_test_git.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Fri Feb  2 17:53:54 2018\r\n\r\n@author: Vasantha kumar\r\n""""""\r\n\r\nimport os\r\nimport cv2\r\nimport time\r\nimport argparse\r\nimport multiprocessing\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom utils.app_utils import FPS, WebcamVideoStream\r\nfrom multiprocessing import Queue, Pool\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\n\r\nCWD_PATH = os.getcwd()\r\n\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\nMODEL_NAME = \'ssd_mobilenet_v1_coco_11_06_2017\'\r\nPATH_TO_CKPT = os.path.join(CWD_PATH, \'object_detection\', MODEL_NAME, \'frozen_inference_graph.pb\')\r\n\r\n# List of the strings that is used to add correct label for each box.\r\nPATH_TO_LABELS = os.path.join(CWD_PATH, \'data\', \'mscoco_label_map.pbtxt\')\r\n\r\n#NUM_CLASSES = 90\r\nNUM_CLASSES = 90\r\n# Loading label map\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES,\r\n                                                            use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n\r\ndef detect_objects(image_np, sess, detection_graph):\r\n    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n    image_np_expanded = np.expand_dims(image_np, axis=0)\r\n    image_tensor = detection_graph.get_tensor_by_name(\'image_tensor:0\')\r\n\r\n    # Each box represents a part of the image where a particular object was detected.\r\n    boxes = detection_graph.get_tensor_by_name(\'detection_boxes:0\')\r\n\r\n    # Each score represent how level of confidence for each of the objects.\r\n    # Score is shown on the result image, together with the class label.\r\n    scores = detection_graph.get_tensor_by_name(\'detection_scores:0\')\r\n    classes = detection_graph.get_tensor_by_name(\'detection_classes:0\')\r\n    num_detections = detection_graph.get_tensor_by_name(\'num_detections:0\')\r\n\r\n    # Actual detection.\r\n    (boxes, scores, classes, num_detections) = sess.run(\r\n        [boxes, scores, classes, num_detections],\r\n        feed_dict={image_tensor: image_np_expanded})\r\n\r\n    # Visualization of the results of a detection.\r\n    vis_util.visualize_boxes_and_labels_on_image_array(\r\n        image_np,\r\n        np.squeeze(boxes),\r\n        np.squeeze(classes).astype(np.int32),\r\n        np.squeeze(scores),\r\n        category_index,\r\n        use_normalized_coordinates=True,\r\n        line_thickness=8)\r\n    return image_np\r\n\r\n\r\ndef worker(input_q, output_q):\r\n    # Load a (frozen) Tensorflow model into memory.\r\n    detection_graph = tf.Graph()\r\n    with detection_graph.as_default():\r\n        od_graph_def = tf.GraphDef()\r\n        with tf.gfile.GFile(PATH_TO_CKPT, \'rb\') as fid:\r\n            serialized_graph = fid.read()\r\n            od_graph_def.ParseFromString(serialized_graph)\r\n            tf.import_graph_def(od_graph_def, name=\'\')\r\n\r\n        sess = tf.Session(graph=detection_graph)\r\n\r\n    fps = FPS().start()\r\n    while True:\r\n        fps.update()\r\n        frame = input_q.get()\r\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        output_q.put(detect_objects(frame_rgb, sess, detection_graph))\r\n\r\n    fps.stop()\r\n    sess.close()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'-src\', \'--source\', dest=\'video_source\', type=int,\r\n                        default=0, help=\'Device index of the camera.\')\r\n    parser.add_argument(\'-wd\', \'--width\', dest=\'width\', type=int,\r\n                        default=480, help=\'Width of the frames in the video stream.\')\r\n    parser.add_argument(\'-ht\', \'--height\', dest=\'height\', type=int,\r\n                        default=360, help=\'Height of the frames in the video stream.\')\r\n    parser.add_argument(\'-num-w\', \'--num-workers\', dest=\'num_workers\', type=int,\r\n                        default=2, help=\'Number of workers.\')\r\n    parser.add_argument(\'-q-size\', \'--queue-size\', dest=\'queue_size\', type=int,\r\n                        default=5, help=\'Size of the queue.\')\r\n    args = parser.parse_args()\r\n\r\n    logger = multiprocessing.log_to_stderr()\r\n    logger.setLevel(multiprocessing.SUBDEBUG)\r\n\r\n    input_q = Queue(maxsize=args.queue_size)\r\n    output_q = Queue(maxsize=args.queue_size)\r\n    pool = Pool(args.num_workers, worker, (input_q, output_q))\r\n\r\n    video_capture = WebcamVideoStream(src=args.video_source,\r\n                                      width=args.width,\r\n                                      height=args.height).start()\r\n    fps = FPS().start()\r\n\r\n    while True:  # fps._numFrames < 120\r\n        frame = video_capture.read()\r\n        input_q.put(frame)\r\n\r\n        t = time.time()\r\n\r\n        output_rgb = cv2.cvtColor(cv2.resize(output_q.get(),(1280,960)), cv2.COLOR_RGB2BGR)\r\n        cv2.imshow(\'Video\', output_rgb)\r\n        fps.update()\r\n\r\n        print(\'[INFO] elapsed time: {:.2f}\'.format(time.time() - t))\r\n\r\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n            break\r\n\r\n    fps.stop()\r\n    print(\'[INFO] elapsed time (total): {:.2f}\'.format(fps.elapsed()))\r\n    print(\'[INFO] approx. FPS: {:.2f}\'.format(fps.fps()))\r\n\r\n    pool.terminate()\r\n    video_capture.stop()\r\n    cv2.destroyAllWindows()'"
custom_plate/Extra_Work/frame_divide.py,0,"b'import numpy as np\t\t      # importing Numpy for use w/ OpenCV\r\nimport cv2                            # importing Python OpenCV\r\nfrom datetime import datetime         # importing datetime for naming files w/ timestamp\r\n\r\ndef diffImg(t0, t1, t2):              # Function to calculate difference between images.\r\n  d1 = cv2.absdiff(t2, t1)\r\n  d2 = cv2.absdiff(t1, t0)\r\n  return cv2.bitwise_and(d1, d2)\r\n\r\nthreshold = 70000                     # Threshold for triggering ""motion detection""\r\n#cam = cv2.VideoCapture(\'http://10.100.16.61:8080/video\')             # Lets initialize capture on webcam\r\ncam = cv2.VideoCapture(0)\r\nwinName = ""Movement Indicator""\t      # comment to hide window\r\ncv2.namedWindow(winName)              # comment to hide window\r\n\r\n# Read three images first:\r\nt_minus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n# Lets use a time check so we only take 1 pic per sec\r\ntimeCheck = datetime.now().strftime(\'%Ss\')\r\n\r\nwhile True:\r\n  ret, frame = cam.read()\t      # read from camera\r\n  totalDiff = cv2.countNonZero(diffImg(t_minus, t, t_plus))\t# this is total difference number\r\n  text = ""threshold: "" + str(totalDiff)\t\t\t\t# make a text showing total diff.\r\n  cv2.putText(frame, text, (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)   # display it on screen\r\n  if totalDiff > threshold and timeCheck != datetime.now().strftime(\'%Ss\'):\r\n    dimg= cam.read()[1]\r\n    cv2.imwrite(datetime.now().strftime(\'%Y%m%d_%Hh%Mm%Ss%f\') + \'.jpg\', dimg)\r\n  timeCheck = datetime.now().strftime(\'%Ss\')\r\n  # Read next image\r\n  t_minus = t\r\n  t = t_plus\r\n  t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n  cv2.imshow(winName, frame)\r\n  \r\n  key = cv2.waitKey(10)\r\n  if key == 27:\t\t\t # comment this \'if\' to hide window\r\n    cv2.destroyWindow(winName)\r\n    break\r\n'"
custom_plate/Extra_Work/motion_detection_opcv_andrine.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Wed Feb 14 13:52:44 2018\r\n\r\n@author: Dharun\r\n""""""\r\n\r\nimport cv2                            # importing Python OpenCV\r\nfrom datetime import datetime         # importing datetime for naming files w/ timestamp\r\n#import os\r\n#path = os.path.join(\'motion_pics\')\r\ndef diffImg(t0, t1, t2):              # Function to calculate difference between images.\r\n  d1 = cv2.absdiff(t2, t1)\r\n  d2 = cv2.absdiff(t1, t0)\r\n  return cv2.bitwise_and(d1, d2)\r\n\r\nthreshold = 81500                     # Threshold for triggering ""motion detection""\r\ncam = cv2.VideoCapture(0)             # Lets initialize capture on webcam\r\n#count=0\r\nwinName = ""Movement Indicator""\t\t    # comment to hide window\r\ncv2.namedWindow(winName)\t\t          # comment to hide window\r\n\r\n# Read three images first:\r\nt_minus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n# Lets use a time check so we only take 1 pic per sec\r\ntimeCheck = datetime.now().strftime(\'%Ss\')\r\n\r\nwhile True:\r\n  cv2.imshow( winName, cam.read()[1] )\t\t# comment to hide window\r\n  if cv2.countNonZero(diffImg(t_minus, t, t_plus)) > threshold and timeCheck != datetime.now().strftime(\'%Ss\'):\r\n#    dimg= cam.read()[1]\r\n#    count += 1\r\n#    filename = os.path.join(path,\'frame{}.jpg\'.format(count))\r\n#        cv2.imwrite(""frame%d.jpg"" % count, frame)\r\n#    cv2.imwrite(filename, dimg)\r\n#    filename = os.path.join(path)\r\n     dimg= cam.read()[1]\r\n     cv2.imwrite(datetime.now().strftime(\'%Y%m%d_%Hh%Mm%Ss%f\') + \'.jpg\', dimg)\r\n  timeCheck = datetime.now().strftime(\'%Ss\')\r\n  # Read next image\r\n  t_minus = t\r\n  t = t_plus\r\n  t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n\r\n  key = cv2.waitKey(10)\r\n  if key == 27:\r\n    cv2.destroyWindow(winName)\t\t\t# comment to hide window\r\n    break\r\n'"
custom_plate/Extra_Work/object_det_num100_serve.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Wed Jan 24 14:47:35 2018\r\n\r\n@author: Vasantha kumar\r\n""""""\r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nimport os,glob,cv2\r\nimport sys,argparse\r\n# First, pass the path of the image\r\ndir_path = os.path.dirname(os.path.realpath(file))\r\nimage_path=sys.argv[1]\r\nfilename = dir_path +\'/\' +image_path\r\nimage_size=128\r\nnum_channels=3\r\nimages = []\r\n \r\n##  Reading the image using OpenCV\r\nimage = cv2.imread(filename)\r\n \r\n##   Resizing the image to our desired size and preprocessing will be done exactly as done during training\r\nimage = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\r\nimages.append(image)\r\nimages = np.array(images, dtype=np.uint8)\r\nimages = images.astype(\'float32\')\r\nimages = np.multiply(images, 1.0/255.0)\r\n##   The input to the network is of shape [None image_size image_size num_channels]. \r\n## Hence we reshape.\r\n \r\nx_batch = images.reshape(1, image_size,image_size,num_channels)\r\n \r\nfrozen_graph=""/numplate/numplate29517_100img/frozen_inference_graph.pb""\r\nwith tf.gfile.GFile(frozen_graph, ""rb"") as f:\r\n      graph_def = tf.GraphDef()\r\n      graph_def.ParseFromString(f.read())\r\n \r\nwith tf.Graph().as_default() as graph:\r\n      tf.import_graph_def(graph_def,\r\n                          input_map=None,\r\n                          return_elements=None,\r\n                          name=""""\r\n      )\r\n## NOW the complete graph with values has been restored\r\ny_pred = graph.get_tensor_by_name(""y_pred:0"")\r\n## Let\'s feed the images to the input placeholders\r\nx= graph.get_tensor_by_name(""x:0"")\r\ny_test_images = np.zeros((1, 2))\r\nsess= tf.Session(graph=graph)\r\n### Creating the feed_dict that is required to be fed to calculate y_pred \r\nfeed_dict_testing = {x: x_batch}\r\nresult=sess.run(y_pred, feed_dict=feed_dict_testing)\r\nprint(result)'"
custom_plate/Extra_Work/record_gen_dharun_MNIST.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Tue Jan 30 15:35:47 2018\r\n\r\n@author: Vasantha kumar\r\n"""""" \r\n\r\nfrom utils import MNIST_data_to_TF_record as MNIST\r\nimport os\r\npath=os.path.join(\'MNIST_data\')\r\nMNIST.run(path)'"
custom_plate/Extra_Work/scikit_numdetection.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Fri Feb  9 14:46:55 2018\r\n\r\n@author: Dharun\r\n""""""\r\n\r\n""""""\r\n================================\r\nRecognizing hand-written digits\r\n================================\r\n\r\nAn example showing how the scikit-learn can be used to recognize images of\r\nhand-written digits.\r\n\r\nThis example is commented in the\r\n:ref:`tutorial section of the user manual <introduction>`.\r\n\r\n""""""\r\nprint(__doc__)\r\n\r\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\r\n# License: BSD 3 clause\r\n\r\n# Standard scientific Python imports\r\nimport matplotlib.pyplot as plt\r\n\r\n# Import datasets, classifiers and performance metrics\r\nfrom sklearn import datasets, svm, metrics\r\n\r\n# The digits dataset\r\ndigits = datasets.load_digits()\r\n\r\n# The data that we are interested in is made of 8x8 images of digits, let\'s\r\n# have a look at the first 4 images, stored in the `images` attribute of the\r\n# dataset.  If we were working from image files, we could load them using\r\n# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\r\n# images, we know which digit they represent: it is given in the \'target\' of\r\n# the dataset.\r\nimages_and_labels = list(zip(digits.images, digits.target))\r\nfor index, (image, label) in enumerate(images_and_labels[:4]):\r\n    plt.subplot(2, 4, index + 1)\r\n    plt.axis(\'off\')\r\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=\'nearest\')\r\n    plt.title(\'Training: %i\' % label)\r\n\r\n# To apply a classifier on this data, we need to flatten the image, to\r\n# turn the data in a (samples, feature) matrix:\r\nn_samples = len(digits.images)\r\ndata = digits.images.reshape((n_samples, -1))\r\n\r\n# Create a classifier: a support vector classifier\r\nclassifier = svm.SVC(gamma=0.001)\r\n\r\n# We learn the digits on the first half of the digits\r\nclassifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\r\n\r\n# Now predict the value of the digit on the second half:\r\nexpected = digits.target[n_samples // 2:]\r\npredicted = classifier.predict(data[n_samples // 2:])\r\n\r\nprint(""Classification report for classifier %s:\\n%s\\n""\r\n      % (classifier, metrics.classification_report(expected, predicted)))\r\nprint(""Confusion matrix:\\n%s"" % metrics.confusion_matrix(expected, predicted))\r\n\r\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\r\nfor index, (image, prediction) in enumerate(images_and_predictions[:4]):\r\n    plt.subplot(2, 4, index + 5)\r\n    plt.axis(\'off\')\r\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=\'nearest\')\r\n    plt.title(\'Prediction: %i\' % prediction)\r\n\r\nplt.show()\r\n'"
custom_plate/Extra_Work/serve_web.py,4,"b'import flask\r\napp = flask.Flask(__name__)\r\n\r\n \r\ndef load_graph(trained_model):   \r\n    with tf.gfile.GFile(trained_model, ""rb"") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n \r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(\r\n            graph_def,\r\n            input_map=None,\r\n            return_elements=None,\r\n            name=""""\r\n            )\r\n    return graph\r\n\r\napp.graph=load_graph(\'/numplate/numplate29517_100img/frozen_inference_graph.pb\')  \r\nif __name__ == \'__main__\':\r\n    app.run(host=""0.0.0.0"", port=int(""5000""), debug=True, use_reloader=False)\r\n'"
custom_plate/Extra_Work/tesseract.py,0,"b'# import the necessary packages\r\nfrom PIL import Image\r\nimport pytesseract\r\nimport argparse\r\nimport cv2\r\nimport os\r\n#from pytesseract import image_to_string\r\n# construct the argument parse and parse the arguments\r\nap = argparse.ArgumentParser()\r\nap.add_argument(""-i"", ""--image"", required=True,\r\n\thelp=""path to input image to be OCR\'d"")\r\nap.add_argument(""-p"", ""--preprocess"", type=str, default=""thresh"",\r\n\thelp=""type of preprocessing to be done"")\r\nargs = vars(ap.parse_args())\r\n\r\n# load the example image and convert it to grayscale\r\nimage = cv2.imread(args[""image""])\r\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n \r\n# check to see if we should apply thresholding to preprocess the\r\n# image\r\nif args[""preprocess""] == ""thresh"":\r\n\tgray = cv2.threshold(gray, 0, 255,\r\n\t\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\r\n \r\n# make a check to see if median blurring should be done to remove\r\n# noise\r\nelif args[""preprocess""] == ""blur"":\r\n\tgray = cv2.medianBlur(gray, 3)\r\n \r\n# write the grayscale image to disk as a temporary file so we can\r\n# apply OCR to it\r\npath_png=\'png_tesseract\'\r\npath_id=format(os.getpid())\r\n#print(path_id)\r\n#filename = ""{}.png"".format(os.getpid())\r\n#filename = ""image{}.png"".format(os.path.join(\'png_tesseract/\'))\r\nfilename = os.path.join(path_png,\'image{}.png\'.format(os.getpid()))\r\ncv2.imwrite(filename, gray)\r\n# load the image as a PIL/Pillow image, apply OCR, and then delete\r\n# the temporary file \r\n#file2=os.path.join(\'\\png_tesseract\\image12244.png\')\r\n#tets=Image.open(filename)\r\npytesseract.pytesseract.tesseract_cmd = \'D:\\\\tensorflow_extras\\\\dharun_custom\\\\Tesseract-OCR\\\\tesseract.exe\'\r\ntext = pytesseract.image_to_string(Image.open(filename),lang=None) \r\nos.remove(filename)\r\nprint(text)\r\n \r\n# show the output images\r\ncv2.imshow(""Image"", image)\r\ncv2.imshow(""Output"", gray)\r\ncv2.waitKey(0)\r\n'"
custom_plate/Extra_Work/test_cv_xvid.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Sat Feb 10 15:43:34 2018\r\n\r\n@author: Dharun\r\n""""""\r\n\r\nimport numpy as np\r\nimport cv2\r\nimport os\r\npath = os.path.join(\'frame_cv\')\r\ncap = cv2.VideoCapture(0)\r\ncount = 0\r\n\r\nfourcc = cv2.VideoWriter_fourcc(*\'XVID\')\r\nout = cv2.VideoWriter(\'output.avi\',fourcc, 30.0, (640,480))\r\n\r\nwhile(cap.isOpened()):\r\n    ret, frame = cap.read()\r\n    if ret==True:\r\n#        frame = cv2.flip(frame,0)\r\n        count += 1\r\n        filename = os.path.join(path,\'frame{}.jpg\'.format(count))\r\n#        cv2.imwrite(""frame%d.jpg"" % count, frame)\r\n        cv2.imwrite(filename, frame)\r\n        # write the flipped frame\r\n#        out.write(frame)\r\n        \r\n        cv2.imshow(\'frame\',frame)\r\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n            break\r\n    else:\r\n        break\r\n\r\n\r\ncap.release()\r\n#out.release()\r\ncv2.destroyAllWindows()'"
custom_plate/Extra_Work/test_mnist.py,22,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Wed Jan 31 14:08:15 2018\r\n\r\n@author: Vasantha kumar\r\n""""""\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot=True)\r\nsession = tf.InteractiveSession()\r\nx = tf.placeholder(tf.float32, shape=[None, 784], name=""x"")\r\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\r\n\r\ndef weight_variable(shape):\r\n  initial = tf.truncated_normal(shape, stddev=0.1)\r\n  return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n  initial = tf.constant(0.1, shape=shape)\r\n  return tf.Variable(initial)\r\n\r\ndef conv2d(x, W):\r\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\r\n\r\ndef max_pool_2x2(x):\r\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\r\n                            strides=[1, 2, 2, 1], padding=\'SAME\')\r\n\r\nx_image = tf.reshape(x, [-1,28,28,1])\r\n\r\nW_conv1 = weight_variable([5, 5, 1, 32])\r\nb_conv1 = bias_variable([32])\r\n\r\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\nh_pool1 = max_pool_2x2(h_conv1)\r\n\r\nW_conv2 = weight_variable([5, 5, 32, 64])\r\nb_conv2 = bias_variable([64])\r\n\r\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\r\nh_pool2 = max_pool_2x2(h_conv2)\r\n\r\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\r\nb_fc1 = bias_variable([1024])\r\n\r\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\r\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\r\nW_fc2 = weight_variable([1024, 10])\r\nb_fc2 = bias_variable([10])\r\n\r\ny_conv = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=""softmax"")\r\n\r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\r\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\ninit = tf.initialize_all_variables()\r\nsaver = tf.train.Saver()\r\nsession.run(init)\r\n\r\nfor i in range(20000):\r\n  batch = mnist.train.next_batch(50)\r\n  if i % 100 == 0:\r\n    train_accuracy = accuracy.eval(feed_dict={      x:batch[0], y_: batch[1]\r\n    })\r\n    print(""step %d, training accuracy %g"" % (i, train_accuracy))\r\n  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\r\n\r\nsaver.save(session, ""model.ckpt"")\r\ntf.train.write_graph(session.graph_def, \'\', \'graph.pb\')\r\n\r\nprint(""test accuracy %g"" % accuracy.eval(feed_dict={  x: mnist.test.images, y_: mnist.test.labels\r\n}))\r\n'"
custom_plate/codezero - andy/motion_detection_opcv_andrine(version2).py,0,"b'import cv2                            # importing Python OpenCV\r\nfrom datetime import datetime         # importing datetime for naming files w/ timestamp\r\n\r\ndef diffImg(t0, t1, t2):              # Function to calculate difference between images.\r\n  d1 = cv2.absdiff(t2, t1)\r\n  d2 = cv2.absdiff(t1, t0)\r\n  return cv2.bitwise_and(d1, d2)\r\n\r\nthreshold = 130000                     # Threshold for triggering ""motion detection""\r\ncam = cv2.VideoCapture(\'rtsp://admin:kgisl@123@10.100.10.192/doc/page/preview.asp\')             # Lets initialize capture on webcam\r\n#cam = cv2.VideoCapture(0)\r\n\r\nwinName = ""Movement Indicator""\t\t    # comment to hide window\r\ncv2.namedWindow(winName)\t\t          # comment to hide window\r\n\r\n# Read three images first:\r\nt_minus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n# Lets use a time check so we only take 1 pic per sec\r\ntimeCheck = datetime.now().strftime(\'%Ss\')\r\n\r\nwhile True:\r\n  cv2.imshow( winName, cam.read()[1] )\t\t# comment to hide window\r\n  if cv2.countNonZero(diffImg(t_minus, t, t_plus)) > threshold and timeCheck != datetime.now().strftime(\'%Ss\'):\r\n    dimg= cam.read()[1]\r\n    cv2.imwrite(datetime.now().strftime(\'%Y%m%d_%Hh%Mm%Ss%f\') + \'.jpg\', dimg)\r\n  timeCheck = datetime.now().strftime(\'%Ss\')\r\n  # Read next image\r\n  t_minus = t\r\n  t = t_plus\r\n  t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n  \r\n  key = cv2.waitKey(10)\r\n  if key == 27:\r\n    cv2.destroyWindow(winName)\t\t\t# comment to hide window\r\n    break\r\n'"
custom_plate/codezero - andy/new_text.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Feb 28 09:35:18 2018\n\n@author: tensorflow-cuda\n""""""\n\n'"
custom_plate/codezero - andy/object_detection_tesseract(version2).py,13,"b'\n# coding: utf-8\nprint(\'ok\')\n# # Object Detection Demo\n# Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start.\n\n# # Imports\n\n# In[ ]:\n\n\nimport numpy as np\nimport os\n#import six.moves.urllib as urllib\nimport sys\n#import tarfile\nimport tensorflow as tf\n#import zipfile\n\n#from collections import defaultdict\n#from io import StringIO\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\n#if tf.__version__ != \'1.4.0\':\n#  raise ImportError(\'Please upgrade your tensorflow installation to v1.4.0!\')\n#tesseract imports\nimport cv2\nimport pytesseract\n# ## Env setup\n\n# In[ ]:\n\n\n# This is needed to display the images.\nget_ipython().magic(\'matplotlib inline\')\n\n# This is needed since the notebook is stored in the object_detection folder.\nsys.path.append("".."")\n\n\n# ## Object detection imports\n# Here are the imports from the object detection module.\n\n# In[ ]:\n\n\nfrom utils import label_map_util\n\nfrom utils import visualization_utils as vis_util\n\n\n# # Model preparation \n\n# ## Variables\n# \n# Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n# \n# By default we use an ""SSD with Mobilenet"" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\n\n# In[ ]:\n\n\n# What model to download.\nMODEL_NAME = \'numplate\'\n#MODEL_FILE = MODEL_NAME + \'.tar.gz\'\n#DOWNLOAD_BASE = \'http://download.tensorflow.org/models/object_detection/\'\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = MODEL_NAME + \'/graph-200000/frozen_inference_graph.pb\'\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = os.path.join(\'training\', \'object-detection.pbtxt\')\n\nNUM_CLASSES = 1\n\n\n# ## Download Model\n\n# In[ ]:\n\n\n# opener = urllib.request.URLopener()\n# opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n# tar_file = tarfile.open(MODEL_FILE)\n# for file in tar_file.getmembers():\n#  file_name = os.path.basename(file.name)\n#  if \'frozen_inference_graph.pb\' in file_name:\n#    tar_file.extract(file, os.getcwd())\n\n\n# ## Load a (frozen) Tensorflow model into memory.\n\n# In[ ]:\n\n\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n  od_graph_def = tf.GraphDef()\n  with tf.gfile.GFile(PATH_TO_CKPT, \'rb\') as fid:\n    serialized_graph = fid.read()\n    od_graph_def.ParseFromString(serialized_graph)\n    tf.import_graph_def(od_graph_def, name=\'\')\n\n\n# ## Loading label map\n# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\n\n# In[ ]:\n\n\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\n\n# ## Helper code\n\n# In[ ]:\n\n\ndef load_image_into_numpy_array(image):\n  (im_width, im_height) = image.size\n  return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\n\n# # Detection\n\n# In[ ]:\n\n\n# For the sake of simplicity we will use only 2 images:\n# image1.jpg\n# image2.jpg\n# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\nPATH_TO_TEST_IMAGES_DIR = \'png_tesseract/test_tesseract\'\nTEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, \'image{}.jpg\'.format(i)) for i in range(1, 14) ]\n\n# Size, in inches, of the output images.\nIMAGE_SIZE = (12, 8)\nTEST_DHARUN=os.path.join(\'numplate\')\ncount = 0\n# In[ ]:\n\n\nwith detection_graph.as_default():\n  with tf.Session(graph=detection_graph) as sess:\n    # Definite input and output Tensors for detection_graph\n    image_tensor = detection_graph.get_tensor_by_name(\'image_tensor:0\')\n    # Each box represents a part of the image where a particular object was detected.\n    detection_boxes = detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n    # Each score represent how level of confidence for each of the objects.\n    # Score is shown on the result image, together with the class label.\n    detection_scores = detection_graph.get_tensor_by_name(\'detection_scores:0\')\n    detection_classes = detection_graph.get_tensor_by_name(\'detection_classes:0\')\n    num_detections = detection_graph.get_tensor_by_name(\'num_detections:0\')\n    for image_path in TEST_IMAGE_PATHS:\n      image = Image.open(image_path)\n      # the array based representation of the image will be used later in order to prepare the\n      # result image with boxes and labels on it.\n      image_np = load_image_into_numpy_array(image)\n      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n      image_np_expanded = np.expand_dims(image_np, axis=0)\n      # Actual detection.\n      (boxes, scores, classes, num) = sess.run(\n          [detection_boxes, detection_scores, detection_classes, num_detections],\n          feed_dict={image_tensor: image_np_expanded})\n      #numpy.asarray(Image.open(\'1-enhanced.png\').convert(\'L\'))\n      # Visualization of the results of a detection\n#      tf.image.draw_bounding_boxes(image_np, boxes, name=None)\n#      print(np.squeeze(boxes))\n#      print(category_index)\n#      print(boxes.shape[0])\n#      print(100*scores[0])\n#      ase=tf.constant(detection_scores,tf.float32)\n#      print(sess.run(ase))\n#      vis_util.encode_image_array_as_png_str(image_np)\n#      vis_util.save_image_array_as_png(image_np,TEST_DHARUN)\n#      width, height = image.size\n#      print(height)\n#      ymin = boxes[0][i][0]*height\n#      xmin = boxes[0][i][1]*width\n#      ymax = boxes[0][i][2]*height\n#      xmax = boxes[0][i][3]*width\n#      print(ymin)\n#      image=tf.image.convert_image_dtype(image,dtype=float,saturate=False,name=None)\n\n      ymin = boxes[0,0,0]\n      xmin = boxes[0,0,1]\n      ymax = boxes[0,0,2]\n      xmax = boxes[0,0,3]\n      (im_width, im_height) = image.size\n      (xminn, xmaxx, yminn, ymaxx) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n#      print(ymin,xmin,ymax,xmax)\n      xxminn=int(xminn)\n      xxmaxx=int(xmaxx)\n      yyminn=int(yminn)\n      yymaxx=int(ymaxx)\n#      print(xxminn,xxmaxx,yyminn,yymaxx)\n      cropped_image = tf.image.crop_to_bounding_box(image_np, int(yminn), int(xminn),int(ymaxx - yminn), int(xmaxx - xminn))\n#      cropped_image = tf.image.crop_to_bounding_box(image, yyminn, xxminn, yymaxx - yyminn, xxmaxx - xxminn)\n#      output_image = tf.image.encode_png(cropped_image)\n#      sess = tf.Session()\n      # tesseract part\n      img_data = sess.run(cropped_image)\n      \n      gray = cv2.cvtColor(img_data, cv2.COLOR_BGR2GRAY)\n      gray = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n      gray = cv2.medianBlur(gray, 3)\n      path_png=\'png_tesseract\'\n      count += 1\n      filename = os.path.join(path_png,\'image{}.png\'.format(count))\n      cv2.imwrite(filename, gray)\n      pytesseract.tesseract_cmd = \'/home/tensorflow-cuda/tesseract-master/tessdata/\'\n      text = pytesseract.image_to_string(Image.open(filename),lang=None) \n##      os.remove(filename)\n      print(\'NUM PLATE : \',text)\n      with open(""Output1.txt"",""a"") as text_file:\n        text_file.write(""%s\\n\\n"" % (text))\n      # End of tesseract \n#      sess.close()\n      vis_util.visualize_boxes_and_labels_on_image_array(\n          image_np,\n          np.squeeze(boxes),\n          np.squeeze(classes).astype(np.int32),\n          np.squeeze(scores),\n          category_index,\n          use_normalized_coordinates=True,\n          line_thickness=5)\n      \n      \n      plt.figure(figsize=IMAGE_SIZE)\n      plt.imshow(image_np)\n#      plt.imshow(img_data)\n#      plt.imshow(output_image)\n#      plt.show()\n'"
custom_plate/motion_test/ip_camera_connect.py,0,"b""import urllib\r\nimport cv2\r\nimport numpy as np\r\n\r\nurl='http://10.100.17.172:8080/shot.jpg'\r\n\r\nwhile True:\r\n    imgResp=urllib.request.urlopen(url)\r\n    imgNp=np.array(bytearray(imgResp.read()),dtype=np.uint8)\r\n    img=cv2.imdecode(imgNp,-1)\r\n\r\n    # all the opencv processing is done here\r\n    cv2.imshow('test',img)\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n            break"""
custom_plate/motion_test/motion_detection_opcv_andrine.py,0,"b'import cv2                            # importing Python OpenCV\r\nfrom datetime import datetime         # importing datetime for naming files w/ timestamp\r\n\r\ndef diffImg(t0, t1, t2):              # Function to calculate difference between images.\r\n  d1 = cv2.absdiff(t2, t1)\r\n  d2 = cv2.absdiff(t1, t0)\r\n  return cv2.bitwise_and(d1, d2)\r\n\r\nthreshold = 81500                     # Threshold for triggering ""motion detection""\r\ncam = cv2.VideoCapture(\'http://10.100.17.172:8080/video\')             # Lets initialize capture on webcam\r\n\r\nwinName = ""Movement Indicator""\t\t    # comment to hide window\r\ncv2.namedWindow(winName)\t\t          # comment to hide window\r\n\r\n# Read three images first:\r\nt_minus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\nt_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n# Lets use a time check so we only take 1 pic per sec\r\ntimeCheck = datetime.now().strftime(\'%Ss\')\r\n\r\nwhile True:\r\n  cv2.imshow( winName, cam.read()[1] )\t\t# comment to hide window\r\n  if cv2.countNonZero(diffImg(t_minus, t, t_plus)) > threshold and timeCheck != datetime.now().strftime(\'%Ss\'):\r\n    dimg= cam.read()[1]\r\n    cv2.imwrite(datetime.now().strftime(\'%Y%m%d_%Hh%Mm%Ss%f\') + \'.jpg\', dimg)\r\n  timeCheck = datetime.now().strftime(\'%Ss\')\r\n  # Read next image\r\n  t_minus = t\r\n  t = t_plus\r\n  t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\r\n\r\n  key = cv2.waitKey(10)\r\n  if key == 27:\r\n    cv2.destroyWindow(winName)\t\t\t# comment to hide window\r\n    break\r\n'"
