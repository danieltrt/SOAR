file_path,api_count,code
beginners/notebook-examples/chapter-6/app.py,1,"b""import base64\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom io import BytesIO\nfrom flask import Flask, request, jsonify\nfrom keras.models import load_model\nfrom PIL import Image\n\nNUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nLOWERCASE = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n            'v', 'w', 'x', 'y', 'z']\nUPPERCASE = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n           'V', 'W', 'X', 'Y', 'Z']\n\nCAPTCHA_CHARSET = NUMBER   # \xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\xad\x97\xe7\xac\xa6\xe9\x9b\x86\nCAPTCHA_LEN = 4            # \xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe9\x95\xbf\xe5\xba\xa6\nCAPTCHA_HEIGHT = 60        # \xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe9\xab\x98\xe5\xba\xa6\nCAPTCHA_WIDTH = 160        # \xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\xae\xbd\xe5\xba\xa6\n\n# 10 \xe4\xb8\xaa Epochs \xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\nMODEL_FILE = './pre-trained/model/captcha_rmsprop_binary_crossentropy_bs_100_epochs_10.h5'\n\ndef vec2text(vector):\n    if not isinstance(vector, np.ndarray):\n        vector = np.asarray(vector)\n    vector = np.reshape(vector, [CAPTCHA_LEN, -1])\n    text = ''\n    for item in vector:\n        text += CAPTCHA_CHARSET[np.argmax(item)]\n    return text\n\ndef rgb2gray(img):\n    # Y' = 0.299 R + 0.587 G + 0.114 B \n    # https://en.wikipedia.org/wiki/Grayscale#Converting_color_to_grayscale\n    return np.dot(img[...,:3], [0.299, 0.587, 0.114])\n\napp = Flask(__name__) # \xe5\x88\x9b\xe5\xbb\xba Flask \xe5\xae\x9e\xe4\xbe\x8b\n\n# \xe6\xb5\x8b\xe8\xaf\x95 URL\n@app.route('/ping', methods=['GET', 'POST'])\ndef hello_world():\n    return 'pong'\n\n# \xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe8\xaf\x86\xe5\x88\xab URL\n@app.route('/predict', methods=['POST'])\ndef predict():\n    response = {'success': False, 'prediction': '', 'debug': 'error'}\n    received_image= False\n    if request.method == 'POST':\n        if request.files.get('image'): # \xe5\x9b\xbe\xe5\x83\x8f\xe6\x96\x87\xe4\xbb\xb6\n            image = request.files['image'].read()\n            received_image = True\n            response['debug'] = 'get image'\n        elif request.get_json(): # base64 \xe7\xbc\x96\xe7\xa0\x81\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe6\x96\x87\xe4\xbb\xb6\n            encoded_image = request.get_json()['image']\n            image = base64.b64decode(encoded_image)\n            received_image = True\n            response['debug'] = 'get json'\n        if received_image:\n            image = np.array(Image.open(BytesIO(image)))\n            image = rgb2gray(image).reshape(1, 60, 160, 1).astype('float32') / 255\n            with graph.as_default():\n                pred = model.predict(image)\n            response['prediction'] = response['prediction'] + vec2text(pred)\n            response['success'] = True\n            response['debug'] = 'predicted'\n    else:\n        response['debug'] = 'No Post'\n    return jsonify(response)\n\nmodel = load_model(MODEL_FILE) # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\ngraph = tf.get_default_graph() # \xe8\x8e\xb7\xe5\x8f\x96 TensorFlow \xe9\xbb\x98\xe8\xae\xa4\xe6\x95\xb0\xe6\x8d\xae\xe6\xb5\x81\xe5\x9b\xbe"""
beginners/notebook-examples/chapter-7/align.py,0,"b'# Copyright 2015-2016 Carnegie Mellon University\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module for dlib-based alignment.""""""\n\nimport cv2\nimport dlib\nimport numpy as np\n\nTEMPLATE = np.float32([\n    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n\nTPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\nMINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n\n\nclass AlignDlib:\n    """"""\n    Use `dlib\'s landmark estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ to align faces.\n\n    The alignment preprocess faces for input into a neural network.\n    Faces are resized to the same size (such as 96x96) and transformed\n    to make landmarks (such as the eyes and nose) appear at the same\n    location on every image.\n\n    Normalized landmarks:\n\n    .. image:: ../images/dlib-landmark-mean.png\n    """"""\n\n    #: Landmark indices.\n    INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n    OUTER_EYES_AND_NOSE = [36, 45, 33]\n\n    def __init__(self, facePredictor):\n        """"""\n        Instantiate an \'AlignDlib\' object.\n\n        :param facePredictor: The path to dlib\'s\n        :type facePredictor: str\n        """"""\n        assert facePredictor is not None\n\n        self.detector = dlib.get_frontal_face_detector()\n        self.predictor = dlib.shape_predictor(facePredictor)\n\n    def getAllFaceBoundingBoxes(self, rgbImg):\n        """"""\n        Find all face bounding boxes in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :return: All face bounding boxes in an image.\n        :rtype: dlib.rectangles\n        """"""\n        assert rgbImg is not None\n\n        try:\n            return self.detector(rgbImg, 1)\n        except Exception as e:\n            print(""Warning: {}"".format(e))\n            # In rare cases, exceptions are thrown.\n            return []\n\n    def getLargestFaceBoundingBox(self, rgbImg, skipMulti=False):\n        """"""\n        Find the largest face bounding box in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :return: The largest face bounding box in an image, or None.\n        :rtype: dlib.rectangle\n        """"""\n        assert rgbImg is not None\n\n        faces = self.getAllFaceBoundingBoxes(rgbImg)\n        if (not skipMulti and len(faces) > 0) or len(faces) == 1:\n            return max(faces, key=lambda rect: rect.width() * rect.height())\n        else:\n            return None\n\n    def findLandmarks(self, rgbImg, bb):\n        """"""\n        Find the landmarks of a face.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to find landmarks for.\n        :type bb: dlib.rectangle\n        :return: Detected landmark locations.\n        :rtype: list of (x,y) tuples\n        """"""\n        assert rgbImg is not None\n        assert bb is not None\n\n        points = self.predictor(rgbImg, bb)\n        return list(map(lambda p: (p.x, p.y), points.parts()))\n\n    def align(self, imgDim, rgbImg, bb=None,\n              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,\n              skipMulti=False):\n        r""""""align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)\n\n        Transform and align a face in an image.\n\n        :param imgDim: The edge length in pixels of the square the image is resized to.\n        :type imgDim: int\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to align. \\\n                   Defaults to the largest face.\n        :type bb: dlib.rectangle\n        :param landmarks: Detected landmark locations. \\\n                          Landmarks found on `bb` if not provided.\n        :type landmarks: list of (x,y) tuples\n        :param landmarkIndices: The indices to transform to.\n        :type landmarkIndices: list of ints\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :return: The aligned RGB image. Shape: (imgDim, imgDim, 3)\n        :rtype: numpy.ndarray\n        """"""\n        assert imgDim is not None\n        assert rgbImg is not None\n        assert landmarkIndices is not None\n\n        if bb is None:\n            bb = self.getLargestFaceBoundingBox(rgbImg, skipMulti)\n            if bb is None:\n                return\n\n        if landmarks is None:\n            landmarks = self.findLandmarks(rgbImg, bb)\n\n        npLandmarks = np.float32(landmarks)\n        npLandmarkIndices = np.array(landmarkIndices)\n\n        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],\n                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices])\n        thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))\n\n        return thumbnail'"
beginners/notebook-examples/chapter-7/model.py,0,"b""# -----------------------------------------------------------------------------------------\n# Code taken from https://github.com/iwantooxxoox/Keras-OpenFace (with minor modifications)\n# -----------------------------------------------------------------------------------------\n\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.models import Model\nfrom keras import backend as K\n\nimport utils\nfrom utils import LRN2D\n\ndef create_model():\n    myInput = Input(shape=(96, 96, 3))\n\n    x = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\n    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\n    x = Activation('relu')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D(pool_size=3, strides=2)(x)\n    x = Lambda(LRN2D, name='lrn_1')(x)\n    x = Conv2D(64, (1, 1), name='conv2')(x)\n    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\n    x = Activation('relu')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(192, (3, 3), name='conv3')(x)\n    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\n    x = Activation('relu')(x)\n    x = Lambda(LRN2D, name='lrn_2')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D(pool_size=3, strides=2)(x)\n\n    # Inception3a\n    inception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\n    inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\n    inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n    inception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\n    inception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\n    inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\n    inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n\n    inception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\n    inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\n    inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n    inception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\n    inception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\n    inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\n    inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n\n    inception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\n    inception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\n    inception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\n    inception_3a_pool = Activation('relu')(inception_3a_pool)\n    inception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n\n    inception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\n    inception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\n    inception_3a_1x1 = Activation('relu')(inception_3a_1x1)\n\n    inception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n\n    # Inception3b\n    inception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\n    inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\n    inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n    inception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\n    inception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\n    inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\n    inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n\n    inception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\n    inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\n    inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n    inception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\n    inception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\n    inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\n    inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n\n    inception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3a)\n    inception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\n    inception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\n    inception_3b_pool = Activation('relu')(inception_3b_pool)\n    inception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n\n    inception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\n    inception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\n    inception_3b_1x1 = Activation('relu')(inception_3b_1x1)\n\n    inception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n\n    # Inception3c\n    inception_3c_3x3 = utils.conv2d_bn(inception_3b,\n                                       layer='inception_3c_3x3',\n                                       cv1_out=128,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=256,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(2, 2),\n                                       padding=(1, 1))\n\n    inception_3c_5x5 = utils.conv2d_bn(inception_3b,\n                                       layer='inception_3c_5x5',\n                                       cv1_out=32,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=64,\n                                       cv2_filter=(5, 5),\n                                       cv2_strides=(2, 2),\n                                       padding=(2, 2))\n\n    inception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\n    inception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n\n    inception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n\n    #inception 4a\n    inception_4a_3x3 = utils.conv2d_bn(inception_3c,\n                                       layer='inception_4a_3x3',\n                                       cv1_out=96,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=192,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(1, 1),\n                                       padding=(1, 1))\n    inception_4a_5x5 = utils.conv2d_bn(inception_3c,\n                                       layer='inception_4a_5x5',\n                                       cv1_out=32,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=64,\n                                       cv2_filter=(5, 5),\n                                       cv2_strides=(1, 1),\n                                       padding=(2, 2))\n\n    inception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3c)\n    inception_4a_pool = utils.conv2d_bn(inception_4a_pool,\n                                        layer='inception_4a_pool',\n                                        cv1_out=128,\n                                        cv1_filter=(1, 1),\n                                        padding=(2, 2))\n    inception_4a_1x1 = utils.conv2d_bn(inception_3c,\n                                       layer='inception_4a_1x1',\n                                       cv1_out=256,\n                                       cv1_filter=(1, 1))\n    inception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n\n    #inception4e\n    inception_4e_3x3 = utils.conv2d_bn(inception_4a,\n                                       layer='inception_4e_3x3',\n                                       cv1_out=160,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=256,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(2, 2),\n                                       padding=(1, 1))\n    inception_4e_5x5 = utils.conv2d_bn(inception_4a,\n                                       layer='inception_4e_5x5',\n                                       cv1_out=64,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=128,\n                                       cv2_filter=(5, 5),\n                                       cv2_strides=(2, 2),\n                                       padding=(2, 2))\n    inception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\n    inception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n\n    inception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n\n    #inception5a\n    inception_5a_3x3 = utils.conv2d_bn(inception_4e,\n                                       layer='inception_5a_3x3',\n                                       cv1_out=96,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=384,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(1, 1),\n                                       padding=(1, 1))\n\n    inception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4e)\n    inception_5a_pool = utils.conv2d_bn(inception_5a_pool,\n                                        layer='inception_5a_pool',\n                                        cv1_out=96,\n                                        cv1_filter=(1, 1),\n                                        padding=(1, 1))\n    inception_5a_1x1 = utils.conv2d_bn(inception_4e,\n                                       layer='inception_5a_1x1',\n                                       cv1_out=256,\n                                       cv1_filter=(1, 1))\n\n    inception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n\n    #inception_5b\n    inception_5b_3x3 = utils.conv2d_bn(inception_5a,\n                                       layer='inception_5b_3x3',\n                                       cv1_out=96,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=384,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(1, 1),\n                                       padding=(1, 1))\n    inception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\n    inception_5b_pool = utils.conv2d_bn(inception_5b_pool,\n                                        layer='inception_5b_pool',\n                                        cv1_out=96,\n                                        cv1_filter=(1, 1))\n    inception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n\n    inception_5b_1x1 = utils.conv2d_bn(inception_5a,\n                                       layer='inception_5b_1x1',\n                                       cv1_out=256,\n                                       cv1_filter=(1, 1))\n    inception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n\n    av_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\n    reshape_layer = Flatten()(av_pool)\n    dense_layer = Dense(128, name='dense_layer')(reshape_layer)\n    norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n\n    return Model(inputs=[myInput], outputs=norm_layer)\n"""
beginners/notebook-examples/chapter-7/utils.py,4,"b""# -----------------------------------------------------------------------------------------\n# Code taken from https://github.com/iwantooxxoox/Keras-OpenFace (with minor modifications)\n# -----------------------------------------------------------------------------------------\n\nimport tensorflow as tf\nimport numpy as np\nimport os\n\nfrom numpy import genfromtxt\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation\nfrom keras.layers.normalization import BatchNormalization\n\n_FLOATX = 'float32'\n\ndef variable(value, dtype=_FLOATX, name=None):\n  v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n  _get_session().run(v.initializer)\n  return v\n\ndef shape(x):\n  return x.get_shape()\n\ndef square(x):\n  return tf.square(x)\n\ndef zeros(shape, dtype=_FLOATX, name=None):\n  return variable(np.zeros(shape), dtype, name)\n\ndef concatenate(tensors, axis=-1):\n  if axis < 0:\n      axis = axis % len(tensors[0].get_shape())\n  return tf.concat(axis, tensors)\n\ndef LRN2D(x):\n  return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n\ndef conv2d_bn(\n  x,\n  layer=None,\n  cv1_out=None,\n  cv1_filter=(1, 1),\n  cv1_strides=(1, 1),\n  cv2_out=None,\n  cv2_filter=(3, 3),\n  cv2_strides=(1, 1),\n  padding=None,\n):\n  num = '' if cv2_out == None else '1'\n  tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, name=layer+'_conv'+num)(x)\n  tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n  tensor = Activation('relu')(tensor)\n  if padding == None:\n    return tensor\n  tensor = ZeroPadding2D(padding=padding)(tensor)\n  if cv2_out == None:\n    return tensor\n  tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, name=layer+'_conv'+'2')(tensor)\n  tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n  tensor = Activation('relu')(tensor)\n  return tensor\n\nweights = [\n  'conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3',\n  'inception_3a_1x1_conv', 'inception_3a_1x1_bn',\n  'inception_3a_pool_conv', 'inception_3a_pool_bn',\n  'inception_3a_5x5_conv1', 'inception_3a_5x5_conv2', 'inception_3a_5x5_bn1', 'inception_3a_5x5_bn2',\n  'inception_3a_3x3_conv1', 'inception_3a_3x3_conv2', 'inception_3a_3x3_bn1', 'inception_3a_3x3_bn2',\n  'inception_3b_3x3_conv1', 'inception_3b_3x3_conv2', 'inception_3b_3x3_bn1', 'inception_3b_3x3_bn2',\n  'inception_3b_5x5_conv1', 'inception_3b_5x5_conv2', 'inception_3b_5x5_bn1', 'inception_3b_5x5_bn2',\n  'inception_3b_pool_conv', 'inception_3b_pool_bn',\n  'inception_3b_1x1_conv', 'inception_3b_1x1_bn',\n  'inception_3c_3x3_conv1', 'inception_3c_3x3_conv2', 'inception_3c_3x3_bn1', 'inception_3c_3x3_bn2',\n  'inception_3c_5x5_conv1', 'inception_3c_5x5_conv2', 'inception_3c_5x5_bn1', 'inception_3c_5x5_bn2',\n  'inception_4a_3x3_conv1', 'inception_4a_3x3_conv2', 'inception_4a_3x3_bn1', 'inception_4a_3x3_bn2',\n  'inception_4a_5x5_conv1', 'inception_4a_5x5_conv2', 'inception_4a_5x5_bn1', 'inception_4a_5x5_bn2',\n  'inception_4a_pool_conv', 'inception_4a_pool_bn',\n  'inception_4a_1x1_conv', 'inception_4a_1x1_bn',\n  'inception_4e_3x3_conv1', 'inception_4e_3x3_conv2', 'inception_4e_3x3_bn1', 'inception_4e_3x3_bn2',\n  'inception_4e_5x5_conv1', 'inception_4e_5x5_conv2', 'inception_4e_5x5_bn1', 'inception_4e_5x5_bn2',\n  'inception_5a_3x3_conv1', 'inception_5a_3x3_conv2', 'inception_5a_3x3_bn1', 'inception_5a_3x3_bn2',\n  'inception_5a_pool_conv', 'inception_5a_pool_bn',\n  'inception_5a_1x1_conv', 'inception_5a_1x1_bn',\n  'inception_5b_3x3_conv1', 'inception_5b_3x3_conv2', 'inception_5b_3x3_bn1', 'inception_5b_3x3_bn2',\n  'inception_5b_pool_conv', 'inception_5b_pool_bn',\n  'inception_5b_1x1_conv', 'inception_5b_1x1_bn',\n  'dense_layer'\n]\n\nconv_shape = {\n  'conv1': [64, 3, 7, 7],\n  'conv2': [64, 64, 1, 1],\n  'conv3': [192, 64, 3, 3],\n  'inception_3a_1x1_conv': [64, 192, 1, 1],\n  'inception_3a_pool_conv': [32, 192, 1, 1],\n  'inception_3a_5x5_conv1': [16, 192, 1, 1],\n  'inception_3a_5x5_conv2': [32, 16, 5, 5],\n  'inception_3a_3x3_conv1': [96, 192, 1, 1],\n  'inception_3a_3x3_conv2': [128, 96, 3, 3],\n  'inception_3b_3x3_conv1': [96, 256, 1, 1],\n  'inception_3b_3x3_conv2': [128, 96, 3, 3],\n  'inception_3b_5x5_conv1': [32, 256, 1, 1],\n  'inception_3b_5x5_conv2': [64, 32, 5, 5],\n  'inception_3b_pool_conv': [64, 256, 1, 1],\n  'inception_3b_1x1_conv': [64, 256, 1, 1],\n  'inception_3c_3x3_conv1': [128, 320, 1, 1],\n  'inception_3c_3x3_conv2': [256, 128, 3, 3],\n  'inception_3c_5x5_conv1': [32, 320, 1, 1],\n  'inception_3c_5x5_conv2': [64, 32, 5, 5],\n  'inception_4a_3x3_conv1': [96, 640, 1, 1],\n  'inception_4a_3x3_conv2': [192, 96, 3, 3],\n  'inception_4a_5x5_conv1': [32, 640, 1, 1,],\n  'inception_4a_5x5_conv2': [64, 32, 5, 5],\n  'inception_4a_pool_conv': [128, 640, 1, 1],\n  'inception_4a_1x1_conv': [256, 640, 1, 1],\n  'inception_4e_3x3_conv1': [160, 640, 1, 1],\n  'inception_4e_3x3_conv2': [256, 160, 3, 3],\n  'inception_4e_5x5_conv1': [64, 640, 1, 1],\n  'inception_4e_5x5_conv2': [128, 64, 5, 5],\n  'inception_5a_3x3_conv1': [96, 1024, 1, 1],\n  'inception_5a_3x3_conv2': [384, 96, 3, 3],\n  'inception_5a_pool_conv': [96, 1024, 1, 1],\n  'inception_5a_1x1_conv': [256, 1024, 1, 1],\n  'inception_5b_3x3_conv1': [96, 736, 1, 1],\n  'inception_5b_3x3_conv2': [384, 96, 3, 3],\n  'inception_5b_pool_conv': [96, 736, 1, 1],\n  'inception_5b_1x1_conv': [256, 736, 1, 1],\n}\n\ndef load_weights():\n  weightsDir = './weights'\n  fileNames = filter(lambda f: not f.startswith('.'), os.listdir(weightsDir))\n  paths = {}\n  weights_dict = {}\n\n  for n in fileNames:\n    paths[n.replace('.csv', '')] = weightsDir + '/' + n\n\n  for name in weights:\n    if 'conv' in name:\n      conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n      conv_w = np.reshape(conv_w, conv_shape[name])\n      conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n      conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n      weights_dict[name] = [conv_w, conv_b]     \n    elif 'bn' in name:\n      bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n      bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n      bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n      bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n      weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n    elif 'dense' in name:\n      dense_w = genfromtxt(weightsDir+'/dense_w.csv', delimiter=',', dtype=None)\n      dense_w = np.reshape(dense_w, (128, 736))\n      dense_w = np.transpose(dense_w, (1, 0))\n      dense_b = genfromtxt(weightsDir+'/dense_b.csv', delimiter=',', dtype=None)\n      weights_dict[name] = [dense_w, dense_b]\n\n  return weights_dict\n"""
beginners/notebook-examples/chapter-7/face_detection/face_detect_cv3.py,0,"b'# -*- coding: utf-8 -*-\n\nimport cv2\nimport sys\n\n# Get user supplied values\nimagePath = sys.argv[1]\ncascPath = ""haarcascade_frontalface_default.xml""\n\n# Create the haar cascade\nfaceCascade = cv2.CascadeClassifier(cascPath)\n\n# Read the image\nimage = cv2.imread(imagePath)\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Detect faces in the image\nfaces = faceCascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30)\n    #flags = cv2.CV_HAAR_SCALE_IMAGE\n)\n\nprint(""Found {0} faces!"".format(len(faces)))\n\n# Draw a rectangle around the faces\nfor (x, y, w, h) in faces:\n    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\ncv2.imshow(""Faces found"", image)\ncv2.waitKey(0)'"
beginners/notebook-examples/chapter-7/face_detection/face_detect_fr.py,0,"b'# -*- coding: utf-8 -*-\n\nimport cv2\nimport sys\n\nimport face_recognition\n\n\n# Get user supplied values\nimagePath = sys.argv[1]\n\n# Load the image with face_recognition\nimage = face_recognition.load_image_file(imagePath)\n# Detect faces in the image\nface_locations = face_recognition.face_locations(image)\n\nprint(""Found {0} faces!"".format(len(face_locations)))\n\n\n# Read the image with openCV\nimage = cv2.imread(imagePath)\n\n# Draw a rectangle around the faces\nfor (top, right, bottom, left) in face_locations:\n    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n\n\ncv2.imshow(""Faces found"", image)\ncv2.waitKey(0)'"
