file_path,api_count,code
code/10_gan_models/10.3_model.py,55,"b'from __future__ import division\nimport os\nimport time\nimport math\nfrom glob import glob\nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import xrange\n\nfrom ops import *\nfrom utils import *\n\ndef conv_out_size_same(size, stride):\n  return int(math.ceil(float(size) / float(stride)))\n\nclass DCGAN(object):\n  def __init__(self, sess, input_height=108, input_width=108, crop=True,\n         batch_size=64, sample_num = 64, output_height=64, output_width=64,\n         y_dim=None, z_dim=100, gf_dim=64, df_dim=64,\n         gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name=\'default\',\n         input_fname_pattern=\'*.jpg\', checkpoint_dir=None, sample_dir=None, data_dir=\'./data\'):\n    """"""\n    Args:\n      sess: TensorFlow session\n      batch_size: The size of batch. Should be specified before training.\n      y_dim: (optional) Dimension of dim for y. [None]\n      z_dim: (optional) Dimension of dim for Z. [100]\n      gf_dim: (optional) Dimension of gen filters in first conv layer. [64]\n      df_dim: (optional) Dimension of discrim filters in first conv layer. [64]\n      gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024]\n      dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]\n      c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3]\n    """"""\n    self.sess = sess # \xe5\xbd\x93\xe5\x89\x8d\xe4\xbc\x9a\xe8\xaf\x9d\n    self.crop = crop # \xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe8\xa3\x81\xe5\x89\xaa\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f,\xe5\x8d\xb3\xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\n\n    self.batch_size = batch_size # \xe8\xae\xad\xe7\xbb\x83\xe6\x89\x80\xe7\x94\xa8\xe7\x9a\x84\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\n    self.sample_num = sample_num # \xe6\xaf\x8f\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe4\xb8\xad\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\n\n    self.input_height = input_height # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xad\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xae\xbd\xe5\xba\xa6\n    self.input_width = input_width # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xad\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\n    self.output_height = output_height # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe9\xab\x98\xe5\xba\xa6\n    self.output_width = output_width # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe5\xae\xbd\xe5\xba\xa6\n\n    self.y_dim = y_dim # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe6\x88\x96\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x9d\xa1\xe4\xbb\xb6\xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\n    self.z_dim = z_dim # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0\xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\n\n    self.gf_dim = gf_dim # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    self.df_dim = df_dim # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n\n    self.gfc_dim = gfc_dim # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\n    self.dfc_dim = dfc_dim # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\n    # \xe5\xae\x9a\xe4\xb9\x89\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\x92\x8c\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84 BN \xe5\xb1\x82\n    # batch normalization : deals with poor initialization helps gradient flow\n    self.d_bn1 = batch_norm(name=\'d_bn1\')\n    self.d_bn2 = batch_norm(name=\'d_bn2\')\n\n    if not self.y_dim:\n      self.d_bn3 = batch_norm(name=\'d_bn3\')\n\n    self.g_bn0 = batch_norm(name=\'g_bn0\')\n    self.g_bn1 = batch_norm(name=\'g_bn1\')\n    self.g_bn2 = batch_norm(name=\'g_bn2\')\n\n    if not self.y_dim:\n      self.g_bn3 = batch_norm(name=\'g_bn3\')\n\n    self.dataset_name = dataset_name\n    self.input_fname_pattern = input_fname_pattern\n    self.checkpoint_dir = checkpoint_dir\n    self.data_dir = data_dir\n\n    if self.dataset_name == \'mnist\':\n      self.data_X, self.data_y = self.load_mnist()\n      self.c_dim = self.data_X[0].shape[-1]\n    else:\n      self.data = glob(os.path.join(self.data_dir, self.dataset_name, self.input_fname_pattern))\n      np.random.shuffle(self.data)\n      imreadImg = imread(self.data[0])\n      # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x92\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0 c.dim(\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xb8\xba 1,\xe5\xbd\xa9\xe8\x89\xb2\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xb8\xba 3)\n      if len(imreadImg.shape) >= 3: #check if image is a non-grayscale image by checking channel number\n        self.c_dim = imread(self.data[0]).shape[-1]\n      else:\n        self.c_dim = 1\n\n    self.grayscale = (self.c_dim == 1)\n\n    self.build_model() # \xe5\xbb\xba\xe7\xab\x8b\xe6\xa8\xa1\xe5\x9e\x8b\n\n  def build_model(self):\n    if self.y_dim:\n      # \xe5\xae\x9a\xe4\xb9\x89\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe6\x88\x96\xe8\x80\x85\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x9d\xa1\xe4\xbb\xb6\xe5\x90\x91\xe9\x87\x8f\n      self.y = tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name=\'y\')\n    else:\n      self.y = None\n\n    if self.crop:\n      # \xe5\xa6\x82\xe6\x9e\x9c self.crop \xe4\xb8\xba True,\xe5\x88\x99\xe5\x9c\xa8\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe4\xb8\xad\xe5\xa4\xae\xe8\xa3\x81\xe5\x89\xaa\xe5\x87\xba\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba\n      # self.output_height \xe5\x92\x8c self.output_width \xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\n      image_dims = [self.output_height, self.output_width, self.c_dim]\n    else:\n      image_dims = [self.input_height, self.input_width, self.c_dim]\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae,\xe5\x8d\xb3\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\n    self.inputs = tf.placeholder(\n      tf.float32, [self.batch_size] + image_dims, name=\'real_images\')\n\n    inputs = self.inputs\n\n    self.z = tf.placeholder(\n      tf.float32, [None, self.z_dim], name=\'z\')\n    self.z_sum = histogram_summary(""z"", self.z)\n    # \xe5\x88\x86\xe5\x88\xab\xe5\x88\x9b\xe5\xbb\xba\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe3\x80\x81\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe5\x92\x8c\xe9\x87\x87\xe6\xa0\xb7\xe5\x99\xa8,\xe5\xb9\xb6\xe5\xbe\x97\xe5\x88\xb0\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad,\n    # \xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe8\xbf\xad\xe4\xbb\xa3\xe6\xad\xa5\xe6\x95\xb0,\xe9\x87\x87\xe6\xa0\xb7\xe5\x99\xa8\xe8\xa2\xab\xe8\xb0\x83\xe7\x94\xa8\xe4\xb8\x80\xe6\xac\xa1,\xe5\x85\xb6\xe6\x9c\xac\xe8\xb4\xa8\xe6\x98\xaf\xe7\x94\xa8\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\x8e\xa8\xe7\x90\x86\xe5\xb9\xb6\xe5\xbe\x97\xe5\x88\xb0\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f,\n    # \xe9\x80\x9a\xe8\xbf\x87\xe5\xaf\xb9\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe5\x81\x9a\xe8\xb4\xa8\xe9\x87\x8f\xe8\xaf\x84\xe4\xbb\xb7,\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9f\xa5\xe9\x81\x93\xe5\xbd\x93\xe5\x89\x8d\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe5\xa5\xbd\xe5\x9d\x8f\n    self.G                  = self.generator(self.z, self.y)\n    self.D, self.D_logits   = self.discriminator(inputs, self.y, reuse=False)\n    self.sampler            = self.sampler(self.z, self.y)\n    self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True)\n    \n    self.d_sum = histogram_summary(""d"", self.D)\n    self.d__sum = histogram_summary(""d_"", self.D_)\n    self.G_sum = image_summary(""G"", self.G)\n\n    def sigmoid_cross_entropy_with_logits(x, y):\n      try:\n        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)\n      except:\n        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe5\x9c\xa8\xe7\x9c\x9f\xe5\xae\x9e\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x86\xe5\xb8\x83\xe4\xb8\x8a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    self.d_loss_real = tf.reduce_mean(\n      sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe5\x9c\xa8\xe7\x94\x9f\xe6\x88\x90\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x86\xe5\xb8\x83\xe4\xb8\x8a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    self.d_loss_fake = tf.reduce_mean(\n      sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n    # \xe5\xae\x9a\xe4\xb9\x89\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\x9c\xa8\xe7\x94\x9f\xe6\x88\x90\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x86\xe5\xb8\x83\xe4\xb8\x8a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    self.g_loss = tf.reduce_mean(\n      sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))\n\n    self.d_loss_real_sum = scalar_summary(""d_loss_real"", self.d_loss_real)\n    self.d_loss_fake_sum = scalar_summary(""d_loss_fake"", self.d_loss_fake)\n                          \n    self.d_loss = self.d_loss_real + self.d_loss_fake # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe6\x80\xbb\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n\n    self.g_loss_sum = scalar_summary(""g_loss"", self.g_loss)\n    self.d_loss_sum = scalar_summary(""d_loss"", self.d_loss)\n\n    t_vars = tf.trainable_variables() # \xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xaf\xe8\xa2\xab\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n\n    self.d_vars = [var for var in t_vars if \'d_\' in var.name] # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe4\xb8\xad\xe5\x8f\xaf\xe8\xa2\xab\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    self.g_vars = [var for var in t_vars if \'g_\' in var.name] # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe4\xb8\xad\xe5\x8f\xaf\xe8\xa2\xab\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n\n    self.saver = tf.train.Saver()\n\n  def train(self, config):\n    # \xe8\xb0\x83\xe7\x94\xa8 Adam \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84 minimize \xe6\x96\xb9\xe6\xb3\x95\xe5\xbe\x97\xe5\x88\xb0\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\x92\x8c\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9c\n    d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n              .minimize(self.d_loss, var_list=self.d_vars)\n    g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n              .minimize(self.g_loss, var_list=self.g_vars)\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xbd\x93\xe5\x89\x8d\xe4\xbc\x9a\xe8\xaf\x9d\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\n    try:\n      tf.global_variables_initializer().run()\n    except:\n      tf.initialize_all_variables().run()\n\n    self.g_sum = merge_summary([self.z_sum, self.d__sum,\n      self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n    self.d_sum = merge_summary(\n        [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n    self.writer = SummaryWriter(""./logs"", self.sess.graph)\n\n    sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim))\n    \n    if config.dataset == \'mnist\':\n      sample_inputs = self.data_X[0:self.sample_num]\n      sample_labels = self.data_y[0:self.sample_num]\n    else:\n      sample_files = self.data[0:self.sample_num]\n      sample = [\n          get_image(sample_file,\n                    input_height=self.input_height,\n                    input_width=self.input_width,\n                    resize_height=self.output_height,\n                    resize_width=self.output_width,\n                    crop=self.crop,\n                    grayscale=self.grayscale) for sample_file in sample_files]\n      if (self.grayscale):\n        sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None]\n      else:\n        sample_inputs = np.array(sample).astype(np.float32)\n  \n    counter = 1 # \xe8\xbf\xad\xe4\xbb\xa3\xe6\xad\xa5\xe6\x95\xb0\xe7\x9a\x84\xe8\xae\xa1\xe6\x95\xb0\xe5\x99\xa8\n    start_time = time.time()\n    could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n    if could_load:\n      counter = checkpoint_counter\n      print("" [*] Load SUCCESS"")\n    else:\n      print("" [!] Load failed..."")\n    # \xe6\xa0\xb9\xe6\x8d\xae config \xe4\xb8\xad\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84 epoch \xe6\x80\xbb\xe6\x95\xb0(\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba 25)\xe5\xbc\x80\xe5\xa7\x8b\xe5\xbe\xaa\xe7\x8e\xaf\xe6\x89\xa7\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n    for epoch in xrange(config.epoch):\n      if config.dataset == \'mnist\':\n        batch_idxs = min(len(self.data_X), config.train_size) // config.batch_size\n      else:      \n        self.data = glob(os.path.join(\n          config.data_dir, config.dataset, self.input_fname_pattern))\n        np.random.shuffle(self.data)\n        batch_idxs = min(len(self.data), config.train_size) // config.batch_size\n\n      for idx in xrange(0, batch_idxs):\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae\xe7\x94\xa8\xe4\xba\x8e\xe5\x8d\x95\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\n        if config.dataset == \'mnist\':\n          batch_images = self.data_X[idx*config.batch_size:(idx+1)*config.batch_size]\n          batch_labels = self.data_y[idx*config.batch_size:(idx+1)*config.batch_size]\n        else:\n          batch_files = self.data[idx*config.batch_size:(idx+1)*config.batch_size]\n          # \xe8\xb0\x83\xe7\x94\xa8 utils.py \xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84 get_image \xe6\x96\xb9\xe6\xb3\x95\xe8\xb0\x83\xe6\x95\xb4\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f,\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe7\x94\xb1 self.input_height \n          # \xe5\x8f\x98\xe4\xb8\xba self.output_height,\xe5\xae\xbd\xe5\xba\xa6\xe7\x94\xb1 self.input_width \xe5\x8f\x98\xe4\xb8\xba self.output_width\n          batch = [\n              get_image(batch_file,\n                        input_height=self.input_height,\n                        input_width=self.input_width,\n                        resize_height=self.output_height,\n                        resize_width=self.output_width,\n                        crop=self.crop,\n                        grayscale=self.grayscale) for batch_file in batch_files]\n          if self.grayscale:\n            batch_images = np.array(batch).astype(np.float32)[:, :, :, None]\n          else:\n            batch_images = np.array(batch).astype(np.float32)\n\n        batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\\n              .astype(np.float32)\n        # \xe5\xaf\xb9\xe4\xba\x8e MNIST \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86,\xe6\x89\xa7\xe8\xa1\x8c\xe7\x89\xb9\xe6\xae\x8a\xe7\x9a\x84\xe5\xa4\x84\xe7\x90\x86\xe9\x80\xbb\xe8\xbe\x91\n        if config.dataset == \'mnist\':\n          # Update D network\n          # \xe6\x89\xa7\xe8\xa1\x8c d_optim \xe6\x93\x8d\xe4\xbd\x9c,\xe6\x9b\xb4\xe6\x96\xb0\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n          _, summary_str = self.sess.run([d_optim, self.d_sum],\n            feed_dict={ \n              self.inputs: batch_images,\n              self.z: batch_z,\n              self.y:batch_labels,\n            })\n          self.writer.add_summary(summary_str, counter)\n\n          # Update G network\n          # \xe6\x89\xa7\xe8\xa1\x8c g_optim \xe6\x93\x8d\xe4\xbd\x9c,\xe6\x9b\xb4\xe6\x96\xb0\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={\n              self.z: batch_z, \n              self.y:batch_labels,\n            })\n          self.writer.add_summary(summary_str, counter)\n\n          # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={ self.z: batch_z, self.y:batch_labels })\n          self.writer.add_summary(summary_str, counter)\n          \n          errD_fake = self.d_loss_fake.eval({\n              self.z: batch_z, \n              self.y:batch_labels\n          })\n          errD_real = self.d_loss_real.eval({\n              self.inputs: batch_images,\n              self.y:batch_labels\n          })\n          errG = self.g_loss.eval({\n              self.z: batch_z,\n              self.y: batch_labels\n          })\n        else:\n          # \xe5\xaf\xb9\xe4\xba\x8e\xe5\x85\xb6\xe4\xbb\x96\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86,\xe5\x90\x8c\xe6\xa0\xb7\xe4\xbe\x9d\xe6\xac\xa1\xe6\x89\xa7\xe8\xa1\x8c d_optim \xe5\x92\x8c g_optim \xe6\x93\x8d\xe4\xbd\x9c,\n          # \xe8\xaf\xa5\xe5\x88\x86\xe6\x94\xaf\xe4\xb8\x8e mnist \xe5\x88\x86\xe6\x94\xaf\xe4\xbb\xa3\xe7\xa0\x81\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\xab\xe5\x9c\xa8\xe4\xba\x8e\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe5\x92\x8c\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe6\xb2\xa1\xe6\x9c\x89\xe6\x9d\xa1\xe4\xbb\xb6\xe7\xba\xa6\xe6\x9d\x9f\n          # Update D network\n          _, summary_str = self.sess.run([d_optim, self.d_sum],\n            feed_dict={ self.inputs: batch_images, self.z: batch_z })\n          self.writer.add_summary(summary_str, counter)\n\n          # Update G network\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={ self.z: batch_z })\n          self.writer.add_summary(summary_str, counter)\n\n          # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={ self.z: batch_z })\n          self.writer.add_summary(summary_str, counter)\n          \n          errD_fake = self.d_loss_fake.eval({ self.z: batch_z })\n          errD_real = self.d_loss_real.eval({ self.inputs: batch_images })\n          errG = self.g_loss.eval({self.z: batch_z})\n\n        counter += 1\n        print(""Epoch: [%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f"" \\\n          % (epoch, config.epoch, idx, batch_idxs,\n            time.time() - start_time, errD_fake+errD_real, errG))\n\n        if np.mod(counter, 100) == 1:\n          if config.dataset == \'mnist\':\n            samples, d_loss, g_loss = self.sess.run(\n              [self.sampler, self.d_loss, self.g_loss],\n              feed_dict={\n                  self.z: sample_z,\n                  self.inputs: sample_inputs,\n                  self.y:sample_labels,\n              }\n            )\n            save_images(samples, image_manifold_size(samples.shape[0]),\n                  \'./{}/train_{:02d}_{:04d}.png\'.format(config.sample_dir, epoch, idx))\n            print(""[Sample] d_loss: %.8f, g_loss: %.8f"" % (d_loss, g_loss)) \n          else:\n            try:\n              samples, d_loss, g_loss = self.sess.run(\n                [self.sampler, self.d_loss, self.g_loss],\n                feed_dict={\n                    self.z: sample_z,\n                    self.inputs: sample_inputs,\n                },\n              )\n              save_images(samples, image_manifold_size(samples.shape[0]),\n                    \'./{}/train_{:02d}_{:04d}.png\'.format(config.sample_dir, epoch, idx))\n              print(""[Sample] d_loss: %.8f, g_loss: %.8f"" % (d_loss, g_loss)) \n            except:\n              print(""one pic error!..."")\n\n        if np.mod(counter, 500) == 2:\n          self.save(config.checkpoint_dir, counter)\n\n  def discriminator(self, image, y=None, reuse=False):\n    with tf.variable_scope(""discriminator"") as scope:\n      if reuse:\n        # \xe5\xa4\x8d\xe7\x94\xa8\xe5\xb7\xb2\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\n        scope.reuse_variables()\n      # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe6\x9d\xa1\xe4\xbb\xb6\xe5\x90\x91\xe9\x87\x8f y,\xe5\x88\x99\xe5\xbb\xba\xe7\xab\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\xa6\xe6\x9c\x89\xe5\x9b\x9b\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\xe4\xb8\xba\xe4\xba\x86\xe9\x81\xbf\xe5\x85\x8d\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb6\x88\xe5\xa4\xb1\xe7\x8e\xb0\xe8\xb1\xa1,\n      # \xe6\xaf\x8f\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb9\x8b\xe5\x90\x8e\xe9\x83\xbd\xe5\x8a\xa0 BN \xe5\xb1\x82\xe3\x80\x82\xe9\x99\xa4\xe4\xba\x86\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xba sigmoid \xe4\xb9\x8b\xe5\xa4\x96,\xe5\x85\xb6\xe4\xbb\x96\xe5\xb1\x82\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xba leaky ReLU\n      if not self.y_dim:\n        h0 = lrelu(conv2d(image, self.df_dim, name=\'d_h0_conv\'))\n        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name=\'d_h1_conv\')))\n        h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name=\'d_h2_conv\')))\n        h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name=\'d_h3_conv\')))\n        h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, \'d_h4_lin\')\n\n        return tf.nn.sigmoid(h4), h4\n      else:\n        # \xe4\xb8\x8e not self.y_dim \xe5\x88\x86\xe6\x94\xaf\xe4\xb8\xad\xe5\xbb\xba\xe6\xa8\xa1\xe7\x9a\x84\xe7\x89\xb9\xe7\x82\xb9\xe7\x9b\xb8\xe5\x90\x8c,\xe9\x99\xa4\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe5\xa4\x96\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe9\x83\xbd\xe5\x8c\x85\xe5\x90\xab \n        # BN\xe3\x80\x81leaky ReLU \xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe7\xae\x97\xe5\xad\x90,\xe5\x8c\xba\xe5\x88\xab\xe5\x9c\xa8\xe4\xba\x8e\xe5\x90\x8e\xe4\xb8\xa4\xe5\xb1\x82\xe4\xb8\x8d\xe6\x98\xaf\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82,\xe8\x80\x8c\xe6\x98\xaf\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n        x = conv_cond_concat(image, yb)\n\n        h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=\'d_h0_conv\'))\n        h0 = conv_cond_concat(h0, yb)\n\n        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name=\'d_h1_conv\')))\n        h1 = tf.reshape(h1, [self.batch_size, -1])      \n        h1 = concat([h1, y], 1)\n        \n        h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, \'d_h2_lin\')))\n        h2 = concat([h2, y], 1)\n\n        h3 = linear(h2, 1, \'d_h3_lin\')\n        \n        return tf.nn.sigmoid(h3), h3\n\n  def generator(self, z, y=None):\n    with tf.variable_scope(""generator"") as scope:\n      if not self.y_dim:\n        # \xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\xe5\x88\x86\xe5\x88\xab\xe8\xae\xbe\xe5\xae\x9a\xe4\xb8\xba self.output_height \xe5\x92\x8c self.output_width\xe3\x80\x82\n        # \xe5\x9b\xa0\xe4\xb8\xba\xe6\xaf\x8f\xe5\xb1\x82\xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84 stride \xe9\x83\xbd\xe8\xae\xbe\xe4\xb8\xba 2,\xe6\x89\x80\xe4\xbb\xa5\xe6\xaf\x8f\xe5\xb1\x82\xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb9\x8b\xe5\x90\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\xe9\x83\xbd\xe4\xbc\x9a\xe5\x8a\xa0\xe5\x80\x8d\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n        s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n\n        # project `z` and reshape\n        # \xe5\xb0\x86\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x99\xaa\xe5\xa3\xb0\xe5\x90\x91\xe9\x87\x8f\xe9\x80\x9a\xe8\xbf\x87\xe7\xba\xbf\xe6\x80\xa7\xe5\x8f\x98\xe6\x8d\xa2\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 self.gf_dim*8*s_h16*s_w16 \xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\n        self.z_, self.h0_w, self.h0_b = linear(\n            z, self.gf_dim*8*s_h16*s_w16, \'g_h0_lin\', with_w=True)\n\n        self.h0 = tf.reshape(\n            self.z_, [-1, s_h16, s_w16, self.gf_dim * 8])\n        h0 = tf.nn.relu(self.g_bn0(self.h0))\n        # \xe6\x8c\x89\xe7\x85\xa7\xe8\xb0\x83\xe5\x8f\x82\xe7\xbb\x8f\xe9\xaa\x8c,\xe9\x99\xa4\xe4\xba\x86\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe9\x87\x87\xe7\x94\xa8 tanh \xe4\xb9\x8b\xe5\xa4\x96,\xe5\x85\xb6\xe4\xbb\x96\xe5\xb1\x82\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe9\x87\x87\xe7\x94\xa8 ReLU\n        self.h1, self.h1_w, self.h1_b = deconv2d(\n            h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=\'g_h1\', with_w=True)\n        h1 = tf.nn.relu(self.g_bn1(self.h1))\n        # \xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8e h1 \xe7\x9a\x84\xe6\xb1\x82\xe5\x8f\x96\xe8\xbf\x87\xe7\xa8\x8b,\xe9\x80\x9a\xe8\xbf\x87\xe8\xbf\x9e\xe7\xbb\xad\xe8\xb0\x83\xe7\x94\xa8\xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c(deconv2d)\xe3\x80\x81\n        # BN\xe6\x93\x8d\xe4\xbd\x9c(self.g_bn1\xe3\x80\x81self.g_bn2\xe3\x80\x81self.g_bn3)\xe5\x92\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe6\x93\x8d\xe4\xbd\x9c(tf.nn.relu),\xe5\xbe\x97\xe5\x88\xb0h2\xe5\x92\x8ch3\n        h2, self.h2_w, self.h2_b = deconv2d(\n            h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=\'g_h2\', with_w=True)\n        h2 = tf.nn.relu(self.g_bn2(h2))\n\n        h3, self.h3_w, self.h3_b = deconv2d(\n            h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=\'g_h3\', with_w=True)\n        h3 = tf.nn.relu(self.g_bn3(h3))\n\n        h4, self.h4_w, self.h4_b = deconv2d(\n            h3, [self.batch_size, s_h, s_w, self.c_dim], name=\'g_h4\', with_w=True)\n\n        return tf.nn.tanh(h4)\n      else:\n        # \xe5\xbd\x93\xe6\x9c\x89\xe6\x9d\xa1\xe4\xbb\xb6\xe5\x90\x91\xe9\x87\x8f y \xe6\x97\xb6,\xe6\x9e\x84\xe9\x80\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\xbb\xe8\xa6\x81\xe7\x94\xb1\xe4\xb8\xa4\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x92\x8c\xe4\xb8\xa4\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84\xe7\x9b\xb8\xe5\xaf\xb9\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        # \xe5\x9b\xa0\xe4\xb8\xba\xe6\x9c\x89\xe6\x9d\xa1\xe4\xbb\xb6\xe5\x90\x91\xe9\x87\x8f y,\xe6\x89\x80\xe4\xbb\xa5\xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x90\x91\xe9\x87\x8f\xe4\xb8\x8e\xe6\x9d\xa1\xe4\xbb\xb6\xe5\x90\x91\xe9\x87\x8f y \xe8\xbf\x9e\xe6\x8e\xa5\xe8\xb5\xb7\xe6\x9d\xa5,\xe4\xbb\xa5\xe5\xbd\xa2\xe6\x88\x90\xe6\x9d\xa1\xe4\xbb\xb6\xe7\xba\xa6\xe6\x9d\x9f\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_h4 = int(s_h/2), int(s_h/4)\n        s_w2, s_w4 = int(s_w/2), int(s_w/4)\n\n        # yb = tf.expand_dims(tf.expand_dims(y, 1),2)\n        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n        z = concat([z, y], 1)\n\n        h0 = tf.nn.relu(\n            self.g_bn0(linear(z, self.gfc_dim, \'g_h0_lin\')))\n        h0 = concat([h0, y], 1)\n\n        h1 = tf.nn.relu(self.g_bn1(\n            linear(h0, self.gf_dim*2*s_h4*s_w4, \'g_h1_lin\')))\n        h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2])\n\n        h1 = conv_cond_concat(h1, yb)\n\n        h2 = tf.nn.relu(self.g_bn2(deconv2d(h1,\n            [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=\'g_h2\')))\n        h2 = conv_cond_concat(h2, yb)\n\n        return tf.nn.sigmoid(\n            deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=\'g_h3\'))\n\n  def sampler(self, z, y=None):\n    with tf.variable_scope(""generator"") as scope:\n      scope.reuse_variables()\n\n      if not self.y_dim:\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n        s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n\n        # project `z` and reshape\n        h0 = tf.reshape(\n            linear(z, self.gf_dim*8*s_h16*s_w16, \'g_h0_lin\'),\n            [-1, s_h16, s_w16, self.gf_dim * 8])\n        h0 = tf.nn.relu(self.g_bn0(h0, train=False))\n\n        h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=\'g_h1\')\n        h1 = tf.nn.relu(self.g_bn1(h1, train=False))\n\n        h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=\'g_h2\')\n        h2 = tf.nn.relu(self.g_bn2(h2, train=False))\n\n        h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=\'g_h3\')\n        h3 = tf.nn.relu(self.g_bn3(h3, train=False))\n\n        h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name=\'g_h4\')\n\n        return tf.nn.tanh(h4)\n      else:\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_h4 = int(s_h/2), int(s_h/4)\n        s_w2, s_w4 = int(s_w/2), int(s_w/4)\n\n        # yb = tf.reshape(y, [-1, 1, 1, self.y_dim])\n        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n        z = concat([z, y], 1)\n\n        h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, \'g_h0_lin\'), train=False))\n        h0 = concat([h0, y], 1)\n\n        h1 = tf.nn.relu(self.g_bn1(\n            linear(h0, self.gf_dim*2*s_h4*s_w4, \'g_h1_lin\'), train=False))\n        h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2])\n        h1 = conv_cond_concat(h1, yb)\n\n        h2 = tf.nn.relu(self.g_bn2(\n            deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=\'g_h2\'), train=False))\n        h2 = conv_cond_concat(h2, yb)\n\n        return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=\'g_h3\'))\n\n  def load_mnist(self):\n    data_dir = os.path.join(self.data_dir, self.dataset_name)\n    \n    fd = open(os.path.join(data_dir,\'train-images-idx3-ubyte\'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float)\n\n    fd = open(os.path.join(data_dir,\'train-labels-idx1-ubyte\'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    trY = loaded[8:].reshape((60000)).astype(np.float)\n\n    fd = open(os.path.join(data_dir,\'t10k-images-idx3-ubyte\'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float)\n\n    fd = open(os.path.join(data_dir,\'t10k-labels-idx1-ubyte\'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    teY = loaded[8:].reshape((10000)).astype(np.float)\n\n    trY = np.asarray(trY)\n    teY = np.asarray(teY)\n    \n    X = np.concatenate((trX, teX), axis=0)\n    y = np.concatenate((trY, teY), axis=0).astype(np.int)\n    \n    seed = 547\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(y)\n    \n    y_vec = np.zeros((len(y), self.y_dim), dtype=np.float)\n    for i, label in enumerate(y):\n      y_vec[i,y[i]] = 1.0\n    \n    return X/255.,y_vec\n\n  @property\n  def model_dir(self):\n    return ""{}_{}_{}_{}"".format(\n        self.dataset_name, self.batch_size,\n        self.output_height, self.output_width)\n      \n  def save(self, checkpoint_dir, step):\n    model_name = ""DCGAN.model""\n    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n    if not os.path.exists(checkpoint_dir):\n      os.makedirs(checkpoint_dir)\n\n    self.saver.save(self.sess,\n            os.path.join(checkpoint_dir, model_name),\n            global_step=step)\n\n  def load(self, checkpoint_dir):\n    import re\n    print("" [*] Reading checkpoints..."")\n    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n      counter = int(next(re.finditer(""(\\d+)(?!.*\\d)"",ckpt_name)).group(0))\n      print("" [*] Success to read {}"".format(ckpt_name))\n      return True, counter\n    else:\n      print("" [*] Failed to find a checkpoint"")\n      return False, 0\n'"
code/11_rnn_models/11.1_rnn_cell_impl.py,7,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Module implementing RNN Cells.\nThis module provides a number of basic commonly used RNN cells, such as LSTM\n(Long Short Term Memory) or GRU (Gated Recurrent Unit), and a number of\noperators that allow adding dropouts, projections, or embeddings for inputs.\nConstructing multi-layer cells is supported by the class `MultiRNNCell`, or by\ncalling the `rnn` ops several times.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport hashlib\nimport numbers\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.layers import base as base_layer\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import variables as tf_variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training.checkpointable import base as checkpointable\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n_BIAS_VARIABLE_NAME = ""bias""\n_WEIGHTS_VARIABLE_NAME = ""kernel""\n\n\n# TODO(jblespiau): Remove this function when we are sure there are no longer\n# any usage (even if protected, it is being used). Prefer assert_like_rnncell.\ndef _like_rnncell(cell):\n  """"""Checks that a given object is an RNNCell by using duck typing.""""""\n  conditions = [hasattr(cell, ""output_size""), hasattr(cell, ""state_size""),\n                hasattr(cell, ""zero_state""), callable(cell)]\n  return all(conditions)\n\n\n# This can be used with self.assertRaisesRegexp for assert_like_rnncell.\nASSERT_LIKE_RNNCELL_ERROR_REGEXP = ""is not an RNNCell""\n\n\ndef assert_like_rnncell(cell_name, cell):\n  """"""Raises a TypeError if cell is not like an RNNCell.\n  NOTE: Do not rely on the error message (in particular in tests) which can be\n  subject to change to increase readability. Use\n  ASSERT_LIKE_RNNCELL_ERROR_REGEXP.\n  Args:\n    cell_name: A string to give a meaningful error referencing to the name\n      of the functionargument.\n    cell: The object which should behave like an RNNCell.\n  Raises:\n    TypeError: A human-friendly exception.\n  """"""\n  conditions = [\n      hasattr(cell, ""output_size""),\n      hasattr(cell, ""state_size""),\n      hasattr(cell, ""zero_state""),\n      callable(cell),\n  ]\n  errors = [\n      ""\'output_size\' property is missing"",\n      ""\'state_size\' property is missing"",\n      ""\'zero_state\' method is missing"",\n      ""is not callable""\n  ]\n\n  if not all(conditions):\n\n    errors = [error for error, cond in zip(errors, conditions) if not cond]\n    raise TypeError(""The argument {!r} ({}) is not an RNNCell: {}."".format(\n        cell_name, cell, "", "".join(errors)))\n\n\ndef _concat(prefix, suffix, static=False):\n  """"""Concat that enables int, Tensor, or TensorShape values.\n  This function takes a size specification, which can be an integer, a\n  TensorShape, or a Tensor, and converts it into a concatenated Tensor\n  (if static = False) or a list of integers (if static = True).\n  Args:\n    prefix: The prefix; usually the batch size (and/or time step size).\n      (TensorShape, int, or Tensor.)\n    suffix: TensorShape, int, or Tensor.\n    static: If `True`, return a python list with possibly unknown dimensions.\n      Otherwise return a `Tensor`.\n  Returns:\n    shape: the concatenation of prefix and suffix.\n  Raises:\n    ValueError: if `suffix` is not a scalar or vector (or TensorShape).\n    ValueError: if prefix or suffix was `None` and asked for dynamic\n      Tensors out.\n  """"""\n  if isinstance(prefix, ops.Tensor):\n    p = prefix\n    p_static = tensor_util.constant_value(prefix)\n    if p.shape.ndims == 0:\n      p = array_ops.expand_dims(p, 0)\n    elif p.shape.ndims != 1:\n      raise ValueError(""prefix tensor must be either a scalar or vector, ""\n                       ""but saw tensor: %s"" % p)\n  else:\n    p = tensor_shape.as_shape(prefix)\n    p_static = p.as_list() if p.ndims is not None else None\n    p = (constant_op.constant(p.as_list(), dtype=dtypes.int32)\n         if p.is_fully_defined() else None)\n  if isinstance(suffix, ops.Tensor):\n    s = suffix\n    s_static = tensor_util.constant_value(suffix)\n    if s.shape.ndims == 0:\n      s = array_ops.expand_dims(s, 0)\n    elif s.shape.ndims != 1:\n      raise ValueError(""suffix tensor must be either a scalar or vector, ""\n                       ""but saw tensor: %s"" % s)\n  else:\n    s = tensor_shape.as_shape(suffix)\n    s_static = s.as_list() if s.ndims is not None else None\n    s = (constant_op.constant(s.as_list(), dtype=dtypes.int32)\n         if s.is_fully_defined() else None)\n\n  if static:\n    shape = tensor_shape.as_shape(p_static).concatenate(s_static)\n    shape = shape.as_list() if shape.ndims is not None else None\n  else:\n    if p is None or s is None:\n      raise ValueError(""Provided a prefix or suffix of None: %s and %s""\n                       % (prefix, suffix))\n    shape = array_ops.concat((p, s), 0)\n  return shape\n\n\ndef _zero_state_tensors(state_size, batch_size, dtype):\n  """"""Create tensors of zeros based on state_size, batch_size, and dtype.""""""\n  def get_state_shape(s):\n    """"""Combine s with batch_size to get a proper tensor shape.""""""\n    c = _concat(batch_size, s)\n    size = array_ops.zeros(c, dtype=dtype)\n    if not context.executing_eagerly():\n      c_static = _concat(batch_size, s, static=True)\n      size.set_shape(c_static)\n    return size\n  return nest.map_structure(get_state_shape, state_size)\n\n\n@tf_export(""nn.rnn_cell.RNNCell"")\nclass RNNCell(base_layer.Layer):\n  """"""Abstract object representing an RNN cell.\n  Every `RNNCell` must have the properties below and implement `call` with\n  the signature `(output, next_state) = call(input, state)`.  The optional\n  third input argument, `scope`, is allowed for backwards compatibility\n  purposes; but should be left off for new subclasses.\n  This definition of cell differs from the definition used in the literature.\n  In the literature, \'cell\' refers to an object with a single scalar output.\n  This definition refers to a horizontal array of such units.\n  An RNN cell, in the most abstract setting, is anything that has\n  a state and performs some operation that takes a matrix of inputs.\n  This operation results in an output matrix with `self.output_size` columns.\n  If `self.state_size` is an integer, this operation also results in a new\n  state matrix with `self.state_size` columns.  If `self.state_size` is a\n  (possibly nested tuple of) TensorShape object(s), then it should return a\n  matching structure of Tensors having shape `[batch_size].concatenate(s)`\n  for each `s` in `self.batch_size`.\n  """"""\n\n  def __call__(self, inputs, state, scope=None):\n    """"""Run this RNN cell on inputs, starting from the given state.\n    Args:\n      inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n      state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n        with shape `[batch_size, self.state_size]`.  Otherwise, if\n        `self.state_size` is a tuple of integers, this should be a tuple\n        with shapes `[batch_size, s] for s in self.state_size`.\n      scope: VariableScope for the created subgraph; defaults to class name.\n    Returns:\n      A pair containing:\n      - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n      - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n        the arity and shapes of `state`.\n    """"""\n    if scope is not None:\n      with vs.variable_scope(scope,\n                             custom_getter=self._rnn_get_variable) as scope:\n        return super(RNNCell, self).__call__(inputs, state, scope=scope)\n    else:\n      scope_attrname = ""rnncell_scope""\n      scope = getattr(self, scope_attrname, None)\n      if scope is None:\n        scope = vs.variable_scope(vs.get_variable_scope(),\n                                  custom_getter=self._rnn_get_variable)\n        setattr(self, scope_attrname, scope)\n      with scope:\n        return super(RNNCell, self).__call__(inputs, state)\n\n  def _rnn_get_variable(self, getter, *args, **kwargs):\n    variable = getter(*args, **kwargs)\n    if context.executing_eagerly():\n      trainable = variable._trainable  # pylint: disable=protected-access\n    else:\n      trainable = (\n          variable in tf_variables.trainable_variables() or\n          (isinstance(variable, tf_variables.PartitionedVariable) and\n           list(variable)[0] in tf_variables.trainable_variables()))\n    if trainable and variable not in self._trainable_weights:\n      self._trainable_weights.append(variable)\n    elif not trainable and variable not in self._non_trainable_weights:\n      self._non_trainable_weights.append(variable)\n    return variable\n\n  @property\n  def state_size(self):\n    """"""size(s) of state(s) used by this cell.\n    It can be represented by an Integer, a TensorShape or a tuple of Integers\n    or TensorShapes.\n    """"""\n    raise NotImplementedError(""Abstract method"")\n\n  @property\n  def output_size(self):\n    """"""Integer or TensorShape: size of outputs produced by this cell.""""""\n    raise NotImplementedError(""Abstract method"")\n\n  def build(self, _):\n    # This tells the parent Layer object that it\'s OK to call\n    # self.add_variable() inside the call() method.\n    pass\n\n  def zero_state(self, batch_size, dtype):\n    """"""Return zero-filled state tensor(s).\n    Args:\n      batch_size: int, float, or unit Tensor representing the batch size.\n      dtype: the data type to use for the state.\n    Returns:\n      If `state_size` is an int or TensorShape, then the return value is a\n      `N-D` tensor of shape `[batch_size, state_size]` filled with zeros.\n      If `state_size` is a nested list or tuple, then the return value is\n      a nested list or tuple (of the same structure) of `2-D` tensors with\n      the shapes `[batch_size, s]` for each s in `state_size`.\n    """"""\n    # Try to use the last cached zero_state. This is done to avoid recreating\n    # zeros, especially when eager execution is enabled.\n    state_size = self.state_size\n    is_eager = context.executing_eagerly()\n    if is_eager and hasattr(self, ""_last_zero_state""):\n      (last_state_size, last_batch_size, last_dtype,\n       last_output) = getattr(self, ""_last_zero_state"")\n      if (last_batch_size == batch_size and\n          last_dtype == dtype and\n          last_state_size == state_size):\n        return last_output\n    with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n      output = _zero_state_tensors(state_size, batch_size, dtype)\n    if is_eager:\n      self._last_zero_state = (state_size, batch_size, dtype, output)\n    return output\n\n\nclass LayerRNNCell(RNNCell):\n  """"""Subclass of RNNCells that act like proper `tf.Layer` objects.\n  For backwards compatibility purposes, most `RNNCell` instances allow their\n  `call` methods to instantiate variables via `tf.get_variable`.  The underlying\n  variable scope thus keeps track of any variables, and returning cached\n  versions.  This is atypical of `tf.layer` objects, which separate this\n  part of layer building into a `build` method that is only called once.\n  Here we provide a subclass for `RNNCell` objects that act exactly as\n  `Layer` objects do.  They must provide a `build` method and their\n  `call` methods do not access Variables `tf.get_variable`.\n  """"""\n\n  def __call__(self, inputs, state, scope=None, *args, **kwargs):\n    """"""Run this RNN cell on inputs, starting from the given state.\n    Args:\n      inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n      state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n        with shape `[batch_size, self.state_size]`.  Otherwise, if\n        `self.state_size` is a tuple of integers, this should be a tuple\n        with shapes `[batch_size, s] for s in self.state_size`.\n      scope: optional cell scope.\n      *args: Additional positional arguments.\n      **kwargs: Additional keyword arguments.\n    Returns:\n      A pair containing:\n      - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n      - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n        the arity and shapes of `state`.\n    """"""\n    # Bypass RNNCell\'s variable capturing semantics for LayerRNNCell.\n    # Instead, it is up to subclasses to provide a proper build\n    # method.  See the class docstring for more details.\n    return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n                                     *args, **kwargs)\n\n\n@tf_export(""nn.rnn_cell.BasicRNNCell"")\nclass BasicRNNCell(LayerRNNCell):\n  """"""The most basic RNN cell.\n  Args:\n    num_units: int, The number of units in the RNN cell.\n    activation: Nonlinearity to use.  Default: `tanh`.\n    reuse: (optional) Python boolean describing whether to reuse variables\n     in an existing scope.  If not `True`, and the existing scope already has\n     the given variables, an error is raised.\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such\n      cases.\n    dtype: Default dtype of the layer (default of `None` means use the type\n      of the first input). Required when `build` is called before `call`.\n  """"""\n\n  def __init__(self,\n               num_units,\n               activation=None,\n               reuse=None,\n               name=None,\n               dtype=None):\n    super(BasicRNNCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)\n\n    # Inputs must be 2-dimensional.\n    self.input_spec = base_layer.InputSpec(ndim=2)\n\n    self._num_units = num_units\n    self._activation = activation or math_ops.tanh\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def build(self, inputs_shape):\n    if inputs_shape[1].value is None:\n      raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""\n                       % inputs_shape)\n\n    input_depth = inputs_shape[1].value\n    self._kernel = self.add_variable(\n        _WEIGHTS_VARIABLE_NAME,\n        shape=[input_depth + self._num_units, self._num_units])\n    self._bias = self.add_variable(\n        _BIAS_VARIABLE_NAME,\n        shape=[self._num_units],\n        initializer=init_ops.zeros_initializer(dtype=self.dtype))\n\n    self.built = True\n\n  def call(self, inputs, state):\n    """"""Most basic RNN: output = new_state = act(W * input + U * state + B).""""""\n\n    gate_inputs = math_ops.matmul(\n        array_ops.concat([inputs, state], 1), self._kernel)\n    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)\n    output = self._activation(gate_inputs)\n    return output, output\n\n\n@tf_export(""nn.rnn_cell.GRUCell"")\nclass GRUCell(LayerRNNCell):\n  """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n  Args:\n    num_units: int, The number of units in the GRU cell.\n    activation: Nonlinearity to use.  Default: `tanh`.\n    reuse: (optional) Python boolean describing whether to reuse variables\n     in an existing scope.  If not `True`, and the existing scope already has\n     the given variables, an error is raised.\n    kernel_initializer: (optional) The initializer to use for the weight and\n    projection matrices.\n    bias_initializer: (optional) The initializer to use for the bias.\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such\n      cases.\n    dtype: Default dtype of the layer (default of `None` means use the type\n      of the first input). Required when `build` is called before `call`.\n  """"""\n\n  def __init__(self,\n               num_units,\n               activation=None,\n               reuse=None,\n               kernel_initializer=None,\n               bias_initializer=None,\n               name=None,\n               dtype=None):\n    super(GRUCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)\n\n    # Inputs must be 2-dimensional.\n    self.input_spec = base_layer.InputSpec(ndim=2)\n\n    self._num_units = num_units\n    self._activation = activation or math_ops.tanh\n    self._kernel_initializer = kernel_initializer\n    self._bias_initializer = bias_initializer\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def build(self, inputs_shape):\n    if inputs_shape[1].value is None:\n      raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""\n                       % inputs_shape)\n\n    input_depth = inputs_shape[1].value\n    self._gate_kernel = self.add_variable(\n        ""gates/%s"" % _WEIGHTS_VARIABLE_NAME,\n        shape=[input_depth + self._num_units, 2 * self._num_units],\n        initializer=self._kernel_initializer)\n    self._gate_bias = self.add_variable(\n        ""gates/%s"" % _BIAS_VARIABLE_NAME,\n        shape=[2 * self._num_units],\n        initializer=(\n            self._bias_initializer\n            if self._bias_initializer is not None\n            else init_ops.constant_initializer(1.0, dtype=self.dtype)))\n    self._candidate_kernel = self.add_variable(\n        ""candidate/%s"" % _WEIGHTS_VARIABLE_NAME,\n        shape=[input_depth + self._num_units, self._num_units],\n        initializer=self._kernel_initializer)\n    self._candidate_bias = self.add_variable(\n        ""candidate/%s"" % _BIAS_VARIABLE_NAME,\n        shape=[self._num_units],\n        initializer=(\n            self._bias_initializer\n            if self._bias_initializer is not None\n            else init_ops.zeros_initializer(dtype=self.dtype)))\n\n    self.built = True\n\n  def call(self, inputs, state):\n    """"""Gated recurrent unit (GRU) with nunits cells.""""""\n\n    gate_inputs = math_ops.matmul(\n        array_ops.concat([inputs, state], 1), self._gate_kernel)\n    gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)\n\n    value = math_ops.sigmoid(gate_inputs)\n    r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n\n    r_state = r * state\n\n    candidate = math_ops.matmul(\n        array_ops.concat([inputs, r_state], 1), self._candidate_kernel)\n    candidate = nn_ops.bias_add(candidate, self._candidate_bias)\n\n    c = self._activation(candidate)\n    new_h = u * state + (1 - u) * c\n    return new_h, new_h\n\n\n_LSTMStateTuple = collections.namedtuple(""LSTMStateTuple"", (""c"", ""h""))\n\n\n@tf_export(""nn.rnn_cell.LSTMStateTuple"")\nclass LSTMStateTuple(_LSTMStateTuple):\n  """"""Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n  Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state\n  and `h` is the output.\n  Only used when `state_is_tuple=True`.\n  """"""\n  __slots__ = ()\n\n  @property\n  def dtype(self):\n    (c, h) = self\n    if c.dtype != h.dtype:\n      raise TypeError(""Inconsistent internal state: %s vs %s"" %\n                      (str(c.dtype), str(h.dtype)))\n    return c.dtype\n\n\n@tf_export(""nn.rnn_cell.BasicLSTMCell"")\nclass BasicLSTMCell(LayerRNNCell):\n  """"""Basic LSTM recurrent network cell.\n  The implementation is based on: http://arxiv.org/abs/1409.2329.\n  We add forget_bias (default: 1) to the biases of the forget gate in order to\n  reduce the scale of forgetting in the beginning of the training.\n  It does not allow cell clipping, a projection layer, and does not\n  use peep-hole connections: it is the basic baseline.\n  For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\n  that follows.\n  """"""\n\n  def __init__(self,\n               num_units,\n               forget_bias=1.0,\n               state_is_tuple=True,\n               activation=None,\n               reuse=None,\n               name=None,\n               dtype=None):\n    """"""Initialize the basic LSTM cell.\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      forget_bias: float, The bias added to forget gates (see above).\n        Must set to `0.0` manually when restoring from CudnnLSTM-trained\n        checkpoints.\n      state_is_tuple: If True, accepted and returned states are 2-tuples of\n        the `c_state` and `m_state`.  If False, they are concatenated\n        along the column axis.  The latter behavior will soon be deprecated.\n      activation: Activation function of the inner states.  Default: `tanh`.\n      reuse: (optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already has\n        the given variables, an error is raised.\n      name: String, the name of the layer. Layers with the same name will\n        share weights, but to avoid mistakes we require reuse=True in such\n        cases.\n      dtype: Default dtype of the layer (default of `None` means use the type\n        of the first input). Required when `build` is called before `call`.\n      When restoring from CudnnLSTM-trained checkpoints, must use\n      `CudnnCompatibleLSTMCell` instead.\n    """"""\n    super(BasicLSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)\n    if not state_is_tuple:\n      logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                   ""deprecated.  Use state_is_tuple=True."", self)\n\n    # Inputs must be 2-dimensional.\n    self.input_spec = base_layer.InputSpec(ndim=2)\n\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._activation = activation or math_ops.tanh\n\n  @property\n  def state_size(self):\n    return (LSTMStateTuple(self._num_units, self._num_units)\n            if self._state_is_tuple else 2 * self._num_units)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def build(self, inputs_shape):\n    if inputs_shape[1].value is None:\n      raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""\n                       % inputs_shape)\n\n    input_depth = inputs_shape[1].value\n    h_depth = self._num_units\n    self._kernel = self.add_variable(\n        _WEIGHTS_VARIABLE_NAME,\n        shape=[input_depth + h_depth, 4 * self._num_units])\n    self._bias = self.add_variable(\n        _BIAS_VARIABLE_NAME,\n        shape=[4 * self._num_units],\n        initializer=init_ops.zeros_initializer(dtype=self.dtype))\n\n    self.built = True\n\n  def call(self, inputs, state):\n    """"""Long short-term memory cell (LSTM).\n    Args:\n      inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n      state: An `LSTMStateTuple` of state tensors, each shaped\n        `[batch_size, num_units]`, if `state_is_tuple` has been set to\n        `True`.  Otherwise, a `Tensor` shaped\n        `[batch_size, 2 * num_units]`.\n    Returns:\n      A pair containing the new hidden state, and the new state (either a\n        `LSTMStateTuple` or a concatenated state, depending on\n        `state_is_tuple`).\n    """"""\n    sigmoid = math_ops.sigmoid\n    one = constant_op.constant(1, dtype=dtypes.int32)\n    # Parameters of gates are concatenated into one multiply for efficiency.\n    if self._state_is_tuple:\n      c, h = state\n    else:\n      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one)\n\n    gate_inputs = math_ops.matmul(\n        array_ops.concat([inputs, h], 1), self._kernel)\n    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)\n\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    i, j, f, o = array_ops.split(\n        value=gate_inputs, num_or_size_splits=4, axis=one)\n\n    forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)\n    # Note that using `add` and `multiply` instead of `+` and `*` gives a\n    # performance improvement. So using those at the cost of readability.\n    add = math_ops.add\n    multiply = math_ops.multiply\n    new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))),\n                multiply(sigmoid(i), self._activation(j)))\n    new_h = multiply(self._activation(new_c), sigmoid(o))\n\n    if self._state_is_tuple:\n      new_state = LSTMStateTuple(new_c, new_h)\n    else:\n      new_state = array_ops.concat([new_c, new_h], 1)\n    return new_h, new_state\n\n\n@tf_export(""nn.rnn_cell.LSTMCell"")\nclass LSTMCell(LayerRNNCell):\n  """"""Long short-term memory unit (LSTM) recurrent network cell.\n  The default non-peephole implementation is based on:\n    http://www.bioinf.jku.at/publications/older/2604.pdf\n  S. Hochreiter and J. Schmidhuber.\n  ""Long Short-Term Memory"". Neural Computation, 9(8):1735-1780, 1997.\n  The peephole implementation is based on:\n    https://research.google.com/pubs/archive/43905.pdf\n  Hasim Sak, Andrew Senior, and Francoise Beaufays.\n  ""Long short-term memory recurrent neural network architectures for\n   large scale acoustic modeling."" INTERSPEECH, 2014.\n  The class uses optional peep-hole connections, optional cell clipping, and\n  an optional projection layer.\n  """"""\n\n  def __init__(self, num_units,\n               use_peepholes=False, cell_clip=None,\n               initializer=None, num_proj=None, proj_clip=None,\n               num_unit_shards=None, num_proj_shards=None,\n               forget_bias=1.0, state_is_tuple=True,\n               activation=None, reuse=None, name=None, dtype=None):\n    """"""Initialize the parameters for an LSTM cell.\n    Args:\n      num_units: int, The number of units in the LSTM cell.\n      use_peepholes: bool, set True to enable diagonal/peephole connections.\n      cell_clip: (optional) A float value, if provided the cell state is clipped\n        by this value prior to the cell output activation.\n      initializer: (optional) The initializer to use for the weight and\n        projection matrices.\n      num_proj: (optional) int, The output dimensionality for the projection\n        matrices.  If None, no projection is performed.\n      proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n        provided, then the projected values are clipped elementwise to within\n        `[-proj_clip, proj_clip]`.\n      num_unit_shards: Deprecated, will be removed by Jan. 2017.\n        Use a variable_scope partitioner instead.\n      num_proj_shards: Deprecated, will be removed by Jan. 2017.\n        Use a variable_scope partitioner instead.\n      forget_bias: Biases of the forget gate are initialized by default to 1\n        in order to reduce the scale of forgetting at the beginning of\n        the training. Must set it manually to `0.0` when restoring from\n        CudnnLSTM trained checkpoints.\n      state_is_tuple: If True, accepted and returned states are 2-tuples of\n        the `c_state` and `m_state`.  If False, they are concatenated\n        along the column axis.  This latter behavior will soon be deprecated.\n      activation: Activation function of the inner states.  Default: `tanh`.\n      reuse: (optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already has\n        the given variables, an error is raised.\n      name: String, the name of the layer. Layers with the same name will\n        share weights, but to avoid mistakes we require reuse=True in such\n        cases.\n      dtype: Default dtype of the layer (default of `None` means use the type\n        of the first input). Required when `build` is called before `call`.\n      When restoring from CudnnLSTM-trained checkpoints, use\n      `CudnnCompatibleLSTMCell` instead.\n    """"""\n    super(LSTMCell, self).__init__(_reuse=reuse, name=name, dtype=dtype)\n    if not state_is_tuple:\n      logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                   ""deprecated.  Use state_is_tuple=True."", self)\n    if num_unit_shards is not None or num_proj_shards is not None:\n      logging.warn(\n          ""%s: The num_unit_shards and proj_unit_shards parameters are ""\n          ""deprecated and will be removed in Jan 2017.  ""\n          ""Use a variable scope with a partitioner instead."", self)\n\n    # Inputs must be 2-dimensional.\n    self.input_spec = base_layer.InputSpec(ndim=2)\n\n    self._num_units = num_units\n    self._use_peepholes = use_peepholes\n    self._cell_clip = cell_clip\n    self._initializer = initializer\n    self._num_proj = num_proj\n    self._proj_clip = proj_clip\n    self._num_unit_shards = num_unit_shards\n    self._num_proj_shards = num_proj_shards\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._activation = activation or math_ops.tanh\n\n    if num_proj:\n      self._state_size = (\n          LSTMStateTuple(num_units, num_proj)\n          if state_is_tuple else num_units + num_proj)\n      self._output_size = num_proj\n    else:\n      self._state_size = (\n          LSTMStateTuple(num_units, num_units)\n          if state_is_tuple else 2 * num_units)\n      self._output_size = num_units\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  def build(self, inputs_shape):\n    if inputs_shape[1].value is None:\n      raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""\n                       % inputs_shape)\n\n    input_depth = inputs_shape[1].value\n    h_depth = self._num_units if self._num_proj is None else self._num_proj\n    maybe_partitioner = (\n        partitioned_variables.fixed_size_partitioner(self._num_unit_shards)\n        if self._num_unit_shards is not None\n        else None)\n    self._kernel = self.add_variable(\n        _WEIGHTS_VARIABLE_NAME,\n        shape=[input_depth + h_depth, 4 * self._num_units],\n        initializer=self._initializer,\n        partitioner=maybe_partitioner)\n    if self.dtype is None:\n      initializer = init_ops.zeros_initializer\n    else:\n      initializer = init_ops.zeros_initializer(dtype=self.dtype)\n    self._bias = self.add_variable(\n        _BIAS_VARIABLE_NAME,\n        shape=[4 * self._num_units],\n        initializer=initializer)\n    if self._use_peepholes:\n      self._w_f_diag = self.add_variable(""w_f_diag"", shape=[self._num_units],\n                                         initializer=self._initializer)\n      self._w_i_diag = self.add_variable(""w_i_diag"", shape=[self._num_units],\n                                         initializer=self._initializer)\n      self._w_o_diag = self.add_variable(""w_o_diag"", shape=[self._num_units],\n                                         initializer=self._initializer)\n\n    if self._num_proj is not None:\n      maybe_proj_partitioner = (\n          partitioned_variables.fixed_size_partitioner(self._num_proj_shards)\n          if self._num_proj_shards is not None\n          else None)\n      self._proj_kernel = self.add_variable(\n          ""projection/%s"" % _WEIGHTS_VARIABLE_NAME,\n          shape=[self._num_units, self._num_proj],\n          initializer=self._initializer,\n          partitioner=maybe_proj_partitioner)\n\n    self.built = True\n\n  def call(self, inputs, state):\n    """"""Run one step of LSTM.\n    Args:\n      inputs: input Tensor, 2D, `[batch, num_units].\n      state: if `state_is_tuple` is False, this must be a state Tensor,\n        `2-D, [batch, state_size]`.  If `state_is_tuple` is True, this must be a\n        tuple of state Tensors, both `2-D`, with column sizes `c_state` and\n        `m_state`.\n    Returns:\n      A tuple containing:\n      - A `2-D, [batch, output_dim]`, Tensor representing the output of the\n        LSTM after reading `inputs` when previous state was `state`.\n        Here output_dim is:\n           num_proj if num_proj was set,\n           num_units otherwise.\n      - Tensor(s) representing the new state of LSTM after reading `inputs` when\n        the previous state was `state`.  Same type and shape(s) as `state`.\n    Raises:\n      ValueError: If input size cannot be inferred from inputs via\n        static shape inference.\n    """"""\n    num_proj = self._num_units if self._num_proj is None else self._num_proj\n    sigmoid = math_ops.sigmoid\n\n    if self._state_is_tuple:\n      (c_prev, m_prev) = state\n    else:\n      c_prev = array_ops.slice(state, [0, 0], [-1, self._num_units])\n      m_prev = array_ops.slice(state, [0, self._num_units], [-1, num_proj])\n\n    input_size = inputs.get_shape().with_rank(2)[1]\n    if input_size.value is None:\n      raise ValueError(""Could not infer input size from inputs.get_shape()[-1]"")\n\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    lstm_matrix = math_ops.matmul(\n        array_ops.concat([inputs, m_prev], 1), self._kernel)\n    lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias)\n\n    i, j, f, o = array_ops.split(\n        value=lstm_matrix, num_or_size_splits=4, axis=1)\n    # Diagonal connections\n    if self._use_peepholes:\n      c = (sigmoid(f + self._forget_bias + self._w_f_diag * c_prev) * c_prev +\n           sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))\n    else:\n      c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *\n           self._activation(j))\n\n    if self._cell_clip is not None:\n      # pylint: disable=invalid-unary-operand-type\n      c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)\n      # pylint: enable=invalid-unary-operand-type\n    if self._use_peepholes:\n      m = sigmoid(o + self._w_o_diag * c) * self._activation(c)\n    else:\n      m = sigmoid(o) * self._activation(c)\n\n    if self._num_proj is not None:\n      m = math_ops.matmul(m, self._proj_kernel)\n\n      if self._proj_clip is not None:\n        # pylint: disable=invalid-unary-operand-type\n        m = clip_ops.clip_by_value(m, -self._proj_clip, self._proj_clip)\n        # pylint: enable=invalid-unary-operand-type\n\n    new_state = (LSTMStateTuple(c, m) if self._state_is_tuple else\n                 array_ops.concat([c, m], 1))\n    return m, new_state\n\n\ndef _enumerated_map_structure_up_to(shallow_structure, map_fn, *args, **kwargs):\n  ix = [0]\n  def enumerated_fn(*inner_args, **inner_kwargs):\n    r = map_fn(ix[0], *inner_args, **inner_kwargs)\n    ix[0] += 1\n    return r\n  return nest.map_structure_up_to(shallow_structure,\n                                  enumerated_fn, *args, **kwargs)\n\n\ndef _default_dropout_state_filter_visitor(substate):\n  if isinstance(substate, LSTMStateTuple):\n    # Do not perform dropout on the memory state.\n    return LSTMStateTuple(c=False, h=True)\n  elif isinstance(substate, tensor_array_ops.TensorArray):\n    return False\n  return True\n\n\n@tf_export(""nn.rnn_cell.DropoutWrapper"")\nclass DropoutWrapper(RNNCell):\n  """"""Operator adding dropout to inputs and outputs of the given cell.""""""\n\n  def __init__(self, cell, input_keep_prob=1.0, output_keep_prob=1.0,\n               state_keep_prob=1.0, variational_recurrent=False,\n               input_size=None, dtype=None, seed=None,\n               dropout_state_filter_visitor=None):\n    """"""Create a cell with added input, state, and/or output dropout.\n    If `variational_recurrent` is set to `True` (**NOT** the default behavior),\n    then the same dropout mask is applied at every step, as described in:\n    Y. Gal, Z Ghahramani.  ""A Theoretically Grounded Application of Dropout in\n    Recurrent Neural Networks"".  https://arxiv.org/abs/1512.05287\n    Otherwise a different dropout mask is applied at every time step.\n    Note, by default (unless a custom `dropout_state_filter` is provided),\n    the memory state (`c` component of any `LSTMStateTuple`) passing through\n    a `DropoutWrapper` is never modified.  This behavior is described in the\n    above article.\n    Args:\n      cell: an RNNCell, a projection to output_size is added to it.\n      input_keep_prob: unit Tensor or float between 0 and 1, input keep\n        probability; if it is constant and 1, no input dropout will be added.\n      output_keep_prob: unit Tensor or float between 0 and 1, output keep\n        probability; if it is constant and 1, no output dropout will be added.\n      state_keep_prob: unit Tensor or float between 0 and 1, output keep\n        probability; if it is constant and 1, no output dropout will be added.\n        State dropout is performed on the outgoing states of the cell.\n        **Note** the state components to which dropout is applied when\n        `state_keep_prob` is in `(0, 1)` are also determined by\n        the argument `dropout_state_filter_visitor` (e.g. by default dropout\n        is never applied to the `c` component of an `LSTMStateTuple`).\n      variational_recurrent: Python bool.  If `True`, then the same\n        dropout pattern is applied across all time steps per run call.\n        If this parameter is set, `input_size` **must** be provided.\n      input_size: (optional) (possibly nested tuple of) `TensorShape` objects\n        containing the depth(s) of the input tensors expected to be passed in to\n        the `DropoutWrapper`.  Required and used **iff**\n         `variational_recurrent = True` and `input_keep_prob < 1`.\n      dtype: (optional) The `dtype` of the input, state, and output tensors.\n        Required and used **iff** `variational_recurrent = True`.\n      seed: (optional) integer, the randomness seed.\n      dropout_state_filter_visitor: (optional), default: (see below).  Function\n        that takes any hierarchical level of the state and returns\n        a scalar or depth=1 structure of Python booleans describing\n        which terms in the state should be dropped out.  In addition, if the\n        function returns `True`, dropout is applied across this sublevel.  If\n        the function returns `False`, dropout is not applied across this entire\n        sublevel.\n        Default behavior: perform dropout on all terms except the memory (`c`)\n        state of `LSTMCellState` objects, and don\'t try to apply dropout to\n        `TensorArray` objects:\n        ```\n        def dropout_state_filter_visitor(s):\n          if isinstance(s, LSTMCellState):\n            # Never perform dropout on the c state.\n            return LSTMCellState(c=False, h=True)\n          elif isinstance(s, TensorArray):\n            return False\n          return True\n        ```\n    Raises:\n      TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided\n        but not `callable`.\n      ValueError: if any of the keep_probs are not between 0 and 1.\n    """"""\n    super(DropoutWrapper, self).__init__()\n    assert_like_rnncell(""cell"", cell)\n\n    if (dropout_state_filter_visitor is not None\n        and not callable(dropout_state_filter_visitor)):\n      raise TypeError(""dropout_state_filter_visitor must be callable"")\n    self._dropout_state_filter = (\n        dropout_state_filter_visitor or _default_dropout_state_filter_visitor)\n    with ops.name_scope(""DropoutWrapperInit""):\n      def tensor_and_const_value(v):\n        tensor_value = ops.convert_to_tensor(v)\n        const_value = tensor_util.constant_value(tensor_value)\n        return (tensor_value, const_value)\n      for prob, attr in [(input_keep_prob, ""input_keep_prob""),\n                         (state_keep_prob, ""state_keep_prob""),\n                         (output_keep_prob, ""output_keep_prob"")]:\n        tensor_prob, const_prob = tensor_and_const_value(prob)\n        if const_prob is not None:\n          if const_prob < 0 or const_prob > 1:\n            raise ValueError(""Parameter %s must be between 0 and 1: %d""\n                             % (attr, const_prob))\n          setattr(self, ""_%s"" % attr, float(const_prob))\n        else:\n          setattr(self, ""_%s"" % attr, tensor_prob)\n\n    # Set cell, variational_recurrent, seed before running the code below\n    self._cell = cell\n    if isinstance(cell, checkpointable.CheckpointableBase):\n      self._track_checkpointable(self._cell, name=""cell"")\n    self._variational_recurrent = variational_recurrent\n    self._seed = seed\n\n    self._recurrent_input_noise = None\n    self._recurrent_state_noise = None\n    self._recurrent_output_noise = None\n\n    if variational_recurrent:\n      if dtype is None:\n        raise ValueError(\n            ""When variational_recurrent=True, dtype must be provided"")\n\n      def convert_to_batch_shape(s):\n        # Prepend a 1 for the batch dimension; for recurrent\n        # variational dropout we use the same dropout mask for all\n        # batch elements.\n        return array_ops.concat(\n            ([1], tensor_shape.TensorShape(s).as_list()), 0)\n\n      def batch_noise(s, inner_seed):\n        shape = convert_to_batch_shape(s)\n        return random_ops.random_uniform(shape, seed=inner_seed, dtype=dtype)\n\n      if (not isinstance(self._input_keep_prob, numbers.Real) or\n          self._input_keep_prob < 1.0):\n        if input_size is None:\n          raise ValueError(\n              ""When variational_recurrent=True and input_keep_prob < 1.0 or ""\n              ""is unknown, input_size must be provided"")\n        self._recurrent_input_noise = _enumerated_map_structure_up_to(\n            input_size,\n            lambda i, s: batch_noise(s, inner_seed=self._gen_seed(""input"", i)),\n            input_size)\n      self._recurrent_state_noise = _enumerated_map_structure_up_to(\n          cell.state_size,\n          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(""state"", i)),\n          cell.state_size)\n      self._recurrent_output_noise = _enumerated_map_structure_up_to(\n          cell.output_size,\n          lambda i, s: batch_noise(s, inner_seed=self._gen_seed(""output"", i)),\n          cell.output_size)\n\n  def _gen_seed(self, salt_prefix, index):\n    if self._seed is None:\n      return None\n    salt = ""%s_%d"" % (salt_prefix, index)\n    string = (str(self._seed) + salt).encode(""utf-8"")\n    return int(hashlib.md5(string).hexdigest()[:8], 16) & 0x7FFFFFFF\n\n  @property\n  def wrapped_cell(self):\n    return self._cell\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def zero_state(self, batch_size, dtype):\n    with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n      return self._cell.zero_state(batch_size, dtype)\n\n  def _variational_recurrent_dropout_value(\n      self, index, value, noise, keep_prob):\n    """"""Performs dropout given the pre-calculated noise tensor.""""""\n    # uniform [keep_prob, 1.0 + keep_prob)\n    random_tensor = keep_prob + noise\n\n    # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n    binary_tensor = math_ops.floor(random_tensor)\n    ret = math_ops.div(value, keep_prob) * binary_tensor\n    ret.set_shape(value.get_shape())\n    return ret\n\n  def _dropout(self, values, salt_prefix, recurrent_noise, keep_prob,\n               shallow_filtered_substructure=None):\n    """"""Decides whether to perform standard dropout or recurrent dropout.""""""\n\n    if shallow_filtered_substructure is None:\n      # Put something so we traverse the entire structure; inside the\n      # dropout function we check to see if leafs of this are bool or not.\n      shallow_filtered_substructure = values\n\n    if not self._variational_recurrent:\n      def dropout(i, do_dropout, v):\n        if not isinstance(do_dropout, bool) or do_dropout:\n          return nn_ops.dropout(\n              v, keep_prob=keep_prob, seed=self._gen_seed(salt_prefix, i))\n        else:\n          return v\n      return _enumerated_map_structure_up_to(\n          shallow_filtered_substructure, dropout,\n          *[shallow_filtered_substructure, values])\n    else:\n      def dropout(i, do_dropout, v, n):\n        if not isinstance(do_dropout, bool) or do_dropout:\n          return self._variational_recurrent_dropout_value(i, v, n, keep_prob)\n        else:\n          return v\n      return _enumerated_map_structure_up_to(\n          shallow_filtered_substructure, dropout,\n          *[shallow_filtered_substructure, values, recurrent_noise])\n\n  def __call__(self, inputs, state, scope=None):\n    """"""Run the cell with the declared dropouts.""""""\n    def _should_dropout(p):\n      return (not isinstance(p, float)) or p < 1\n\n    if _should_dropout(self._input_keep_prob):\n      inputs = self._dropout(inputs, ""input"",\n                             self._recurrent_input_noise,\n                             self._input_keep_prob)\n    output, new_state = self._cell(inputs, state, scope=scope)\n    if _should_dropout(self._state_keep_prob):\n      # Identify which subsets of the state to perform dropout on and\n      # which ones to keep.\n      shallow_filtered_substructure = nest.get_traverse_shallow_structure(\n          self._dropout_state_filter, new_state)\n      new_state = self._dropout(new_state, ""state"",\n                                self._recurrent_state_noise,\n                                self._state_keep_prob,\n                                shallow_filtered_substructure)\n    if _should_dropout(self._output_keep_prob):\n      output = self._dropout(output, ""output"",\n                             self._recurrent_output_noise,\n                             self._output_keep_prob)\n    return output, new_state\n\n\n@tf_export(""nn.rnn_cell.ResidualWrapper"")\nclass ResidualWrapper(RNNCell):\n  """"""RNNCell wrapper that ensures cell inputs are added to the outputs.""""""\n\n  def __init__(self, cell, residual_fn=None):\n    """"""Constructs a `ResidualWrapper` for `cell`.\n    Args:\n      cell: An instance of `RNNCell`.\n      residual_fn: (Optional) The function to map raw cell inputs and raw cell\n        outputs to the actual cell outputs of the residual network.\n        Defaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n        and outputs.\n    """"""\n    super(ResidualWrapper, self).__init__()\n    self._cell = cell\n    if isinstance(cell, checkpointable.CheckpointableBase):\n      self._track_checkpointable(self._cell, name=""cell"")\n    self._residual_fn = residual_fn\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def zero_state(self, batch_size, dtype):\n    with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n      return self._cell.zero_state(batch_size, dtype)\n\n  def __call__(self, inputs, state, scope=None):\n    """"""Run the cell and then apply the residual_fn on its inputs to its outputs.\n    Args:\n      inputs: cell inputs.\n      state: cell state.\n      scope: optional cell scope.\n    Returns:\n      Tuple of cell outputs and new state.\n    Raises:\n      TypeError: If cell inputs and outputs have different structure (type).\n      ValueError: If cell inputs and outputs have different structure (value).\n    """"""\n    outputs, new_state = self._cell(inputs, state, scope=scope)\n    # Ensure shapes match\n    def assert_shape_match(inp, out):\n      inp.get_shape().assert_is_compatible_with(out.get_shape())\n    def default_residual_fn(inputs, outputs):\n      nest.assert_same_structure(inputs, outputs)\n      nest.map_structure(assert_shape_match, inputs, outputs)\n      return nest.map_structure(lambda inp, out: inp + out, inputs, outputs)\n    res_outputs = (self._residual_fn or default_residual_fn)(inputs, outputs)\n    return (res_outputs, new_state)\n\n\n@tf_export(""nn.rnn_cell.DeviceWrapper"")\nclass DeviceWrapper(RNNCell):\n  """"""Operator that ensures an RNNCell runs on a particular device.""""""\n\n  def __init__(self, cell, device):\n    """"""Construct a `DeviceWrapper` for `cell` with device `device`.\n    Ensures the wrapped `cell` is called with `tf.device(device)`.\n    Args:\n      cell: An instance of `RNNCell`.\n      device: A device string or function, for passing to `tf.device`.\n    """"""\n    super(DeviceWrapper, self).__init__()\n    self._cell = cell\n    if isinstance(cell, checkpointable.CheckpointableBase):\n      self._track_checkpointable(self._cell, name=""cell"")\n    self._device = device\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def zero_state(self, batch_size, dtype):\n    with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n      with ops.device(self._device):\n        return self._cell.zero_state(batch_size, dtype)\n\n  def __call__(self, inputs, state, scope=None):\n    """"""Run the cell on specified device.""""""\n    with ops.device(self._device):\n      return self._cell(inputs, state, scope=scope)\n\n\n@tf_export(""nn.rnn_cell.MultiRNNCell"")\nclass MultiRNNCell(RNNCell):\n  """"""RNN cell composed sequentially of multiple simple cells.\n  Example:\n  ```python\n  num_units = [128, 64]\n  cells = [BasicLSTMCell(num_units=n) for n in num_units]\n  stacked_rnn_cell = MultiRNNCell(cells)\n  ```\n  """"""\n\n  def __init__(self, cells, state_is_tuple=True):\n    """"""Create a RNN cell composed sequentially of a number of RNNCells.\n    Args:\n      cells: list of RNNCells that will be composed in this order.\n      state_is_tuple: If True, accepted and returned states are n-tuples, where\n        `n = len(cells)`.  If False, the states are all\n        concatenated along the column axis.  This latter behavior will soon be\n        deprecated.\n    Raises:\n      ValueError: if cells is empty (not allowed), or at least one of the cells\n        returns a state tuple but the flag `state_is_tuple` is `False`.\n    """"""\n    super(MultiRNNCell, self).__init__()\n    if not cells:\n      raise ValueError(""Must specify at least one cell for MultiRNNCell."")\n    if not nest.is_sequence(cells):\n      raise TypeError(\n          ""cells must be a list or tuple, but saw: %s."" % cells)\n\n    self._cells = cells\n    for cell_number, cell in enumerate(self._cells):\n      # Add Checkpointable dependencies on these cells so their variables get\n      # saved with this object when using object-based saving.\n      if isinstance(cell, checkpointable.CheckpointableBase):\n        # TODO(allenl): Track down non-Checkpointable callers.\n        self._track_checkpointable(cell, name=""cell-%d"" % (cell_number,))\n    self._state_is_tuple = state_is_tuple\n    if not state_is_tuple:\n      if any(nest.is_sequence(c.state_size) for c in self._cells):\n        raise ValueError(""Some cells return tuples of states, but the flag ""\n                         ""state_is_tuple is not set.  State sizes are: %s""\n                         % str([c.state_size for c in self._cells]))\n\n  @property\n  def state_size(self):\n    if self._state_is_tuple:\n      return tuple(cell.state_size for cell in self._cells)\n    else:\n      return sum([cell.state_size for cell in self._cells])\n\n  @property\n  def output_size(self):\n    return self._cells[-1].output_size\n\n  def zero_state(self, batch_size, dtype):\n    with ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n      if self._state_is_tuple:\n        return tuple(cell.zero_state(batch_size, dtype) for cell in self._cells)\n      else:\n        # We know here that state_size of each cell is not a tuple and\n        # presumably does not contain TensorArrays or anything else fancy\n        return super(MultiRNNCell, self).zero_state(batch_size, dtype)\n\n  def call(self, inputs, state):\n    """"""Run this multi-layer cell on inputs, starting from state.""""""\n    cur_state_pos = 0\n    cur_inp = inputs\n    new_states = []\n    for i, cell in enumerate(self._cells):\n      with vs.variable_scope(""cell_%d"" % i):\n        if self._state_is_tuple:\n          if not nest.is_sequence(state):\n            raise ValueError(\n                ""Expected state to be a tuple of length %d, but received: %s"" %\n                (len(self.state_size), state))\n          cur_state = state[i]\n        else:\n          cur_state = array_ops.slice(state, [0, cur_state_pos],\n                                      [-1, cell.state_size])\n          cur_state_pos += cell.state_size\n        cur_inp, new_state = cell(cur_inp, cur_state)\n        new_states.append(new_state)\n\n    new_states = (tuple(new_states) if self._state_is_tuple else\n                  array_ops.concat(new_states, 1))\n\n    return cur_inp, new_states\n\n\nclass _SlimRNNCell(RNNCell, checkpointable.NotCheckpointable):\n  """"""A simple wrapper for slim.rnn_cells.""""""\n\n  def __init__(self, cell_fn):\n    """"""Create a SlimRNNCell from a cell_fn.\n    Args:\n      cell_fn: a function which takes (inputs, state, scope) and produces the\n        outputs and the new_state. Additionally when called with inputs=None and\n        state=None it should return (initial_outputs, initial_state).\n    Raises:\n      TypeError: if cell_fn is not callable\n      ValueError: if cell_fn cannot produce a valid initial state.\n    """"""\n    if not callable(cell_fn):\n      raise TypeError(""cell_fn %s needs to be callable"", cell_fn)\n    self._cell_fn = cell_fn\n    self._cell_name = cell_fn.func.__name__\n    init_output, init_state = self._cell_fn(None, None)\n    output_shape = init_output.get_shape()\n    state_shape = init_state.get_shape()\n    self._output_size = output_shape.with_rank(2)[1].value\n    self._state_size = state_shape.with_rank(2)[1].value\n    if self._output_size is None:\n      raise ValueError(""Initial output created by %s has invalid shape %s"" %\n                       (self._cell_name, output_shape))\n    if self._state_size is None:\n      raise ValueError(""Initial state created by %s has invalid shape %s"" %\n                       (self._cell_name, state_shape))\n\n  @property\n  def state_size(self):\n    return self._state_size\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  def __call__(self, inputs, state, scope=None):\n    scope = scope or self._cell_name\n    output, state = self._cell_fn(inputs, state, scope=scope)\n    return output, state\n'"
code/11_rnn_models/11.2_ptb_word_lm.py,74,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Example / benchmark for building a PTB LSTM model.\nTrains the model described in:\n(Zaremba, et. al.) Recurrent Neural Network Regularization\nhttp://arxiv.org/abs/1409.2329\nThere are 3 supported model configurations:\n===========================================\n| config | epochs | train | valid  | test\n===========================================\n| small  | 13     | 37.99 | 121.39 | 115.91\n| medium | 39     | 48.45 |  86.16 |  82.07\n| large  | 55     | 37.87 |  82.62 |  78.29\nThe exact results may vary depending on the random initialization.\nThe hyperparameters used in the model:\n- init_scale - the initial scale of the weights\n- learning_rate - the initial value of the learning rate\n- max_grad_norm - the maximum permissible norm of the gradient\n- num_layers - the number of LSTM layers\n- num_steps - the number of unrolled steps of LSTM\n- hidden_size - the number of LSTM units\n- max_epoch - the number of epochs trained with the initial learning rate\n- max_max_epoch - the total number of epochs for training\n- keep_prob - the probability of keeping weights in the dropout layer\n- lr_decay - the decay of the learning rate for each epoch after ""max_epoch""\n- batch_size - the batch size\n- rnn_mode - the low level implementation of lstm cell: one of CUDNN,\n             BASIC, or BLOCK, representing cudnn_lstm, basic_lstm, and\n             lstm_block_cell classes.\nThe data required for this example is in the data/ dir of the\nPTB dataset from Tomas Mikolov\'s webpage:\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\nTo run:\n$ python ptb_word_lm.py --data_path=simple-examples/data/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport reader\nimport util\n\nfrom tensorflow.python.client import device_lib\n\nflags = tf.flags\nlogging = tf.logging\n\nflags.DEFINE_string(\n    ""model"", ""small"",\n    ""A type of model. Possible options are: small, medium, large."")\nflags.DEFINE_string(""data_path"", None,\n                    ""Where the training/test data is stored."")\nflags.DEFINE_string(""save_path"", None,\n                    ""Model output directory."")\nflags.DEFINE_bool(""use_fp16"", False,\n                  ""Train using 16-bit floats instead of 32bit floats"")\nflags.DEFINE_integer(""num_gpus"", 1,\n                     ""If larger than 1, Grappler AutoParallel optimizer ""\n                     ""will create multiple training replicas with each GPU ""\n                     ""running one replica."")\nflags.DEFINE_string(""rnn_mode"", None,\n                    ""The low level implementation of lstm cell: one of CUDNN, ""\n                    ""BASIC, and BLOCK, representing cudnn_lstm, basic_lstm, ""\n                    ""and lstm_block_cell classes."")\nFLAGS = flags.FLAGS\nBASIC = ""basic""\nCUDNN = ""cudnn""\nBLOCK = ""block""\n\n\ndef data_type():\n  return tf.float16 if FLAGS.use_fp16 else tf.float32\n\n\nclass PTBInput(object):\n  """"""The input data.""""""\n\n  def __init__(self, config, data, name=None):\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n    self.input_data, self.targets = reader.ptb_producer(\n        data, batch_size, num_steps, name=name)\n\n\nclass PTBModel(object):\n  """"""The PTB model.""""""\n\n  def __init__(self, is_training, config, input_):\n    self._is_training = is_training # \xe5\x88\xa4\xe6\x96\xad\xe8\xaf\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe8\xa2\xab\xe8\xae\xad\xe7\xbb\x83\n    self._input = input_ # input_\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa PTBInput \xe5\xaf\xb9\xe8\xb1\xa1,\xe7\x94\xa8\xe4\xba\x8e\xe8\xa1\xa8\xe7\xa4\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\n    self._rnn_params = None # \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n    self._cell = None # \xe7\xbb\x84\xe6\x88\x90\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84 RNN \xe5\x8d\x95\xe5\x85\x83\n    self.batch_size = input_.batch_size # \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\n    self.num_steps = input_.num_steps # \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6(\xe4\xbb\xa5\xe5\x8d\x95\xe8\xaf\x8d\xe4\xb8\xba\xe5\x8d\x95\xe4\xbd\x8d)\n    size = config.hidden_size # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82 RNN \xe5\x8d\x95\xe5\x85\x83\xe4\xb8\xad\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f,\xe5\x8d\xb3\xe6\xaf\x8f\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe5\x8d\x95\xe8\xaf\x8d\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe9\x95\xbf\n    vocab_size = config.vocab_size # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe5\x8d\x95\xe8\xaf\x8d\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n    # \xe5\x9c\xa8 CPU \xe4\xb8\x8a\xe5\xae\x8c\xe6\x88\x90\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe6\x9f\xa5\xe8\xaf\xa2\n    with tf.device(""/cpu:0""):\n      # embedding \xe8\xa1\xa8\xe7\xa4\xba\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x9f\xa9\xe9\x98\xb5\xe3\x80\x82\xe8\xaf\xa5\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe8\xa1\x8c\xe6\x95\xb0\xe5\x92\x8c\xe5\x88\x97\xe6\x95\xb0\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba vocab_size \xe5\x92\x8c size,\n      # \xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\n      embedding = tf.get_variable(\n          ""embedding"", [vocab_size, size], dtype=data_type())\n      # \xe9\x80\x9a\xe8\xbf\x87 input_.input_data \xe4\xb8\xad\xe7\x9a\x84\xe5\x8d\x95\xe8\xaf\x8d ID,\xe5\x9c\xa8\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x9f\xa9\xe9\x98\xb5 embedding \xe4\xb8\xad\xe6\x9f\xa5\xe8\xaf\xa2\xe5\x8d\x95\xe8\xaf\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\n      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xbd\x93\xe5\x89\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xa4\x84\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xe6\x80\x81,\xe5\xb9\xb6\xe4\xb8\x94 config \xe5\xaf\xb9\xe8\xb1\xa1\xe4\xb8\xad\xe8\xae\xbe\xe7\xbd\xae\xe4\xba\x86 Dropout \xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0 keep_prob(\xe8\xaf\xa5\xe5\x8f\x82\xe6\x95\xb0\xe5\x80\xbc\xe5\xb0\x8f\xe4\xba\x8e 1), \n    # \xe5\x88\x99\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae(\xe5\xb7\xb2\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f)\xe5\x9c\xa8\xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb9\x8b\xe5\x89\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x85\x88\xe7\xbb\x8f\xe8\xbf\x87 Dropout \xe5\xb1\x82\xe3\x80\x82\n    # \xe9\x87\x87\xe7\x94\xa8 Dropout \xe5\xb1\x82\xe7\x9a\x84\xe5\xa5\xbd\xe5\xa4\x84\xe5\x9c\xa8\xe4\xba\x8e\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa8\xe4\xb8\x80\xe5\xae\x9a\xe7\xa8\x8b\xe5\xba\xa6\xe4\xb8\x8a\xe5\x87\x8f\xe7\xbc\x93\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe9\xa3\x8e\xe9\x99\xa9\n    if is_training and config.keep_prob < 1:\n      inputs = tf.nn.dropout(inputs, config.keep_prob)\n\n    output, state = self._build_rnn_graph(inputs, config, is_training)\n    # \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x8f\xe8\xbf\x87 LSTM \xe6\xa8\xa1\xe5\x9e\x8b\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xe8\xbf\x9b\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe3\x80\x82\xe5\x9c\xa8\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe4\xb8\xad,\n    # \xe7\x9f\xa9\xe9\x98\xb5 softmax_w \xe7\x9a\x84\xe8\xbd\xac\xe7\xbd\xae\xe4\xb8\x8e\xe6\xaf\x8f\xe4\xb8\xaa LSTM \xe5\x8d\x95\xe5\x85\x83\xe8\xbe\x93\xe5\x87\xba\xe5\x90\x91\xe9\x87\x8f\xe7\x9b\xb8\xe4\xb9\x98\xe5\x90\x8e,\xe5\x86\x8d\xe4\xb8\x8e softmax_b \xe7\x9b\xb8\xe5\x8a\xa0,\xe5\xbe\x97\xe5\x88\xb0 logits\xe3\x80\x82\n    # logits \xe7\x94\xa8\xe4\xba\x8e\xe8\xa1\xa8\xe7\xa4\xba\xe8\xbe\x93\xe5\x87\xba\xe5\x8d\x95\xe8\xaf\x8d\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe8\xaf\x8d\xe5\xba\x93\xe4\xb8\xad\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\n    softmax_w = tf.get_variable(\n        ""softmax_w"", [size, vocab_size], dtype=data_type())\n    softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=data_type())\n    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n    # \xe4\xb8\xba\xe4\xba\x86\xe6\xbb\xa1\xe8\xb6\xb3 tf.contrib.seq2seq.sequence_loss \xe6\x8e\xa5\xe5\x8f\xa3\xe5\xaf\xb9 logits \xe5\xbd\xa2\xe7\x8a\xb6\xe7\x9a\x84\xe8\xa6\x81\xe6\xb1\x82,\xe5\xb0\x86 logits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe5\x8f\x98\xe4\xb8\xba \n    # [batch_size, num_steps, vocab_size]\n    logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n\n    # \xe9\x80\x9a\xe8\xbf\x87\xe6\xaf\x94\xe8\xbe\x83 logits \xe5\x92\x8c input_.targets,\xe5\xbe\x97\xe5\x88\xb0\xe5\xbd\x93\xe5\x89\x8d\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe5\x9c\xa8 batch_size \xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc loss\n    loss = tf.contrib.seq2seq.sequence_loss(\n        logits,\n        input_.targets,\n        tf.ones([self.batch_size, self.num_steps], dtype=data_type()),\n        average_across_timesteps=False,\n        average_across_batch=True)\n\n    # Update the cost\n    # \xe5\xb0\x86\xe5\xbc\xa0\xe9\x87\x8f loss \xe4\xb8\xad\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\x80\xbc\xe5\xbd\x92\xe7\xba\xa6\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc,\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8 PTBModel \xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe6\x88\x90\xe5\x91\x98\xe5\x8f\x98\xe9\x87\x8f _cost \xe4\xb8\xad\n    self._cost = tf.reduce_sum(loss)\n    self._final_state = state\n\n    if not is_training:\n      return\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    # \xe4\xb8\xba\xe4\xba\x86\xe9\x98\xb2\xe6\xad\xa2\xe6\xa2\xaf\xe5\xba\xa6\xe7\x88\x86\xe7\x82\xb8,\xe9\x9c\x80\xe8\xa6\x81\xe6\xa0\xb9\xe6\x8d\xae config.max_grad_norm \xe8\xae\xbe\xe5\xae\x9a\xe7\x9a\x84\xe4\xb8\x8a\xe9\x99\x90\xe5\xaf\xb9\xe6\xa2\xaf\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa3\x81\xe5\x89\xaa\n    grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars),\n                                      config.max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n    self._train_op = optimizer.apply_gradients(\n        zip(grads, tvars),\n        global_step=tf.train.get_or_create_global_step())\n    # \xe5\x85\x81\xe8\xae\xb8\xe5\xad\xa6\xe4\xb9\xa0\xe9\x80\x9f\xe7\x8e\x87\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe8\xa2\xab\xe6\x9b\xb4\xe6\x96\xb0\n    self._new_lr = tf.placeholder(\n        tf.float32, shape=[], name=""new_learning_rate"")\n    self._lr_update = tf.assign(self._lr, self._new_lr)\n\n  def _build_rnn_graph(self, inputs, config, is_training):\n    if config.rnn_mode == CUDNN:\n      return self._build_rnn_graph_cudnn(inputs, config, is_training)\n    else:\n      return self._build_rnn_graph_lstm(inputs, config, is_training)\n\n  def _build_rnn_graph_cudnn(self, inputs, config, is_training):\n    """"""Build the inference graph using CUDNN cell.""""""\n    inputs = tf.transpose(inputs, [1, 0, 2])\n    self._cell = tf.contrib.cudnn_rnn.CudnnLSTM(\n        num_layers=config.num_layers,\n        num_units=config.hidden_size,\n        input_size=config.hidden_size,\n        dropout=1 - config.keep_prob if is_training else 0)\n    params_size_t = self._cell.params_size()\n    self._rnn_params = tf.get_variable(\n        ""lstm_params"",\n        initializer=tf.random_uniform(\n            [params_size_t], -config.init_scale, config.init_scale),\n        validate_shape=False)\n    c = tf.zeros([config.num_layers, self.batch_size, config.hidden_size],\n                 tf.float32)\n    h = tf.zeros([config.num_layers, self.batch_size, config.hidden_size],\n                 tf.float32)\n    self._initial_state = (tf.contrib.rnn.LSTMStateTuple(h=h, c=c),)\n    outputs, h, c = self._cell(inputs, h, c, self._rnn_params, is_training)\n    outputs = tf.transpose(outputs, [1, 0, 2])\n    outputs = tf.reshape(outputs, [-1, config.hidden_size])\n    return outputs, (tf.contrib.rnn.LSTMStateTuple(h=h, c=c),)\n\n  def _get_lstm_cell(self, config, is_training):\n    if config.rnn_mode == BASIC:\n      return tf.contrib.rnn.BasicLSTMCell(\n          config.hidden_size, forget_bias=0.0, state_is_tuple=True,\n          reuse=not is_training)\n    if config.rnn_mode == BLOCK:\n      return tf.contrib.rnn.LSTMBlockCell(\n          config.hidden_size, forget_bias=0.0)\n    raise ValueError(""rnn_mode %s not supported"" % config.rnn_mode)\n\n  def _build_rnn_graph_lstm(self, inputs, config, is_training):\n    """"""Build the inference graph using canonical LSTM cells.""""""\n    # Slightly better results can be obtained with forget gate biases\n    # initialized to 1 but the hyperparameters of the model would need to be\n    # different than reported in the paper.\n    def make_cell():\n      cell = self._get_lstm_cell(config, is_training)\n      if is_training and config.keep_prob < 1:\n        cell = tf.contrib.rnn.DropoutWrapper(\n            cell, output_keep_prob=config.keep_prob)\n      return cell\n\n    cell = tf.contrib.rnn.MultiRNNCell(\n        [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n\n    self._initial_state = cell.zero_state(config.batch_size, data_type())\n    state = self._initial_state\n    # Simplified version of tf.nn.static_rnn().\n    # This builds an unrolled LSTM for tutorial purposes only.\n    # In general, use tf.nn.static_rnn() or tf.nn.static_state_saving_rnn().\n    #\n    # The alternative version of the code below is:\n    #\n    # inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n    # outputs, state = tf.nn.static_rnn(cell, inputs,\n    #                                   initial_state=self._initial_state)\n    outputs = []\n    with tf.variable_scope(""RNN""):\n      for time_step in range(self.num_steps):\n        if time_step > 0: tf.get_variable_scope().reuse_variables()\n        (cell_output, state) = cell(inputs[:, time_step, :], state)\n        outputs.append(cell_output)\n    output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n    return output, state\n\n  def assign_lr(self, session, lr_value):\n    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n\n  def export_ops(self, name):\n    """"""Exports ops to collections.""""""\n    self._name = name\n    ops = {util.with_prefix(self._name, ""cost""): self._cost}\n    if self._is_training:\n      ops.update(lr=self._lr, new_lr=self._new_lr, lr_update=self._lr_update)\n      if self._rnn_params:\n        ops.update(rnn_params=self._rnn_params)\n    for name, op in ops.items():\n      tf.add_to_collection(name, op)\n    self._initial_state_name = util.with_prefix(self._name, ""initial"")\n    self._final_state_name = util.with_prefix(self._name, ""final"")\n    util.export_state_tuples(self._initial_state, self._initial_state_name)\n    util.export_state_tuples(self._final_state, self._final_state_name)\n\n  def import_ops(self):\n    """"""Imports ops from collections.""""""\n    if self._is_training:\n      self._train_op = tf.get_collection_ref(""train_op"")[0]\n      self._lr = tf.get_collection_ref(""lr"")[0]\n      self._new_lr = tf.get_collection_ref(""new_lr"")[0]\n      self._lr_update = tf.get_collection_ref(""lr_update"")[0]\n      rnn_params = tf.get_collection_ref(""rnn_params"")\n      if self._cell and rnn_params:\n        params_saveable = tf.contrib.cudnn_rnn.RNNParamsSaveable(\n            self._cell,\n            self._cell.params_to_canonical,\n            self._cell.canonical_to_params,\n            rnn_params,\n            base_variable_scope=""Model/RNN"")\n        tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, params_saveable)\n    self._cost = tf.get_collection_ref(util.with_prefix(self._name, ""cost""))[0]\n    num_replicas = FLAGS.num_gpus if self._name == ""Train"" else 1\n    self._initial_state = util.import_state_tuples(\n        self._initial_state, self._initial_state_name, num_replicas)\n    self._final_state = util.import_state_tuples(\n        self._final_state, self._final_state_name, num_replicas)\n\n  @property\n  def input(self):\n    return self._input\n\n  @property\n  def initial_state(self):\n    return self._initial_state\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n  @property\n  def initial_state_name(self):\n    return self._initial_state_name\n\n  @property\n  def final_state_name(self):\n    return self._final_state_name\n\n\nclass SmallConfig(object):\n  """"""Small config.""""""\n  init_scale = 0.1\n  learning_rate = 1.0\n  max_grad_norm = 5\n  num_layers = 2\n  num_steps = 20\n  hidden_size = 200\n  max_epoch = 4\n  max_max_epoch = 13\n  keep_prob = 1.0\n  lr_decay = 0.5\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\nclass MediumConfig(object):\n  """"""Medium config.""""""\n  init_scale = 0.05\n  learning_rate = 1.0\n  max_grad_norm = 5\n  num_layers = 2\n  num_steps = 35\n  hidden_size = 650\n  max_epoch = 6\n  max_max_epoch = 39\n  keep_prob = 0.5\n  lr_decay = 0.8\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\nclass LargeConfig(object):\n  """"""Large config.""""""\n  init_scale = 0.04\n  learning_rate = 1.0\n  max_grad_norm = 10\n  num_layers = 2\n  num_steps = 35\n  hidden_size = 1500\n  max_epoch = 14\n  max_max_epoch = 55\n  keep_prob = 0.35\n  lr_decay = 1 / 1.15\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\nclass TestConfig(object):\n  """"""Tiny config, for testing.""""""\n  init_scale = 0.1\n  learning_rate = 1.0\n  max_grad_norm = 1\n  num_layers = 1\n  num_steps = 2\n  hidden_size = 2\n  max_epoch = 1\n  max_max_epoch = 1\n  keep_prob = 1.0\n  lr_decay = 0.5\n  batch_size = 20\n  vocab_size = 10000\n  rnn_mode = BLOCK\n\n\ndef run_epoch(session, model, eval_op=None, verbose=False):\n  """"""Runs the model on the given data.""""""\n  start_time = time.time()\n  costs = 0.0\n  iters = 0\n  state = session.run(model.initial_state)\n\n  fetches = {\n      ""cost"": model.cost,\n      ""final_state"": model.final_state,\n  }\n  if eval_op is not None:\n    fetches[""eval_op""] = eval_op\n\n  for step in range(model.input.epoch_size):\n    feed_dict = {}\n    for i, (c, h) in enumerate(model.initial_state):\n      feed_dict[c] = state[i].c\n      feed_dict[h] = state[i].h\n\n    vals = session.run(fetches, feed_dict)\n    cost = vals[""cost""]\n    state = vals[""final_state""]\n\n    costs += cost\n    iters += model.input.num_steps\n\n    if verbose and step % (model.input.epoch_size // 10) == 10:\n      print(""%.3f perplexity: %.3f speed: %.0f wps"" %\n            (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),\n             iters * model.input.batch_size * max(1, FLAGS.num_gpus) /\n             (time.time() - start_time)))\n\n  return np.exp(costs / iters)\n\n\ndef get_config():\n  """"""Get model config.""""""\n  config = None\n  if FLAGS.model == ""small"":\n    config = SmallConfig()\n  elif FLAGS.model == ""medium"":\n    config = MediumConfig()\n  elif FLAGS.model == ""large"":\n    config = LargeConfig()\n  elif FLAGS.model == ""test"":\n    config = TestConfig()\n  else:\n    raise ValueError(""Invalid model: %s"", FLAGS.model)\n  if FLAGS.rnn_mode:\n    config.rnn_mode = FLAGS.rnn_mode\n  if FLAGS.num_gpus != 1 or tf.__version__ < ""1.3.0"" :\n    config.rnn_mode = BASIC\n  return config\n\n\ndef main(_):\n  if not FLAGS.data_path:\n    raise ValueError(""Must set --data_path to PTB data directory"")\n  gpus = [\n      x.name for x in device_lib.list_local_devices() if x.device_type == ""GPU""\n  ]\n  if FLAGS.num_gpus > len(gpus):\n    raise ValueError(\n        ""Your machine has only %d gpus ""\n        ""which is less than the requested --num_gpus=%d.""\n        % (len(gpus), FLAGS.num_gpus))\n\n  raw_data = reader.ptb_raw_data(FLAGS.data_path)\n  train_data, valid_data, test_data, _ = raw_data\n\n  config = get_config()\n  eval_config = get_config()\n  eval_config.batch_size = 1\n  eval_config.num_steps = 1\n\n  with tf.Graph().as_default():\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                config.init_scale)\n\n    with tf.name_scope(""Train""):\n      train_input = PTBInput(config=config, data=train_data, name=""TrainInput"")\n      with tf.variable_scope(""Model"", reuse=None, initializer=initializer):\n        m = PTBModel(is_training=True, config=config, input_=train_input)\n      tf.summary.scalar(""Training Loss"", m.cost)\n      tf.summary.scalar(""Learning Rate"", m.lr)\n\n    with tf.name_scope(""Valid""):\n      valid_input = PTBInput(config=config, data=valid_data, name=""ValidInput"")\n      with tf.variable_scope(""Model"", reuse=True, initializer=initializer):\n        mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n      tf.summary.scalar(""Validation Loss"", mvalid.cost)\n\n    with tf.name_scope(""Test""):\n      test_input = PTBInput(\n          config=eval_config, data=test_data, name=""TestInput"")\n      with tf.variable_scope(""Model"", reuse=True, initializer=initializer):\n        mtest = PTBModel(is_training=False, config=eval_config,\n                         input_=test_input)\n\n    models = {""Train"": m, ""Valid"": mvalid, ""Test"": mtest}\n    for name, model in models.items():\n      model.export_ops(name)\n    metagraph = tf.train.export_meta_graph()\n    if tf.__version__ < ""1.1.0"" and FLAGS.num_gpus > 1:\n      raise ValueError(""num_gpus > 1 is not supported for TensorFlow versions ""\n                       ""below 1.1.0"")\n    soft_placement = False\n    if FLAGS.num_gpus > 1:\n      soft_placement = True\n      util.auto_parallel(metagraph, m)\n\n  with tf.Graph().as_default():\n    tf.train.import_meta_graph(metagraph)\n    for model in models.values():\n      model.import_ops()\n    sv = tf.train.Supervisor(logdir=FLAGS.save_path)\n    config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)\n    with sv.managed_session(config=config_proto) as session:\n      for i in range(config.max_max_epoch):\n        lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n        m.assign_lr(session, config.learning_rate * lr_decay)\n\n        print(""Epoch: %d Learning rate: %.3f"" % (i + 1, session.run(m.lr)))\n        train_perplexity = run_epoch(session, m, eval_op=m.train_op,\n                                     verbose=True)\n        print(""Epoch: %d Train Perplexity: %.3f"" % (i + 1, train_perplexity))\n        valid_perplexity = run_epoch(session, mvalid)\n        print(""Epoch: %d Valid Perplexity: %.3f"" % (i + 1, valid_perplexity))\n\n      test_perplexity = run_epoch(session, mtest)\n      print(""Test Perplexity: %.3f"" % test_perplexity)\n\n      if FLAGS.save_path:\n        print(""Saving model to %s."" % FLAGS.save_path)\n        sv.saver.save(session, FLAGS.save_path, global_step=sv.global_step)\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
code/11_rnn_models/11.2_reader.py,13,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Utilities for parsing PTB text files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport sys\n\nimport tensorflow as tf\n\nPy3 = sys.version_info[0] == 3\n\ndef _read_words(filename):\n  with tf.gfile.GFile(filename, ""r"") as f:\n    if Py3:\n      return f.read().replace(""\\n"", ""<eos>"").split()\n    else:\n      return f.read().decode(""utf-8"").replace(""\\n"", ""<eos>"").split()\n\n\ndef _build_vocab(filename):\n  data = _read_words(filename)\n\n  counter = collections.Counter(data)\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n  words, _ = list(zip(*count_pairs))\n  word_to_id = dict(zip(words, range(len(words))))\n\n  return word_to_id\n\n\ndef _file_to_word_ids(filename, word_to_id):\n  data = _read_words(filename)\n  return [word_to_id[word] for word in data if word in word_to_id]\n\n\ndef ptb_raw_data(data_path=None):\n  """"""Load PTB raw data from data directory ""data_path"".\n  Reads PTB text files, converts strings to integer ids,\n  and performs mini-batching of the inputs.\n  The PTB dataset comes from Tomas Mikolov\'s webpage:\n  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n  Returns:\n    tuple (train_data, valid_data, test_data, vocabulary)\n    where each of the data objects can be passed to PTBIterator.\n  """"""\n\n  train_path = os.path.join(data_path, ""ptb.train.txt"") # \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n  valid_path = os.path.join(data_path, ""ptb.valid.txt"") # \xe9\xaa\x8c\xe8\xaf\x81\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n  test_path = os.path.join(data_path, ""ptb.test.txt"") # \xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  vocabulary = len(word_to_id)\n  return train_data, valid_data, test_data, vocabulary\n\n\ndef ptb_producer(raw_data, batch_size, num_steps, name=None):\n  """"""Iterate on the raw PTB data.\n  This chunks up raw_data into batches of examples and returns Tensors that\n  are drawn from these batches.\n  Args:\n    raw_data: one of the raw data outputs from ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n    name: the name of this operation (optional).\n  Returns:\n    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n    of the tuple is the same data time-shifted to the right by one.\n  Raises:\n    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n  """"""\n  with tf.name_scope(name, ""PTBProducer"", [raw_data, batch_size, num_steps]):\n    # \xe5\xb0\x86 ptb_raw_data \xe6\x96\xb9\xe6\xb3\x95\xe8\x8e\xb7\xe5\xbe\x97\xe7\x9a\x84\xe6\x9f\x90\xe4\xb8\x80\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86(train_data\xe3\x80\x81valid_data \xe5\x92\x8c test_data \xe4\xb9\x8b\xe4\xb8\x80) \n    # \xe4\xb8\xad\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\x8d\x95\xe8\xaf\x8d\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 ID \xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba tf.int32 \xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f,\xe5\xb9\xb6\xe4\xbb\x8d\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\xba raw_data\n    raw_data = tf.convert_to_tensor(raw_data, name=""raw_data"", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    # \xe5\xb0\x86 raw_data \xe5\x8f\x98\xe4\xb8\xba\xe4\xba\x8c\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f data,\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6 batch_size \xe8\xa1\xa8\xe7\xa4\xba\xe6\x89\xb9\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f,\n    # \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6 batch_len \xe8\xa1\xa8\xe7\xa4\xba\xe8\xaf\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84\xe6\x89\xb9\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n    # num_steps \xe8\xa1\xa8\xe7\xa4\xba\xe6\x97\xb6\xe9\x97\xb4\xe5\xba\x8f\xe5\x88\x97\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a LSTM \xe5\x8d\x95\xe5\x85\x83\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0,\xe5\x8d\xb3 LSTM \xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x8d\x95\xe8\xaf\x8d\xe6\x95\xb0\xe9\x87\x8f;\n    # epoch_size \xe8\xa1\xa8\xe7\xa4\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84\xe6\x89\xb9\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=""epoch_size == 0, decrease batch_size or num_steps"")\n    with tf.control_dependencies([assertion]):\n      epoch_size = tf.identity(epoch_size, name=""epoch_size"")\n    # \xe9\xa1\xba\xe5\xba\x8f\xe4\xba\xa7\xe7\x94\x9f\xe4\xbb\x8e 0 \xe5\x88\xb0(epoch_size-1)\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7 i\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    # \xe5\xbc\xa0\xe9\x87\x8f x \xe5\x92\x8c y \xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x9d\x87\xe4\xb8\xba[batch_size, num_steps],\xe9\x80\x9a\xe8\xbf\x87 i \xe7\x9a\x84\xe5\x8f\x98\xe5\x8c\x96\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x81\x8d\xe5\x8e\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe7\x9a\x84\xe5\x85\xa8\xe9\x83\xa8\xe5\x8d\x95\xe8\xaf\x8d\n    x = tf.strided_slice(data, [0, i * num_steps],\n                         [batch_size, (i + 1) * num_steps])\n    x.set_shape([batch_size, num_steps])\n    y = tf.strided_slice(data, [0, i * num_steps + 1],\n                         [batch_size, (i + 1) * num_steps + 1])\n    y.set_shape([batch_size, num_steps])\n    return x, y'"
code/11_rnn_models/11.2_seq2seq.py,18,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Library for creating sequence-to-sequence models in TensorFlow.\nSequence-to-sequence recurrent neural networks can learn complex functions\nthat map input sequences to output sequences. These models yield very good\nresults on a number of tasks, such as speech recognition, parsing, machine\ntranslation, or even constructing automated replies to emails.\nBefore using this module, it is recommended to read the TensorFlow tutorial\non sequence-to-sequence models. It explains the basic concepts of this module\nand shows an end-to-end example of how to build a translation model.\n  https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html\nHere is an overview of functions available in this module. They all use\na very similar interface, so after reading the above tutorial and using\none of them, others should be easy to substitute.\n* Full sequence-to-sequence models.\n  - basic_rnn_seq2seq: The most basic RNN-RNN model.\n  - tied_rnn_seq2seq: The basic model with tied encoder and decoder weights.\n  - embedding_rnn_seq2seq: The basic model with input embedding.\n  - embedding_tied_rnn_seq2seq: The tied model with input embedding.\n  - embedding_attention_seq2seq: Advanced model with input embedding and\n      the neural attention mechanism; recommended for complex tasks.\n* Multi-task sequence-to-sequence models.\n  - one2many_rnn_seq2seq: The embedding model with multiple decoders.\n* Decoders (when you write your own encoder, you can use these to decode;\n    e.g., if you want to write a model that generates captions for images).\n  - rnn_decoder: The basic decoder based on a pure RNN.\n  - attention_decoder: A decoder that uses the attention mechanism.\n* Losses.\n  - sequence_loss: Loss for a sequence model returning average log-perplexity.\n  - sequence_loss_by_example: As above, but not averaging over all examples.\n* model_with_buckets: A convenience function to create models with bucketing\n    (see the tutorial above for an explanation of why and how to use it).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\n# We disable pylint because we need python3 compatibility.\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom six.moves import zip  # pylint: disable=redefined-builtin\n\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.util import nest\n\n# TODO(ebrevdo): Remove once _linear is fully deprecated.\nLinear = core_rnn_cell._Linear  # pylint: disable=protected-access,invalid-name\n\n\ndef _extract_argmax_and_embed(embedding,\n                              output_projection=None,\n                              update_embedding=True):\n  """"""Get a loop_function that extracts the previous symbol and embeds it.\n  Args:\n    embedding: embedding tensor for symbols.\n    output_projection: None or a pair (W, B). If provided, each fed previous\n      output will first be multiplied by W and added B.\n    update_embedding: Boolean; if False, the gradients will not propagate\n      through the embeddings.\n  Returns:\n    A loop function.\n  """"""\n\n  def loop_function(prev, _):\n    if output_projection is not None:\n      prev = nn_ops.xw_plus_b(prev, output_projection[0], output_projection[1])\n    prev_symbol = math_ops.argmax(prev, 1)\n    # Note that gradients will not propagate through the second parameter of\n    # embedding_lookup.\n    emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n    if not update_embedding:\n      emb_prev = array_ops.stop_gradient(emb_prev)\n    return emb_prev\n\n  return loop_function\n\n\ndef rnn_decoder(decoder_inputs,\n                initial_state,\n                cell,\n                loop_function=None,\n                scope=None):\n  """"""RNN decoder for the sequence-to-sequence model.\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n    cell: rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to the i-th output\n      in order to generate the i+1-st input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing generated outputs.\n      state: The state of each cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n        (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n         states can be the same. They are different for LSTM cells though.)\n  """"""\n  with variable_scope.variable_scope(scope or ""rnn_decoder""):\n    state = initial_state # state \xe8\xa1\xa8\xe7\xa4\xba\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x8a\xb6\xe6\x80\x81,\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\xba\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x8a\xb6\xe6\x80\x81\n    outputs = [] # outputs \xe8\xa1\xa8\xe7\xa4\xba\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba,\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8e decoder_inputs \xe4\xb8\x80\xe8\x87\xb4\n    prev = None # prev \xe8\xa1\xa8\xe7\xa4\xba\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe4\xb8\xad\xe5\x89\x8d\xe4\xb8\x80\xe6\x97\xb6\xe5\x88\xbb LSTM \xe5\x8d\x95\xe5\x85\x83\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n    for i, inp in enumerate(decoder_inputs):\n      # \xe5\xa6\x82\xe6\x9e\x9c\xe9\x87\x87\xe7\x94\xa8 loop_function,\xe9\x82\xa3\xe4\xb9\x88\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe4\xb8\xad\xe5\x89\x8d\xe4\xb8\x80\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbc\x9a\xe4\xbd\x9c\xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8d\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe3\x80\x82 \n      # \xe8\xbf\x99\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe5\xb8\xb8\xe8\xa7\x81\xe4\xba\x8e Seq2Seq \xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x8e\xa8\xe7\x90\x86\xe6\x80\x81(\xe6\x88\x96\xe8\x80\x85\xe6\x9f\x90\xe4\xba\x9b\xe8\xae\xad\xe7\xbb\x83\xe6\x80\x81,\xe5\xa6\x82\xe5\x89\x8d\xe9\x9d\xa2\xe6\x8f\x90\xe5\x88\xb0\xe7\x9a\x84 Samy Bengio \xe7\xad\x89\xe4\xba\xba\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95)\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      if i > 0:\n        # \xe4\xb8\x8d\xe5\x90\x8c\xe6\x97\xb6\xe5\x88\xbb\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 LSTM \xe5\x8d\x95\xe5\x85\x83\xe5\x86\x85\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe5\x85\xb1\xe4\xba\xab\xe7\x9a\x84\n        variable_scope.get_variable_scope().reuse_variables()\n      output, state = cell(inp, state)\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state\n\n\ndef basic_rnn_seq2seq(encoder_inputs,\n                      decoder_inputs,\n                      cell,\n                      dtype=dtypes.float32,\n                      scope=None):\n  """"""Basic RNN sequence-to-sequence model.\n  This model first runs an RNN to encode encoder_inputs into a state vector,\n  then runs decoder, initialized with the last encoder state, on decoder_inputs.\n  Encoder and decoder use the same RNN cell type, but don\'t share parameters.\n  Args:\n    encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    dtype: The dtype of the initial state of the RNN cell (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""basic_rnn_seq2seq"".\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell in the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(scope or ""basic_rnn_seq2seq""):\n    # enc_cell \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xb8\xba LSTM\xe3\x80\x81GRU \xe6\x88\x96\xe8\x80\x85\xe5\x85\xb6\xe4\xbb\x96 RNN \xe5\x8d\x95\xe5\x85\x83\n    enc_cell = copy.deepcopy(cell)\n    _, enc_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n    return rnn_decoder(decoder_inputs, enc_state, cell)\n\n\ndef tied_rnn_seq2seq(encoder_inputs,\n                     decoder_inputs,\n                     cell,\n                     loop_function=None,\n                     dtype=dtypes.float32,\n                     scope=None):\n  """"""RNN sequence-to-sequence model with tied encoder and decoder parameters.\n  This model first runs an RNN to encode encoder_inputs into a state vector, and\n  then runs decoder, initialized with the last encoder state, on decoder_inputs.\n  Encoder and decoder use the same RNN cell and share parameters.\n  Args:\n    encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol), see rnn_decoder for details.\n    dtype: The dtype of the initial state of the rnn cell (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""tied_rnn_seq2seq"".\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(""combined_tied_rnn_seq2seq""):\n    scope = scope or ""tied_rnn_seq2seq""\n    _, enc_state = rnn.static_rnn(\n        cell, encoder_inputs, dtype=dtype, scope=scope)\n    variable_scope.get_variable_scope().reuse_variables()\n    return rnn_decoder(\n        decoder_inputs,\n        enc_state,\n        cell,\n        loop_function=loop_function,\n        scope=scope)\n\n\ndef embedding_rnn_decoder(decoder_inputs,\n                          initial_state,\n                          cell,\n                          num_symbols,\n                          embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          update_embedding_for_previous=True,\n                          scope=None):\n  """"""RNN decoder with embedding and a pure-decoding option.\n  Args:\n    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function.\n    num_symbols: Integer, how many symbols come into the embedding.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has\n      shape [num_symbols]; if provided and feed_previous=True, each fed\n      previous output will first be multiplied by W and added B.\n    feed_previous: Boolean; if True, only the first of decoder_inputs will be\n      used (the ""GO"" symbol), and all other decoder inputs will be generated by:\n        next = embedding_lookup(embedding, argmax(previous_output)),\n      In effect, this implements a greedy decoder. It can also be used\n      during training to emulate http://arxiv.org/abs/1506.03099.\n      If False, decoder_inputs are used as given (the standard decoder case).\n    update_embedding_for_previous: Boolean; if False and feed_previous=True,\n      only the embedding for the first symbol of decoder_inputs (the ""GO""\n      symbol) will be updated by back propagation. Embeddings for the symbols\n      generated from the decoder itself remain unchanged. This parameter has\n      no effect if feed_previous=False.\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_rnn_decoder"".\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors. The\n        output is of shape [batch_size x cell.output_size] when\n        output_projection is not None (and represents the dense representation\n        of predicted tokens). It is of shape [batch_size x num_decoder_symbols]\n        when output_projection is None.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  with variable_scope.variable_scope(scope or ""embedding_rnn_decoder"") as scope:\n    if output_projection is not None:\n      dtype = scope.dtype\n      proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)\n      proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n      proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n      proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    embedding = variable_scope.get_variable(""embedding"",\n                                            [num_symbols, embedding_size])\n    loop_function = _extract_argmax_and_embed(\n        embedding, output_projection,\n        update_embedding_for_previous) if feed_previous else None\n    emb_inp = (embedding_ops.embedding_lookup(embedding, i)\n               for i in decoder_inputs)\n    return rnn_decoder(\n        emb_inp, initial_state, cell, loop_function=loop_function)\n\n\ndef embedding_rnn_seq2seq(encoder_inputs,\n                          decoder_inputs,\n                          cell,\n                          num_encoder_symbols,\n                          num_decoder_symbols,\n                          embedding_size,\n                          output_projection=None,\n                          feed_previous=False,\n                          dtype=None,\n                          scope=None):\n  """"""Embedding RNN sequence-to-sequence model.\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n  embedded encoder_inputs into a state vector. Next, it embeds decoder_inputs\n  by another newly created embedding (of shape [num_decoder_symbols x\n  input_size]). Then it runs RNN decoder, initialized with the last\n  encoder state, on embedded decoder_inputs.\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_decoder_symbols] and B has\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial state for both the encoder and encoder\n      rnn cells (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_rnn_seq2seq""\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors. The\n        output is of shape [batch_size x cell.output_size] when\n        output_projection is not None (and represents the dense representation\n        of predicted tokens). It is of shape [batch_size x num_decoder_symbols]\n        when output_projection is None.\n      state: The state of each decoder cell in each time-step. This is a list\n        with length len(decoder_inputs) -- one item for each time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(scope or ""embedding_rnn_seq2seq"") as scope:\n    if dtype is not None:\n      scope.set_dtype(dtype)\n    else:\n      dtype = scope.dtype\n\n    # Encoder.\n    encoder_cell = copy.deepcopy(cell)\n    encoder_cell = core_rnn_cell.EmbeddingWrapper(\n        encoder_cell,\n        embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.static_rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    if output_projection is None:\n      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n\n    if isinstance(feed_previous, bool):\n      return embedding_rnn_decoder(\n          decoder_inputs,\n          encoder_state,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse):\n        outputs, state = embedding_rnn_decoder(\n            decoder_inputs,\n            encoder_state,\n            cell,\n            num_decoder_symbols,\n            embedding_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(\n          structure=encoder_state, flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\ndef embedding_tied_rnn_seq2seq(encoder_inputs,\n                               decoder_inputs,\n                               cell,\n                               num_symbols,\n                               embedding_size,\n                               num_decoder_symbols=None,\n                               output_projection=None,\n                               feed_previous=False,\n                               dtype=None,\n                               scope=None):\n  """"""Embedding RNN sequence-to-sequence model with tied (shared) parameters.\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_symbols x input_size]). Then it runs an RNN to encode embedded\n  encoder_inputs into a state vector. Next, it embeds decoder_inputs using\n  the same embedding. Then it runs RNN decoder, initialized with the last\n  encoder state, on embedded decoder_inputs. The decoder output is over symbols\n  from 0 to num_decoder_symbols - 1 if num_decoder_symbols is none; otherwise it\n  is over 0 to num_symbols - 1.\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    num_symbols: Integer; number of symbols for both encoder and decoder.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_decoder_symbols: Integer; number of output symbols for decoder. If\n      provided, the decoder output is over symbols 0 to num_decoder_symbols - 1.\n      Otherwise, decoder output is over symbols 0 to num_symbols - 1. Note that\n      this assumes that the vocabulary is set up such that the first\n      num_decoder_symbols of num_symbols are part of decoding.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has\n      shape [num_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype to use for the initial RNN states (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_tied_rnn_seq2seq"".\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_symbols] containing the generated\n        outputs where output_symbols = num_decoder_symbols if\n        num_decoder_symbols is not None otherwise output_symbols = num_symbols.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  with variable_scope.variable_scope(\n      scope or ""embedding_tied_rnn_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    if output_projection is not None:\n      proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)\n      proj_weights.get_shape().assert_is_compatible_with([None, num_symbols])\n      proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n      proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    embedding = variable_scope.get_variable(\n        ""embedding"", [num_symbols, embedding_size], dtype=dtype)\n\n    emb_encoder_inputs = [\n        embedding_ops.embedding_lookup(embedding, x) for x in encoder_inputs\n    ]\n    emb_decoder_inputs = [\n        embedding_ops.embedding_lookup(embedding, x) for x in decoder_inputs\n    ]\n\n    output_symbols = num_symbols\n    if num_decoder_symbols is not None:\n      output_symbols = num_decoder_symbols\n    if output_projection is None:\n      cell = core_rnn_cell.OutputProjectionWrapper(cell, output_symbols)\n\n    if isinstance(feed_previous, bool):\n      loop_function = _extract_argmax_and_embed(embedding, output_projection,\n                                                True) if feed_previous else None\n      return tied_rnn_seq2seq(\n          emb_encoder_inputs,\n          emb_decoder_inputs,\n          cell,\n          loop_function=loop_function,\n          dtype=dtype)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      loop_function = _extract_argmax_and_embed(\n          embedding, output_projection, False) if feed_previous_bool else None\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse):\n        outputs, state = tied_rnn_seq2seq(\n            emb_encoder_inputs,\n            emb_decoder_inputs,\n            cell,\n            loop_function=loop_function,\n            dtype=dtype)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    # Calculate zero-state to know it\'s structure.\n    static_batch_size = encoder_inputs[0].get_shape()[0]\n    for inp in encoder_inputs[1:]:\n      static_batch_size.merge_with(inp.get_shape()[0])\n    batch_size = static_batch_size.value\n    if batch_size is None:\n      batch_size = array_ops.shape(encoder_inputs[0])[0]\n    zero_state = cell.zero_state(batch_size, dtype)\n    if nest.is_sequence(zero_state):\n      state = nest.pack_sequence_as(\n          structure=zero_state, flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\ndef attention_decoder(decoder_inputs,\n                      initial_state,\n                      attention_states,\n                      cell,\n                      output_size=None,\n                      num_heads=1,\n                      loop_function=None,\n                      dtype=None,\n                      scope=None,\n                      initial_state_attention=False):\n  """"""RNN decoder with attention for the sequence-to-sequence model.\n  In this context ""attention"" means that, during decoding, the RNN can look up\n  information in the additional tensor attention_states, and it does this by\n  focusing on a few entries from the tensor. This model has proven to yield\n  especially good results in a number of sequence-to-sequence tasks. This\n  implementation is based on http://arxiv.org/abs/1412.7449 (see below for\n  details). It is recommended for complex sequence-to-sequence tasks.\n  Args:\n    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    output_size: Size of the output vectors; if None, we use cell.output_size.\n    num_heads: Number of attention heads that read from attention_states.\n    loop_function: If not None, this function will be applied to i-th output\n      in order to generate i+1-th input, and decoder_inputs will be ignored,\n      except for the first element (""GO"" symbol). This can be used for decoding,\n      but also for training to emulate http://arxiv.org/abs/1506.03099.\n      Signature -- loop_function(prev, i) = next\n        * prev is a 2D Tensor of shape [batch_size x output_size],\n        * i is an integer, the step number (when advanced control is needed),\n        * next is a 2D Tensor of shape [batch_size x input_size].\n    dtype: The dtype to use for the RNN initial state (default: tf.float32).\n    scope: VariableScope for the created subgraph; default: ""attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors of\n        shape [batch_size x output_size]. These represent the generated outputs.\n        Output i is computed from input i (which is either the i-th element\n        of decoder_inputs or loop_function(output {i-1}, i)) as follows.\n        First, we run the cell on a combination of the input and previous\n        attention masks:\n          cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n        Then, we calculate new attention masks:\n          new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n        and then we calculate the output:\n          output = linear(cell_output, new_attn).\n      state: The state of each decoder cell the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  Raises:\n    ValueError: when num_heads is not positive, there are no inputs, shapes\n      of attention_states are not set, or input size cannot be inferred\n      from the input.\n  """"""\n  if not decoder_inputs:\n    raise ValueError(""Must provide at least 1 input to attention decoder."")\n  if num_heads < 1:\n    raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  if attention_states.get_shape()[2].value is None:\n    raise ValueError(""Shape[2] of attention_states must be known: %s"" %\n                     attention_states.get_shape())\n  if output_size is None:\n    output_size = cell.output_size\n\n  with variable_scope.variable_scope(\n      scope or ""attention_decoder"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n    attn_length = attention_states.get_shape()[1].value\n    if attn_length is None:\n      attn_length = array_ops.shape(attention_states)[1]\n    attn_size = attention_states.get_shape()[2].value\n\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n    hidden = array_ops.reshape(attention_states,\n                               [-1, attn_length, 1, attn_size])\n    hidden_features = []\n    v = []\n    attention_vec_size = attn_size  # Size of query vectors for attention.\n    for a in xrange(num_heads):\n      k = variable_scope.get_variable(""AttnW_%d"" % a,\n                                      [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n      v.append(\n          variable_scope.get_variable(""AttnV_%d"" % a, [attention_vec_size]))\n\n    state = initial_state\n\n    def attention(query):\n      """"""Put attention masks on hidden using hidden_features and query.""""""\n      ds = []  # Results of attention reads will be stored here.\n      if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n        query_list = nest.flatten(query)\n        for q in query_list:  # Check that ndims == 2 if specified.\n          ndims = q.get_shape().ndims\n          if ndims:\n            assert ndims == 2\n        query = array_ops.concat(query_list, 1)\n      for a in xrange(num_heads):\n        with variable_scope.variable_scope(""Attention_%d"" % a):\n          y = Linear(query, attention_vec_size, True)(query)\n          y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n          # Attention mask is a softmax of v^T * tanh(...).\n          s = math_ops.reduce_sum(v[a] * math_ops.tanh(hidden_features[a] + y),\n                                  [2, 3])\n          a = nn_ops.softmax(s)\n          # Now calculate the attention-weighted vector d.\n          d = math_ops.reduce_sum(\n              array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden, [1, 2])\n          ds.append(array_ops.reshape(d, [-1, attn_size]))\n      return ds\n\n    outputs = []\n    prev = None\n    batch_attn_size = array_ops.stack([batch_size, attn_size])\n    attns = [\n        array_ops.zeros(\n            batch_attn_size, dtype=dtype) for _ in xrange(num_heads)\n    ]\n    for a in attns:  # Ensure the second shape of attention vectors is set.\n      a.set_shape([None, attn_size])\n    if initial_state_attention:\n      attns = attention(initial_state)\n    for i, inp in enumerate(decoder_inputs):\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n      # If loop_function is set, we use it instead of decoder_inputs.\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(""loop_function"", reuse=True):\n          inp = loop_function(prev, i)\n      # Merge input and previous attentions into one vector of the right size.\n      input_size = inp.get_shape().with_rank(2)[1]\n      if input_size.value is None:\n        raise ValueError(""Could not infer input size from input: %s"" % inp.name)\n\n      inputs = [inp] + attns\n      x = Linear(inputs, input_size, True)(inputs)\n      # Run the RNN.\n      cell_output, state = cell(x, state)\n      # Run the attention mechanism.\n      if i == 0 and initial_state_attention:\n        with variable_scope.variable_scope(\n            variable_scope.get_variable_scope(), reuse=True):\n          attns = attention(state)\n      else:\n        attns = attention(state)\n\n      with variable_scope.variable_scope(""AttnOutputProjection""):\n        inputs = [cell_output] + attns\n        output = Linear(inputs, output_size, True)(inputs)\n      if loop_function is not None:\n        prev = output\n      outputs.append(output)\n\n  return outputs, state\n\n\ndef embedding_attention_decoder(decoder_inputs,\n                                initial_state,\n                                attention_states,\n                                cell,\n                                num_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_size=None,\n                                output_projection=None,\n                                feed_previous=False,\n                                update_embedding_for_previous=True,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n  """"""RNN decoder with embedding and attention and a pure-decoding option.\n  Args:\n    decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n    initial_state: 2D Tensor [batch_size x cell.state_size].\n    attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function.\n    num_symbols: Integer, how many symbols come into the embedding.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_size: Size of the output vectors; if None, use output_size.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_symbols] and B has shape\n      [num_symbols]; if provided and feed_previous=True, each fed previous\n      output will first be multiplied by W and added B.\n    feed_previous: Boolean; if True, only the first of decoder_inputs will be\n      used (the ""GO"" symbol), and all other decoder inputs will be generated by:\n        next = embedding_lookup(embedding, argmax(previous_output)),\n      In effect, this implements a greedy decoder. It can also be used\n      during training to emulate http://arxiv.org/abs/1506.03099.\n      If False, decoder_inputs are used as given (the standard decoder case).\n    update_embedding_for_previous: Boolean; if False and feed_previous=True,\n      only the embedding for the first symbol of decoder_inputs (the ""GO""\n      symbol) will be updated by back propagation. Embeddings for the symbols\n      generated from the decoder itself remain unchanged. This parameter has\n      no effect if feed_previous=False.\n    dtype: The dtype to use for the RNN initial states (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_attention_decoder"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states -- useful when we wish to resume decoding from a previously\n      stored decoder state and attention states.\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x output_size] containing the generated outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  Raises:\n    ValueError: When output_projection has the wrong shape.\n  """"""\n  if output_size is None:\n    output_size = cell.output_size\n  if output_projection is not None:\n    proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n    proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n  with variable_scope.variable_scope(\n      scope or ""embedding_attention_decoder"", dtype=dtype) as scope:\n\n    embedding = variable_scope.get_variable(""embedding"",\n                                            [num_symbols, embedding_size])\n    loop_function = _extract_argmax_and_embed(\n        embedding, output_projection,\n        update_embedding_for_previous) if feed_previous else None\n    emb_inp = [\n        embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs\n    ]\n    return attention_decoder(\n        emb_inp,\n        initial_state,\n        attention_states,\n        cell,\n        output_size=output_size,\n        num_heads=num_heads,\n        loop_function=loop_function,\n        initial_state_attention=initial_state_attention)\n\n\ndef embedding_attention_seq2seq(encoder_inputs,\n                                decoder_inputs,\n                                cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n  """"""Embedding sequence-to-sequence model with attention.\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n  embedded encoder_inputs into a state vector. It keeps the outputs of this\n  RNN at every step to use for attention later. Next, it embeds decoder_inputs\n  by another newly created embedding (of shape [num_decoder_symbols x\n  input_size]). Then it runs attention decoder, initialized with the last\n  encoder state, on embedded decoder_inputs and attending to encoder outputs.\n  Warning: when output_projection is None, the size of the attention vectors\n  and variables will be made proportional to num_decoder_symbols, can be large.\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_decoder_symbols] and B has\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial RNN state (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""embedding_attention_seq2seq"".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states.\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x num_decoder_symbols] containing the generated\n        outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  """"""\n  with variable_scope.variable_scope(\n      scope or ""embedding_attention_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n    # Encoder.\n    encoder_cell = copy.deepcopy(cell)\n    encoder_cell = core_rnn_cell.EmbeddingWrapper(\n        encoder_cell,\n        embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.static_rnn(\n        encoder_cell, encoder_inputs, dtype=dtype)\n\n    # First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [\n        array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs\n    ]\n    attention_states = array_ops.concat(top_states, 1)\n\n    # Decoder.\n    output_size = None\n    if output_projection is None:\n      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n    if isinstance(feed_previous, bool):\n      return embedding_attention_decoder(\n          decoder_inputs,\n          encoder_state,\n          attention_states,\n          cell,\n          num_decoder_symbols,\n          embedding_size,\n          num_heads=num_heads,\n          output_size=output_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          initial_state_attention=initial_state_attention)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse):\n        outputs, state = embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            cell,\n            num_decoder_symbols,\n            embedding_size,\n            num_heads=num_heads,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False,\n            initial_state_attention=initial_state_attention)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(\n          structure=encoder_state, flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n\n\ndef one2many_rnn_seq2seq(encoder_inputs,\n                         decoder_inputs_dict,\n                         enc_cell,\n                         dec_cells_dict,\n                         num_encoder_symbols,\n                         num_decoder_symbols_dict,\n                         embedding_size,\n                         feed_previous=False,\n                         dtype=None,\n                         scope=None):\n  """"""One-to-many RNN sequence-to-sequence model (multi-task).\n  This is a multi-task sequence-to-sequence model with one encoder and multiple\n  decoders. Reference to multi-task sequence-to-sequence learning can be found\n  here: http://arxiv.org/abs/1511.06114\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs_dict: A dictionary mapping decoder name (string) to\n      the corresponding decoder_inputs; each decoder_inputs is a list of 1D\n      Tensors of shape [batch_size]; num_decoders is defined as\n      len(decoder_inputs_dict).\n    enc_cell: tf.nn.rnn_cell.RNNCell defining the encoder cell function and\n      size.\n    dec_cells_dict: A dictionary mapping encoder name (string) to an\n      instance of tf.nn.rnn_cell.RNNCell.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols_dict: A dictionary mapping decoder name (string) to an\n      integer specifying number of symbols for the corresponding decoder;\n      len(num_decoder_symbols_dict) must be equal to num_decoders.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first of\n      decoder_inputs will be used (the ""GO"" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial state for both the encoder and encoder\n      rnn cells (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      ""one2many_rnn_seq2seq""\n  Returns:\n    A tuple of the form (outputs_dict, state_dict), where:\n      outputs_dict: A mapping from decoder name (string) to a list of the same\n        length as decoder_inputs_dict[name]; each element in the list is a 2D\n        Tensors with shape [batch_size x num_decoder_symbol_list[name]]\n        containing the generated outputs.\n      state_dict: A mapping from decoder name (string) to the final state of the\n        corresponding decoder RNN; it is a 2D Tensor of shape\n        [batch_size x cell.state_size].\n  Raises:\n    TypeError: if enc_cell or any of the dec_cells are not instances of RNNCell.\n    ValueError: if len(dec_cells) != len(decoder_inputs_dict).\n  """"""\n  outputs_dict = {}\n  state_dict = {}\n\n  if not isinstance(enc_cell, rnn_cell_impl.RNNCell):\n    raise TypeError(""enc_cell is not an RNNCell: %s"" % type(enc_cell))\n  if set(dec_cells_dict) != set(decoder_inputs_dict):\n    raise ValueError(""keys of dec_cells_dict != keys of decodre_inputs_dict"")\n  for dec_cell in dec_cells_dict.values():\n    if not isinstance(dec_cell, rnn_cell_impl.RNNCell):\n      raise TypeError(""dec_cell is not an RNNCell: %s"" % type(dec_cell))\n\n  with variable_scope.variable_scope(\n      scope or ""one2many_rnn_seq2seq"", dtype=dtype) as scope:\n    dtype = scope.dtype\n\n    # Encoder.\n    enc_cell = core_rnn_cell.EmbeddingWrapper(\n        enc_cell,\n        embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n\n    # Decoder.\n    for name, decoder_inputs in decoder_inputs_dict.items():\n      num_decoder_symbols = num_decoder_symbols_dict[name]\n      dec_cell = dec_cells_dict[name]\n\n      with variable_scope.variable_scope(""one2many_decoder_"" + str(\n          name)) as scope:\n        dec_cell = core_rnn_cell.OutputProjectionWrapper(\n            dec_cell, num_decoder_symbols)\n        if isinstance(feed_previous, bool):\n          outputs, state = embedding_rnn_decoder(\n              decoder_inputs,\n              encoder_state,\n              dec_cell,\n              num_decoder_symbols,\n              embedding_size,\n              feed_previous=feed_previous)\n        else:\n          # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n          def filled_embedding_rnn_decoder(feed_previous):\n            """"""The current decoder with a fixed feed_previous parameter.""""""\n            # pylint: disable=cell-var-from-loop\n            reuse = None if feed_previous else True\n            vs = variable_scope.get_variable_scope()\n            with variable_scope.variable_scope(vs, reuse=reuse):\n              outputs, state = embedding_rnn_decoder(\n                  decoder_inputs,\n                  encoder_state,\n                  dec_cell,\n                  num_decoder_symbols,\n                  embedding_size,\n                  feed_previous=feed_previous)\n            # pylint: enable=cell-var-from-loop\n            state_list = [state]\n            if nest.is_sequence(state):\n              state_list = nest.flatten(state)\n            return outputs + state_list\n\n          outputs_and_state = control_flow_ops.cond(\n              feed_previous, lambda: filled_embedding_rnn_decoder(True),\n              lambda: filled_embedding_rnn_decoder(False))\n          # Outputs length is the same as for decoder inputs.\n          outputs_len = len(decoder_inputs)\n          outputs = outputs_and_state[:outputs_len]\n          state_list = outputs_and_state[outputs_len:]\n          state = state_list[0]\n          if nest.is_sequence(encoder_state):\n            state = nest.pack_sequence_as(\n                structure=encoder_state, flat_sequence=state_list)\n      outputs_dict[name] = outputs\n      state_dict[name] = state\n\n  return outputs_dict, state_dict\n\n\ndef sequence_loss_by_example(logits,\n                             targets,\n                             weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None,\n                             name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits (per example).\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    softmax_loss_function: Function (labels, logits) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n      **Note that to avoid confusion, it is required for the function to accept\n      named arguments.**\n    name: Optional name for this operation, default: ""sequence_loss_by_example"".\n  Returns:\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  """"""\n  if len(targets) != len(logits) or len(weights) != len(logits):\n    raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n                     ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n  with ops.name_scope(name, ""sequence_loss_by_example"",\n                      logits + targets + weights):\n    log_perp_list = []\n    for logit, target, weight in zip(logits, targets, weights):\n      if softmax_loss_function is None:\n        # TODO(irving,ebrevdo): This reshape is needed because\n        # sequence_loss_by_example is called with scalars sometimes, which\n        # violates our general scalar strictness policy.\n        target = array_ops.reshape(target, [-1])\n        crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n            labels=target, logits=logit)\n      else:\n        crossent = softmax_loss_function(labels=target, logits=logit)\n      log_perp_list.append(crossent * weight)\n    log_perps = math_ops.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = math_ops.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sequence_loss(logits,\n                  targets,\n                  weights,\n                  average_across_timesteps=True,\n                  average_across_batch=True,\n                  softmax_loss_function=None,\n                  name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n  Args:\n    logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    softmax_loss_function: Function (labels, logits) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n      **Note that to avoid confusion, it is required for the function to accept\n      named arguments.**\n    name: Optional name for this operation, defaults to ""sequence_loss"".\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n  Raises:\n    ValueError: If len(logits) is different from len(targets) or len(weights).\n  """"""\n  with ops.name_scope(name, ""sequence_loss"", logits + targets + weights):\n    cost = math_ops.reduce_sum(\n        sequence_loss_by_example(\n            logits,\n            targets,\n            weights,\n            average_across_timesteps=average_across_timesteps,\n            softmax_loss_function=softmax_loss_function))\n    if average_across_batch:\n      batch_size = array_ops.shape(targets[0])[0]\n      return cost / math_ops.cast(batch_size, cost.dtype)\n    else:\n      return cost\n\n\ndef model_with_buckets(encoder_inputs,\n                       decoder_inputs,\n                       targets,\n                       weights,\n                       buckets,\n                       seq2seq,\n                       softmax_loss_function=None,\n                       per_example_loss=False,\n                       name=None):\n  """"""Create a sequence-to-sequence model with support for bucketing.\n  The seq2seq argument is a function that defines a sequence-to-sequence model,\n  e.g., seq2seq = lambda x, y: basic_rnn_seq2seq(\n      x, y, rnn_cell.GRUCell(24))\n  Args:\n    encoder_inputs: A list of Tensors to feed the encoder; first seq2seq input.\n    decoder_inputs: A list of Tensors to feed the decoder; second seq2seq input.\n    targets: A list of 1D batch-sized int32 Tensors (desired output sequence).\n    weights: List of 1D batch-sized float-Tensors to weight the targets.\n    buckets: A list of pairs of (input size, output size) for each bucket.\n    seq2seq: A sequence-to-sequence model function; it takes 2 input that\n      agree with encoder_inputs and decoder_inputs, and returns a pair\n      consisting of outputs and states (as, e.g., basic_rnn_seq2seq).\n    softmax_loss_function: Function (labels, logits) -> loss-batch\n      to be used instead of the standard softmax (the default if this is None).\n      **Note that to avoid confusion, it is required for the function to accept\n      named arguments.**\n    per_example_loss: Boolean. If set, the returned loss will be a batch-sized\n      tensor of losses for each sequence in the batch. If unset, it will be\n      a scalar with the averaged loss from all examples.\n    name: Optional name for this operation, defaults to ""model_with_buckets"".\n  Returns:\n    A tuple of the form (outputs, losses), where:\n      outputs: The outputs for each bucket. Its j\'th element consists of a list\n        of 2D Tensors. The shape of output tensors can be either\n        [batch_size x output_size] or [batch_size x num_decoder_symbols]\n        depending on the seq2seq model used.\n      losses: List of scalar Tensors, representing losses for each bucket, or,\n        if per_example_loss is set, a list of 1D batch-sized float Tensors.\n  Raises:\n    ValueError: If length of encoder_inputs, targets, or weights is smaller\n      than the largest (last) bucket.\n  """"""\n  if len(encoder_inputs) < buckets[-1][0]:\n    raise ValueError(""Length of encoder_inputs (%d) must be at least that of la""\n                     ""st bucket (%d)."" % (len(encoder_inputs), buckets[-1][0]))\n  if len(targets) < buckets[-1][1]:\n    raise ValueError(""Length of targets (%d) must be at least that of last ""\n                     ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n  if len(weights) < buckets[-1][1]:\n    raise ValueError(""Length of weights (%d) must be at least that of last ""\n                     ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n  all_inputs = encoder_inputs + decoder_inputs + targets + weights\n  losses = []\n  outputs = []\n  with ops.name_scope(name, ""model_with_buckets"", all_inputs):\n    for j, bucket in enumerate(buckets):\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=True if j > 0 else None):\n        bucket_outputs, _ = seq2seq(encoder_inputs[:bucket[0]],\n                                    decoder_inputs[:bucket[1]])\n        outputs.append(bucket_outputs)\n        if per_example_loss:\n          losses.append(\n              sequence_loss_by_example(\n                  outputs[-1],\n                  targets[:bucket[1]],\n                  weights[:bucket[1]],\n                  softmax_loss_function=softmax_loss_function))\n        else:\n          losses.append(\n              sequence_loss(\n                  outputs[-1],\n                  targets[:bucket[1]],\n                  weights[:bucket[1]],\n                  softmax_loss_function=softmax_loss_function))\n\n  return outputs, losses'"
code/3_basic_concepts/3.6_best_practice.py,9,"b'# -*- coding=utf-8 -*-\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# \xe6\x89\x93\xe5\x8d\xb0\xe6\x97\xa5\xe5\xbf\x97\xe7\x9a\x84\xe6\xad\xa5\xe9\x95\xbf\nlog_step = 50\n# ================ 1.\xe5\xae\x9a\xe4\xb9\x89\xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb0 ================\n# \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\nlearning_rate = 0.01\n# \xe6\x9c\x80\xe5\xa4\xa7\xe8\xae\xad\xe7\xbb\x83\xe6\xad\xa5\xe6\x95\xb0\nmax_train_steps = 1000\n# ================ 2.\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae ================\n# \xe6\x9e\x84\xe9\x80\xa0\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\ntrain_X = np.array([[3.3],[4.4],[5.5],[6.71],[6.93],[4.168],[9.779],[6.182],[7.59],[2.167],[7.042],[10.791],[5.313],[7.997],[5.654],[9.27],[3.1]], dtype=np.float32)\ntrain_Y = np.array([[1.7],[2.76],[2.09],[3.19],[1.694],[1.573],[3.366],[2.596],[2.53],[1.221],[2.827],[3.465],[1.65],[2.904],[2.42],[2.94],[1.3]], dtype=np.float32)\ntotal_samples = train_X.shape[0]\n# ================ 3.\xe6\x9e\x84\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b ================\n# \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\nX = tf.placeholder(tf.float32, [None, 1])\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\nW = tf.Variable(tf.random_normal([1, 1]), name=""weight"")\nb = tf.Variable(tf.zeros([1]), name=""bias"")\n# \xe6\x8e\xa8\xe7\x90\x86\xe5\x80\xbc\nY = tf.matmul(X, W) + b\n# ================ 4.\xe5\xae\x9a\xe4\xb9\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 ================\n# \xe5\xae\x9e\xe9\x99\x85\xe5\x80\xbc\nY_ = tf.placeholder(tf.float32, [None, 1])\n# \xe5\x9d\x87\xe6\x96\xb9\xe5\xb7\xae\nloss = tf.reduce_sum(tf.pow(Y-Y_, 2))/(2*total_samples)\n# ================ 5.\xe5\x88\x9b\xe5\xbb\xba\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8 ================\n# \xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\n# ================ 6.\xe5\xae\x9a\xe4\xb9\x89\xe5\x8d\x95\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c ================\n# \xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\ntrain_op = optimizer.minimize(loss)\n# ================ 7.\xe5\x88\x9b\xe5\xbb\xba\xe4\xbc\x9a\xe8\xaf\x9d ================\nwith tf.Session() as sess:\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe5\xb1\x80\xe5\x8f\x98\xe9\x87\x8f\n    sess.run(tf.global_variables_initializer()) \n# ================ 8.\xe8\xbf\xad\xe4\xbb\xa3\xe8\xae\xad\xe7\xbb\x83 ================\n    print(""Start training:"")\n    for step in xrange(max_train_steps):\n        sess.run(train_op, feed_dict={X: train_X, Y_: train_Y})\n        # \xe6\xaf\x8f\xe9\x9a\x94log_step\xe6\xad\xa5\xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe6\xac\xa1\xe6\x97\xa5\xe5\xbf\x97\n        if step % log_step == 0:\n            c = sess.run(loss, feed_dict={X: train_X, Y_:train_Y})\n            print(""Step:%d, loss==%.4f, W==%.4f, b==%.4f"" % \n                    (step, c, sess.run(W), sess.run(b)))\n    # \xe8\xae\xa1\xe7\xae\x97\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x8c\xe6\xaf\x95\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xef\xbc\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe6\x8c\x87\xe6\xa0\x87\xe8\xbe\x93\xe5\x87\xba\n    final_loss = sess.run(loss, feed_dict={X: train_X, Y_: train_Y})\n    # \xe8\xae\xa1\xe7\xae\x97\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x8c\xe6\xaf\x95\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0W\xe5\x92\x8cb\n    weight, bias = sess.run([W, b])\n    print(""Step:%d, loss==%.4f, W==%.4f, b==%.4f"" % \n            (max_train_steps, final_loss, sess.run(W), sess.run(b)))\n    print(""Linear Regression Model: Y==%.4f*X+%.4f"" % (weight, bias))\n# ================ \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96 ================\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96Matplotlib\xe5\x90\x8e\xe7\xab\xaf\n    %matplotlib\n    # \xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xaeX\xe5\x92\x8cY\xef\xbc\x8c\xe6\xb7\xbb\xe5\x8a\xa0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xba\xa2\xe8\x89\xb2\xe5\x9c\x86\xe7\x82\xb9\n    plt.plot(train_X, train_Y, \'ro\', label=\'Training data\')\n    # \xe6\xa0\xb9\xe6\x8d\xae\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xb7\xbb\xe5\x8a\xa0\xe8\x93\x9d\xe8\x89\xb2\xef\xbc\x88\xe7\xbc\xba\xe7\x9c\x81\xe8\x89\xb2\xef\xbc\x89\xe6\x8b\x9f\xe5\x90\x88\xe7\x9b\xb4\xe7\xba\xbf\n    plt.plot(train_X, weight * train_X + bias, label=\'Fitted line\')\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe5\x9b\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\n    plt.legend()\n    # \xe7\x94\xbb\xe5\x87\xba\xe4\xb8\x8a\xe9\x9d\xa2\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe5\x9b\xbe\xe6\xa1\x88\n    plt.show()\n\n\'\'\'\n\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a\nStart training:\nStep:0, loss==2.8679, W==0.0054, b==0.0411\nStep:50, loss==0.1045, W==0.3457, b==0.1317\nStep:100, loss==0.1013, W==0.3402, b==0.1710\nStep:150, loss==0.0985, W==0.3350, b==0.2080\nStep:200, loss==0.0961, W==0.3301, b==0.2428\nStep:250, loss==0.0939, W==0.3254, b==0.2755\nStep:300, loss==0.0919, W==0.3211, b==0.3064\nStep:350, loss==0.0902, W==0.3170, b==0.3354\nStep:400, loss==0.0887, W==0.3131, b==0.3627\nStep:450, loss==0.0874, W==0.3095, b==0.3884\nStep:500, loss==0.0862, W==0.3061, b==0.4126\nStep:550, loss==0.0851, W==0.3029, b==0.4353\nStep:600, loss==0.0842, W==0.2999, b==0.4567\nStep:650, loss==0.0833, W==0.2970, b==0.4769\nStep:700, loss==0.0826, W==0.2944, b==0.4959\nStep:750, loss==0.0820, W==0.2918, b==0.5137\nStep:800, loss==0.0814, W==0.2895, b==0.5305\nStep:850, loss==0.0809, W==0.2872, b==0.5463\nStep:900, loss==0.0804, W==0.2851, b==0.5612\nStep:950, loss==0.0800, W==0.2832, b==0.5752\nStep:1000, loss==0.0797, W==0.2814, b==0.5881\nLinear Regression Model: Y==0.2814*X+0.5881\n\'\'\''"
code/4_data_io/4.1_best_practice.py,14,"b'# -*- coding:utf-8 -*-\n# \xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\xba1\xe5\xad\x97\xe8\x8a\x82\nLABEL_BYTES = 1\n# \xe5\x9b\xbe\xe7\x89\x87\xe5\xb0\xba\xe5\xaf\xb8\xe4\xb8\xba32\xe5\xad\x97\xe8\x8a\x82\nIMAGE_SIZE = 32\n# \xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xbaRGB 3\xe9\x80\x9a\xe9\x81\x93\nIMAGE_DEPTH = 3\n# \xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\xba32x32x3\xef\xbc\x9d3072\xe5\xad\x97\xe8\x8a\x82\nIMAGE_BYTES = IMAGE_SIZE * IMAGE_SIZE * IMAGE_DEPTH\n# 10\xe7\xb1\xbb\xe6\xa0\x87\xe7\xad\xbe\nNUM_CLASSES = 10\n\nimport tensorflow as tf\n\ndef read_cifar10(data_file, batch_size):\n  """"""\xe4\xbb\x8eCIFAR-10\xe6\x95\xb0\xe6\x8d\xae\xe6\x96\x87\xe4\xbb\xb6\xe8\xaf\xbb\xe5\x8f\x96\xe6\x89\xb9\xe6\xa0\xb7\xe4\xbe\x8b\n  \xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0:\n    data_file: CIFAR-10\xe6\x95\xb0\xe6\x8d\xae\xe6\x96\x87\xe4\xbb\xb6\n    batch_size: \xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\xa7\xe5\xb0\x8f\n  \xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc:\n    images: \xe5\xbd\xa2\xe5\xa6\x82[batch_size, IMAGE_SIZE, IMAGE_SIZE, 3]\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae\n    labels: \xe5\xbd\xa2\xe5\xa6\x82[batch_size\xef\xbc\x8cNUM_CLASSES]\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae\n  """"""\n  # \xe5\x8d\x95\xe6\x9d\xa1\xe6\x95\xb0\xe6\x8d\xae\xe8\xae\xb0\xe5\xbd\x95\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba1+3072=3073\xe5\xad\x97\xe8\x8a\x82\n  record_bytes = LABEL_BYTES + IMAGE_BYTES\n  # \xe5\x88\x9b\xe5\xbb\xba\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\n  data_files = tf.gfile.Glob(data_file)\n  # \xe5\x88\x9b\xe5\xbb\xba\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe9\x98\x9f\xe5\x88\x97\n  file_queue = tf.train.string_input_producer(data_files, shuffle=True)\n  # \xe5\x88\x9b\xe5\xbb\xba\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\x96\x87\xe4\xbb\xb6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84Reader\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe6\x8c\x89\xe7\x85\xa7\xe8\xae\xb0\xe5\xbd\x95\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xbb\x8e\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\xa0\xb7\xe4\xbe\x8b\n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n  _, value = reader.read(file_queue)\n  # \xe5\xb0\x86\xe6\xa0\xb7\xe4\xbe\x8b\xe6\x8b\x86\xe5\x88\x86\xe4\xb8\xba\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe\xe5\x92\x8c\xe5\x9b\xbe\xe7\x89\x87\n  record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n  label = tf.cast(tf.slice(record, [0], [LABEL_BYTES]), tf.int32)\n  # \xe5\xb0\x86\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba[depth * height * width]\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe5\xbd\xa2\xe5\xa6\x82[depth, height, width]\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\xbc\xa0\xe9\x87\x8f\n  depth_major = tf.reshape(tf.slice(record, [LABEL_BYTES], [IMAGE_BYTES]),\n                           [IMAGE_DEPTH, IMAGE_SIZE, IMAGE_SIZE])\n  # \xe6\x94\xb9\xe5\x8f\x98\xe5\x9b\xbe\xe7\x89\x87\xe5\xbc\xa0\xe9\x87\x8f\xe5\x90\x84\xe7\xbb\xb4\xe5\xba\xa6\xe9\xa1\xba\xe5\xba\x8f\xef\xbc\x8c\xe4\xbb\x8e[depth, height, width]\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba[height, width, depth]\n  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n  # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa0\xb7\xe4\xbe\x8b\xe9\x98\x9f\xe5\x88\x97\n  example_queue = tf.RandomShuffleQueue(\n      capacity=16 * batch_size,\n      min_after_dequeue=8 * batch_size,\n      dtypes=[tf.float32, tf.int32],\n      shapes=[[IMAGE_SIZE, IMAGE_SIZE, IMAGE_DEPTH], [1]])\n  num_threads = 16\n  # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa0\xb7\xe4\xbe\x8b\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe5\x85\xa5\xe9\x98\x9f\xe6\x93\x8d\xe4\xbd\x9c\n  example_enqueue_op = example_queue.enqueue([image, label])\n  # \xe5\xb0\x86\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x8416\xe4\xb8\xaa\xe7\xba\xbf\xe7\xa8\x8b\xe5\x85\xa8\xe9\x83\xa8\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0queue runner\xe4\xb8\xad\n  tf.train.add_queue_runner(tf.train.queue_runner.QueueRunner(\n      example_queue, [example_enqueue_op] * num_threads))\n\n  # \xe4\xbb\x8e\xe6\xa0\xb7\xe4\xbe\x8b\xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\x89\xb9\xe6\xa0\xb7\xe4\xbe\x8b\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\n  images, labels = example_queue.dequeue_many(batch_size)\n  labels = tf.reshape(labels, [batch_size, 1])\n  indices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n  labels = tf.sparse_to_dense(\n      tf.concat(values=[indices, labels], axis=1),\n      [batch_size, NUM_CLASSES], 1.0, 0.0)\n\n  # \xe5\xb1\x95\xe7\xa4\xbaimages\xe5\x92\x8clabels\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\n  assert len(images.get_shape()) == 4\n  assert images.get_shape()[0] == batch_size\n  assert images.get_shape()[-1] == 3\n  assert len(labels.get_shape()) == 2\n  assert labels.get_shape()[0] == batch_size\n  assert labels.get_shape()[1] == NUM_CLASSES\n\n  return images, labels'"
code/4_data_io/4.1_reader.py,7,"b""# -*- coding: utf-8 -*-\nimport tensorflow as tf\n# \xe5\x88\x9b\xe5\xbb\xba\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe9\x98\x9f\xe5\x88\x97filename_queue\nfilename_queue = tf.train.string_input_producer(['stat.tfrecord'])\n# \xe5\x88\x9b\xe5\xbb\xba\xe8\xaf\xbb\xe5\x8f\x96TFRecords\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84reader\nreader = tf.TFRecordReader()\n# \xe5\x8f\x96\xe5\x87\xbastat.tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe6\x9d\xa1\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe7\x9a\x84\xe6\xa0\xb7\xe4\xbe\x8bserialized_example\n_, serialized_example = reader.read(filename_queue)\n# \xe5\xb0\x86\xe4\xb8\x80\xe6\x9d\xa1\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe7\x9a\x84\xe6\xa0\xb7\xe4\xbe\x8b\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe5\x85\xb6\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xb9\xe5\xbe\x81\xe5\xbc\xa0\xe9\x87\x8f\nfeatures = tf.parse_single_example(\n        serialized_example,\n        features={\n            'id': tf.FixedLenFeature([], tf.int64),\n            'age': tf.FixedLenFeature([], tf.int64),\n            'income': tf.FixedLenFeature([], tf.float32),\n            'outgo': tf.FixedLenFeature([], tf.float32),\n        }\n)"""
code/4_data_io/4.1_writer.py,11,"b""# -*- coding: utf-8 -*-\nimport tensorflow as tf\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\x90\x91TFRecords\xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x99\xe6\x95\xb0\xe6\x8d\xae\xe8\xae\xb0\xe5\xbd\x95\xe7\x9a\x84writer\nwriter = tf.python_io.TFRecordWriter('stat.tfrecord')\n# 2\xe8\xbd\xae\xe5\xbe\xaa\xe7\x8e\xaf\xe6\x9e\x84\xe9\x80\xa0\xe8\xbe\x93\xe5\x85\xa5\xe6\xa0\xb7\xe4\xbe\x8b\nfor i in range(1,3):\n  # \xe5\x88\x9b\xe5\xbb\xbaexample.proto\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\xa0\xb7\xe4\xbe\x8b\n  example = tf.train.Example(\n      features = tf.train.Features(\n          feature = {\n            'id': tf.train.Feature(int64_list =\n                tf.train.Int64List(value=[i])),\n            'age': tf.train.Feature(int64_list =\n                tf.train.Int64List(value=[i*24])),\n            'income': tf.train.Feature(float_list =\n                tf.train.FloatList(value=[i*2048.0])),\n            'outgo': tf.train.Feature(float_list =\n                tf.train.FloatList(value=[i*1024.0]))\n          }\n      )\n  )\n  # \xe5\xb0\x86\xe6\xa0\xb7\xe4\xbe\x8b\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe4\xb8\xba\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\x90\x8e\xef\xbc\x8c\xe5\x86\x99\xe5\x85\xa5stat.tfrecord\xe6\x96\x87\xe4\xbb\xb6\n  writer.write(example.SerializeToString())\n# \xe5\x85\xb3\xe9\x97\xad\xe8\xbe\x93\xe5\x87\xba\xe6\xb5\x81\nwriter.close()"""
code/4_data_io/4.3_demo.py,0,"b""# -*- coding: utf-8 -*-\nimport argparse \nparser = argparse.ArgumentParser(prog='demo', description='A demo program', epilog='The end of usage')\n\nparser.add_argument('name')\nparser.add_argument('-a', '--age', type=int, required=True)\nparser.add_argument('-s', '--status', choices=['alpha', 'beta', 'released'], type=str, dest='myStatus')\n\nargs = parser.parse_args() # \xe5\xb0\x86\xe5\x90\x8d\xe5\xad\x97\xe7\xa9\xba\xe9\x97\xb4\xe8\xb5\x8b\xe5\x80\xbc\xe7\xbb\x99args\nprint(args) # \xe8\xbe\x93\xe5\x87\xba\xe5\x90\x8d\xe5\xad\x97\xe7\xa9\xba\xe9\x97\xb4"""
code/4_data_io/4.3_flags_demo.py,2,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nflags = tf.app.flags\nflags.DEFINE_string(""data_dir"", ""/tmp/mnist-data"",\n                    ""Direcotry for storing mnist data"")\n\nFLAGS = flags.FLAGS\ndef main(_):\n    print(FLAGS.data_dir)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
code/4_data_io/4.3_flags_mnist.py,2,"b'# -*- coding: utf-8 -*-\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nflags = tf.app.flags\nflags.DEFINE_string(""data_dir"", ""/tmp/mnist-data"",\n                    ""Directory for storing mnist data"")\nFLAGS = flags.FLAGS\ndef main(_):\n  # \xe5\xaf\xbc\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n  # ...\xe7\x9c\x81\xe7\x95\xa5\xe4\xb8\xad\xe9\x97\xb4\xe6\xad\xa5\xe9\xaa\xa4...\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
code/4_data_io/4.3_new_demo.py,0,"b""# -*- coding: utf-8 -*-\nimport argparse\nparser = argparse.ArgumentParser(prog='demo')\nparser.add_argument('name')\nparser.add_argument('-a', '--age', type=int, required=True)\nparser.add_argument('-s', '--status', choices=['alpha', 'beta', 'released'], type=str, dest='myStatus')\nparser.add_argument('-v', '--version', action='version', version='%(prog)s 1.0')\n\nargs, unparsed = parser.parse_known_args() # \xe5\xb0\x86\xe8\xa7\xa3\xe6\x9e\x90\xe5\x99\xa8\xe4\xb8\xad\xe6\x9c\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x94\xe5\x9b\x9e\xe7\xbb\x99unparsed\nprint('args=%s, unparsed=%s' % (args, unparsed))"""
code/5_control_flow_analysis/5.1_best_practice.py,15,"b'""""""5.1_best_practice.py"""""" \n# -*- coding: utf-8 -*-\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nflags = tf.app.flags\nflags.DEFINE_string(""data_dir"", ""/tmp/mnist-data"",\n                    ""Directory for storing mnist data"")\nflags.DEFINE_float(""learning_rate"", 0.5, ""Learning rate"")\nFLAGS = flags.FLAGS\n\ndef main(_):\n  # \xe5\x88\x9b\xe5\xbb\xbaMNIST\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xae\x9e\xe4\xbe\x8b\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n  # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\n  x = tf.placeholder(tf.float32, [None, 784])\t# \xe5\x9b\xbe\xe5\x83\x8f\xe6\x95\xb0\xe6\x8d\xae\n  W = tf.Variable(tf.zeros([784, 10]))\t\t\t  # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\x83\xe9\x87\x8d\n  b = tf.Variable(tf.zeros([10]))\t\t\t\t      # \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x81\x8f\xe7\xbd\xae\n  y = tf.matmul(x, W) + b\t\t\t\t\t\t          # \xe6\x8e\xa8\xe7\x90\x86\xe6\x93\x8d\xe4\xbd\x9c\n  y_ = tf.placeholder(tf.float32, [None, 10]) # \xe5\x9b\xbe\xe5\x83\x8f\xe6\xa0\x87\xe7\xad\xbe\n  # \xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe4\xbd\x9c\xe4\xb8\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n  # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n  optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n  # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8d\x95\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\n  train_op = optimizer.minimize(cross_entropy)\n  # \xe5\x88\x9b\xe5\xbb\xbaSaver\n  saver = tf.train.Saver()\n  sess = tf.InteractiveSession()\n  tf.global_variables_initializer().run()\n  # \xe6\x9c\x80\xe5\xa4\xa7\xe8\xae\xad\xe7\xbb\x83\xe6\xad\xa5\xe6\x95\xb0\n  for i in range(1000): \n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_op, feed_dict={x: batch_xs, y_: batch_ys})\n     # \xe6\xaf\x8f100\xe6\xad\xa5\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n    if i % 100 == 0:\n      saver.save(sess, \'mnist.ckpt\')\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  print(\'acc=%s\' % sess.run(accuracy, \n                            feed_dict={x: mnist.test.images,\n                                       y_: mnist.test.labels}))\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
code/5_control_flow_analysis/5.2_best_practice.py,25,"b'""""""5.2_best_practice.py""""""\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport sys\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\nflags = tf.app.flags\nflags.DEFINE_string(""data_dir"", ""/tmp/mnist-data"",\n                    ""Directory for storing mnist data"")\nflags.DEFINE_string(""train_dir"", ""/tmp/mnist-log"",\n                    ""Directory for storing checkpoint and summary files"")\nflags.DEFINE_integer(""task_index"", None,\n                     ""Worker task index, should be >= 0. task_index=0 is ""\n                     ""the master worker task the performs the variable ""\n                     ""initialization "")\nflags.DEFINE_integer(""num_gpus"", 0, ""Total number of gpus for each machine.""\n                     ""If you don\'t use GPU, please set it to \'0\'"")\nflags.DEFINE_integer(""replicas_to_aggregate"", None,\n                     ""Number of replicas to aggregate before parameter update ""\n                     ""is applied (For sync_replicas mode only; default: ""\n                     ""num_workers)"")\nflags.DEFINE_integer(""hidden_units"", 100,\n                     ""Number of units in the hidden layer of the NN"")\nflags.DEFINE_integer(""train_steps"", 200,\n                     ""Number of (global) training steps to perform"")\nflags.DEFINE_integer(""batch_size"", 100, ""Training batch size"")\nflags.DEFINE_float(""learning_rate"", 0.01, ""Learning rate"")\nflags.DEFINE_boolean(""sync_replicas"", False,\n                     ""Use the sync_replicas (synchronized replicas) mode, ""\n                     ""wherein the parameter updates from workers are aggregated ""\n                     ""before applied to avoid stale gradients"")\nflags.DEFINE_string(""ps_hosts"",""localhost:2222"",\n                    ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""worker_hosts"", ""localhost:2223,localhost:2224"",\n                    ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""job_name"", None, ""job name: worker or ps"")\n\nFLAGS = flags.FLAGS\n\n\nIMAGE_PIXELS = 28\n\n\ndef main(unused_argv):\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n  if FLAGS.job_name is None or FLAGS.job_name == """":\n    raise ValueError(""Must specify an explicit `job_name`"")\n  if FLAGS.task_index is None or FLAGS.task_index == """":\n    raise ValueError(""Must specify an explicit `task_index`"")\n\n  # \xe8\xa7\xa3\xe6\x9e\x90ps\xe5\x92\x8cworker\xe7\x9a\x84\xe4\xb8\xbb\xe6\x9c\xba\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\n  ps_spec = FLAGS.ps_hosts.split("","")\n  worker_spec = FLAGS.worker_hosts.split("","")\n\n  # \xe8\xae\xa1\xe7\xae\x97worker\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n  num_workers = len(worker_spec)\n\n  cluster = tf.train.ClusterSpec({\n      ""ps"": ps_spec,\n      ""worker"": worker_spec})\n  \n  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xafps\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x90\xaf\xe5\x8a\xa8\xe6\x9c\x8d\xe5\x8a\xa1\xef\xbc\x8c\xe5\xb9\xb6\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9b\x91\xe5\x90\xacworker\xe5\x8f\x91\xe8\xb5\xb7\xe7\x9a\x84\xe8\xaf\xb7\xe6\xb1\x82\n  if FLAGS.job_name == ""ps"":\n      server.join()\n\n  # \xe5\x88\xa4\xe6\x96\xad\xe5\xbd\x93\xe5\x89\x8d\xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\xbachief worker\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\xe8\xbf\x9b\xe7\xa8\x8b\n  is_chief = (FLAGS.task_index == 0)\n\n  if FLAGS.num_gpus > 0:\n    # \xe5\x81\x87\xe8\xae\xbe\xe6\xaf\x8f\xe5\x8f\xb0\xe6\x9c\xba\xe5\x99\xa8\xe7\x9a\x84 GPU \xe6\x95\xb0\xe9\x87\x8f\xe9\x83\xbd\xe7\x9b\xb8\xe5\x90\x8c\xe6\x97\xb6\xef\xbc\x8c\xe4\xb8\xba\xe6\xaf\x8f\xe5\x8f\xb0\xe6\x9c\xba\xe5\x99\xa8\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\xaa GPU \xe4\xbe\x9d\xe6\xac\xa1\xe5\x88\x86\xe9\x85\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe4\xbb\xbb\xe5\x8a\xa1\xe3\x80\x82\n    gpu = (FLAGS.task_index % FLAGS.num_gpus)\n    worker_device = ""/job:worker/task:%d/gpu:%d"" % (FLAGS.task_index, gpu)\n  elif FLAGS.num_gpus == 0:\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89 GPU\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xb0\x86\xe8\xae\xa1\xe7\xae\x97\xe4\xbb\xbb\xe5\x8a\xa1\xe5\x88\x86\xe9\x85\x8d\xe5\x88\xb0 CPU\n    cpu = 0\n    worker_device = ""/job:worker/task:%d/cpu:%d"" % (FLAGS.task_index, cpu)\n\n  # \xe6\xa0\xb9\xe6\x8d\xaeTensorFlow\xe9\x9b\x86\xe7\xbe\xa4\xe7\x9a\x84\xe5\xae\x9a\xe4\xb9\x89\xe5\x92\x8c\xe5\xbd\x93\xe5\x89\x8d\xe8\xae\xbe\xe5\xa4\x87\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe6\x94\xbe\xe7\xbd\xae\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x93\x8d\xe4\xbd\x9c\n  with tf.device(\n      tf.train.replica_device_setter(\n          worker_device=worker_device,\n          ps_device=""/job:ps/cpu:0"",\n          cluster=cluster)):\n      global_step = tf.Variable(0, name=""global_step"", trainable=False)\n\n      # \xe9\x9a\x90\xe5\xb1\x82\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n      hid_w = tf.Variable(\n          tf.truncated_normal(\n              [IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n              stddev=1.0 / IMAGE_PIXELS),\n          name=""hid_w"")\n      hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=""hid_b"")\n\n      # softmax\xe5\xb1\x82\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n      sm_w = tf.Variable(\n          tf.truncated_normal(\n              [FLAGS.hidden_units, 10],\n              stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\n          name=""sm_w"")\n      sm_b = tf.Variable(tf.zeros([10]), name=""sm_b"")\n\n      # \xe6\xa0\xb9\xe6\x8d\xae\xe4\xbb\xbb\xe5\x8a\xa1\xe7\xbc\x96\xe5\x8f\xb7\xe6\x94\xbe\xe7\xbd\xae\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84placeholder\n      x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n      y_ = tf.placeholder(tf.float32, [None, 10])\n      # tf.nn.xw_plus_b\xe5\x8d\xb3\xe4\xb8\xbamatmul(x, w) + b\n      hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n      # \xe4\xbd\xbf\xe7\x94\xa8relu\xe4\xbd\x9c\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8chid\xe4\xb8\xba\xe9\x9a\x90\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\n      hid = tf.nn.relu(hid_lin)\n      # \xe5\xae\x9a\xe4\xb9\x89softmax\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xbay\xef\xbc\x8c\xe5\x8d\xb3\xe6\x8e\xa8\xe7\x90\x86\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe5\x80\xbc\n      y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n      # \xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe8\xaf\x84\xe4\xbc\xb0\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa6\x82\xe7\x8e\x87\xe5\x88\x86\xe5\xb8\x83\xe9\x97\xb4\xe7\x9a\x84\xe7\x9b\xb8\xe4\xbc\xbc\xe6\x80\xa7\xe3\x80\x82\xe5\x9b\xa0\xe4\xb8\xba\xe6\xa6\x82\xe7\x8e\x87\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba[0, 1]\xef\xbc\x8c\n      # \xe5\x90\x8c\xe6\x97\xb6\xe9\x81\xbf\xe5\x85\x8d\xe5\x87\xba\xe7\x8e\xb0\xe6\x97\xa0\xe6\x84\x8f\xe4\xb9\x89\xe7\x9a\x84log(0)\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xa3\x81\xe5\x89\xaay\xe5\x80\xbc\xe5\x88\xb0\xe5\x8c\xba\xe9\x97\xb4[1e-10, 1.0]\n      cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n      # \xe4\xbd\xbf\xe7\x94\xa8Adam\xe5\x81\x9a\xe6\x9c\x80\xe4\xbc\x98\xe5\x8c\x96\xe6\xb1\x82\xe8\xa7\xa3\n      opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n    \n    # \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xbf\xe7\x94\xa8\xe5\x90\x8c\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xba\xe5\x88\xb6\n    if FLAGS.sync_replicas:\n      # \xe5\xa6\x82\xe6\x9e\x9c\xe7\x94\xa8\xe6\x88\xb7\xe6\xb2\xa1\xe6\x9c\x89\xe8\xbe\x93\xe5\x85\xa5\xe5\xb9\xb6\xe8\xa1\x8c\xe5\x89\xaf\xe6\x9c\xac\xe6\x95\xb0\xef\xbc\x8c\xe5\x88\x99\xe4\xbb\xa4\xe5\x85\xb6\xe7\xad\x89\xe4\xba\x8eworker\xe4\xbb\xbb\xe5\x8a\xa1\xe6\x95\xb0\n      if FLAGS.replicas_to_aggregate is None:\n        replicas_to_aggregate = num_workers\n      # \xe5\xa6\x82\xe6\x9e\x9c\xe7\x94\xa8\xe6\x88\xb7\xe8\xbe\x93\xe5\x85\xa5\xe4\xba\x86\xe5\xb9\xb6\xe8\xa1\x8c\xe5\x89\xaf\xe6\x9c\xac\xe6\x95\xb0\xef\xbc\x8c\xe5\x88\x99\xe8\xb5\x8b\xe5\x80\xbc\xe4\xb8\xba\xe5\x91\xbd\xe4\xbb\xa4\xe8\xa1\x8c\xe8\xa7\xa3\xe6\x9e\x90\xe7\x9a\x84\xe5\xb9\xb6\xe8\xa1\x8c\xe5\x89\xaf\xe6\x9c\xac\xe6\x95\xb0\n      else:\n        replicas_to_aggregate = FLAGS.replicas_to_aggregate\n      # \xe5\x88\x9b\xe5\xbb\xba\xe5\x90\x8c\xe6\xad\xa5\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe8\xb4\x9f\xe8\xb4\xa3\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe5\x92\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n      opt = tf.train.SyncReplicasOptimizer(\n          opt,\n          replicas_to_aggregate=replicas_to_aggregate,\n          total_num_replicas=num_workers,\n          name=""mnist_sync_replicas"")\n    # \xe5\x8d\x95\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x8d\xb3\xe5\x88\xa9\xe7\x94\xa8\xe5\x90\x8c\xe6\xad\xa5\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\x9c\x80\xe4\xbc\x98\xe5\x8c\x96\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n    train_op = opt.minimize(cross_entropy, global_step=global_step)\n\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe5\x90\x8c\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xba\xe5\x88\xb6\n    if FLAGS.sync_replicas:\n      # \xe5\x85\xb6\xe5\xae\x83worker\xef\xbc\x9a\xe4\xb8\xbalocal_step\xe8\xae\xbe\xe7\xbd\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x80\xbc\n      local_init_op = opt.local_step_init_op\n      # chief worker\xef\xbc\x9a\xe4\xb8\xbaglobal_step\xe8\xae\xbe\xe7\xbd\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x80\xbc\n      if is_chief:\n        local_init_op = opt.chief_init_op\n      # \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xba\xe6\x9c\xaa\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84Variable\xe8\xae\xbe\xe7\xbd\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\n      ready_for_local_init_op = opt.ready_for_local_init_op\n\n      # \xe5\xae\x9a\xe4\xb9\x89\xe5\x90\xaf\xe5\x8a\xa8\xe5\x90\x8c\xe6\xad\xa5\xe6\xa0\x87\xe8\xae\xb0\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84QueueRunner\xe5\xae\x9e\xe4\xbe\x8b\n      chief_queue_runner = opt.get_chief_queue_runner()\n      # \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xba\xe5\x90\x8c\xe6\xad\xa5\xe6\xa0\x87\xe8\xae\xb0\xe9\x98\x9f\xe5\x88\x97\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\x9d\xe5\xa7\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\n      sync_init_op = opt.get_init_tokens_op()\n    # \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xba\xe5\x85\xa8\xe5\xb1\x80Variable\xe8\xae\xbe\xe7\xbd\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\n    init_op = tf.global_variables_initializer()\n\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe5\x90\x8c\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xba\xe5\x88\xb6\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5\xe6\x9c\xac\xe5\x9c\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9b\xb8\xe5\x85\xb3\xe6\x93\x8d\xe4\xbd\x9c\n    if FLAGS.sync_replicas:\n      sv = tf.train.Supervisor(\n          is_chief=is_chief,\n          logdir=FLAGS.train_dir,\n          init_op=init_op,\n          local_init_op=local_init_op,\n          ready_for_local_init_op=ready_for_local_init_op,\n          recovery_wait_secs=1,\n          global_step=global_step)\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe5\xbc\x82\xe6\xad\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9c\xba\xe5\x88\xb6\xef\xbc\x8c\xe5\x90\x84worker\xe7\x8b\xac\xe8\x87\xaa\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe4\xb8\x8e\xe5\x8d\x95\xe6\x9c\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\x80\xe8\x87\xb4\n    else:\n      sv = tf.train.Supervisor(\n          is_chief=is_chief,\n          logdir=FLAGS.train_dir,\n          init_op=init_op,\n          recovery_wait_secs=1,\n          global_step=global_step)\n    # \xe9\x85\x8d\xe7\xbd\xae\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe4\xbc\x9a\xe8\xaf\x9d\xef\xbc\x9a\n    #     \xe5\x9c\xa8\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8f\xaf\xe7\x94\xa8\xe7\x9a\x84GPU\xe6\x97\xb6\xef\xbc\x8c\xe5\xb0\x86\xe6\x93\x8d\xe4\xbd\x9c\xe6\x94\xbe\xe7\xbd\xae\xe5\x88\xb0CPU\n    #     \xe4\xb8\x8d\xe6\x89\x93\xe5\x8d\xb0\xe8\xae\xbe\xe5\xa4\x87\xe6\x94\xbe\xe7\xbd\xae\xe4\xbf\xa1\xe6\x81\xaf\n    #     \xe8\xbf\x87\xe6\xbb\xa4\xe6\x9c\xaa\xe7\xbb\x91\xe5\xae\x9a\xe5\x9c\xa8ps\xe5\x92\x8cworker\xe4\xb8\x8a\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\n    sess_config = tf.ConfigProto(\n        allow_soft_placement=True,  \n        log_device_placement=False, \n        device_filters=[""/job:ps"", ""/job:worker/task:%d"" % FLAGS.task_index])\n\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xafchief worker\xef\xbc\x8c\xe5\x88\x99\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x89\x80\xe6\x9c\x89worker\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe4\xbc\x9a\xe8\xaf\x9d\n    if is_chief:\n      print(""Worker %d: Initializing session..."" % FLAGS.task_index)\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe5\x85\xb6\xe5\xae\x83worker\xef\xbc\x8c\xe5\x88\x99\xe7\xad\x89\xe5\xbe\x85chief worker\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe4\xbc\x9a\xe8\xaf\x9d\n    else:\n      print(""Worker %d: Waiting for session to be initialized..."" %\n            FLAGS.task_index)\n\n    sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n\n    print(""Worker %d: Session initialization complete."" % FLAGS.task_index)\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe5\x90\x8c\xe6\xad\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\xbd\x93\xe5\x89\x8d\xe8\xbf\x9b\xe7\xa8\x8b\xe4\xb8\xbachief worker\n    if FLAGS.sync_replicas and is_chief:\n      # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x90\x8c\xe6\xad\xa5\xe6\xa0\x87\xe8\xae\xb0\xe9\x98\x9f\xe5\x88\x97\n      sess.run(sync_init_op)\n      # \xe9\x80\x9a\xe8\xbf\x87queue runner\xe5\x90\xaf\xe5\x8a\xa83\xe4\xb8\xaa\xe7\xba\xbf\xe7\xa8\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe8\xbf\x90\xe8\xa1\x8c\xe5\x90\x84\xe8\x87\xaa\xe7\x9a\x84\xe6\xa0\x87\xe5\x87\x86\xe6\x9c\x8d\xe5\x8a\xa1\n      sv.start_queue_runners(sess, [chief_queue_runner])\n\n    # \xe8\xae\xb0\xe5\xbd\x95\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\xe8\xae\xad\xe7\xbb\x83\xe5\xbc\x80\xe5\xa7\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\n    time_begin = time.time()\n    print(""Training begins @ %f"" % time_begin)\n    # \xe5\xb0\x86local_step\xe8\xb5\x8b\xe5\x80\xbc\xe4\xb8\xba0\n    local_step = 0\n    while True:\n      # \xe5\xa1\xab\xe5\x85\x85\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n      batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n      train_feed = {x: batch_xs, y_: batch_ys}\n      # \xe6\x89\xa7\xe8\xa1\x8c\xe5\x8d\x95\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\n      _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n      local_step += 1\n      # \xe8\xae\xb0\xe5\xbd\x95\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\xe5\xae\x8c\xe6\x88\x90\xe5\xbd\x93\xe5\x89\x8d\xe5\x8d\x95\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x89\x80\xe9\x9c\x80\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\n      now = time.time()\n      print(""%f: Worker %d: training step %d done (global step: %d)"" %\n            (now, FLAGS.task_index, local_step, step))\n      # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xbd\x93\xe5\x89\x8d\xe8\xb6\x85\xe8\xbf\x87\xe6\x9c\x80\xe5\xa4\xa7\xe8\xae\xad\xe7\xbb\x83\xe6\xad\xa5\xe6\x95\xb0\xef\xbc\x8c\xe9\x80\x80\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\xaa\xe7\x8e\xaf\n      if step >= FLAGS.train_steps:\n        break\n    # \xe8\xae\xb0\xe5\xbd\x95\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\xe8\xae\xad\xe7\xbb\x83\xe7\xbb\x93\xe6\x9d\x9f\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\n    time_end = time.time()\n    print(""Training ends @ %f"" % time_end)\n    # \xe6\x80\xbb\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe9\x97\xb4\xe4\xb8\xba\xe4\xb8\xa4\xe8\x80\x85\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\xe5\xb7\xae\n    training_time = time_end - time_begin\n    print(""Training elapsed time: %f s"" % training_time)\n\n    # \xe5\xa1\xab\xe5\x85\x85\xe9\xaa\x8c\xe8\xaf\x81\xe6\x95\xb0\xe6\x8d\xae\n    val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n    # \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\x8a\xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n    val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n    print(""After %d training step(s), validation cross entropy = %g"" %\n          (FLAGS.train_steps, val_xent))\n\n\nif __name__ == ""__main__"":\n  tf.app.run()'"
code/5_control_flow_analysis/5.2_trainer.py,2,"b'""""""trainer.py""""""\n# -*- coding: utf-8 -*-\nfrom tensorflow import flags\nimport tensorflow as tf\n\n# \xe5\xae\x9a\xe4\xb9\x89TensorFlow\xe9\x9b\x86\xe7\xbe\xa4\xe5\x8f\x82\xe6\x95\xb0\nflags.DEFINE_integer(""task_index"", None,\n                     ""Worker task index, should be >= 0. task_index=0 is ""\n                     ""the master worker task the performs the variable ""\n                     ""initialization."")\nflags.DEFINE_string(""ps_hosts"", None,\n                    ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""worker_hosts"", None,\n                    ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""job_name"", None, ""job name: worker or PS"")\ndef main(unused_argv):\n  # \xe8\xa7\xa3\xe6\x9e\x90\xe9\x9b\x86\xe7\xbe\xa4\xe5\x8f\x82\xe6\x95\xb0ps_hosts\xe5\x92\x8cworker_hosts\n  PS_spec = FLAGS.ps_hosts.split("","")\n  worker_spec = FLAGS.worker_hosts.split("","")\n  # \xe5\xae\x9a\xe4\xb9\x89TensorFlow\xe9\x9b\x86\xe7\xbe\xa4\n  cluster = tf.train.ClusterSpec({\n      ""PS"": PS_spec,\n      ""worker"": worker_spec})\n\n  server = tf.train.Server(\n      cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n  # \xe5\x90\xaf\xe5\x8a\xa8PS\xef\xbc\x8c\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9b\x91\xe5\x90\xac\xe5\x90\x84worker\xe7\x9a\x84\xe8\xaf\xb7\xe6\xb1\x82\n  if FLAGS.job_name == ""PS"":\n    server.join()\n  # \xe5\xb0\x86\xe4\xbb\xbb\xe5\x8a\xa1\xe7\xbc\x96\xe5\x8f\xb7\xe4\xb8\xba0\xe7\x9a\x84worker\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbachief worker\n  is_chief = (FLAGS.task_index == 0)'"
code/6_tensorboard/6.2_best_practice.py,24,"b""# -*- coding:utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets('/tmp/data/mnist', one_hot=True)\n\nwith tf.name_scope('input'):\n  x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n  y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n\nwith tf.name_scope('softmax_layer'):\n  with tf.name_scope('weights'):\n      weights = tf.Variable(tf.zeros([784, 10]))\n  with tf.name_scope('biases'):\n      biases = tf.Variable(tf.zeros([10]))\n  with tf.name_scope('Wx_plus_b'):\n      y = tf.matmul(x, weights) + biases\n  \nwith tf.name_scope('cross_entropy'):\n  diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n  with tf.name_scope('total'):\n    cross_entropy = tf.reduce_mean(diff)\n\nwith tf.name_scope('train'):\n  train_step = tf.train.AdamOptimizer(0.001).minimize(\n      cross_entropy)\n\nwith tf.name_scope('accuracy'):\n  with tf.name_scope('correct_prediction'):\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  with tf.name_scope('accuracy'):\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsess = tf.InteractiveSession()\nwriter = tf.summary.FileWriter('/tmp/summary/mnist', sess.graph)\ntf.global_variables_initializer().run()\n\nwriter.close()\n"""
code/6_tensorboard/6.3_best_practice.py,35,"b'# -*- coding:utf-8 -*-\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets(\'/tmp/tensorflow/mnist/input_data\', one_hot=True)\n\nwith tf.name_scope(\'input\'):\n  x = tf.placeholder(tf.float32, [None, 784], name=\'x-input\')\n  y_ = tf.placeholder(tf.float32, [None, 10], name=\'y-input\')\n\nwith tf.name_scope(\'input_reshape\'):\n  # \xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8fx\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe5\x9b\x9b\xe9\x98\xb6\xe5\xbc\xa0\xe9\x87\x8f\n  image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n  # \xe6\xb7\xbb\xe5\x8a\xa0\xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x8b\xe5\x86\x99\xe4\xbd\x93\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\xb1\x87\xe6\x80\xbb\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe8\xae\xbe\xe7\xbd\xae\xe6\x9c\x80\xe5\xa4\xa7\xe7\x94\x9f\xe6\x88\x9010\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\n  tf.summary.image(\'input\', image_shaped_input, 10)\n\nwith tf.name_scope(\'softmax_layer\'):\n  with tf.name_scope(\'weights\'):\n    weights = tf.Variable(tf.zeros([784, 10]))\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe8\x8e\xb7\xe5\x8f\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\x83\xe9\x87\x8d\xe5\x80\xbc\xe7\x9a\x84\xe6\xb1\x87\xe6\x80\xbb\xe6\x93\x8d\xe4\xbd\x9c\n    tf.summary.histogram(\'weights\', weights)\n  with tf.name_scope(\'biases\'):\n    biases = tf.Variable(tf.zeros([10]))\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe8\x8e\xb7\xe5\x8f\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\xe7\x9a\x84\xe6\xb1\x87\xe6\x80\xbb\xe6\x93\x8d\xe4\xbd\x9c\n    tf.summary.histogram(\'biases\', biases)\n  with tf.name_scope(\'Wx_plus_b\'):\n    y = tf.matmul(x, weights) + biases\n  \nwith tf.name_scope(\'cross_entropy\'):\n  diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n  with tf.name_scope(\'total\'):\n    cross_entropy = tf.reduce_mean(diff)\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe8\x8e\xb7\xe5\x8f\x96\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe7\x9a\x84\xe6\xb1\x87\xe6\x80\xbb\xe6\x93\x8d\xe4\xbd\x9c\n    tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\nwith tf.name_scope(\'train\'):\n  train_step = tf.train.AdamOptimizer(0.001).minimize(\n      cross_entropy)\n\nwith tf.name_scope(\'accuracy\'):\n  with tf.name_scope(\'correct_prediction\'):\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  with tf.name_scope(\'accuracy\'):\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe8\x8e\xb7\xe5\x8f\x96\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe7\x9a\x84\xe6\xb1\x87\xe6\x80\xbb\xe6\x93\x8d\xe4\xbd\x9c\n    tf.summary.scalar(\'accuracy\', accuracy)\n\nmerged = tf.summary.merge_all()\nsess = tf.InteractiveSession()\n\ntrain_writer = tf.summary.FileWriter(\'/tmp/summary/mnist\' + \'/train\', sess.graph)\ntest_writer = tf.summary.FileWriter(\'/tmp/summary/mnist\' + \'/test\')\ntf.global_variables_initializer().run()\n\ndef feed_dict(train):\n  """"""\xe5\xa1\xab\xe5\x85\x85\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe6\x88\x96\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95""""""\n  if train:\n    xs, ys = mnist.train.next_batch(100, fake_data=False)\n  else:\n    xs, ys = mnist.test.images, mnist.test.labels\n  return {x: xs, y_: ys}\n\nfor i in range(1000):\n  if i % 10 == 0:  # \xe5\x86\x99\xe6\xb1\x87\xe6\x80\xbb\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n    test_writer.add_summary(summary, i)\n    print(\'Accuracy at step %s: %s\' % (i, acc))\n  else:  # Record train set summaries, and train\n    if i % 100 == 99:  # \xe5\x86\x99\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe7\x9a\x84\xe4\xba\x8b\xe4\xbb\xb6\xe6\x95\xb0\xe6\x8d\xae\n      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n      run_metadata = tf.RunMetadata()\n      summary, _ = sess.run([merged, train_step],\n                            feed_dict=feed_dict(True),\n                            options=run_options,\n                            run_metadata=run_metadata)\n      train_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n      train_writer.add_summary(summary, i)\n      print(\'Adding run metadata for\', i)\n    else:  # \xe5\x86\x99\xe6\xb1\x87\xe6\x80\xbb\xe6\x95\xb0\xe6\x8d\xae\n      summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n      train_writer.add_summary(summary, i)\n\ntrain_writer.close()\ntest_writer.close()\n'"
code/6_tensorboard/6.3_mnist_softmax_histogram.py,31,"b'# -*- coding:utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets(\'/tmp/data/mnist\', one_hot=True)\n\nwith tf.name_scope(\'input\'):\n  x = tf.placeholder(tf.float32, [None, 784], name=\'x-input\')\n  y_ = tf.placeholder(tf.float32, [None, 10], name=\'y-input\')\n\nwith tf.name_scope(\'softmax_layer\'):\n  with tf.name_scope(\'weights\'):\n    weights = tf.Variable(tf.zeros([784, 10]))\n    tf.summary.histogram(\'weights\', weights)\n  with tf.name_scope(\'biases\'):\n    biases = tf.Variable(tf.zeros([10]))\n  with tf.name_scope(\'Wx_plus_b\'):\n    y = tf.matmul(x, weights) + biases\n  \nwith tf.name_scope(\'cross_entropy\'):\n  diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n  with tf.name_scope(\'total\'):\n    cross_entropy = tf.reduce_mean(diff)\n    tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\nwith tf.name_scope(\'train\'):\n  train_step = tf.train.AdamOptimizer(0.001).minimize(\n      cross_entropy)\n\nwith tf.name_scope(\'accuracy\'):\n  with tf.name_scope(\'correct_prediction\'):\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  with tf.name_scope(\'accuracy\'):\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    tf.summary.scalar(\'accuracy\', accuracy)\n\nmerged = tf.summary.merge_all()\nsess = tf.InteractiveSession()\n\ntrain_writer = tf.summary.FileWriter(\'/tmp/summary/mnist/histogram\' + \'/train\', sess.graph)\ntest_writer = tf.summary.FileWriter(\'/tmp/summary/mnist/histogram\' + \'/test\')\ntf.global_variables_initializer().run()\n\ndef feed_dict(train):\n  """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""\n  if train:\n    xs, ys = mnist.train.next_batch(100, fake_data=False)\n  else:\n    xs, ys = mnist.test.images, mnist.test.labels\n  return {x: xs, y_: ys}\n\nfor i in range(1000):\n  if i % 10 == 0:  # Record summaries and test-set accuracy\n    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n    test_writer.add_summary(summary, i)\n    print(\'Accuracy at step %s: %s\' % (i, acc))\n  else:  # Record train set summaries, and train\n    if i % 100 == 99:  # Record execution stats\n      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n      run_metadata = tf.RunMetadata()\n      summary, _ = sess.run([merged, train_step],\n                            feed_dict=feed_dict(True),\n                            options=run_options,\n                            run_metadata=run_metadata)\n      train_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n      train_writer.add_summary(summary, i)\n      print(\'Adding run metadata for\', i)\n    else:  # Record a summary\n      summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n      train_writer.add_summary(summary, i)\ntrain_writer.close()\ntest_writer.close()\n'"
code/6_tensorboard/6.3_mnist_softmax_scalar.py,27,"b'# -*- coding:utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets(\'/tmp/data/mnist\', one_hot=True)\n\nwith tf.name_scope(\'input\'):\n  x = tf.placeholder(tf.float32, [None, 784], name=\'x-input\')\n  y_ = tf.placeholder(tf.float32, [None, 10], name=\'y-input\')\n\nwith tf.name_scope(\'softmax_layer\'):\n  weights = tf.Variable(tf.zeros([784, 10]))\n  biases = tf.Variable(tf.zeros([10]))\n  y = tf.matmul(x, weights) + biases\n  \nwith tf.name_scope(\'cross_entropy\'):\n  diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n  with tf.name_scope(\'total\'):\n    cross_entropy = tf.reduce_mean(diff)\n    tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\nwith tf.name_scope(\'train\'):\n  train_step = tf.train.AdamOptimizer(0.001).minimize(\n      cross_entropy)\n\nwith tf.name_scope(\'accuracy\'):\n  with tf.name_scope(\'correct_prediction\'):\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  with tf.name_scope(\'accuracy\'):\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    tf.summary.scalar(\'accuracy\', accuracy)\n\nmerged = tf.summary.merge_all()\nsess = tf.InteractiveSession()\n\ntrain_writer = tf.summary.FileWriter(\'/tmp/summary/mnist1\' + \'/train\', sess.graph)\ntest_writer = tf.summary.FileWriter(\'/tmp/summary/mnist1\' + \'/test\')\ntf.global_variables_initializer().run()\n\ndef feed_dict(train):\n  """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""\n  if train:\n    xs, ys = mnist.train.next_batch(100, fake_data=False)\n  else:\n    xs, ys = mnist.test.images, mnist.test.labels\n  return {x: xs, y_: ys}\n\nfor i in range(1000):\n  if i % 10 == 0:  # Record summaries and test-set accuracy\n    summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n    test_writer.add_summary(summary, i)\n    print(\'Accuracy at step %s: %s\' % (i, acc))\n  else:  # Record train set summaries, and train\n    if i % 100 == 99:  # Record execution stats\n      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n      run_metadata = tf.RunMetadata()\n      summary, _ = sess.run([merged, train_step],\n                            feed_dict=feed_dict(True),\n                            options=run_options,\n                            run_metadata=run_metadata)\n      train_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n      train_writer.add_summary(summary, i)\n      print(\'Adding run metadata for\', i)\n    else:  # Record a summary\n      summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n      train_writer.add_summary(summary, i)\ntrain_writer.close()\ntest_writer.close()\n'"
code/6_tensorboard/6.4_best_practice.py,9,"b""# -*- coding:utf-8 -*-\nimport argparse\nimport sys\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib.tensorboard.plugins import projector\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nFLAGS = None\n\ndef main(_):\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\x97\xa5\xe5\xbf\x97\xe7\x9b\xae\xe5\xbd\x95\n    if tf.gfile.Exists(FLAGS.log_dir):\n        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n    tf.gfile.MakeDirs(FLAGS.log_dir)\n\n    # \xe8\xaf\xbb\xe5\x8f\x96MNIST\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    mnist = input_data.read_data_sets(FLAGS.data_dir,\n                                      one_hot=True,\n                                      fake_data=FLAGS.fake_data)\n\n    # \xe5\x88\x9b\xe5\xbb\xba\xe5\xb5\x8c\xe5\x85\xa5\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\xad\xe7\x9a\x8410000\xe5\xbc\xa0\xe6\x89\x8b\xe5\x86\x99\xe4\xbd\x93\xe6\x95\xb0\xe5\xad\x97\xe5\x9b\xbe\xe5\x83\x8f\n    embedding_var = tf.Variable(tf.stack(mnist.test.images[:10000]),\n                                trainable=False, name='embedding')\n    # \xe5\x88\x9b\xe5\xbb\xba\xe4\xba\xa4\xe4\xba\x92\xe5\xbc\x8f\xe4\xbc\x9a\xe8\xaf\x9d\xef\xbc\x8c\xe5\xb9\xb6\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe5\xb1\x80\xe5\x8f\x98\xe9\x87\x8f\n    sess = tf.InteractiveSession()\n    tf.global_variables_initializer().run()\n\n    # \xe5\x88\x9b\xe5\xbb\xbasaver, \xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\xb5\x8c\xe5\x85\xa5\xe5\x8f\x98\xe9\x87\x8f\n    saver = tf.train.Saver()\n    saver.save(sess, os.path.join(FLAGS.log_dir + '/model.ckpt'))\n\n    # \xe5\x88\x9b\xe5\xbb\xba\xe5\x85\x83\xe4\xbf\xa1\xe6\x81\xaf\xe6\x96\x87\xe4\xbb\xb6,\xe5\xb9\xb6\xe5\x86\x99\xe5\x85\xa5\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\xad10000\xe5\xbc\xa0\xe6\x89\x8b\xe5\x86\x99\xe4\xbd\x93\xe6\x95\xb0\xe5\xad\x97\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n    metadata_file = FLAGS.log_dir + '/metadata.tsv'\n    with open(metadata_file, 'w') as f:\n      for i in range(FLAGS.max_nums):\n        c = np.nonzero(mnist.test.labels[::1])[1:][0][i]\n        f.write('{}\\n'.format(c))\n    \n    # \xe5\x88\x9b\xe5\xbb\xbaFileWriter\n    writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\x8a\x95\xe5\xbd\xb1\xe9\x85\x8d\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0\n    config = projector.ProjectorConfig()\n    embeddings= config.embeddings.add()\n    embeddings.tensor_name = 'embedding:0'\n    embeddings.metadata_path = os.path.join(FLAGS.log_dir + '/metadata.tsv')\n\n    # \xe8\xae\xbe\xe7\xbd\xae\xe5\x85\xa8\xe6\x99\xaf\xe5\x9b\xbe\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\x89\x8b\xe5\x86\x99\xe4\xbd\x93\xe6\x95\xb0\xe5\xad\x97\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\n    embeddings.sprite.image_path = os.path.join('/tmp/summary/images/mnist_10k_sprite.png')\n    embeddings.sprite.single_image_dim.extend([28, 28])\n\n    # \xe5\xb0\x86\xe5\x8f\x82\xe6\x95\xb0\xe9\x85\x8d\xe7\xbd\xae\xe5\x86\x99\xe5\x85\xa5\xe6\x96\xb0\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84\xe6\x8a\x95\xe5\xbd\xb1\xe5\x8f\x82\xe6\x95\xb0\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n    # TensorBoard\xe5\x90\xaf\xe5\x8a\xa8\xe6\x97\xb6\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe5\x8a\xa0\xe8\xbd\xbd\xe8\xaf\xa5\xe6\x96\x87\xe4\xbb\xb6\n    projector.visualize_embeddings(writer, config)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--fake_data', nargs='?', const=True, type=bool,\n                        default=False,\n                        help='If true, uses fake data for unit testing.')\n    parser.add_argument('--max_nums', type=int, default=10000,\n                        help='Number of steps to run trainer.')\n    parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data',\n                        help='Directory for storing input data')\n    parser.add_argument('--log_dir', type=str, default='/tmp/summary/embeddings',\n                        help='Summaries log directory')\n    FLAGS, unparsed = parser.parse_known_args()\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"""
code/7_tf_serving/7.4_mnist_saved_model.py,41,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#!/usr/bin/env python2.7\nr""""""Train and export a simple Softmax Regression TensorFlow model.\nThe model is from the TensorFlow ""MNIST For ML Beginner"" tutorial. This program\nsimply follows all its training instructions, and uses TensorFlow SavedModel to\nexport the trained model with proper signatures that can be loaded by standard\ntensorflow_model_server.\nUsage: mnist_saved_model.py [--training_iteration=x] [--model_version=y] \\\n    export_dir\n""""""\n\nimport os\nimport sys\n\n# This is a placeholder for a Google-internal import.\n\nimport tensorflow as tf\n\nimport mnist_input_data\n\ntf.app.flags.DEFINE_integer(\'training_iteration\', 1000,\n                            \'number of training iterations.\')\ntf.app.flags.DEFINE_integer(\'model_version\', 1, \'version number of the model.\')\ntf.app.flags.DEFINE_string(\'work_dir\', \'/tmp/mnist/\', \'Working directory.\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  if len(sys.argv) < 2 or sys.argv[-1].startswith(\'-\'):\n    print(\'Usage: mnist_export.py [--training_iteration=x] \'\n          \'[--model_version=y] export_dir\')\n    sys.exit(-1)\n  if FLAGS.training_iteration <= 0:\n    print \'Please specify a positive value for training iteration.\'\n    sys.exit(-1)\n  if FLAGS.model_version <= 0:\n    print \'Please specify a positive value for version number.\'\n    sys.exit(-1)\n\n  # Train model\n  print \'Training model...\'\n  mnist = mnist_input_data.read_data_sets(FLAGS.work_dir, one_hot=True)\n  print \'Create session\'\n  sess = tf.InteractiveSession()\n  serialized_tf_example = tf.placeholder(tf.string, name=\'tf_example\')\n  feature_configs = {\'x\': tf.FixedLenFeature(shape=[784], dtype=tf.float32),}\n  tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n  x = tf.identity(tf_example[\'x\'], name=\'x\')  # use tf.identity() to assign name\n  y_ = tf.placeholder(\'float\', shape=[None, 10])\n  w = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  sess.run(tf.global_variables_initializer())\n  y = tf.nn.softmax(tf.matmul(x, w) + b, name=\'y\')\n  cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n  values, indices = tf.nn.top_k(y, 10)\n  table = tf.contrib.lookup.index_to_string_table_from_tensor(\n      tf.constant([str(i) for i in xrange(10)]))\n  prediction_classes = table.lookup(tf.to_int64(indices))\n  for _ in range(FLAGS.training_iteration):\n    batch = mnist.train.next_batch(50)\n    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \'float\'))\n  print \'training accuracy %g\' % sess.run(\n      accuracy, feed_dict={x: mnist.test.images,\n                           y_: mnist.test.labels})\n  print \'Done training!\'\n\n  # Export model\n  # WARNING(break-tutorial-inline-code): The following code snippet is\n  # in-lined in tutorials, please update tutorial documents accordingly\n  # whenever code changes.\n  export_path_base = sys.argv[-1]\n  export_path = os.path.join(\n      tf.compat.as_bytes(export_path_base),\n      tf.compat.as_bytes(str(FLAGS.model_version)))\n  print \'Exporting trained model to\', export_path\n  builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n\n  # Build the signature_def_map.\n  classification_inputs = tf.saved_model.utils.build_tensor_info(\n      serialized_tf_example)\n  classification_outputs_classes = tf.saved_model.utils.build_tensor_info(\n      prediction_classes)\n  classification_outputs_scores = tf.saved_model.utils.build_tensor_info(values)\n\n  classification_signature = (\n      tf.saved_model.signature_def_utils.build_signature_def(\n          inputs={\n              tf.saved_model.signature_constants.CLASSIFY_INPUTS:\n                  classification_inputs\n          },\n          outputs={\n              tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\n                  classification_outputs_classes,\n              tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\n                  classification_outputs_scores\n          },\n          method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME))\n\n  tensor_info_x = tf.saved_model.utils.build_tensor_info(x)\n  tensor_info_y = tf.saved_model.utils.build_tensor_info(y)\n\n  prediction_signature = (\n      tf.saved_model.signature_def_utils.build_signature_def(\n          inputs={\'images\': tensor_info_x},\n          outputs={\'scores\': tensor_info_y},\n          method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n\n  legacy_init_op = tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n  builder.add_meta_graph_and_variables(\n      sess, [tf.saved_model.tag_constants.SERVING],\n      signature_def_map={\n          \'predict_images\':\n              prediction_signature,\n          tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n              classification_signature,\n      },\n      legacy_init_op=legacy_init_op)\n\n  builder.save()\n\n  print \'Done exporting!\'\n\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
code/9_cnn_models/9.2_alexnet.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  # \xe5\x9c\xa8\xe8\xaf\xa5\xe5\x8f\x82\xe6\x95\xb0\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9f\x9f\xe4\xb8\x8b\xe7\x9a\x84 conv2d \xe5\x92\x8c fully_connected \xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xad,\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe9\xbb\x98\xe8\xae\xa4\xe9\x87\x87\xe7\x94\xa8 \n  # tf.nn.relu,biases \xe5\x8f\x82\xe6\x95\xb0\xe9\xbb\x98\xe8\xae\xa4\xe9\x87\x87\xe7\x94\xa8 0.1 \xe6\x81\x92\xe5\xae\x9a\xe5\x80\xbc\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x80\xbc,\n  # weights \xe7\x9a\x84\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe9\xbb\x98\xe8\xae\xa4\xe9\x87\x87\xe7\x94\xa8 L2 \xe8\x8c\x83\xe5\xbc\x8f(\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88)\xe3\x80\x82\xe7\x94\xa8\xe6\x88\xb7\xe5\x8f\xaf\xe6\x98\xbe\xe5\xbc\x8f\xe6\x8c\x87\xe5\xae\x9a\xe5\x85\xb6\xe4\xbb\x96\xe6\x96\xb9\xe5\xbc\x8f\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    # \xe5\x9c\xa8\xe8\xaf\xa5\xe5\x8f\x82\xe6\x95\xb0\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9f\x9f\xe4\xb8\x8b\xe7\x9a\x84 conv2d \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xad,\xe8\xa1\xa5\xe9\x9b\xb6\xe9\xbb\x98\xe8\xae\xa4\xe9\x87\x87\xe7\x94\xa8 SAME \xe6\x96\xb9\xe5\xbc\x8f,\xe7\x94\xa8\xe6\x88\xb7\xe5\x8f\xaf\xe6\x98\xbe\xe5\xbc\x8f\xe6\x8c\x87\xe5\xae\x9a\xe5\x85\xb6\xe4\xbb\x96\xe6\x96\xb9\xe5\xbc\x8f\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      # \xe5\x9c\xa8\xe8\xaf\xa5\xe5\x8f\x82\xe6\x95\xb0\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9f\x9f\xe4\xb8\x8b\xe7\x9a\x84 max_pool2d \xe4\xb8\xad,\xe8\xa1\xa5\xe9\x9b\xb6\xe9\xbb\x98\xe8\xae\xa4\xe9\x87\x87\xe7\x94\xa8 VALID \xe6\x96\xb9\xe5\xbc\x8f,\xe7\x94\xa8\xe6\x88\xb7\xe5\x8f\xaf\xe6\x98\xbe\xe5\xbc\x8f\xe6\x8c\x87\xe5\xae\x9a\xe5\x85\xb6\xe4\xbb\x96\xe6\x96\xb9\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\',\n               global_pool=False):\n  """"""AlexNet version 2.\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224 or set\n        global_pool=True. To use in fully convolutional mode, set\n        spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: the number of predicted classes. If 0 or None, the logits layer\n    is omitted and the input features to the logits layer are returned instead.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      logits. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    global_pool: Optional boolean flag. If True, the input to the classification\n      layer is avgpooled to size 1x1, for any input size. (This is not part\n      of the original AlexNet.)\n  Returns:\n    net: the output of the logits layer (if num_classes is a non-zero integer),\n      or the non-dropped-out input to the logits layer (if num_classes is 0\n      or None).\n    end_points: a dict of tensors with intermediate activations.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    # tensorflow.contrib.slim.arg_scope(list_ops_or_scope, **kwargs)\xe6\x96\xb9\xe6\xb3\x95 \n    #(tensorflow.contrib.framework.python.ops.argscope(list_ops_or_scope,**kwargs)\n    # \xe6\x96\xb9\xe6\xb3\x95)\xe7\x9a\x84\xe4\xbd\x9c\xe7\x94\xa8\xe6\x98\xaf:\n    # \xe5\xbd\x93 list_ops_or_scope \xe4\xb8\xba ops \xe5\x88\x97\xe8\xa1\xa8\xe6\x97\xb6,\n    # \xe5\xb0\x86 kwargs \xe4\xb8\xad\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe9\x94\xae\xe5\x80\xbc\xe5\xaf\xb9\xe4\xbd\x9c\xe4\xb8\xba ops \xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa Op \xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\n    # \xe6\xad\xa4\xe5\xa4\x84,\xe8\xb0\x83\xe7\x94\xa8 slim.arg_scope \xe6\x96\xb9\xe6\xb3\x95,\xe4\xbd\xbf\xe5\xbe\x97\xe5\x8d\xb7\xe7\xa7\xaf(conv2d)\xe3\x80\x81\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5(fully_connected)\n    # \xe5\x92\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96(max_pool2d)\xe4\xb8\x89\xe7\xa7\x8d\xe7\xae\x97\xe5\xad\x90\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe9\x83\xbd\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8 end_points_collection \xe4\xb8\xad\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n      # \xe5\x88\xa9\xe7\x94\xa8 conv2d \xe5\xae\x9e\xe7\x8e\xb0\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        # \xe5\x9b\xa0\xe4\xb8\xba\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x98\xaf\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82,\xe5\x8d\xb3\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xe4\xb8\xba 1\xc3\x971,\xe6\x89\x80\xe4\xbb\xa5\xe6\xad\xa4\xe5\xa4\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb9\x9f\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba 1\xc3\x971\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        # \xe5\xb0\x86 end_points_collection \xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba end_points \xe5\xad\x97\xe5\x85\xb8\xe3\x80\x82\xe8\xaf\xa5\xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad\xe7\x9a\x84 key \xe8\xa1\xa8\xe7\xa4\xba\xe8\xaf\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97, \n        # value \xe8\xa1\xa8\xe7\xa4\xba\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe3\x80\x82\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if global_pool:\n          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'global_pool\')\n          end_points[\'global_pool\'] = net\n        if num_classes:\n          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                             scope=\'dropout7\')\n          net = slim.conv2d(net, num_classes, [1, 1],\n                            activation_fn=None,\n                            normalizer_fn=None,\n                            biases_initializer=tf.zeros_initializer(),\n                            scope=\'fc8\')\n          if spatial_squeeze:\n            # \xe5\xa6\x82\xe6\x9e\x9c spatial_squeeze \xe4\xb8\xba True,\xe5\x88\x99\xe5\xaf\xb9\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2,\n            # \xe8\xbf\x99\xe6\xa0\xb7\xe5\x81\x9a\xe7\x9a\x84\xe5\xa5\xbd\xe5\xa4\x84\xe6\x98\xaf\xe8\x83\xbd\xe5\xa4\x9f\xe5\x8e\xbb\xe9\x99\xa4\xe4\xb8\x80\xe4\xba\x9b\xe4\xb8\x8d\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6,\xe6\x96\xb9\xe4\xbe\xbf\xe8\xae\xa1\xe7\xae\x97\xe3\x80\x82\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba [N,H,W,C],\n            # N \xe8\xa1\xa8\xe7\xa4\xba\xe5\xbd\x93\xe5\x89\x8d\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f(batch_size),H \xe5\x92\x8c W \xe8\xa1\xa8\xe7\xa4\xba\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd,C \xe8\xa1\xa8\xe7\xa4\xba\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0 \n            # (\xe5\x8d\xb3\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe4\xb8\xad\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0)\xe3\x80\x82\xe5\x9b\xa0\xe4\xb8\xba\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe6\x98\xaf\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba,\xe6\x89\x80\xe4\xbb\xa5 H \xe5\x92\x8c W \xe9\x83\xbd\xe4\xb8\xba 1\xe3\x80\x82\n            # \xe9\x80\x9a\xe8\xbf\x87 tf.squeeze \xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\x86 H \xe5\x92\x8c W \xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe5\x8e\xbb\xe6\x8e\x89,\xe8\xaf\xa5\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x8f\x98\xe4\xb8\xba [N,C]\n            net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n          end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224'"
code/9_cnn_models/9.2_data_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import flowers\nfrom datasets import imagenet\nfrom datasets import mnist\n# \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86 map,\xe5\xbd\x93\xe5\x89\x8d\xe6\x94\xaf\xe6\x8c\x81\xe4\xbb\xa5\xe4\xb8\x8b\xe8\xbf\x99\xe5\x9b\x9b\xe7\xa7\x8d,\xe6\xaf\x8f\xe7\xa7\x8d\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe9\x83\xbd\xe5\xaf\xb9\xe5\xba\x94\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe7\x8b\xac\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97,\xe9\x83\xbd\xe6\x9c\x89 get_split \xe6\x96\xb9\xe6\xb3\x95\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'flowers\': flowers,\n    \'imagenet\': imagenet,\n    \'mnist\': mnist,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Given a dataset name and a split_name returns a Dataset.\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n  Returns:\n    A `Dataset` class.\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n  if name not in datasets_map:\n    raise ValueError(\'Name of dataset unknown %s\' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      file_pattern,\n      reader)'"
code/9_cnn_models/9.2_model_deploy.py,56,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\nUsage:\n  g = tf.Graph()\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n          ]\n\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # \xe8\xaf\xa5 Clone \xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\n                                \'scope\',  # \xe8\xaf\xa5 Clone \xe7\x9a\x84\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9f\x9f\n                                \'device\',  # \xe8\xaf\xa5Clone\xe7\x9a\x84\xe8\xae\xbe\xe5\xa4\x87\n                               ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # \xe5\x8d\x95\xe6\xad\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\n                                        \'summary_op\',  # \xe8\xae\xb0\xe5\xbd\x95\xe5\x8f\x98\xe9\x87\x8f\xe5\x8f\x98\xe5\x8c\x96\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\n                                        \'total_loss\',  # \xe6\x89\x80\xe6\x9c\x89Clone\xe5\xaf\xb9\xe8\xb1\xa1\xe4\xb8\x8a\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe7\x9a\x84\xe6\x80\xbb\xe5\x92\x8c\n                                        \'clones\',  # \xe5\xa4\x9a\xe4\xb8\xaaClone\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe9\x9b\x86\xe5\x90\x88\n                                       ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n  """"""Creates multiple clones according to config using a `model_fn`.\n  The returned values of `model_fn(*args, **kwargs)` are collected along with\n  the scope and device used to created it in a namedtuple\n  `Clone(outputs, scope, device)`\n  Note: it is assumed that any loss created by `model_fn` is collected at\n  the tf.GraphKeys.LOSSES collection.\n  To recover the losses, summaries or update_ops created by the clone use:\n  ```python\n    losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n  ```\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n  Args:\n    config: A DeploymentConfig object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n  Returns:\n    A list of namedtuples `Clone`.\n  """"""\n  clones = []\n  args = args or []\n  kwargs = kwargs or {}\n  with slim.arg_scope([slim.model_variable, slim.variable],\n                      device=config.variables_device()):\n    # Create clones.\n    for i in range(0, config.num_clones):\n      with tf.name_scope(config.clone_scope(i)) as clone_scope:\n        clone_device = config.clone_device(i)\n        with tf.device(clone_device):\n          with tf.variable_scope(tf.get_variable_scope(),\n                                 reuse=True if i > 0 else None):\n            outputs = model_fn(*args, **kwargs)\n          clones.append(Clone(outputs, clone_scope, clone_device))\n  return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n  """"""Gather the loss for a single clone.\n  Args:\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n  Returns:\n    A tensor for the total loss for the clone.  Can be None.\n  """"""\n  # The return value.\n  sum_loss = None\n  # Individual components of the loss that will need summaries.\n  clone_loss = None\n  regularization_loss = None\n  # Compute and aggregate losses on the clone device.\n  with tf.device(clone.device):\n    all_losses = []\n    clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    if clone_losses:\n      clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n      if num_clones > 1:\n        clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                            name=\'scaled_clone_loss\')\n      all_losses.append(clone_loss)\n    if regularization_losses:\n      regularization_loss = tf.add_n(regularization_losses,\n                                     name=\'regularization_loss\')\n      all_losses.append(regularization_loss)\n    if all_losses:\n      sum_loss = tf.add_n(all_losses)\n  # Add the summaries out of the clone device block.\n  if clone_loss is not None:\n    tf.summary.scalar(\'/\'.join(filter(None,\n                                      [\'Losses\', clone.scope, \'clone_loss\'])),\n                      clone_loss)\n  if regularization_loss is not None:\n    tf.summary.scalar(\'Losses/regularization_loss\', regularization_loss)\n  return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n  """"""Compute losses and gradients for a single clone.\n  Args:\n    optimizer: A tf.Optimizer  object.\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n    **kwargs: Dict of kwarg to pass to compute_gradients().\n  Returns:\n    A tuple (clone_loss, clone_grads_and_vars).\n      - clone_loss: A tensor for the total loss for the clone.  Can be None.\n      - clone_grads_and_vars: List of (gradient, variable) for the clone.\n        Can be empty.\n  """"""\n  sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n  clone_grad = None\n  if sum_loss is not None:\n    with tf.device(clone.device):\n      clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n  return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n  """"""Compute clone losses and gradients for the given list of `Clones`.\n  Note: The regularization_losses are added to the first clone losses.\n  Args:\n   clones: List of `Clones` created by `create_clones()`.\n   optimizer: An `Optimizer` object.\n   regularization_losses: Optional list of regularization losses. If None it\n     will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n     exclude them.\n   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n  Returns:\n   A tuple (total_loss, grads_and_vars).\n     - total_loss: A Tensor containing the average of the clone losses including\n       the regularization loss.\n     - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n       of the gradients for each variable.\n  """"""\n  grads_and_vars = []\n  clones_losses = []\n  num_clones = len(clones)\n  if regularization_losses is None:\n    regularization_losses = tf.get_collection(\n        tf.GraphKeys.REGULARIZATION_LOSSES)\n  for clone in clones:\n    with tf.name_scope(clone.scope):\n      clone_loss, clone_grad = _optimize_clone(\n          optimizer, clone, num_clones, regularization_losses, **kwargs)\n      if clone_loss is not None:\n        clones_losses.append(clone_loss)\n        grads_and_vars.append(clone_grad)\n      # Only use regularization_losses for the first clone\n      regularization_losses = None\n  # Compute the total_loss summing all the clones_losses.\n  total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n  # Sum the gradients across clones.\n  grads_and_vars = _sum_clones_gradients(grads_and_vars)\n  return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n  """"""Deploys a Slim-constructed model across multiple clones.\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n  The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n  the deployed model is configured for training with that optimizer.\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n  Args:\n    config: A `DeploymentConfig` object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n    optimizer: Optional `Optimizer` object.  If passed the model is deployed\n      for training with that optimizer.\n    summarize_gradients: Whether or not add summaries to the gradients.\n  Returns:\n    A `DeployedModel` namedtuple.\n  """"""\n  # Gather initial summaries.\n  summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n  # Create Clones.\n  clones = create_clones(config, model_fn, args, kwargs)\n  first_clone = clones[0]\n\n  # Gather update_ops from the first clone. These contain, for example,\n  # the updates for the batch_norm variables created by model_fn.\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n  train_op = None\n  total_loss = None\n  with tf.device(config.optimizer_device()):\n    if optimizer:\n      # Place the global step on the device storing the variables.\n      with tf.device(config.variables_device()):\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\xa4\x9a\xe4\xb8\xaa worker \xe5\xb9\xb6\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x97\xb6\xe7\x9a\x84\xe5\x90\x8c\xe6\xad\xa5\xe6\xa0\x87\xe8\xae\xb0 global_step\n        global_step = slim.get_or_create_global_step()\n        # \xe8\xb0\x83\xe7\x94\xa8model_deploy\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\xe7\x9a\x84optimize_clones\xe6\x96\xb9\xe6\xb3\x95,\xe5\xbe\x97\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89Clone\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\xe7\x9a\x84\xe7\xbb\xbc\xe5\x90\x88\n        # total_loss\xe5\x92\x8cclones_gradients\xe3\x80\x82clones_gradients\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xba:\n        # [(grad_1,var_1),(grad_2,var_2),...,\n        # (grad_i,var_i),...,(grad_N,var_N)],\xe5\x85\xb6\xe4\xb8\xad var_i \xe8\xa1\xa8\xe7\xa4\xba\xe7\xac\xac i \xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0(weights \xe6\x88\x96 biases \xe7\xad\x89),\n        # grad_i \xe8\xa1\xa8\xe7\xa4\xba var_i \xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6,grad_i \xe6\x98\xaf\xe6\x89\x80\xe6\x9c\x89 Clone \xe4\xb8\x8a\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe6\x80\xbb\xe5\x92\x8c\n      # Compute the gradients for the clones.\n      total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n      if clones_gradients:\n        if summarize_gradients:\n          # Add summaries to the gradients.\n          summaries |= set(_add_gradients_summaries(clones_gradients))\n        # \xe8\xb0\x83\xe7\x94\xa8\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84 apply_gradients \xe6\x96\xb9\xe6\xb3\x95,\xe5\xbe\x97\xe5\x88\xb0 grad_updates \xe6\x93\x8d\xe4\xbd\x9c, \n        # \xe8\xaf\xa5\xe6\x93\x8d\xe4\xbd\x9c\xe5\x88\xa9\xe7\x94\xa8 clones_gradients \xe4\xb8\xad\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x9b\xb4\xe6\x96\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        # \xe6\x9c\x80\xe7\xbb\x88\xe5\xbe\x97\xe5\x88\xb0 train_op,\xe5\xae\x83\xe8\xa1\xa8\xe7\xa4\xba\xe5\xae\x8c\xe6\x88\x90\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe6\x89\x80\xe9\x9c\x80\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x9b\xb4\xe6\x96\xb0\xe6\x93\x8d\xe4\xbd\x9c\n        with tf.control_dependencies([update_op]):\n          train_op = tf.identity(total_loss, name=\'train_op\')\n    else:\n      clones_losses = []\n      regularization_losses = tf.get_collection(\n          tf.GraphKeys.REGULARIZATION_LOSSES)\n      for clone in clones:\n        with tf.name_scope(clone.scope):\n          # \xe8\xb0\x83\xe7\x94\xa8 model_deploy \xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\xe7\x9a\x84 _gather_clone_loss \xe6\x96\xb9\xe6\xb3\x95,\xe5\xbe\x97\xe5\x88\xb0\xe5\xbd\x93\xe5\x89\x8d Clone \xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc, \n          # \xe8\xaf\xa5\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe7\x94\xb1 tf.GraphKeys.LOSSES \xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe5\x92\x8c\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0 regularization_losses\n          # \xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe5\x85\xb1\xe5\x90\x8c\xe6\x9e\x84\xe6\x88\x90\xe3\x80\x82\xe5\x85\xb6\xe4\xb8\xad,tf.GraphKeys.LOSSES \xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe9\x9c\x80\xe8\xa6\x81\xe9\x99\xa4\xe4\xbb\xa5 Clone \xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n          clone_loss = _gather_clone_loss(clone, len(clones),\n                                          regularization_losses)\n          if clone_loss is not None:\n            clones_losses.append(clone_loss)\n          # Only use regularization_losses for the first clone\n          # \xe9\x99\xa4\xe4\xba\x86\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa Clone \xe4\xb9\x8b\xe5\xa4\x96, \xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84 Clone \xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 regularization_losses \xe9\x83\xbd\xe8\xa2\xab\xe5\xbf\xbd\xe7\x95\xa5\n          regularization_losses = None\n      if clones_losses:\n        # \xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89 Clone \xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc clones_losses \xe7\x9b\xb8\xe5\x8a\xa0,\xe5\xbe\x97\xe5\x88\xb0\xe6\x80\xbb\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc total_loss\n        total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone.scope))\n\n    if total_loss is not None:\n      # Add total_loss to summary.\n      summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    if summaries:\n      # Merge all summaries together.\n      summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n    else:\n      summary_op = None\n\n  return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n  """"""Calculate the sum gradient for each shared variable across all clones.\n  This function assumes that the clone_grads has been scaled appropriately by\n  1 / num_clones.\n  Args:\n    clone_grads: A List of List of tuples (gradient, variable), one list per\n    `Clone`.\n  Returns:\n     List of tuples of (gradient, variable) where the gradient has been summed\n     across all clones.\n  """"""\n  sum_grads = []\n  for grad_and_vars in zip(*clone_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n    grads = []\n    var = grad_and_vars[0][1]\n    for g, v in grad_and_vars:\n      assert v == var\n      if g is not None:\n        grads.append(g)\n    if grads:\n      if len(grads) > 1:\n        sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n      else:\n        sum_grad = grads[0]\n      sum_grads.append((sum_grad, var))\n  return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n  """"""Add histogram summaries to gradients.\n  Note: The summaries are also added to the SUMMARIES collection.\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n  Returns:\n    The _list_ of the added summaries for grads_and_vars.\n  """"""\n  summaries = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        grad_values = grad.values\n      else:\n        grad_values = grad\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient\',\n                                            grad_values))\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient_norm\',\n                                            tf.global_norm([grad_values])))\n    else:\n      tf.logging.info(\'Var %s has no gradient\', var.op.name)\n  return summaries\n\n\nclass DeploymentConfig(object):\n  """"""Configuration for deploying a model with `deploy()`.\n  You can pass an instance of this class to `deploy()` to specify exactly\n  how to deploy the model to build.  If you do not pass one, an instance built\n  from the default deployment_hparams will be used.\n  """"""\n\n  def __init__(self,\n               num_clones=1,\n               clone_on_cpu=False,\n               replica_id=0,\n               num_replicas=1,\n               num_ps_tasks=0,\n               worker_job_name=\'worker\',\n               ps_job_name=\'ps\'):\n    """"""Create a DeploymentConfig.\n    The config describes how to deploy a model across multiple clones and\n    replicas.  The model will be replicated `num_clones` times in each replica.\n    If `clone_on_cpu` is True, each clone will placed on CPU.\n    If `num_replicas` is 1, the model is deployed via a single process.  In that\n    case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n    If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n    must specify TensorFlow devices for the `worker` and `ps` jobs and\n    `num_ps_tasks` must be positive.\n    Args:\n      num_clones: Number of model clones to deploy in each replica.\n      clone_on_cpu: If True clones would be placed on CPU.\n      replica_id: Integer.  Index of the replica for which the model is\n        deployed.  Usually 0 for the chief replica.\n      num_replicas: Number of replicas to use.\n      num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n      worker_job_name: A name for the worker job.\n      ps_job_name: A name for the parameter server job.\n    Raises:\n      ValueError: If the arguments are invalid.\n    """"""\n    if num_replicas > 1:\n      if num_ps_tasks < 1:\n        raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n    if num_replicas > 1 or num_ps_tasks > 0:\n      if not worker_job_name:\n        raise ValueError(\'Must specify worker_job_name when using replicas\')\n      if not ps_job_name:\n        raise ValueError(\'Must specify ps_job_name when using parameter server\')\n    if replica_id >= num_replicas:\n      raise ValueError(\'replica_id must be less than num_replicas\')\n    self._num_clones = num_clones\n    self._clone_on_cpu = clone_on_cpu\n    self._replica_id = replica_id\n    self._num_replicas = num_replicas\n    self._num_ps_tasks = num_ps_tasks\n    self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n    self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n  @property\n  def num_clones(self):\n    return self._num_clones\n\n  @property\n  def clone_on_cpu(self):\n    return self._clone_on_cpu\n\n  @property\n  def replica_id(self):\n    return self._replica_id\n\n  @property\n  def num_replicas(self):\n    return self._num_replicas\n\n  @property\n  def num_ps_tasks(self):\n    return self._num_ps_tasks\n\n  @property\n  def ps_device(self):\n    return self._ps_device\n\n  @property\n  def worker_device(self):\n    return self._worker_device\n\n  def caching_device(self):\n    """"""Returns the device to use for caching variables.\n    Variables are cached on the worker CPU when using replicas.\n    Returns:\n      A device string or None if the variables do not need to be cached.\n    """"""\n    if self._num_ps_tasks > 0:\n      return lambda op: op.device\n    else:\n      return None\n\n  def clone_device(self, clone_index):\n    """"""Device used to create the clone and all the ops inside the clone.\n    Args:\n      clone_index: Int, representing the clone_index.\n    Returns:\n      A value suitable for `tf.device()`.\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    if self._clone_on_cpu:\n      device += \'/device:CPU:0\'\n    else:\n      device += \'/device:GPU:%d\' % clone_index\n    return device\n\n  def clone_scope(self, clone_index):\n    """"""Name scope to create the clone.\n    Args:\n      clone_index: Int, representing the clone_index.\n    Returns:\n      A name_scope suitable for `tf.name_scope()`.\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    scope = \'\'\n    if self._num_clones > 1:\n      scope = \'clone_%d\' % clone_index\n    return scope\n\n  def optimizer_device(self):\n    """"""Device to use with the optimizer.\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    if self._num_ps_tasks > 0 or self._num_clones > 0:\n      return self._worker_device + \'/device:CPU:0\'\n    else:\n      return \'\'\n\n  def inputs_device(self):\n    """"""Device to use to build the inputs.\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    device += \'/device:CPU:0\'\n    return device\n\n  def variables_device(self):\n    """"""Returns the device to use for variables created inside the clone.\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._ps_device\n    device += \'/device:CPU:0\'\n\n    class _PSDeviceChooser(object):\n      """"""Slim device chooser for variables when using PS.""""""\n\n      def __init__(self, device, tasks):\n        self._device = device\n        self._tasks = tasks\n        self._task = 0\n\n      def choose(self, op):\n        if op.device:\n          return op.device\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n        if node_def.op.startswith(\'Variable\'):\n          t = self._task\n          self._task = (self._task + 1) % self._tasks\n          d = \'%s/task:%d\' % (self._device, t)\n          return d\n        else:\n          return op.device\n\n    if not self._num_ps_tasks:\n      return device\n    else:\n      chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n      return chooser.choose\n'"
code/9_cnn_models/9.2_nets_factory.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import mobilenet_v1\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\nfrom nets.mobilenet import mobilenet_v2\nfrom nets.nasnet import nasnet\nfrom nets.nasnet import pnasnet\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n                \'mobilenet_v1\': mobilenet_v1.mobilenet_v1,\n                \'mobilenet_v1_075\': mobilenet_v1.mobilenet_v1_075,\n                \'mobilenet_v1_050\': mobilenet_v1.mobilenet_v1_050,\n                \'mobilenet_v1_025\': mobilenet_v1.mobilenet_v1_025,\n                \'mobilenet_v2\': mobilenet_v2.mobilenet,\n                \'nasnet_cifar\': nasnet.build_nasnet_cifar,\n                \'nasnet_mobile\': nasnet.build_nasnet_mobile,\n                \'nasnet_large\': nasnet.build_nasnet_large,\n                \'pnasnet_large\': pnasnet.build_pnasnet_large,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                  \'mobilenet_v1\': mobilenet_v1.mobilenet_v1_arg_scope,\n                  \'mobilenet_v1_075\': mobilenet_v1.mobilenet_v1_arg_scope,\n                  \'mobilenet_v1_050\': mobilenet_v1.mobilenet_v1_arg_scope,\n                  \'mobilenet_v1_025\': mobilenet_v1.mobilenet_v1_arg_scope,\n                  \'mobilenet_v2\': mobilenet_v2.training_scope,\n                  \'nasnet_cifar\': nasnet.nasnet_cifar_arg_scope,\n                  \'nasnet_mobile\': nasnet.nasnet_mobile_arg_scope,\n                  \'nasnet_large\': nasnet.nasnet_large_arg_scope,\n                  \'pnasnet_large\': pnasnet.pnasnet_large_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification. If 0 or None,\n      the logits layer is omitted and its input features are returned instead.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n          net, end_points = network_fn(images)\n      The `images` input is a tensor of shape [batch_size, height, width, 3]\n      with height = width = network_fn.default_image_size. (The permissibility\n      and treatment of other sizes depends on the network_fn.)\n      The returned `end_points` are a dictionary of intermediate activations.\n      The returned `net` is the topmost layer, depending on `num_classes`:\n      If `num_classes` was a non-zero integer, `net` is a logits tensor\n      of shape [batch_size, num_classes].\n      If `num_classes` was 0 or `None`, `net` is a tensor with the input\n      to the logits layer of shape [batch_size, 1, 1, num_features] or\n      [batch_size, num_features]. Dropout has not been applied to this\n      (even if the network\'s original classification does); it remains for\n      the caller to do this or not.\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  # \xe5\x9c\xa8 network_map \xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad,\xe9\x80\x9a\xe8\xbf\x87\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8d\xe5\xad\x97(\xe5\xa6\x82 alexnet_v2)\n  # \xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe5\x87\xbd\xe6\x95\xb0(\xe5\xa6\x82 alexnet.alexnet_v2 \xe6\x96\xb9\xe6\xb3\x95)\n  func = networks_map[name]\n  # \xe6\xad\xa4\xe5\xa4\x84\xe4\xbd\xbf\xe7\x94\xa8 functools.wraps \xe8\xa3\x85\xe9\xa5\xb0\xe5\x99\xa8,\xe5\x9c\xa8\xe5\x8e\x9f\xe5\x87\xbd\xe6\x95\xb0 func \xe4\xb9\x8b\xe4\xb8\x8a\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x8f\x82\xe6\x95\xb0\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9f\x9f arg_scope, \n  # \xe4\xbb\xa5\xe9\x81\xbf\xe5\x85\x8d\xe5\x9c\xa8\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9a\xe4\xb9\x89\xe6\x97\xb6\xe9\x87\x8d\xe5\xa4\x8d\xe5\x86\x99\xe8\xbf\x87\xe5\xa4\x9a\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n  @functools.wraps(func)\n  def network_fn(images, **kwargs):\n    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training, **kwargs)\n  if hasattr(func, \'default_image_size\'):\n    # \xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe4\xbc\x9a\xe8\xae\xbe\xe5\xae\x9a\xe8\xaf\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\x88\x86\xe8\xbe\xa8\xe7\x8e\x87(\xe6\x88\x96\xe5\xa4\xa7\xe5\xb0\x8f)\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
code/9_cnn_models/9.2_train_image_classifier.py,92,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'/tmp/tfmodel/\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 600,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 600,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'task\', 0, \'Task id of the replica running the training.\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\n    \'adadelta_rho\', 0.95,\n    \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\n    \'adagrad_initial_accumulator_value\', 0.1,\n    \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_initial_accumulator_value\', 0.1,\n    \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 2.0,\n    \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n  Returns:\n    A `Tensor` representing the learning rate.\n  Raises:\n    ValueError: if\n  """"""\n  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                    FLAGS.num_epochs_per_decay)\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=1.0,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                     FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n  Returns:\n    An instance of an optimizer.\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n  return optimizer\n\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n  # ignoring the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        break\n    else:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n      checkpoint_path,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars)\n\n\ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    #######################\n    # Config model_deploy #\n    #######################\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=FLAGS.num_clones, # Clone \xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n        clone_on_cpu=FLAGS.clone_on_cpu, # \xe5\xb8\x83\xe5\xb0\x94\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f,\xe8\xa1\xa8\xe7\xa4\xba\xe6\x98\xaf\xe5\x90\xa6\xe5\xb0\x86 Clone \xe5\xaf\xb9\xe8\xb1\xa1\xe9\x83\xa8\xe7\xbd\xb2\xe5\x9c\xa8 CPU \xe4\xb8\x8a\n        replica_id=FLAGS.task, # worker \xe6\x88\x96 PS \xe8\xbf\x9b\xe7\xa8\x8b\xe7\x9a\x84 ID\n        num_replicas=FLAGS.worker_replicas, # worker \xe4\xbb\xbb\xe5\x8a\xa1\xe6\x95\xb0(\xe8\xaf\xa6\xe8\xa7\x81 5.2 \xe8\x8a\x82)\n        num_ps_tasks=FLAGS.num_ps_tasks) # PS \xe4\xbb\xbb\xe5\x8a\xa1\xe6\x95\xb0\n\n    # Create global_step\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    # \xe6\xa0\xb9\xe6\x8d\xae FLAGS \xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x90\x8d\xe5\xad\x97 dataset_name(\xe5\xa6\x82 imagenet)\xe3\x80\x81\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xa2\xab\xe5\x88\x86\xe5\x89\xb2\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x90\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x90\x8d\xe7\xa7\xb0 dataset_split_name(\xe5\xa6\x82 train)\n    # \xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe7\xbb\x9d\xe5\xaf\xb9\xe8\xb7\xaf\xe5\xbe\x84 dataset_dir,\xe4\xbb\x8e dataset_factory \xe4\xb8\xad\xe8\x8e\xb7\xe5\xbe\x97\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xaf\xb9\xe8\xb1\xa1 dataset\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ######################\n    # Select the network #\n    ######################\n    # \xe6\xa0\xb9\xe6\x8d\xae FLAGS \xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8d\xe7\xa7\xb0 model_name(\xe5\xa6\x82 alexnet_v2)\xe3\x80\x81\n    # \xe5\x88\x86\xe7\xb1\xbb\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0 num_classes \xe5\x92\x8c\xe6\x9d\x83\xe5\x80\xbc\xe8\xa1\xb0\xe5\x87\x8f weight_decay(\xe5\x8d\xb3 L2 \xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe7\xb3\xbb\xe6\x95\xb0), \n    # \xe4\xbb\x8e nets_factory \xe4\xb8\xad\xe8\x8e\xb7\xe5\xbe\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x87\xbd\xe6\x95\xb0\xe5\xaf\xb9\xe8\xb1\xa1 network_fn\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        weight_decay=FLAGS.weight_decay,\n        is_training=True)\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    # \xe6\x8c\x87\xe5\xae\x9a\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x87\xbd\xe6\x95\xb0\xe5\x90\x8d\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    # \xe6\xa0\xb9\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x87\xbd\xe6\x95\xb0\xe5\x90\x8d,\xe4\xbb\x8e preprocessing_factory \xe4\xb8\xad\xe8\x8e\xb7\xe5\xbe\x97\xe5\x9b\xbe\xe5\x83\x8f\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x87\xbd\xe6\x95\xb0\xe5\xaf\xb9\xe8\xb1\xa1\n    # image_preprocessing_fn\xe3\x80\x82\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=True)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    with tf.device(deploy_config.inputs_device()):\n      # FLAGS.num_readers \xe6\x8c\x87\xe5\xae\x9a\xe4\xba\x86\xe5\x90\x8c\xe6\x97\xb6\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0(\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba 4),\n      # \xe4\xb8\x8d\xe5\x90\x8c\xe7\xba\xbf\xe7\xa8\x8b\xe8\xaf\xbb\xe5\x8f\x96\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\xb0 common_queue \xe4\xb8\xad\xe3\x80\x82\xe6\xad\xa4\xe5\xa4\x84\xe9\xbb\x98\xe8\xae\xa4\xe8\xae\xbe\xe5\xae\x9a common_queue \n      # \xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\xae\xb9\xe9\x87\x8f\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f(batch_size)\xe7\x9a\x84 20 \xe5\x80\x8d\xe3\x80\x82common_queue_min\n      # \xe8\xa1\xa8\xe7\xa4\xba common_queue \xe9\x98\x9f\xe5\x88\x97\xe4\xb8\xad\xe6\x9c\x80\xe5\xb0\x91\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f,\xe9\xbb\x98\xe8\xae\xa4\xe8\xae\xbe\xe5\xae\x9a\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84 10 \xe5\x80\x8d\n      provider = slim.dataset_data_provider.DatasetDataProvider(\n          dataset,\n          num_readers=FLAGS.num_readers,\n          common_queue_capacity=20 * FLAGS.batch_size,\n          common_queue_min=10 * FLAGS.batch_size)\n      # \xe5\xa6\x82 9.2.1 \xe8\x8a\x82\xe6\x89\x80\xe8\xbf\xb0,\xe5\x8f\xaf\xe4\xbb\xa5\xe6\xa0\xb9\xe6\x8d\xae key \xe5\x80\xbc image \xe5\x92\x8c label \xe4\xbb\x8e provider \xe5\xaf\xb9\xe8\xb1\xa1\xe4\xb8\xad\xe8\x8e\xb7\xe5\xbe\x97\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x8a\xe5\x85\xb6\xe6\xa0\x87\xe7\xad\xbe\xe5\xbc\xa0\xe9\x87\x8f\n      [image, label] = provider.get([\'image\', \'label\'])\n      # \xe5\x9b\xa0\xe4\xb8\xba\xe5\x9c\xa8 VGG \xe6\x88\x96 ResNet \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad,\xe8\x83\x8c\xe6\x99\xaf\xe6\xb2\xa1\xe6\x9c\x89\xe8\xa2\xab\xe5\xbd\x93\xe4\xbd\x9c\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab,\n      # \xe6\x89\x80\xe4\xbb\xa5\xe5\xbd\x93\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x99\xe4\xb8\xa4\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x97\xb6,labels_offset \xe8\xa6\x81\xe8\xa2\xab\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba 1\n      label -= FLAGS.labels_offset\n      # \xe8\xae\xbe\xe5\xae\x9a\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\x88\x86\xe8\xbe\xa8\xe7\x8e\x87\n      train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n      # \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x8f\xe8\xbf\x87\xe5\x9b\xbe\xe5\x83\x8f\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x87\xbd\xe6\x95\xb0\xe5\xa4\x84\xe7\x90\x86\n      image = image_preprocessing_fn(image, train_image_size, train_image_size)\n      # \xe9\x80\x9a\xe8\xbf\x87 FLAGS.num_preprocessing_threads \xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\xe5\xb9\xb6\xe8\xa1\x8c\xe8\xaf\xbb\xe5\x8f\x96,\n      # \xe5\xbe\x97\xe5\x88\xb0\xe5\xbd\x93\xe5\x89\x8d\xe8\xbf\xad\xe4\xbb\xa3\xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae images \xe5\x92\x8c labels \xe5\xbc\xa0\xe9\x87\x8f\n      images, labels = tf.train.batch(\n          [image, label],\n          batch_size=FLAGS.batch_size,\n          num_threads=FLAGS.num_preprocessing_threads,\n          capacity=5 * FLAGS.batch_size)\n      labels = slim.one_hot_encoding(\n          labels, dataset.num_classes - FLAGS.labels_offset)\n      # \xe8\xb0\x83\xe7\x94\xa8 prefetch_queue \xe6\x96\xb9\xe6\xb3\x95,\xe5\x90\xaf\xe5\x8a\xa8\xe4\xb8\x80\xe4\xb8\xaa QueueRunner \xe5\xaf\xb9\xe8\xb1\xa1\xe7\x94\xa8\xe4\xba\x8e\xe4\xbf\x9d\xe5\xad\x98\xe9\xa2\x84\xe5\x85\x88\xe5\x87\x86\xe5\xa4\x87\xe5\xa5\xbd\xe3\x80\x81 \n      # \xe5\x8d\xb3\xe5\xb0\x86\xe8\xa2\xab\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe5\x87\x86\xe5\xa4\x87\xe5\xa5\xbd\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x94\xbe\xe5\x9c\xa8\xe7\xbc\x93\xe5\x86\xb2\xe5\x8c\xba\xe9\x98\x9f\xe5\x88\x97 batch_queue \xe4\xb8\xad\n      batch_queue = slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * deploy_config.num_clones)\n\n    ####################\n    # Define the model #\n    ####################\n    def clone_fn(batch_queue):\n      """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n      # \xe4\xbb\x8e batch_queue \xe4\xb8\xad\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\xac\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe6\x89\x80\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe2\x80\x94\xe2\x80\x94\xe2\x80\x94\xe2\x80\x94images \xe5\x92\x8c labels\n      images, labels = batch_queue.dequeue()\n      # \xe8\xb0\x83\xe7\x94\xa8 network_fn,\xe5\xbe\x97\xe5\x88\xb0 CNN \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f logits,\n      # \xe4\xbb\xa5\xe5\x8f\x8a\xe7\x94\xb1 CNN \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe6\x89\x80\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84\xe9\x9b\x86\xe5\x90\x88 end_points\n      logits, end_points = network_fn(images)\n\n      #############################\n      # Specify the loss function #\n      #############################\n      # \xe5\x9c\xa8\xe6\x9f\x90\xe4\xba\x9b CNN \xe6\xa8\xa1\xe5\x9e\x8b(\xe5\xa6\x82 Inception V3)\xe4\xb8\xad,\xe4\xb8\xba\xe4\xba\x86\xe5\x87\x8f\xe5\xb0\x91\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb6\x88\xe5\xa4\xb1\xe7\x8e\xb0\xe8\xb1\xa1,\n      # \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe9\x97\xb4\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe6\x88\x96\xe5\xa4\x9a\xe4\xb8\xaa\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xa2\xab\xe7\x94\xa8\xe4\xba\x8e\xe8\xbe\x85\xe5\x8a\xa9\xe5\x88\x86\xe7\xb1\xbb\xe3\x80\x82\xe8\xbf\x99\xe4\xba\x9b\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xba AuxLogits\n      if \'AuxLogits\' in end_points:\n        # \xe5\xb0\x86\xe8\xbe\x85\xe5\x8a\xa9\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\xe4\xb9\x9f\xe8\xae\xa1\xe7\xae\x97\xe5\x9c\xa8\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x95\xb4\xe4\xbd\x93\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe4\xb8\xad\xe3\x80\x82\n        # weight \xe5\x8f\x82\xe6\x95\xb0\xe8\xa1\xa8\xe7\xa4\xba\xe8\xbe\x85\xe5\x8a\xa9\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe5\x9c\xa8\xe8\xae\xa1\xe5\x85\xa5\xe6\x80\xbb\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xe6\x97\xb6\xe8\xa2\xab\xe4\xb9\x98\xe7\x9a\x84\xe6\x8a\x98\xe6\x89\xa3\xe7\xb3\xbb\xe6\x95\xb0\n        slim.losses.softmax_cross_entropy(\n            end_points[\'AuxLogits\'], labels,\n            label_smoothing=FLAGS.label_smoothing, weights=0.4,\n            scope=\'aux_loss\')\n      # \xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x80\xe5\x90\x8e\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\n      slim.losses.softmax_cross_entropy(\n          logits, labels, label_smoothing=FLAGS.label_smoothing, weights=1.0)\n      # \xe8\xbf\x94\xe5\x9b\x9e\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbc\xa0\xe9\x87\x8f\xe6\x89\x80\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84\xe9\x9b\x86\xe5\x90\x88\n      return end_points\n\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n    first_clone_scope = deploy_config.clone_scope(0)\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by network_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n    # Add summaries for end_points.\n    end_points = clones[0].outputs\n    for end_point in end_points:\n      x = end_points[end_point]\n      summaries.add(tf.summary.histogram(\'activations/\' + end_point, x))\n      summaries.add(tf.summary.scalar(\'sparsity/\' + end_point,\n                                      tf.nn.zero_fraction(x)))\n\n    # Add summaries for losses.\n    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n      summaries.add(tf.summary.scalar(\'losses/%s\' % loss.op.name, loss))\n\n    # Add summaries for variables.\n    for variable in slim.get_model_variables():\n      summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n    #################################\n    # Configure the moving averages #\n    #################################\n    if FLAGS.moving_average_decay:\n      # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xa1\xb0\xe5\x87\x8f\xe7\x8e\x87 FLAGS.moving_average_decay \xe7\x9a\x84\xe5\x80\xbc\xe8\xa2\xab\xe6\x8c\x87\xe5\xae\x9a,\n      # \xe5\x88\x99 moving_average_variables \xe8\xa1\xa8\xe7\xa4\xba\xe5\x85\xb7\xe6\x9c\x89\xe6\xbb\x91\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe7\x89\xb9\xe6\x80\xa7\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe5\x8f\x98\xe9\x87\x8f, \n      # variable_averages \xe8\xa1\xa8\xe7\xa4\xba\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe6\xbb\x91\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x8f\x98\xe9\x87\x8f\n      moving_average_variables = slim.get_model_variables()\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, global_step)\n    else:\n      moving_average_variables, variable_averages = None, None\n\n    #########################################\n    # Configure the optimization procedure. #\n    #########################################\n    with tf.device(deploy_config.optimizer_device()):\n      # \xe5\xbd\x93\xe5\x89\x8d,\xe5\x9c\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe9\x80\x9f\xe7\x8e\x87\xe7\x9a\x84\xe8\xb0\x83\xe6\x95\xb4\xe6\x96\xb9\xe9\x9d\xa2,\xe6\x94\xaf\xe6\x8c\x81 exponential\xe3\x80\x81fixed\xe3\x80\x81polynomial \xe8\xbf\x99\xe4\xb8\x89\xe7\xa7\x8d\xe7\xad\x96\xe7\x95\xa5\n      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n      # \xe6\xa0\xb9\xe6\x8d\xae FLAGS \xe6\x89\x80\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x88\x9b\xe5\xbb\xba\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8 optimizer\xe3\x80\x82\n      # \xe5\xbd\x93\xe5\x89\x8d\xe6\x94\xaf\xe6\x8c\x81 adadelta\xe3\x80\x81adagrad\xe3\x80\x81adam\xe3\x80\x81ftrl\xe3\x80\x81momentum\xe3\x80\x81rmsprop \xe5\x92\x8c sgd \xe8\xbf\x99\xe4\xb8\x83\xe7\xa7\x8d\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n\n    if FLAGS.sync_replicas:\n      # If sync_replicas is enabled, the averaging will be done in the chief\n      # queue runner.\n      # \xe5\xa6\x82 5.2 \xe8\x8a\x82\xe6\x89\x80\xe8\xbf\xb0,\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xae\xa1\xe7\xae\x97\xe6\x97\xb6,\xe9\x9c\x80\xe8\xa6\x81\xe5\xae\x9a\xe4\xb9\x89\xe5\x90\x8c\xe6\xad\xa5\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe3\x80\x82\n      # \xe5\xbd\x93\xe5\x89\x8d\xe5\xbc\x80\xe6\xba\x90\xe7\x9a\x84 train_image_classifier.py \xe5\xaf\xb9\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe6\x94\xaf\xe6\x8c\x81\xe8\xbf\x98\xe4\xb8\x8d\xe5\xae\x8c\xe5\x96\x84,\n      # \xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\xe9\x9c\x80\xe8\xa6\x81\xe9\x85\x8d\xe5\x90\x88 tf.train.ClusterSpec\xe3\x80\x81tf.train.Server \xe7\xad\x89\xe6\x8e\xa5\xe5\x8f\xa3\xe4\xb8\x80\xe8\xb5\xb7\xe4\xbd\xbf\xe7\x94\xa8,\xe6\x89\x8d\xe8\x83\xbd\xe5\xae\x9e\xe7\x8e\xb0\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xae\xad\xe7\xbb\x83\n      optimizer = tf.train.SyncReplicasOptimizer(\n          opt=optimizer,\n          replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n          total_num_replicas=FLAGS.worker_replicas,\n          variable_averages=variable_averages,\n          variables_to_average=moving_average_variables)\n    elif FLAGS.moving_average_decay:\n      # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xa1\xb0\xe5\x87\x8f\xe7\x8e\x87 FLAGS.moving_average_decay \xe7\x9a\x84\xe5\x80\xbc\xe8\xa2\xab\xe6\x8c\x87\xe5\xae\x9a,\xe5\x88\x99\xe5\xaf\xb9\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe9\x87\x87\xe5\x8f\x96\xe6\xbb\x91\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\x93\x8d\xe4\xbd\x9c\n      update_ops.append(variable_averages.apply(moving_average_variables))\n\n    # Variables to train.\n    # \xe6\xad\xa4\xe6\xae\xb5\xe4\xbb\xa3\xe7\xa0\x81\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8e 9.2.3 \xe8\x8a\x82\xe4\xbb\x8b\xe7\xbb\x8d\xe7\x9a\x84 deploy \xe6\x96\xb9\xe6\xb3\x95\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe4\xbb\xa3\xe7\xa0\x81(\xe5\xbd\x93 optimizer \xe9\x9d\x9e None \xe6\x97\xb6)\n    variables_to_train = _get_variables_to_train()\n\n    #  and returns a train_tensor and summary_op\n    total_loss, clones_gradients = model_deploy.optimize_clones(\n        clones,\n        optimizer,\n        var_list=variables_to_train)\n    # Add total_loss to summary.\n    summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    # Create gradient updates.\n    grad_updates = optimizer.apply_gradients(clones_gradients,\n                                             global_step=global_step)\n    update_ops.append(grad_updates)\n\n    update_op = tf.group(*update_ops)\n    with tf.control_dependencies([update_op]):\n      train_tensor = tf.identity(total_loss, name=\'train_op\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n\n    # Merge all summaries together.\n    summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n    ###########################\n    # Kicks off the training. #\n    ###########################\n    slim.learning.train(\n        train_tensor, # \xe5\x8d\x95\xe6\xad\xa5\xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\n        logdir=FLAGS.train_dir, # \xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe6\x97\xa5\xe5\xbf\x97\xe5\x92\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xa3\x80\xe6\x9f\xa5\xe7\x82\xb9\xe6\x96\x87\xe4\xbb\xb6\xe7\xad\x89\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\xe7\x9b\xae\xe5\xbd\x95\n        master=FLAGS.master, # master \xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80,\xe5\x9c\xa8\xe5\x8d\x95\xe6\x9c\xba\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe7\x94\xa8\xe5\x88\xb0\n        is_chief=(FLAGS.task == 0), # \xe5\xbd\x93\xe5\x89\x8d worker \xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\xba chief worker(\xe5\x9c\xa8\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xae\xad\xe7\xbb\x83\xe5\x9c\xba\xe6\x99\xaf\xe4\xb8\xad\xe7\x94\xa8\xe5\x88\xb0)\n        init_fn=_get_init_fn(), # \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x87\xbd\xe6\x95\xb0\n        summary_op=summary_op, # summary \xe6\x93\x8d\xe4\xbd\x9c\n        number_of_steps=FLAGS.max_number_of_steps, # \xe6\x9c\x80\xe5\xa4\xa7\xe8\xae\xad\xe7\xbb\x83\xe6\xad\xa5\xe6\x95\xb0\n        log_every_n_steps=FLAGS.log_every_n_steps, # \xe8\xbe\x93\xe5\x87\xba\xe6\x97\xa5\xe5\xbf\x97\xe7\x9a\x84\xe9\x97\xb4\xe9\x9a\x94(\xe4\xbb\xa5\xe6\xad\xa5\xe6\x95\xb0\xe4\xb8\xba\xe5\x8d\x95\xe4\xbd\x8d)\n        save_summaries_secs=FLAGS.save_summaries_secs, # \xe8\xbe\x93\xe5\x87\xba summary \xe6\x97\xa5\xe5\xbf\x97\xe7\x9a\x84\xe9\x97\xb4\xe9\x9a\x94(\xe4\xbb\xa5\xe7\xa7\x92\xe4\xb8\xba\xe5\x8d\x95\xe4\xbd\x8d)\n        save_interval_secs=FLAGS.save_interval_secs, # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xa3\x80\xe6\x9f\xa5\xe7\x82\xb9\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe9\x97\xb4\xe9\x9a\x94(\xe4\xbb\xa5\xe7\xa7\x92\xe4\xb8\xba\xe5\x8d\x95\xe4\xbd\x8d)\n        sync_optimizer=optimizer if FLAGS.sync_replicas else None) # \xe5\x90\x8c\xe6\xad\xa5\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8(\xe5\x9c\xa8\xe5\x8d\x95\xe6\x9c\xba\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe4\xb8\xba None)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
