file_path,api_count,code
conan_build.py,0,"b'from conan.packager import ConanMultiPackager\nimport os\n\nif __name__ == ""__main__"":\n    if os.getenv(""CXX"") == ""g++-7"":\n        version = os.getenv(""TRAVIS_TAG"")\n        if not version:\n            version = ""dev""\n        reference = ""frugally-deep/%s"" % version\n        username = ""dobiasd""\n        channel = ""stable""\n        upload_remote = ""https://api.bintray.com/conan/dobiasd/public-conan"" if version is not ""dev"" else None\n\n        print(""Conan package metadata:"", reference, username, channel, upload_remote)\n\n        builder = ConanMultiPackager(reference=reference,\n                                    username=username,\n                                    channel=channel,\n                                    upload=upload_remote)\n        builder.add(settings={\n            \'os\': \'Linux\',\n            \'compiler.version\': \'7\',\n            \'compiler.libcxx\': \'libstdc++11\',\n            \'arch\': \'x86_64\',\n            \'build_type\': \'Release\',\n            \'compiler\': \'gcc\'\n        })\n        builder.run()\n'"
conanfile.py,0,"b'from conans import ConanFile\n\n\nclass FrugallyDeepConan(ConanFile):\n    name = ""frugally-deep""\n    license = ""The MIT License (MIT)""\n    url = ""https://github.com/Dobiasd/frugally-deep""\n    description = ""Use Keras models in C++ with ease""\n    exports_sources = [""include*"", ""LICENSE""]\n    requires = (""eigen/3.3.7@conan/stable"",\n                ""functionalplus/v0.2.6-p0@dobiasd/stable"",\n                ""jsonformoderncpp/3.1.0@vthiery/stable"")\n\n    def package(self):\n        self.copy(""*LICENSE*"", dst=""licenses"")\n        self.copy(""*.h"", dst=""."")\n        self.copy(""*.hpp"", dst=""."")\n'"
keras_export/convert_model.py,0,"b'#!/usr/bin/env python3\n""""""Convert a Keras model to frugally-deep format.\n""""""\n\nimport base64\nimport datetime\nimport hashlib\nimport json\nimport sys\n\nimport numpy as np\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, Embedding\nfrom tensorflow.keras.models import Model, load_model\n\n__author__ = ""Tobias Hermann""\n__copyright__ = ""Copyright 2017, Tobias Hermann""\n__license__ = ""MIT""\n__maintainer__ = ""Tobias Hermann, https://github.com/Dobiasd/frugally-deep""\n__email__ = ""editgym@gmail.com""\n\nSTORE_FLOATS_HUMAN_READABLE = False\n\n\ndef transform_input_kernel(kernel):\n    """"""Transforms weights of a single CuDNN input kernel into the regular Keras format.""""""\n    return kernel.T.reshape(kernel.shape, order=\'F\')\n\n\ndef transform_recurrent_kernel(kernel):\n    """"""Transforms weights of a single CuDNN recurrent kernel into the regular Keras format.""""""\n    return kernel.T\n\n\ndef transform_kernels(kernels, n_gates, transform_func):\n    """"""\n    Transforms CuDNN kernel matrices (either LSTM or GRU) into the regular Keras format.\n\n    Parameters\n    ----------\n    kernels : numpy.ndarray\n        Composite matrix of input or recurrent kernels.\n    n_gates : int\n        Number of recurrent unit gates, 3 for GRU, 4 for LSTM.\n    transform_func: function(numpy.ndarray)\n        Function to apply to each input or recurrent kernel.\n\n    Returns\n    -------\n    numpy.ndarray\n        Transformed composite matrix of input or recurrent kernels in C-contiguous layout.\n    """"""\n    return np.require(np.hstack([transform_func(kernel) for kernel in np.hsplit(kernels, n_gates)]), requirements=\'C\')\n\n\ndef transform_bias(bias):\n    """"""Transforms bias weights of an LSTM layer into the regular Keras format.""""""\n    return np.sum(np.split(bias, 2, axis=0), axis=0)\n\n\ndef write_text_file(path, text):\n    """"""Write a string to a file""""""\n    with open(path, ""w"") as text_file:\n        print(text, file=text_file)\n\n\ndef int_or_none(value):\n    """"""Leave None values as is, convert everything else to int""""""\n    if value is None:\n        return value\n    return int(value)\n\n\ndef keras_shape_to_fdeep_tensor_shape(raw_shape):\n    """"""Convert a keras shape to an fdeep shape""""""\n    return singleton_list_to_value(raw_shape)[1:]\n\n\ndef get_layer_input_shape_tensor_shape(layer):\n    """"""Convert layer input shape to an fdeep shape""""""\n    return keras_shape_to_fdeep_tensor_shape(layer.input_shape)\n\n\ndef show_tensor(tens):\n    """"""Serialize 3-tensor to a dict""""""\n    return {\n        \'shape\': tens.shape[1:],\n        \'values\': encode_floats(tens.flatten())\n    }\n\n\ndef get_model_input_layers(model):\n    """"""Works for different Keras version.""""""\n    if hasattr(model, \'_input_layers\'):\n        return model._input_layers\n    if hasattr(model, \'input_layers\'):\n        return model.input_layers\n    raise ValueError(\'can not get (_)input_layers from model\')\n\n\ndef measure_predict(model, data_in):\n    """"""Returns output and duration in seconds""""""\n    start_time = datetime.datetime.now()\n    data_out = model.predict(data_in)\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n    print(\'Forward pass took {} s.\'.format(duration.total_seconds()))\n    return data_out, duration.total_seconds()\n\n\ndef replace_none_with(value, shape):\n    """"""Replace every None with a fixed value.""""""\n    return tuple(list(map(lambda x: x if x is not None else value, shape)))\n\n\ndef are_embedding_layer_positions_ok_for_testing(model):\n    """"""\n    Test data can only be generated if all embeddings layers\n    are positioned directly behind the input nodes\n    """"""\n\n    def embedding_layer_names(model):\n        layers = model.layers\n        result = set()\n        for layer in layers:\n            if isinstance(layer, Embedding):\n                result.add(layer.name)\n        layer_type = type(layer).__name__\n        if layer_type in [\'Model\', \'Sequential\']:\n            result.union(embedding_layer_names(layer))\n        return result\n\n    def embedding_layer_names_at_input_nodes(model):\n        result = set()\n        for input_layer in get_model_input_layers(model):\n            if input_layer._outbound_nodes and isinstance(\n                    input_layer._outbound_nodes[0].outbound_layer, Embedding):\n                result.add(input_layer._outbound_nodes[0].outbound_layer.name)\n        return set(result)\n\n    return embedding_layer_names(model) == embedding_layer_names_at_input_nodes(model)\n\n\ndef gen_test_data(model):\n    """"""Generate data for model verification test.""""""\n\n    def set_shape_idx_0_to_1_if_none(shape):\n        """"""Change first element in tuple to 1.""""""\n        if shape[0] is not None:\n            return shape\n        shape_lst = list(shape)\n        shape_lst[0] = 1\n        shape = tuple(shape_lst)\n        return shape\n\n    def generate_input_data(input_layer):\n        """"""Random data fitting the input shape of a layer.""""""\n        if input_layer._outbound_nodes and isinstance(\n                input_layer._outbound_nodes[0].outbound_layer, Embedding):\n            random_fn = lambda size: np.random.randint(\n                0, input_layer._outbound_nodes[0].outbound_layer.input_dim, size)\n        else:\n            random_fn = np.random.normal\n        try:\n            shape = input_layer.batch_input_shape\n        except AttributeError:\n            shape = input_layer.input_shape\n        return random_fn(\n            size=replace_none_with(32, set_shape_idx_0_to_1_if_none(singleton_list_to_value(shape)))).astype(np.float32)\n\n    assert are_embedding_layer_positions_ok_for_testing(\n        model), ""Test data can only be generated if embedding layers are positioned directly after input nodes.""\n\n    data_in = list(map(generate_input_data, get_model_input_layers(model)))\n\n    warm_up_runs = 3\n    test_runs = 5\n    for i in range(warm_up_runs):\n        if i == 0:\n            # store the results of first call for the test\n            # this is because states of recurrent layers is 0.\n            # cannot call model.reset_states() in some cases in keras without an error.\n            # an error occurs when recurrent layer is stateful and the initial state is passed as input\n            data_out_test, duration = measure_predict(model, data_in)\n        else:\n            measure_predict(model, data_in)\n    duration_sum = 0\n    print(\'Starting performance measurements.\')\n    for _ in range(test_runs):\n        data_out, duration = measure_predict(model, data_in)\n        duration_sum = duration_sum + duration\n    duration_avg = duration_sum / test_runs\n    print(\'Forward pass took {} s on average.\'.format(duration_avg))\n    return {\n        \'inputs\': list(map(show_tensor, as_list(data_in))),\n        \'outputs\': list(map(show_tensor, as_list(data_out_test)))\n    }\n\n\ndef split_every(size, seq):\n    """"""Split a sequence every seq elements.""""""\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n\ndef encode_floats(arr):\n    """"""Serialize a sequence of floats.""""""\n    if STORE_FLOATS_HUMAN_READABLE:\n        return arr.flatten().tolist()\n    return list(split_every(1024, base64.b64encode(arr).decode(\'ascii\')))\n\n\ndef prepare_filter_weights_conv_2d(weights):\n    """"""Change dimension order of 2d filter weights to the one used in fdeep""""""\n    assert len(weights.shape) == 4\n    return np.moveaxis(weights, [0, 1, 2, 3], [1, 2, 3, 0]).flatten()\n\n\ndef prepare_filter_weights_slice_conv_2d(weights):\n    """"""Change dimension order of 2d filter weights to the one used in fdeep""""""\n    assert len(weights.shape) == 4\n    return np.moveaxis(weights, [0, 1, 2, 3], [1, 2, 0, 3]).flatten()\n\n\ndef prepare_filter_weights_conv_1d(weights):\n    """"""Change dimension order of 1d filter weights to the one used in fdeep""""""\n    assert len(weights.shape) == 3\n    return np.moveaxis(weights, [0, 1, 2], [1, 2, 0]).flatten()\n\n\ndef show_conv_1d_layer(layer):\n    """"""Serialize Conv1D layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1 or len(weights) == 2\n    assert len(weights[0].shape) == 3\n    weights_flat = prepare_filter_weights_conv_1d(weights[0])\n    assert layer.padding in [\'valid\', \'same\', \'causal\']\n    assert len(layer.input_shape) == 3\n    assert layer.input_shape[0] in {None, 1}\n    result = {\n        \'weights\': encode_floats(weights_flat)\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_conv_2d_layer(layer):\n    """"""Serialize Conv2D layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1 or len(weights) == 2\n    assert len(weights[0].shape) == 4\n    weights_flat = prepare_filter_weights_conv_2d(weights[0])\n    assert layer.padding in [\'valid\', \'same\']\n    assert len(layer.input_shape) == 4\n    assert layer.input_shape[0] in {None, 1}\n    result = {\n        \'weights\': encode_floats(weights_flat)\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_separable_conv_2d_layer(layer):\n    """"""Serialize SeparableConv2D layer to dict""""""\n    weights = layer.get_weights()\n    assert layer.depth_multiplier == 1\n    assert len(weights) == 2 or len(weights) == 3\n    assert len(weights[0].shape) == 4\n    assert len(weights[1].shape) == 4\n\n    # probably incorrect for depth_multiplier > 1?\n    slice_weights = prepare_filter_weights_slice_conv_2d(weights[0])\n    stack_weights = prepare_filter_weights_conv_2d(weights[1])\n\n    assert layer.padding in [\'valid\', \'same\']\n    assert len(layer.input_shape) == 4\n    assert layer.input_shape[0] in {None, 1}\n    result = {\n        \'slice_weights\': encode_floats(slice_weights),\n        \'stack_weights\': encode_floats(stack_weights),\n    }\n    if len(weights) == 3:\n        bias = weights[2]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_depthwise_conv_2d_layer(layer):\n    """"""Serialize DepthwiseConv2D layer to dict""""""\n    weights = layer.get_weights()\n    assert layer.depth_multiplier == 1\n    assert len(weights) in [1, 2]\n    assert len(weights[0].shape) == 4\n\n    # probably incorrect for depth_multiplier > 1?\n    slice_weights = prepare_filter_weights_slice_conv_2d(weights[0])\n\n    assert layer.padding in [\'valid\', \'same\']\n    assert len(layer.input_shape) == 4\n    assert layer.input_shape[0] in {None, 1}\n    result = {\n        \'slice_weights\': encode_floats(slice_weights),\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_batch_normalization_layer(layer):\n    """"""Serialize batch normalization layer to dict""""""\n    moving_mean = K.get_value(layer.moving_mean)\n    moving_variance = K.get_value(layer.moving_variance)\n    result = {}\n    result[\'moving_mean\'] = encode_floats(moving_mean)\n    result[\'moving_variance\'] = encode_floats(moving_variance)\n    if layer.center:\n        beta = K.get_value(layer.beta)\n        result[\'beta\'] = encode_floats(beta)\n    if layer.scale:\n        gamma = K.get_value(layer.gamma)\n        result[\'gamma\'] = encode_floats(gamma)\n    return result\n\n\ndef show_dense_layer(layer):\n    """"""Serialize dense layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1 or len(weights) == 2\n    assert len(weights[0].shape) == 2\n    weights_flat = weights[0].flatten()\n    result = {\n        \'weights\': encode_floats(weights_flat)\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_prelu_layer(layer):\n    """"""Serialize prelu layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1\n    weights_flat = weights[0].flatten()\n    result = {\n        \'alpha\': encode_floats(weights_flat)\n    }\n    return result\n\n\ndef show_relu_layer(layer):\n    """"""Serialize relu layer to dict""""""\n    assert layer.negative_slope == 0\n    assert layer.threshold == 0\n    return {}\n\n\ndef show_embedding_layer(layer):\n    """"""Serialize Embedding layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1\n    result = {\n        \'weights\': encode_floats(weights[0])\n    }\n    return result\n\n\ndef show_lstm_layer(layer):\n    """"""Serialize LSTM layer to dict""""""\n    assert not layer.go_backwards\n    assert not layer.unroll\n    weights = layer.get_weights()\n    if isinstance(layer.input, list):\n        assert len(layer.input) in [1, 3]\n    assert len(weights) == 2 or len(weights) == 3\n    result = {\'weights\': encode_floats(weights[0]),\n              \'recurrent_weights\': encode_floats(weights[1])}\n\n    if len(weights) == 3:\n        result[\'bias\'] = encode_floats(weights[2])\n\n    return result\n\n\ndef show_gru_layer(layer):\n    """"""Serialize GRU layer to dict""""""\n    assert not layer.go_backwards\n    assert not layer.unroll\n    assert not layer.return_state\n    weights = layer.get_weights()\n    assert len(weights) == 2 or len(weights) == 3\n    result = {\'weights\': encode_floats(weights[0]),\n              \'recurrent_weights\': encode_floats(weights[1])}\n\n    if len(weights) == 3:\n        result[\'bias\'] = encode_floats(weights[2])\n\n    return result\n\n\ndef transform_cudnn_weights(input_weights, recurrent_weights, n_gates):\n    return transform_kernels(input_weights, n_gates, transform_input_kernel), \\\n           transform_kernels(recurrent_weights, n_gates, transform_recurrent_kernel)\n\n\ndef show_cudnn_lstm_layer(layer):\n    """"""Serialize a GPU-trained LSTM layer to dict""""""\n    weights = layer.get_weights()\n    if isinstance(layer.input, list):\n        assert len(layer.input) in [1, 3]\n    assert len(weights) == 3  # CuDNN LSTM always has a bias\n\n    n_gates = 4\n    input_weights, recurrent_weights = transform_cudnn_weights(weights[0], weights[1], n_gates)\n\n    result = {\'weights\': encode_floats(input_weights),\n              \'recurrent_weights\': encode_floats(recurrent_weights),\n              \'bias\': encode_floats(transform_bias(weights[2]))}\n\n    return result\n\n\ndef show_cudnn_gru_layer(layer):\n    """"""Serialize a GPU-trained GRU layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 3  # CuDNN GRU always has a bias\n\n    n_gates = 3\n    input_weights, recurrent_weights = transform_cudnn_weights(weights[0], weights[1], n_gates)\n\n    result = {\'weights\': encode_floats(input_weights),\n              \'recurrent_weights\': encode_floats(recurrent_weights),\n              \'bias\': encode_floats(weights[2])}\n\n    return result\n\n\ndef get_transform_func(layer):\n    """"""Returns functions that can be applied to layer weights to transform them into the standard Keras format, if applicable.""""""\n    if layer.__class__.__name__ in [\'CuDNNGRU\', \'CuDNNLSTM\']:\n        if layer.__class__.__name__ == \'CuDNNGRU\':\n            n_gates = 3\n        elif layer.__class__.__name__ == \'CuDNNLSTM\':\n            n_gates = 4\n\n        input_transform_func = lambda kernels: transform_kernels(kernels, n_gates, transform_input_kernel)\n        recurrent_transform_func = lambda kernels: transform_kernels(kernels, n_gates, transform_recurrent_kernel)\n    else:\n        input_transform_func = lambda kernels: kernels\n        recurrent_transform_func = lambda kernels: kernels\n\n    if layer.__class__.__name__ == \'CuDNNLSTM\':\n        bias_transform_func = transform_bias\n    else:\n        bias_transform_func = lambda bias: bias\n\n    return input_transform_func, recurrent_transform_func, bias_transform_func\n\n\ndef show_bidirectional_layer(layer):\n    """"""Serialize Bidirectional layer to dict""""""\n    forward_weights = layer.forward_layer.get_weights()\n    assert len(forward_weights) == 2 or len(forward_weights) == 3\n    forward_input_transform_func, forward_recurrent_transform_func, forward_bias_transform_func = get_transform_func(\n        layer.forward_layer)\n\n    backward_weights = layer.backward_layer.get_weights()\n    assert len(backward_weights) == 2 or len(backward_weights) == 3\n    backward_input_transform_func, backward_recurrent_transform_func, backward_bias_transform_func = get_transform_func(\n        layer.backward_layer)\n\n    result = {\'forward_weights\': encode_floats(forward_input_transform_func(forward_weights[0])),\n              \'forward_recurrent_weights\': encode_floats(forward_recurrent_transform_func(forward_weights[1])),\n              \'backward_weights\': encode_floats(backward_input_transform_func(backward_weights[0])),\n              \'backward_recurrent_weights\': encode_floats(backward_recurrent_transform_func(backward_weights[1]))}\n\n    if len(forward_weights) == 3:\n        result[\'forward_bias\'] = encode_floats(forward_bias_transform_func(forward_weights[2]))\n    if len(backward_weights) == 3:\n        result[\'backward_bias\'] = encode_floats(backward_bias_transform_func(backward_weights[2]))\n\n    return result\n\n\ndef show_input_layer(layer):\n    """"""Serialize input layer to dict""""""\n    assert not layer.sparse\n    return {}\n\n\ndef show_softmax_layer(layer):\n    """"""Serialize softmax layer to dict""""""\n    assert layer.axis == -1\n\n\ndef show_reshape_layer(layer):\n    """"""Serialize reshape layer to dict""""""\n    for dim_size in layer.target_shape:\n        assert dim_size != -1, \'Reshape inference not supported\'\n\n\ndef get_layer_functions_dict():\n    return {\n        \'Conv1D\': show_conv_1d_layer,\n        \'Conv2D\': show_conv_2d_layer,\n        \'SeparableConv2D\': show_separable_conv_2d_layer,\n        \'DepthwiseConv2D\': show_depthwise_conv_2d_layer,\n        \'BatchNormalization\': show_batch_normalization_layer,\n        \'Dense\': show_dense_layer,\n        \'PReLU\': show_prelu_layer,\n        \'ReLU\': show_relu_layer,\n        \'Embedding\': show_embedding_layer,\n        \'LSTM\': show_lstm_layer,\n        \'GRU\': show_gru_layer,\n        \'CuDNNLSTM\': show_cudnn_lstm_layer,\n        \'CuDNNGRU\': show_cudnn_gru_layer,\n        \'Bidirectional\': show_bidirectional_layer,\n        \'TimeDistributed\': show_time_distributed_layer,\n        \'Input\': show_input_layer,\n        \'Softmax\': show_softmax_layer\n    }\n\n\ndef show_time_distributed_layer(layer):\n    show_layer_functions = get_layer_functions_dict()\n    config = layer.get_config()\n    class_name = config[\'layer\'][\'class_name\']\n\n    if class_name in show_layer_functions:\n\n        if len(layer.input_shape) == 3:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2])\n        elif len(layer.input_shape) == 4:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2], layer.input_shape[3])\n        elif len(layer.input_shape) == 5:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2], layer.input_shape[3], layer.input_shape[4])\n        elif len(layer.input_shape) == 6:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2], layer.input_shape[3], layer.input_shape[4],\n                               layer.input_shape[5])\n        else:\n            raise Exception(\'Wrong input shape\')\n\n        layer_function = show_layer_functions[class_name]\n        attributes = dir(layer.layer)\n\n        class CopiedLayer:\n            pass\n\n        copied_layer = CopiedLayer()\n\n        for attr in attributes:\n            try:\n                if attr not in [\'input_shape\', \'__class__\']:\n                    setattr(copied_layer, attr, getattr(layer.layer, attr))\n                elif attr == \'input_shape\':\n                    setattr(copied_layer, \'input_shape\', input_shape_new)\n            except Exception:\n                continue\n\n        setattr(copied_layer, ""output_shape"", getattr(layer, ""output_shape""))\n\n        return layer_function(copied_layer)\n\n    else:\n        return None\n\n\ndef get_dict_keys(d):\n    """"""Return keys of a dictionary""""""\n    return [key for key in d]\n\n\ndef merge_two_disjunct_dicts(x, y):\n    """"""Given two dicts, merge them into a new dict as a shallow copy.\n    No Key is allowed to be present in both dictionaries.\n    """"""\n    assert set(get_dict_keys(x)).isdisjoint(get_dict_keys(y))\n    z = x.copy()\n    z.update(y)\n    return z\n\n\ndef is_ascii(some_string):\n    """"""Check if a string only contains ascii characters""""""\n    try:\n        some_string.encode(\'ascii\')\n    except UnicodeEncodeError:\n        return False\n    else:\n        return True\n\n\ndef get_all_weights(model):\n    """"""Serialize all weights of the models layers""""""\n    show_layer_functions = get_layer_functions_dict()\n    result = {}\n    layers = model.layers\n    assert K.image_data_format() == \'channels_last\'\n    for layer in layers:\n        layer_type = type(layer).__name__\n        if layer_type in [\'Model\', \'Sequential\']:\n            result = merge_two_disjunct_dicts(result, get_all_weights(layer))\n        else:\n            if hasattr(layer, \'data_format\'):\n                if layer_type in [\'AveragePooling1D\', \'MaxPooling1D\', \'AveragePooling2D\', \'MaxPooling2D\',\n                                  \'GlobalAveragePooling1D\', \'GlobalMaxPooling1D\', \'GlobalAveragePooling2D\',\n                                  \'GlobalMaxPooling2D\']:\n                    assert layer.data_format == \'channels_last\' or layer.data_format == \'channels_first\'\n                else:\n                    assert layer.data_format == \'channels_last\'\n\n            show_func = show_layer_functions.get(layer_type, None)\n            name = layer.name\n            assert is_ascii(name)\n            if name in result:\n                raise ValueError(\'duplicate layer name \' + name)\n            shown_layer = None\n            if show_func:\n                shown_layer = show_func(layer)\n            if shown_layer:\n                result[name] = shown_layer\n            if show_func and layer_type == \'TimeDistributed\':\n                if name not in result:\n                    result[name] = {}\n\n                result[name][\'td_input_len\'] = encode_floats(np.array([len(layer.input_shape) - 1], dtype=np.float32))\n                result[name][\'td_output_len\'] = encode_floats(np.array([len(layer.output_shape) - 1], dtype=np.float32))\n    return result\n\n\ndef get_model_name(model):\n    """"""Return .name or ._name or \'dummy_model_name\'""""""\n    if hasattr(model, \'name\'):\n        return model.name\n    if hasattr(model, \'_name\'):\n        return model._name\n    return \'dummy_model_name\'\n\n\ndef convert_sequential_to_model(model):\n    """"""Convert a sequential model to the underlying functional format""""""\n    if type(model).__name__ == \'Sequential\':\n        name = get_model_name(model)\n        if hasattr(model, \'_inbound_nodes\'):\n            inbound_nodes = model._inbound_nodes\n        elif hasattr(model, \'inbound_nodes\'):\n            inbound_nodes = model.inbound_nodes\n        else:\n            raise ValueError(\'can not get (_)inbound_nodes from model\')\n        input_layer = Input(batch_shape=model.layers[0].input_shape)\n        prev_layer = input_layer\n        for layer in model.layers:\n            layer._inbound_nodes = []\n            prev_layer = layer(prev_layer)\n        funcmodel = Model([input_layer], [prev_layer], name=name)\n        model = funcmodel\n        if hasattr(model, \'_inbound_nodes\'):\n            model._inbound_nodes = inbound_nodes\n        elif hasattr(model, \'inbound_nodes\'):\n            model.inbound_nodes = inbound_nodes\n    assert model.layers\n    for i in range(len(model.layers)):\n        if type(model.layers[i]).__name__ in [\'Model\', \'Sequential\']:\n            # ""model.layers[i] = ..."" would not overwrite the layer.\n            model._layers[i] = convert_sequential_to_model(model.layers[i])\n    return model\n\n\ndef offset_conv2d_eval(depth, padding, x):\n    """"""Perform a conv2d on x with a given padding""""""\n    kernel = K.variable(value=np.array([[[[1]] + [[0]] * (depth - 1)]]),\n                        dtype=\'float32\')\n    return K.conv2d(x, kernel, strides=(3, 3), padding=padding)\n\n\ndef offset_sep_conv2d_eval(depth, padding, x):\n    """"""Perform a separable conv2d on x with a given padding""""""\n    depthwise_kernel = K.variable(value=np.array([[[[1]] * depth]]),\n                                  dtype=\'float32\')\n    pointwise_kernel = K.variable(value=np.array([[[[1]] + [[0]] * (depth - 1)]]),\n                                  dtype=\'float32\')\n    return K.separable_conv2d(x, depthwise_kernel,\n                              pointwise_kernel, strides=(3, 3), padding=padding)\n\n\ndef conv2d_offset_max_pool_eval(_, padding, x):\n    """"""Perform a max pooling operation on x""""""\n    return K.pool2d(x, (1, 1), strides=(3, 3), padding=padding, pool_mode=\'max\')\n\n\ndef conv2d_offset_average_pool_eval(_, padding, x):\n    """"""Perform an average pooling operation on x""""""\n    return K.pool2d(x, (1, 1), strides=(3, 3), padding=padding, pool_mode=\'avg\')\n\n\ndef check_operation_offset(depth, eval_f, padding):\n    """"""Check if backend used an offset while placing the filter\n    e.g. during a convolution.\n    TensorFlow is inconsistent in doing so depending\n    on the type of operation, the used device (CPU/GPU) and the input depth.\n    """"""\n    in_arr = np.array([[[[i] * depth for i in range(6)]]])\n    input_data = K.variable(value=in_arr, dtype=\'float32\')\n    output = eval_f(depth, padding, input_data)\n    result = K.eval(output).flatten().tolist()\n    assert result in [[0, 3], [1, 4]]\n    return result == [1, 4]\n\n\ndef get_shapes(tensors):\n    """"""Return shapes of a list of tensors""""""\n    return [t[\'shape\'] for t in tensors]\n\n\ndef calculate_hash(model):\n    layers = model.layers\n    hash_m = hashlib.sha256()\n    for layer in layers:\n        for weights in layer.get_weights():\n            assert isinstance(weights, np.ndarray)\n            hash_m.update(weights.tobytes())\n        hash_m.update(layer.name.encode(\'ascii\'))\n    return hash_m.hexdigest()\n\n\ndef as_list(value_or_values):\n    """"""Leave lists untouched, convert non-list types to a singleton list""""""\n    if isinstance(value_or_values, list):\n        return value_or_values\n    return [value_or_values]\n\n\ndef singleton_list_to_value(value_or_values):\n    """"""\n    Leaves non-list values untouched.\n    Raises an Exception in case the input list does not have exactly one element.\n    """"""\n    if isinstance(value_or_values, list):\n        assert len(value_or_values) == 1\n        return value_or_values[0]\n    return value_or_values\n\n\ndef model_to_fdeep_json(model, no_tests=False):\n    """"""Convert any Keras model to the frugally-deep model format.""""""\n\n    # Force creation of underlying functional model.\n    # see: https://github.com/fchollet/keras/issues/8136\n    # Loss and optimizer type do not matter, since we do not train the model.\n    model.compile(loss=\'mse\', optimizer=\'sgd\')\n\n    model = convert_sequential_to_model(model)\n\n    test_data = None if no_tests else gen_test_data(model)\n\n    json_output = {}\n    print(\'Converting model architecture.\')\n    json_output[\'architecture\'] = json.loads(model.to_json())\n    json_output[\'image_data_format\'] = K.image_data_format()\n    json_output[\'input_shapes\'] = list(map(get_layer_input_shape_tensor_shape, get_model_input_layers(model)))\n    json_output[\'output_shapes\'] = list(map(keras_shape_to_fdeep_tensor_shape, as_list(model.output_shape)))\n\n    if test_data:\n        json_output[\'tests\'] = [test_data]\n\n    print(\'Converting model weights.\')\n    json_output[\'trainable_params\'] = get_all_weights(model)\n    print(\'Done converting model weights.\')\n\n    print(\'Calculating model hash.\')\n    json_output[\'hash\'] = calculate_hash(model)\n    print(\'Model conversion finished.\')\n\n    return json_output\n\n\ndef convert(in_path, out_path, no_tests=False):\n    """"""Convert any (h5-)stored Keras model to the frugally-deep model format.""""""\n\n    print(\'loading {}\'.format(in_path))\n    model = load_model(in_path)\n    json_output = model_to_fdeep_json(model, no_tests)\n    print(\'writing {}\'.format(out_path))\n    write_text_file(out_path, json.dumps(\n        json_output, allow_nan=False, indent=2, sort_keys=True))\n\n\ndef main():\n    """"""Parse command line and convert model.""""""\n\n    usage = \'usage: [Keras model in HDF5 format] [output path] (--no-tests)\'\n\n    # todo: Use ArgumentParser instead.\n    if len(sys.argv) not in [3, 4]:\n        print(usage)\n        sys.exit(1)\n\n    in_path = sys.argv[1]\n    out_path = sys.argv[2]\n\n    no_tests = False\n    if len(sys.argv) == 4:\n        if sys.argv[3] not in [\'--no-tests\']:\n            print(usage)\n            sys.exit(1)\n        if sys.argv[3] == \'--no-tests\':\n            no_tests = True\n\n    convert(in_path, out_path, no_tests)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
keras_export/generate_test_models.py,0,"b'#!/usr/bin/env python3\n""""""Generate a test model for frugally-deep.\n""""""\n\nimport sys\n\nimport numpy as np\nfrom tensorflow.keras.layers import BatchNormalization, Concatenate\nfrom tensorflow.keras.layers import Bidirectional, TimeDistributed\nfrom tensorflow.keras.layers import Conv1D, ZeroPadding1D, Cropping1D\nfrom tensorflow.keras.layers import Conv2D, ZeroPadding2D, Cropping2D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.layers import LSTM, GRU\nfrom tensorflow.keras.layers import LeakyReLU, ELU, PReLU\nfrom tensorflow.keras.layers import MaxPooling1D, AveragePooling1D, UpSampling1D\nfrom tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, UpSampling2D\nfrom tensorflow.keras.layers import Multiply, Add, Subtract, Average, Maximum\nfrom tensorflow.keras.layers import Permute, Reshape\nfrom tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\nfrom tensorflow.keras.models import Model, load_model, Sequential\n\n__author__ = ""Tobias Hermann""\n__copyright__ = ""Copyright 2017, Tobias Hermann""\n__license__ = ""MIT""\n__maintainer__ = ""Tobias Hermann, https://github.com/Dobiasd/frugally-deep""\n__email__ = ""editgym@gmail.com""\n\n\ndef replace_none_with(value, shape):\n    """"""Replace every None with a fixed value.""""""\n    return tuple(list(map(lambda x: x if x is not None else value, shape)))\n\n\ndef get_shape_for_random_data(data_size, shape):\n    """"""Include size of data to generate into shape.""""""\n    if len(shape) == 5:\n        return (data_size, shape[0], shape[1], shape[2], shape[3], shape[4])\n    if len(shape) == 4:\n        return (data_size, shape[0], shape[1], shape[2], shape[3])\n    if len(shape) == 3:\n        return (data_size, shape[0], shape[1], shape[2])\n    if len(shape) == 2:\n        return (data_size, shape[0], shape[1])\n    if len(shape) == 1:\n        return (data_size, shape[0])\n    raise ValueError(\'can not determine shape for random data\')\n\n\ndef generate_random_data(data_size, shape):\n    """"""Random data for training.""""""\n    return np.random.random(\n        size=get_shape_for_random_data(data_size, replace_none_with(42, shape)))\n\n\ndef generate_input_data(data_size, input_shapes):\n    """"""Random input data for training.""""""\n    return [generate_random_data(data_size, input_shape)\n            for input_shape in input_shapes]\n\n\ndef generate_integer_random_data(data_size, low, high, shape):\n    """"""Random data for training.""""""\n    return np.random.randint(\n        low=low, high=high, size=get_shape_for_random_data(data_size, replace_none_with(42, shape)))\n\n\ndef generate_integer_input_data(data_size, low, highs, input_shapes):\n    """"""Random input data for training.""""""\n    return [generate_integer_random_data(data_size, low, high, input_shape)\n            for high, input_shape in zip(highs, input_shapes)]\n\n\ndef as_list(value_or_values):\n    """"""Leave lists untouched, convert non-list types to a singleton list""""""\n    if isinstance(value_or_values, list):\n        return value_or_values\n    return [value_or_values]\n\n\ndef generate_output_data(data_size, outputs):\n    """"""Random output data for training.""""""\n    return [generate_random_data(data_size, output.shape[1:])\n            for output in as_list(outputs)]\n\n\ndef get_test_model_exhaustive():\n    """"""Returns a exhaustive test model.""""""\n    input_shapes = [\n        (2, 3, 4, 5, 6),\n        (2, 3, 4, 5, 6),\n        (7, 8, 9, 10),\n        (7, 8, 9, 10),\n        (11, 12, 13),\n        (11, 12, 13),\n        (14, 15),\n        (14, 15),\n        (16,),\n        (16,),\n        (2,),\n        (1,),\n        (2,),\n        (1,),\n        (1, 3),\n        (1, 4),\n        (1, 1, 3),\n        (1, 1, 4),\n        (1, 1, 1, 3),\n        (1, 1, 1, 4),\n        (1, 1, 1, 1, 3),\n        (1, 1, 1, 1, 4),\n        (26, 28, 3),\n        (4, 4, 3),\n        (4, 4, 3),\n        (4,),\n        (2, 3),\n        (1,),\n        (1,),\n        (1,),\n        (2, 3),\n        (9, 16, 1),\n        (1, 9, 16)\n    ]\n\n    inputs = [Input(shape=s) for s in input_shapes]\n\n    outputs = []\n\n    outputs.append(Conv1D(1, 3, padding=\'valid\')(inputs[6]))\n    outputs.append(Conv1D(2, 1, padding=\'same\')(inputs[6]))\n    outputs.append(Conv1D(3, 4, padding=\'causal\', dilation_rate=2)(inputs[6]))\n    outputs.append(ZeroPadding1D(2)(inputs[6]))\n    outputs.append(Cropping1D((2, 3))(inputs[6]))\n    outputs.append(MaxPooling1D(2)(inputs[6]))\n    outputs.append(MaxPooling1D(2, strides=2, padding=\'same\')(inputs[6]))\n    outputs.append(AveragePooling1D(2)(inputs[6]))\n    outputs.append(AveragePooling1D(2, strides=2, padding=\'same\')(inputs[6]))\n    outputs.append(GlobalMaxPooling1D()(inputs[6]))\n    outputs.append(GlobalMaxPooling1D(data_format=""channels_first"")(inputs[6]))\n    outputs.append(GlobalAveragePooling1D()(inputs[6]))\n    outputs.append(GlobalAveragePooling1D(data_format=""channels_first"")(inputs[6]))\n\n    outputs.append(Conv2D(4, (3, 3))(inputs[4]))\n    outputs.append(Conv2D(4, (3, 3), use_bias=False)(inputs[4]))\n    outputs.append(Conv2D(4, (2, 4), strides=(2, 3), padding=\'same\')(inputs[4]))\n    outputs.append(Conv2D(4, (2, 4), padding=\'same\', dilation_rate=(2, 3))(inputs[4]))\n\n    outputs.append(SeparableConv2D(3, (3, 3))(inputs[4]))\n    outputs.append(DepthwiseConv2D((3, 3))(inputs[4]))\n    outputs.append(DepthwiseConv2D((1, 2))(inputs[4]))\n\n    outputs.append(MaxPooling2D((2, 2))(inputs[4]))\n    # todo: check if TensorFlow >= 2.1 supports this\n    #outputs.append(MaxPooling2D((2, 2), data_format=""channels_first"")(inputs[4])) # Default MaxPoolingOp only supports NHWC on device type CPU\n    outputs.append(MaxPooling2D((1, 3), strides=(2, 3), padding=\'same\')(inputs[4]))\n    outputs.append(AveragePooling2D((2, 2))(inputs[4]))\n    # todo: check if TensorFlow >= 2.1 supports this\n    #outputs.append(AveragePooling2D((2, 2), data_format=""channels_first"")(inputs[4])) # Default AvgPoolingOp only supports NHWC on device type CPU\n    outputs.append(AveragePooling2D((1, 3), strides=(2, 3), padding=\'same\')(inputs[4]))\n\n    outputs.append(GlobalAveragePooling2D()(inputs[4]))\n    outputs.append(GlobalAveragePooling2D(data_format=""channels_first"")(inputs[4]))\n    outputs.append(GlobalMaxPooling2D()(inputs[4]))\n    outputs.append(GlobalMaxPooling2D(data_format=""channels_first"")(inputs[4]))\n\n    outputs.append(Permute((3, 4, 1, 5, 2))(inputs[0]))\n    outputs.append(Permute((1, 5, 3, 2, 4))(inputs[0]))\n    outputs.append(Permute((3, 4, 1, 2))(inputs[2]))\n    outputs.append(Permute((2, 1, 3))(inputs[4]))\n    outputs.append(Permute((2, 1))(inputs[6]))\n    outputs.append(Permute((1,))(inputs[8]))\n\n    outputs.append(Permute((3, 1, 2))(inputs[31]))\n    outputs.append(Permute((3, 1, 2))(inputs[32]))\n    outputs.append(BatchNormalization()(Permute((3, 1, 2))(inputs[31])))\n    outputs.append(BatchNormalization()(Permute((3, 1, 2))(inputs[32])))\n\n    outputs.append(BatchNormalization()(inputs[0]))\n    outputs.append(BatchNormalization(axis=1)(inputs[0]))\n    outputs.append(BatchNormalization(axis=2)(inputs[0]))\n    outputs.append(BatchNormalization(axis=3)(inputs[0]))\n    outputs.append(BatchNormalization(axis=4)(inputs[0]))\n    outputs.append(BatchNormalization(axis=5)(inputs[0]))\n    outputs.append(BatchNormalization()(inputs[2]))\n    outputs.append(BatchNormalization(axis=1)(inputs[2]))\n    outputs.append(BatchNormalization(axis=2)(inputs[2]))\n    outputs.append(BatchNormalization(axis=3)(inputs[2]))\n    outputs.append(BatchNormalization(axis=4)(inputs[2]))\n    outputs.append(BatchNormalization()(inputs[4]))\n    # todo: check if TensorFlow >= 2.1 supports this\n    #outputs.append(BatchNormalization(axis=1)(inputs[4])) # tensorflow.python.framework.errors_impl.InternalError:  The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.\n    outputs.append(BatchNormalization(axis=2)(inputs[4]))\n    outputs.append(BatchNormalization(axis=3)(inputs[4]))\n    outputs.append(BatchNormalization()(inputs[6]))\n    outputs.append(BatchNormalization(axis=1)(inputs[6]))\n    outputs.append(BatchNormalization(axis=2)(inputs[6]))\n    outputs.append(BatchNormalization()(inputs[8]))\n    outputs.append(BatchNormalization(axis=1)(inputs[8]))\n    outputs.append(BatchNormalization()(inputs[27]))\n    outputs.append(BatchNormalization(axis=1)(inputs[27]))\n    outputs.append(BatchNormalization()(inputs[14]))\n    outputs.append(BatchNormalization(axis=1)(inputs[14]))\n    outputs.append(BatchNormalization(axis=2)(inputs[14]))\n    outputs.append(BatchNormalization()(inputs[16]))\n    # todo: check if TensorFlow >= 2.1 supports this\n    #outputs.append(BatchNormalization(axis=1)(inputs[16])) # tensorflow.python.framework.errors_impl.InternalError:  The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.\n    outputs.append(BatchNormalization(axis=2)(inputs[16]))\n    outputs.append(BatchNormalization(axis=3)(inputs[16]))\n    outputs.append(BatchNormalization()(inputs[18]))\n    outputs.append(BatchNormalization(axis=1)(inputs[18]))\n    outputs.append(BatchNormalization(axis=2)(inputs[18]))\n    outputs.append(BatchNormalization(axis=3)(inputs[18]))\n    outputs.append(BatchNormalization(axis=4)(inputs[18]))\n    outputs.append(BatchNormalization()(inputs[20]))\n    outputs.append(BatchNormalization(axis=1)(inputs[20]))\n    outputs.append(BatchNormalization(axis=2)(inputs[20]))\n    outputs.append(BatchNormalization(axis=3)(inputs[20]))\n    outputs.append(BatchNormalization(axis=4)(inputs[20]))\n    outputs.append(BatchNormalization(axis=5)(inputs[20]))\n\n    outputs.append(Dropout(0.5)(inputs[4]))\n\n    outputs.append(ZeroPadding2D(2)(inputs[4]))\n    outputs.append(ZeroPadding2D((2, 3))(inputs[4]))\n    outputs.append(ZeroPadding2D(((1, 2), (3, 4)))(inputs[4]))\n    outputs.append(Cropping2D(2)(inputs[4]))\n    outputs.append(Cropping2D((2, 3))(inputs[4]))\n    outputs.append(Cropping2D(((1, 2), (3, 4)))(inputs[4]))\n\n    outputs.append(Dense(3, use_bias=True)(inputs[13]))\n    outputs.append(Dense(3, use_bias=True)(inputs[14]))\n    outputs.append(Dense(4, use_bias=False)(inputs[16]))\n    outputs.append(Dense(4, use_bias=False, activation=\'tanh\')(inputs[18]))\n    outputs.append(Dense(4, use_bias=False)(inputs[20]))\n\n    outputs.append(Reshape(((2 * 3 * 4 * 5 * 6),))(inputs[0]))\n    outputs.append(Reshape((2, 3 * 4 * 5 * 6))(inputs[0]))\n    outputs.append(Reshape((2, 3, 4 * 5 * 6))(inputs[0]))\n    outputs.append(Reshape((2, 3, 4, 5 * 6))(inputs[0]))\n    outputs.append(Reshape((2, 3, 4, 5, 6))(inputs[0]))\n\n    outputs.append(Reshape((16,))(inputs[8]))\n    outputs.append(Reshape((2, 8))(inputs[8]))\n    outputs.append(Reshape((2, 2, 4))(inputs[8]))\n    outputs.append(Reshape((2, 2, 2, 2))(inputs[8]))\n    outputs.append(Reshape((2, 2, 1, 2, 2))(inputs[8]))\n\n    outputs.append(UpSampling2D(size=(1, 2), interpolation=\'nearest\')(inputs[4]))\n    outputs.append(UpSampling2D(size=(5, 3), interpolation=\'nearest\')(inputs[4]))\n    outputs.append(UpSampling2D(size=(1, 2), interpolation=\'bilinear\')(inputs[4]))\n    outputs.append(UpSampling2D(size=(5, 3), interpolation=\'bilinear\')(inputs[4]))\n\n    for axis in [-5, -4, -3, -2, -1, 1, 2, 3, 4, 5]:\n        outputs.append(Concatenate(axis=axis)([inputs[0], inputs[1]]))\n    for axis in [-4, -3, -2, -1, 1, 2, 3, 4]:\n        outputs.append(Concatenate(axis=axis)([inputs[2], inputs[3]]))\n    for axis in [-3, -2, -1, 1, 2, 3]:\n        outputs.append(Concatenate(axis=axis)([inputs[4], inputs[5]]))\n    for axis in [-2, -1, 1, 2]:\n        outputs.append(Concatenate(axis=axis)([inputs[6], inputs[7]]))\n    for axis in [-1, 1]:\n        outputs.append(Concatenate(axis=axis)([inputs[8], inputs[9]]))\n    for axis in [-1, 2]:\n        outputs.append(Concatenate(axis=axis)([inputs[14], inputs[15]]))\n    for axis in [-1, 3]:\n        outputs.append(Concatenate(axis=axis)([inputs[16], inputs[17]]))\n    for axis in [-1, 4]:\n        outputs.append(Concatenate(axis=axis)([inputs[18], inputs[19]]))\n    for axis in [-1, 5]:\n        outputs.append(Concatenate(axis=axis)([inputs[20], inputs[21]]))\n\n    outputs.append(UpSampling1D(size=2)(inputs[6]))\n    # outputs.append(UpSampling1D(size=2)(inputs[8])) # ValueError: Input 0 of layer up_sampling1d_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 16]\n\n    outputs.append(Multiply()([inputs[10], inputs[11]]))\n    outputs.append(Multiply()([inputs[11], inputs[10]]))\n    outputs.append(Multiply()([inputs[11], inputs[13]]))\n    outputs.append(Multiply()([inputs[10], inputs[11], inputs[12]]))\n    outputs.append(Multiply()([inputs[11], inputs[12], inputs[13]]))\n\n    shared_conv = Conv2D(1, (1, 1),\n                         padding=\'valid\', name=\'shared_conv\', activation=\'relu\')\n\n    up_scale_2 = UpSampling2D((2, 2))\n    x1 = shared_conv(up_scale_2(inputs[23]))  # (1, 8, 8)\n    x2 = shared_conv(up_scale_2(inputs[24]))  # (1, 8, 8)\n    x3 = Conv2D(1, (1, 1), padding=\'valid\')(up_scale_2(inputs[24]))  # (1, 8, 8)\n    x = Concatenate()([x1, x2, x3])  # (3, 8, 8)\n    outputs.append(x)\n\n    x = Conv2D(3, (1, 1), padding=\'same\', use_bias=False)(x)  # (3, 8, 8)\n    outputs.append(x)\n    x = Dropout(0.5)(x)\n    outputs.append(x)\n    x = Concatenate()([\n        MaxPooling2D((2, 2))(x),\n        AveragePooling2D((2, 2))(x)])  # (6, 4, 4)\n    outputs.append(x)\n\n    x = Flatten()(x)  # (1, 1, 96)\n    x = Dense(4, use_bias=False)(x)\n    outputs.append(x)\n    x = Dense(3)(x)  # (1, 1, 3)\n    outputs.append(x)\n\n    outputs.append(Add()([inputs[26], inputs[30], inputs[30]]))\n    outputs.append(Subtract()([inputs[26], inputs[30]]))\n    outputs.append(Multiply()([inputs[26], inputs[30], inputs[30]]))\n    outputs.append(Average()([inputs[26], inputs[30], inputs[30]]))\n    outputs.append(Maximum()([inputs[26], inputs[30], inputs[30]]))\n    outputs.append(Concatenate()([inputs[26], inputs[30], inputs[30]]))\n\n    intermediate_input_shape = (3,)\n    intermediate_in = Input(intermediate_input_shape)\n    intermediate_x = intermediate_in\n    intermediate_x = Dense(8)(intermediate_x)\n    intermediate_x = Dense(5)(intermediate_x)\n    intermediate_model = Model(\n        inputs=[intermediate_in], outputs=[intermediate_x],\n        name=\'intermediate_model\')\n    intermediate_model.compile(loss=\'mse\', optimizer=\'nadam\')\n\n    x = intermediate_model(x)  # (1, 1, 5)\n\n    intermediate_model_2 = Sequential()\n    intermediate_model_2.add(Dense(7, input_shape=(5,)))\n    intermediate_model_2.add(Dense(5))\n    intermediate_model_2.compile(optimizer=\'rmsprop\',\n                                 loss=\'categorical_crossentropy\')\n\n    x = intermediate_model_2(x)  # (1, 1, 5)\n\n    x = Dense(3)(x)  # (1, 1, 3)\n\n    shared_activation = Activation(\'tanh\')\n\n    outputs = outputs + [\n        Activation(\'tanh\')(inputs[25]),\n        Activation(\'hard_sigmoid\')(inputs[25]),\n        Activation(\'selu\')(inputs[25]),\n        Activation(\'sigmoid\')(inputs[25]),\n        Activation(\'softplus\')(inputs[25]),\n        Activation(\'softmax\')(inputs[25]),\n        Activation(\'softmax\')(inputs[25]),\n        Activation(\'relu\')(inputs[25]),\n        LeakyReLU()(inputs[25]),\n        ELU()(inputs[25]),\n        PReLU()(inputs[24]),\n        PReLU()(inputs[25]),\n        PReLU()(inputs[26]),\n        shared_activation(inputs[25]),\n        Activation(\'linear\')(inputs[26]),\n        Activation(\'linear\')(inputs[23]),\n        x,\n        shared_activation(x),\n    ]\n\n    model = Model(inputs=inputs, outputs=outputs, name=\'test_model_exhaustive\')\n    model.compile(loss=\'mse\', optimizer=\'nadam\')\n\n    # fit to dummy data\n    training_data_size = 2\n    data_in = generate_input_data(training_data_size, input_shapes)\n    initial_data_out = model.predict(data_in)\n    data_out = generate_output_data(training_data_size, initial_data_out)\n    model.fit(data_in, data_out, epochs=10)\n    return model\n\n\ndef get_test_model_embedding():\n    """"""Returns a minimalistic test model for the embedding layer.""""""\n\n    input_dims = [\n        1023,  # maximum integer value in input data\n        255\n    ]\n    input_shapes = [\n        (100,),  # must be single-element tuple (for sequence length)\n        (1000,)\n    ]\n    assert len(input_dims) == len(input_shapes)\n    output_dims = [8, 3]  # embedding dimension\n\n    inputs = [Input(shape=s) for s in input_shapes]\n\n    outputs = []\n    for k in range(0, len(input_shapes)):\n        embedding = Embedding(input_dim=input_dims[k], output_dim=output_dims[k])(inputs[k])\n        lstm = LSTM(\n            units=4,\n            recurrent_activation=\'sigmoid\',\n            return_sequences=False\n        )(embedding)\n\n        outputs.append(lstm)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\'test_model_embedding\')\n    model.compile(loss=\'mse\', optimizer=\'adam\')\n\n    # fit to dummy data\n    training_data_size = 2\n    data_in = generate_integer_input_data(training_data_size, 0, input_dims, input_shapes)\n    initial_data_out = model.predict(data_in)\n    data_out = generate_output_data(training_data_size, initial_data_out)\n    model.fit(data_in, data_out, epochs=1)\n    return model\n\n\ndef get_test_model_recurrent():\n    """"""Returns a minimalistic test model for recurrent layers.""""""\n    input_shapes = [\n        (17, 4),\n        (1, 10),\n        (20, 40),\n        (6, 7, 10, 3)\n    ]\n\n    outputs = []\n\n    inputs = [Input(shape=s) for s in input_shapes]\n\n    inp = PReLU()(inputs[0])\n\n    lstm = Bidirectional(LSTM(units=4,\n                              return_sequences=True,\n                              bias_initializer=\'random_uniform\',  # default is zero use random to test computation\n                              activation=\'tanh\',\n                              recurrent_activation=\'relu\'), merge_mode=\'concat\')(inp)\n\n    lstm2 = Bidirectional(LSTM(units=6,\n                               return_sequences=True,\n                               bias_initializer=\'random_uniform\',\n                               activation=\'elu\',\n                               recurrent_activation=\'hard_sigmoid\'), merge_mode=\'sum\')(lstm)\n\n    lstm3 = LSTM(units=10,\n                 return_sequences=False,\n                 bias_initializer=\'random_uniform\',\n                 activation=\'selu\',\n                 recurrent_activation=\'sigmoid\')(lstm2)\n\n    outputs.append(lstm3)\n\n    conv1 = Conv1D(2, 1, activation=\'sigmoid\')(inputs[1])\n    lstm4 = LSTM(units=15,\n                 return_sequences=False,\n                 bias_initializer=\'random_uniform\',\n                 activation=\'tanh\',\n                 recurrent_activation=\'elu\')(conv1)\n\n    dense = (Dense(23, activation=\'sigmoid\'))(lstm4)\n    outputs.append(dense)\n\n    time_dist_1 = TimeDistributed(Conv2D(2, (3, 3), use_bias=True))(inputs[3])\n    flatten_1 = TimeDistributed(Flatten())(time_dist_1)\n\n    outputs.append(Bidirectional(LSTM(units=6,\n                                      return_sequences=True,\n                                      bias_initializer=\'random_uniform\',\n                                      activation=\'tanh\',\n                                      recurrent_activation=\'sigmoid\'), merge_mode=\'ave\')(flatten_1))\n\n    outputs.append(TimeDistributed(MaxPooling2D(2, 2))(inputs[3]))\n    outputs.append(TimeDistributed(AveragePooling2D(2, 2))(inputs[3]))\n\n    model = Model(inputs=inputs, outputs=outputs, name=\'test_model_recurrent\')\n    model.compile(loss=\'mse\', optimizer=\'nadam\')\n\n    # fit to dummy data\n    training_data_size = 2\n    data_in = generate_input_data(training_data_size, input_shapes)\n    initial_data_out = model.predict(data_in)\n    data_out = generate_output_data(training_data_size, initial_data_out)\n    model.fit(data_in, data_out, epochs=10)\n    return model\n\n\ndef get_test_model_lstm():\n    """"""Returns a test model for Long Short-Term Memory (LSTM) layers.""""""\n\n    input_shapes = [\n        (17, 4),\n        (1, 10),\n        (None, 4),\n        (12,),\n        (12,)\n    ]\n    inputs = [Input(shape=s) for s in input_shapes]\n    outputs = []\n\n    for inp in inputs[:2]:\n        lstm_sequences = LSTM(\n            units=8,\n            recurrent_activation=\'relu\',\n            return_sequences=True\n        )(inp)\n        lstm_regular = LSTM(\n            units=3,\n            recurrent_activation=\'sigmoid\',\n            return_sequences=False\n        )(lstm_sequences)\n        outputs.append(lstm_regular)\n        lstm_state, state_h, state_c = LSTM(\n            units=3,\n            recurrent_activation=\'sigmoid\',\n            return_state=True\n        )(inp)\n        outputs.append(lstm_state)\n        outputs.append(state_h)\n        outputs.append(state_c)\n\n        lstm_bidi_sequences = Bidirectional(\n            LSTM(\n                units=4,\n                recurrent_activation=\'hard_sigmoid\',\n                return_sequences=True\n            )\n        )(inp)\n        lstm_bidi = Bidirectional(\n            LSTM(\n                units=6,\n                recurrent_activation=\'linear\',\n                return_sequences=False\n            )\n        )(lstm_bidi_sequences)\n        outputs.append(lstm_bidi)\n\n        lstm_gpu_regular = LSTM(\n            units=3,\n            activation=\'tanh\',\n            recurrent_activation=\'sigmoid\',\n            use_bias=True\n        )(inp)\n\n        lstm_gpu_bidi = Bidirectional(\n            LSTM(\n                units=3,\n                activation=\'tanh\',\n                recurrent_activation=\'sigmoid\',\n                use_bias=True\n            )\n        )(inp)\n    outputs.append(lstm_gpu_regular)\n    outputs.append(lstm_gpu_bidi)\n\n    outputs.extend(LSTM(units=12, return_sequences=True,\n                        return_state=True)(inputs[2], initial_state=[inputs[3], inputs[4]]))\n\n    model = Model(inputs=inputs, outputs=outputs, name=\'test_model_lstm\')\n    model.compile(loss=\'mse\', optimizer=\'nadam\')\n\n    # fit to dummy data\n    training_data_size = 2\n    data_in = generate_input_data(training_data_size, input_shapes)\n    initial_data_out = model.predict(data_in)\n    data_out = generate_output_data(training_data_size, initial_data_out)\n    model.fit(data_in, data_out, epochs=10)\n    return model\n\n\ndef get_test_model_gru():\n    return get_test_model_gru_stateful_optional(False)\n\n\ndef get_test_model_gru_stateful():\n    return get_test_model_gru_stateful_optional(True)\n\n\ndef get_test_model_gru_stateful_optional(stateful):\n    """"""Returns a test model for Gated Recurrent Unit (GRU) layers.""""""\n    input_shapes = [\n        (17, 4),\n        (1, 10)\n    ]\n    stateful_batch_size = 1\n    inputs = [Input(batch_shape=(stateful_batch_size,) + s) for s in input_shapes]\n    outputs = []\n\n    for inp in inputs:\n        gru_sequences = GRU(\n            stateful=stateful,\n            units=8,\n            recurrent_activation=\'relu\',\n            reset_after=True,\n            return_sequences=True,\n            use_bias=True\n        )(inp)\n        gru_regular = GRU(\n            stateful=stateful,\n            units=3,\n            recurrent_activation=\'sigmoid\',\n            reset_after=True,\n            return_sequences=False,\n            use_bias=False\n        )(gru_sequences)\n        outputs.append(gru_regular)\n\n        gru_bidi_sequences = Bidirectional(\n            GRU(\n                stateful=stateful,\n                units=4,\n                recurrent_activation=\'hard_sigmoid\',\n                reset_after=False,\n                return_sequences=True,\n                use_bias=True\n            )\n        )(inp)\n        gru_bidi = Bidirectional(\n            GRU(\n                stateful=stateful,\n                units=6,\n                recurrent_activation=\'sigmoid\',\n                reset_after=True,\n                return_sequences=False,\n                use_bias=False\n            )\n        )(gru_bidi_sequences)\n        outputs.append(gru_bidi)\n\n        gru_gpu_regular = GRU(\n            stateful=stateful,\n            units=3,\n            activation=\'tanh\',\n            recurrent_activation=\'sigmoid\',\n            reset_after=True,\n            use_bias=True\n        )(inp)\n\n        gru_gpu_bidi = Bidirectional(\n            GRU(\n                stateful=stateful,\n                units=3,\n                activation=\'tanh\',\n                recurrent_activation=\'sigmoid\',\n                reset_after=True,\n                use_bias=True\n            )\n        )(inp)\n        outputs.append(gru_gpu_regular)\n        outputs.append(gru_gpu_bidi)\n\n    model = Model(inputs=inputs, outputs=outputs, name=\'test_model_gru\')\n    model.compile(loss=\'mse\', optimizer=\'nadam\')\n    # fit to dummy data\n    training_data_size = 2\n    data_in = generate_input_data(training_data_size, input_shapes)\n    initial_data_out = model.predict(data_in)\n    data_out = generate_output_data(training_data_size, initial_data_out)\n    model.fit(data_in, data_out, batch_size=stateful_batch_size, epochs=10)\n    return model\n\n\ndef get_test_model_variable():\n    """"""Returns a exhaustive model for variably shaped input tensors.""""""\n\n    input_shapes = [\n        (None, None, 1),\n        (None, None, 3),\n        (None, 4),\n    ]\n\n    inputs = [Input(shape=s) for s in input_shapes]\n\n    outputs = []\n\n    # same as axis=-1\n    outputs.append(Concatenate()([inputs[0], inputs[1]]))\n    outputs.append(Conv2D(8, (3, 3), padding=\'same\', activation=\'elu\')(inputs[0]))\n    outputs.append(Conv2D(8, (3, 3), padding=\'same\', activation=\'relu\')(inputs[1]))\n    outputs.append(GlobalMaxPooling2D()(inputs[0]))\n    outputs.append(MaxPooling2D()(inputs[1]))\n    outputs.append(AveragePooling1D()(inputs[2]))\n\n    outputs.append(PReLU(shared_axes=[1, 2])(inputs[0]))\n    outputs.append(PReLU(shared_axes=[1, 2])(inputs[1]))\n    outputs.append(PReLU(shared_axes=[1, 2, 3])(inputs[1]))\n    outputs.append(PReLU(shared_axes=[1])(inputs[2]))\n\n    model = Model(inputs=inputs, outputs=outputs, name=\'test_model_variable\')\n    model.compile(loss=\'mse\', optimizer=\'nadam\')\n\n    # fit to dummy data\n    training_data_size = 2\n    data_in = generate_input_data(training_data_size, input_shapes)\n    initial_data_out = model.predict(data_in)\n    data_out = generate_output_data(training_data_size, initial_data_out)\n    model.fit(data_in, data_out, epochs=10)\n    return model\n\n\ndef get_test_model_sequential():\n    """"""Returns a typical (VGG-like) sequential test model.""""""\n    model = Sequential()\n    model.add(Conv2D(8, (3, 3), activation=\'relu\', input_shape=(32, 32, 3)))\n    model.add(Conv2D(8, (3, 3), activation=\'relu\'))\n    model.add(Permute((3, 1, 2)))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Permute((2, 3, 1)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(16, (3, 3), activation=\'elu\'))\n    model.add(Conv2D(16, (3, 3)))\n    model.add(ELU())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(64, activation=\'sigmoid\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation=\'softmax\'))\n\n    model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\')\n\n    # fit to dummy data\n    training_data_size = 2\n    data_in = [np.random.random(size=(training_data_size, 32, 32, 3))]\n    data_out = [np.random.random(size=(training_data_size, 10))]\n    model.fit(data_in, data_out, epochs=10)\n    return model\n\n\ndef get_test_model_lstm_stateful():\n    stateful_batch_size = 1\n    input_shapes = [\n        (17, 4),\n        (1, 10),\n        (None, 4),\n        (12,),\n        (12,)\n    ]\n\n    inputs = [Input(batch_shape=(stateful_batch_size,) + s) for s in input_shapes]\n    outputs = []\n    for in_num, inp in enumerate(inputs[:2]):\n        stateful = bool((in_num + 1) % 2)\n        lstm_sequences = LSTM(\n            stateful=stateful,\n            units=8,\n            recurrent_activation=\'relu\',\n            return_sequences=True,\n            name=\'lstm_sequences_\' + str(in_num) + \'_st-\' + str(stateful)\n        )(inp)\n        stateful = bool((in_num) % 2)\n        lstm_regular = LSTM(\n            stateful=stateful,\n            units=3,\n            recurrent_activation=\'sigmoid\',\n            return_sequences=False,\n            name=\'lstm_regular_\' + str(in_num) + \'_st-\' + str(stateful)\n        )(lstm_sequences)\n        outputs.append(lstm_regular)\n        stateful = bool((in_num + 1) % 2)\n        lstm_state, state_h, state_c = LSTM(\n            stateful=stateful,\n            units=3,\n            recurrent_activation=\'sigmoid\',\n            return_state=True,\n            name=\'lstm_state_return_\' + str(in_num) + \'_st-\' + str(stateful)\n        )(inp)\n        outputs.append(lstm_state)\n        outputs.append(state_h)\n        outputs.append(state_c)\n        stateful = bool((in_num + 1) % 2)\n        lstm_bidi_sequences = Bidirectional(\n            LSTM(\n                stateful=stateful,\n                units=4,\n                recurrent_activation=\'hard_sigmoid\',\n                return_sequences=True,\n                name=\'bi-lstm1_\' + str(in_num) + \'_st-\' + str(stateful)\n            )\n        )(inp)\n        stateful = bool((in_num) % 2)\n        lstm_bidi = Bidirectional(\n            LSTM(\n                stateful=stateful,\n                units=6,\n                recurrent_activation=\'linear\',\n                return_sequences=False,\n                name=\'bi-lstm2_\' + str(in_num) + \'_st-\' + str(stateful)\n            )\n        )(lstm_bidi_sequences)\n        outputs.append(lstm_bidi)\n\n    initial_state_stateful = LSTM(units=12, return_sequences=True, stateful=True, return_state=True,\n                                  name=\'initial_state_stateful\')(inputs[2], initial_state=[inputs[3], inputs[4]])\n    outputs.extend(initial_state_stateful)\n    initial_state_not_stateful = LSTM(units=12, return_sequences=False, stateful=False, return_state=True,\n                                      name=\'initial_state_not_stateful\')(inputs[2],\n                                                                         initial_state=[inputs[3], inputs[4]])\n    outputs.extend(initial_state_not_stateful)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(loss=\'mean_squared_error\', optimizer=\'nadam\')\n\n    # fit to dummy data\n    training_data_size = stateful_batch_size\n    data_in = generate_input_data(training_data_size, input_shapes)\n    initial_data_out = model.predict(data_in)\n    data_out = generate_output_data(training_data_size, initial_data_out)\n\n    model.fit(data_in, data_out, batch_size=stateful_batch_size, epochs=10)\n    return model\n\n\ndef main():\n    """"""Generate different test models and save them to the given directory.""""""\n    if len(sys.argv) != 3:\n        print(\'usage: [model name] [destination file path]\')\n        sys.exit(1)\n    else:\n        model_name = sys.argv[1]\n        dest_path = sys.argv[2]\n\n        get_model_functions = {\n            \'exhaustive\': get_test_model_exhaustive,\n            \'embedding\': get_test_model_embedding,\n            \'recurrent\': get_test_model_recurrent,\n            \'lstm\': get_test_model_lstm,\n            \'gru\': get_test_model_gru,\n            \'variable\': get_test_model_variable,\n            \'sequential\': get_test_model_sequential,\n            \'lstm_stateful\': get_test_model_lstm_stateful,\n            \'gru_stateful\': get_test_model_gru_stateful\n        }\n\n        if not model_name in get_model_functions:\n            print(\'unknown model name: \', model_name)\n            sys.exit(2)\n\n        np.random.seed(0)\n\n        model_func = get_model_functions[model_name]\n        model = model_func()\n        model.save(dest_path, include_optimizer=False)\n\n        # Make sure models can be loaded again,\n        # see https://github.com/fchollet/keras/issues/7682\n        model = load_model(dest_path)\n        print(model.summary())\n        # plot_model(model, to_file= str(model_name) + \'.png\', show_shapes=True, show_layer_names=True)  #### DEBUG stateful\n\n\nif __name__ == ""__main__"":\n    main()\n'"
keras_export/save_application_examples.py,0,"b'#!/usr/bin/env python3\n""""""Save application models mentioned in Keras documentation\n""""""\n\nimport convert_model\nimport tensorflow\n\n__author__ = ""Tobias Hermann""\n__copyright__ = ""Copyright 2017, Tobias Hermann""\n__license__ = ""MIT""\n__maintainer__ = ""Tobias Hermann, https://github.com/Dobiasd/frugally-deep""\n__email__ = ""editgym@gmail.com""\n\n\ndef save_model(file_name_base, model):\n    """"""Save and convert Keras model""""""\n    keras_file = f\'{file_name_base}.h5\'\n    fdeep_file = f\'{file_name_base}.json\'\n    print(f\'Saving {keras_file}\')\n    model.save(keras_file, include_optimizer=False)\n    print(f\'Converting {keras_file} to {fdeep_file}.\')\n    convert_model.convert(keras_file, fdeep_file)\n    print(f\'Conversion of model {keras_file} to {fdeep_file} done.\')\n\n\ndef main():\n    """"""Save famous example models in Keras-h5 and fdeep-json format.""""""\n    print(\'Saving application examples\')\n    save_model(\'densenet121\', tensorflow.keras.applications.densenet.DenseNet121())\n    save_model(\'densenet169\', tensorflow.keras.applications.densenet.DenseNet169())\n    save_model(\'densenet201\', tensorflow.keras.applications.densenet.DenseNet201())\n    # save_model(\'inceptionresnetv2\', tensorflow.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=(299, 299, 3)))  # lambda\n    save_model(\'inceptionv3\', tensorflow.keras.applications.inception_v3.InceptionV3(input_shape=(299, 299, 3)))\n    save_model(\'mobilenet\', tensorflow.keras.applications.mobilenet.MobileNet())\n    save_model(\'mobilenetv2\', tensorflow.keras.applications.mobilenet_v2.MobileNetV2())\n    save_model(\'nasnetlarge\', tensorflow.keras.applications.nasnet.NASNetLarge(input_shape=(331, 331, 3)))\n    save_model(\'nasnetmobile\', tensorflow.keras.applications.nasnet.NASNetMobile(input_shape=(224, 224, 3)))\n    save_model(\'resnet101\', tensorflow.keras.applications.ResNet101())\n    save_model(\'resnet101v2\', tensorflow.keras.applications.ResNet101V2())\n    save_model(\'resnet152\', tensorflow.keras.applications.ResNet152())\n    save_model(\'resnet152v2\', tensorflow.keras.applications.ResNet152V2())\n    save_model(\'resnet50\', tensorflow.keras.applications.ResNet50())\n    save_model(\'resnet50v2\', tensorflow.keras.applications.ResNet50V2())\n    save_model(\'vgg16\', tensorflow.keras.applications.vgg16.VGG16())\n    save_model(\'vgg19\', tensorflow.keras.applications.vgg19.VGG19())\n    save_model(\'xception\', tensorflow.keras.applications.xception.Xception(input_shape=(299, 299, 3)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
keras_export/visualize_layers.py,0,"b'#!/usr/bin/env python3\n""""""Generate images maximally activating filters/neurons of a given model.\n""""""\n\nimport datetime\nimport sys\n\nimport numpy as np\nfrom scipy.misc import imsave\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import load_model\n\n# based on: https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n__author__ = ""Francois Chollet, Tobias Hermann""\n__copyright__ = ""Copyright 2016 Francois Chollet, 2017 Tobias Hermann""\n__license__ = ""MIT""\n__maintainer__ = ""Tobias Hermann, https://github.com/Dobiasd/frugally-deep""\n__email__ = ""editgym@gmail.com""\n\nGRADIENT_ASCENT_STEPS = 24\nGRADIENT_ASCENT_STEP_SIZE = 1.\n\n\ndef deprocess_image(x):\n    """"""normalize tensor: center on 0., ensure std is 0.1""""""\n    x -= x.mean()\n    x /= (x.std() + K.epsilon())\n    x *= 0.1\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    if K.image_data_format() == \'channels_first\':\n        x = x.transpose((1, 2, 0))\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n\n\ndef normalize(x):\n    """"""utility function to normalize a tensor by its L2 norm""""""\n    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n\n\ndef process_conv_2d_layer(layer, input_img):\n    """"""Generate images maximizing the activation of conv2d filters""""""\n    filter_cnt = layer.get_weights()[0].shape[-1]\n    img_width, img_height, img_chans = input_img.shape[1:4]\n    kept_filters = []\n    for filter_index in range(filter_cnt):\n        print(\'{}:, filter {} of {}\'.format(\n            layer.name, filter_index, filter_cnt))\n\n        # we build a loss function that maximizes the activation\n        # of the nth filter of the layer considered\n        loss = K.mean(layer.output[:, :, :, filter_index])\n\n        # we compute the gradient of the input picture wrt this loss\n        grads = K.gradients(loss, input_img)[0]\n\n        # normalization trick: we normalize the gradient\n        grads = normalize(grads)\n\n        # this function returns the loss and grads given the input picture\n        iterate = K.function([input_img], [loss, grads])\n\n        # we start from a gray image with some random noise\n        input_img_data = np.random.random((1, img_width, img_height, img_chans))\n\n        # run gradient ascent\n        for _ in range(GRADIENT_ASCENT_STEPS):\n            loss_value, grads_value = iterate([input_img_data])\n            input_img_data += grads_value * GRADIENT_ASCENT_STEP_SIZE\n\n        # decode the resulting input image\n        img = deprocess_image(input_img_data[0])\n        kept_filters.append((img, loss_value))\n\n    return kept_filters\n\n\ndef is_ascii(some_string):\n    """"""Check if a string only contains ascii characters""""""\n    try:\n        some_string.encode(\'ascii\')\n    except UnicodeEncodeError:\n        return False\n    else:\n        return True\n\n\ndef process_layers(model, out_dir):\n    """"""Visualize all filters of all layers""""""\n    process_layer_functions = {\n        # \'Conv1D\': process_conv_1d_layer,\n        \'Conv2D\': process_conv_2d_layer,\n        # \'Conv2DTranspose\': process_conv_2d_transpose_layer,\n        # \'SeparableConv2D\': process_separable_conv_2d_layer,\n        # \'Dense\': process_dense_layer\n    }\n    result = {}\n    layers = model.layers\n    for i, layer in enumerate(layers):\n        layer_type = type(layer).__name__\n        if layer_type in [\'Model\', \'Sequential\']:\n            result = process_layers(layer, out_dir)\n        else:\n            process_func = process_layer_functions.get(layer_type, None)\n            name = layer.name\n            assert is_ascii(name)\n            if process_func:\n                print(\'Processing layer {} ({} of {})\'.format(\n                    layer.name, i, len(layers)))\n                images_with_loss = process_func(layer, model.get_input_at(0))\n                for _, (image, loss) in enumerate(images_with_loss):\n                    date_time_str = datetime.datetime.now().strftime(""%Y-%m-%d_%H_%M_%S_%f"")\n                    if image.shape[-1] == 1:\n                        image = image.reshape(image.shape[:-1])\n                    imsave(\'{}/{}_{}_{}_{}.png\'.format(\n                        out_dir, date_time_str, name, i, loss), image)\n    return result\n\n\ndef convert_sequential_to_model(model):\n    """"""Convert a sequential model to the underlying functional format""""""\n    if type(model).__name__ == \'Sequential\':\n        name = model.name\n        inbound_nodes = model.inbound_nodes\n        model = model.model\n        model.name = name\n        model.inbound_nodes = inbound_nodes\n    assert model.layers\n    for i in range(len(model.layers)):\n        if type(model.layers[i]).__name__ in [\'Model\', \'Sequential\']:\n            model.layers[i] = convert_sequential_to_model(model.layers[i])\n    return model\n\n\ndef main():\n    """"""Convert any Keras model to the frugally-deep model format.""""""\n\n    usage = \'usage: [Keras model in HDF5 format] [image output directory]\'\n    if len(sys.argv) != 3:\n        print(usage)\n        sys.exit(1)\n    else:\n        in_path = sys.argv[1]\n        out_dir = sys.argv[2]\n        print(\'loading {}\'.format(in_path))\n        K.set_learning_phase(1)\n        model = load_model(in_path)\n        model = convert_sequential_to_model(model)\n        process_layers(model, out_dir)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
test/readme_example_generate.py,0,"b""#!/usr/bin/env python3\n\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\n\ninputs = Input(shape=(4,))\nx = Dense(5, activation='relu')(inputs)\npredictions = Dense(3, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(loss='categorical_crossentropy', optimizer='nadam')\n\nmodel.fit(\n    np.asarray([[1, 2, 3, 4], [2, 3, 4, 5]]),\n    np.asarray([[1, 0, 0], [0, 0, 1]]), epochs=10)\n\nmodel.save('readme_example_model.h5', include_optimizer=False)\n"""
test_package/conanfile.py,0,"b'from conans import ConanFile, CMake\nimport os\n\n\nclass TestPackageConan(ConanFile):\n    settings = ""os"", ""compiler"", ""build_type"", ""arch""\n    generators = ""cmake""\n\n    def build(self):\n        cmake = CMake(self)\n        cmake.configure()\n        cmake.build()\n\n    def test(self):\n        bin_path = os.path.join(""bin"", ""test_package"")\n        self.run(bin_path)'"
test/stateful_test/stateful_recurrent_tests.py,3,"b'# to hide any GPUs.\n# import os\n# os.environ[\'CUDA_DEVICE_ORDER\']=\'PCI_BUS_ID\'\n# os.environ[\'CUDA_VISIBLE_DEVICES\']=\'\'\n\nimport errno\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, GRU, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\n\n__author__ = ""Keith Chugg""\n__copyright__ = ""Copyright 2019, Keith Chugg""\n__license__ = ""MIT""\n__maintainer__ = ""Tobias Hermann, https://github.com/Dobiasd/frugally-deep""\n__email__ = ""editgym@gmail.com""\n\nPRINT_SUMMARIES = False\nVERBOSE = False\nTEST_EPSILON = 0.0001\n\n\ndef get_trained_model(x_train, y_train, layer_name, n_recurrent_units, bidi):\n    if layer_name == \'LSTM\':\n        REC_LAYER = LSTM\n    else:\n        REC_LAYER = GRU\n    #  Define/Build/Train Training Model\n    training_in_shape = x_train.shape[1:]\n    training_in = Input(shape=training_in_shape)\n    if bidi:\n        recurrent_out = Bidirectional(REC_LAYER(n_recurrent_units, return_sequences=True, stateful=False))(training_in)\n    else:\n        recurrent_out = REC_LAYER(n_recurrent_units, return_sequences=True, stateful=False)(training_in)\n    training_pred = Dense(1)(recurrent_out)\n    training_model = Model(inputs=training_in, outputs=training_pred)\n    training_model.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n    if PRINT_SUMMARIES:\n        training_model.summary()\n    training_model.fit(x_train, y_train, batch_size=2, epochs=10, verbose=0)\n    trained_weights = training_model.get_weights()\n    return training_model, trained_weights\n\n\ndef get_test_model(n_recurrent_units, sequence_length, feature_dim, layer_name, stateful, initialize_states, weights,\n                   bidi):\n    features_in = Input(batch_shape=(1, sequence_length, feature_dim))  # stateful ==> needs batch_shape specified\n    if layer_name == \'LSTM\':\n        REC_LAYER = LSTM\n    else:\n        REC_LAYER = GRU\n    if bidi:\n        if not initialize_states:\n            recurrent_out = Bidirectional(REC_LAYER(n_recurrent_units, return_sequences=True, stateful=stateful))(\n                features_in)\n            pred = Dense(1)(recurrent_out)\n            test_model = Model(inputs=features_in, outputs=pred)\n        else:\n            state_h_fwd_in = Input(batch_shape=(1, n_recurrent_units))\n            state_h_bwd_in = Input(batch_shape=(1, n_recurrent_units))\n            state_c_fwd_in = Input(batch_shape=(1, n_recurrent_units))\n            state_c_bwd_in = Input(batch_shape=(1, n_recurrent_units))\n            if layer_name == \'LSTM\':\n                recurrent_out = Bidirectional(REC_LAYER(n_recurrent_units, return_sequences=True, stateful=stateful))(\n                    features_in, initial_state=[state_h_fwd_in, state_h_bwd_in, state_c_fwd_in, state_c_bwd_in])\n                pred = Dense(1)(recurrent_out)\n                test_model = Model(\n                    inputs=[features_in, state_h_fwd_in, state_h_bwd_in, state_c_fwd_in, state_c_bwd_in], outputs=pred)\n            else:\n                # GRU\n                recurrent_out = Bidirectional(REC_LAYER(n_recurrent_units, return_sequences=True, stateful=stateful))(\n                    features_in, initial_state=[state_h_fwd_in, state_h_bwd_in])\n                pred = Dense(1)(recurrent_out)\n                test_model = Model(inputs=[features_in, state_h_fwd_in, state_h_bwd_in], outputs=pred)\n    else:  # not bidi\n        if not initialize_states:\n            recurrent_out = REC_LAYER(n_recurrent_units, return_sequences=True, stateful=stateful)(features_in)\n            pred = Dense(1)(recurrent_out)\n            test_model = Model(inputs=features_in, outputs=pred)\n        else:\n            state_h_in = Input(batch_shape=(1, n_recurrent_units))\n            state_c_in = Input(batch_shape=(1, n_recurrent_units))\n            if layer_name == \'LSTM\':\n                recurrent_out = REC_LAYER(n_recurrent_units,\n                                          return_sequences=True,\n                                          stateful=stateful)(features_in, initial_state=[state_h_in, state_c_in])\n                pred = Dense(1)(recurrent_out)\n                test_model = Model(inputs=[features_in, state_h_in, state_c_in], outputs=pred)\n            else:\n                # GRU\n                recurrent_out = REC_LAYER(n_recurrent_units, return_sequences=True, stateful=stateful)(features_in,\n                                                                                                       initial_state=state_h_in)\n                pred = Dense(1)(recurrent_out)\n                test_model = Model(inputs=[features_in, state_h_in], outputs=pred)\n    test_model.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n    if PRINT_SUMMARIES:\n        test_model.summary()\n    test_model.set_weights(weights)\n    model_fname = \'./models/\'\n    if bidi:\n        model_fname += \'bidi-\' + layer_name\n    else:\n        model_fname += layer_name\n    if stateful:\n        model_fname += \'_stateful\'\n    else:\n        model_fname += \'_nonstateful\'\n    if initialize_states:\n        model_fname += \'_init_state\'\n    else:\n        model_fname += \'_no_init_state\'\n\n    model_fname += \'.h5\'\n    test_model.save(model_fname, include_optimizer=False)\n    return test_model, model_fname\n\n\ndef eval_test_model(baseline_out, test_model, x_in, layer_name, bidi, stateful, states_initialized, initial_states=[]):\n    num_test_seqs, sequence_length, feature_dim = x_in.shape\n    results = np.zeros(0)\n    for state_reset in [True, False]:\n        for s in range(num_test_seqs):\n            in_seq = x_in[s].reshape((1, sequence_length, feature_dim))\n            baseline = baseline_out[s].reshape(sequence_length)\n            msg = \'\\n\\nRunning \'\n            if bidi:\n                msg += \'Bidi-\'\n            msg += layer_name + \'; Sequence \' + str(s) + \'; stateful \' + str(stateful) + \'; Initialzied state: \' + str(\n                states_initialized) + \'; State Reset: \' + str(state_reset) + \'\\n\'\n            # msg += f\'{layer_name}; Sequence {s}; stateful {stateful}; Initialzied state: {states_initialized}; State Reset: {state_reset}\\n\'\n            if VERBOSE:\n                print(msg)\n            pred_in = x_in[s].reshape(x_in[1:].shape)\n            if not states_initialized:  # no initial state\n                pred_seq = test_model.predict(pred_in)\n            else:\n                if layer_name == \'LSTM\':\n                    if bidi:\n                        pred_seq = test_model.predict(\n                            [pred_in, initial_states[0], initial_states[1], initial_states[2], initial_states[3]])\n                    else:\n                        pred_seq = test_model.predict([pred_in, initial_states[0], initial_states[1]])\n                else:  # GRU\n                    if bidi:\n                        pred_seq = test_model.predict([pred_in, initial_states[0], initial_states[1]])\n                    else:\n                        pred_seq = test_model.predict([pred_in, initial_states[0]])\n            pred_seq = pred_seq.reshape(sequence_length)\n            results = np.append(results, pred_seq)\n            if VERBOSE:\n                print(\'Baseline: \', baseline)\n                print(\'Prediction: \', pred_seq)\n                print(\'Difference: \', baseline - pred_seq)\n            if state_reset:\n                test_model.reset_states()\n                # if stateful and states_initialized:\n                #     if VERBOSE:\n                #         print(\'Keras does not handle reset_state calls with stateful=True with initial state inputs\')\n                # else:\n                #     test_model.reset_states()\n    return results\n\n\ndef main():\n    print(""Starting stateful recurrent tests"")\n\n    # See https://www.tensorflow.org/guide/gpu\n    print(\'Adjusting GPU-memory settings to avoid CUDA_ERROR_OUT_OF_MEMORY.\')\n    gpus = tf.config.experimental.list_physical_devices(\'GPU\')\n    if gpus:\n        try:\n            # Currently, memory growth needs to be the same across GPUs\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n            print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")\n        except RuntimeError as e:\n            # Memory growth must be set before GPUs have been initialized\n            print(e)\n\n    # generate toy data\n    train_seq_length = 4\n    feature_dim = 1\n    num_seqs = 8\n\n    x_train = np.random.normal(0, 1, num_seqs * train_seq_length * feature_dim)\n    x_train = x_train.reshape(num_seqs, train_seq_length, feature_dim)\n    y_train = np.random.normal(0, 1, num_seqs * train_seq_length)\n    y_train = y_train.reshape(num_seqs, train_seq_length, 1)\n\n    n_recurrent_units = 2\n    all_results = np.zeros(0, dtype=np.float32)\n\n    # hand-generate the input data to make it easy to input into C++ code by hand.\n    # this hard-codes the train_seq_length, feature_dim vars for this purpose\n    x_inf = np.asarray([2.1, -1.2, 3.14, 1.2, 1, 3, -2, 10], dtype=np.float32)  # simple\n    x_inf = x_inf.reshape((2, train_seq_length, 1))\n\n    initial_states = np.asarray([40.1, -25.1, 34.7, 56.1, -62.5, 12.0, -33.0, -100.0], dtype=np.float32)\n    # initial_states = np.asarray([1.1, -2.1, 2.7, 3.1, -2.5, 3.0, -2.0, -10.0], dtype=np.float32)\n    initial_states = initial_states.reshape((4, 1, 2))\n\n    model_file_names = []\n\n    for bidi in [False, True]:\n        for layer_name in [\'GRU\', \'LSTM\']:\n            # train with no initial state, no statefulness\n            training_model, trained_weights = get_trained_model(x_train, y_train, layer_name, n_recurrent_units, bidi)\n            y_inf = training_model.predict(x_inf)\n            for stateful in [False, True]:\n                for initialize_states in [False, True]:\n                    # evaluate the model\n                    test_model, model_fname = get_test_model(n_recurrent_units, train_seq_length, feature_dim,\n                                                             layer_name,\n                                                             stateful, initialize_states, trained_weights, bidi)\n                    result = eval_test_model(y_inf, test_model, x_inf, layer_name, bidi, stateful, initialize_states,\n                                             initial_states)\n                    all_results = np.append(all_results, result)\n                    model_file_names.append(model_fname)\n\n    if VERBOSE:\n        print(\'\\n\\n\')\n        print(all_results)\n        print(model_file_names)\n\n    all_results.tofile(\'models/python_results.npy\')\n\n    for h5_fname in model_file_names:\n        json_fname = h5_fname.replace(\'.h5\', \'.json\')\n        cmd = \'python3 ../../keras_export/convert_model.py \' + h5_fname + \' \' + json_fname\n        os.system(cmd)\n\n    os.system(\'./stateful_recurrent_tests_cpp\')\n\n    frugally_deep_results = np.fromfile(\'models/fd_results.bin\', dtype=np.float32)\n\n    num_test_models = len(model_file_names)\n    test_sequences_per_model = 4\n    frugally_deep_results = frugally_deep_results.reshape(\n        (num_test_models, test_sequences_per_model * train_seq_length))\n    all_results = all_results.reshape((num_test_models, test_sequences_per_model * train_seq_length))\n    all_results = all_results.astype(np.float32)\n\n    all_tests_passed = True\n    for i, model_fname in enumerate(model_file_names):\n        test_name = os.path.basename(model_fname)\n        test_name = test_name.split(\'.h5\')[0]\n        print(\'Test \', i + 1, \' \', test_name, \': \')\n        diff = np.abs(all_results[i] - frugally_deep_results[i])\n        max_delta = np.max(diff)\n        if VERBOSE:\n            print(\'Max delta: \', ""{:4.3e}"".format(max_delta))\n        if max_delta < TEST_EPSILON:\n            print(\'PASSED\')\n        else:\n            print(\'********* FAILED !!!!!!!!!!!!\\n\\n\')\n            print(\'Keras: \', all_results[i], \'\\n\')\n            print(\'Frugally-deep: \', frugally_deep_results[i], \'\\n\')\n            print(\'Difference: \', all_results[i] - frugally_deep_results[i], \'\\n\')\n            all_tests_passed = False\n\n    if not all_tests_passed:\n        print(\'\\n\\nAt least one test failed.\')\n        sys.exit(errno.EIO)\n    print(\'\\n\\nPassed all stateful tests\')\n\nif __name__ == ""__main__"":\n    main()\n'"
test/stateful_test/tf_behaivor_scripts/temp_bidi_no_state_in.py,0,"b""import os\nos.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\nos.environ['CUDA_VISIBLE_DEVICES']=''\n\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\n\nREC = LSTM\n\nsequence_length = 3\nfeature_dim = 1\nfeatures_in = Input(batch_shape=(1, sequence_length, feature_dim)) \n\nrnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in)\nstateless_model = Model(inputs=[features_in], outputs=[rnn_out])\n\nstateful_rnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in)\nstateful_model = Model(inputs=features_in, outputs=stateful_rnn_out)\n\nstateful_model.set_weights( stateless_model.get_weights() )\n\nx_in = np.random.normal(0,10,sequence_length)\nx_in = x_in.reshape( (1, sequence_length, feature_dim) )\n\ndef print_bidi_out(non_stateful_out, stateful_out):\n\tfb = ['FWD::', 'BWD::']\n\n\tfor i in range(2):\n\t\tprint(fb[i])\n\t\tprint(f'non_stateful: {non_stateful_out.T[i]}')\n\t\tprint(f'stateful: {stateful_out.T[i]}')\n\t\tprint(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')\n\n\nnon_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))\nstateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))\nprint_bidi_out(non_stateful_out, stateful_out)\n\nnon_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))\nstateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))\nprint_bidi_out(non_stateful_out, stateful_out)\n\nprint('\\n** RESETING STATES in STATEFUL MODEL **\\n')\nstateful_model.reset_states()\nnon_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))\nstateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))\nprint_bidi_out(non_stateful_out, stateful_out)\n"""
test/stateful_test/tf_behaivor_scripts/temp_bidi_state_in.py,0,"b""import os\nos.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\nos.environ['CUDA_VISIBLE_DEVICES']=''\n\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\n\nREC = LSTM\n\nsequence_length = 3\nfeature_dim = 1\nfeatures_in = Input(batch_shape=(1, sequence_length, feature_dim)) \nstate_h_fwd_in = Input(batch_shape=(1, 1))\nstate_h_bwd_in = Input(batch_shape=(1, 1))\nstate_c_fwd_in = Input(batch_shape=(1, 1))\nstate_c_bwd_in = Input(batch_shape=(1, 1))\n\nfour_state_shape = [state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]\ntwo_state_shape = [state_h_fwd_in, state_h_bwd_in]\n\nif REC == LSTM:\n    rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=four_state_shape)\n    stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=four_state_shape)\n    rnn_inputs = [features_in, state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]\nelse:\n    if REC == SimpleRNN:\n        rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)\n        stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)\n    else:\n        rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)\n        stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)\n    rnn_inputs = [features_in, state_h_fwd_in, state_h_bwd_in]\n\nstateless_model = Model(inputs=rnn_inputs, outputs=rnn_out)\nstateful_model = Model(inputs=rnn_inputs, outputs=stateful_rnn_out)\n\n\n# toy_weights = [np.asarray([[ 1.0]], dtype=np.float32), np.asarray([[0.5 ]], dtype=np.float32), np.asarray([[ -1.0 ]], dtype=np.float32), np.asarray([[ -0.5 ]], dtype=np.float32)]\n# stateless_model.set_weights(toy_weights)\n# stateful_model.set_weights(toy_weights)\n\nstateful_model.set_weights( stateless_model.get_weights() )\n\nstateful_model.save('temp_stateful.h5')\nstateless_model.save('temp_stateless.h5')\n\nx_in = np.random.normal(0,10,sequence_length)\nx_in = np.asarray([1,0,0])\nx_in = x_in.reshape( (1, sequence_length, feature_dim) )\n\nfwd_initial_h = np.asarray(2.75).reshape(1,1)\nfwd_initial_c = np.asarray(1.3).reshape(1,1)\nbwd_initial_h = np.asarray(-2.0).reshape(1,1)\nbwd_initial_c = np.asarray(-1.2).reshape(1,1)\n\n# fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)\n# fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)\n# bwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)\n# fwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)\n# bwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)\n\nif REC == LSTM:\n    rnn_input = [x_in, fwd_initial_h, fwd_initial_c, bwd_initial_h, bwd_initial_c]\nelse:\n    rnn_input = [x_in, fwd_initial_h, bwd_initial_h] \n    \n\ndef print_bidi_out(non_stateful_out, stateful_out):\n\tfb = ['FWD::', 'BWD::']\n\n\tfor i in range(2):\n\t\tprint(fb[i])\n\t\tprint(f'non_stateful: {non_stateful_out.T[i]}')\n\t\tprint(f'stateful: {stateful_out.T[i]}')\n\t\tprint(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')\n\nnon_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))\nstateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))\nprint_bidi_out(non_stateful_out, stateful_out)\n\nnon_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))\nstateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))\nprint_bidi_out(non_stateful_out, stateful_out)\n\nprint('\\n** RESETING STATES in STATEFUL MODEL **\\n')\nstateful_model.reset_states()\nnon_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))\nstateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))\nprint_bidi_out(non_stateful_out, stateful_out)\n"""
test/stateful_test/tf_behaivor_scripts/temp_rnn_test.py,0,"b""import os\nos.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\nos.environ['CUDA_VISIBLE_DEVICES']=''\n\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional\nfrom tensorflow.keras.models import Model\n\nUSE_TOY_WEIGHTS = True\nREC_LAYER = GRU\nsequence_length = 3\nfeature_dim = 1\nfeatures_in = Input(batch_shape=(1, sequence_length, feature_dim)) \nstate_h_in = Input(batch_shape=(1, 1))\n\nrnn_out = REC_LAYER(1, activation=None,  use_bias=False, return_sequences=True, return_state=False, stateful=False)(features_in, initial_state=state_h_in)\nstateless_model = Model(inputs=[features_in, state_h_in], outputs=rnn_out)\n\nstateful_rnn_out = REC_LAYER(1, activation=None,  use_bias=False, return_sequences=True, return_state=False, stateful=True)(features_in, initial_state=state_h_in)\nstateful_model = Model(inputs=[features_in, state_h_in], outputs=stateful_rnn_out)\n\nif USE_TOY_WEIGHTS:\n\tif REC_LAYER == SimpleRNN:\n\t\ttoy_weights = [ np.asarray([[1.0]], dtype=np.float32), np.asarray([[-0.5]], dtype=np.float32)]\n\n\telif REC_LAYER == GRU:\n\t\t# for a GRU, the first are the non-recurrent kernels W, and the second are the recurrent kernels U (V)\n\t\ttoy_weights = [np.asarray([[ 1.0, -2.0,  3.0 ]], dtype=np.float32), np.asarray([[ -0.5 , 2.0, -1.1 ]], dtype=np.float32)]\n\n\tstateless_model.set_weights(toy_weights)\n\tstateful_model.set_weights(toy_weights)\n\n# w = stateless_model.get_weights()\n# print(w)\n\nstateless_model.save('temp_stateless.h5', include_optimizer=False)\nstateful_model.save('temp_stateful.h5', include_optimizer=False)\n\nx_in = np.zeros(sequence_length)\nx_in[0] = 1\nx_in = x_in.reshape( (1, sequence_length, feature_dim) )\ninitial_state = np.asarray( [10])\ninitial_state = initial_state.reshape((1,1))\n\ndef print_rnn_out(non_stateful_out, stateful_out):\n\tfb = ['FWD::', 'BWD::']\n\n\tprint(f'non_stateful: {non_stateful_out}')\n\tprint(f'stateful: {stateful_out}')\n\tprint(f'delta: {stateful_out-non_stateful_out}')\n\nnon_stateful_out = stateless_model.predict([x_in, initial_state]).reshape((sequence_length))\nstateful_out = stateful_model.predict([x_in, initial_state]).reshape((sequence_length))\nprint_rnn_out(non_stateful_out, stateful_out)\n\nnon_stateful_out = stateless_model.predict([x_in, initial_state]).reshape((sequence_length))\nstateful_out = stateful_model.predict([x_in, initial_state]).reshape((sequence_length))\nprint_rnn_out(non_stateful_out, stateful_out)\n\nprint('\\n** RESETING STATES in STATEFUL MODEL **\\n')\nstateful_model.reset_states()\nnon_stateful_out = stateless_model.predict([x_in, initial_state]).reshape((sequence_length))\nstateful_out = stateful_model.predict([x_in, initial_state]).reshape((sequence_length))\nprint_rnn_out(non_stateful_out, stateful_out)\n\nnon_stateful_out = stateless_model.predict([x_in, initial_state]).reshape((sequence_length))\nstateful_out = stateful_model.predict([x_in, initial_state]).reshape((sequence_length))\nprint_rnn_out(non_stateful_out, stateful_out)\n"""
