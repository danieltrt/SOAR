file_path,api_count,code
data_loader.py,9,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\nimport argparse\nimport csv\nimport os\nimport random\nimport re\n\nimport numpy as np\nfrom hbconfig import Config\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    """"""\n    string = string.decode(\'utf-8\')\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels(positive_data_file, negative_data_file):\n    """"""\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    """"""\n    # Load data from files\n    positive_examples = list(open(positive_data_file, ""rb"").readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(negative_data_file, ""rb"").readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    # Generate labels\n    positive_labels = [\'1\' for _ in positive_examples]\n    negative_labels = [\'0\' for _ in negative_examples]\n    y = positive_labels + negative_labels\n    return x_text, y\n\n\ndef prepare_raw_data():\n    print(\'Preparing raw data into train set and test set ...\')\n    raw_data_path = os.path.join(Config.data.base_path, Config.data.raw_data_path)\n\n    data_type = Config.data.type\n    if data_type == ""kaggle_movie_review"":\n        train_path = os.path.join(raw_data_path, \'train.tsv\')\n        train_reader = csv.reader(open(train_path), delimiter=""\\t"")\n\n        prepare_dataset(dataset=list(train_reader))\n\n    elif data_type == ""rt-polarity"":\n        pos_path = os.path.join(Config.data.base_path, Config.data.raw_data_path, ""rt-polarity.pos"")\n        neg_path = os.path.join(Config.data.base_path, Config.data.raw_data_path, ""rt-polarity.neg"")\n        x_text, y = load_data_and_labels(pos_path, neg_path)\n\n        prepare_dataset(x_text=x_text, y=y)\n\n\ndef prepare_dataset(dataset=None, x_text=None, y=None):\n    make_dir(os.path.join(Config.data.base_path, Config.data.processed_path))\n\n    filenames = [\'train_X\', \'train_y\', \'test_X\', \'test_y\']\n    files = []\n    for filename in filenames:\n        files.append(open(os.path.join(Config.data.base_path, Config.data.processed_path, filename), \'wb\'))\n\n    if dataset is not None:\n\n        print(""Total data length : "", len(dataset))\n        test_ids = random.sample([i for i in range(len(dataset))], Config.data.testset_size)\n\n        for i in tqdm(range(len(dataset))):\n            if i == 0:\n                continue\n\n            data = dataset[i]\n            X, y = data[2], data[3]\n\n            if i in test_ids:\n                files[2].write((X + ""\\n"").encode(\'utf-8\'))\n                files[3].write((y + \'\\n\').encode(\'utf-8\'))\n            else:\n                files[0].write((X + \'\\n\').encode(\'utf-8\'))\n                files[1].write((y + \'\\n\').encode(\'utf-8\'))\n\n    else:\n\n        print(""Total data length : "", len(y))\n        test_ids = random.sample([i for i in range(len(y))], Config.data.testset_size)\n\n        for i in tqdm(range(len(y))):\n            if i in test_ids:\n                files[2].write((x_text[i] + ""\\n"").encode(\'utf-8\'))\n                files[3].write((y[i] + \'\\n\').encode(\'utf-8\'))\n            else:\n                files[0].write((x_text[i] + \'\\n\').encode(\'utf-8\'))\n                files[1].write((y[i] + \'\\n\').encode(\'utf-8\'))\n\n    for file in files:\n        file.close()\n\n\ndef make_dir(path):\n    """""" Create a directory if there isn\'t one already. """"""\n    try:\n        os.mkdir(path)\n    except OSError:\n        pass\n\n\ndef basic_tokenizer(line, normalize_digits=True):\n    """""" A basic tokenizer to tokenize text into tokens.\n    Feel free to change this to suit your need. """"""\n    line = re.sub(\'<u>\', \'\', line)\n    line = re.sub(\'</u>\', \'\', line)\n    line = re.sub(\'\\[\', \'\', line)\n    line = re.sub(\'\\]\', \'\', line)\n    words = []\n    _WORD_SPLIT = re.compile(""([.,!?\\""\'-<>:;)(])"")\n    _DIGIT_RE = re.compile(r""\\d"")\n    for fragment in line.strip().lower().split():\n        for token in re.split(_WORD_SPLIT, fragment):\n            if not token:\n                continue\n            if normalize_digits:\n                token = re.sub(_DIGIT_RE, \'#\', token)\n            words.append(token)\n    return words\n\n\ndef build_vocab(train_fname, test_fname, normalize_digits=True):\n    vocab = {}\n    def count_vocab(fname):\n        with open(fname, \'rb\') as f:\n            for line in f.readlines():\n                line = line.decode(\'utf-8\')\n                for token in basic_tokenizer(line):\n                    if not token in vocab:\n                        vocab[token] = 0\n                    vocab[token] += 1\n\n    train_path = os.path.join(Config.data.base_path, Config.data.processed_path, train_fname)\n    test_path = os.path.join(Config.data.base_path, Config.data.processed_path, test_fname)\n\n    count_vocab(train_path)\n    count_vocab(test_path)\n\n    sorted_vocab = sorted(vocab, key=vocab.get, reverse=True)\n\n    dest_path = os.path.join(Config.data.base_path, Config.data.processed_path, \'vocab\')\n    with open(dest_path, \'wb\') as f:\n        f.write((\'<pad>\' + \'\\n\').encode(\'utf-8\'))\n        index = 1\n        for word in sorted_vocab:\n            f.write((word + \'\\n\').encode(\'utf-8\'))\n            index += 1\n\n\ndef load_vocab(vocab_fname):\n    print(""load vocab ..."")\n    with open(os.path.join(Config.data.base_path, Config.data.processed_path, vocab_fname), \'rb\') as f:\n        words = f.read().decode(\'utf-8\').splitlines()\n    return {words[i]: i for i in range(len(words))}\n\n\ndef sentence2id(vocab, line):\n    return [vocab.get(token, vocab[\'<pad>\']) for token in basic_tokenizer(line)]\n\n\ndef token2id(data):\n    """""" Convert all the tokens in the data into their corresponding\n    index in the vocabulary. """"""\n    vocab_path = \'vocab\'\n    in_path = data\n    out_path = data + \'_ids\'\n\n    vocab = load_vocab(vocab_path)\n    in_file = open(os.path.join(Config.data.base_path, Config.data.processed_path, in_path), \'rb\')\n    out_file = open(os.path.join(Config.data.base_path, Config.data.processed_path, out_path), \'wb\')\n\n    lines = in_file.read().decode(\'utf-8\').splitlines()\n    for line in lines:\n        ids = []\n        sentence_ids = sentence2id(vocab, line)\n        ids.extend(sentence_ids)\n\n        out_file.write(b\' \'.join(str(id_).encode(\'utf-8\') for id_ in ids) + b\'\\n\')\n\n\ndef process_data():\n    print(\'Preparing data to be model-ready ...\')\n\n    build_vocab(\'train_X\', \'test_X\')\n\n    token2id(\'train_X\')\n    token2id(\'test_X\')\n\n\ndef make_train_and_test_set(shuffle=True):\n    print(""make Training data and Test data Start...."")\n\n    if Config.data.get(\'max_seq_length\', None) is None:\n        set_max_seq_length([\'train_X_ids\', \'test_X_ids\'])\n\n    train_X, train_y = load_data(\'train_X_ids\', \'train_y\')\n    test_X, test_y = load_data(\'test_X_ids\', \'test_y\')\n\n    assert len(train_X) == len(train_y)\n    assert len(test_X) == len(test_y)\n\n    print(f""train data count : {len(train_y)}"")\n    print(f""test data count : {len(test_y)}"")\n\n    if shuffle:\n        print(""shuffle dataset ..."")\n        train_p = np.random.permutation(len(train_y))\n        test_p = np.random.permutation(len(test_y))\n\n        return ((train_X[train_p], train_y[train_p]),\n                (test_X[test_p], test_y[test_p]))\n    else:\n        return ((train_X, train_y),\n                (test_X, test_y))\n\n\ndef load_data(X_fname, y_fname):\n    X_input_data = open(os.path.join(Config.data.base_path, Config.data.processed_path, X_fname), \'r\')\n    y_input_data = open(os.path.join(Config.data.base_path, Config.data.processed_path, y_fname), \'r\')\n\n    X_data, y_data = [], []\n    for X_line, y_line in zip(X_input_data.readlines(), y_input_data.readlines()):\n        X_ids = [int(id_) for id_ in X_line.split()]\n        y_id = int(y_line)\n\n        if len(X_ids) == 0 or y_id >= Config.data.num_classes:\n            continue\n\n        if len(X_ids) <= Config.data.max_seq_length:\n            X_data.append(_pad_input(X_ids, Config.data.max_seq_length))\n\n        y_one_hot = np.zeros(Config.data.num_classes)\n        y_one_hot[int(y_line)] = 1\n        y_data.append(y_one_hot)\n\n    print(f""load data from {X_fname}, {y_fname}..."")\n    return np.array(X_data, dtype=np.int32), np.array(y_data, dtype=np.int32)\n\n\ndef _pad_input(input_, size):\n    return input_ + [0] * (size - len(input_))\n\n\ndef set_max_seq_length(dataset_fnames):\n\n    max_seq_length = Config.data.get(\'max_seq_length\', 10)\n\n    for fname in dataset_fnames:\n        input_data = open(os.path.join(Config.data.base_path, Config.data.processed_path, fname), \'r\')\n\n        for line in input_data.readlines():\n            ids = [int(id_) for id_ in line.split()]\n            seq_length = len(ids)\n\n            if seq_length > max_seq_length:\n                max_seq_length = seq_length\n\n    Config.data.max_seq_length = max_seq_length\n    print(f""Setting max_seq_length to Config : {max_seq_length}"")\n\n\ndef make_batch(data, buffer_size=10000, batch_size=64, scope=""train""):\n\n    class IteratorInitializerHook(tf.train.SessionRunHook):\n        """"""Hook to initialise data iterator after Session is created.""""""\n\n        def __init__(self):\n            super(IteratorInitializerHook, self).__init__()\n            self.iterator_initializer_func = None\n\n        def after_create_session(self, session, coord):\n            """"""Initialise the iterator after the session has been created.""""""\n            self.iterator_initializer_func(session)\n\n\n    def get_inputs():\n\n        iterator_initializer_hook = IteratorInitializerHook()\n\n        def train_inputs():\n            with tf.name_scope(scope):\n\n                X, y = data\n\n                # Define placeholders\n                input_placeholder = tf.placeholder(\n                    tf.int32, [None, Config.data.max_seq_length])\n                output_placeholder = tf.placeholder(\n                    tf.int32, [None, Config.data.num_classes])\n\n                # Build dataset iterator\n                dataset = tf.data.Dataset.from_tensor_slices(\n                    (input_placeholder, output_placeholder))\n\n                if scope == ""train"":\n                    dataset = dataset.repeat(None)  # Infinite iterations\n                else:\n                    dataset = dataset.repeat(1)  # 1 Epoch\n                # dataset = dataset.shuffle(buffer_size=buffer_size)\n                dataset = dataset.batch(batch_size)\n\n                iterator = dataset.make_initializable_iterator()\n                next_X, next_y = iterator.get_next()\n\n                tf.identity(next_X[0], \'input_0\')\n                tf.identity(next_y[0], \'target_0\')\n\n                # Set runhook to initialize iterator\n                iterator_initializer_hook.iterator_initializer_func = \\\n                    lambda sess: sess.run(\n                        iterator.initializer,\n                        feed_dict={input_placeholder: X,\n                                   output_placeholder: y})\n\n                # Return batched (features, labels)\n                return next_X, next_y\n\n        # Return function and hook\n        return train_inputs, iterator_initializer_hook\n\n    return get_inputs()\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(\n                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config\', type=str, default=\'config\',\n                        help=\'config file name\')\n    args = parser.parse_args()\n\n    Config(args.config)\n\n    prepare_raw_data()\n    process_data()\n'"
hook.py,2,"b'\nfrom hbconfig import Config\nimport numpy as np\nimport tensorflow as tf\n\n\n\n\ndef print_variables(variables, rev_vocab=None, every_n_iter=100):\n\n    return tf.train.LoggingTensorHook(\n        variables,\n        every_n_iter=every_n_iter,\n        formatter=format_variable(variables, rev_vocab=rev_vocab))\n\n\ndef format_variable(keys, rev_vocab=None):\n\n    def to_str(sequence):\n        if type(sequence) == np.ndarray:\n            tokens = [\n                rev_vocab.get(x, \'\') for x in sequence if x != Config.data.PAD_ID]\n            return \' \'.join(tokens)\n        else:\n            x = int(sequence)\n            return rev_vocab[x]\n\n    def format(values):\n        result = []\n        for key in keys:\n            if rev_vocab is None:\n                result.append(f""{key} = {values[key]}"")\n            else:\n                result.append(f""{key} = {to_str(values[key])}"")\n\n        try:\n            return \'\\n - \'.join(result)\n        except:\n            pass\n\n    return format\n\n\ndef get_rev_vocab(vocab):\n    if vocab is None:\n        return None\n    return {idx: key for key, idx in vocab.items()}\n\n\ndef print_target(variables, every_n_iter=100):\n\n    return tf.train.LoggingTensorHook(\n        variables,\n        every_n_iter=every_n_iter,\n        formatter=print_pos_or_neg(variables))\n\n\ndef print_pos_or_neg(keys):\n\n    def format(values):\n        result = []\n        for key in keys:\n            if type(values[key]) == np.ndarray:\n                value = max(values[key])\n            else:\n                value = values[key]\n            result.append(f""{key} = {value}"")\n\n        try:\n            return \', \'.join(result)\n        except:\n            pass\n\n    return format\n'"
main.py,6,"b'#-- coding: utf-8 -*-\n\nimport argparse\nimport atexit\nimport logging\n\nfrom hbconfig import Config\nimport tensorflow as tf\n\nimport data_loader\nimport hook\nfrom model import Model\nimport utils\n\n\ndef experiment_fn(run_config, params):\n\n    model = Model()\n    estimator = tf.estimator.Estimator(\n            model_fn=model.model_fn,\n            model_dir=Config.train.model_dir,\n            params=params,\n            config=run_config)\n\n    vocab = data_loader.load_vocab(""vocab"")\n    Config.data.vocab_size = len(vocab)\n\n    train_data, test_data = data_loader.make_train_and_test_set()\n    train_input_fn, train_input_hook = data_loader.make_batch(train_data,\n                                                              batch_size=Config.model.batch_size,\n                                                              scope=""train"")\n    test_input_fn, test_input_hook = data_loader.make_batch(test_data,\n                                                            batch_size=Config.model.batch_size,\n                                                            scope=""test"")\n\n    train_hooks = [train_input_hook]\n    if Config.train.print_verbose:\n        train_hooks.append(hook.print_variables(\n            variables=[\'train/input_0\'],\n            rev_vocab=get_rev_vocab(vocab),\n            every_n_iter=Config.train.check_hook_n_iter))\n        train_hooks.append(hook.print_target(\n            variables=[\'train/target_0\', \'train/pred_0\'],\n            every_n_iter=Config.train.check_hook_n_iter))\n    if Config.train.debug:\n        train_hooks.append(tf_debug.LocalCLIDebugHook())\n\n    eval_hooks = [test_input_hook]\n    if Config.train.debug:\n        eval_hooks.append(tf_debug.LocalCLIDebugHook())\n\n    experiment = tf.contrib.learn.Experiment(\n        estimator=estimator,\n        train_input_fn=train_input_fn,\n        eval_input_fn=test_input_fn,\n        train_steps=Config.train.train_steps,\n        min_eval_frequency=Config.train.min_eval_frequency,\n        train_monitors=train_hooks,\n        eval_hooks=eval_hooks\n    )\n    return experiment\n\n\ndef get_rev_vocab(vocab):\n    if vocab is None:\n        return None\n    return {idx: key for key, idx in vocab.items()}\n\n\ndef main(mode):\n    params = tf.contrib.training.HParams(**Config.model.to_dict())\n\n    run_config = tf.contrib.learn.RunConfig(\n            model_dir=Config.train.model_dir,\n            save_checkpoints_steps=Config.train.save_checkpoints_steps)\n\n    tf.contrib.learn.learn_runner.run(\n        experiment_fn=experiment_fn,\n        run_config=run_config,\n        schedule=mode,\n        hparams=params\n    )\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(\n                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config\', type=str, default=\'config\',\n                        help=\'config file name\')\n    parser.add_argument(\'--mode\', type=str, default=\'train\',\n                        help=\'Mode (train/test/train_and_evaluate)\')\n    args = parser.parse_args()\n\n    tf.logging._logger.setLevel(logging.INFO)\n\n    # Print Config setting\n    Config(args.config)\n    print(""Config: "", Config)\n    if Config.get(""description"", None):\n        print(""Config Description"")\n        for key, value in Config.description.items():\n            print(f"" - {key}: {value}"")\n\n    # After terminated Notification to Slack\n    atexit.register(utils.send_message_to_slack, config_name=args.config)\n\n    main(args.mode)\n'"
model.py,8,"b'from __future__ import print_function\n\n\nfrom hbconfig import Config\nimport tensorflow as tf\nfrom tensorflow.contrib import layers\n\nimport text_cnn\n\n\n\nclass Model:\n\n    def __init__(self):\n        pass\n\n    def model_fn(self, mode, features, labels, params):\n        self.dtype = tf.float32\n\n        self.mode = mode\n        self.params = params\n\n        self.loss, self.train_op, self.metrics, self.predictions = None, None, None, None\n        self._init_placeholder(features, labels)\n        self.build_graph()\n\n        # train mode: required loss and train_op\n        # eval mode: required loss\n        # predict mode: required predictions\n\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            loss=self.loss,\n            train_op=self.train_op,\n            eval_metric_ops=self.metrics,\n            predictions={""prediction"": self.predictions})\n\n    def _init_placeholder(self, features, labels):\n        self.input_data = features\n        if type(features) == dict:\n            self.input_data = features[""input_data""]\n\n        self.targets = labels\n\n    def build_graph(self):\n        graph = text_cnn.Graph(self.mode)\n        output = graph.build(self.input_data)\n\n        self._build_prediction(output)\n        if self.mode != tf.estimator.ModeKeys.PREDICT:\n            self._build_loss(output)\n            self._build_optimizer()\n            self._build_metric()\n\n    def _build_loss(self, output):\n        self.loss = tf.losses.softmax_cross_entropy(\n                self.targets,\n                output,\n                scope=""loss"")\n\n    def _build_prediction(self, output):\n        tf.argmax(output[0], name=\'train/pred_0\') # for print_verbose\n        self.predictions = tf.argmax(output, axis=1)\n\n    def _build_optimizer(self):\n        self.train_op = layers.optimize_loss(\n            self.loss, tf.train.get_global_step(),\n            optimizer=\'Adam\',\n            learning_rate=Config.train.learning_rate,\n            summaries=[\'loss\', \'learning_rate\'],\n            name=""train_op"")\n\n    def _build_metric(self):\n        self.metrics = {\n            ""accuracy"": tf.metrics.accuracy(tf.argmax(self.targets, axis=1), self.predictions)\n        }\n'"
predict.py,6,"b'\n#-*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport sys\n\nfrom hbconfig import Config\nimport numpy as np\nimport tensorflow as tf\n\nimport data_loader\nfrom model import Model\n\n\n\ndef predict(ids):\n\n    X = np.array(data_loader._pad_input(ids, Config.data.max_seq_length), dtype=np.int32)\n    X = np.reshape(X, (1, Config.data.max_seq_length))\n\n    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n            x={""input_data"": X},\n            num_epochs=1,\n            shuffle=False)\n\n    estimator = _make_estimator()\n    result = estimator.predict(input_fn=predict_input_fn)\n\n    prediction = next(result)[""prediction""]\n    return prediction\n\n\ndef _make_estimator():\n    params = tf.contrib.training.HParams(**Config.model.to_dict())\n    # Using CPU\n    run_config = tf.contrib.learn.RunConfig(\n        model_dir=Config.train.model_dir,\n        session_config=tf.ConfigProto(\n            device_count={\'GPU\': 0}\n        ))\n\n    model = Model()\n    return tf.estimator.Estimator(\n            model_fn=model.model_fn,\n            model_dir=Config.train.model_dir,\n            params=params,\n            config=run_config)\n\n\ndef _get_user_input():\n    """""" Get user\'s input, which will be transformed into encoder input later """"""\n    print(""> "", end="""")\n    sys.stdout.flush()\n    return sys.stdin.readline()\n\n\ndef main():\n    data_loader.set_max_seq_length([\'train_X_ids\', \'test_X_ids\'])\n    vocab = data_loader.load_vocab(""vocab"")\n    Config.data.vocab_size = len(vocab)\n\n    print(""Typing anything :) \\n"")\n\n    while True:\n        sentence = _get_user_input()\n        ids = data_loader.sentence2id(vocab, sentence)\n\n        if len(ids) > Config.data.max_seq_length:\n            print(f""Max length I can handle is: {Config.data.max_seq_length}"")\n            continue\n\n        result = predict(ids)\n        print(result)\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(\n                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config\', type=str, default=\'config\',\n                        help=\'config file name\')\n    args = parser.parse_args()\n\n    Config(args.config)\n    Config.model.batch_size = 1\n\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n    tf.logging.set_verbosity(tf.logging.ERROR)\n\n    main()\n'"
utils.py,0,"b'\nimport json\nimport os.path\n\nfrom hbconfig import Config\nimport requests\n\n\n\ndef send_message_to_slack(config_name):\n    project_name = os.path.basename(os.path.abspath("".""))\n\n    data = {\n        ""text"": f""The learning is finished with *{project_name}* Project using `{config_name}` config.""\n    }\n\n    webhook_url = Config.slack.webhook_url\n    if webhook_url == """":\n        print(data[""text""])\n    else:\n        requests.post(Config.slack.webhook_url, data=json.dumps(data))\n'"
text_cnn/__init__.py,17,"b'\nfrom hbconfig import Config\nimport tensorflow as tf\n\n\n\nclass Graph:\n\n    def __init__(self, mode, dtype=tf.float32):\n        self.mode = mode\n        self.dtype = dtype\n\n    def build(self, input_data):\n        embedding_input = self.build_embed(input_data)\n        conv_output = self.build_conv_layers(embedding_input)\n        return self.build_fully_connected_layers(conv_output)\n\n    def build_embed(self, input_data):\n        with tf.variable_scope(""embeddings"", dtype=self.dtype) as scope:\n            embed_type = Config.model.embed_type\n\n            if embed_type == ""rand"":\n                embedding = tf.get_variable(\n                        ""embedding-rand"",\n                        [Config.data.vocab_size, Config.model.embed_dim],\n                        self.dtype)\n            elif embed_type == ""static"":\n                raise NotImplementedError(""CNN-static not implemented yet."")\n            elif embed_type == ""non-static"":\n                raise NotImplementedError(""CNN-non-static not implemented yet."")\n            elif embed_type == ""multichannel"":\n                raise NotImplementedError(""CNN-multichannel not implemented yet."")\n            else:\n                raise ValueError(f""Unknown embed_type {self.embed_type}"")\n\n            return tf.expand_dims(tf.nn.embedding_lookup(embedding, input_data), -1)\n\n    def build_conv_layers(self, embedding_input):\n        with tf.variable_scope(""convolutions"", dtype=self.dtype) as scope:\n            pooled_outputs = self._build_conv_maxpool(embedding_input)\n\n            num_total_filters = Config.model.num_filters * len(Config.model.filter_sizes)\n            concat_pooled = tf.concat(pooled_outputs, 3)\n            flat_pooled = tf.reshape(concat_pooled, [-1, num_total_filters])\n\n            if self.mode == tf.estimator.ModeKeys.TRAIN:\n                h_dropout = tf.layers.dropout(flat_pooled, Config.model.dropout)\n            else:\n                h_dropout = tf.layers.dropout(flat_pooled, 0)\n            return h_dropout\n\n    def _build_conv_maxpool(self, embedding_input):\n        pooled_outputs = []\n        for filter_size in Config.model.filter_sizes:\n            with tf.variable_scope(f""conv-maxpool-{filter_size}-filter""):\n                conv = tf.layers.conv2d(\n                        embedding_input,\n                        Config.model.num_filters,\n                        (filter_size, Config.model.embed_dim),\n                        activation=tf.nn.relu)\n\n                pool = tf.layers.max_pooling2d(\n                        conv,\n                        (Config.data.max_seq_length - filter_size + 1, 1),\n                        (1, 1))\n\n                pooled_outputs.append(pool)\n        return pooled_outputs\n\n    def build_fully_connected_layers(self, conv_output):\n        with tf.variable_scope(""fully-connected"", dtype=self.dtype) as scope:\n            return tf.layers.dense(\n                    conv_output,\n                    Config.data.num_classes,\n                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n'"
