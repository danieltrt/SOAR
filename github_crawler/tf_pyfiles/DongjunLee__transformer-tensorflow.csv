file_path,api_count,code
data_loader.py,9,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\n\nimport argparse\nimport operator\nimport os\nimport random\nimport re\n\nfrom nltk.tokenize import TweetTokenizer\nimport numpy as np\nfrom hbconfig import Config\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n\n\ntokenizer = TweetTokenizer()\n\n\nclass IteratorInitializerHook(tf.train.SessionRunHook):\n    """"""Hook to initialise data iterator after Session is created.""""""\n\n    def __init__(self):\n        super(IteratorInitializerHook, self).__init__()\n        self.iterator_initializer_func = None\n\n    def after_create_session(self, session, coord):\n        """"""Initialise the iterator after the session has been created.""""""\n        self.iterator_initializer_func(session)\n\n\ndef get_dataset_batch(data, buffer_size=10000, batch_size=64, scope=""train""):\n\n    iterator_initializer_hook = IteratorInitializerHook()\n\n    def inputs():\n        with tf.name_scope(scope):\n\n            nonlocal data\n            enc_inputs, targets = data\n\n            # Define placeholders\n            enc_placeholder = tf.placeholder(\n                tf.int32, [None, None], name=""enc_placeholder"")\n            target_placeholder = tf.placeholder(\n                tf.int32, [None, None], name=""target_placeholder"")\n\n            # Build dataset iterator\n            dataset = tf.data.Dataset.from_tensor_slices(\n                (enc_placeholder, target_placeholder))\n\n            if scope == ""train"":\n                dataset = dataset.repeat(None)  # Infinite iterations\n            else:\n                dataset = dataset.repeat(1)  # one Epoch\n\n            dataset = dataset.shuffle(buffer_size=buffer_size)\n            dataset = dataset.batch(batch_size)\n\n            iterator = dataset.make_initializable_iterator()\n            next_enc, next_target = iterator.get_next()\n\n            tf.identity(next_enc[0], \'enc_0\')\n            tf.identity(next_target[0], \'target_0\')\n\n            # Set runhook to initialize iterator\n            iterator_initializer_hook.iterator_initializer_func = \\\n                lambda sess: sess.run(\n                    iterator.initializer,\n                    feed_dict={enc_placeholder: enc_inputs,\n                               target_placeholder: targets})\n\n            # Return batched (features, labels)\n            return {""enc_inputs"": next_enc}, next_target\n\n    # Return function and hook\n    return inputs, iterator_initializer_hook\n\n\ndef prepare_dataset(questions, answers):\n\n    # random convos to create the test set\n    test_ids = random.sample([i for i in range(len(questions))], Config.data.testset_size)\n\n    filenames = [\'train.enc\', \'train.dec\', \'test.enc\', \'test.dec\']\n    files = []\n    for filename in filenames:\n        files.append(open(os.path.join(Config.data.base_path, Config.data.processed_path, filename), \'wb\'))\n\n    for i in tqdm(range(len(questions))):\n\n        question = questions[i]\n        answer = answers[i]\n\n        if i in test_ids:\n            files[2].write((question + ""\\n"").encode(\'utf-8\'))\n            files[3].write((answer + \'\\n\').encode(\'utf-8\'))\n        else:\n            files[0].write((question + \'\\n\').encode(\'utf-8\'))\n            files[1].write((answer + \'\\n\').encode(\'utf-8\'))\n\n    for file in files:\n        file.close()\n\n\ndef make_dir(path):\n    """""" Create a directory if there isn\'t one already. """"""\n    try:\n        os.mkdir(path)\n    except OSError:\n        pass\n\n\ndef basic_tokenizer(line, normalize_digits=True):\n    """""" A basic tokenizer to tokenize text into tokens.\n    Feel free to change this to suit your need. """"""\n    line = re.sub(\'<u>\', \'\', line)\n    line = re.sub(\'</u>\', \'\', line)\n    line = re.sub(\'\\[\', \'\', line)\n    line = re.sub(\'\\]\', \'\', line)\n    words = []\n    _WORD_SPLIT = re.compile(""([.,!?\\""\'-<>:;)(])"")\n    _DIGIT_RE = re.compile(r""\\d"")\n    for fragment in line.strip().lower().split():\n        for token in re.split(_WORD_SPLIT, fragment):\n            if not token:\n                continue\n            if normalize_digits:\n                token = re.sub(_DIGIT_RE, \'#\', token)\n            words.append(token)\n    return words\n\n\ndef build_vocab(in_fname, out_fname, normalize_digits=True):\n    print(""Count each vocab frequency ..."")\n\n    def count_vocab(fname):\n        vocab = {}\n        with open(fname, \'rb\') as f:\n            for line in tqdm(f.readlines()):\n                line = line.decode(\'utf-8\')\n                for token in tokenizer.tokenize(line):\n                    if not token in vocab:\n                        vocab[token] = 0\n                    vocab[token] += 1\n        return vocab\n\n    in_path = os.path.join(Config.data.base_path, Config.data.raw_data_path, in_fname)\n    out_path = os.path.join(Config.data.base_path, Config.data.raw_data_path, out_fname)\n\n    source_vocab = count_vocab(in_path)\n    target_vocab = count_vocab(out_path)\n\n    print(""total vocab size:"", len(source_vocab), len(target_vocab))\n\n    def write_vocab(fname, sorted_vocab):\n        dest_path = os.path.join(Config.data.base_path, Config.data.processed_path, fname)\n        with open(dest_path, \'wb\') as f:\n            f.write((\'<pad>\' + \'\\n\').encode(\'utf-8\'))\n            f.write((\'<unk>\' + \'\\n\').encode(\'utf-8\'))\n            f.write((\'<s>\' + \'\\n\').encode(\'utf-8\'))\n            f.write((\'<\\s>\' + \'\\n\').encode(\'utf-8\'))\n            index = 4\n            for word, count in tqdm(sorted_vocab):\n                if count < Config.data.word_threshold:\n                    break\n\n                f.write((word + \'\\n\').encode(\'utf-8\'))\n                index += 1\n\n    sorted_source_vocab = sorted(source_vocab.items(), key=operator.itemgetter(1), reverse=True)\n    sorted_target_vocab = sorted(target_vocab.items(), key=operator.itemgetter(1), reverse=True)\n\n    write_vocab(""source_vocab"", sorted_source_vocab)\n    write_vocab(""target_vocab"", sorted_target_vocab)\n\ndef load_vocab(vocab_fname):\n    print(""load vocab ..."")\n    with open(os.path.join(Config.data.base_path, Config.data.processed_path, vocab_fname), \'rb\') as f:\n        words = f.read().decode(\'utf-8\').splitlines()\n        print(""vocab size:"", len(words))\n    return {words[i]: i for i in range(len(words))}\n\n\ndef sentence2id(vocab, line):\n    return [vocab.get(token, vocab[\'<unk>\']) for token in tokenizer.tokenize(line)]\n\n\ndef token2id(data, mode):\n    """""" Convert all the tokens in the data into their corresponding\n    index in the vocabulary. """"""\n    vocab_path = \'vocab\'\n    if mode == ""enc"":\n        vocab_path = \'source_\' + vocab_path\n    elif mode == ""dec"":\n        vocab_path = \'target_\' + vocab_path\n\n    in_path = data + \'.\' + mode\n    out_path = data + \'_ids.\' + mode\n\n    vocab = load_vocab(vocab_path)\n    in_file = open(os.path.join(Config.data.base_path, Config.data.raw_data_path, in_path), \'rb\')\n    out_file = open(os.path.join(Config.data.base_path, Config.data.processed_path, out_path), \'wb\')\n\n    lines = in_file.read().decode(\'utf-8\').splitlines()\n    for line in tqdm(lines):\n        ids = []\n\n        sentence_ids = sentence2id(vocab, line)\n        ids.extend(sentence_ids)\n        if mode == \'dec\':\n            ids.append(vocab[\'<\\s>\'])\n            ids.append(vocab[\'<pad>\'])\n\n        out_file.write(b\' \'.join(str(id_).encode(\'utf-8\') for id_ in ids) + b\'\\n\')\n\n\ndef process_data():\n    print(\'Preparing data to be model-ready ...\')\n\n    # create path to store all the train & test encoder & decoder\n    make_dir(Config.data.base_path + Config.data.processed_path)\n\n    build_vocab(\'train.enc\', \'train.dec\')\n\n    token2id(\'train\', \'enc\')\n    token2id(\'train\', \'dec\')\n    token2id(\'test\', \'enc\')\n    token2id(\'test\', \'dec\')\n\n\ndef make_train_and_test_set(shuffle=True):\n    print(""make Training data and Test data Start...."")\n\n    if Config.data.get(\'max_seq_length\', None) is None:\n        set_max_seq_length([\'train_ids.enc\', \'train_ids.dec\', \'test_ids.enc\', \'test_ids.dec\'])\n\n    train_enc, train_dec = load_data(\'train_ids.enc\', \'train_ids.dec\')\n    test_enc, test_dec = load_data(\'test_ids.enc\', \'test_ids.dec\', train=False)\n\n    assert len(train_enc) == len(train_dec)\n    assert len(test_enc) == len(test_dec)\n\n    print(f""train data count : {len(train_dec)}"")\n    print(f""test data count : {len(test_dec)}"")\n\n    if shuffle:\n        print(""shuffle dataset ..."")\n        train_p = np.random.permutation(len(train_dec))\n        test_p = np.random.permutation(len(test_dec))\n\n        return ((train_enc[train_p], train_dec[train_p]),\n                (test_enc[test_p], test_dec[test_p]))\n    else:\n        return ((train_enc, train_dec),\n                (test_enc, test_dec))\n\ndef load_data(enc_fname, dec_fname, train=True):\n    enc_input_data = open(os.path.join(Config.data.base_path, Config.data.processed_path, enc_fname), \'r\')\n    dec_input_data = open(os.path.join(Config.data.base_path, Config.data.processed_path, dec_fname), \'r\')\n\n    enc_data, dec_data = [], []\n    for e_line, d_line in tqdm(zip(enc_input_data.readlines(), dec_input_data.readlines())):\n        e_ids = [int(id_) for id_ in e_line.split()]\n        d_ids = [int(id_) for id_ in d_line.split()]\n\n        if len(e_ids) == 0 or len(d_ids) == 0:\n            continue\n\n        if len(e_ids) <= Config.data.max_seq_length and len(d_ids) < Config.data.max_seq_length:\n            enc_data.append(_pad_input(e_ids, Config.data.max_seq_length))\n            dec_data.append(_pad_input(d_ids, Config.data.max_seq_length))\n\n    print(f""load data from {enc_fname}, {dec_fname}..."")\n    return np.array(enc_data, dtype=np.int32), np.array(dec_data, dtype=np.int32)\n\n\ndef _pad_input(input_, size):\n    return input_ + [Config.data.PAD_ID] * (size - len(input_))\n\n\ndef set_max_seq_length(dataset_fnames):\n\n    max_seq_length = Config.data.get(\'max_seq_length\', 10)\n\n    for fname in dataset_fnames:\n        input_data = open(os.path.join(Config.data.base_path, Config.data.processed_path, fname), \'r\')\n\n        for line in input_data.readlines():\n            ids = [int(id_) for id_ in line.split()]\n            seq_length = len(ids)\n\n            if seq_length > max_seq_length:\n                max_seq_length = seq_length\n\n    Config.data.max_seq_length = max_seq_length\n    print(f""Setting max_seq_length to Config : {max_seq_length}"")\n\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(\n                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config\', type=str, default=\'config\',\n                        help=\'config file name\')\n    args = parser.parse_args()\n\n    Config(args.config)\n\n    process_data()\n'"
hook.py,1,"b'\nfrom hbconfig import Config\nimport numpy as np\nimport tensorflow as tf\n\n\n\ndef print_variables(variables, rev_vocab=None, every_n_iter=100):\n\n    return tf.train.LoggingTensorHook(\n        variables,\n        every_n_iter=every_n_iter,\n        formatter=format_variable(variables, rev_vocab=rev_vocab))\n\n\ndef format_variable(keys, rev_vocab=None):\n\n    def to_str(sequence):\n        if type(sequence) == np.ndarray:\n            tokens = [\n                rev_vocab.get(x, \'\') for x in sequence if x != Config.data.PAD_ID]\n            return \' \'.join(tokens)\n        else:\n            x = int(sequence)\n            return rev_vocab[x]\n\n    def format(values):\n        result = []\n        for key in keys:\n            if rev_vocab is None:\n                result.append(f""{key} = {values[key]}"")\n            else:\n                result.append(f""{key} = {to_str(values[key])}"")\n\n        try:\n            return \'\\n - \'.join(result)\n        except:\n            pass\n\n    return format\n\n\n\n'"
main.py,6,"b'#-- coding: utf-8 -*-\n\nimport argparse\nimport atexit\nimport logging\n\nfrom hbconfig import Config\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\n\nimport data_loader\nfrom model import Model\nimport hook\nimport utils\n\n\n\ndef experiment_fn(run_config, params):\n\n    model = Model()\n    estimator = tf.estimator.Estimator(\n            model_fn=model.model_fn,\n            model_dir=Config.train.model_dir,\n            params=params,\n            config=run_config)\n\n    source_vocab = data_loader.load_vocab(""source_vocab"")\n    target_vocab = data_loader.load_vocab(""target_vocab"")\n\n    Config.data.rev_source_vocab = utils.get_rev_vocab(source_vocab)\n    Config.data.rev_target_vocab = utils.get_rev_vocab(target_vocab)\n    Config.data.source_vocab_size = len(source_vocab)\n    Config.data.target_vocab_size = len(target_vocab)\n\n    train_data, test_data = data_loader.make_train_and_test_set()\n    train_input_fn, train_input_hook = data_loader.get_dataset_batch(train_data,\n                                                                     batch_size=Config.model.batch_size,\n                                                                     scope=""train"")\n    test_input_fn, test_input_hook = data_loader.get_dataset_batch(test_data,\n                                                                   batch_size=Config.model.batch_size,\n                                                                   scope=""test"")\n\n    train_hooks = [train_input_hook]\n    if Config.train.print_verbose:\n        train_hooks.append(hook.print_variables(\n            variables=[\'train/enc_0\'],\n            rev_vocab=utils.get_rev_vocab(source_vocab),\n            every_n_iter=Config.train.check_hook_n_iter))\n        train_hooks.append(hook.print_variables(\n            variables=[\'train/target_0\', \'train/pred_0\'],\n            rev_vocab=utils.get_rev_vocab(target_vocab),\n            every_n_iter=Config.train.check_hook_n_iter))\n    if Config.train.debug:\n        train_hooks.append(tf_debug.LocalCLIDebugHook())\n\n    eval_hooks = [test_input_hook]\n    if Config.train.debug:\n        eval_hooks.append(tf_debug.LocalCLIDebugHook())\n\n    experiment = tf.contrib.learn.Experiment(\n        estimator=estimator,\n        train_input_fn=train_input_fn,\n        eval_input_fn=test_input_fn,\n        train_steps=Config.train.train_steps,\n        min_eval_frequency=Config.train.min_eval_frequency,\n        train_monitors=train_hooks,\n        eval_hooks=eval_hooks\n    )\n    return experiment\n\n\ndef main(mode):\n    params = tf.contrib.training.HParams(**Config.model.to_dict())\n\n    run_config = tf.contrib.learn.RunConfig(\n            model_dir=Config.train.model_dir,\n            save_checkpoints_steps=Config.train.save_checkpoints_steps)\n\n    tf.contrib.learn.learn_runner.run(\n        experiment_fn=experiment_fn,\n        run_config=run_config,\n        schedule=mode,\n        hparams=params\n    )\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(\n                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config\', type=str, default=\'config\',\n                        help=\'config file name\')\n    parser.add_argument(\'--mode\', type=str, default=\'train\',\n                        help=\'Mode (train/test/train_and_evaluate)\')\n    args = parser.parse_args()\n\n    tf.logging.set_verbosity(logging.INFO)\n\n    # Print Config setting\n    Config(args.config)\n    print(""Config: "", Config)\n    if Config.get(""description"", None):\n        print(""Config Description"")\n        for key, value in Config.description.items():\n            print(f"" - {key}: {value}"")\n\n    # After terminated Notification to Slack\n    atexit.register(utils.send_message_to_slack, config_name=args.config)\n\n    main(args.mode)\n'"
model.py,21,"b'from __future__ import print_function\n\n\nfrom hbconfig import Config\nimport tensorflow as tf\n\nimport nltk\nimport transformer\n\n\n\nclass Model:\n\n    def __init__(self):\n        pass\n\n    def model_fn(self, mode, features, labels, params):\n        self.dtype = tf.float32\n\n        self.mode = mode\n        self.params = params\n\n        self.loss, self.train_op, self.metrics, self.predictions = None, None, None, None\n        self._init_placeholder(features, labels)\n        self.build_graph()\n\n        # train mode: required loss and train_op\n        # eval mode: required loss\n        # predict mode: required predictions\n\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            loss=self.loss,\n            train_op=self.train_op,\n            eval_metric_ops=self.metrics,\n            predictions={""prediction"": self.predictions})\n\n    def _init_placeholder(self, features, labels):\n        self.encoder_inputs = features[""enc_inputs""]\n        self.targets = labels\n\n        self.batch_size = tf.shape(self.encoder_inputs)[0]\n        start_tokens = tf.fill([self.batch_size, 1], Config.data.START_ID)\n\n        if self.mode == tf.estimator.ModeKeys.TRAIN:\n            # slice last pad token\n            target_slice_last_1 = tf.slice(self.targets, [0, 0],\n                    [self.batch_size, Config.data.max_seq_length-1])\n            self.decoder_inputs = tf.concat([start_tokens, target_slice_last_1], axis=1)\n\n            tf.identity(self.decoder_inputs[0], \'train/dec_0\')\n        else:\n            pad_tokens = tf.zeros([self.batch_size, Config.data.max_seq_length-1], dtype=tf.int32) # 0: PAD ID\n            self.decoder_inputs = tf.concat([start_tokens, pad_tokens], axis=1)\n\n            tf.identity(self.decoder_inputs[0], \'test/dec_0\')\n\n    def build_graph(self):\n        graph = transformer.Graph(self.mode)\n        output, predictions = graph.build(encoder_inputs=self.encoder_inputs,\n                             decoder_inputs=self.decoder_inputs)\n\n        self.predictions = predictions\n        if self.mode != tf.estimator.ModeKeys.PREDICT:\n            self._build_loss(output)\n            self._build_optimizer()\n            self._build_metric()\n\n    def _build_loss(self, logits):\n        with tf.variable_scope(\'loss\'):\n            target_lengths = tf.reduce_sum(\n                    tf.to_int32(tf.not_equal(self.targets, Config.data.PAD_ID)), 1)\n            weight_masks = tf.sequence_mask(\n                    lengths=target_lengths,\n                    maxlen=Config.data.max_seq_length,\n                    dtype=self.dtype, name=\'masks\')\n\n            self.loss = tf.contrib.seq2seq.sequence_loss(\n                    logits=logits,\n                    targets=self.targets,\n                    weights=weight_masks,\n                    name=""sequence-loss"")\n\n    def _build_optimizer(self):\n        self.train_op = tf.contrib.layers.optimize_loss(\n            self.loss, tf.train.get_global_step(),\n            optimizer=Config.train.get(\'optimizer\', \'Adam\'),\n            learning_rate=Config.train.learning_rate,\n            summaries=[\'loss\', \'gradients\', \'learning_rate\'],\n            name=""train_op"")\n\n    def _build_metric(self):\n\n        def blue_score(labels, predictions,\n                       weights=None, metrics_collections=None,\n                       updates_collections=None, name=None):\n\n            def _nltk_blue_score(labels, predictions):\n\n                # slice after <eos>\n                predictions = predictions.tolist()\n                for i in range(len(predictions)):\n                    prediction = predictions[i]\n                    if Config.data.EOS_ID in prediction:\n                        predictions[i] = prediction[:prediction.index(Config.data.EOS_ID)+1]\n\n                rev_target_vocab = Config.data.rev_target_vocab\n\n                labels = [\n                    [[rev_target_vocab.get(w_id, """") for w_id in label if w_id != Config.data.PAD_ID]]\n                    for label in labels.tolist()]\n                predictions = [\n                    [rev_target_vocab.get(w_id, """") for w_id in prediction]\n                    for prediction in predictions]\n\n                if Config.train.print_verbose:\n                    print(""label: "", labels[0][0])\n                    print(""prediction: "", predictions[0])\n\n                return float(nltk.translate.bleu_score.corpus_bleu(labels, predictions))\n\n            score = tf.py_func(_nltk_blue_score, (labels, predictions), tf.float64)\n            return tf.metrics.mean(score * 100.0)\n\n        self.metrics = {\n            ""bleu"": blue_score(self.targets, self.predictions)\n        }\n'"
predict.py,6,"b'#-*- coding: utf-8 -*-\n\nimport argparse\nimport os\n\nfrom hbconfig import Config\nimport numpy as np\nimport tensorflow as tf\n\nimport data_loader\nfrom model import Model\nimport utils\n\n\n\n\ndef main(ids, vocab):\n\n    X = np.array(data_loader._pad_input(ids, Config.data.max_seq_length), dtype=np.int32)\n    X = np.reshape(X, (1, Config.data.max_seq_length))\n\n    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={""enc_inputs"": X},\n        num_epochs=1,\n        shuffle=False)\n\n    estimator = _make_estimator()\n    result = estimator.predict(input_fn=predict_input_fn)\n\n    prediction = next(result)[""prediction""]\n\n    rev_vocab = utils.get_rev_vocab(vocab)\n    def to_str(sequence):\n        tokens = [\n            rev_vocab.get(x, \'\') for x in sequence if x != Config.data.PAD_ID]\n        return \' \'.join(tokens)\n\n    return to_str(prediction)\n\n\ndef _make_estimator():\n    params = tf.contrib.training.HParams(**Config.model.to_dict())\n    # Using CPU\n    run_config = tf.contrib.learn.RunConfig(\n        model_dir=Config.train.model_dir,\n        session_config=tf.ConfigProto(\n            device_count={\'GPU\': 0}\n        ))\n\n    model = Model()\n    return tf.estimator.Estimator(\n            model_fn=model.model_fn,\n            model_dir=Config.train.model_dir,\n            params=params,\n            config=run_config)\n\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(\n                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--config\', type=str, default=\'config\',\n                        help=\'config file name\')\n    parser.add_argument(\'--src\', type=str, default=\'example source sentence\',\n                        help=\'input source sentence\')\n    args = parser.parse_args()\n\n    Config(args.config)\n    Config.train.batch_size = 1\n\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n    tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # set data property\n    data_loader.set_max_seq_length([\'train_ids.enc\', \'train_ids.dec\', \'test_ids.enc\', \'test_ids.dec\'])\n\n    source_vocab = data_loader.load_vocab(""source_vocab"")\n    target_vocab = data_loader.load_vocab(""target_vocab"")\n\n    Config.data.rev_source_vocab = utils.get_rev_vocab(source_vocab)\n    Config.data.rev_target_vocab = utils.get_rev_vocab(target_vocab)\n    Config.data.source_vocab_size = len(source_vocab)\n    Config.data.target_vocab_size = len(target_vocab)\n\n    print(""------------------------------------"")\n    print(""Source: "" + args.src)\n    token_ids = data_loader.sentence2id(source_vocab, args.src)\n    prediction = main(token_ids, target_vocab)\n\n    print("" > Result: "" + prediction)\n'"
utils.py,0,"b'import json\nimport os.path\n\nfrom hbconfig import Config\nimport requests\n\n\n\ndef get_rev_vocab(vocab):\n    if vocab is None:\n        return None\n    return {idx: key for key, idx in vocab.items()}\n\n\ndef send_message_to_slack(config_name):\n    project_name = os.path.basename(os.path.abspath("".""))\n\n    data = {\n        ""text"": f""The learning is finished with *{project_name}* Project using `{config_name}` config.""\n    }\n\n    webhook_url = Config.slack.webhook_url\n    if webhook_url == """":\n        print(data[""text""])\n    else:\n        requests.post(Config.slack.webhook_url, data=json.dumps(data))\n'"
transformer/__init__.py,27,"b'\nfrom hbconfig import Config\nimport numpy as np\nimport tensorflow as tf\n\nfrom .attention import positional_encoding\nfrom .encoder import Encoder\nfrom .decoder import Decoder\n\n\n\nclass Graph:\n\n    def __init__(self, mode, dtype=tf.float32):\n        self.mode = mode\n        self.dtype = dtype\n\n    def build(self,\n              encoder_inputs=None,\n              decoder_inputs=None):\n\n        self.batch_size = tf.shape(encoder_inputs)[0]\n\n        encoder_emb_inp = self.build_embed(encoder_inputs, encoder=True)\n        self.encoder_outputs = self.build_encoder(encoder_emb_inp)\n\n        decoder_emb_inp = self.build_embed(decoder_inputs, encoder=False, reuse=True)\n        decoder_outputs = self.build_decoder(decoder_emb_inp, self.encoder_outputs)\n        output =  self.build_output(decoder_outputs)\n\n        if self.mode == tf.estimator.ModeKeys.TRAIN:\n            predictions = tf.argmax(output, axis=2)\n            return output, predictions\n        else:\n            next_decoder_inputs = self._filled_next_token(decoder_inputs, output, 1)\n\n            # predict output with loop. [encoder_outputs, decoder_inputs (filled next token)]\n            for i in range(2, Config.data.max_seq_length):\n                decoder_emb_inp = self.build_embed(next_decoder_inputs, encoder=False, reuse=True)\n                decoder_outputs = self.build_decoder(decoder_emb_inp, self.encoder_outputs, reuse=True)\n                next_output = self.build_output(decoder_outputs, reuse=True)\n\n                next_decoder_inputs = self._filled_next_token(next_decoder_inputs, next_output, i)\n\n            # slice start_token\n            decoder_input_start_1 = tf.slice(next_decoder_inputs, [0, 1],\n                    [self.batch_size, Config.data.max_seq_length-1])\n            predictions = tf.concat(\n                    [decoder_input_start_1, tf.zeros([self.batch_size, 1], dtype=tf.int32)], axis=1)\n            return next_output, predictions\n\n    def build_embed(self, inputs, encoder=True, reuse=False):\n        with tf.variable_scope(""Embeddings"", reuse=reuse, dtype=self.dtype) as scope:\n            # Word Embedding\n            embedding_encoder = tf.get_variable(\n                ""embedding_encoder"", [Config.data.source_vocab_size, Config.model.model_dim], self.dtype)\n            embedding_decoder = tf.get_variable(\n                ""embedding_decoder"", [Config.data.target_vocab_size, Config.model.model_dim], self.dtype)\n\n            # Positional Encoding\n            with tf.variable_scope(""positional-encoding""):\n                positional_encoded = positional_encoding(Config.model.model_dim,\n                                                         Config.data.max_seq_length,\n                                                         dtype=self.dtype)\n\n            # Add\n            position_inputs = tf.tile(tf.range(0, Config.data.max_seq_length), [self.batch_size])\n            position_inputs = tf.reshape(position_inputs,\n                                         [self.batch_size, Config.data.max_seq_length]) # batch_size x [0, 1, 2, ..., n]\n\n            if encoder:\n                embedding_inputs = embedding_encoder\n            else:\n                embedding_inputs = embedding_decoder\n\n            encoded_inputs = tf.add(tf.nn.embedding_lookup(embedding_inputs, inputs),\n                             tf.nn.embedding_lookup(positional_encoded, position_inputs))\n\n            return tf.nn.dropout(encoded_inputs, 1.0 - Config.model.dropout)\n\n    def build_encoder(self, encoder_emb_inp, reuse=False):\n        with tf.variable_scope(""Encoder"", reuse=reuse):\n            encoder = Encoder(num_layers=Config.model.num_layers,\n                              num_heads=Config.model.num_heads,\n                              linear_key_dim=Config.model.linear_key_dim,\n                              linear_value_dim=Config.model.linear_value_dim,\n                              model_dim=Config.model.model_dim,\n                              ffn_dim=Config.model.ffn_dim)\n\n            return encoder.build(encoder_emb_inp)\n\n    def build_decoder(self, decoder_emb_inp, encoder_outputs,reuse=False):\n        with tf.variable_scope(""Decoder"", reuse=reuse):\n            decoder = Decoder(num_layers=Config.model.num_layers,\n                              num_heads=Config.model.num_heads,\n                              linear_key_dim=Config.model.linear_key_dim,\n                              linear_value_dim=Config.model.linear_value_dim,\n                              model_dim=Config.model.model_dim,\n                              ffn_dim=Config.model.ffn_dim)\n\n            return decoder.build(decoder_emb_inp, encoder_outputs)\n\n    def build_output(self, decoder_outputs, reuse=False):\n        with tf.variable_scope(""Output"", reuse=reuse):\n            logits = tf.layers.dense(decoder_outputs, Config.data.target_vocab_size)\n\n        self.train_predictions = tf.argmax(logits[0], axis=1, name=""train/pred_0"")\n        return logits\n\n    def _filled_next_token(self, inputs, logits, decoder_index):\n        tf.identity(tf.argmax(logits[0], axis=1, output_type=tf.int32), f\'test/pred_{decoder_index}\')\n\n        next_token = tf.slice(\n                tf.argmax(logits, axis=2, output_type=tf.int32),\n                [0, decoder_index-1],\n                [self.batch_size, 1])\n        left_zero_pads = tf.zeros([self.batch_size, decoder_index], dtype=tf.int32)\n        right_zero_pads = tf.zeros([self.batch_size, (Config.data.max_seq_length-decoder_index-1)], dtype=tf.int32)\n        next_token = tf.concat((left_zero_pads, next_token, right_zero_pads), axis=1)\n\n        return inputs + next_token\n'"
transformer/attention.py,20,"b'\nimport numpy as np\nimport tensorflow as tf\n\n\n\n__all__ = [\n    ""positional_encoding"", ""Attention""\n]\n\n\ndef positional_encoding(dim, sentence_length, dtype=tf.float32):\n\n    encoded_vec = np.array([pos/np.power(10000, 2*i/dim) for pos in range(sentence_length) for i in range(dim)])\n    encoded_vec[::2] = np.sin(encoded_vec[::2])\n    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n\n    return tf.convert_to_tensor(encoded_vec.reshape([sentence_length, dim]), dtype=dtype)\n\n\nclass Attention:\n    """"""Attention class""""""\n\n    def __init__(self,\n                 num_heads=1,\n                 masked=False,\n                 linear_key_dim=50,\n                 linear_value_dim=50,\n                 model_dim=100,\n                 dropout=0.2):\n\n        assert linear_key_dim % num_heads == 0\n        assert linear_value_dim % num_heads == 0\n\n        self.num_heads = num_heads\n        self.masked = masked\n        self.linear_key_dim = linear_key_dim\n        self.linear_value_dim = linear_value_dim\n        self.model_dim = model_dim\n        self.dropout = dropout\n\n    def multi_head(self, q, k, v):\n        q, k, v = self._linear_projection(q, k, v)\n        qs, ks, vs = self._split_heads(q, k, v)\n        outputs = self._scaled_dot_product(qs, ks, vs)\n        output = self._concat_heads(outputs)\n        output = tf.layers.dense(output, self.model_dim)\n\n        return tf.nn.dropout(output, 1.0 - self.dropout)\n\n    def _linear_projection(self, q, k, v):\n        q = tf.layers.dense(q, self.linear_key_dim, use_bias=False)\n        k = tf.layers.dense(k, self.linear_key_dim, use_bias=False)\n        v = tf.layers.dense(v, self.linear_value_dim, use_bias=False)\n        return q, k, v\n\n    def _split_heads(self, q, k, v):\n\n        def split_last_dimension_then_transpose(tensor, num_heads, dim):\n            t_shape = tensor.get_shape().as_list()\n            tensor = tf.reshape(tensor, [-1] + t_shape[1:-1] + [num_heads, dim // num_heads])\n            return tf.transpose(tensor, [0, 2, 1, 3]) # [batch_size, num_heads, max_seq_len, dim]\n\n        qs = split_last_dimension_then_transpose(q, self.num_heads, self.linear_key_dim)\n        ks = split_last_dimension_then_transpose(k, self.num_heads, self.linear_key_dim)\n        vs = split_last_dimension_then_transpose(v, self.num_heads, self.linear_value_dim)\n\n        return qs, ks, vs\n\n    def _scaled_dot_product(self, qs, ks, vs):\n        key_dim_per_head = self.linear_key_dim // self.num_heads\n\n        o1 = tf.matmul(qs, ks, transpose_b=True)\n        o2 = o1 / (key_dim_per_head**0.5)\n\n        if self.masked:\n            diag_vals = tf.ones_like(o2[0, 0, :, :]) # (batch_size, num_heads, query_dim, key_dim)\n            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (q_dim, k_dim)\n            masks = tf.tile(tf.reshape(tril, [1, 1] + tril.get_shape().as_list()),\n                            [tf.shape(o2)[0], tf.shape(o2)[1], 1, 1])\n            paddings = tf.ones_like(masks) * -1e9\n            o2 = tf.where(tf.equal(masks, 0), paddings, o2)\n\n        o3 = tf.nn.softmax(o2)\n        return tf.matmul(o3, vs)\n\n    def _concat_heads(self, outputs):\n\n        def transpose_then_concat_last_two_dimenstion(tensor):\n            tensor = tf.transpose(tensor, [0, 2, 1, 3]) # [batch_size, max_seq_len, num_heads, dim]\n            t_shape = tensor.get_shape().as_list()\n            num_heads, dim = t_shape[-2:]\n            return tf.reshape(tensor, [-1] + t_shape[1:-2] + [num_heads * dim])\n\n        return transpose_then_concat_last_two_dimenstion(outputs)\n'"
transformer/decoder.py,8,"b'\nimport tensorflow as tf\n\nfrom .attention import Attention\nfrom .layer import FFN\n\n\n\nclass Decoder:\n    """"""Decoder class""""""\n\n    def __init__(self,\n                 num_layers=8,\n                 num_heads=8,\n                 linear_key_dim=50,\n                 linear_value_dim=50,\n                 model_dim=50,\n                 ffn_dim=50,\n                 dropout=0.2):\n\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.linear_key_dim = linear_key_dim\n        self.linear_value_dim = linear_value_dim\n        self.model_dim = model_dim\n        self.ffn_dim = ffn_dim\n        self.dropout = dropout\n\n    def build(self, decoder_inputs, encoder_outputs):\n        o1 = tf.identity(decoder_inputs)\n\n        for i in range(1, self.num_layers+1):\n            with tf.variable_scope(f""layer-{i}""):\n                o2 = self._add_and_norm(o1, self._masked_self_attention(q=o1,\n                                                                        k=o1,\n                                                                        v=o1), num=1)\n                o3 = self._add_and_norm(o2, self._encoder_decoder_attention(q=o2,\n                                                                            k=encoder_outputs,\n                                                                            v=encoder_outputs), num=2)\n                o4 = self._add_and_norm(o3, self._positional_feed_forward(o3), num=3)\n                o1 = tf.identity(o4)\n\n        return o4\n\n    def _masked_self_attention(self, q, k, v):\n        with tf.variable_scope(""masked-self-attention""):\n            attention = Attention(num_heads=self.num_heads,\n                                    masked=True,  # Not implemented yet\n                                    linear_key_dim=self.linear_key_dim,\n                                    linear_value_dim=self.linear_value_dim,\n                                    model_dim=self.model_dim,\n                                    dropout=self.dropout)\n            return attention.multi_head(q, k, v)\n\n    def _add_and_norm(self, x, sub_layer_x, num=0):\n        with tf.variable_scope(f""add-and-norm-{num}""):\n            return tf.contrib.layers.layer_norm(tf.add(x, sub_layer_x)) # with Residual connection\n\n    def _encoder_decoder_attention(self, q, k, v):\n        with tf.variable_scope(""encoder-decoder-attention""):\n            attention = Attention(num_heads=self.num_heads,\n                                    masked=False,\n                                    linear_key_dim=self.linear_key_dim,\n                                    linear_value_dim=self.linear_value_dim,\n                                    model_dim=self.model_dim,\n                                    dropout=self.dropout)\n            return attention.multi_head(q, k, v)\n\n    def _positional_feed_forward(self, output):\n        with tf.variable_scope(""feed-forward""):\n            ffn = FFN(w1_dim=self.ffn_dim,\n                      w2_dim=self.model_dim,\n                      dropout=self.dropout)\n            return ffn.dense_relu_dense(output)\n\n'"
transformer/encoder.py,7,"b'\nimport tensorflow as tf\n\nfrom .attention import Attention\nfrom .layer import FFN\n\n\n\nclass Encoder:\n    """"""Encoder class""""""\n\n    def __init__(self,\n                 num_layers=8,\n                 num_heads=8,\n                 linear_key_dim=50,\n                 linear_value_dim=50,\n                 model_dim=50,\n                 ffn_dim=50,\n                 dropout=0.2):\n\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.linear_key_dim = linear_key_dim\n        self.linear_value_dim = linear_value_dim\n        self.model_dim = model_dim\n        self.ffn_dim = ffn_dim\n        self.dropout = dropout\n\n    def build(self, encoder_inputs):\n        o1 = tf.identity(encoder_inputs)\n\n        for i in range(1, self.num_layers+1):\n            with tf.variable_scope(f""layer-{i}""):\n                o2 = self._add_and_norm(o1, self._self_attention(q=o1,\n                                                                 k=o1,\n                                                                 v=o1), num=1)\n                o3 = self._add_and_norm(o2, self._positional_feed_forward(o2), num=2)\n                o1 = tf.identity(o3)\n\n        return o3\n\n    def _self_attention(self, q, k, v):\n        with tf.variable_scope(""self-attention""):\n            attention = Attention(num_heads=self.num_heads,\n                                    masked=False,\n                                    linear_key_dim=self.linear_key_dim,\n                                    linear_value_dim=self.linear_value_dim,\n                                    model_dim=self.model_dim,\n                                    dropout=self.dropout)\n            return attention.multi_head(q, k, v)\n\n    def _add_and_norm(self, x, sub_layer_x, num=0):\n        with tf.variable_scope(f""add-and-norm-{num}""):\n            return tf.contrib.layers.layer_norm(tf.add(x, sub_layer_x)) # with Residual connection\n\n    def _positional_feed_forward(self, output):\n        with tf.variable_scope(""feed-forward""):\n            ffn = FFN(w1_dim=self.ffn_dim,\n                      w2_dim=self.model_dim,\n                      dropout=self.dropout)\n            return ffn.dense_relu_dense(output)\n\n'"
transformer/layer.py,3,"b'\nimport tensorflow as tf\n\n\n\nclass FFN:\n    """"""FFN class (Position-wise Feed-Forward Networks)""""""\n\n    def __init__(self,\n                 w1_dim=200,\n                 w2_dim=100,\n                 dropout=0.1):\n\n        self.w1_dim = w1_dim\n        self.w2_dim = w2_dim\n        self.dropout = dropout\n\n    def dense_relu_dense(self, inputs):\n        output = tf.layers.dense(inputs, self.w1_dim, activation=tf.nn.relu)\n        output =tf.layers.dense(output, self.w2_dim)\n\n        return tf.nn.dropout(output, 1.0 - self.dropout)\n\n    def conv_relu_conv(self):\n        raise NotImplementedError(""i will implement it!"")\n'"
