file_path,api_count,code
evaluate.py,11,"b'""""""Evaluation script for the DeepLab-LargeFOV network on the validation subset\n   of PASCAL VOC dataset.\n\nThis script evaluates the model on around 1500 validation images.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nfrom datetime import datetime\nimport os\nimport sys\nimport time\n\nfrom PIL import Image\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom deeplab_lfov import DeepLabLFOVModel, ImageReader, decode_labels\n\nDATA_DIRECTORY = \'/home//VOCdevkit\'\nDATA_LIST_PATH = \'./dataset/val.txt\'\nNUM_STEPS = 1449\nRESTORE_FROM = None\nSAVE_DIR = \'./images_val/\'\nWEIGHTS_PATH   = None\n\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n    \n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLabLFOV Network"")\n    parser.add_argument(""--data_dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the PASCAL VOC dataset."")\n    parser.add_argument(""--data_list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the dataset."")\n    parser.add_argument(""--num_steps"", type=int, default=NUM_STEPS,\n                        help=""Number of images in the validation set."")\n    parser.add_argument(""--restore_from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--save_dir"", type=str, default=SAVE_DIR,\n                        help=""Where to save predicted masks."")\n    parser.add_argument(""--weights_path"", type=str, default=WEIGHTS_PATH,\n                        help=""Path to the file with caffemodel weights. ""\n                             ""If not set, all the variables are initialised randomly."")\n    return parser.parse_args()\n\ndef load(saver, sess, ckpt_path):\n    \'\'\'Load trained weights.\n    \n    Args:\n      saver: TensorFlow saver object.\n      sess: TensorFlow session.\n      ckpt_path: path to checkpoint file with parameters.\n    \'\'\' \n    saver.restore(sess, ckpt_path)\n    print(""Restored model parameters from {}"".format(ckpt_path))\n\ndef main():\n    """"""Create the model and start the evaluation process.""""""\n    args = get_arguments()\n    \n    # Create queue coordinator.\n    coord = tf.train.Coordinator()\n    \n    # Load reader.\n    with tf.name_scope(""create_inputs""):\n        reader = ImageReader(\n            args.data_dir,\n            args.data_list,\n            input_size=None,\n            random_scale=False,\n            coord=coord)\n        image, label = reader.image, reader.label\n    image_batch, label_batch = tf.expand_dims(image, dim=0), tf.expand_dims(label, dim=0) # Add the batch dimension.\n    # Create network.\n    net = DeepLabLFOVModel(args.weights_path)\n\n    # Which variables to load.\n    trainable = tf.trainable_variables()\n    \n    # Predictions.\n    pred = net.preds(image_batch)\n    \n    # mIoU\n    mIoU, update_op = tf.contrib.metrics.streaming_mean_iou(pred, label_batch, num_classes=21) \n    \n    # Set up tf session and initialize variables. \n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    init = tf.initialize_all_variables()\n    \n    sess.run(init)\n    sess.run(tf.initialize_local_variables())\n    \n    # Load weights.\n    saver = tf.train.Saver(var_list=trainable)\n    if args.restore_from is not None:\n        load(saver, sess, args.restore_from)\n    \n    # Start queue threads.\n    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n    \n    if not os.path.exists(args.save_dir):\n      os.makedirs(args.save_dir)\n      \n    # Iterate over images.\n    for step in range(args.num_steps):\n        #mIoU_value = sess.run([mIoU])\n        #_ = update_op.eval(session=sess)\n        preds, _ = sess.run([pred, update_op])\n        \n        if args.save_dir is not None:\n            img = decode_labels(preds[0, :, :, 0])\n            im = Image.fromarray(img)\n            im.save(args.save_dir + str(step) + \'.png\')\n        if step % 100 == 0:\n            print(\'step {:d} \\t\'.format(step))\n    print(\'Mean IoU: {:.3f}\'.format(mIoU.eval(session=sess)))\n    coord.request_stop()\n    coord.join(threads)\n    \nif __name__ == \'__main__\':\n    main()\n'"
inference.py,9,"b'""""""Run DeepLab-LargeFOV on a given image.\n\nThis script computes a segmentation mask for a given image.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nfrom datetime import datetime\nimport os\nimport sys\nimport time\n\nfrom PIL import Image\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom deeplab_lfov import DeepLabLFOVModel, ImageReader, decode_labels\n\nSAVE_DIR = \'./output/\'\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n    \n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLabLFOV Network Inference."")\n    parser.add_argument(""img_path"", type=str,\n                        help=""Path to the RGB image file."")\n    parser.add_argument(""model_weights"", type=str,\n                        help=""Path to the file with model weights."")\n    parser.add_argument(""--save_dir"", type=str, default=SAVE_DIR,\n                        help=""Where to save predicted mask."")\n    return parser.parse_args()\n\ndef load(saver, sess, ckpt_path):\n    \'\'\'Load trained weights.\n    \n    Args:\n      saver: TensorFlow saver object.\n      sess: TensorFlow session.\n      ckpt_path: path to checkpoint file with parameters.\n    \'\'\' \n    saver.restore(sess, ckpt_path)\n    print(""Restored model parameters from {}"".format(ckpt_path))\n\ndef main():\n    """"""Create the model and start the evaluation process.""""""\n    args = get_arguments()\n    \n    # Prepare image.\n    img = tf.image.decode_jpeg(tf.read_file(args.img_path), channels=3)\n    # Convert RGB to BGR.\n    img_r, img_g, img_b = tf.split(split_dim=2, num_split=3, value=img)\n    img = tf.cast(tf.concat(2, [img_b, img_g, img_r]), dtype=tf.float32)\n    # Extract mean.\n    img -= IMG_MEAN \n    \n    # Create network.\n    net = DeepLabLFOVModel()\n\n    # Which variables to load.\n    trainable = tf.trainable_variables()\n    \n    # Predictions.\n    pred = net.preds(tf.expand_dims(img, dim=0))\n      \n    # Set up TF session and initialize variables. \n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    init = tf.initialize_all_variables()\n    \n    sess.run(init)\n    \n    # Load weights.\n    saver = tf.train.Saver(var_list=trainable)\n    load(saver, sess, args.model_weights)\n    \n    # Perform inference.\n    preds = sess.run([pred])\n    \n    msk = decode_labels(np.array(preds)[0, 0, :, :, 0])\n    im = Image.fromarray(msk)\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n    im.save(args.save_dir + \'mask.png\')\n    \n    print(\'The output file has been saved to {}\'.format(args.save_dir + \'mask.png\'))\n\n    \nif __name__ == \'__main__\':\n    main()\n'"
train.py,10,"b'""""""Training script for the DeepLab-LargeFOV network on the PASCAL VOC dataset\n   for semantic image segmentation.\n\nThis script trains the model using augmented PASCAL VOC dataset,\nwhich contains approximately 10000 images for training and 1500 images for validation.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nfrom datetime import datetime\nimport os\nimport sys\nimport time\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom deeplab_lfov import DeepLabLFOVModel, ImageReader, decode_labels\n\nBATCH_SIZE = 16\nDATA_DIRECTORY = \'/home/VOCdevkit\'\nDATA_LIST_PATH = \'./dataset/train.txt\'\nINPUT_SIZE = \'321,321\'\nLEARNING_RATE = 1e-4\nMEAN_IMG = tf.Variable(np.array((104.00698793,116.66876762,122.67891434)), trainable=False, dtype=tf.float32)\nNUM_STEPS = 20000\nRANDOM_SCALE = True\nRESTORE_FROM = \'./deeplab_lfov.ckpt\'\nSAVE_DIR = \'./images/\'\nSAVE_NUM_IMAGES = 2\nSAVE_PRED_EVERY = 500\nSNAPSHOT_DIR = \'./snapshots/\'\nWEIGHTS_PATH   = None\n\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n    \n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLabLFOV Network"")\n    parser.add_argument(""--batch_size"", type=int, default=BATCH_SIZE,\n                        help=""Number of images sent to the network in one step."")\n    parser.add_argument(""--data_dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the PASCAL VOC dataset."")\n    parser.add_argument(""--data_list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the dataset."")\n    parser.add_argument(""--input_size"", type=str, default=INPUT_SIZE,\n                        help=""Comma-separated string with height and width of images."")\n    parser.add_argument(""--learning_rate"", type=float, default=LEARNING_RATE,\n                        help=""Learning rate for training."")\n    parser.add_argument(""--num_steps"", type=int, default=NUM_STEPS,\n                        help=""Number of training steps."")\n    parser.add_argument(""--restore_from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--save_dir"", type=str, default=SAVE_DIR,\n                        help=""Where to save figures with predictions."")\n    parser.add_argument(""--save_num_images"", type=int, default=SAVE_NUM_IMAGES,\n                        help=""How many images to save."")\n    parser.add_argument(""--save_pred_every"", type=int, default=SAVE_PRED_EVERY,\n                        help=""Save figure with predictions and ground truth every often."")\n    parser.add_argument(""--snapshot_dir"", type=str, default=SNAPSHOT_DIR,\n                        help=""Where to save snapshots of the model."")\n    parser.add_argument(""--weights_path"", type=str, default=WEIGHTS_PATH,\n                        help=""Path to the file with caffemodel weights. ""\n                             ""If not set, all the variables are initialised randomly."")\n    return parser.parse_args()\n\ndef save(saver, sess, logdir, step):\n    model_name = \'model.ckpt\'\n    checkpoint_path = os.path.join(logdir, model_name)\n\n    if not os.path.exists(logdir):\n        os.makedirs(logdir)\n\n    saver.save(sess, checkpoint_path, global_step=step)\n    print(\'The checkpoint has been created.\')\n    \ndef load(loader, sess, ckpt_path):\n    \'\'\'Load trained weights.\n    \n    Args:\n      loader: TensorFlow saver object.\n      sess: TensorFlow session.\n      ckpt_path: path to checkpoint file with parameters.\n    \'\'\' \n    loader.restore(sess, ckpt_path)\n    print(""Restored model parameters from {}"".format(ckpt_path))\n\ndef main():\n    """"""Create the model and start the training.""""""\n    args = get_arguments()\n    \n    h, w = map(int, args.input_size.split(\',\'))\n    input_size = (h, w)\n    \n    # Create queue coordinator.\n    coord = tf.train.Coordinator()\n    \n    # Load reader.\n    with tf.name_scope(""create_inputs""):\n        reader = ImageReader(\n            args.data_dir,\n            args.data_list,\n            input_size,\n            RANDOM_SCALE,\n            coord)\n        image_batch, label_batch = reader.dequeue(args.batch_size)\n    \n    # Create network.\n    net = DeepLabLFOVModel(args.weights_path)\n\n    # Define the loss and optimisation parameters.\n    loss = net.loss(image_batch, label_batch)\n    optimiser = tf.train.AdamOptimizer(learning_rate=args.learning_rate)\n    trainable = tf.trainable_variables()\n    optim = optimiser.minimize(loss, var_list=trainable)\n    \n    pred = net.preds(image_batch)\n    \n    # Set up tf session and initialize variables. \n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    init = tf.initialize_all_variables()\n    \n    sess.run(init)\n    \n    # Saver for storing checkpoints of the model.\n    saver = tf.train.Saver(var_list=trainable, max_to_keep=40)\n    if args.restore_from is not None:\n        load(saver, sess, args.restore_from)\n    \n    # Start queue threads.\n    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n    \n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n   \n    # Iterate over training steps.\n    for step in range(args.num_steps):\n        start_time = time.time()\n        \n        if step % args.save_pred_every == 0:\n            loss_value, images, labels, preds, _ = sess.run([loss, image_batch, label_batch, pred, optim])\n            fig, axes = plt.subplots(args.save_num_images, 3, figsize = (16, 12))\n            for i in xrange(args.save_num_images):\n                axes.flat[i * 3].set_title(\'data\')\n                axes.flat[i * 3].imshow((images[i] + IMG_MEAN)[:, :, ::-1].astype(np.uint8))\n\n                axes.flat[i * 3 + 1].set_title(\'mask\')\n                axes.flat[i * 3 + 1].imshow(decode_labels(labels[i, :, :, 0]))\n\n                axes.flat[i * 3 + 2].set_title(\'pred\')\n                axes.flat[i * 3 + 2].imshow(decode_labels(preds[i, :, :, 0]))\n            plt.savefig(args.save_dir + str(start_time) + "".png"")\n            plt.close(fig)\n            save(saver, sess, args.snapshot_dir, step)\n        else:\n            loss_value, _ = sess.run([loss, optim])\n        duration = time.time() - start_time\n        print(\'step {:d} \\t loss = {:.3f}, ({:.3f} sec/step)\'.format(step, loss_value, duration))\n    coord.request_stop()\n    coord.join(threads)\n    \nif __name__ == \'__main__\':\n    main()\n'"
deeplab_lfov/__init__.py,0,b'from .model import DeepLabLFOVModel\nfrom .image_reader import ImageReader\nfrom .utils import decode_labels'
deeplab_lfov/image_reader.py,19,"b'import os\n\nimport numpy as np\nimport tensorflow as tf\n\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\ndef read_labeled_image_list(data_dir, data_list):\n    """"""Reads txt file containing paths to images and ground truth masks.\n    \n    Args:\n      data_dir: path to the directory with images and masks.\n      data_list: path to the file with lines of the form \'/path/to/image /path/to/mask\'.\n       \n    Returns:\n      Two lists with all file names for images and masks, respectively.\n    """"""\n    f = open(data_list, \'r\')\n    images = []\n    masks = []\n    for line in f:\n        image, mask = line.strip(""\\n"").split(\' \')\n        images.append(data_dir + image)\n        masks.append(data_dir + mask)\n    return images, masks\n\ndef read_images_from_disk(input_queue, input_size, random_scale): \n    """"""Read one image and its corresponding mask with optional pre-processing.\n    \n    Args:\n      input_queue: tf queue with paths to the image and its mask.\n      input_size: a tuple with (height, width) values.\n                  If not given, return images of original size.\n      random_scale: whether to randomly scale the images prior\n                    to random crop.\n      \n    Returns:\n      Two tensors: the decoded image and its mask.\n    """"""\n    img_contents = tf.read_file(input_queue[0])\n    label_contents = tf.read_file(input_queue[1])\n    \n    img = tf.image.decode_jpeg(img_contents, channels=3)\n    label = tf.image.decode_png(label_contents, channels=1)\n    if input_size is not None:\n        h, w = input_size\n        if random_scale:\n            scale = tf.random_uniform([1], minval=0.75, maxval=1.25, dtype=tf.float32, seed=None)\n            h_new = tf.to_int32(tf.mul(tf.to_float(tf.shape(img)[0]), scale))\n            w_new = tf.to_int32(tf.mul(tf.to_float(tf.shape(img)[1]), scale))\n            new_shape = tf.squeeze(tf.pack([h_new, w_new]), squeeze_dims=[1])\n            img = tf.image.resize_images(img, new_shape)\n            label = tf.image.resize_nearest_neighbor(tf.expand_dims(label, 0), new_shape)\n            label = tf.squeeze(label, squeeze_dims=[0]) # resize_image_with_crop_or_pad accepts 3D-tensor.\n        img = tf.image.resize_image_with_crop_or_pad(img, h, w)\n        label = tf.image.resize_image_with_crop_or_pad(label, h, w)\n    # RGB -> BGR.\n    img_r, img_g, img_b = tf.split(split_dim=2, num_split=3, value=img)\n    img = tf.cast(tf.concat(2, [img_b, img_g, img_r]), dtype=tf.float32)\n    # Extract mean.\n    img -= IMG_MEAN \n    return img, label\n\nclass ImageReader(object):\n    \'\'\'Generic ImageReader which reads images and corresponding segmentation\n       masks from the disk, and enqueues them into a TensorFlow queue.\n    \'\'\'\n\n    def __init__(self, data_dir, data_list, input_size, random_scale, coord):\n        \'\'\'Initialise an ImageReader.\n        \n        Args:\n          data_dir: path to the directory with images and masks.\n          data_list: path to the file with lines of the form \'/path/to/image /path/to/mask\'.\n          input_size: a tuple with (height, width) values, to which all the images will be resized.\n          random_scale: whether to randomly scale the images prior to random crop.\n          coord: TensorFlow queue coordinator.\n        \'\'\'\n        self.data_dir = data_dir\n        self.data_list = data_list\n        self.input_size = input_size\n        self.coord = coord\n        \n        self.image_list, self.label_list = read_labeled_image_list(self.data_dir, self.data_list)\n        self.images = tf.convert_to_tensor(self.image_list, dtype=tf.string)\n        self.labels = tf.convert_to_tensor(self.label_list, dtype=tf.string)\n        self.queue = tf.train.slice_input_producer([self.images, self.labels],\n                                                   shuffle=input_size is not None) # Not shuffling if it is val.\n        self.image, self.label = read_images_from_disk(self.queue, self.input_size, random_scale) \n\n    def dequeue(self, num_elements):\n        \'\'\'Pack images and labels into a batch.\n        \n        Args:\n          num_elements: the batch size.\n          \n        Returns:\n          Two tensors of size (batch_size, h, w, {3,1}) for images and masks.\'\'\'\n        image_batch, label_batch = tf.train.batch([self.image, self.label],\n                                                  num_elements)\n        return image_batch, label_batch\n'"
deeplab_lfov/model.py,30,"b'import tensorflow as tf\nfrom six.moves import cPickle\n\n# Loading net skeleton with parameters name and shapes.\nwith open(""./util/net_skeleton.ckpt"", ""rb"") as f:\n    net_skeleton = cPickle.load(f)\n\n# The DeepLab-LargeFOV model can be represented as follows:\n## input -> [conv-relu](dilation=1, channels=64) x 2 -> [max_pool](stride=2)\n##       -> [conv-relu](dilation=1, channels=128) x 2 -> [max_pool](stride=2)\n##       -> [conv-relu](dilation=1, channels=256) x 3 -> [max_pool](stride=2)\n##       -> [conv-relu](dilation=1, channels=512) x 3 -> [max_pool](stride=1)\n##       -> [conv-relu](dilation=2, channels=512) x 3 -> [max_pool](stride=1) -> [avg_pool](stride=1)\n##       -> [conv-relu](dilation=12, channels=1024) -> [dropout]\n##       -> [conv-relu](dilation=1, channels=1024) -> [dropout]\n##       -> [conv-relu](dilation=1, channels=21) -> [pixel-wise softmax loss].\nnum_layers    = [2, 2, 3, 3, 3, 1, 1, 1]\ndilations     = [[1, 1],\n                 [1, 1],\n                 [1, 1, 1],\n                 [1, 1, 1],\n                 [2, 2, 2],\n                 [12], \n                 [1], \n                 [1]]\nn_classes = 21\n# All convolutional and pooling operations are applied using kernels of size 3x3; \n# padding is added so that the output of the same size as the input.\nks = 3\n\ndef create_variable(name, shape):\n    """"""Create a convolution filter variable of the given name and shape,\n       and initialise it using Xavier initialisation \n       (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf).\n    """"""\n    initialiser = tf.contrib.layers.xavier_initializer_conv2d(dtype=tf.float32)\n    variable = tf.Variable(initialiser(shape=shape), name=name)\n    return variable\n\ndef create_bias_variable(name, shape):\n    """"""Create a bias variable of the given name and shape,\n       and initialise it to zero.\n    """"""\n    initialiser = tf.constant_initializer(value=0.0, dtype=tf.float32)\n    variable = tf.Variable(initialiser(shape=shape), name=name)\n    return variable\n\nclass DeepLabLFOVModel(object):\n    """"""DeepLab-LargeFOV model with atrous convolution and bilinear upsampling.\n    \n    This class implements a multi-layer convolutional neural network for semantic image segmentation task.\n    This is the same as the model described in this paper: https://arxiv.org/abs/1412.7062 - please look\n    there for details.\n    """"""\n    \n    def __init__(self, weights_path=None):\n        """"""Create the model.\n        \n        Args:\n          weights_path: the path to the cpkt file with dictionary of weights from .caffemodel.\n        """"""\n        self.variables = self._create_variables(weights_path)\n        \n    def _create_variables(self, weights_path):\n        """"""Create all variables used by the network.\n        This allows to share them between multiple calls \n        to the loss function.\n        \n        Args:\n          weights_path: the path to the ckpt file with dictionary of weights from .caffemodel. \n                        If none, initialise all variables randomly.\n        \n        Returns:\n          A dictionary with all variables.\n        """"""\n        var = list()\n        index = 0\n        \n        if weights_path is not None:\n            with open(weights_path, ""rb"") as f:\n                weights = cPickle.load(f) # Load pre-trained weights.\n                for name, shape in net_skeleton:\n                    var.append(tf.Variable(weights[name],\n                                           name=name))\n                del weights\n        else:\n            # Initialise all weights randomly with the Xavier scheme,\n            # and \n            # all biases to 0\'s.\n            for name, shape in net_skeleton:\n                if ""/w"" in name: # Weight filter.\n                    w = create_variable(name, list(shape))\n                    var.append(w)\n                else:\n                    b = create_bias_variable(name, list(shape))\n                    var.append(b)\n        return var\n    \n    \n    def _create_network(self, input_batch, keep_prob):\n        """"""Construct DeepLab-LargeFOV network.\n        \n        Args:\n          input_batch: batch of pre-processed images.\n          keep_prob: probability of keeping neurons intact.\n          \n        Returns:\n          A downsampled segmentation mask. \n        """"""\n        current = input_batch\n        \n        v_idx = 0 # Index variable.\n        \n        # Last block is the classification layer.\n        for b_idx in xrange(len(dilations) - 1):\n            for l_idx, dilation in enumerate(dilations[b_idx]):\n                w = self.variables[v_idx * 2]\n                b = self.variables[v_idx * 2 + 1]\n                if dilation == 1:\n                    conv = tf.nn.conv2d(current, w, strides=[1, 1, 1, 1], padding=\'SAME\')\n                else:\n                    conv = tf.nn.atrous_conv2d(current, w, dilation, padding=\'SAME\')\n                current = tf.nn.relu(tf.nn.bias_add(conv, b))\n                v_idx += 1\n            # Optional pooling and dropout after each block.\n            if b_idx < 3:\n                current = tf.nn.max_pool(current, \n                                         ksize=[1, ks, ks, 1],\n                                         strides=[1, 2, 2, 1],\n                                         padding=\'SAME\')\n            elif b_idx == 3:\n                current = tf.nn.max_pool(current, \n                             ksize=[1, ks, ks, 1],\n                             strides=[1, 1, 1, 1],\n                             padding=\'SAME\')\n            elif b_idx == 4:\n                current = tf.nn.max_pool(current, \n                                         ksize=[1, ks, ks, 1],\n                                         strides=[1, 1, 1, 1],\n                                         padding=\'SAME\')\n                current = tf.nn.avg_pool(current, \n                                         ksize=[1, ks, ks, 1],\n                                         strides=[1, 1, 1, 1],\n                                         padding=\'SAME\')\n            elif b_idx <= 6:\n                current = tf.nn.dropout(current, keep_prob=keep_prob)\n        \n        # Classification layer; no ReLU.\n        w = self.variables[v_idx * 2]\n        b = self.variables[v_idx * 2 + 1]\n        conv = tf.nn.conv2d(current, w, strides=[1, 1, 1, 1], padding=\'SAME\')\n        current = tf.nn.bias_add(conv, b)\n\n        return current\n    \n    def prepare_label(self, input_batch, new_size):\n        """"""Resize masks and perform one-hot encoding.\n\n        Args:\n          input_batch: input tensor of shape [batch_size H W 1].\n          new_size: a tensor with new height and width.\n\n        Returns:\n          Outputs a tensor of shape [batch_size h w 21]\n          with last dimension comprised of 0\'s and 1\'s only.\n        """"""\n        with tf.name_scope(\'label_encode\'):\n            input_batch = tf.image.resize_nearest_neighbor(input_batch, new_size) # As labels are integer numbers, need to use NN interp.\n            input_batch = tf.squeeze(input_batch, squeeze_dims=[3]) # Reducing the channel dimension.\n            input_batch = tf.one_hot(input_batch, depth=21)\n        return input_batch\n      \n    def preds(self, input_batch):\n        """"""Create the network and run inference on the input batch.\n        \n        Args:\n          input_batch: batch of pre-processed images.\n          \n        Returns:\n          Argmax over the predictions of the network of the same shape as the input.\n        """"""\n        raw_output = self._create_network(tf.cast(input_batch, tf.float32), keep_prob=tf.constant(1.0))\n        raw_output = tf.image.resize_bilinear(raw_output, tf.shape(input_batch)[1:3,])\n        raw_output = tf.argmax(raw_output, dimension=3)\n        raw_output = tf.expand_dims(raw_output, dim=3) # Create 4D-tensor.\n        return tf.cast(raw_output, tf.uint8)\n        \n    \n    def loss(self, img_batch, label_batch):\n        """"""Create the network, run inference on the input batch and compute loss.\n        \n        Args:\n          input_batch: batch of pre-processed images.\n          \n        Returns:\n          Pixel-wise softmax loss.\n        """"""\n        raw_output = self._create_network(tf.cast(img_batch, tf.float32), keep_prob=tf.constant(0.5))\n        prediction = tf.reshape(raw_output, [-1, n_classes])\n        \n        # Need to resize labels and convert using one-hot encoding.\n        label_batch = self.prepare_label(label_batch, tf.pack(raw_output.get_shape()[1:3]))\n        gt = tf.reshape(label_batch, [-1, n_classes])\n        \n        # Pixel-wise softmax loss.\n        loss = tf.nn.softmax_cross_entropy_with_logits(prediction, gt)\n        reduced_loss = tf.reduce_mean(loss)\n        \n        return reduced_loss\n'"
deeplab_lfov/utils.py,0,"b'from PIL import Image\nimport numpy as np\n\n# Colour map.\nlabel_colours = [(0,0,0)\n                # 0=background\n                ,(128,0,0),(0,128,0),(128,128,0),(0,0,128),(128,0,128)\n                # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n                ,(0,128,128),(128,128,128),(64,0,0),(192,0,0),(64,128,0)\n                # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n                ,(192,128,0),(64,0,128),(192,0,128),(64,128,128),(192,128,128)\n                # 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person\n                ,(0,64,0),(128,64,0),(0,192,0),(128,192,0),(0,64,128)]\n                # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n    \ndef decode_labels(mask):\n    """"""Decode batch of segmentation masks.\n    \n    Args:\n      label_batch: result of inference after taking argmax.\n    \n    Returns:\n      An batch of RGB images of the same size\n    """"""\n    img = Image.new(\'RGB\', (len(mask[0]), len(mask)))\n    pixels = img.load()\n    for j_, j in enumerate(mask):\n        for k_, k in enumerate(j):\n            if k < 21:\n                pixels[k_,j_] = label_colours[k]\n    return np.array(img)\n'"
util/extract_params.py,0,"b'""""""Extract parameters of the DeepLab-LargeFOV model\n   from the provided .caffemodel file.\n   \nThis scripts extracts and saves the network skeleton \nwith names and shape of the parameters, \nas well as all the corresponding weights.\n\nTo run the script, PyCaffe should be installed.\n""""""\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nfrom six.moves import cPickle\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n    \n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""Extract model parameters of DeepLab-LargeFOV from the provided .caffemodel."")\n    parser.add_argument(""caffemodel"", type=str,\n                        help=""Caffemodel from which the parameters will be extracted."")\n    parser.add_argument(""--output_dir"", type=str, default=""./"",\n                        help=""Whether to store the network skeleton and weights."")\n    parser.add_argument(""--pycaffe_path"", type=str, default="""",\n                        help=""Path to PyCaffe (e.g., \'CAFFE_ROOT/python\')."")\n    return parser.parse_args()\n\ndef main():\n    """"""Extract and save network skeleton with the corresponding weights.\n    \n    Raises:\n      ImportError: PyCaffe module is not found.""""""\n    args = get_arguments()\n    sys.path.append(args.pycaffe_path)\n    try:\n        import caffe\n    except ImportError:\n        raise\n    # Load net definition.\n    net = caffe.Net(\'./util/deploy.prototxt\', args.caffemodel, caffe.TEST)\n    \n    # Check the existence of output_dir.\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    # Net skeleton with parameters names and shapes.\n    # In TF, the filter shape is as follows: [ks, ks, input_channels, output_channels],\n    # while in Caffe it looks like this: [output_channels, input_channels, ks, ks].\n    net_skeleton = list() \n    for name, item in net.params.iteritems():\n        net_skeleton.append([name + \'/w\', item[0].data.shape[::-1]]) # See the explanataion on filter formats above.\n        net_skeleton.append([name + \'/b\', item[1].data.shape])\n    \n    with open(os.path.join(args.output_dir, \'net_skeleton.ckpt\'), \'wb\') as f:\n        cPickle.dump(net_skeleton, f, protocol=cPickle.HIGHEST_PROTOCOL)\n    \n    # Net weights. \n    net_weights = dict()\n    for name, item in net.params.iteritems():\n        net_weights[name + \'/w\'] = item[0].data.transpose(2, 3, 1, 0) # See the explanation on filter formats above.\n        net_weights[name + \'/b\'] = item[1].data\n    with open(os.path.join(args.output_dir,\'net_weights.ckpt\'), \'wb\') as f:\n        cPickle.dump(net_weights, f, protocol=cPickle.HIGHEST_PROTOCOL)\n    del net, net_skeleton, net_weights\n\nif __name__ == \'__main__\':\n    main()'"
