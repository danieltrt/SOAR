file_path,api_count,code
Utils.py,0,"b'import os\n\ndef checkPath(path):\n    for dirname, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            print filename\n\ndef get_Files_List(path):\n    files_list=[]\n    for path, subdirs,files in os.walk(path):\n        for filename in files:\n            files_list.append(os.path.join(path, filename))\n    return files_list\n'"
Utils_Image.py,0,"b'\n\nfrom PIL import Image, ImageChops,ImageDraw, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport os\nimport cv2\nfrom sklearn.cluster import KMeans\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport webcolors\n\n\nsize = (640,480)\nimg_save_type=\'PNG\'\n\n### Function to check the existence of an image\n\ndef check_image_with_pil(path):\n    try:\n        Image.open(path)\n    except IOError:\n        return False\n    return True\n\ndef get_Image_List(path, ext):\n    files_list=[]\n    for path, subdirs,files in os.walk(path):\n        for filename in files:\n            if not filename.endswith(ext): continue\n            files_list.append(os.path.join(path, filename))\n    return files_list\n\ndef get_Image_Size(path):\n\n    if check_image_with_pil(path):\n        im = Image.open(path)\n        width, height = im.size\n        return width, height\n    else : print \'ERROR: Image not exists in filesystem:%s\'%path\n\ndef change_extension(file_path, ext_1, ext_2):\n    #Resize Cropping & Padding an image to the 640x480 pixel size\n    if check_image_with_pil(file_path):\n        image = Image.open(file_path)\n    #print\'Starting Path: %s\'% file_path\n    \n    new_path=file_path.replace(ext_1,ext_2)\n        #print\'New Path: %s\'%  new_path\n    image.save(new_path, img_save_type)\n    if check_image_with_pil(new_path):\n        #print \'Rename & Save completed Correct for: %s\'%new_path \n        os.remove(file_path)\n    else : print \'ERROR: Rename & Save for: %s\'%new_path\n\n\n\n### Functions to resize /and save an image\n\ndef resizeImage(file_path): \n    #Resize Cropping & Padding an image to the 640x480 pixel size\n    if file_path is not -1:\n        if check_image_with_pil(file_path):\n            image = Image.open(file_path)\n            image.thumbnail(size, Image.ANTIALIAS)\n            image_size = image.size\n\n            padding_0 = max( (size[0] - image_size[0]) / 2, 0 )\n            padding_1 = max( (size[1] - image_size[1]) / 2, 0 )\n            cv2.namedWindow(\'Original Image\')\n            cv2.namedWindow(\'Resized Image\')\n            cv2.startWindowThread()\n            orig_img = cv2.imread(file_path, 0)\n            cv2.imshow(\'Original Image\',orig_img)\n            cv2.waitKey(2)\n\n            if((padding_0==0) & (padding_1==0)):\n                image.save(file_path, img_save_type)\n            else:\n                thumb = image.crop( (0, 0, size[0], size[1]) )\n                thumb = ImageChops.offset(thumb, int(padding_0), int(padding_1))\n                thumb.save(file_path)\n\n            resized_img = cv2.imread(file_path, 0)\n            cv2.imshow(\'Resized Image\',resized_img)\n    else :\n        cv2.destroyAllWindows()\n        cv2.waitKey(2)\n\n\ndef resize_saveImage(file_path, new_path): \n    #Resize Cropping & Padding an image to the 640x480 pixel size\n    ##The method thumbnail mantain the aspect ratio and resize the image to fit the max size passed\n    ##depending on the orientation of the image.\n    ##Than with Image chops we set the smaller ones in \n    \n    image = Image.open(file_path)\n    image.thumbnail(size, Image.ANTIALIAS)\n    image_size = image.size\n\n    padding_0 = max( (size[0] - image_size[0]) / 2, 0 )\n    padding_1 = max( (size[1] - image_size[1]) / 2, 0 )\n\n    orig_img = cv2.imread(file_path, 0)\n    cv2.imshow(\'Original Image\',orig_img)\n    cv2.waitKey(2)\n\n    if((padding_0==0) & (padding_1==0)):\n        image.save(new_path, img_save_type)\n    else:\n        thumb = image.crop( (0, 0, size[0], size[1]) )\n        thumb = ImageChops.offset(thumb, int(padding_0), int(padding_1))\n        thumb.save(new_path)\n\n    resized_img = cv2.imread(new_path, 0)\n    cv2.imshow(\'Resized Image\',resized_img)\n    cv2.waitKey(2)\n\n    if not check_image_with_pil(new_path):\n        print \'ERROR: Rename & Save for: %s\'%new_path\n    if check_image_with_pil(file_path):\n        os.remove(file_path)\n        #print \'Delected old File: %s\'%file_path\n\n    return padding\n\n\n### Function to get the image padd\n\ndef getpadd_Image(size_img_0, size_img_1, max_size_0, max_size_1): \n    #Get Padd of the image\n    orig_ratio=float(size_img_0/size_img_1)\n    new_ratio=-1\n    max_ratio=float(max(float(size_img_0/max_size_0),float(size_img_1/max_size_1)))\n    new_img_0=int(size_img_0/max_ratio)\n    new_img_1=int(size_img_1/max_ratio)\n    new_ratio=int(new_img_0/new_img_1)\n    if new_ratio is not int(max_ratio):\n        print ""Ratio Error""\n    padding[0] = max( (max_size_0 - new_img_0) / 2, 0 )\n    padding[1] = max( (max_size_1 - new_img_1) / 2, 0 )\n\n    return padding\n\n### Functions to manage the point resizing\n\ndef transform_point(size_img_0, size_img_1, max_size_0, max_size_1, point, xory):\n    orig_ratio=float(size_img_0)/float(size_img_1)\n    new_ratio=-1\n    max_ratio=float(max(float(size_img_0/max_size_0),float(size_img_1/max_size_1),1))\n    # print \'Size W Img: %d\'% size_img_0\n    # print \'Size H Img: %d\'% size_img_1\n    # print \'Size MW Img: %d\'% max_size_0\n    # print \'Size MH Img: %d\'% max_size_1\n    # print \'Starting Point Img: %d\'% point\n    # print \'Max Ratio New Img: %d\'%max_ratio\n    if(max_ratio==1):\n        if xory:\n            #print ""x point""\n           padding = max( (max_size_0 - size_img_0) / 2, 0 )\n        else:\n            #print ""y point""    \n            padding = max( (max_size_1 - size_img_1) / 2, 0 )\n        point = point + padding\n    else:   \n        new_img_0=int(size_img_0/max_ratio)\n        new_img_1=int(size_img_1/max_ratio)\n        new_ratio=int(new_img_0/new_img_1)\n        old_ratio=int(size_img_0/size_img_1)\n        if new_ratio is not old_ratio:\n            print ""Ratio Error %d : %d""%(new_ratio,old_ratio)\n            if xory:\n               # print ""x point""\n               padding = max( (max_size_0 - new_img_0) / 2, 0 )\n            else:\n               # print ""y point""\n                padding = max( (max_size_1 - new_img_1) / 2, 0 )\n            point = int(point/max_ratio) + padding\n    # print \'Padding Point Img: %d\'%padding \n    # print \'Ending Point Img: %d\'%point\n    return point\n\n# def get_orig_point(size_img_0, size_img_1, max_size_0, max_size_1, point, xory):\n#     orig_ratio=float(size_img_0/size_img_1)\n#     new_ratio=-1\n#     max_ratio=float(max(float(size_img_0/max_size_0),float(size_img_1/max_size_1),1))\n#     # print \'Size W Img: %d\'% size_img_0\n#     # print \'Size H Img: %d\'% size_img_1\n#     # print \'Size MW Img: %d\'% max_size_0\n#     # print \'Size MH Img: %d\'% max_size_1\n#     # print \'Starting Point Img: %d\'% point\n#     # print \'Max Ratio New Img: %d\'%max_ratio\n#     if(max_ratio==1):\n#         if xory:\n#             # print ""x point""\n#             padding = max( (max_size_0 - size_img_0) / 2, 0 )\n#         else: \n#             # print ""y point""\n#             padding = max( (max_size_1 - size_img_1) / 2, 0 )\n#         point = point - padding\n#     else:   \n#         new_img_0=int(size_img_0/max_ratio)\n#         new_img_1=int(size_img_1/max_ratio)\n#         new_ratio=int(new_img_0/new_img_1)\n#         old_ratio=int(size_img_0/size_img_1)\n#         if new_ratio is not old_ratio:\n#             print ""Ratio Error %d : %d""%(new_ratio,old_ratio)\n#         if xory:\n#             # print ""x point""\n#             padding = max( (max_size_0 - new_img_0) / 2, 0 )\n#         else:\n#             # print ""y point""\n#             padding = max( (max_size_1 - new_img_1) / 2, 0 )\n#         point = int(point*max_ratio)- padding\n#     # print \'Padding Point Img: %d\'%padding \n#     # print \'Ending Point Img: %d\'%point\n#     return point\n\ndef get_orig_point(size_0_orig, size_1_orig, size_0_trasf, size_1_trasf, point, xory):\n    orig_ratio=float(size_0_orig)/float(size_1_orig)\n    print size_0_orig\n    print size_1_orig\n    new_ratio=-1\n    max_ratio=float(max(float(size_0_orig/size_0_trasf),float(size_1_orig/size_1_trasf),1.0))\n    # print \'Size W Img: %d\'% size_img_0\n    # print \'Size H Img: %d\'% size_img_1\n    # print \'Size MW Img: %d\'% max_size_0\n    # print \'Size MH Img: %d\'% max_size_1\n    # print \'Starting Point Img: %d\'% point\n    # print \'Max Ratio New Img: %d\'%max_ratio\n    if(max_ratio==1):\n        if xory:\n            # print ""x point""\n            padding = max( (size_0_trasf - size_0_orig) / 2.0, 0.)\n        else: \n            # print ""y point""\n            padding = max( (size_1_trasf - size_1_orig) / 2.0, 0.)\n        point = point - padding\n        if xory:\n            if(point < 0):\n                point = 0\n            if(point > size_0_orig):\n                point= size_0_orig\n        else:\n            if(point < 0):\n                point = 0\n            if(point > size_1_orig):\n                point= size_1_orig\n    else:   \n        new_img_0=float(size_0_orig/max_ratio)\n        print new_img_0\n        new_img_1=float(size_1_orig/max_ratio)\n        print new_img_1\n        new_ratio=float(new_img_0/new_img_1)\n        if new_ratio == orig_ratio:\n            if xory:\n                # print ""x point""\n                padding = max( (size_0_trasf - new_img_0) / 2.0, 0.)\n            else:\n                # print ""y point""\n                padding = max( (size_1_trasf - new_img_1) / 2.0, 0.)\n            point = float(point - padding)*max_ratio\n            if xory:\n                if(point < 0):\n                    point = 0\n                if(point > size_0_orig):\n                    point= size_0_orig\n            else:\n                if(point < 0):\n                    point = 0\n                if(point > size_1_orig):\n                    point= size_1_orig\n        else: print ""Ratio Error Old %.2f : New %.2f""%(new_ratio,orig_ratio)\n    # print \'Padding Point Img: %d\'%padding \n    # print \'Ending Point Img: %d\'%point\n    return point\n\ndef transform_rect(size_img_0, size_img_1, max_size_0, max_size_1, x1point, y1point, x2point, y2point):\n    \n    newx1=transform_point(size_img_0, size_img_1, max_size_0, max_size_1, x1point, True)\n    newy1=transform_point(size_img_0, size_img_1, max_size_0, max_size_1, y1point, False)\n    newx2=transform_point(size_img_0, size_img_1, max_size_0, max_size_1, x2point, True)\n    newy2=transform_point(size_img_0, size_img_1, max_size_0, max_size_1, y2point, False)\n\n    return (newx1,newy1,newx2,newy2)\n\ndef get_orig_rect(size_img_0, size_img_1, max_size_0, max_size_1,x1point, y1point, x2point, y2point):\n    \n    newx1=get_orig_point(size_img_0, size_img_1, max_size_0, max_size_1, x1point, True)\n    newy1=get_orig_point(size_img_0, size_img_1, max_size_0, max_size_1, y1point, False)\n    newx2=get_orig_point(size_img_0, size_img_1, max_size_0, max_size_1, x2point, True)\n    newy2=get_orig_point(size_img_0, size_img_1, max_size_0, max_size_1, y2point, False)\n\n    return (newx1,newy1,newx2,newy2)\n\n### Functions for colors managing\n\ndef centroid_histogram(clt):\n    # grab the number of different clusters and create a histogram\n    # based on the number of pixels assigned to each cluster\n    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n    (hist, _) = np.histogram(clt.labels_, bins = numLabels)\n \n    # normalize the histogram, such that it sums to one\n    hist = hist.astype(""float"")\n    hist /= hist.sum()\n    # return the histogram\n    return hist\n\ndef closest_colour(requested_colour):\n    min_colours = {}\n    for key, name in webcolors.css3_hex_to_names.items():\n        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n        rd = (r_c - requested_colour[0]) ** 2\n        gd = (g_c - requested_colour[1]) ** 2\n        bd = (b_c - requested_colour[2]) ** 2\n        min_colours[(rd + gd + bd)] = name\n    return min_colours[min(min_colours.keys())]\n\ndef get_colour_name(requested_colour):\n\n    closest_name = closest_colour(requested_colour)\n\n    return closest_name\n\ndef get_dominant_color(file_path):\n\n    image = cv2.imread(file_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = image.reshape((image.shape[0] * image.shape[1], 3))\n    clt = KMeans(n_clusters = 4)\n    clt.fit(image)\n    hist = centroid_histogram(clt)\n    return hist, (get_colour_name(clt.cluster_centers_[0]),get_colour_name(clt.cluster_centers_[1]),get_colour_name(clt.cluster_centers_[2]),get_colour_name(clt.cluster_centers_[3]))\n\ndef isnotBlack(file_path):\n    percentages, colors = get_dominant_color(file_path)\n    tot_black=0.0\n    for i in range(0,len(colors)):\n        if colors[i] in [\'black\']:\n            # print tot_black\n            tot_black=tot_black+percentages[i]\n    if(tot_black>=0.9):\n        # print(""Is black"")\n        return False\n    else: \n        # print(""Is not black"")\n        return True\n\n'"
Utils_Imagenet.py,30,"b'# """"""\n# Preparing model:\n#  - Install bazel ( check tensorflow\'s github for more info )\n#     Ubuntu 14.04:\n#         - Requirements:\n#             sudo add-apt-repository ppa:webupd8team/java\n#             sudo apt-get update\n#             sudo apt-get install oracle-java8-installer\n#         - Download bazel, ( https://github.com/bazelbuild/bazel/releases )\n#           tested on: https://github.com/bazelbuild/bazel/releases/download/0.2.0/bazel-0.2.0-jdk7-installer-linux-x86_64.sh\n#         - chmod +x PATH_TO_INSTALL.SH\n#         - ./PATH_TO_INSTALL.SH --user\n#         - Place bazel onto path ( exact path to store shown in the output)\n# - For retraining, prepare folder structure as\n#     - root_folder_name\n#         - class 1\n#             - file1\n#             - file2\n#         - class 2\n#             - file1\n#             - file2\n# - Clone tensorflow\n# - Go to root of tensorflow\n# - bazel build tensorflow/examples/image_retraining:retrain\n# - bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir /path/to/root_folder_name  --output_graph /path/output_graph.pb -- output_labels /path/output_labels.txt -- bottleneck_dir /path/bottleneck\n# ** Training done. **\n# For testing through bazel,\n#     bazel build tensorflow/examples/label_image:label_image && \\\n#     bazel-bin/tensorflow/examples/label_image/label_image \\\n#     --graph=/path/output_graph.pb --labels=/path/output_labels.txt \\\n#     --output_layer=final_result \\\n#     --image=/path/to/test/image\n# For testing through python, change and run this code.\n# """"""\n\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport sys\nimport vid_classes\nimport progressbar\nimport utils_image\nimport multiclass_rectangle\nfrom PIL import Image\n\n\n\nmodelFullPath = \'output_model/retrained_graph.pb\' ##### Put the \ncheckpoint_dir= ""output_model/model.ckpt-250000""\nlabel_file=\'output_model/retrained_labels.txt\'\n\ndef create_graph():\n    """"""Creates a graph from saved GraphDef file and returns a saver.""""""\n    # Creates graph from saved graph_def.pb.\n    with tf.gfile.FastGFile(modelFullPath, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        _ = tf.import_graph_def(graph_def, name=\'\')\n\ndef run_inception_once(picture_path):\n\n    if not tf.gfile.Exists(picture_path):\n        tf.logging.fatal(\'File does not exist %s\', picture_path)\n        sys.exit()\n\n    image_data = tf.gfile.FastGFile(picture_path, \'rb\').read()\n\n    # Loads label file, strips off carriage return\n    label_lines = [line.rstrip() for line in tf.gfile.GFile(label_file)]\n    # Creates graph from saved GraphDef.\n    create_graph()\n    saver = tf.train.Saver()  # defaults to saving all variables - in this case w and b\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        # load_checkpoint(sess, saver)\n\n        softmax_tensor = sess.graph.get_tensor_by_name(\'final_result:0\')\n        predictions = sess.run(softmax_tensor,\n                               {\'DecodeJpeg/contents:0\': image_data})\n        predictions = np.squeeze(predictions)\n\n        top_k = predictions.argsort()[-3:][::-1]  # Getting top 5 predictions\n\n        #CHECK OUTPUT\n        for node_id in top_k:\n            human_string = label_lines[node_id]\n            score = predictions[node_id]\n            print(\'%s (score = %.5f)\' % (human_string, score))\n\n        # CHECK BEST LABEL\n        print ""Best Label: %s with conf: %.5f""%(label_lines[top_k[0]],predictions[top_k[0]])\n\n        return label_lines[top_k[0]],predictions[top_k[0]]\n\n# def run_inception(pictures_path_array):\n\n#     labels=[]\n#     confidences=[]\n#     # Creates graph from saved GraphDef.\n#     # Creates graph from saved GraphDef.\n#     create_graph()\n#     saver = tf.train.Saver()  # defaults to saving all variables - in this case w and b\n    \n#     with tf.Session() as sess:\n#         sess.run(tf.initialize_all_variables())\n#         # load_checkpoint(sess, saver)\n\n#         softmax_tensor = sess.graph.get_tensor_by_name(\'final_result:0\')\n\n#         for picture_path in pictures_path_array:\n\n#             if not tf.gfile.Exists(picture_path):\n#                 tf.logging.fatal(\'File does not exist %s\', picture_path)\n#                 sys.exit()\n\n#             image_data = tf.gfile.FastGFile(picture_path, \'rb\').read()\n#             predictions = sess.run(softmax_tensor,\n#                                    {\'DecodeJpeg/contents:0\': image_data})\n#             predictions = np.squeeze(predictions)\n\n#             top_k = predictions.argsort()[-5:][::-1]  # Getting top 5 predictions\n\n#             #CHECK OUTPUT\n#             # for node_id in top_k:\n#             #     human_string = vid_classes.code_comp_to_class(node_id)\n#             #     score = predictions[node_id]\n#             #     print(\'%s (score = %.5f)\' % (human_string, score))\n\n#             #CHECK BEST LABEL\n#             #print ""Best Label: %s with conf: %.5f""%(vid_classes.code_comp_to_class(top_k[0]),predictions[top_k[0]])\n\n#             labels.append(vid_classes.code_comp_to_class(top_k[0]), len(labels))\n#             confidences.append(predictions[top_k[0]], len(confidences))\n\n#         return labels, confidences\n\n\ndef label_video(video_info, frames):\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    if not os.path.exists(frames[0].split(""/"")[0]+""/cropped_rects/""):\n        os.makedirs(frames[0].split(""/"")[0]+""/cropped_rects/"")\n        print(""Created Folder: %s""%(frames[0].split(""/"")[0]+""/cropped_rects/""))\n    # Loads label file, strips off carriage return\n    label_lines = [line.rstrip() for line in tf.gfile.GFile(label_file)]\n    # Creates graph from saved GraphDef.\n    create_graph()\n    saver = tf.train.Saver()  # defaults to saving all variables - in this case w and b\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        # load_checkpoint(sess, saver)\n        softmax_tensor = sess.graph.get_tensor_by_name(\'final_result:0\')\n        idx=0\n        for frame_info in progress(video_info):\n            print ""Tracking Frame Nr: %d""%frame_info.frame\n            print len(frame_info.rects)\n            rect_id=0\n            frame_info.filename = frames[idx]\n            for rect in frame_info.rects:\n                \n                img= Image.open(frames[idx])\n                width, height= utils_image.get_Image_Size(frames[idx])\n                print rect.x1,rect.y1,rect.x2 ,rect.y2\n                x1,y1,x2,y2=utils_image.get_orig_rect(width, height, 640, 480, rect.x1,rect.y1,rect.x2 ,rect.y2)\n                print x1,y1,x2,y2\n                if(x1==x2):\n                    x2=x2-10\n                if(y1==y2):\n                    y2=y2-10    \n                cor = (min(x1,x2),min(y1,y2),max(x1,x2),max(y1,y2))\n                print cor\n                cropped_img=img.crop(cor)\n                cropped_img_name=frames[0].split(""/"")[0]+""/cropped_rects/cropped_frame_%d_rect_%d.JPEG""%(frame_info.frame, rect_id)\n                cropped_img.save(cropped_img_name)\n                print ""Frame: %d Rect: %d conf: %.2f""%(frame_info.frame, rect_id, rect.true_confidence)\n                if not tf.gfile.Exists(cropped_img_name):\n                    tf.logging.fatal(\'File does not exist %s\', cropped_img_name)\n                    sys.exit()\n                image_data = tf.gfile.FastGFile(cropped_img_name, \'rb\').read()\n\n                predictions = sess.run(softmax_tensor,{\'DecodeJpeg/contents:0\': image_data})\n                predictions = np.squeeze(predictions)\n\n                top_k = predictions.argsort()[-3:][::-1]  # Getting top 5 predictions\n\n                #CHECK OUTPUT\n                for node_id in top_k:\n                    human_string = label_lines[node_id]\n                    score = predictions[node_id]\n                    print(\'%s (score = %.5f)\' % (human_string, score))\n\n                # CHECK BEST LABEL\n                print ""Best Label: %s with conf: %.5f""%(vid_classes.code_to_class_string(label_lines[top_k[0]]),predictions[top_k[0]])\n                rect.set_rect_coordinates(x1,x2,y1,y2)\n                rect.set_label(predictions[top_k[0]], vid_classes.code_to_class_string(label_lines[top_k[0]]), top_k[0], label_lines[top_k[0]])\n                rect_id=rect_id+1\n            idx=idx+1\n    return video_info\n\n\ndef recurrent_label_video(video_info, frames):\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    decomposed_path =frames[0].split(""/"")\n    folder = decomposed_path[len(decomposed_path)-2]\n    if not os.path.exists(folder+""/cropped_rects/""):\n    \tos.makedirs(folder+""/cropped_rects/"")\n        print(""Created Folder: %s""%(folder+""/cropped_rects/""))\n\n    # Loads label file, strips off carriage return\n    label_lines = [line.rstrip() for line in tf.gfile.GFile(label_file)]\n    # Creates graph from saved GraphDef.\n    create_graph()\n    saver = tf.train.Saver()  # defaults to saving all variables - in this case w and b\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        # load_checkpoint(sess, saver)\n        softmax_tensor = sess.graph.get_tensor_by_name(\'final_result:0\')\n        idx=0\n        video_labels=[]\n        for frame_info in progress(video_info):\n            print ""Tracking Frame Nr: %d""%frame_info.frame\n            print len(frame_info.rects)\n            rect_id=0\n            frame_labels=[]\n            frame_info.filename = frames[idx]\n            for rect in frame_info.rects:\n                \n                img= Image.open(frames[idx])\n                width, height= utils_image.get_Image_Size(frames[idx])\n                print rect.x1,rect.y1,rect.x2 ,rect.y2\n                x1,y1,x2,y2=utils_image.get_orig_rect(width, height, 640, 480, rect.x1,rect.y1,rect.x2 ,rect.y2)\n                print x1,y1,x2,y2\n                cor = (min(x1,x2),min(y1,y2),max(x1,x2),max(y1,y2))\n                print cor\n                cropped_img=img.crop(cor)\n                cropped_img_name=folder+""/cropped_rects/cropped_frame_%d_rect_%d.JPEG""%(frame_info.frame, rect_id)\n                cropped_img.save(cropped_img_name)\n                print ""Frame: %d Rect: %d conf: %.2f""%(frame_info.frame, rect_id, rect.true_confidence)\n                if not tf.gfile.Exists(cropped_img_name):\n                    tf.logging.fatal(\'File does not exist %s\', cropped_img_name)\n                    sys.exit()\n                image_data = tf.gfile.FastGFile(cropped_img_name, \'rb\').read()\n\n                predictions = sess.run(softmax_tensor,{\'DecodeJpeg/contents:0\': image_data})\n                predictions = np.squeeze(predictions)\n\n                top_k = predictions.argsort()[-3:][::-1]  # Getting top 5 predictions\n\n                #CHECK OUTPUT\n                for node_id in top_k:\n                    human_string = label_lines[node_id]\n                    score = predictions[node_id]\n                    print(\'%s (score = %.5f)\' % (human_string, score))\n\n                if(len(video_labels)>0):\n                    if(video_labels[idx-1][rect_id][0]==top_k[0]):\n                        # CHECK BEST LABEL\n                        print ""Best Label: %s with conf: %.5f""%(vid_classes.code_to_class_string(label_lines[top_k[0]]),predictions[top_k[0]])\n                        rect.set_rect_coordinates(x1,x2,y1,y2)\n                        rect.set_label(predictions[top_k[0]], vid_classes.code_to_class_string(label_lines[top_k[0]]), top_k[0], label_lines[top_k[0]])\n                        frame_labels.append((top_k[0], predictions[top_k[0]]))\n                    else:\n                        label = video_labels[idx-1][rect_id][0] \n                        print ""Best Label setted recurrently: %s ""%(vid_classes.code_to_class_string(label_lines[label]))\n                        rect.set_rect_coordinates(x1,x2,y1,y2)\n                        rect.set_label(video_labels[idx-1][rect_id][1], vid_classes.code_to_class_string(label_lines[label]), label, label_lines[label])\n                        frame_labels.append((label, video_labels[idx-1][rect_id][1]))\n                else:\n                    # CHECK BEST LABEL\n                    print ""Best Label: %s with conf: %.5f""%(vid_classes.code_to_class_string(label_lines[top_k[0]]),predictions[top_k[0]])\n                    rect.set_rect_coordinates(x1,x2,y1,y2)\n                    rect.set_label(predictions[top_k[0]], vid_classes.code_to_class_string(label_lines[top_k[0]]), top_k[0], label_lines[top_k[0]])\n                    frame_labels.append((top_k[0], predictions[top_k[0]]))\n                    \n                rect_id=rect_id+1\n            video_labels.append(frame_labels)\n            idx=idx+1\n    return video_info\n'"
Utils_Tensorbox.py,20,"b'#### Import from Tensorbox Project\n\nimport tensorflow as tf\nimport json\nimport subprocess\nfrom scipy.misc import imread\nimport numpy as np\nimport sys\n# Import DET Alg package\n\n# import sys\n# sys.path.insert(0, \'TENSORBOX\')\nsys.path.insert(0, \'TENSORBOX\')\n\n# Original\nfrom utils import googlenet_load, train_utils, rect_multiclass\nfrom utils.annolist import AnnotationLib as al\nfrom utils.rect import Rect\n#Modified\n\n#### My import\n\nimport vid_classes\nimport frame\nimport multiclass_rectangle\nimport utils_image\nimport utils_video\nimport progressbar\nimport os\nimport cv2\n\n###Best higher_dyn -0.1 | NMS overlap 0.9\n\n\n# def test(image_path): shit\n#     im = cv2.imread(image_path,0)\n#     img_filt = cv2.medianBlur(im, 5)\n#     img_th = cv2.adaptiveThreshold(img_filt,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n#     contours, hierarchy = cv2.findContours(img_th, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n#     idx =0 \n#     for cnt in contours:\n#         idx += 1\n#         x,y,w,h = cv2.boundingRect(cnt)\n#         roi=im[y:y+h,x:x+w]\n#         cv2.imwrite(str(idx) + \'.jpg\', roi)\n#         cv2.rectangle(im,(x,y),(x+w,y+h),(200,0,0),2)\n#     cv2.imshow(\'img\',im)\n\n####### FUNCTIONS DEFINITIONS\n\ndef NMS(rects,overlapThresh=0.3):\n    # if there are no boxes, return an empty list\n    if len(rects) == 0:\n        print ""WARNING: Passed Empty Boxes Array""\n        return []\n \n    # initialize the list of picked indexes\n    pick = []\n    x1, x2, y1, y2, conf=[],[],[],[], []\n    for rect in rects:\n        x1.append(rect.x1)\n        x2.append(rect.x2)\n        y1.append(rect.y1)\n        y2.append(rect.y2)\n        conf.append(rect.true_confidence)\n    # grab the coordinates of the bounding boxes\n    x1 = np.array(x1)\n    y1 = np.array(y1)\n    x2 = np.array(x2)\n    y2 = np.array(y2)\n    conf = np.array(conf)\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by the bottom-right y-coordinate of the bounding box\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    idxs = np.argsort(conf)\n    # keep looping while some indexes still remain in the indexes\n    # list\n    while len(idxs) > 0:\n        # grab the last index in the indexes list, add the index\n        # value to the list of picked indexes, then initialize\n        # the suppression list (i.e. indexes that will be deleted)\n        # using the last index\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n        suppress = [last]\n        # loop over all indexes in the indexes list\n        for pos in xrange(0, last):\n            # grab the current index\n            j = idxs[pos]\n\n            # find the largest (x, y) coordinates for the start of\n            # the bounding box and the smallest (x, y) coordinates\n            # for the end of the bounding box\n            xx1 = max(x1[i], x1[j])\n            yy1 = max(y1[i], y1[j])\n            xx2 = min(x2[i], x2[j])\n            yy2 = min(y2[i], y2[j])\n\n            # compute the width and height of the bounding box\n            w = max(0, xx2 - xx1 + 1)\n            h = max(0, yy2 - yy1 + 1)\n \n            # compute the ratio of overlap between the computed\n            # bounding box and the bounding box in the area list\n            overlap = float(w * h) / area[j]\n            # union = area[j] + float(w * h) - overlap\n\n            # iou = overlap/union\n\n            # if there is sufficient overlap, suppress the\n            # current bounding box\n            if (overlap > overlapThresh):\n                suppress.append(pos)\n \n        # delete all indexes from the index list that are in the\n        # suppression list\n        idxs = np.delete(idxs, suppress)\n \n    # return only the bounding boxes that were picked\n    picked =[]\n    for i in pick: picked.append(rects[i])\n    return picked\n\ndef getTextIDL(annotations):\n\n\tframe = -1\n\tconf=0\n\tsilhouette=-1\n\txmin,ymin,xmax,ymax=0,0,0,0\n\n\tdetections_array=[]\n\n\tif annotations.frameNr is not -1:\n\t\tframe=annotations.frameNr\n\tfor rect in annotations.rects:\n\t\tif rect.silhouetteID is not -1:\n\t\t\tsilhouette=rect.silhouetteID\n\t\tconf = rect.score\n\t\txmin,ymin,xmax,ymax = rect.x1,rect.y1,rect.x2 ,rect.y2\n\t\tdetections_array.append(str(frame)+\' \'+str(silhouette)+\' \'+str(conf)+\' \'+str(xmin)+\' \'+str(ymin)+\' \'+str(xmax)+\' \'+str(ymax))\n\treturn detections_array\n\ndef writeText(annotations, file):\n\tdetections= getTextIDL(annotations)\n\tfor detection in detections:\n\t\tfile.write(detection + os.linesep)\n\ndef saveTextResults(filename, annotations):\n    if not os.path.exists(filename):\n        print ""Created File: ""+ filename\n    file = open(filename, \'w\')\n    for annotation in annotations:\n        writeText(annotation,file)\n    file.close()\n\ndef get_silhouette_confidence(silhouettes_confidence):\n    higher=0.0\n    index=0\n    # print ""conf_sil : "" + str(silhouettes_confidence)\n    # print ""conf_sil LEN : "" + str(len(silhouettes_confidence))\n\n    for i in range(0,len(silhouettes_confidence)):\n        # print ""conf_sil I : "" + str(silhouettes_confidence[i])\n        if silhouettes_confidence[i]>higher:\n            higher = silhouettes_confidence[i]\n            index = i\n    # print str(index+1),str(higher)\n    return index+1 , higher\n\ndef get_higher_confidence(rectangles):\n    higher=0.0\n    index=0\n    # print ""conf_sil : "" + str(silhouettes_confidence)\n    # print ""conf_sil LEN : "" + str(len(silhouettes_confidence))\n\n    for rect in rectangles:\n        # print ""conf_sil I : "" + str(silhouettes_confidence[i])\n        if rect.true_confidence>higher:\n            higher = rect.true_confidence\n    # print str(index+1),str(higher)\n    # print ""higher: %.2f""%higher\n    higher=higher*10\n    # print ""higher: %.1f""%higher\n    higher=int(higher)\n    # print ""higher: %.d""%higher\n    higher=float(higher)/10.0\n    # print ""rounded max: %.1f""%(higher)\n    if(higher>0.5):\n        return  higher-0.3\n    if(higher<0.3):\n        return  higher-0.1\n    else: return  higher-0.2\n\n\ndef print_logits(logits):\n    higher=0.0\n    index=0\n    # print ""logits_sil shape : "" + str(logits.shape)\n\n    for i in range(0,len(logits)):\n        # print ""conf_sil I : "" + str(logits[i])\n        for j in range(0,len(logits[i])):\n            if logits[i][0][j]>higher:\n                higher = logits[i][0][j]\n                index = j\n    print str(index+1),str(higher)\n    return index+1 , higher\n\ndef get_multiclass_rectangles(H, confidences, boxes, rnn_len):\n    boxes_r = np.reshape(boxes, (-1,\n                                 H[""grid_height""],\n                                 H[""grid_width""],\n                                 rnn_len,\n                                 4))\n    confidences_r = np.reshape(confidences, (-1,\n                                             H[""grid_height""],\n                                             H[""grid_width""],\n                                             rnn_len,\n                                             H[\'num_classes\']))\n    # print ""boxes_r shape"" + str(boxes_r.shape)\n    # print ""confidences"" + str(confidences.shape)\n    cell_pix_size = H[\'region_size\']\n    all_rects = [[[] for _ in range(H[""grid_width""])] for _ in range(H[""grid_height""])]\n    for n in range(rnn_len):\n        for y in range(H[""grid_height""]):\n            for x in range(H[""grid_width""]):\n                bbox = boxes_r[0, y, x, n, :]\n                abs_cx = int(bbox[0]) + cell_pix_size/2 + cell_pix_size * x\n                abs_cy = int(bbox[1]) + cell_pix_size/2 + cell_pix_size * y\n                w = bbox[2]\n                h = bbox[3]\n                # conf = np.max(confidences_r[0, y, x, n, 1:])\n                index, conf = get_silhouette_confidence(confidences_r[0, y, x, n, 1:])\n                # print index, conf\n                # print np.max(confidences_r[0, y, x, n, 1:])\n                # print ""conf"" + str(conf)\n                # print ""conf"" + str(confidences_r[0, y, x, n, 1:])\n                new_rect=multiclass_rectangle.Rectangle_Multiclass()\n                new_rect.set_unlabeled_rect(abs_cx,abs_cy,w,h,conf)\n                all_rects[y][x].append(new_rect)\n    # print ""confidences_r"" + str(confidences_r.shape)\n\n    all_rects_r = [r for row in all_rects for cell in row for r in cell]\n    min_conf = get_higher_confidence(all_rects_r)\n    acc_rects=[rect for rect in all_rects_r if rect.true_confidence>min_conf]\n    rects = []\n    for rect in all_rects_r:\n    \tif rect.true_confidence>min_conf:\n\t        r = al.AnnoRect()\n\t        r.x1 = rect.cx - rect.width/2.\n\t        r.x2 = rect.cx + rect.width/2.\n\t        r.y1 = rect.cy - rect.height/2.\n\t        r.y2 = rect.cy + rect.height/2.\n\t        r.score = rect.true_confidence\n\t        r.silhouetteID=rect.label\n\t        rects.append(r)\n    print len(rects),len(acc_rects)\n    return rects, acc_rects\n\n# def still_image_TENSORBOX_multiclass(frames_list,path_video_folder,hypes_file,weights_file,pred_idl):\n    \n#     from train import build_forward\n\n#     print(""Starting DET Phase"")\n    \n#     det_frames_list=[]\n\n#     #### START TENSORBOX CODE ###\n#     idl_filename=path_video_folder+\'/\'+path_video_folder+\'.idl\'\n\n#     ### Opening Hypes file for parameters\n    \n#     with open(hypes_file, \'r\') as f:\n#         H = json.load(f)\n\n#     ### Building Network\n\n#     tf.reset_default_graph()\n#     googlenet = googlenet_load.init(H)\n#     x_in = tf.placeholder(tf.float32, name=\'x_in\', shape=[H[\'image_height\'], H[\'image_width\'], 3])\n\n#     if H[\'use_rezoom\']:\n#         pred_boxes, pred_logits, pred_confidences, pred_confs_deltas, pred_boxes_deltas = build_forward(H, tf.expand_dims(x_in, 0), googlenet, \'test\', reuse=None)\n#         grid_area = H[\'grid_height\'] * H[\'grid_width\']\n#         pred_confidences = tf.reshape(tf.nn.softmax(tf.reshape(pred_confs_deltas, [grid_area * H[\'rnn_len\'], H[\'num_classes\']])), [grid_area, H[\'rnn_len\'], H[\'num_classes\']])\n#         pred_logits = tf.reshape(tf.nn.softmax(tf.reshape(pred_logits, [grid_area * H[\'rnn_len\'], H[\'num_classes\']])), [grid_area, H[\'rnn_len\'], H[\'num_classes\']])\n#     if H[\'reregress\']:\n#         pred_boxes = pred_boxes + pred_boxes_deltas\n#     else:\n#         pred_boxes, pred_logits, pred_confidences = build_forward(H, tf.expand_dims(x_in, 0), googlenet, \'test\', reuse=None)\n\n#     saver = tf.train.Saver()\n\n#     with tf.Session() as sess:\n\n\n#         sess.run(tf.initialize_all_variables())\n#         saver.restore(sess, weights_file )##### Restore a Session of the Model to get weights and everything working\n    \n#         annolist = al.AnnoList()\n    \n#         #### Starting Evaluating the images\n#         lenght=int(len(frames_list))\n        \n#         print(""%d Frames to DET""%len(frames_list))\n        \n#         progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n#         frameNr=0\n#         skipped=0\n#         for i in progress(range(0, len(frames_list))):\n\n#             if utils_image.isnotBlack(frames_list[i]) & utils_image.check_image_with_pil(frames_list[i]):\n\n#                 img = imread(frames_list[i])\n#                 feed = {x_in: img}\n#                 (np_pred_boxes,np_pred_logits, np_pred_confidences) = sess.run([pred_boxes,pred_logits, pred_confidences], feed_dict=feed)\n#                 # print_logits(np_pred_confidences)\n#                 pred_anno = al.Annotation()\n#                 #pred_anno.imageName = test_anno.imageName\n            \n#                 # print ""np_pred_confidences shape"" + str(np_pred_confidences.shape)\n#                 # print ""np_pred_boxes shape"" + str(np_pred_boxes.shape)\n#                 # for i in range(0, np_pred_confidences.shape[0]):\n#                 #     print np_pred_confidences[i]\n#                 #     for j in range(0, np_pred_confidences.shape[2]):\n#                 #         print np_pred_confidences[i][0][j]\n\n#                 rects, _ = get_multiclass_rectangles(H, np_pred_confidences, np_pred_boxes, rnn_len=H[\'rnn_len\'])\n#                 pred_anno.rects = rects\n#                 pred_anno.imageName = frames_list[i]\n#                 pred_anno.frameNr = frameNr\n#                 frameNr=frameNr+1\n#                 det_frames_list.append(frames_list[i])\n#                 pick = NMS(rects)\n#                 # draw_rectangles(frames_list[i],frames_list[i], pick)\n\n#                 annolist.append(pred_anno)\n\n#             else: skipped=skipped+1 \n\n#         saveTextResults(idl_filename,annolist)\n#         annolist.save(pred_idl)\n#         print(""Skipped %d Black Frames""%skipped)\n\n#     #### END TENSORBOX CODE ###\n\n#     return det_frames_list\n\ndef bbox_det_TENSORBOX_multiclass(frames_list,path_video_folder,hypes_file,weights_file,pred_idl):\n    \n    from train import build_forward\n\n    print(""Starting DET Phase"")\n    \n    #### START TENSORBOX CODE ###\n\n    lenght=int(len(frames_list))\n    video_info = []\n    ### Opening Hypes file for parameters\n    \n    with open(hypes_file, \'r\') as f:\n        H = json.load(f)\n\n    ### Building Network\n\n    tf.reset_default_graph()\n    googlenet = googlenet_load.init(H)\n    x_in = tf.placeholder(tf.float32, name=\'x_in\', shape=[H[\'image_height\'], H[\'image_width\'], 3])\n\n    if H[\'use_rezoom\']:\n        pred_boxes, pred_logits, pred_confidences, pred_confs_deltas, pred_boxes_deltas = build_forward(H, tf.expand_dims(x_in, 0), googlenet, \'test\', reuse=None)\n        grid_area = H[\'grid_height\'] * H[\'grid_width\']\n        pred_confidences = tf.reshape(tf.nn.softmax(tf.reshape(pred_confs_deltas, [grid_area * H[\'rnn_len\'], H[\'num_classes\']])), [grid_area, H[\'rnn_len\'], H[\'num_classes\']])\n        pred_logits = tf.reshape(tf.nn.softmax(tf.reshape(pred_logits, [grid_area * H[\'rnn_len\'], H[\'num_classes\']])), [grid_area, H[\'rnn_len\'], H[\'num_classes\']])\n    if H[\'reregress\']:\n        pred_boxes = pred_boxes + pred_boxes_deltas\n    else:\n        pred_boxes, pred_logits, pred_confidences = build_forward(H, tf.expand_dims(x_in, 0), googlenet, \'test\', reuse=None)\n\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n\n        if(int(tf.__version__.split(""."")[0])==0 and int(tf.__version__.split(""."")[1])<12): ### for tf v<0.12.0\n            sess.run(tf.initialize_all_variables())\n        else: ### for tf v>=0.12.0\n            sess.run(tf.global_variables_initializer())\n        saver.restore(sess, weights_file )##### Restore a Session of the Model to get weights and everything working\n    \n        #### Starting Evaluating the images\n        \n        print(""%d Frames to DET""%len(frames_list))\n        \n        progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n        frameNr=0\n        skipped=0\n        for i in progress(range(0, len(frames_list))):\n\n            current_frame = frame.Frame_Info()\n            current_frame.frame=frameNr\n            current_frame.filename=frames_list[i]\n\n            if utils_image.isnotBlack(frames_list[i]) & utils_image.check_image_with_pil(frames_list[i]):\n\n                img = imread(frames_list[i])\n                # test(frames_list[i])\n                feed = {x_in: img}\n                (np_pred_boxes,np_pred_logits, np_pred_confidences) = sess.run([pred_boxes,pred_logits, pred_confidences], feed_dict=feed)\n\n                _,rects = get_multiclass_rectangles(H, np_pred_confidences, np_pred_boxes, rnn_len=H[\'rnn_len\'])\n                if len(rects)>0:\n                    # pick = NMS(rects)\n                    pick = rects\n                    print len(rects),len(pick)\n                    current_frame.rects=pick\n                    frameNr=frameNr+1\n                    video_info.insert(len(video_info), current_frame)\n                    print len(current_frame.rects)\n                else: skipped=skipped+1 \n            else: skipped=skipped+1 \n\n        print(""Skipped %d Black Frames""%skipped)\n\n    #### END TENSORBOX CODE ###\n\n    return video_info\n'"
Utils_Video.py,0,"b'import os\nimport cv2\nimport progressbar\nimport copy\nimport utils_image\nimport Utils_Imagenet\nimport Utils_Tensorbox\nimport frame\nimport multiclass_rectangle\nimport vid_classes\nfrom PIL import Image,ImageDraw\nimport sys\n\n### Fucntions to mount the video from frames\n\ndef draw_rectangles(path_video_folder, labeled_video_frames):\n\n    labeled_frames =[]\n    folder_path=path_video_folder+""/labeled_frames/""\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n        print(""Created Folder: %s""%folder_path)\n    for frame in labeled_video_frames:\n        bb_img = Image.open(frame.filename)\n        new_img = folder_path + os.path.splitext(os.path.basename(frame.filename))[0]+ ""_labeled"" + os.path.splitext(os.path.basename(frame.filename))[1]\n        print ""Original filename:%s""%frame.filename\n        print ""New filename:%s""%new_img\n        for bb_rect in frame.rects:\n        ################ Adding Rectangle ###################\n            dr = ImageDraw.Draw(bb_img)\n            cor = (bb_rect.x1,bb_rect.y1,bb_rect.x2 ,bb_rect.y2) # DA VERIFICARE Try_2 (x1,y1, x2,y2) cor = (bb_rect.left() ,bb_rect.right(),bb_rect.bottom(),bb_rect.top()) Try_1\n            if bb_rect.label_code is \'Not Set\':\n                outline_class=(240,255,240)\n            else :  \n                outline_class=vid_classes.code_to_color(bb_rect.label_chall)\n            dr.rectangle(cor, outline=outline_class)\n        # print save_img  \n        bb_img.save(new_img)\n        labeled_frames.append(new_img)\n    return labeled_frames\n\ndef draw_rectangle(image_path, rect_box):\n\n    bb_img = Image.open(image_path)\n    ################ Adding Rectangle ###################\n    dr = ImageDraw.Draw(bb_img)\n    cor = (rect_box[0],rect_box[1],rect_box[2],rect_box[3]) # DA VERIFICARE Try_2 (x1,y1, x2,y2) cor = (bb_rect.left() ,bb_rect.right(),bb_rect.bottom(),bb_rect.top()) Try_1\n    outline_class=(240,255,240)\n    dr.rectangle(cor, outline=outline_class)\n    # print save_img  \n    bb_img.save(image_path)\n\n\ndef make_tracked_video(out_vid_path, labeled_video_frames):\n\n    if labeled_video_frames[0] is not None:\n\n        img = cv2.imread(labeled_video_frames[0], True)\n        print ""Reading Filename: %s""%labeled_video_frames[0]\n        h, w = img.shape[:2]\n        print ""Video Size: width: %d height: %d""%(h, w)\n        fourcc = cv2.cv.CV_FOURCC(\'m\', \'p\', \'4\', \'v\')\n        out = cv2.VideoWriter(out_vid_path,fourcc, 20.0, (w, h), True)\n        print(""Start Making File Video:%s "" % out_vid_path)\n        print(""%d Frames to Compress""%len(labeled_video_frames))\n        progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n        for i in progress(range(0,len(labeled_video_frames))):\n            if utils_image.check_image_with_pil(labeled_video_frames[i]):\n                out.write(img)\n                img = cv2.imread(labeled_video_frames[i], True)\n        out.release()\n        print(""Finished Making File Video:%s "" % out_vid_path)\n\n\ndef make_video_from_list(out_vid_path, frames_list):\n    if frames_list[0] is not None:\n        img = cv2.imread(frames_list[0], True)\n        print frames_list[0]\n        h, w = img.shape[:2]\n        fourcc = cv2.cv.CV_FOURCC(\'m\', \'p\', \'4\', \'v\')\n        out = cv2.VideoWriter(out_vid_path,fourcc, 20.0, (w, h), True)\n        print(""Start Making File Video:%s "" % out_vid_path)\n        print(""%d Frames to Compress""%len(frames_list))\n        progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n        for i in progress(range(0,len(frames_list))):\n            if utils_image.check_image_with_pil(frames_list[i]):\n                out.write(img)\n                img = cv2.imread(frames_list[i], True)\n        out.release()\n        print(""Finished Making File Video:%s "" % out_vid_path)\n\n\ndef make_video_from_frames(out_vid_path, frames):\n    if frames[0] is not None:\n        h, w = frames[0].shape[:2]\n        fourcc = cv2.FOURCC(\'m\', \'p\', \'4\', \'v\')\n        out = cv2.VideoWriter(out_vid_path,fourcc, 20.0, (w, h), True)\n        print(""Start Making File Video:%s "" % out_vid_path)\n        print(""%d Frames to Compress""%len(frames))\n        progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n        for i in progress(range(0,len(frames))):\n            out.write(frames[i])\n        out.release()\n        print(""Finished Making File Video:%s "" % out_vid_path)\n\n\n####### FOR TENSORBOX ###########\n\ndef extract_idl_from_frames(vid_path, video_perc, path_video_folder, folder_path_frames, idl_filename):\n    \n    ####### Creating Folder for the video frames and the idl file for the list\n    \n    if not os.path.exists(path_video_folder):\n        os.makedirs(path_video_folder)\n        print(""Created Folder: %s""%path_video_folder)\n    if not os.path.exists(path_video_folder+\'/\'+folder_path_frames):\n        os.makedirs(path_video_folder+\'/\'+folder_path_frames)\n        print(""Created Folder: %s""% (path_video_folder+\'/\'+folder_path_frames))\n    if not os.path.exists(idl_filename):\n        open(idl_filename, \'a\')\n        print ""Created File: ""+ idl_filename\n    list=[]\n    # Opening & Reading the Video\n\n    print(""Opening File Video:%s "" % vid_path)\n    vidcap = cv2.VideoCapture(vid_path)\n    if not vidcap.isOpened():\n        print ""could Not Open :"",vid_path\n        return\n    print(""Opened File Video:%s "" % vid_path)\n    print(""Start Reading File Video:%s "" % vid_path)\n    \n    total = int((vidcap.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT)/100)*video_perc)\n    \n    print(""%d Frames to Read""%total)\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    image = vidcap.read()\n    with open(idl_filename, \'w\') as f:\n        for i in progress(range(0,total)):\n            #frame_name=""%s/%s/fram%d.jpeg""%(path_video_folder,folder_path_frames,i)\n            list.append(""%s/%sframe%d.jpeg""%(path_video_folder,folder_path_frames,i))\n            cv2.imwrite(""%s/%sframe%d.jpeg""%(path_video_folder,folder_path_frames,i), image[1])     # save frame as JPEG file\n            image = vidcap.read()\n\n    print(""Finish Reading File Video:%s "" % vid_path)\n    return list\n\ndef extract_frames_incten(vid_path, video_perc, path_video_folder, idl_filename):\n    \n    ####### Creating Folder for the video frames and the idl file for the list\n    \n    if not os.path.exists(path_video_folder):\n        os.makedirs(path_video_folder)\n        print(""Created Folder: %s""%path_video_folder)\n    if not os.path.exists(path_video_folder+\'/frames_tensorbox/\'):\n        os.makedirs(path_video_folder+\'/frames_tensorbox/\')\n        print(""Created Folder: %s""% (path_video_folder+\'/frames_tensorbox/\'))\n    if not os.path.exists(path_video_folder+\'/frames_inception/\'):\n        os.makedirs(path_video_folder+\'/frames_inception/\')\n        print(""Created Folder: %s""% (path_video_folder+\'/frames_inception/\'))\n    if not os.path.exists(idl_filename):\n        open(idl_filename, \'a\')\n        print ""Created File: ""+ idl_filename\n        \n    list_tensorbox=[]\n    list_inception=[]\n    # Opening & Reading the Video\n\n    print(""Opening File Video:%s "" % vid_path)\n    vidcap = cv2.VideoCapture(vid_path)\n    if not vidcap.isOpened():\n        print ""could Not Open :"",vid_path\n        return\n    print(""Opened File Video:%s "" % vid_path)\n    print(""Start Reading File Video:%s "" % vid_path)\n    \n    total = int((vidcap.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT)/100)*video_perc)\n    \n    print(""%d Frames to Read""%total)\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    image = vidcap.read()\n    with open(idl_filename, \'w\') as f:\n        for i in progress(range(0,total)):\n            #frame_name=""%s/%s/fram%d.jpeg""%(path_video_folder,folder_path_frames,i)\n            list_tensorbox.append(""%s/%sframe%d.jpeg""%(path_video_folder,""frames_tensorbox/"",i))\n            cv2.imwrite(""%s/%sframe%d.jpeg""%(path_video_folder,""frames_tensorbox/"",i), image[1])     # save frame as JPEG file\n            list_inception.append(""%s/%sframe%d.jpeg""%(path_video_folder,""frames_inception/"",i))\n            cv2.imwrite(""%s/%sframe%d.jpeg""%(path_video_folder,""frames_inception/"",i), image[1])     # save frame as JPEG file\n            image = vidcap.read()\n\n    print(""Finish Reading File Video:%s "" % vid_path)\n    return list_tensorbox, list_inception\n\n\n### Function to track objects and spread informations between frames\n\ndef recurrent_track_objects(video_info):\n\n    previous_frame= None\n    previous_num_obj=-1\n\n    tracked_video=[]\n    deltas_video=[]\n    deltas_frame=[]\n    dx1,dx2,dy1,dy2=0,0,0,0\n\n    for frame_info in video_info:\n        print ""Tracking Frame Nr: %d""%frame_info.frame\n        print ""Len Rects Frame: %d""%len(frame_info.rects)\n        current_frame = frame.Frame_Info()\n        current_frame=frame_info.duplicate()\n        current_frame.rects=[]\n        if previous_frame is not None:\n            deltas_frame=[]\n            if frame_info.frame>1:\n                print ""Len Previous Rects Frame: %d""%len(previous_frame.rects)\n                rect_idx=0\n                for rect in previous_frame.rects:\n                    print len(current_frame.rects)\n                    rect.add_delta(deltas_video[frame_info.frame-2][rect_idx][0],deltas_video[frame_info.frame-2][rect_idx][1],deltas_video[frame_info.frame-2][rect_idx][2],deltas_video[frame_info.frame-2][rect_idx][3])\n                    current_rect = multiclass_rectangle.pop_max_iou(frame_info.rects,rect)\n                    if current_rect is not None:\n                        current_rect.load_trackID(rect.trackID)\n                        current_rect.check_rects_motion(frame_info.filename, rect, deltas_video[frame_info.frame-2][rect_idx][0],deltas_video[frame_info.frame-2][rect_idx][1],deltas_video[frame_info.frame-2][rect_idx][2],deltas_video[frame_info.frame-2][rect_idx][3])\n                        current_frame.append_labeled_rect(current_rect)\n                        dx1=current_rect.x1-rect.x1\n                        dx2=current_rect.x2-rect.x2\n                        dy1=current_rect.y1-rect.y1\n                        dy2=current_rect.y2-rect.y2\n                        deltas_frame.append((dx1,dx2,dy1,dy2))\n                    else: break\n            else:\n                print ""Len Previous Rects Frame: %d""%len(previous_frame.rects)\n                for rect in previous_frame.rects:\n                    print len(current_frame.rects)\n                    current_rect = multiclass_rectangle.pop_max_iou(frame_info.rects,rect)\n                    if current_rect is not None:\n                        dx1=current_rect.x1-rect.x1\n                        dx2=current_rect.x2-rect.x2\n                        dy1=current_rect.y1-rect.y1\n                        dy2=current_rect.y2-rect.y2\n                        deltas_frame.append((dx1,dx2,dy1,dy2))\n                        current_rect.load_trackID(rect.trackID)\n                        current_frame.append_labeled_rect(current_rect)\n                    else: break\n            deltas_video.append(deltas_frame)\n        else:\n            trackID=1\n            picked_rect=Utils_Tensorbox.NMS(frame_info.rects)\n            for rect in picked_rect:       \n                current_rect = rect.duplicate()\n                current_rect.load_trackID(trackID)\n                current_frame.append_labeled_rect(current_rect)\n                trackID=trackID+1\n\n        previous_frame=current_frame.duplicate()\n        previous_frame.rects= multiclass_rectangle.duplicate_rects(current_frame.rects)\n\n        print ""Current Frame obj:%d""%len(current_frame.rects)\n        tracked_video.insert(len(tracked_video), current_frame)\n\n    return tracked_video\n\n\ndef track_objects(video_info):\n\n    previous_frame= None\n    previous_num_obj=-1\n\n    tracked_video=[]\n\n\n    for frame_info in video_info:\n        print ""Tracking Frame Nr: %d""%frame_info.frame\n        print ""Len Rects Frame: %d""%len(frame_info.rects)\n        current_frame = frame.Frame_Info()\n        current_frame=frame_info.duplicate()\n        current_frame.rects=[]\n        if previous_frame is not None:\n            print ""Len Previous Rects Frame: %d""%len(previous_frame.rects)\n            for rect in previous_frame.rects:\n                print len(current_frame.rects)\n                current_rect = multiclass_rectangle.pop_max_iou(frame_info.rects,rect)\n                current_rect.load_trackID(rect.trackID)\n                current_frame.append_labeled_rect(current_rect)\n        else:\n            trackID=1\n            picked_rect=Utils_Tensorbox.NMS(frame_info.rects)\n            for rect in picked_rect:       \n                current_rect = rect.duplicate()\n                current_rect.load_trackID(trackID)\n                current_frame.append_labeled_rect(current_rect)\n                trackID=trackID+1\n\n        previous_frame=current_frame.duplicate()\n        previous_frame.rects= multiclass_rectangle.duplicate_rects(current_frame.rects)\n\n        print ""Current Frame obj:%d""%len(current_frame.rects)\n        tracked_video.insert(len(tracked_video), current_frame)\n\n    return tracked_video\n\ndef track_min_objects(video_info):\n\n    previous_frame= None\n    previous_num_obj=-1\n\n    tracked_video=[]\n\n    frame_id=0\n    min_rects=[]\n    min_frame_id=None\n    min_num_obj=None\n    for frame_info in video_info:\n        if (min_num_obj is None) & (len(frame_info.rects) >0):\n            min_num_obj = len(frame_info.rects)\n            min_frame_id=frame_id\n        if (len(frame_info.rects) < min_num_obj ) & (len(frame_info.rects) >0):\n            min_num_obj = len(frame_info.rects)\n            min_frame_id=frame_id\n        frame_id=frame_id+1\n    min_rects = multiclass_rectangle.duplicate_rects(video_info[min_frame_id].rects)\n    print ""Min num object video:%d""%min_num_obj\n\n    for frame_info in video_info:\n        print ""Tracking Frame Nr: %d""%frame_info.frame\n        print ""Len Rects Frame: %d""%len(frame_info.rects)\n        current_frame = frame.Frame_Info()\n        current_frame=frame_info.duplicate()\n        current_frame.rects=[]\n        if previous_frame is not None:\n            print ""Min num object video:%d""%min_num_obj\n            print ""Len Previous Rects Frame: %d""%len(previous_frame.rects)\n            for rect in previous_frame.rects:\n                print len(current_frame.rects)\n                if len(current_frame.rects)<=min_num_obj:               \n                    current_rect = multiclass_rectangle.pop_max_iou(frame_info.rects,rect)\n                    current_rect.load_trackID(rect.trackID)\n                    current_frame.append_labeled_rect(current_rect)\n        else:\n            trackID=1\n            for rect in min_rects:\n                if len(current_frame.rects)<min_num_obj:               \n                    current_rect = multiclass_rectangle.pop_max_iou(frame_info.rects,rect)\n                    current_rect.load_trackID(trackID)\n                    current_frame.append_labeled_rect(current_rect)\n                    trackID=trackID+1\n\n        previous_frame=current_frame.duplicate()\n        previous_frame.rects= multiclass_rectangle.duplicate_rects(current_frame.rects)\n\n        print ""Current Frame obj:%d""%len(current_frame.rects)\n        tracked_video.insert(len(tracked_video), current_frame)\n\n    return tracked_video\n\n\ndef track_and_label_objects(video_info):\n\n    previous_frame= None\n    previous_num_obj=-1\n\n    cropped_img_array=[]\n    tracked_video=[]\n\n    for frame_info in video_info:\n        print ""Tracking Frame Nr: %d""%frame_info.frame\n        print len(frame_info.rects)\n        current_frame = frame.Frame_Info()\n        current_frame=frame_info.duplicate()\n        current_frame.rects=[]\n        print len(frame_info.rects)\n        if previous_frame is not None:\n            print ""Previous Frame obj:%d""%previous_num_obj\n            for rect in frame_info.rects:\n                print ""Entered into the rect check""\n                max_rect=None\n                max_iou=0\n                current_rect= Rectangle_Multiclass()\n                trackID=-1\n                if previous_num_obj >0: ### If i come here means that there\'s the same number of object between the previous and the current frame\n                    print ""Entered into the rect check with :%d objects""%previous_num_obj\n                    id_rect=0\n                    max_id=0\n                    for prev_rect in previous_frame.rects:\n                        print ""Entered""\n                        if rect.iou(prev_rect)>max_iou:\n                            max_iou=rect.iou(prev_rect)\n                            max_id=id_rect\n                        id_rect=id_rect+1\n                    print ""Lenght previous rects array: %d""%len(previous_frame.rects)\n                    print ""max_rect track ID: %d""%previous_frame.rects[max_id].trackID\n                    print ""max_rect label: %s""%previous_frame.rects[max_id].label\n                    current_rect.load_labeled_rect(previous_frame.rects[max_id].trackID, previous_frame.rects[max_id].true_confidence, previous_frame.rects[max_id].label_confidence, previous_frame.rects[max_id].x1,previous_frame.rects[max_id].y1,previous_frame.rects[max_id].x2 ,previous_frame.rects[max_id].y2, previous_frame.rects[max_id].label, previous_frame.rects[max_id].label_chall, previous_frame.rects[max_id].label_code)\n                    current_frame.append_labeled_rect(current_rect)\n                    rect.load_label(previous_frame.rects[max_id].trackID,previous_frame.rects[max_id].label_confidence, previous_frame.rects[max_id].label, previous_frame.rects[max_id].label_chall, previous_frame.rects[max_id].label_code)\n                    previous_frame.rects.pop(max_id)\n                    previous_num_obj=previous_num_obj-1\n                else:\n                    ### If i come here means that there\'s more objects in the current frame respect to che previous\n                    if previous_num_obj == 0:\n                        trackID = len(frame_info.rects)\n                        previous_num_obj = -1\n                    current_rect= Rectangle_Multiclass()\n\n                    img= Image.open(frame_info.filename)\n                    cor = (rect.x1,rect.y1,rect.x2 ,rect.y2)\n\n                    cropped_img=img.crop(cor)\n                    cropped_img_name=""cropped_frame_%d.JPEG""%(frame_info.frame)\n                    cropped_img.save(cropped_img_name)\n                    cropped_img_array.append(cropped_img_name)\n\n                    label, confidence = Utils_Imagenet.run_inception_once(cropped_img_name)\n                    rect.load_label(trackID,confidence, vid_classes.code_to_class_string(label), vid_classes.code_to_code_chall(vid_classes), label)\n                    current_rect.load_labeled_rect(trackID, rect.true_confidence, confidence, rect.x1,rect.y1,rect.x2 ,rect.y2, vid_classes.code_to_class_string(label), vid_classes.code_to_code_chall(vid_classes), label)\n                    print ""current_rect track ID: %d""%current_rect.trackID\n                    print ""current_rect label: %s""%current_rect.label\n                    current_frame.append_labeled_rect(current_rect)\n        else:\n            trackID=1\n\n            for rect in frame_info.rects:\n                \n                current_rect= Rectangle_Multiclass()\n\n                img= Image.open(frame_info.filename)\n                cor = (rect.x1,rect.y1,rect.x2 ,rect.y2)\n\n                cropped_img=img.crop(cor)\n                cropped_img_name=""cropped_frame_%d.JPEG""%(frame_info.frame)\n                cropped_img.save(cropped_img_name)\n                cropped_img_array.append(cropped_img_name)\n\n                label, confidence = Utils_Imagenet.run_inception_once(cropped_img_name)\n                rect.load_label(trackID,confidence, vid_classes.code_to_class_string(label), vid_classes.code_to_code_chall(vid_classes), label)\n                current_rect.load_labeled_rect(trackID, rect.true_confidence, confidence, rect.x1,rect.y1,rect.x2 ,rect.y2, vid_classes.code_to_class_string(label), vid_classes.code_to_code_chall(vid_classes), label)\n                current_frame.append_labeled_rect(current_rect)\n                \n                trackID=trackID+1\n\n        previous_num_obj=len(frame_info.rects)\n        previous_frame=frame_info.duplicate()\n        previous_frame.duplicate_rects(frame_info.rects)\n\n        print previous_frame\n        print ""Previous Frame obj:%d""%previous_num_obj\n        print ""prev_rect 0 track ID: %d""%previous_frame.rects[0].trackID\n        print ""prev_rect 0 label: %s""%previous_frame.rects[0].label\n        tracked_video.insert(len(tracked_video), current_frame)\n\n    return tracked_video\n\n\n####### FOR YOLO ###########\n\ndef extract_frames(vid_path, video_perc):\n    list=[]\n    frames=[]\n    # Opening & Reading the Video\n    print(""Opening File Video:%s "" % vid_path)\n    vidcap = cv2.VideoCapture(vid_path)\n    if not vidcap.isOpened():\n        print ""could Not Open :"",vid_path\n        return\n    print(""Opened File Video:%s "" % vid_path)\n    print(""Start Reading File Video:%s "" % vid_path)\n    image = vidcap.read()\n    total = int((vidcap.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT)/100)*video_perc)\n    print(""%d Frames to Read""%total)\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    for i in progress(range(0,total)):\n        list.append(""frame%d.jpg"" % i)\n        frames.append(image)\n        image = vidcap.read()\n    print(""Finish Reading File Video:%s "" % vid_path)\n    return frames, list\n'"
VID_tensorbox.py,0,"b'#### My import\n\nimport argparse\nimport utils_image\nimport utils_video\nimport Utils_Tensorbox\nimport Utils_Imagenet\nimport frame\nimport vid_classes\nimport progressbar\nimport time\nimport os\n\n######### MAIN ###############\n\ndef main():\n    \'\'\'\n    Parse command line arguments and execute the code \n\n    \'\'\'\n\n    ######### TENSORBOX PARAMETERS\n\n\n    start = time.time()\n\n    parser = argparse.ArgumentParser()\n    # parser.add_argument(\'--result_folder\', default=\'summary_result/\', type=str)\n    # parser.add_argument(\'--summary_file\', default=\'results.txt\', type=str)\n    parser.add_argument(\'--output_name\', default=\'output.mp4\', type=str)\n    parser.add_argument(\'--hypes\', default=\'./TENSORBOX/hypes/overfeat_rezoom.json\', type=str)\n    parser.add_argument(\'--weights\', default=\'./TENSORBOX/data/save.ckpt-1250000\', type=str)\n    parser.add_argument(\'--perc\', default=100, type=int)\n    parser.add_argument(\'--path_video\', default=\'ILSVRC2015_val_00004000.mp4\', type=str)# required=True, type=str)\n\n    args = parser.parse_args()\n\n    # hypes_file = \'./hypes/overfeat_rezoom.json\'\n    # weights_file= \'./output/save.ckpt-1090000\'\n\n    path_video_folder = os.path.splitext(os.path.basename(args.path_video))[0]\n    pred_idl = \'./%s/%s_val.idl\' % (path_video_folder, path_video_folder)\n    idl_filename=path_video_folder+\'/\'+path_video_folder+\'.idl\'\n    frame_tensorbox=[]\n    frame_inception=[]\n    frame_tensorbox, frame_inception = utils_video.extract_frames_incten(args.path_video, args.perc, path_video_folder, idl_filename )\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n\n    for image_path in progress(frame_tensorbox):\n        utils_image.resizeImage(image_path)\n    utils_image.resizeImage(-1)\n\n    video_info=Utils_Tensorbox.bbox_det_TENSORBOX_multiclass(frame_tensorbox, path_video_folder, args.hypes, args.weights, pred_idl)\n    tracked_video=utils_video.recurrent_track_objects(video_info)\n    # tracked_video=utils_video.track_objects(video_info)\n    # labeled_video=Utils_Imagenet.label_video(tracked_video, frame_inception)\n    labeled_video=Utils_Imagenet.recurrent_label_video(tracked_video, frame_inception)\n    # tracked_video=utils_video.track_objects(video_info)\n\n    # tracked_video=utils_video.track_and_label_objects(video_info)\n    labeled_frames=utils_video.draw_rectangles(path_video_folder, labeled_video)\n    utils_video.make_tracked_video(args.output_name, labeled_frames)\n    frame.saveVideoResults(idl_filename,labeled_video)\n\n    # utils_video.make_tracked_video(args.output_name, labeled_video)\n    end = time.time()\n\n    print(""Elapsed Time:%d Seconds""%(end-start))\n    print(""Running Completed with Success!!!"")\n\nif __name__ == \'__main__\':\n    main()\n\n\n\n\n'"
VID_yolo.py,1,"b'# Import the necessary packages\n\nimport numpy as np\nimport cv2\nimport os\nimport time\nimport progressbar\nimport pandas\nimport sys\nimport argparse\nimport Utils_Video\n\n# Import DET Alg package\nsys.path.insert(0, \'YOLO\')\nimport YOLO_small_tf\n\n# DET Alg Params\n\n# yolo.disp_console = (True or False, default = True)\n# yolo.imshow = (True or False, default = True)\n# yolo.tofile_img = (output image filename)\n# yolo.tofile_txt = (output txt filename)\n# yolo.filewrite_img = (True or False, default = False)\n# yolo.filewrite_txt = (True of False, default = False)\n# yolo.detect_from_file(filename)\n# yolo.detect_from_cvmat(cvmat)\n\n########## SETTING PARAMETERS\n\ndef still_image_YOLO_DET(frames_list, frames_name, folder_path_det_frames,folder_path_det_result):\n    print(""Starting DET Phase"")\n    if not os.path.exists(folder_path_det_frames):\n        os.makedirs(folder_path_det_frames)\n        print(""Created Folder: %s""%folder_path_det_frames)\n    if not os.path.exists(folder_path_det_result):\n        os.makedirs(folder_path_det_result)\n        print(""Created Folder: %s""%folder_path_det_result)\n    yolo = YOLO_small_tf.YOLO_TF()\n    det_frames_list=[]\n    det_result_list=[]\n    print(""%d Frames to DET""%len(frames_list))\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    for i in progress(range(0,len(frames_list))):\n        # det_frame_name = frames_name[i]\n        #print frames_name[i]\n        det_frame_name = frames_name[i].replace(\'.jpg\',\'_det.jpg\')\n        det_frame_name = folder_path_det_frames + det_frame_name\n        det_frames_list.append(det_frame_name)\n        \n        det_result_name= frames_name[i].replace(\'.jpg\',\'.txt\')\n        det_result_name = folder_path_det_result + det_result_name\n        det_result_list.append(det_result_name)\n        \n        yolo.tofile_txt = det_result_name\n        yolo.filewrite_txt = True\n        yolo.disp_console = False\n        yolo.filewrite_img = True\n        yolo.tofile_img = det_frame_name\n        yolo.detect_from_cvmat(frames_list[i][1])\n    return det_frames_list,det_result_list\n\n\ndef print_YOLO_DET_result(det_results_list,folder_path_summary_result, file_path_summary_result ):\n    results_list=[]\n    if not os.path.exists(folder_path_summary_result):\n        os.makedirs(folder_path_summary_result)\n        print(""Created Folder: %s""%folder_path_summary_result)\n    print(""Starting Loading Results "")\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    names=[\'class_name\', \'x1\',\'y1\',\'x2\',\'y2\',\'score\']\n    df = pandas.DataFrame(columns=names)\n    mean=0.0\n    with open(file_path_summary_result, ""w"") as out_file:\n        for i in progress(range(0,len(det_results_list))):\n        #df.append(pandas.read_csv(det_results_list[i], sep=\',\',names=names, encoding=""utf8""))\n        #results_list.append(pandas.read_csv(det_results_list[i], sep=\',\',names=names, encoding=""utf8""))\n            for line in open(det_results_list[i], ""r""):\n                df.loc[i] =tuple(line.strip().split(\',\'))\n                mean=mean+float(df.loc[i].score)\n                out_file.write(str(tuple(line.strip().split(\',\')))+ os.linesep)\n    print(""Finished Loading Results "")\n    print(""Computing Final Mean Reasults.."")\n    print ""Class: "" + df.class_name.max()\n    print ""Max Value: "" + df.score.max()\n    print ""Min Value: "" + df.score.min()\n    print ""Avg Value: "" + str(mean/len(df))\n    return\n\n######### MAIN ###############\n\ndef main():\n    \'\'\'\n    Parse command line arguments and execute the code\n\n    \'\'\'\n    start = time.time()\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--det_frames_folder\', default=\'det_frames/\', type=str)\n    parser.add_argument(\'--det_result_folder\', default=\'det_results/\', type=str)\n    parser.add_argument(\'--result_folder\', default=\'summary_result/\', type=str)\n    parser.add_argument(\'--summary_file\', default=\'results.txt\', type=str)\n    parser.add_argument(\'--output_name\', default=\'output.mp4\', type=str)\n    parser.add_argument(\'--perc\', default=5, type=int)\n    parser.add_argument(\'--path_video\', required=True, type=str)\n    args = parser.parse_args()\n\n    frame_list, frames = Utils_Video.extract_frames(args.path_video, args.perc)\n    det_frame_list,det_result_list=still_image_YOLO_DET(frame_list, frames, args.det_frames_folder,args.det_result_folder)\n    Utils_Video.make_video_from_list(args.output_name, det_frame_list)\n    print_YOLO_DET_result(det_result_list,args.result_folder, args.summary_file)\n\n    end = time.time()\n\n    print(""Elapsed Time:%d Seconds""%(end-start))\n    print(""Running Completed with Success!!!"")\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n\n'"
eval_script.py,0,"b'import frame as fm\nimport multiclass_rectangle\nimport Utils\nimport progressbar\nimport os\nimport vid_classes\nfrom xml.etree import ElementTree\nimport utils_image\n\ndef parse_XML_to_data(xml_list_video):\n    frames_list=[]\n    video_list=[]\n    # image_multi_class= None\n    # rectangle_multi = None\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    for i in progress(range(0, len(xml_list_video))):\n        # print ""Iterating on Video:""+ str(xml_list_video[i][0][0])\n        for j in range(0, len(xml_list_video[i])):\n            # print ""Iterating on Frame:""+ str(xml_list_video[i][j][0])\n            with open(xml_list_video[i][j][0], \'rt\') as f:\n                tree = ElementTree.parse(f)\n                for obj in tree.findall(\'object\'):                \n                    name = obj.find(\'name\').text\n                    class_code= name\n                    name = vid_classes.code_to_class_string(name)\n                    if name in [""nothing""]:\n                        continue\n                    else:\n                        #The files with the original data path are made in both: multiclass e single class\n                        jump=0\n                        image_multi_class= fm.Frame_Info()\n                        image_multi_class.frame= xml_list_video[i][j][1]\n                        # print image_multi_class.frame\n                        rectangle_multi= multiclass_rectangle.Rectangle_Multiclass()\n                        for node in tree.iter():\n                            tag=str(node.tag)\n                            if tag in [\'name\']:\n                                if str(vid_classes.code_to_class_string(str(node.text))) in [""nothing""]:\n                                    jump = 1\n                                else : \n                                    jump=0\n                                    rectangle_multi.label_chall=int(vid_classes.class_string_to_comp_code(str(vid_classes.code_to_class_string(str(node.text)))))\n                                    # print rectangle_multi.label_chall\n                                    rectangle_multi.label_code=str(node.text)\n                                    rectangle_multi.label=vid_classes.code_to_class_string(str(node.text))                                \n                            if tag in [""xmax""]:\n                                if jump == 0:\n                                    rectangle_multi.x2=float(node.text)\n                            if tag in [""xmin""]:\n                                if jump == 0:\n                                    rectangle_multi.x1=float(node.text)\n                            if tag in [""ymax""]:\n                                if jump == 0:\n                                    rectangle_multi.y2=float(node.text)                            \n                            if tag in [""ymin""]:\n                                if jump == 0:    \n                                    rectangle_multi.y1=float(node.text)\n                                    image_multi_class.append_rect(rectangle_multi)\n                        if jump == 0:\n                            image_multi_class.append_labeled_rect(rectangle_multi)\n                        break\n                frames_list.append(image_multi_class)\n        video_list.append(frames_list)\n        # frames_list=None\n        # frames_list=[]        \n    return video_list\n\ndef read_xml_files(filename , data_folder):\n    xml_list_video = []\n    video_frame =[]\n    with open(filename) as f:\n        video=None\n        for line in f:\n            file, idx = line.strip().split(\' \')\n            # new_file=\'ILSVRC2016_\'+file.split(\'_\')[1]+\'_\'+file.split(\'_\')[2]\n            # #file.replace(\'ILSVRC2015\',\'ILSVRC2016\')\n            # file=new_file\n            # print \'File name:%s\'%file    \n            if video is None:\n                video_frame.append((data_folder + file + \'.xml\',idx))\n                video=file.split(\'/\')[0]\n            else:\n                if video==file.split(\'/\')[0]:\n                    video_frame.append((data_folder + file + \'.xml\',idx))\n                else :\n                    xml_list_video.append(video_frame)\n                    video_frame = []\n                    video_frame.append((data_folder + file + \'.xml\',idx))\n                    video=file.split(\'/\')[0]\n        xml_list_video.append(video_frame)\n    return xml_list_video\n\ndef val_to_data(source):\n    text_lines=[]\n    frames_list=[]\n    frame = None\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    with open(source, \'r\') as s: \n        for line in s:\n            id_frame, id_class, conf, xmin, ymin, xmax, ymax = line.strip().split(\' \')\n            text_lines.append((id_frame, id_class, conf, xmin, ymin, xmax, ymax))\n    for i in range(0, len(text_lines)):\n        if frame is None:\n            frame = fm.Frame_Info()\n            frame.frame= text_lines[i][0]\n            rect= multiclass_rectangle.Rectangle_Multiclass()\n            # Not all the inserted values are really used\n            rect.load_labeled_rect(0, text_lines[i][2], text_lines[i][2], text_lines[i][3], text_lines[i][4], text_lines[i][5], text_lines[i][6], text_lines[i][1], text_lines[i][1], text_lines[i][1])\n            frame.append_labeled_rect(rect)\n        else :\n            if frame.frame == text_lines[i][0]:\n                rect= multiclass_rectangle.Rectangle_Multiclass()\n                # Not all the inserted values are really used\n                rect.load_labeled_rect(0, text_lines[i][2], text_lines[i][2], text_lines[i][3], text_lines[i][4], text_lines[i][5], text_lines[i][6], text_lines[i][1], text_lines[i][1], text_lines[i][1])\n                frame.append_labeled_rect(rect)\n            else :\n                frames_list.append(frame)\n                frame = fm.Frame_Info()\n                frame.frame= text_lines[i][0]\n                rect= multiclass_rectangle.Rectangle_Multiclass()\n                # Not all the inserted values are really used\n                rect.load_labeled_rect(0, text_lines[i][2], text_lines[i][2], text_lines[i][3], text_lines[i][4], text_lines[i][5], text_lines[i][6], text_lines[i][1], text_lines[i][1], text_lines[i][1])\n                frame.append_labeled_rect(rect)\n    frames_list.append(frame)\n    return frames_list\n\ndef parse_video_to_framelist(video_list):\n\n    frames_list=[]\n    for video in video_list:\n        for frame in video:\n            frames_list.append(frame)\n    return frames_list\n\ndef save_best_overlap(val_bbox, output_bbox):\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    count_best_bbox=0\n    len_val_bbox=len(val_bbox)\n    len_output_bbox=len(output_bbox)\n    count_missing_boxes=0\n    with open(""best_overlap.txt"", \'a\') as d:\n        for i in progress(range(0, len(val_bbox))):\n            for rect in val_bbox[i].rects:\n                if(len(output_bbox[i].rects)>0):\n                    selected=multiclass_rectangle.pop_max_overlap(output_bbox[i].rects,rect)\n                    count_best_bbox=count_best_bbox+1\n                    d.write(str(val_bbox[i].frame)+\' \'+str(rect.label_chall)+ \' 0.5 \'+str(selected.x1)+\' \'+str(selected.y1)+\' \'+str(selected.x2)+\' \'+str(selected.y2) + os.linesep)\n                else:\n                    count_missing_boxes=count_missing_boxes+1\n    print ""Total Frame Number: ""+ str(len_val_bbox) \n    print ""Total Output Bounding Boxes: ""+ str(len_output_bbox) \n    print ""Total Best Bounding Boxes: ""+ str(count_best_bbox) \n    print ""Total Missing Bounding Boxes: ""+ str(count_missing_boxes) \n    print ""Total False Positive Bounding Boxes: ""+ str(len_output_bbox-count_best_bbox) \n    print ""BBox/Frame Number: ""+ str(float(count_best_bbox)/float(len_val_bbox)) \n    print ""Missing BBox/Frame Number: ""+ str(float(float(count_missing_boxes)/float(len_val_bbox)))\n    print ""False Positive BBox/Frame Number: ""+ str(float(float(len_output_bbox-count_best_bbox)/float(len_val_bbox)))\n\ndef save_best_iou(val_bbox, output_bbox):\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    count_best_bbox=0\n    len_val_bbox=len(val_bbox)\n    len_output_bbox=len(output_bbox)\n    count_missing_boxes=0\n    with open(""best_iou.txt"", \'a\') as d:\n        for i in progress(range(0, len(val_bbox))):\n            for rect in val_bbox[i].rects:\n                if(len(output_bbox[i].rects)>0):\n                    selected=multiclass_rectangle.pop_max_iou(output_bbox[i].rects,rect)\n                    count_best_bbox=count_best_bbox+1\n                    d.write(str(val_bbox[i].frame)+\' \'+str(rect.label_chall)+ \' 0.5 \'+str(selected.x1)+\' \'+str(selected.y1)+\' \'+str(selected.x2)+\' \'+str(selected.y2) + os.linesep)\n                else:\n                    count_missing_boxes=count_missing_boxes+1\n    print ""Total Frame Number: ""+ str(len_val_bbox) \n    print ""Total Output Bounding Boxes: ""+ str(len_output_bbox) \n    print ""Total Best Bounding Boxes: ""+ str(count_best_bbox) \n    print ""Total Missing Bounding Boxes: ""+ str(count_missing_boxes) \n    print ""Total False Positive Bounding Boxes: ""+ str(len_output_bbox-count_best_bbox) \n    print ""BBox/Frame Number: ""+ str(float(count_best_bbox)/float(len_val_bbox)) \n    print ""Missing BBox/Frame Number: ""+ str(float(float(count_missing_boxes)/float(len_val_bbox)))\n    print ""False Positive BBox/Frame Number: ""+ str(float(float(len_output_bbox-count_best_bbox)/float(len_val_bbox)))\n\n\ndef main():\n    xml_file_list = read_xml_files(\'val.txt\', \'val/\')\n    parsed_xml=parse_XML_to_data(xml_file_list)\n    parsed_frames=parse_video_to_framelist(parsed_xml)\n    # for xml in parsed_xml:\n    #     for frame in xml:\n    #         print frame\n    val_video_info = val_to_data(\'output.txt\')\n    # print val_video_info\n    save_best_iou(val_video_info,parsed_frames)\n\n\n\nif __name__ == \'__main__\':\n    main()'"
frame.py,0,"b'import copy \nimport os\nimport multiclass_rectangle\nfrom multiclass_rectangle import Rectangle_Multiclass\nimport vid_classes\n\n#####################################################################################################################################################################\n########################## CLASS AND FUNCTIONS DEFINED TO USE WITH INCEPTION EXTENDS THE ONES ABOVE AND HAVE THE SAME SCOPE) ########################################\n#####################################################################################################################################################################\n\n####  LABELED Picture Informations Class\n\nclass Frame_Info(object):\n    ### Here the rects are Labeled_BB not BB_Rectangle\n    def __init__(self):\n\n        self.num_obj=-1\n        self.rects = []\n        self.folder=\'Not Set\'\n        self.filename=\'Not Set\'\n        self.dataset_path=\'Not Set\'\n        self.default_path=\'Not Set\'\n        self.width=-1\n        self.height=-1\n        self.frame=-1\n\n    ### Save Copy functions\n\n    def duplicate(self):\n        \n        new_copy= Frame_Info()\n        new_copy.folder=copy.copy(self.folder)\n        new_copy.filename=copy.copy(self.filename)\n        new_copy.dataset_path=copy.copy(self.dataset_path)\n        new_copy.default_path=copy.copy(self.default_path)\n        new_copy.width=copy.copy(self.width)\n        new_copy.height=copy.copy(self.height)\n        new_copy.frame=copy.copy(self.frame)\n\n        return new_copy\n\n    def duplicate_rects(self, rects):\n        for rect in rects:\n            self.append_labeled_rect(rect.duplicate())\n\n    ### Save insert of rect into the frame rects list\n\n    def append_labeled_rect(self, rectangle):\n        """"""Adding rect to the picture_info.""""""\n        index= len(self.rects)\n        self.rects.insert(index, rectangle)\n\n    def append_rect(self, rectangle):\n        """"""Adding rect to the picture_info.""""""\n        rect= Rectangle_Multiclass()\n        rect.load_BBox( rectangle.x1, rectangle.x2, rectangle.y1, rectangle.y2, rectangle.label, rectangle.label_chall, rectangle.label_code)\n        index= len(self.rects)\n        self.rects.insert(index, rect)\n\n    ### Functions for parse the xml into the .idl file to train TENSORBOX\n\n    def get_rects_string(self):\n        """"""Get the string of the coordinates of all the rects of the image.""""""\n        string=\'""\'+self.dataset_path+\'/\'+self.folder+\'/\'+self.filename+\'""\' \n        if self.frame is not -1:\n           string= string+ \' @\'+ str(self.frame)\n        string=string+\' : \'\n        n_obj=len(self.rects)\n        for rectangle in self.rects:\n            string = string + rectangle.get_rect_string()\n            n_obj=n_obj-1\n            if n_obj>0:\n                string= string + \',\'\n            else: string= string + \';\'\n        return string\n\n    def get_rects_labels(self):\n        """"""Get the string of the coordinates of all the rects of the image.""""""\n        \n        string=\'""\'+self.dataset_path+\'/\'+self.folder+\'/\'+self.filename+\'""\'\n        if self.frame is not -1:\n           string= string+ \' @\'+ str(self.frame)\n        string=string+\' : \'\n        n_obj=len(self.rects)\n        for rectangle in self.rects:\n            string = string + rectangle.get_label_string()\n            n_obj=n_obj-1\n            if n_obj>0:\n                string= string + \',\'\n            else: string= string + \';\'\n        return string\n\n    def get_rects_code(self):\n        """"""Get the string of the coordinates of all the rects of the image.""""""\n        string=\'""\'+self.dataset_path+\'/\'+self.folder+\'/\'+self.filename+\'""\'\n        if self.frame is not -1:\n           string= string+ \' @\'+ str(self.frame)\n        string=string+\' : \'\n        n_obj=len(self.rects)\n        for rectangle in self.rects:\n            string = string + rectangle.get_code_string()\n            n_obj=n_obj-1\n            if n_obj>0:\n                string= string + \',\'\n            else: string= string + \';\'\n        return string\n\n    def get_rects_chall(self):\n        """"""Get the string of the coordinates of all the rects of the image.""""""\n        string=\'""\'+self.dataset_path+\'/\'+self.folder+\'/\'+self.filename+\'""\'\n        if self.frame is not -1:\n           string= string+ \' @\'+ str(self.frame)\n        string=string+\' : \'\n        n_obj=len(self.rects)\n        for rectangle in self.rects:\n            string = string + rectangle.get_chall_string()\n            n_obj=n_obj-1\n            if n_obj>0:\n                string= string + \',\'\n            else: string= string + \';\'\n        return string\n\n    def get_info_string(self):\n        """"""Get the string of the infos of the image.""""""\n        return self.get_rects_string()\n\n### Save frames array function to .idl\n\ndef saveVideoResults(filename, annotations):\n    if not os.path.exists(filename):\n        print ""Created File: ""+ filename\n    file = open(filename, \'w\')\n    for annotation in annotations:\n        frame = -1\n        trackID=-1\n        conf=0\n        silhouette=-1\n        xmin,ymin,xmax,ymax=0,0,0,0\n\n        detections_array=[]\n\n        if annotation.frame is not -1:\n            frame=annotation.frame\n        for rect in annotation.rects:\n            if vid_classes.class_string_to_comp_code(rect.label) is not \'nothing\':\n                silhouette=rect.label\n            if rect.trackID is not -1:\n                trackID=rect.trackID\n            conf = rect.true_confidence\n            xmin,ymin,xmax,ymax = rect.x1,rect.y1,rect.x2 ,rect.y2\n            file.write(str(frame)+\' \'+str(silhouette)+\' \'+str(trackID)+\' \'+str(conf)+\' \'+str(xmin)+\' \'+str(ymin)+\' \'+str(xmax)+\' \'+str(ymax) + os.linesep)\n    file.close()\n'"
multiclass_rectangle.py,0,"b'import copy \nimport utils_video\n\n#####################################################################################################################################################################\n########################## CLASS AND FUNCTIONS DEFINED TO USE WITH TENSORBOX (MAINLY TO PARSE & WRITE .IDL FILE AND MANAGE RESULTS) #################################\n#####################################################################################################################################################################\n\n\nclass Rectangle_Multiclass(object):\n\n\n\n    def __init__(self):\n        # Initialization Function\n        # BBox Parameters\n\n        self.cx = -1\n        self.cy = -1\n        self.width = -1\n        self.height = -1\n        self.true_confidence = -1\n        self.x1 = -1\n        self.x2 = -1\n        self.y1 = -1\n        self.y2 = -1      \n\n        # Track Parameter\n        \n        self.trackID=-1\n\n        # Label Parameters\n\n        self.label_confidence = -1\n        self.label= \'Not Set\'\n        self.label_chall=\'Not Set\'\n        self.label_code= \'Not Set\'\n\n    ### Safe Loading Values Functions\n\n    def load_labeled_rect(self, trackID, rect_conf, label_conf, x1, x2, y1, y2, label, label_chall, code):\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n\n        self.label=label\n        self.label_code=code\n        self.label_chall=label_chall\n\n        self.trackID=trackID\n\n        self.label_confidence=label_conf\n        self.true_confidence=rect_conf\n\n    def load_BBox(self, x1, x2, y1, y2, label, label_chall, code):\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n\n        self.label=label\n        self.label_code=code\n        self.label_chall=label_chall\n\n    def set_unlabeled_rect(self, cx, cy, width, height, confidence):\n        # Set unlabeled rect info to be processed forward\n        self.cx = cx\n        self.cy = cy\n        self.width = width\n        self.height = height\n        self.true_confidence = confidence\n        self.x1 = self.cx - self.width/2.\n        self.x2 = self.cx + self.width/2.\n        self.y1 = self.cy - self.height/2.\n        self.y2 = self.cy + self.height/2.\n\n    def add_delta(self, dx1, dx2, dy1, dy2):\n        # Set unlabeled rect info to be processed forward\n        self.x1 = self.x1+dx1 \n        self.x2 = self.x2+dx2\n        self.y1 = self.y1+dy1\n        self.y2 = self.y2+dy2\n        self.cx = (self.x1 + self.x2)/2.\n        self.cy = (self.y1 + self.y2)/2.\n        self.width = max(self.x1,self.x2) - min(self.x1,self.x2)\n        self.height = max(self.y1,self.y2) - min(self.y1,self.y2)\n\n    def set_rect_coordinates(self, x1, x2, y1, y2):\n        # Set rect coordinates info to be processed forward\n\n        self.x1 = x1 \n        self.x2 = x2\n        self.y1 = y1\n        self.y2 = y2\n        self.cx = (self.x1 + self.x2)/2.\n        self.cy = (self.y1 + self.y2)/2.\n        self.width = max(self.x1,self.x2) - min(self.x1,self.x2)\n        self.height = max(self.y1,self.y2) - min(self.y1,self.y2)\n\n    def load_label(self, trackID, label_conf, label, label_chall, code):\n\n        self.label=label\n        self.label_code=code\n        self.label_chall=label_chall\n        self.trackID=trackID\n        self.label_confidence=label_conf\n\n    def load_trackID(self, trackID):\n\n        self.trackID=trackID\n\n    def set_label(self, label_conf, label, label_chall, code):\n\n        self.label=label\n        self.label_code=code\n        self.label_chall=label_chall\n        self.label_confidence=label_conf\n\n    def check_rects_motion(self,filename, rect, dx1, dx2, dy1,dy2, error=1.2, attenuation=1.1):\n        ## Rect is considered passed befor through add_delta\n        if((self.x1-rect.x1)>dx1*error)| ((self.y1-rect.y1)>dy1*error)|((self.x2-rect.x2)>dx2*error)|((self.y2-rect.y2)>dy2*error):\n            utils_video.draw_rectangle(filename,(self.x1, self.y1,self.x2, self.y2))\n            delta_cx=self.cx-rect.cx\n            delta_cy=self.cy-rect.cy\n            self.x1 =rect.x1 + delta_cx\n            self.y1 =rect.y1 + delta_cy\n            self.x2 =rect.x2 + delta_cx\n            self.y2 =rect.y2 + delta_cy\n            self.cx = (self.x1 + self.x2)/2.\n            self.cy = (self.y1 + self.y2)/2.\n            self.width = max(self.x1,self.x2) - min(self.x1,self.x2)\n            self.height = max(self.y1,self.y2) - min(self.y1,self.y2)\n\n    ### Safe Duplicate functions \n\n    def duplicate(self):\n        new_rect=Rectangle_Multiclass()\n        new_rect.cx = copy.copy(self.cx)\n        new_rect.cy = copy.copy(self.cy)\n        new_rect.width = copy.copy(self.width)\n        new_rect.height = copy.copy(self.height)\n        new_rect.true_confidence = copy.copy(self.true_confidence)\n        new_rect.label_confidence = copy.copy(self.label_confidence)\n        new_rect.label= copy.copy(self.label)\n        new_rect.trackID=copy.copy(self.trackID)\n        new_rect.x1 = copy.copy(self.x1)\n        new_rect.x2 = copy.copy(self.x2)\n        new_rect.y1 = copy.copy(self.y1)\n        new_rect.y2 = copy.copy(self.y2)\n        return new_rect\n\n    ### Computation Functions\n\n    def overlaps(self, other):\n        if abs(self.cx - other.cx) > (self.width + other.width) / 1.5:\n            return False\n        elif abs(self.cy - other.cy) > (self.height + other.height) / 2.0:\n            return False\n        else:\n            return True\n    def distance(self, other):\n        return sum(map(abs, [self.cx - other.cx, self.cy - other.cy,\n                       self.width - other.width, self.height - other.height]))\n    def intersection(self, other):\n        left = max(self.cx - self.width/2., other.cx - other.width/2.)\n        right = min(self.cx + self.width/2., other.cx + other.width/2.)\n        width = max(right - left, 0)\n        top = max(self.cy - self.height/2., other.cy - other.height/2.)\n        bottom = min(self.cy + self.height/2., other.cy + other.height/2.)\n        height = max(bottom - top, 0)\n        return width * height\n    def area(self):\n        return self.height * self.width\n    def union(self, other):\n        return self.area() + other.area() - self.intersection(other)\n    def iou(self, other):\n        return self.intersection(other) / self.union(other)\n    def __eq__(self, other):\n        return (self.cx == other.cx and \n            self.cy == other.cy and\n            self.width == other.width and\n            self.height == other.height and\n            self.confidence == other.confidence and\n            self.label_confidence == other.label_confidence and self.label == other.label and self.trackID == other.trackID)\n\n    ### Functions for parse the xml into the .idl file to train TENSORBOX\n\n    def get_label_string(self):\n        """"""Get the string of the label of the rect.""""""\n        string=""""\n        if self.label is not \'Not Set\':\n            string=self.label +\' \'\n        return string\n\n\n    def get_code_string(self):\n        """"""Get the string of the label of the rect.""""""\n        string=""""\n        if self.label_code is not -1:\n            string=self.label_code +\' \'\n        return string\n\n\n    def get_chall_string(self):\n        """"""Get the string of the label of the rect.""""""\n        string=""""\n        if self.label_chall is not \'Not Set\':\n            string=str(self.label_chall) +\' \'\n        return string\n    \n    def get_coord_string(self):\n        """"""Get the string of the coordinates of the rect.""""""\n        string=\'(\'+str(self.x1)+\',\'+str(self.y1)+\',\'+str(self.x2)+\',\'+str(self.y2)+\')\'\n        return string\n\n    def get_rect_string(self):\n        """"""Get the string of the infos of the rect.""""""\n        string=\'(\'+str(self.x1)+\',\'+str(self.y1)+\',\'+str(self.x2)+\',\'+str(self.y2)+\')\'\n        if self.label_chall is not \'Not Set\':\n            string=string+\' /\' + str(self.label_chall) \n        return string\n\n\ndef duplicate_rects(rects):\n\n    new_rects=[]\n    for rect in rects:\n        new_rect=Rectangle_Multiclass()\n        new_rect.cx = copy.copy(rect.cx)\n        new_rect.cy = copy.copy(rect.cy)\n        new_rect.width = copy.copy(rect.width)\n        new_rect.height = copy.copy(rect.height)\n        new_rect.true_confidence = copy.copy(rect.true_confidence)\n        new_rect.label_confidence = copy.copy(rect.label_confidence)\n        new_rect.label= copy.copy(rect.label)\n        new_rect.trackID=copy.copy(rect.trackID)\n        new_rect.x1 = copy.copy(rect.x1)\n        new_rect.x2 = copy.copy(rect.x2)\n        new_rect.y1 = copy.copy(rect.y1)\n        new_rect.y2 = copy.copy(rect.y2)\n        new_rects.append(new_rect)\n    return new_rects\n\ndef pop_max_iou(rects, rect):\n    max_iou=None\n    max_id=0\n    rect_id=0\n    for rectangle in rects:\n        if max_iou is None:\n            max_iou=rect.iou(rectangle)\n            max_id=rect_id\n        if rect.iou(rectangle)>max_iou:\n            max_iou=rect.iou(rectangle)\n            max_id=rect_id\n        rect_id=rect_id+1\n    if len(rects)>max_id:\n        new_rect=rects[max_id].duplicate()\n        rects.pop(max_id)\n        return new_rect\n    else: return None \n\ndef pop_max_overlap(rects, rect):\n    max_overlap=None\n    max_id=0\n    rect_id=0\n    for rectangle in rects:\n        if max_overlap is None:\n            max_overlap=rect.overlaps(rectangle)\n            max_id=rect_id\n        if rect.iou(rectangle)>max_overlap:\n            max_overlap=rect.overlaps(rectangle)\n            max_id=rect_id\n        rect_id=rect_id+1\n    if len(rects)>max_id:\n        new_rect=rects[max_id].duplicate()\n        rects.pop(max_id)\n        return new_rect\n    else: return None '"
vid_classes.py,0,"b'####DEFINE COLOR SWITCH\n\ndef code_to_class_string(argument):\n    switcher = {\n                    \'n02691156\': ""airplane"",\n                    \'n02419796\': ""antelope"",\n                    \'n02131653\': ""bear"",\n                    \'n02834778\': ""bicycle"",\n                    \'n01503061\': ""bird"",\n                    \'n02924116\': ""bus"",\n                    \'n02958343\': ""car"",\n                    \'n02402425\': ""cattle"",\n                    \'n02084071\': ""dog"",\n                    \'n02121808\': ""domestic_cat"",\n                    \'n02503517\': ""elephant"",\n                    \'n02118333\': ""fox"",\n                    \'n02510455\': ""giant_panda"",\n                    \'n02342885\': ""hamster"",\n                    \'n02374451\': ""horse"",\n                    \'n02129165\': ""lion"",\n                    \'n01674464\': ""lizard"",\n                    \'n02484322\': ""monkey"",\n                    \'n03790512\': ""motorcycle"",\n                    \'n02324045\': ""rabbit"",\n                    \'n02509815\': ""red_panda"",\n                    \'n02411705\': ""sheep"",\n                    \'n01726692\': ""snake"",\n                    \'n02355227\': ""squirrel"",\n                    \'n02129604\': ""tiger"",\n                    \'n04468005\': ""train"",\n                    \'n01662784\': ""turtle"",\n                    \'n04530566\': ""watercraft"",\n                    \'n02062744\': ""whale"",\n                    \'n02391049\': ""zebra""            }\n    return switcher.get(argument, ""nothing"")\n\ndef code_to_code_chall(argument):\n    switcher = {\n                    \'n02691156\': 1,\n                    \'n02419796\': 2,\n                    \'n02131653\': 3,\n                    \'n02834778\': 4,\n                    \'n01503061\': 5,\n                    \'n02924116\': 6,\n                    \'n02958343\': 7,\n                    \'n02402425\': 8,\n                    \'n02084071\': 9,\n                    \'n02121808\': 10,\n                    \'n02503517\': 11,\n                    \'n02118333\': 12,\n                    \'n02510455\': 13,\n                    \'n02342885\': 14,\n                    \'n02374451\': 15,\n                    \'n02129165\': 16,\n                    \'n01674464\': 17,\n                    \'n02484322\': 18,\n                    \'n03790512\': 19,\n                    \'n02324045\': 20,\n                    \'n02509815\': 21,\n                    \'n02411705\': 22,\n                    \'n01726692\': 23,\n                    \'n02355227\': 24,\n                    \'n02129604\': 25,\n                    \'n04468005\': 26,\n                    \'n01662784\': 27,\n                    \'n04530566\': 28,\n                    \'n02062744\': 29,\n                    \'n02391049\': 30            }\n    return switcher.get(argument, ""nothing"")\n\ndef class_string_to_comp_code(argument):\n    switcher = {\n                    \'airplane\': 1,\n                    \'antelope\': 2,\n                    \'bear\': 3,\n                    \'bicycle\': 4,\n                    \'bird\': 5,\n                    \'bus\': 6,\n                    \'car\': 7,\n                    \'cattle\': 8,\n                    \'dog\': 9,\n                    \'domestic_cat\': 10,\n                    \'elephant\': 11,\n                    \'fox\': 12,\n                    \'giant_panda\': 13,\n                    \'hamster\': 14,\n                    \'horse\': 15,\n                    \'lion\': 16,\n                    \'lizard\': 17,\n                    \'monkey\': 18,\n                    \'motorcycle\': 19,\n                    \'rabbit\': 20,\n                    \'red_panda\': 21,\n                    \'sheep\': 22,\n                    \'snake\': 23,\n                    \'squirrel\': 24,\n                    \'tiger\': 25,\n                    \'train\': 26,\n                    \'turtle\': 27,\n                    \'watercraft\': 28,\n                    \'whale\': 29,\n                    \'zebra\': 30                 \n               }\n    return switcher.get(argument, None)\n\ndef code_comp_to_class(argument):\n    switcher = {\n                    1:\'airplane\',\n                    2:\'antelope\',\n                    3:\'bear\',\n                    4:\'bicycle\',\n                    5:\'bird\',\n                    6:\'bus\',\n                    7:\'car\',\n                    8:\'cattle\',\n                    9:\'dog\',\n                    10:\'domestic_cat\',\n                    11:\'elephant\',\n                    12:\'fox\',\n                    13:\'giant_panda\',\n                    14:\'hamster\',\n                    15:\'horse\',\n                    16:\'lion\',\n                    17:\'lizard\',\n                    18:\'monkey\',\n                    19:\'motorcycle\',\n                    20:\'rabbit\',\n                    21:\'red_panda\',\n                    22:\'sheep\',\n                    23:\'snake\',\n                    24:\'squirrel\',\n                    25:\'tiger\',\n                    26:\'train\',\n                    27:\'turtle\',\n                    28:\'watercraft\',\n                    29:\'whale\',\n                    30:\'zebra\'                 }\n    return switcher.get(argument, ""nothing"")\n\n\n\n### Color Switching \n\ndef name_string_to_color(argument):\n    switcher = {\n                    \'airplane\': \'black\' ,\n                    \'antelope\': \'white\',\n                    \'bear\': \'red\',\n                    \'bicycle\': \'lime\' ,\n                    \'bird\': \'blue\',\n                    \'bus\': \'yellow\',\n                    \'car\': \'cyan\',\n                    \'cattle\':  \'magenta\',\n                    \'dog\': \'silver\',\n                    \'domestic_cat\': \'gray\' ,\n                    \'elephant\':\'maroon\' ,\n                    \'fox\':\'olive\' ,\n                    \'giant_panda\':\'green\' ,\n                    \'hamster\':\'purple\' ,\n                    \'horse\':\'teal\' ,\n                    \'lion\':\'navy\' ,\n                    \'lizard\':\'pale violet red\' ,\n                    \'monkey\':\'deep pink\' ,\n                    \'motorcycle\':\'aqua marine\' ,\n                    \'rabbit\':\'powder blue\' ,\n                    \'red_panda\':\'spring green\' ,\n                    \'sheep\':\'sea green\' ,\n                    \'snake\':\'forest green\' ,\n                    \'squirrel\':\'orange red\' ,\n                    \'tiger\':\'dark orange\' ,\n                    \'train\':\'orange\' ,\n                    \'turtle\':\'dark golden rod\' ,\n                    \'watercraft\':\'golden rod\' ,\n                    \'whale\':\'dark red\' ,\n                    \'zebra\':\'light coral\'                  }\n    return switcher.get(argument, ""nothing"")\n\ndef code_to_color(argument):\n    switcher = {\n                    1:(0,0,0),\n                    2:(255,255,255),\n                    3:(255,0,0),\n                    4:(0,255,0),\n                    5:(0,0,255),\n                    6:(255,255,0),\n                    7:(0,255,255),\n                    8:(255,0,255),\n                    9:(192,192,192),\n                    10:(128,128,128),\n                    11:(128,0,0),\n                    12:(128,128,0),\n                    13:(0,128,0),\n                    14:(128,0,128),\n                    15:(0,128,128),\n                    16:(0,0,128),\n                    17:(219,112,147),\n                    18:(255,20,147),\n                    19:(127,255,212),\n                    20:(176,224,230),\n                    21:(0,255,127),\n                    22:(46,139,87),\n                    23:(34,139,34),\n                    24:(255,69,0),\n                    25:(255,140,0),\n                    26:(255,165,0),\n                    27:(184,134,11),\n                    28:(218,165,32),\n                    29:(139,0,0),\n                    30:(240,128,128) \n                      }\n    return switcher.get(argument,(0,0,0) )\n\ndef label_to_color(argument):\n    switcher = {\n                    \'n02691156\':(0,0,0),\n                    \'n02419796\':(255,255,255),\n                    \'n02131653\':(255,0,0),\n                    \'n02834778\':(0,255,0),\n                    \'n01503061\':(0,0,255),\n                    \'n02924116\':(255,255,0),\n                    \'n02958343\':(0,255,255),\n                    \'n02402425\':(255,0,255),\n                    \'n02084071\':(192,192,192),\n                    \'n02121808\':(128,128,128),\n                    \'n02503517\':(128,0,0),\n                    \'n02118333\':(128,128,0),\n                    \'n02510455\':(0,128,0),\n                    \'n02342885\':(128,0,128),\n                    \'n02374451\':(0,128,128),\n                    \'n02129165\':(0,0,128),\n                    \'n01674464\':(219,112,147),\n                    \'n02484322\':(255,20,147),\n                    \'n03790512\':(127,255,212),\n                    \'n02324045\':(176,224,230),\n                    \'n02509815\':(0,255,127),\n                    \'n02411705\':(46,139,87),\n                    \'n01726692\':(34,139,34),\n                    \'n02355227\':(255,69,0),\n                    \'n02129604\':(255,140,0),\n                    \'n04468005\':(255,165,0),\n                    \'n01662784\':(184,134,11),\n                    \'n04530566\':(218,165,32),\n                    \'n02062744\':(139,0,0),\n                    \'n02391049\':(240,128,128) \n                      }\n    return switcher.get(argument,(0,0,0) )\n\n\n ####### COLOR LEGEND #######\n #\t\tBlack \t#000000 \t(0,0,0)\n #  \tWhite \t#FFFFFF \t(255,255,255)\n #  \tRed \t#FF0000 \t(255,0,0)\n #  \tLime \t#00FF00 \t(0,255,0)\n #  \tBlue \t#0000FF \t(0,0,255)\n #  \tYellow \t#FFFF00 \t(255,255,0)\n #  \tCyan / Aqua \t#00FFFF \t(0,255,255)\n #  \tMagenta / Fuchsia \t#FF00FF \t(255,0,255)\n #  \tSilver \t#C0C0C0 \t(192,192,192)\n #  \tGray \t#808080 \t(128,128,128)\n #  \tMaroon \t#800000 \t(128,0,0)\n #  \tOlive \t#808000 \t(128,128,0)\n #  \tGreen \t#008000 \t(0,128,0)\n #  \tPurple \t#800080 \t(128,0,128)\n #  \tTeal \t#008080 \t(0,128,128)\n #  \tNavy \t#000080 \t(0,0,128)\n #  \tpale violet red \t#DB7093 \t(219,112,147)\n #  \tdeep pink \t#FF1493 \t(255,20,147)\n #  \taqua marine \t#7FFFD4 \t(127,255,212)\n #  \tpowder blue \t#B0E0E6 \t(176,224,230)\n #  \tspring green \t#00FF7F \t(0,255,127)\n #  \tsea green \t#2E8B57 \t(46,139,87)\n #  \tforest green \t#228B22 \t(34,139,34)\n #  \tlime \t#00FF00 \t(0,255,0)\n #  \torange red \t#FF4500 \t(255,69,0)\n #  \tdark orange \t#FF8C00 \t(255,140,0)\n #  \torange \t#FFA500 \t(255,165,0)\n #  \tdark golden rod \t#B8860B \t(184,134,11)\n #  \tgolden rod \t#DAA520 \t(218,165,32)\n #  \tdark red \t#8B0000 \t(139,0,0)\n #  \tlight coral \t#F08080 \t(240,128,128)\n\nclass Classes_List(object):\n        \n     class_name_string_list= [\'airplane\',\'antelope\',\'bear\',\'bicycle\',\'bird\',\'bus\',\'car\',\'cattle\',\'dog\',\'domestic_cat\',\'elephant\',\'fox\',\'giant_panda\',\'hamster\',\'horse\',\'lion\',\'lizard\',\'monkey\',\'motorcycle\',\'rabbit\',\'red_panda\',\'sheep\',\'snake\',\'squirrel\',\'tiger\',\'train\',\'turtle\',\'watercraft\',\'whale\',\'zebra\']\n\n     class_code_string_list= [\'n02691156\',\'n02419796\',\'n02131653\',\'n02834778\',\'n01503061\',\'n02924116\',\'n02958343\',\'n02402425\',\'n02084071\',\'n02121808\',\'n02503517\',\'n02118333\',\'n02510455\',\'n02342885\',\'n02374451\',\'n02129165\',\'n01674464\',\'n02484322\',\'n03790512\',\'n02324045\',\'n02509815\',\'n02411705\',\'n01726692\',\'n02355227\',\'n02129604\',\'n04468005\',\'n01662784\',\'n04530566\',\'n02062744\',\'n02391049\']\n\n     colors_string_list=[\'black\' ,\'white\',\'red\',\'lime\' ,\'blue\',\'yellow\',\'cyan\', \'magenta\',\'silver\', \'gray\' ,\'maroon\' ,\'olive\' ,\'green\' ,\'purple\' ,\'teal\' ,\'navy\' ,\'pale violet red\' ,\'deep pink\' ,\'aqua marine\' ,\'powder blue\' ,\'spring green\' ,\'sea green\' ,\'forest green\' ,\'orange red\' ,\'dark orange\' ,\'orange\' ,\'dark golden rod\' ,\'golden rod\' ,\'dark red\' ,\'light coral\'  ]\n\n     colors_code_list=[(0,0,0),(255,255,255),(255,0,0),(0,255,0),(0,0,255),(255,255,0),(0,255,255),(255,0,255),(192,192,192),(128,128,128),(128,0,0),(128,128,0),(0,128,0),(128,0,128),(0,128,128),(0,0,128),(219,112,147),(255,20,147),(127,255,212),(176,224,230),(0,255,127),(46,139,87),(34,139,34),(255,69,0),(255,140,0),(255,165,0),(184,134,11),(218,165,32),(139,0,0),(240,128,128)]\n'"
TENSORBOX/train.py,124,"b'#!/usr/bin/env python\nimport json\nimport datetime\nimport random\nimport time\nimport string\nimport argparse\nimport os\nfrom scipy import misc\nimport tensorflow as tf\nimport numpy as np\ntry:\n    from tensorflow.models.rnn import rnn_cell\nexcept ImportError:\n    rnn_cell = tf.nn.rnn_cell\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\n\nif(int(tf.__version__.split(""."")[0])<1): ### For tf v<1.0\n    merge_all_summaries,histogram_summary = tf.merge_all_summaries,tf.histogram_summary\n    image_summary,scalar_summary = tf.image_summary,tf.scalar_summary\nelse:\n    merge_all_summaries,histogram_summary = tf.summary.merge_all,tf.summary.histogram\n    image_summary,scalar_summary = tf.summary.image , tf.summary.scalar\n\nrandom.seed(0)\nnp.random.seed(0)\n\nfrom utils import train_utils, googlenet_load\n\n@ops.RegisterGradient(""Hungarian"")\ndef _hungarian_grad(op, *args):\n    return map(array_ops.zeros_like, op.inputs)\n\ndef build_lstm_inner(H, lstm_input):\n    \'\'\'\n    build lstm decoder\n    \'\'\'\n    lstm_cell = rnn_cell.BasicLSTMCell(H[\'lstm_size\'], forget_bias=0.0)\n    if H[\'num_lstm_layers\'] > 1:\n        lstm = rnn_cell.MultiRNNCell([lstm_cell] * H[\'num_lstm_layers\'])\n    else:\n        lstm = lstm_cell\n\n    batch_size = H[\'batch_size\'] * H[\'grid_height\'] * H[\'grid_width\']\n    state = tf.zeros([batch_size, lstm.state_size])\n\n    outputs = []\n    with tf.variable_scope(\'RNN\', initializer=tf.random_uniform_initializer(-0.1, 0.1)):\n        for time_step in range(H[\'rnn_len\']):\n            if time_step > 0: tf.get_variable_scope().reuse_variables()\n            output, state = lstm(lstm_input, state)\n            outputs.append(output)\n    return outputs\n\ndef build_overfeat_inner(H, lstm_input):\n    \'\'\'\n    build simple overfeat decoder\n    \'\'\'\n    if H[\'rnn_len\'] > 1:\n        raise ValueError(\'rnn_len > 1 only supported with use_lstm == True\')\n    outputs = []\n    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n    with tf.variable_scope(\'Overfeat\', initializer=initializer):\n        w = tf.get_variable(\'ip\', shape=[1024, H[\'lstm_size\']])\n        outputs.append(tf.matmul(lstm_input, w))\n    return outputs\n\ndef deconv(x, output_shape, channels):\n    k_h = 2\n    k_w = 2\n    w = tf.get_variable(\'w_deconv\', initializer=tf.random_normal_initializer(stddev=0.01),\n                        shape=[k_h, k_w, channels[1], channels[0]])\n    y = tf.nn.conv2d_transpose(x, w, output_shape, strides=[1, k_h, k_w, 1], padding=\'VALID\')\n    return y\n\ndef rezoom(H, pred_boxes, early_feat, early_feat_channels, w_offsets, h_offsets):\n    \'\'\'\n    Rezoom into a feature map at multiple interpolation points in a grid. \n\n    If the predicted object center is at X, len(w_offsets) == 3, and len(h_offsets) == 5,\n    the rezoom grid will look as follows:\n\n    [o o o]\n    [o o o]\n    [o X o]\n    [o o o]\n    [o o o]\n\n    Where each letter indexes into the feature map with bilinear interpolation\n    \'\'\'\n\n\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n    indices = []\n    for w_offset in w_offsets:\n        for h_offset in h_offsets:\n            indices.append(train_utils.bilinear_select(H,\n                                                       pred_boxes,\n                                                       early_feat,\n                                                       early_feat_channels,\n                                                       w_offset, h_offset))\n\n    interp_indices = tf.concat(0, indices)\n    rezoom_features = train_utils.interp(early_feat,\n                                         interp_indices,\n                                         early_feat_channels)\n    rezoom_features_r = tf.reshape(rezoom_features,\n                                   [len(w_offsets) * len(h_offsets),\n                                    outer_size,\n                                    H[\'rnn_len\'],\n                                    early_feat_channels])\n    rezoom_features_t = tf.transpose(rezoom_features_r, [1, 2, 0, 3])\n    return tf.reshape(rezoom_features_t,\n                      [outer_size,\n                       H[\'rnn_len\'],\n                       len(w_offsets) * len(h_offsets) * early_feat_channels])\n\ndef build_forward(H, x, googlenet, phase, reuse):\n    \'\'\'\n    Construct the forward model\n    \'\'\'\n\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n    input_mean = 117.\n    x -= input_mean\n    cnn, early_feat, _ = googlenet_load.model(x, googlenet, H)\n    early_feat_channels = H[\'early_feat_channels\']\n    early_feat = early_feat[:, :, :, :early_feat_channels]\n    \n    if H[\'deconv\']:\n        size = 3\n        stride = 2\n        pool_size = 5\n\n        with tf.variable_scope(""deconv"", reuse=reuse):\n            w = tf.get_variable(\'conv_pool_w\', shape=[size, size, 1024, 1024],\n                                initializer=tf.random_normal_initializer(stddev=0.01))\n            cnn_s = tf.nn.conv2d(cnn, w, strides=[1, stride, stride, 1], padding=\'SAME\')\n            cnn_s_pool = tf.nn.avg_pool(cnn_s[:, :, :, :256], ksize=[1, pool_size, pool_size, 1],\n                                        strides=[1, 1, 1, 1], padding=\'SAME\')\n\n            cnn_s_with_pool = tf.concat(3, [cnn_s_pool, cnn_s[:, :, :, 256:]])\n            cnn_deconv = deconv(cnn_s_with_pool, output_shape=[H[\'batch_size\'], H[\'grid_height\'], H[\'grid_width\'], 256], channels=[1024, 256])\n            cnn = tf.concat(3, (cnn_deconv, cnn[:, :, :, 256:]))\n\n    elif H[\'avg_pool_size\'] > 1:\n        pool_size = H[\'avg_pool_size\']\n        cnn1 = cnn[:, :, :, :700]\n        cnn2 = cnn[:, :, :, 700:]\n        cnn2 = tf.nn.avg_pool(cnn2, ksize=[1, pool_size, pool_size, 1],\n                              strides=[1, 1, 1, 1], padding=\'SAME\')\n        cnn = tf.concat(3, [cnn1, cnn2])\n\n    cnn = tf.reshape(cnn,\n                     [H[\'batch_size\'] * H[\'grid_width\'] * H[\'grid_height\'], 1024])\n    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n    with tf.variable_scope(\'decoder\', reuse=reuse, initializer=initializer):\n        scale_down = 0.01\n        lstm_input = tf.reshape(cnn * scale_down, (H[\'batch_size\'] * grid_size, 1024))\n        if H[\'use_lstm\']:\n            lstm_outputs = build_lstm_inner(H, lstm_input)\n        else:\n            lstm_outputs = build_overfeat_inner(H, lstm_input)\n\n        pred_boxes = []\n        pred_logits = []\n        for k in range(H[\'rnn_len\']):\n            output = lstm_outputs[k]\n            if phase == \'train\':\n                output = tf.nn.dropout(output, 0.5)\n            box_weights = tf.get_variable(\'box_ip%d\' % k,\n                                          shape=(H[\'lstm_size\'], 4))\n            conf_weights = tf.get_variable(\'conf_ip%d\' % k,\n                                           shape=(H[\'lstm_size\'], H[\'num_classes\']))\n\n            pred_boxes_step = tf.reshape(tf.matmul(output, box_weights) * 50,\n                                         [outer_size, 1, 4])\n\n            pred_boxes.append(pred_boxes_step)\n            pred_logits.append(tf.reshape(tf.matmul(output, conf_weights),\n                                         [outer_size, 1, H[\'num_classes\']]))\n \n        pred_boxes = tf.concat(1, pred_boxes)\n        pred_logits = tf.concat(1, pred_logits)\n        pred_logits_squash = tf.reshape(pred_logits,\n                                        [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n        pred_confidences_squash = tf.nn.softmax(pred_logits_squash)\n        pred_confidences = tf.reshape(pred_confidences_squash,\n                                      [outer_size, H[\'rnn_len\'], H[\'num_classes\']])\n\n        if H[\'use_rezoom\']:\n            pred_confs_deltas = []\n            pred_boxes_deltas = []\n            w_offsets = H[\'rezoom_w_coords\']\n            h_offsets = H[\'rezoom_h_coords\']\n            num_offsets = len(w_offsets) * len(h_offsets)\n            rezoom_features = rezoom(H, pred_boxes, early_feat, early_feat_channels, w_offsets, h_offsets)\n            if phase == \'train\':\n                rezoom_features = tf.nn.dropout(rezoom_features, 0.5)\n            for k in range(H[\'rnn_len\']):\n                delta_features = tf.concat(1, [lstm_outputs[k], rezoom_features[:, k, :] / 1000.])\n                dim = 128\n                delta_weights1 = tf.get_variable(\n                                    \'delta_ip1%d\' % k,\n                                    shape=[H[\'lstm_size\'] + early_feat_channels * num_offsets, dim])\n                # TODO: add dropout here ?\n                ip1 = tf.nn.relu(tf.matmul(delta_features, delta_weights1))\n                if phase == \'train\':\n                    ip1 = tf.nn.dropout(ip1, 0.5)\n                delta_confs_weights = tf.get_variable(\n                                    \'delta_ip2%d\' % k,\n                                    shape=[dim, H[\'num_classes\']])\n                if H[\'reregress\']:\n                    delta_boxes_weights = tf.get_variable(\n                                        \'delta_ip_boxes%d\' % k,\n                                        shape=[dim, 4])\n                    pred_boxes_deltas.append(tf.reshape(tf.matmul(ip1, delta_boxes_weights) * 5,\n                                                        [outer_size, 1, 4]))\n                scale = H.get(\'rezoom_conf_scale\', 50) \n                pred_confs_deltas.append(tf.reshape(tf.matmul(ip1, delta_confs_weights) * scale,\n                                                    [outer_size, 1, H[\'num_classes\']]))\n            pred_confs_deltas = tf.concat(1, pred_confs_deltas)\n            if H[\'reregress\']:\n                pred_boxes_deltas = tf.concat(1, pred_boxes_deltas)\n            return pred_boxes, pred_logits, pred_confidences, pred_confs_deltas, pred_boxes_deltas\n\n    return pred_boxes, pred_logits, pred_confidences\n\ndef build_forward_backward(H, x, googlenet, phase, boxes, flags):\n    \'\'\'\n    Call build_forward() and then setup the loss functions\n    \'\'\'\n\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n    reuse = {\'train\': None, \'test\': True}[phase]\n    if H[\'use_rezoom\']:\n        (pred_boxes, pred_logits,\n         pred_confidences, pred_confs_deltas, pred_boxes_deltas) = build_forward(H, x, googlenet, phase, reuse)\n    else:\n        pred_boxes, pred_logits, pred_confidences = build_forward(H, x, googlenet, phase, reuse)\n    with tf.variable_scope(\'decoder\', reuse={\'train\': None, \'test\': True}[phase]):\n        outer_boxes = tf.reshape(boxes, [outer_size, H[\'rnn_len\'], 4])\n        outer_flags = tf.cast(tf.reshape(flags, [outer_size, H[\'rnn_len\']]), \'int32\')\n        if H[\'use_lstm\']:\n            assignments, classes, perm_truth, pred_mask = (\n                tf.user_ops.hungarian(pred_boxes, outer_boxes, outer_flags, H[\'solver\'][\'hungarian_iou\']))\n        else:\n            classes = tf.reshape(flags, (outer_size, 1))\n            perm_truth = tf.reshape(outer_boxes, (outer_size, 1, 4))\n            pred_mask = tf.reshape(tf.cast(tf.greater(classes, 0), \'float32\'), (outer_size, 1, 1))\n        true_classes = tf.reshape(tf.cast(tf.greater(classes, 0), \'int64\'),\n                                  [outer_size * H[\'rnn_len\']])\n        pred_logit_r = tf.reshape(pred_logits,\n                                  [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n        confidences_loss = (tf.reduce_sum(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(pred_logit_r, true_classes))\n            ) / outer_size * H[\'solver\'][\'head_weights\'][0]\n        residual = tf.reshape(perm_truth - pred_boxes * pred_mask,\n                              [outer_size, H[\'rnn_len\'], 4])\n        boxes_loss = tf.reduce_sum(tf.abs(residual)) / outer_size * H[\'solver\'][\'head_weights\'][1]\n        if H[\'use_rezoom\']:\n            if H[\'rezoom_change_loss\'] == \'center\':\n                error = (perm_truth[:, :, 0:2] - pred_boxes[:, :, 0:2]) / tf.maximum(perm_truth[:, :, 2:4], 1.)\n                square_error = tf.reduce_sum(tf.square(error), 2)\n                if(int(tf.__version__.split(""."")[1])<13 and int(tf.__version__.split(""."")[0])<2): ### for tf version < 1.13\n                    inside = tf.reshape(tf.to_int64(tf.logical_and(tf.less(square_error, 0.2**2), tf.greater(classes, 0))), [-1])\n                else: ### for tf version >= 1.13\n                    inside = tf.reshape(tf.cast(tf.logical_and(tf.less(square_error, 0.2**2), tf.greater(classes, 0)),tf.int64), [-1])\n            elif H[\'rezoom_change_loss\'] == \'iou\':\n                iou = train_utils.iou(train_utils.to_x1y1x2y2(tf.reshape(pred_boxes, [-1, 4])),\n                                      train_utils.to_x1y1x2y2(tf.reshape(perm_truth, [-1, 4])))\n                if(int(tf.__version__.split(""."")[1])<13 and int(tf.__version__.split(""."")[0])<2): ### for tf version < 1.13\n                    inside = tf.reshape(tf.to_int64(tf.greater(iou, 0.5)), [-1])\n                else:\n                    inside = tf.reshape(tf.cast(tf.greater(iou, 0.5),tf.int64), [-1]) \n            else:\n                assert H[\'rezoom_change_loss\'] == False\n                if(int(tf.__version__.split(""."")[1])<13 and int(tf.__version__.split(""."")[0])<2): ### for tf version < 1.13\n                    inside = tf.reshape(tf.to_int64((tf.greater(classes, 0))), [-1])\n                else:\n                    inside = tf.reshape(tf.cast((tf.greater(classes, 0)),tf.int64), [-1])\n \n            new_confs = tf.reshape(pred_confs_deltas, [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n            delta_confs_loss = tf.reduce_sum(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(new_confs, inside)) / outer_size * H[\'solver\'][\'head_weights\'][0] * 0.1\n\n            pred_logits_squash = tf.reshape(new_confs,\n                                            [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n            pred_confidences_squash = tf.nn.softmax(pred_logits_squash)\n            pred_confidences = tf.reshape(pred_confidences_squash,\n                                      [outer_size, H[\'rnn_len\'], H[\'num_classes\']])\n            loss = confidences_loss + boxes_loss + delta_confs_loss\n            if H[\'reregress\']:\n                delta_residual = tf.reshape(perm_truth - (pred_boxes + pred_boxes_deltas) * pred_mask,\n                                            [outer_size, H[\'rnn_len\'], 4])\n                delta_boxes_loss = (tf.reduce_sum(tf.minimum(tf.square(delta_residual), 10. ** 2)) / \n                               outer_size * H[\'solver\'][\'head_weights\'][1] * 0.03)\n                boxes_loss = delta_boxes_loss\n\n                histogram_summary(phase + \'/delta_hist0_x\', pred_boxes_deltas[:, 0, 0])\n                histogram_summary(phase + \'/delta_hist0_y\', pred_boxes_deltas[:, 0, 1])\n                histogram_summary(phase + \'/delta_hist0_w\', pred_boxes_deltas[:, 0, 2])\n                histogram_summary(phase + \'/delta_hist0_h\', pred_boxes_deltas[:, 0, 3])\n                loss += delta_boxes_loss\n        else:\n            loss = confidences_loss + boxes_loss\n\n    return pred_boxes, pred_confidences, loss, confidences_loss, boxes_loss\n\ndef build(H, q):\n    \'\'\'\n    Build full model for training, including forward / backward passes,\n    optimizers, and summary statistics.\n    \'\'\'\n    arch = H\n    solver = H[""solver""]\n\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(solver[\'gpu\'])\n\n    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n    gpu_options = tf.GPUOptions()\n    config = tf.ConfigProto(gpu_options=gpu_options)\n\n    encoder_net = googlenet_load.init(H, config)\n\n    learning_rate = tf.placeholder(tf.float32)\n    if solver[\'opt\'] == \'RMS\':\n        opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n                                        decay=0.9, epsilon=solver[\'epsilon\'])\n    elif solver[\'opt\'] == \'Adam\':\n        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n                                        epsilon=solver[\'epsilon\'])\n    elif solver[\'opt\'] == \'SGD\':\n        opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    else:\n        raise ValueError(\'Unrecognized opt type\')\n    loss, accuracy, confidences_loss, boxes_loss = {}, {}, {}, {}\n    for phase in [\'train\', \'test\']:\n        # generate predictions and losses from forward pass\n        x, confidences, boxes = q[phase].dequeue_many(arch[\'batch_size\'])\n        flags = tf.argmax(confidences, 3)\n\n\n        grid_size = H[\'grid_width\'] * H[\'grid_height\']\n\n        (pred_boxes, pred_confidences,\n         loss[phase], confidences_loss[phase],\n         boxes_loss[phase]) = build_forward_backward(H, x, encoder_net, phase, boxes, flags)\n        pred_confidences_r = tf.reshape(pred_confidences, [H[\'batch_size\'], grid_size, H[\'rnn_len\'], arch[\'num_classes\']])\n        pred_boxes_r = tf.reshape(pred_boxes, [H[\'batch_size\'], grid_size, H[\'rnn_len\'], 4])\n\n\n        # Set up summary operations for tensorboard\n        a = tf.equal(tf.argmax(confidences[:, :, 0, :], 2), tf.argmax(pred_confidences_r[:, :, 0, :], 2))\n        accuracy[phase] = tf.reduce_mean(tf.cast(a, \'float32\'), name=phase+\'/accuracy\')\n\n        if phase == \'train\':\n            global_step = tf.Variable(0, trainable=False)\n\n            tvars = tf.trainable_variables()\n            if H[\'clip_norm\'] <= 0:\n                grads = tf.gradients(loss[\'train\'], tvars)\n            else:\n                grads, norm = tf.clip_by_global_norm(tf.gradients(loss[\'train\'], tvars), H[\'clip_norm\'])\n            train_op = opt.apply_gradients(zip(grads, tvars), global_step=global_step)\n        elif phase == \'test\':\n            moving_avg = tf.train.ExponentialMovingAverage(0.95)\n            smooth_op = moving_avg.apply([accuracy[\'train\'], accuracy[\'test\'],\n                                          confidences_loss[\'train\'], boxes_loss[\'train\'],\n                                          confidences_loss[\'test\'], boxes_loss[\'test\'],\n                                          ])\n\n            for p in [\'train\', \'test\']:\n                scalar_summary(\'%s/accuracy\' % p, accuracy[p])\n                scalar_summary(\'%s/accuracy/smooth\' % p, moving_avg.average(accuracy[p]))\n                scalar_summary(""%s/confidences_loss"" % p, confidences_loss[p])\n                scalar_summary(""%s/confidences_loss/smooth"" % p,\n                    moving_avg.average(confidences_loss[p]))\n                scalar_summary(""%s/regression_loss"" % p, boxes_loss[p])\n                scalar_summary(""%s/regression_loss/smooth"" % p,\n                    moving_avg.average(boxes_loss[p]))\n\n        if phase == \'test\':\n            test_image = x\n            # show ground truth to verify labels are correct\n            test_true_confidences = confidences[0, :, :, :]\n            test_true_boxes = boxes[0, :, :, :]\n\n            # show predictions to visualize training progress\n            test_pred_confidences = pred_confidences_r[0, :, :, :]\n            test_pred_boxes = pred_boxes_r[0, :, :, :]\n\n            def log_image(np_img, np_confidences, np_boxes, np_global_step, pred_or_true):\n                \n                merged = train_utils.add_rectangles(H, np_img, np_confidences, np_boxes,\n                                                    use_stitching=True,\n                                                    rnn_len=H[\'rnn_len\'])[0]\n                \n                num_images = 10\n                img_path = os.path.join(H[\'save_dir\'], \'%s_%s.jpg\' % ((np_global_step / H[\'logging\'][\'display_iter\']) % num_images, pred_or_true))\n                misc.imsave(img_path, merged)\n                return merged\n\n            pred_log_img = tf.py_func(log_image,\n                                      [test_image, test_pred_confidences, test_pred_boxes, global_step, \'pred\'],\n                                      [tf.float32])\n            true_log_img = tf.py_func(log_image,\n                                      [test_image, test_true_confidences, test_true_boxes, global_step, \'true\'],\n                                      [tf.float32])\n            image_summary(phase + \'/pred_boxes\', tf.pack(pred_log_img),max_images=10)\n            image_summary(phase + \'/true_boxes\', tf.pack(true_log_img),max_images=10)\n\n    summary_op = merge_all_summaries()\n\n    return (config, loss, accuracy, summary_op, train_op,\n            smooth_op, global_step, learning_rate, encoder_net)\n\n\ndef train(H, test_images):\n    \'\'\'\n    Setup computation graph, run 2 prefetch data threads, and then run the main loop\n    \'\'\'\n\n    if not os.path.exists(H[\'save_dir\']): os.makedirs(H[\'save_dir\'])\n\n    ckpt_file = H[\'save_dir\'] + \'/save.ckpt\'\n    with open(H[\'save_dir\'] + \'/hypes.json\', \'w\') as f:\n        json.dump(H, f, indent=4)\n\n    x_in = tf.placeholder(tf.float32)\n    confs_in = tf.placeholder(tf.float32)\n    boxes_in = tf.placeholder(tf.float32)\n    q = {}\n    enqueue_op = {}\n    for phase in [\'train\', \'test\']:\n        dtypes = [tf.float32, tf.float32, tf.float32]\n        grid_size = H[\'grid_width\'] * H[\'grid_height\']\n        shapes = (\n            [H[\'image_height\'], H[\'image_width\'], 3],\n            [grid_size, H[\'rnn_len\'], H[\'num_classes\']],\n            [grid_size, H[\'rnn_len\'], 4],\n            )\n        q[phase] = tf.FIFOQueue(capacity=30, dtypes=dtypes, shapes=shapes)\n        enqueue_op[phase] = q[phase].enqueue((x_in, confs_in, boxes_in))\n\n    def make_feed(d):\n        return {x_in: d[\'image\'], confs_in: d[\'confs\'], boxes_in: d[\'boxes\'],\n                learning_rate: H[\'solver\'][\'learning_rate\']}\n\n    def thread_loop(sess, enqueue_op, phase, gen):\n        for d in gen:\n            sess.run(enqueue_op[phase], feed_dict=make_feed(d))\n\n    (config, loss, accuracy, summary_op, train_op,\n     smooth_op, global_step, learning_rate, encoder_net) = build(H, q)\n\n    saver = tf.train.Saver(max_to_keep=None)\n    writer = tf.train.SummaryWriter(\n        logdir=H[\'save_dir\'],\n        flush_secs=10\n    )\n\n    with tf.Session(config=config) as sess:\n        tf.train.start_queue_runners(sess=sess)\n        for phase in [\'train\', \'test\']:\n            # enqueue once manually to avoid thread start delay\n            gen = train_utils.load_data_gen(H, phase, jitter=H[\'solver\'][\'use_jitter\'])\n            d = gen.next()\n            sess.run(enqueue_op[phase], feed_dict=make_feed(d))\n            t = tf.train.threading.Thread(target=thread_loop,\n                                          args=(sess, enqueue_op, phase, gen))\n            t.daemon = True\n            t.start()\n\n        tf.set_random_seed(H[\'solver\'][\'rnd_seed\'])\n        if(int(tf.__version__.split(""."")[0])==0 and int(tf.__version__.split(""."")[1])<12): ### for tf v<0.12.0\n            sess.run(tf.initialize_all_variables())\n        else:\n            sess.run(tf.global_variables_initializer())\n        writer.add_graph(sess.graph)\n        weights_str = H[\'solver\'][\'weights\']\n        if len(weights_str) > 0:\n            print(\'Restoring from: %s\' % weights_str)\n            saver.restore(sess, weights_str)\n\n        # train model for N iterations\n        start = time.time()\n        max_iter = H[\'solver\'].get(\'max_iter\', 10000000)\n        for i in xrange(max_iter):\n            display_iter = H[\'logging\'][\'display_iter\']\n            adjusted_lr = (H[\'solver\'][\'learning_rate\'] *\n                           0.5 ** max(0, (i / H[\'solver\'][\'learning_rate_step\']) - 2))\n            lr_feed = {learning_rate: adjusted_lr}\n\n            if i % display_iter != 0:\n                # train network\n                batch_loss_train, _ = sess.run([loss[\'train\'], train_op], feed_dict=lr_feed)\n            else:\n                # test network every N iterations; log additional info\n                if i > 0:\n                    dt = (time.time() - start) / (H[\'batch_size\'] * display_iter)\n                start = time.time()\n                (train_loss, test_accuracy, summary_str,\n                    _, _) = sess.run([loss[\'train\'], accuracy[\'test\'],\n                                      summary_op, train_op, smooth_op,\n                                     ], feed_dict=lr_feed)\n                writer.add_summary(summary_str, global_step=global_step.eval())\n                print_str = string.join([\n                    \'Step: %d\',\n                    \'lr: %f\',\n                    \'Train Loss: %.2f\',\n                    \'Test Accuracy: %.1f%%\',\n                    \'Time/image (ms): %.1f\'\n                ], \', \')\n                print(print_str %\n                      (i, adjusted_lr, train_loss,\n                       test_accuracy * 100, dt * 1000 if i > 0 else 0))\n\n            if global_step.eval() % H[\'logging\'][\'save_iter\'] == 0 or global_step.eval() == max_iter - 1:\n                saver.save(sess, ckpt_file, global_step=global_step)\n\n\ndef main():\n    \'\'\'\n    Parse command line arguments and return the hyperparameter dictionary H.\n    H first loads the --hypes hypes.json file and is further updated with\n    additional arguments as needed.\n    \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--weights\', default=None, type=str)\n    parser.add_argument(\'--gpu\', default=None, type=int)\n    parser.add_argument(\'--hypes\', required=True, type=str)\n    parser.add_argument(\'--logdir\', default=\'output\', type=str)\n    args = parser.parse_args()\n    with open(args.hypes, \'r\') as f:\n        H = json.load(f)\n    if args.gpu is not None:\n        H[\'solver\'][\'gpu\'] = args.gpu\n    if len(H.get(\'exp_name\', \'\')) == 0:\n        H[\'exp_name\'] = args.hypes.split(\'/\')[-1].replace(\'.json\', \'\')\n    H[\'save_dir\'] = args.logdir + \'/%s_%s\' % (H[\'exp_name\'],\n        datetime.datetime.now().strftime(\'%Y_%m_%d_%H.%M\'))\n    if args.weights is not None:\n        H[\'solver\'][\'weights\'] = args.weights\n    train(H, test_images=[])\n\nif __name__ == \'__main__\':\n    main()\n'"
YOLO/YOLO_small_tf.py,20,"b'import numpy as np\nimport tensorflow as tf\nimport cv2\nimport time\nimport sys\n\nclass YOLO_TF:\n\tfromfile = None\n\ttofile_img = \'test/output.jpg\'\n\ttofile_txt = \'test/output.txt\'\n\timshow = True\n\tfilewrite_img = False\n\tfilewrite_txt = False\n\tdisp_console = True\n\tweights_file = \'YOLO_DET_Alg/weights/YOLO_small.ckpt\'\n\talpha = 0.1\n\tthreshold = 0.2\n\tiou_threshold = 0.5\n\tnum_class = 20\n\tnum_box = 2\n\tgrid_size = 7\n\tclasses =  [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"",""tvmonitor""]\n\n\tw_img = 640\n\th_img = 480\n\n\tdef __init__(self,argvs = []):\n\t\tself.argv_parser(argvs)\n\t\tself.build_networks()\n\t\tif self.fromfile is not None: self.detect_from_file(self.fromfile)\n\tdef argv_parser(self,argvs):\n\t\tfor i in range(1,len(argvs),2):\n\t\t\tif argvs[i] == \'-fromfile\' : self.fromfile = argvs[i+1]\n\t\t\tif argvs[i] == \'-tofile_img\' : self.tofile_img = argvs[i+1] ; self.filewrite_img = True\n\t\t\tif argvs[i] == \'-tofile_txt\' : self.tofile_txt = argvs[i+1] ; self.filewrite_txt = True\n\t\t\tif argvs[i] == \'-imshow\' :\n\t\t\t\tif argvs[i+1] == \'1\' :self.imshow = True\n\t\t\t\telse : self.imshow = False\n\t\t\tif argvs[i] == \'-disp_console\' :\n\t\t\t\tif argvs[i+1] == \'1\' :self.disp_console = True\n\t\t\t\telse : self.disp_console = False\n\t\t\t\t\n\tdef build_networks(self):\n\t\tif self.disp_console : print ""Building YOLO_small graph...""\n\t\tself.x = tf.placeholder(\'float32\',[None,448,448,3])\n\t\tself.conv_1 = self.conv_layer(1,self.x,64,7,2)\n\t\tself.pool_2 = self.pooling_layer(2,self.conv_1,2,2)\n\t\tself.conv_3 = self.conv_layer(3,self.pool_2,192,3,1)\n\t\tself.pool_4 = self.pooling_layer(4,self.conv_3,2,2)\n\t\tself.conv_5 = self.conv_layer(5,self.pool_4,128,1,1)\n\t\tself.conv_6 = self.conv_layer(6,self.conv_5,256,3,1)\n\t\tself.conv_7 = self.conv_layer(7,self.conv_6,256,1,1)\n\t\tself.conv_8 = self.conv_layer(8,self.conv_7,512,3,1)\n\t\tself.pool_9 = self.pooling_layer(9,self.conv_8,2,2)\n\t\tself.conv_10 = self.conv_layer(10,self.pool_9,256,1,1)\n\t\tself.conv_11 = self.conv_layer(11,self.conv_10,512,3,1)\n\t\tself.conv_12 = self.conv_layer(12,self.conv_11,256,1,1)\n\t\tself.conv_13 = self.conv_layer(13,self.conv_12,512,3,1)\n\t\tself.conv_14 = self.conv_layer(14,self.conv_13,256,1,1)\n\t\tself.conv_15 = self.conv_layer(15,self.conv_14,512,3,1)\n\t\tself.conv_16 = self.conv_layer(16,self.conv_15,256,1,1)\n\t\tself.conv_17 = self.conv_layer(17,self.conv_16,512,3,1)\n\t\tself.conv_18 = self.conv_layer(18,self.conv_17,512,1,1)\n\t\tself.conv_19 = self.conv_layer(19,self.conv_18,1024,3,1)\n\t\tself.pool_20 = self.pooling_layer(20,self.conv_19,2,2)\n\t\tself.conv_21 = self.conv_layer(21,self.pool_20,512,1,1)\n\t\tself.conv_22 = self.conv_layer(22,self.conv_21,1024,3,1)\n\t\tself.conv_23 = self.conv_layer(23,self.conv_22,512,1,1)\n\t\tself.conv_24 = self.conv_layer(24,self.conv_23,1024,3,1)\n\t\tself.conv_25 = self.conv_layer(25,self.conv_24,1024,3,1)\n\t\tself.conv_26 = self.conv_layer(26,self.conv_25,1024,3,2)\n\t\tself.conv_27 = self.conv_layer(27,self.conv_26,1024,3,1)\n\t\tself.conv_28 = self.conv_layer(28,self.conv_27,1024,3,1)\n\t\tself.fc_29 = self.fc_layer(29,self.conv_28,512,flat=True,linear=False)\n\t\tself.fc_30 = self.fc_layer(30,self.fc_29,4096,flat=False,linear=False)\n\t\t#skip dropout_31\n\t\tself.fc_32 = self.fc_layer(32,self.fc_30,1470,flat=False,linear=True)\n\t\tself.sess = tf.Session()\n\t\tif(int(tf.__version__.split(""."")[0])==0 and int(tf.__version__.split(""."")[1])<12): ### for tf v<0.12.0\n\t\t\tself.sess.run(tf.initialize_all_variables())\n\t\telse:\n\t\t\tself.sess.run(tf.global_variables_initializer())\n\t\tself.saver = tf.train.Saver()\n\t\tself.saver.restore(self.sess,self.weights_file)\n\t\tif self.disp_console : print ""Loading complete!"" + \'\\n\'\n\n\tdef conv_layer(self,idx,inputs,filters,size,stride):\n\t\tchannels = inputs.get_shape()[3]\n\t\tweight = tf.Variable(tf.truncated_normal([size,size,int(channels),filters], stddev=0.1))\n\t\tbiases = tf.Variable(tf.constant(0.1, shape=[filters]))\n\n\t\tpad_size = size//2\n\t\tpad_mat = np.array([[0,0],[pad_size,pad_size],[pad_size,pad_size],[0,0]])\n\t\tinputs_pad = tf.pad(inputs,pad_mat)\n\n\t\tconv = tf.nn.conv2d(inputs_pad, weight, strides=[1, stride, stride, 1], padding=\'VALID\',name=str(idx)+\'_conv\')\t\n\t\tconv_biased = tf.add(conv,biases,name=str(idx)+\'_conv_biased\')\t\n\t\tif self.disp_console : print \'    Layer  %d : Type = Conv, Size = %d * %d, Stride = %d, Filters = %d, Input channels = %d\' % (idx,size,size,stride,filters,int(channels))\n\t\treturn tf.maximum(self.alpha*conv_biased,conv_biased,name=str(idx)+\'_leaky_relu\')\n\n\tdef pooling_layer(self,idx,inputs,size,stride):\n\t\tif self.disp_console : print \'    Layer  %d : Type = Pool, Size = %d * %d, Stride = %d\' % (idx,size,size,stride)\n\t\treturn tf.nn.max_pool(inputs, ksize=[1, size, size, 1],strides=[1, stride, stride, 1], padding=\'SAME\',name=str(idx)+\'_pool\')\n\n\tdef fc_layer(self,idx,inputs,hiddens,flat = False,linear = False):\n\t\tinput_shape = inputs.get_shape().as_list()\t\t\n\t\tif flat:\n\t\t\tdim = input_shape[1]*input_shape[2]*input_shape[3]\n\t\t\tinputs_transposed = tf.transpose(inputs,(0,3,1,2))\n\t\t\tinputs_processed = tf.reshape(inputs_transposed, [-1,dim])\n\t\telse:\n\t\t\tdim = input_shape[1]\n\t\t\tinputs_processed = inputs\n\t\tweight = tf.Variable(tf.truncated_normal([dim,hiddens], stddev=0.1))\n\t\tbiases = tf.Variable(tf.constant(0.1, shape=[hiddens]))\t\n\t\tif self.disp_console : print \'    Layer  %d : Type = Full, Hidden = %d, Input dimension = %d, Flat = %d, Activation = %d\' % (idx,hiddens,int(dim),int(flat),1-int(linear))\t\n\t\tif linear : return tf.add(tf.matmul(inputs_processed,weight),biases,name=str(idx)+\'_fc\')\n\t\tip = tf.add(tf.matmul(inputs_processed,weight),biases)\n\t\treturn tf.maximum(self.alpha*ip,ip,name=str(idx)+\'_fc\')\n\n\tdef detect_from_cvmat(self,img):\n\t\ts = time.time()\n\t\tself.h_img,self.w_img,_ = img.shape\n\t\timg_resized = cv2.resize(img, (448, 448))\n\t\timg_RGB = cv2.cvtColor(img_resized,cv2.COLOR_BGR2RGB)\n\t\timg_resized_np = np.asarray( img_RGB )\n\t\tinputs = np.zeros((1,448,448,3),dtype=\'float32\')\n\t\tinputs[0] = (img_resized_np/255.0)*2.0-1.0\n\t\tin_dict = {self.x: inputs}\n\t\tnet_output = self.sess.run(self.fc_32,feed_dict=in_dict)\n\t\tself.result = self.interpret_output(net_output[0])\n\t\tself.show_results(img,self.result)\n\t\tstrtime = str(time.time()-s)\n\t\tif self.disp_console : print \'Elapsed time : \' + strtime + \' secs\' + \'\\n\'\n\n\tdef detect_from_file(self,filename):\n\t\tif self.disp_console : print \'Detect from \' + filename\n\t\timg = cv2.imread(filename)\n\t\t#img = misc.imread(filename)\n\t\tself.detect_from_cvmat(img)\n\n\tdef detect_from_crop_sample(self):\n\t\tself.w_img = 640\n\t\tself.h_img = 420\n\t\tf = np.array(open(\'person_crop.txt\',\'r\').readlines(),dtype=\'float32\')\n\t\tinputs = np.zeros((1,448,448,3),dtype=\'float32\')\n\t\tfor c in range(3):\n\t\t\tfor y in range(448):\n\t\t\t\tfor x in range(448):\n\t\t\t\t\tinputs[0,y,x,c] = f[c*448*448+y*448+x]\n\n\t\tin_dict = {self.x: inputs}\n\t\tnet_output = self.sess.run(self.fc_32,feed_dict=in_dict)\n\t\tself.boxes, self.probs = self.interpret_output(net_output[0])\n\t\timg = cv2.imread(\'person.jpg\')\n\t\tself.show_results(self.boxes,img)\n\n\tdef interpret_output(self,output):\n\t\tprobs = np.zeros((7,7,2,20))\n\t\tclass_probs = np.reshape(output[0:980],(7,7,20))\n\t\tscales = np.reshape(output[980:1078],(7,7,2))\n\t\tboxes = np.reshape(output[1078:],(7,7,2,4))\n\t\toffset = np.transpose(np.reshape(np.array([np.arange(7)]*14),(2,7,7)),(1,2,0))\n\n\t\tboxes[:,:,:,0] += offset\n\t\tboxes[:,:,:,1] += np.transpose(offset,(1,0,2))\n\t\tboxes[:,:,:,0:2] = boxes[:,:,:,0:2] / 7.0\n\t\tboxes[:,:,:,2] = np.multiply(boxes[:,:,:,2],boxes[:,:,:,2])\n\t\tboxes[:,:,:,3] = np.multiply(boxes[:,:,:,3],boxes[:,:,:,3])\n\t\t\n\t\tboxes[:,:,:,0] *= self.w_img\n\t\tboxes[:,:,:,1] *= self.h_img\n\t\tboxes[:,:,:,2] *= self.w_img\n\t\tboxes[:,:,:,3] *= self.h_img\n\n\t\tfor i in range(2):\n\t\t\tfor j in range(20):\n\t\t\t\tprobs[:,:,i,j] = np.multiply(class_probs[:,:,j],scales[:,:,i])\n\n\t\tfilter_mat_probs = np.array(probs>=self.threshold,dtype=\'bool\')\n\t\tfilter_mat_boxes = np.nonzero(filter_mat_probs)\n\t\tboxes_filtered = boxes[filter_mat_boxes[0],filter_mat_boxes[1],filter_mat_boxes[2]]\n\t\tprobs_filtered = probs[filter_mat_probs]\n\t\tclasses_num_filtered = np.argmax(filter_mat_probs,axis=3)[filter_mat_boxes[0],filter_mat_boxes[1],filter_mat_boxes[2]] \n\n\t\targsort = np.array(np.argsort(probs_filtered))[::-1]\n\t\tboxes_filtered = boxes_filtered[argsort]\n\t\tprobs_filtered = probs_filtered[argsort]\n\t\tclasses_num_filtered = classes_num_filtered[argsort]\n\t\t\n\t\tfor i in range(len(boxes_filtered)):\n\t\t\tif probs_filtered[i] == 0 : continue\n\t\t\tfor j in range(i+1,len(boxes_filtered)):\n\t\t\t\tif self.iou(boxes_filtered[i],boxes_filtered[j]) > self.iou_threshold : \n\t\t\t\t\tprobs_filtered[j] = 0.0\n\t\t\n\t\tfilter_iou = np.array(probs_filtered>0.0,dtype=\'bool\')\n\t\tboxes_filtered = boxes_filtered[filter_iou]\n\t\tprobs_filtered = probs_filtered[filter_iou]\n\t\tclasses_num_filtered = classes_num_filtered[filter_iou]\n\n\t\tresult = []\n\t\tfor i in range(len(boxes_filtered)):\n\t\t\tresult.append([self.classes[classes_num_filtered[i]],boxes_filtered[i][0],boxes_filtered[i][1],boxes_filtered[i][2],boxes_filtered[i][3],probs_filtered[i]])\n\n\t\treturn result\n\n\tdef show_results(self,img,results):\n\t\timg_cp = img.copy()\n\t\tif self.filewrite_txt :\n\t\t\tftxt = open(self.tofile_txt,\'w\')\n\t\tfor i in range(len(results)):\n\t\t\tx = int(results[i][1])\n\t\t\ty = int(results[i][2])\n\t\t\tw = int(results[i][3])//2\n\t\t\th = int(results[i][4])//2\n\t\t\tif self.disp_console : print \'    class : \' + results[i][0] + \' , [x,y,w,h]=[\' + str(x) + \',\' + str(y) + \',\' + str(int(results[i][3])) + \',\' + str(int(results[i][4]))+\'], Confidence = \' + str(results[i][5])\n\t\t\tif self.filewrite_img or self.imshow:\n\t\t\t\tcv2.rectangle(img_cp,(x-w,y-h),(x+w,y+h),(0,255,0),2)\n\t\t\t\tcv2.rectangle(img_cp,(x-w,y-h-20),(x+w,y-h),(125,125,125),-1)\n\t\t\t\tcv2.putText(img_cp,results[i][0] + \' : %.2f\' % results[i][5],(x-w+5,y-h-7),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1)\n\t\t\tif self.filewrite_txt :\t\t\t\t\n\t\t\t\tftxt.write(results[i][0] + \',\' + str(x) + \',\' + str(y) + \',\' + str(w) + \',\' + str(h)+\',\' + str(results[i][5]) + \'\\n\')\n\t\tif self.filewrite_img : \n\t\t\tif self.disp_console : print \'    image file writed : \' + self.tofile_img\n\t\t\tcv2.imwrite(self.tofile_img,img_cp)\t\t\t\n\t\tif self.imshow :\n\t\t\tcv2.imshow(\'YOLO_small detection\',img_cp)\n\t\t\tcv2.waitKey(1)\n\t\tif self.filewrite_txt : \n\t\t\tif self.disp_console : print \'    txt file writed : \' + self.tofile_txt\n\t\t\tftxt.close()\n\n\tdef iou(self,box1,box2):\n\t\ttb = min(box1[0]+0.5*box1[2],box2[0]+0.5*box2[2])-max(box1[0]-0.5*box1[2],box2[0]-0.5*box2[2])\n\t\tlr = min(box1[1]+0.5*box1[3],box2[1]+0.5*box2[3])-max(box1[1]-0.5*box1[3],box2[1]-0.5*box2[3])\n\t\tif tb < 0 or lr < 0 : intersection = 0\n\t\telse : intersection =  tb*lr\n\t\treturn intersection / (box1[2]*box1[3] + box2[2]*box2[3] - intersection)\n\n\tdef training(self): #TODO add training function!\n\t\treturn None\n\n\t\n\t\t\t\n\ndef main(argvs):\n\tyolo = YOLO_TF(argvs)\n\tcv2.waitKey(1000)\n\n\nif __name__==\'__main__\':\t\n\tmain(sys.argv)\n'"
YOLO/YOLO_tiny_tf.py,20,"b'import numpy as np\nimport tensorflow as tf\nimport cv2\nimport time\nimport sys\n\nclass YOLO_TF:\n\tfromfile = None\n\ttofile_img = \'test/output.jpg\'\n\ttofile_txt = \'test/output.txt\'\n\timshow = True\n\tfilewrite_img = False\n\tfilewrite_txt = False\n\tdisp_console = True\n\tweights_file = \'weights/YOLO_tiny.ckpt\'\n\talpha = 0.1\n\tthreshold = 0.2\n\tiou_threshold = 0.5\n\tnum_class = 20\n\tnum_box = 2\n\tgrid_size = 7\n\tclasses =  [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"",""tvmonitor""]\n\n\tw_img = 640\n\th_img = 480\n\n\tdef __init__(self,argvs = []):\n\t\tself.argv_parser(argvs)\n\t\tself.build_networks()\n\t\tif self.fromfile is not None: self.detect_from_file(self.fromfile)\n\tdef argv_parser(self,argvs):\n\t\tfor i in range(1,len(argvs),2):\n\t\t\tif argvs[i] == \'-fromfile\' : self.fromfile = argvs[i+1]\n\t\t\tif argvs[i] == \'-tofile_img\' : self.tofile_img = argvs[i+1] ; self.filewrite_img = True\n\t\t\tif argvs[i] == \'-tofile_txt\' : self.tofile_txt = argvs[i+1] ; self.filewrite_txt = True\n\t\t\tif argvs[i] == \'-imshow\' :\n\t\t\t\tif argvs[i+1] == \'1\' :self.imshow = True\n\t\t\t\telse : self.imshow = False\n\t\t\tif argvs[i] == \'-disp_console\' :\n\t\t\t\tif argvs[i+1] == \'1\' :self.disp_console = True\n\t\t\t\telse : self.disp_console = False\n\t\t\t\t\n\tdef build_networks(self):\n\t\tif self.disp_console : print ""Building YOLO_tiny graph...""\n\t\tself.x = tf.placeholder(\'float32\',[None,448,448,3])\n\t\tself.conv_1 = self.conv_layer(1,self.x,16,3,1)\n\t\tself.pool_2 = self.pooling_layer(2,self.conv_1,2,2)\n\t\tself.conv_3 = self.conv_layer(3,self.pool_2,32,3,1)\n\t\tself.pool_4 = self.pooling_layer(4,self.conv_3,2,2)\n\t\tself.conv_5 = self.conv_layer(5,self.pool_4,64,3,1)\n\t\tself.pool_6 = self.pooling_layer(6,self.conv_5,2,2)\n\t\tself.conv_7 = self.conv_layer(7,self.pool_6,128,3,1)\n\t\tself.pool_8 = self.pooling_layer(8,self.conv_7,2,2)\n\t\tself.conv_9 = self.conv_layer(9,self.pool_8,256,3,1)\n\t\tself.pool_10 = self.pooling_layer(10,self.conv_9,2,2)\n\t\tself.conv_11 = self.conv_layer(11,self.pool_10,512,3,1)\n\t\tself.pool_12 = self.pooling_layer(12,self.conv_11,2,2)\n\t\tself.conv_13 = self.conv_layer(13,self.pool_12,1024,3,1)\n\t\tself.conv_14 = self.conv_layer(14,self.conv_13,1024,3,1)\n\t\tself.conv_15 = self.conv_layer(15,self.conv_14,1024,3,1)\n\t\tself.fc_16 = self.fc_layer(16,self.conv_15,256,flat=True,linear=False)\n\t\tself.fc_17 = self.fc_layer(17,self.fc_16,4096,flat=False,linear=False)\n\t\t#skip dropout_18\n\t\tself.fc_19 = self.fc_layer(19,self.fc_17,1470,flat=False,linear=True)\n\t\tself.sess = tf.Session()\n\t\tif(int(tf.__version__.split(""."")[0])==0 and int(tf.__version__.split(""."")[1])<12): ### for tf v<0.12.0\n\t\t\tself.sess.run(tf.initialize_all_variables())\n\t\telse:\n\t\t\tself.sess.run(tf.global_variables_initializer())\t\n\t\tself.saver = tf.train.Saver()\n\t\tself.saver.restore(self.sess,self.weights_file)\n\t\tif self.disp_console : print ""Loading complete!"" + \'\\n\'\n\n\tdef conv_layer(self,idx,inputs,filters,size,stride):\n\t\tchannels = inputs.get_shape()[3]\n\t\tweight = tf.Variable(tf.truncated_normal([size,size,int(channels),filters], stddev=0.1))\n\t\tbiases = tf.Variable(tf.constant(0.1, shape=[filters]))\n\n\t\tpad_size = size//2\n\t\tpad_mat = np.array([[0,0],[pad_size,pad_size],[pad_size,pad_size],[0,0]])\n\t\tinputs_pad = tf.pad(inputs,pad_mat)\n\n\t\tconv = tf.nn.conv2d(inputs_pad, weight, strides=[1, stride, stride, 1], padding=\'VALID\',name=str(idx)+\'_conv\')\t\n\t\tconv_biased = tf.add(conv,biases,name=str(idx)+\'_conv_biased\')\t\n\t\tif self.disp_console : print \'    Layer  %d : Type = Conv, Size = %d * %d, Stride = %d, Filters = %d, Input channels = %d\' % (idx,size,size,stride,filters,int(channels))\n\t\treturn tf.maximum(self.alpha*conv_biased,conv_biased,name=str(idx)+\'_leaky_relu\')\n\n\tdef pooling_layer(self,idx,inputs,size,stride):\n\t\tif self.disp_console : print \'    Layer  %d : Type = Pool, Size = %d * %d, Stride = %d\' % (idx,size,size,stride)\n\t\treturn tf.nn.max_pool(inputs, ksize=[1, size, size, 1],strides=[1, stride, stride, 1], padding=\'SAME\',name=str(idx)+\'_pool\')\n\n\tdef fc_layer(self,idx,inputs,hiddens,flat = False,linear = False):\n\t\tinput_shape = inputs.get_shape().as_list()\t\t\n\t\tif flat:\n\t\t\tdim = input_shape[1]*input_shape[2]*input_shape[3]\n\t\t\tinputs_transposed = tf.transpose(inputs,(0,3,1,2))\n\t\t\tinputs_processed = tf.reshape(inputs_transposed, [-1,dim])\n\t\telse:\n\t\t\tdim = input_shape[1]\n\t\t\tinputs_processed = inputs\n\t\tweight = tf.Variable(tf.truncated_normal([dim,hiddens], stddev=0.1))\n\t\tbiases = tf.Variable(tf.constant(0.1, shape=[hiddens]))\t\n\t\tif self.disp_console : print \'    Layer  %d : Type = Full, Hidden = %d, Input dimension = %d, Flat = %d, Activation = %d\' % (idx,hiddens,int(dim),int(flat),1-int(linear))\t\n\t\tif linear : return tf.add(tf.matmul(inputs_processed,weight),biases,name=str(idx)+\'_fc\')\n\t\tip = tf.add(tf.matmul(inputs_processed,weight),biases)\n\t\treturn tf.maximum(self.alpha*ip,ip,name=str(idx)+\'_fc\')\n\n\tdef detect_from_cvmat(self,img):\n\t\ts = time.time()\n\t\tself.h_img,self.w_img,_ = img.shape\n\t\timg_resized = cv2.resize(img, (448, 448))\n\t\timg_RGB = cv2.cvtColor(img_resized,cv2.COLOR_BGR2RGB)\n\t\timg_resized_np = np.asarray( img_RGB )\n\t\tinputs = np.zeros((1,448,448,3),dtype=\'float32\')\n\t\tinputs[0] = (img_resized_np/255.0)*2.0-1.0\n\t\tin_dict = {self.x: inputs}\n\t\tnet_output = self.sess.run(self.fc_19,feed_dict=in_dict)\n\t\tself.result = self.interpret_output(net_output[0])\n\t\tself.show_results(img,self.result)\n\t\tstrtime = str(time.time()-s)\n\t\tif self.disp_console : print \'Elapsed time : \' + strtime + \' secs\' + \'\\n\'\n\n\tdef detect_from_file(self,filename):\n\t\tif self.disp_console : print \'Detect from \' + filename\n\t\timg = cv2.imread(filename)\n\t\t#img = misc.imread(filename)\n\t\tself.detect_from_cvmat(img)\n\n\tdef detect_from_crop_sample(self):\n\t\tself.w_img = 640\n\t\tself.h_img = 420\n\t\tf = np.array(open(\'person_crop.txt\',\'r\').readlines(),dtype=\'float32\')\n\t\tinputs = np.zeros((1,448,448,3),dtype=\'float32\')\n\t\tfor c in range(3):\n\t\t\tfor y in range(448):\n\t\t\t\tfor x in range(448):\n\t\t\t\t\tinputs[0,y,x,c] = f[c*448*448+y*448+x]\n\n\t\tin_dict = {self.x: inputs}\n\t\tnet_output = self.sess.run(self.fc_19,feed_dict=in_dict)\n\t\tself.boxes, self.probs = self.interpret_output(net_output[0])\n\t\timg = cv2.imread(\'person.jpg\')\n\t\tself.show_results(self.boxes,img)\n\n\tdef interpret_output(self,output):\n\t\tprobs = np.zeros((7,7,2,20))\n\t\tclass_probs = np.reshape(output[0:980],(7,7,20))\n\t\tscales = np.reshape(output[980:1078],(7,7,2))\n\t\tboxes = np.reshape(output[1078:],(7,7,2,4))\n\t\toffset = np.transpose(np.reshape(np.array([np.arange(7)]*14),(2,7,7)),(1,2,0))\n\n\t\tboxes[:,:,:,0] += offset\n\t\tboxes[:,:,:,1] += np.transpose(offset,(1,0,2))\n\t\tboxes[:,:,:,0:2] = boxes[:,:,:,0:2] / 7.0\n\t\tboxes[:,:,:,2] = np.multiply(boxes[:,:,:,2],boxes[:,:,:,2])\n\t\tboxes[:,:,:,3] = np.multiply(boxes[:,:,:,3],boxes[:,:,:,3])\n\t\t\n\t\tboxes[:,:,:,0] *= self.w_img\n\t\tboxes[:,:,:,1] *= self.h_img\n\t\tboxes[:,:,:,2] *= self.w_img\n\t\tboxes[:,:,:,3] *= self.h_img\n\n\t\tfor i in range(2):\n\t\t\tfor j in range(20):\n\t\t\t\tprobs[:,:,i,j] = np.multiply(class_probs[:,:,j],scales[:,:,i])\n\n\t\tfilter_mat_probs = np.array(probs>=self.threshold,dtype=\'bool\')\n\t\tfilter_mat_boxes = np.nonzero(filter_mat_probs)\n\t\tboxes_filtered = boxes[filter_mat_boxes[0],filter_mat_boxes[1],filter_mat_boxes[2]]\n\t\tprobs_filtered = probs[filter_mat_probs]\n\t\tclasses_num_filtered = np.argmax(filter_mat_probs,axis=3)[filter_mat_boxes[0],filter_mat_boxes[1],filter_mat_boxes[2]] \n\n\t\targsort = np.array(np.argsort(probs_filtered))[::-1]\n\t\tboxes_filtered = boxes_filtered[argsort]\n\t\tprobs_filtered = probs_filtered[argsort]\n\t\tclasses_num_filtered = classes_num_filtered[argsort]\n\t\t\n\t\tfor i in range(len(boxes_filtered)):\n\t\t\tif probs_filtered[i] == 0 : continue\n\t\t\tfor j in range(i+1,len(boxes_filtered)):\n\t\t\t\tif self.iou(boxes_filtered[i],boxes_filtered[j]) > self.iou_threshold : \n\t\t\t\t\tprobs_filtered[j] = 0.0\n\t\t\n\t\tfilter_iou = np.array(probs_filtered>0.0,dtype=\'bool\')\n\t\tboxes_filtered = boxes_filtered[filter_iou]\n\t\tprobs_filtered = probs_filtered[filter_iou]\n\t\tclasses_num_filtered = classes_num_filtered[filter_iou]\n\n\t\tresult = []\n\t\tfor i in range(len(boxes_filtered)):\n\t\t\tresult.append([self.classes[classes_num_filtered[i]],boxes_filtered[i][0],boxes_filtered[i][1],boxes_filtered[i][2],boxes_filtered[i][3],probs_filtered[i]])\n\n\t\treturn result\n\n\tdef show_results(self,img,results):\n\t\timg_cp = img.copy()\n\t\tif self.filewrite_txt :\n\t\t\tftxt = open(self.tofile_txt,\'w\')\n\t\tfor i in range(len(results)):\n\t\t\tx = int(results[i][1])\n\t\t\ty = int(results[i][2])\n\t\t\tw = int(results[i][3])//2\n\t\t\th = int(results[i][4])//2\n\t\t\tif self.disp_console : print \'    class : \' + results[i][0] + \' , [x,y,w,h]=[\' + str(x) + \',\' + str(y) + \',\' + str(int(results[i][3])) + \',\' + str(int(results[i][4]))+\'], Confidence = \' + str(results[i][5])\n\t\t\tif self.filewrite_img or self.imshow:\n\t\t\t\tcv2.rectangle(img_cp,(x-w,y-h),(x+w,y+h),(0,255,0),2)\n\t\t\t\tcv2.rectangle(img_cp,(x-w,y-h-20),(x+w,y-h),(125,125,125),-1)\n\t\t\t\tcv2.putText(img_cp,results[i][0] + \' : %.2f\' % results[i][5],(x-w+5,y-h-7),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1)\n\t\t\tif self.filewrite_txt :\t\t\t\t\n\t\t\t\tftxt.write(results[i][0] + \',\' + str(x) + \',\' + str(y) + \',\' + str(w) + \',\' + str(h)+\',\' + str(results[i][5]) + \'\\n\')\n\t\tif self.filewrite_img : \n\t\t\tif self.disp_console : print \'    image file writed : \' + self.tofile_img\n\t\t\tcv2.imwrite(self.tofile_img,img_cp)\t\t\t\n\t\tif self.imshow :\n\t\t\tcv2.imshow(\'YOLO_tiny detection\',img_cp)\n\t\t\tcv2.waitKey(1)\n\t\tif self.filewrite_txt : \n\t\t\tif self.disp_console : print \'    txt file writed : \' + self.tofile_txt\n\t\t\tftxt.close()\n\n\tdef iou(self,box1,box2):\n\t\ttb = min(box1[0]+0.5*box1[2],box2[0]+0.5*box2[2])-max(box1[0]-0.5*box1[2],box2[0]-0.5*box2[2])\n\t\tlr = min(box1[1]+0.5*box1[3],box2[1]+0.5*box2[3])-max(box1[1]-0.5*box1[3],box2[1]-0.5*box2[3])\n\t\tif tb < 0 or lr < 0 : intersection = 0\n\t\telse : intersection =  tb*lr\n\t\treturn intersection / (box1[2]*box1[3] + box2[2]*box2[3] - intersection)\n\n\tdef training(self): #TODO add training function!\n\t\treturn None\n\n\t\n\t\t\t\n\ndef main(argvs):\n\tyolo = YOLO_TF(argvs)\n\tcv2.waitKey(1000)\n\n\nif __name__==\'__main__\':\t\n\tmain(sys.argv)\n'"
dataset_scripts/pre_process_dataset.py,0,"b'\nfrom xml.etree import ElementTree\nimport os\nimport shutil\nimport argparse\nimport time\nfrom PIL import Image, ImageChops\nimport progressbar\nimport glob\nfrom frame import Frame_Info\nfrom multiclass_rectangle import Rectangle_Multiclass\nimport utils_image\nimport Utils\nimport vid_classes\nfrom vid_classes import Classes_List as CL\n\n## SIZE VARIABLES\n\nwidth=640\nheight=480\n\n##### GENERAL FUNCTIONS\n\n\ndef get_ordered_name_XML(path_bbox_dataset):\n\n    a = []\n    for s in os.listdir(path_bbox_dataset):\n         if os.path.isfile(os.path.join(path_bbox_dataset, s)) & s.endswith(\'.xml\'):\n            a.append(os.path.join(path_bbox_dataset, s))\n    a.sort(key=lambda s: s)\n    return a\n\ndef create_folder_structure(path_dataset): #Create All the files needed for the New Dataset\n    \n    if not os.path.exists(path_dataset):\n        os.makedirs(path_dataset)\n        print(""Created Folder: %s""%path_dataset)\n    if not os.path.exists(path_dataset+\'/annotations\'):\n        os.makedirs(path_dataset+\'/annotations\')\n        print(""Created Folder: %s""%(path_dataset+\'/annotations\'))\n    if not os.path.exists(path_dataset+\'/data\'):\n        os.makedirs(path_dataset+\'/data\')\n        print(""Created Folder: %s""%(path_dataset+\'/data\'))\n    \n    for class_code in CL.class_code_string_list:\n        if not os.path.exists(path_dataset+\'/annotations/\'+class_code):\n            os.makedirs(path_dataset+\'/annotations/\'+class_code)\n            print(""Created Folder: %s""%(path_dataset+\'/annotations/\'+class_code))\n        if not os.path.exists(path_dataset+\'/data/\'+class_code):\n            os.makedirs(path_dataset+\'/data/\'+class_code)\n            print(""Created Folder: %s""%(path_dataset+\'/data/\'+class_code))\n\n    if not os.path.exists(path_dataset+\'/dataset_summary.txt\'):\n        open(path_dataset+\'/dataset_summary.txt\', \'a\')\n        print ""Created File: %s""%(path_dataset+\'/dataset_summary.txt\')\n\ndef pre_process_dataset(bb_XML_file_list, path_val_folder, path_dataset):\n\n    with open(path_dataset+\'/dataset_summary.txt\', \'w\') as summary:\n        for class_code in CL.class_code_string_list:\n            start_string=""Starting making files for class code: %s ; name: %s, May take a while.... ""%(class_code, vid_classes.code_to_class_string(class_code))\n            staring_xml_string=""Strating xml files: %d""%len(bb_XML_file_list)\n            print start_string+ \'/n\', staring_xml_string+\'/n\'\n            summary.write(start_string+ os.linesep+staring_xml_string+ os.linesep)\n            progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n            tot_rect_class=0\n            count_xml=0\n            bb_folder_path=path_dataset+\'/annotations/\'+class_code+\'/\'\n            data_folder_path=path_dataset+\'/data/\'+class_code+\'/\'\n            for file_name in progress(bb_XML_file_list): \n                removed=0            \n                delete=False\n                with open(file_name, \'rt\') as f:\n                    #print ""File Opened: %s""%file_name\n                    tree_new = ElementTree.parse(f)\n                    root_new = tree_new.getroot()\n                    parent_map_new = dict((c, p) for p in tree_new.getiterator() for c in p)\n                    count_rect=0\n                    # list so that we don\'t mess up the order of iteration when removing items.\n                    for obj in list(tree_new.findall(\'object\')):\n                        obj_class_code = obj.find(\'name\').text\n                        if obj_class_code == str(class_code):\n                            #print ""Founded Object: %s""%vid_classes.code_to_class_string(obj_class_code)\n                            #count_rect+1 there\'s an object of that class in the xml file\n                            count_rect=count_rect+1\n                            delete=True\n                        else:\n                            #eliminate node\n                            #print ""Eliminated Node: %s""%vid_classes.code_to_class_string(obj_class_code)\n                            parent_map_new[obj].remove(obj)\n                            removed=removed+1\n                    if count_rect>0:\n                        ### Means the file belongs to the class so we change filename and directory name and we copy the image to the dataset\n                        for node in tree_new.iter():\n                            tag=str(node.tag)\n                            if tag in [""folder""]:\n                                path_orig_file=path_val_folder+\'/\'+str(node.text)\n                                node.text= data_folder_path\n                                # print ""Changed folder from: %s to : %s""%(path_orig_file, node.text)\n                            if tag in [""filename""]:\n                                path_orig_file=path_orig_file+\'/\'+str(node.text)+\'.JPEG\'\n                                new_filename=class_code+\'_\'+str(000000+count_xml)+\'.JPEG\'\n                                path_new_file=data_folder_path+new_filename\n                                node.text= new_filename\n                                # print ""Changed Name from: %s to : %s""%(path_orig_file, path_new_file)\n                        xml_filename=bb_folder_path+class_code+\'_\'+str(000000+count_xml)+\'.xml\'\n                        tree_new.write(xml_filename)\n                        shutil.copy2(path_orig_file, path_new_file)\n                        # print ""Saved New .xml file: %s""%xml_filename\n                        # print ""Saved New .jpeg image: %s""%path_new_file\n                        if (removed == 0 ) & delete:\n                            os.remove(path_orig_file)\n                            # print ""Removed image: %s""%(path_orig_file)\n                        count_xml=count_xml+1\n                        tot_rect_class=tot_rect_class+count_rect\n                        ##TODO: Copy Image\n                # print ""Count_Rect %d""%count_rect\n                # print ""Removed %d""%removed\n                if (removed == 0 ) & delete:\n                    os.remove(file_name)\n                    # print ""Removed XML: %s""%(file_name)\n                    bb_XML_file_list.remove(file_name)\n                if (removed >0)&(count_rect>0):\n                    with open(file_name, \'rt\') as f:\n                        tree_new = ElementTree.parse(f)\n                        root_new = tree_new.getroot()\n                        parent_map_new = dict((c, p) for p in tree_new.getiterator() for c in p)\n                        for obj in list(tree_new.findall(\'object\')):\n                            obj_class_code = obj.find(\'name\').text\n                            if obj_class_code == str(class_code):\n                                parent_map_new[obj].remove(obj)\n                        tree_new.write(file_name)\n            end_string=""Ended with Success Process for class:%s""%class_code \n            parsed_bb_string=""Parsed: %d BB Files""%count_xml\n            added_rect_String=""Added: %d Object Rectangles""%tot_rect_class\n            print end_string+\'/n\',parsed_bb_string +\'/n\',added_rect_String+\'/n\'\n            summary.write(end_string+ os.linesep+parsed_bb_string+ os.linesep+added_rect_String+ os.linesep)\n\ndef main():\n    \'\'\'\n    Parse command line arguments and execute the code\n\n    \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset_path\', default=\'./dataset\', type=str)\n    parser.add_argument(\'--bb_folder\', required=True, type=str)\n    parser.add_argument(\'--val_folder\', required=True, type=str)\n    args = parser.parse_args()\n\n    \n    start = time.time()\n\n    bb_XML_file_list=[]\n    create_folder_structure(args.dataset_path)\n    bb_XML_file_list= get_ordered_name_XML(args.bb_folder)\n    pre_process_dataset(bb_XML_file_list, args.val_folder, args.dataset_path)\n    # shutil.rmtree(args.val_folder, ignore_errors=True)\n    # shutil.rmtree(args.bb_folder, ignore_errors=True)\n\n    end = time.time()\n\n    print(""Elapsed Time:%d Seconds""%(end-start))\n    print(""Running Completed with Success!!!"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
dataset_scripts/process_dataset_heavy.py,0,"b'\nfrom xml.etree import ElementTree\nimport os\nimport shutil\nimport argparse\nimport time\nfrom PIL import Image, ImageChops\nimport progressbar\nfrom frame import Frame_Info\nfrom multiclass_rectangle import Rectangle_Multiclass\nimport utils_image\nimport Utils\nimport vid_classes\nfrom vid_classes import Classes_List as CL\n\n##### GENERAL VARIABLES #######\n\n##MULTICLASS NEW DATASET\n\nstring_mltcl_bb_file = \'mltcl_bb_file_list.txt\'\nstring_mltcl_class_code_file = \'mltcl_class_code_file_list.txt\'\nstring_mltcl_class_name_file = \'mltcl_class_name_file_list.txt\'\nstring_mltcl_chall_code_file = \'mltcl_chall_code_file_list.txt\'\n\n##SINGLE CLASS NEW DATASET\n\nstring_bb_file = \'_bb_file_list.txt\'\nstring_class_code_file = \'_class_code_file_list.txt\'\nstring_class_name_file = \'_class_name_file_list.txt\'\nstring_chall_code_file = \'_chall_code_file_list.txt\'\n\n## SIZE VARIABLES\n\nwidth=640\nheight=480\n\n##### GENERAL FUNCTIONS\n\ndef create_summary_files(path_dataset): #Create All the files needed for the New Dataset\n    \n    path_mltcl_bb_file= path_dataset+\'/\'+string_mltcl_bb_file\n    path_mltcl_class_code_file= path_dataset+\'/\'+string_mltcl_class_code_file\n    path_mltcl_class_name_file= path_dataset+\'/\'+string_mltcl_class_name_file\n    path_mltcl_chall_code_file= path_dataset+\'/\'+string_mltcl_chall_code_file\n\n    if not os.path.exists(path_dataset):\n        os.makedirs(path_dataset)\n        print(""Created Folder: %s""%path_dataset)\n    if not os.path.exists(path_mltcl_bb_file):\n        open(path_mltcl_bb_file, \'a\')\n        print ""Created File: ""+ path_mltcl_bb_file\n    if not os.path.exists(path_mltcl_class_code_file):\n        open(path_mltcl_class_code_file, \'a\')\n        print ""Created File: ""+ path_mltcl_class_code_file\n    if not os.path.exists(path_mltcl_class_name_file):\n        open(path_mltcl_class_name_file, \'a\')\n        print ""Created File: ""+ path_mltcl_class_name_file\n    if not os.path.exists(path_mltcl_chall_code_file):\n        open(path_mltcl_chall_code_file, \'a\')\n        print ""Created File: ""+ path_mltcl_chall_code_file\n    \n    for class_name in CL.class_name_string_list:\n\n\n        path_bb_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_bb_file\n        path_class_code_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_class_code_file\n        path_class_name_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_class_name_file\n        path_chall_code_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_chall_code_file\n        \n\n        if not os.path.exists(path_dataset+\'/\'+class_name):\n            os.makedirs(path_dataset+\'/\'+class_name)\n            print(""Created Folder: %s""%(path_dataset+\'/\'+class_name))\n        \n        if not os.path.exists(path_bb_file):\n            open(path_bb_file, \'a\')\n            print ""Created File: ""+ path_bb_file\n        if not os.path.exists(path_class_code_file):\n            open(path_class_code_file, \'a\')\n            print ""Created File: ""+ path_class_code_file\n        if not os.path.exists(path_class_name_file):\n            open(path_class_name_file, \'a\')\n            print ""Created File: ""+ path_class_name_file\n        if not os.path.exists(path_chall_code_file):\n            open(path_chall_code_file, \'a\')\n            print ""Created File: ""+ path_chall_code_file\n\ndef parse_XML_to_multiclass_txt(bb_XML_file_list, path_val_folder, path_dataset):\n\n    count_rect=0\n    count_img=0\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n    print ""Start Processing & Building the MultiClass Dataset... May take a while...""\n\n    path_mltcl_bb_file=path_dataset+\'/\'+string_mltcl_bb_file # Create this file in .dataset/airplane/airplane_bb_mltcl_file_list.txt\n    path_mltcl_class_code_file=path_dataset+\'/\'+string_mltcl_class_code_file\n    path_mltcl_class_name_file=path_dataset+\'/\'+string_mltcl_class_name_file\n    path_mltcl_chall_code_file=path_dataset+\'/\'+string_mltcl_chall_code_file\n\n    for file_name in progress(bb_XML_file_list):\n        with open(file_name, \'rt\') as f:\n            tree = ElementTree.parse(f)\n            for obj in tree.findall(\'object\'):\n                name = obj.find(\'name\').text\n                class_code= name\n                name = vid_classes.code_to_class_string(name)\n\n                if name in [""nothing""]:\n                    continue\n                else:\n\n                    path_orig_file=path_val_folder\n                    \n                    jump=0\n                    \n                    image_multi_class= Frame_Info()\n                    image_multi_class.dataset_path= path_val_folder\n\n                    rectangle_multi= Rectangle_Multiclass()\n                    \n                    #xmin x1 letf\n                    #ymin y1 bottom\n                    #xmax x2 right\n                    #ymax y2 top\n                \n                    for node in tree.iter():\n                        tag=str(node.tag)\n            \n                        if tag in [""folder""]:\n                            path_orig_file=path_orig_file+\'/\'+str(node.text)\n                            image_multi_class.folder= str(node.text)\n\n                        if tag in [""filename""]:\n                            image_multi_class.filename=str(node.text)+\'.PNG\'\n\n                            path_orig_file=path_orig_file+\'/\'+str(node.text)+\'.JPEG\'\n\n                        if tag in [""width""]:\n                            image_multi_class.width= int(node.text)\n\n                        if tag in [""height""]:\n                            image_multi_class.height= int(node.text)\n\n                        if tag in [""trackid""]:\n                            image_multi_class.frame= int(os.path.basename(file_name).replace(\'.xml\', \'\'))\n\n                        if tag in [\'name\']:\n                            if str(vid_classes.code_to_class_string(str(node.text))) in [""nothing""]:\n                                jump = 1\n                            else : \n                                jump=0\n                                print str(node.text)\n                                rectangle_multi.label_chall=int(vid_classes.code_to_code_chall(str(node.text)))\n                                rectangle_multi.label_code=str(node.text)\n                                rectangle_multi.label=vid_classes.code_to_class_string(str(node.text))      \n                        if tag in [""xmax""]:\n                            if jump == 0:\n                                rectangle_multi.x2=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), True))\n                        if tag in [""xmin""]:\n                            if jump == 0:\n                                rectangle_multi.x1=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), True))\n                        if tag in [""ymin""]:\n                            if jump == 0:\n                                rectangle_multi.y2=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))      \n                        if tag in [""ymax""]:\n                            if jump == 0:    \n                                rectangle_multi.y1=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n                                image_multi_class.append_rect(rectangle_multi)\n                                count_rect=count_rect+1\n\n                    if jump == 0:\n\t                    count_img=count_img+1\n\t    \t            out_stream = open(path_mltcl_bb_file, ""a"")\n\t    \t            out_stream.write(image_multi_class.get_info_string()+ os.linesep)\n\t    \t                \n\t    \t            out_stream = open(path_mltcl_class_code_file, ""a"")\n\t    \t            out_stream.write(image_multi_class.get_rects_code()+ os.linesep)\n\t    \t                \n\t    \t            out_stream = open(path_mltcl_class_name_file, ""a"")\n\t    \t            out_stream.write(image_multi_class.get_rects_labels()+ os.linesep)\n\t    \t                \n\t    \t            out_stream = open(path_mltcl_chall_code_file, ""a"")\n\t    \t            out_stream.write(image_multi_class.get_rects_chall() + os.linesep)\n\n                    break\n    out_stream = open(\'dataset/summary_multiclass.txt\', ""w"")\n    out_stream.write( ""SUMMARY:"" + os.linesep+ (""Parsed: %d BB Files""%len(bb_XML_file_list)) + os.linesep+ (""Added: %d Object Rectangles""%count_rect) + os.linesep+(""Added: %d Images""%count_img)+os.linesep)\n    print ""Ended with Success the Process""\n    print ""SUMMARY:""\n    print ""Parsed: %d BB Files""%len(bb_XML_file_list)\n    print ""Added: %d Object Rectangles""%count_rect\n    print ""Added: %d Images""%count_img\n    if count_img>0: \n        print ""Ratio BB_Files/Images: %.2f""%(float(count_img)/float(len(bb_XML_file_list)))\n        out_stream.write((""Ratio BB_Files/Images: %.2f""%(float(count_img)/float(len(bb_XML_file_list))))+os.linesep)\n    if count_rect>0:\n        print ""Ratio Images/Object_Rectangles: %.2f""%(float(count_img)/float(count_rect))\n        out_stream.write((""Ratio Images/Object_Rectangles: %.2f""%(float(count_img)/float(count_rect)))+os.linesep)\n\ndef parse_XML_to_singleclass_txt(bb_XML_file_list, path_val_folder, path_dataset):\n\n    with open(\'dataset/summary.txt\', \'w\') as summary:\n        for class_name in CL.class_name_string_list:\n\n            print ""Starting making files for class:%s , May take a while.... ""%class_name \n\n            progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n            path_bb_file=path_dataset+\'/\'+class_name+\'/\'+ class_name+string_bb_file\n            path_class_code_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_class_code_file\n            path_class_name_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_class_name_file\n            path_chall_code_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_chall_code_file\n            count_rect=0\n            count_img=0\n            for file_name in progress(bb_XML_file_list):\n                with open(file_name, \'rt\') as f:\n                    tree = ElementTree.parse(f)\n                    for obj in tree.findall(\'object\'):\n                        name = obj.find(\'name\').text\n                        class_code= name\n                        name = vid_classes.code_to_class_string(name)\n\n                        if name in [""nothing""]:\n                            continue\n                        else:\n\n                            same_label=0\n                            \n                            image_single_class= Frame_Info()\n                            image_single_class.dataset_path= path_val_folder\n                            path_orig_file=path_val_folder\n\n                            rectangle_single= Rectangle_Multiclass()\n                            \n                            #xmin x1 letf\n                            #ymin y1 bottom\n                            #xmax x2 right\n                            #ymax y2 top\n                        \n                            for node in tree.iter():\n                                tag=str(node.tag)\n                    \n                                if tag in [""folder""]:\n                                    path_orig_file=path_orig_file+\'/\'+str(node.text)\n                                    image_single_class.folder= str(node.text)\n                                if tag in [""filename""]:\n                                    image_single_class.filename=str(node.text)+\'.PNG\'\n                                    path_orig_file=path_orig_file+\'/\'+str(node.text)+\'.JPEG\'\n\n                                if tag in [""width""]:\n                                    image_single_class.width= int(node.text)\n\n                                if tag in [""height""]:\n                                    image_single_class.height= int(node.text)\n\n                                if tag in [""trackid""]:\n                                    image_single_class.frame= int(os.path.basename(file_name).replace(\'.xml\', \'\'))\n\n                                if tag in [\'name\']:\n                                    if str(vid_classes.code_to_class_string(str(node.text))) in [""nothing""]: \n                                        jump = 1\n                                    if str(vid_classes.code_to_class_string(str(node.text))) is not class_name:\n                                        jump = 1\n                                    else : \n                                        jump=0\n                                        rectangle_single.label_chall=int(vid_classes.class_string_to_comp_code(str(vid_classes.code_to_class_string(str(node.text)))))\n                                        rectangle_single.label_code=str(node.text)\n                                        rectangle_single.label=vid_classes.code_to_class_string(str(node.text))\n                                if tag in [""xmax""]:\n                                    if jump == 0:\n                                        rectangle_single.x2=float(utils_image.transform_point(image_single_class.width,image_single_class.height,width, height,float(node.text),True))\n                                if tag in [""xmin""]:\n                                    if jump == 0:\n                                        rectangle_single.x1=float(utils_image.transform_point(image_single_class.width,image_single_class.height,width, height,float(node.text),True))\n                                if tag in [""ymin""]:\n                                    if jump == 0:\n                                        rectangle_single.y1=float(utils_image.transform_point(image_single_class.width,image_single_class.height,width, height,float(node.text),False))\n                                if tag in [""ymax""]:\n                                    if jump == 0:    \n                                        rectangle_single.y2=float(utils_image.transform_point(image_single_class.width,image_single_class.height,width, height,float(node.text),False))\n                                        image_single_class.append_rect(rectangle_single)\n                                        count_rect=count_rect+1\n\n                            if jump == 0:\n                                \n                                count_img=count_img+1\n                                out_stream = open(path_bb_file, ""a"")\n                                out_stream.write(image_single_class.get_info_string()+ os.linesep)\n                                \n                                out_stream = open(path_class_code_file, ""a"")\n                                out_stream.write(image_single_class.get_rects_chall()+ os.linesep)\n                                \n                                out_stream = open(path_class_name_file, ""a"")\n                                out_stream.write(image_single_class.get_rects_labels()+ os.linesep)\n                                \n                                out_stream = open(path_chall_code_file, ""a"")\n                                out_stream.write(image_single_class.get_rects_code() + os.linesep)\n\n                            break\n            summary.write( (""Ended with Success Process for class:%s""%class_name )+os.linesep +""SUMMARY:"" + os.linesep+ (""Parsed: %d BB Files""%len(bb_XML_file_list)) + os.linesep+ (""Added: %d Object Rectangles""%count_rect) + os.linesep+(""Added: %d Images""%count_img)+os.linesep)\n            print ""Ended with Success Process for class:%s""%class_name \n            print ""SUMMARY:""\n            print ""Parsed: %d BB Files""%len(bb_XML_file_list)\n            print ""Added: %d Object Rectangles""%count_rect\n            print ""Added: %d Images""%count_img\n            if count_img>0: \n                print ""Ratio BB_Files/Images: %.2f""%(float(count_img)/float(len(bb_XML_file_list)))\n                summary.write((""Ratio BB_Files/Images: %.2f""%(float(count_img)/float(len(bb_XML_file_list))))+os.linesep)\n            if count_rect>0:\n                print ""Ratio Images/Object_Rectangles: %.2f""%(float(count_img)/float(count_rect))\n                summary.write((""Ratio Images/Object_Rectangles: %.2f""%(float(count_img)/float(count_rect)))+os.linesep)\n\n\n\ndef main():\n    \'\'\'\n    Parse command line arguments and execute the code\n\n    \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset_path\', default=\'./dataset\', type=str)\n    parser.add_argument(\'--bb_folder\', required=True, type=str)\n    parser.add_argument(\'--val_folder\', required=True, type=str)\n    args = parser.parse_args()\n\n    \n    start = time.time()\n\n    bb_XML_file_list=[]\n    create_summary_files(args.dataset_path)\n    bb_XML_file_list= Utils.get_Files_List(args.bb_folder)\n    parse_XML_to_multiclass_txt(bb_XML_file_list, args.val_folder, args.dataset_path)\n    parse_XML_to_singleclass_txt(bb_XML_file_list, args.val_folder, args.dataset_path)\n\n    # bb_list=[]\n    # path_dataset=\'./dataset\'\n    # path_bb_folder = \'./Annotations/VID/val\'\n    # path_val_folder = \'./Data/VID/val\'\n    # create_summary_files(path_dataset)\n    # bb_list= Utils.get_Files_List(path_bb_folder)\n    # parse_XML_to_multiclass_txt(bb_list, path_val_folder, path_dataset)\n    # parse_XML_to_singleclass_txt(bb_list, path_val_folder, path_dataset)\n    \n\n    end = time.time()\n\n    print(""Elapsed Time:%d Seconds""%(end-start))\n    print(""Running Completed with Success!!!"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
dataset_scripts/process_dataset_lightweight.py,0,"b'\nfrom xml.etree import ElementTree\nimport os\nimport shutil\nimport argparse\nimport time\nfrom PIL import Image, ImageChops\nimport progressbar\nfrom frame import Frame_Info\nfrom multiclass_rectangle import Rectangle_Multiclass\nimport utils_image\nimport Utils\nimport vid_classes\nfrom vid_classes import Classes_List as CL\n\n##### GENERAL VARIABLES #######\n\n##MULTICLASS NEW DATASET\n\nstring_mltcl_bb_file = \'mltcl_bb_file_list.txt\'\nstring_mltcl_class_code_file = \'mltcl_class_code_file_list.txt\'\nstring_mltcl_class_name_file = \'mltcl_class_name_file_list.txt\'\nstring_mltcl_chall_code_file = \'mltcl_chall_code_file_list.txt\'\n\n##SINGLE CLASS NEW DATASET\n\nstring_bb_file = \'_bb_file_list.txt\'\nstring_class_code_file = \'_class_code_file_list.txt\'\nstring_class_name_file = \'_class_name_file_list.txt\'\nstring_chall_code_file = \'_chall_code_file_list.txt\'\n\n## SIZE VARIABLES\n\nwidth=640\nheight=480\n\n##### GENERAL FUNCTIONS\n\ndef create_summary_files(path_dataset): #Create All the files needed for the New Dataset\n    \n    path_mltcl_bb_file= path_dataset+\'/\'+string_mltcl_bb_file\n    path_mltcl_class_code_file= path_dataset+\'/\'+string_mltcl_class_code_file\n    path_mltcl_class_name_file= path_dataset+\'/\'+string_mltcl_class_name_file\n    path_mltcl_chall_code_file= path_dataset+\'/\'+string_mltcl_chall_code_file\n\n    if not os.path.exists(path_dataset):\n        os.makedirs(path_dataset)\n        print(""Created Folder: %s""%path_dataset)\n    if not os.path.exists(path_mltcl_bb_file):\n        open(path_mltcl_bb_file, \'a\')\n        print ""Created File: ""+ path_mltcl_bb_file\n    if not os.path.exists(path_mltcl_class_code_file):\n        open(path_mltcl_class_code_file, \'a\')\n        print ""Created File: ""+ path_mltcl_class_code_file\n    if not os.path.exists(path_mltcl_class_name_file):\n        open(path_mltcl_class_name_file, \'a\')\n        print ""Created File: ""+ path_mltcl_class_name_file\n    if not os.path.exists(path_mltcl_chall_code_file):\n        open(path_mltcl_chall_code_file, \'a\')\n        print ""Created File: ""+ path_mltcl_chall_code_file\n    \n    for class_name in CL.class_name_string_list:\n\n\n        path_bb_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_bb_file\n        path_class_code_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_class_code_file\n        path_class_name_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_class_name_file\n        path_chall_code_file= path_dataset+\'/\'+class_name+\'/\'+class_name+string_chall_code_file\n        \n\n        if not os.path.exists(path_dataset+\'/\'+class_name):\n            os.makedirs(path_dataset+\'/\'+class_name)\n            print(""Created Folder: %s""%(path_dataset+\'/\'+class_name))\n        \n        if not os.path.exists(path_bb_file):\n            open(path_bb_file, \'a\')\n            print ""Created File: ""+ path_bb_file\n        if not os.path.exists(path_class_code_file):\n            open(path_class_code_file, \'a\')\n            print ""Created File: ""+ path_class_code_file\n        if not os.path.exists(path_class_name_file):\n            open(path_class_name_file, \'a\')\n            print ""Created File: ""+ path_class_name_file\n        if not os.path.exists(path_chall_code_file):\n            open(path_chall_code_file, \'a\')\n            print ""Created File: ""+ path_chall_code_file\n\ndef parse_XML_lightweight_txt(bb_XML_file_list, path_val_folder, path_dataset):\n\n    count_rect = 0\n    count_img = 0\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n\n    print ""Start Processing & Building Dataset... may take a while...""\n\n    path_mltcl_bb_file=path_dataset+\'/\'+string_mltcl_bb_file # Create this file in .dataset/airplane/airplane_bb_mltcl_file_list.txt\n    path_mltcl_class_code_file=path_dataset+\'/\'+string_mltcl_class_code_file\n    path_mltcl_class_name_file=path_dataset+\'/\'+string_mltcl_class_name_file\n    path_mltcl_chall_code_file=path_dataset+\'/\'+string_mltcl_chall_code_file\n\n    for file_name in progress(bb_XML_file_list):\n        with open(file_name, \'rt\') as f:\n            tree = ElementTree.parse(f)\n            for obj in tree.findall(\'object\'):\n                name = obj.find(\'name\').text\n                class_code= name\n                name = vid_classes.code_to_class_string(name)\n\n                if name in [""nothing""]:\n                    continue\n                else:\n                    \n                    same_label=0\n                    #The files with the original data path are made in both: multiclass e single class\n                    \n                    \n                    path_bb_file=path_dataset+\'/\'+name+\'/\'+ name+string_bb_file\n                    path_class_code_file= path_dataset+\'/\'+name+\'/\'+name+string_class_code_file\n                    path_class_name_file= path_dataset+\'/\'+name+\'/\'+name+string_class_name_file\n                    path_chall_code_file= path_dataset+\'/\'+name+\'/\'+name+string_chall_code_file\n\n\n                    path_orig_file=path_val_folder\n\n                    \n                    jump=0\n                    \n                    image_single_class= Frame_Info()\n                    image_single_class.dataset_path= path_val_folder\n\n                    image_multi_class= Frame_Info()\n                    image_multi_class.dataset_path= path_val_folder\n\n\n                    rectangle_single= Rectangle_Multiclass()\n                    rectangle_multi= Rectangle_Multiclass()\n                    \n                    #xmin x1 letf\n                    #ymin y1 bottom\n                    #xmax x2 right\n                    #ymax y2 top\n                \n                    for node in tree.iter():\n                        tag=str(node.tag)\n            \n                        if tag in [""folder""]:\n                            path_orig_file=path_orig_file+\'/\'+str(node.text)\n                            image_single_class.folder= str(node.text)                            \n                            image_multi_class.folder= str(node.text)\n\n                        if tag in [""filename""]:\n                            image_single_class.filename=str(node.text)+\'.PNG\'\n                            image_multi_class.filename=str(node.text)+\'.PNG\'\n\n                            path_orig_file=path_orig_file+\'/\'+str(node.text)+\'.JPEG\'\n\n                        if tag in [\'name\']:\n                            if str(vid_classes.code_to_class_string(str(node.text))) in [""nothing""]:\n                                jump = 1\n                            else : \n                                jump=0\n                                rectangle_multi.label_chall=int(vid_classes.class_string_to_comp_code(str(vid_classes.code_to_class_string(str(node.text)))))\n                                rectangle_multi.label_code=str(node.text)\n                                rectangle_multi.label=vid_classes.code_to_class_string(str(node.text))\n\n                                if str(node.text) == class_code: \n                                    same_label = 1\n                                    rectangle_single.label_chall=int(vid_classes.class_string_to_comp_code(str(vid_classes.code_to_class_string(str(node.text)))))\n                                    rectangle_single.label_code=str(node.text)\n                                    rectangle_single.label=vid_classes.code_to_class_string(str(node.text))\n                                else: same_label = 0\n                                \n                        if tag in [""xmax""]:\n                            if jump == 0:\n                                rectangle_multi.x2=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n                                if same_label==1:\n                                    rectangle_single.x2=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n                        if tag in [""xmin""]:\n                            if jump == 0:\n                                rectangle_multi.x1=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n                                if same_label==1:\n                                    rectangle_single.x1=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n                        if tag in [""ymax""]:\n                            if jump == 0:\n                                rectangle_multi.y2=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))                            \n                                image_multi_class.append_rect(rectangle_multi) \n                                count_rect=count_rect+1\n                                if same_label==1:\n                                    rectangle_single.y2=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n                                    image_single_class.append_rect(rectangle_single)\n                        if tag in [""ymin""]:\n                            if jump == 0:    \n                                rectangle_multi.y1=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n                                if same_label==1:\n                                    rectangle_single.y1=float(utils_image.transform_point(image_multi_class.width,image_multi_class.height,width, height,float(node.text), False))\n\n                    if jump == 0:\n                        \n                        count_img=count_img+1\n\n                        out_stream = open(path_mltcl_bb_file, ""a"")\n                        out_stream.write(image_multi_class.get_info_string()+ os.linesep)\n                        \n                        out_stream = open(path_mltcl_class_code_file, ""a"")\n                        out_stream.write(image_multi_class.get_rects_code()+ os.linesep)\n                        \n                        out_stream = open(path_mltcl_class_name_file, ""a"")\n                        out_stream.write(image_multi_class.get_rects_labels()+ os.linesep)\n                        \n                        out_stream = open(path_mltcl_chall_code_file, ""a"")\n                        out_stream.write(image_multi_class.get_rects_chall() + os.linesep)\n\n                        if same_label==1:\n                            out_stream = open(path_bb_file, ""a"")\n                            out_stream.write(image_single_class.get_info_string()+ os.linesep)\n                            \n                            out_stream = open(path_class_code_file, ""a"")\n                            out_stream.write(image_single_class.get_rects_chall()+ os.linesep)\n                            \n                            out_stream = open(path_class_name_file, ""a"")\n                            out_stream.write(image_single_class.get_rects_labels()+ os.linesep)\n                            \n                            out_stream = open(path_chall_code_file, ""a"")\n                            out_stream.write(image_single_class.get_rects_code() + os.linesep)\n                    break\n    print ""SUMMARY:""\n    print ""Parsed: %d BB Files""%len(bb_XML_file_list)\n    print ""Added: %d Object Rectangles""%count_rect\n    print ""Added: %d Images""%count_img\n    if count_img>0: \n        print ""Ratio BB_Files/Images: %.2f""%(float(count_img)/float(len(bb_XML_file_list)))\n    if count_rect>0:\n        print ""Ratio Images/Object_Rectangles: %.2f""%(float(count_img)/float(count_rect))\n\ndef main():\n    \'\'\'\n    Parse command line arguments and execute the code\n\n    \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset_path\', default=\'./dataset\', type=str)\n    parser.add_argument(\'--bb_folder\', required=True, type=str)\n    parser.add_argument(\'--val_folder\', required=True, type=str)\n    args = parser.parse_args()\n\n    \n    start = time.time()\n\n    bb_XML_file_list=[]\n    create_summary_files(args.dataset_path)\n    bb_XML_file_list= Utils.get_Files_List(args.bb_folder)\n    parse_XML_lightweight_txt(bb_XML_file_list, args.val_folder, args.dataset_path)  \n\n    end = time.time()\n\n    print(""Elapsed Time:%d Seconds""%(end-start))\n    print(""Running Completed with Success!!!"")\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n'"
dataset_scripts/resize_dataset.py,0,"b'\nimport os\nimport time\nfrom PIL import Image, ImageChops\nimport progressbar\nimport argparse\nimport utils_image\n\n##### MAIN ############\n\ndef main():\n    \'\'\'\n    Parse command line arguments and execute the code\n    \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset_path\', required=True, type=str)\n    parser.add_argument(\'--newext\', default=\'.PNG\', type=str)\n    parser.add_argument(\'--oldext\', default=\'.JPEG\', type=str)\n    args = parser.parse_args()\n\n    start = time.time()\n\n    image_list= utils_image.get_Image_List(args.dataset_path, args.oldext)\n\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n\n    print ""Start Processing... May take a while...""\n\n    for image_path in progress(image_list):\n        utils_image.resizeImage(image_path)\n        utils_image.change_extension(image_path,args.oldext,args.newext)\n \n    end = time.time()\n    print(""Parsed: %d Image of the Dataset""%(len(image_list)))\n    print(""Elapsed Time:%d Seconds""%(end-start))\n    print(""Running Completed with Success!!!"")\n\n\nif __name__ == \'__main__\':\n    main()'"
dataset_scripts/test_processed_dataset.py,0,"b'#### Import from Tensorbox Project\n\nfrom utils.annolist import AnnotationLib as al\n\n#### My import\n\nfrom PIL import Image, ImageChops,ImageDraw\nimport progressbar\nimport time\nimport cv2\nimport os\nimport argparse\nimport sys\n\n######### PARAMETERS\n\ndef test_IDL(idl_filename):\n\n    print(""Starting Testing Dataset... May take a while"")\n    progress = progressbar.ProgressBar(widgets=[progressbar.Bar(\'=\', \'[\', \']\'), \' \',progressbar.Percentage(), \' \',progressbar.ETA()])\n\n    test_annos = al.parse(idl_filename)\n    for test_anno in progress(test_annos):\n        bb_img = Image.open(test_anno.imageName)\n        orig_img = cv2.imread(test_anno.imageName, 0)\n        cv2.imshow(\'Original Image\',orig_img)\n        cv2.waitKey(2)\n        for test_rect in test_anno.rects:\n            dr = ImageDraw.Draw(bb_img)\n            cor = (test_rect.x2,test_rect.y2,test_rect.x1,test_rect.y1 ) # DA VERIFICARE Try_2 (x1,y1, x2,y2) cor = (bb_rect.left() ,bb_rect.right(),bb_rect.bottom(),bb_rect.top()) Try_1\n            dr.rectangle(cor, outline=""green"")   \n        image_name, image_ext = os.path.splitext(test_anno.imageName) \n        bb_img.save(image_name+\'_copy\'+image_ext)\n        bb_img = cv2.imread(image_name+\'_copy\'+image_ext, 0)\n        cv2.imshow(\'Mine Rectangle detection\',bb_img)\n        cv2.waitKey(2)\n        os.remove(image_name+\'_copy\'+image_ext)\n\n\n######### MAIN ###############\n\n\ndef main():\n    \'\'\'\n    Parse command line arguments and execute the code \n\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--idl\', required=True, type=str)\n    args = parser.parse_args()\n\n    start = time.time()\n\n    test_IDL(args.idl)\n\n    end = time.time()\n\n    print(""Elapsed Time:%d Seconds""%(end-start))\n    print(""Running Completed with Success!!!"")\n\nif __name__ == \'__main__\':\n    main()\n\n\n'"
INCEPTION/image_retraining/__init__.py,0,b''
INCEPTION/image_retraining/retrain.py,100,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Simple transfer learning with an Inception v3 architecture model which\ndisplays summaries in TensorBoard.\n\nThis example shows how to take a Inception v3 architecture model trained on\nImageNet images, and train a new top layer that can recognize other classes of\nimages.\n\nThe top layer receives as input a 2048-dimensional vector for each image. We\ntrain a softmax layer on top of this representation. Assuming the softmax layer\ncontains N labels, this corresponds to learning N + 2048*N model parameters\ncorresponding to the learned biases and weights.\n\nHere\'s an example, which assumes you have a folder containing class-named\nsubfolders, each full of images for each label. The example folder flower_photos\nshould have a structure like this:\n\n~/flower_photos/daisy/photo1.jpg\n~/flower_photos/daisy/photo2.jpg\n...\n~/flower_photos/rose/anotherphoto77.jpg\n...\n~/flower_photos/sunflower/somepicture.jpg\n\nThe subfolder names are important, since they define what label is applied to\neach image, but the filenames themselves don\'t matter. Once your images are\nprepared, you can run the training with a command like this:\n\nbazel build third_party/tensorflow/examples/image_retraining:retrain && \\\nbazel-bin/third_party/tensorflow/examples/image_retraining/retrain \\\n--image_dir ~/flower_photos\n\nYou can replace the image_dir argument with any folder containing subfolders of\nimages. The label for each image is taken from the name of the subfolder it\'s\nin.\n\nThis produces a new model file that can be loaded and run by any TensorFlow\nprogram, for example the label_image sample code.\n\n\nTo use with TensorBoard:\n\nBy default, this script will log summaries to /tmp/retrain_logs directory\n\nVisualize the summaries with this command:\n\ntensorboard --logdir /tmp/retrain_logs\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport glob\nimport hashlib\nimport os.path\nimport random\nimport re\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom tensorflow.python.client import graph_util\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.platform import gfile\nif(int(tf.__version__.split(""."")[0])<1): ### For tf v<1.0\n  scalar_summary = tf.scalar_summary()\nelse: ### For tf v>=1.0 \n  scalar_summary = tf.summary.scalar()\n\nimport struct\n\nFLAGS = tf.app.flags.FLAGS\n\n# Input and output file flags.\ntf.app.flags.DEFINE_string(\'image_dir\', \'\',\n                           """"""Path to folders of labeled images."""""")\ntf.app.flags.DEFINE_string(\'output_graph\', \'/tmp/output_graph.pb\',\n                           """"""Where to save the trained graph."""""")\ntf.app.flags.DEFINE_string(\'output_labels\', \'/tmp/output_labels.txt\',\n                           """"""Where to save the trained graph\'s labels."""""")\ntf.app.flags.DEFINE_string(\'summaries_dir\', \'/tmp/retrain_logs\',\n                          """"""Where to save summary logs for TensorBoard."""""")\n\n# Details of the training configuration.\ntf.app.flags.DEFINE_integer(\'how_many_training_steps\', 500000,\n                            """"""How many training steps to run before ending."""""")\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.001,\n                          """"""How large a learning rate to use when training."""""")\ntf.app.flags.DEFINE_integer(\n    \'testing_percentage\', 10,\n    """"""What percentage of images to use as a test set."""""")\ntf.app.flags.DEFINE_integer(\n    \'validation_percentage\', 10,\n    """"""What percentage of images to use as a validation set."""""")\ntf.app.flags.DEFINE_integer(\'eval_step_interval\', 10,\n                            """"""How often to evaluate the training results."""""")\ntf.app.flags.DEFINE_integer(\'train_batch_size\', 500,\n                            """"""How many images to train on at a time."""""")\ntf.app.flags.DEFINE_integer(\'test_batch_size\', 100,\n                            """"""How many images to test on at a time. This""""""\n                            """""" test set is only used infrequently to verify""""""\n                            """""" the overall accuracy of the model."""""")\ntf.app.flags.DEFINE_integer(\n    \'validation_batch_size\', 100,\n    """"""How many images to use in an evaluation batch. This validation set is""""""\n    """""" used much more often than the test set, and is an early indicator of""""""\n    """""" how accurate the model is during training."""""")\n\n# File-system cache locations.\ntf.app.flags.DEFINE_string(\'model_dir\', \'/tmp/imagenet\',\n                           """"""Path to classify_image_graph_def.pb, """"""\n                           """"""imagenet_synset_to_human_label_map.txt, and """"""\n                           """"""imagenet_2012_challenge_label_map_proto.pbtxt."""""")\ntf.app.flags.DEFINE_string(\n    \'bottleneck_dir\', \'/tmp/bottleneck\',\n    """"""Path to cache bottleneck layer values as files."""""")\ntf.app.flags.DEFINE_string(\'final_tensor_name\', \'final_result\',\n                           """"""The name of the output classification layer in""""""\n                           """""" the retrained graph."""""")\n\n# Controls the distortions used during training.\ntf.app.flags.DEFINE_boolean(\n    \'flip_left_right\', False,\n    """"""Whether to randomly flip half of the training images horizontally."""""")\ntf.app.flags.DEFINE_integer(\n    \'random_crop\', 0,\n    """"""A percentage determining how much of a margin to randomly crop off the""""""\n    """""" training images."""""")\ntf.app.flags.DEFINE_integer(\n    \'random_scale\', 0,\n    """"""A percentage determining how much to randomly scale up the size of the""""""\n    """""" training images by."""""")\ntf.app.flags.DEFINE_integer(\n    \'random_brightness\', 0,\n    """"""A percentage determining how much to randomly multiply the training""""""\n    """""" image input pixels up or down by."""""")\n\n# These are all parameters that are tied to the particular model architecture\n# we\'re using for Inception v3. These include things like tensor names and their\n# sizes. If you want to adapt this script to work with another model, you will\n# need to update these to reflect the values in the network you\'re using.\n# pylint: disable=line-too-long\nDATA_URL = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\n# pylint: enable=line-too-long\nBOTTLENECK_TENSOR_NAME = \'pool_3/_reshape:0\'\nBOTTLENECK_TENSOR_SIZE = 2048\nMODEL_INPUT_WIDTH = 299\nMODEL_INPUT_HEIGHT = 299\nMODEL_INPUT_DEPTH = 3\nJPEG_DATA_TENSOR_NAME = \'DecodeJpeg/contents:0\'\nRESIZED_INPUT_TENSOR_NAME = \'ResizeBilinear:0\'\n\n\ndef create_image_lists(image_dir, testing_percentage, validation_percentage):\n  """"""Builds a list of training images from the file system.\n\n  Analyzes the sub folders in the image directory, splits them into stable\n  training, testing, and validation sets, and returns a data structure\n  describing the lists of images for each label and their paths.\n\n  Args:\n    image_dir: String path to a folder containing subfolders of images.\n    testing_percentage: Integer percentage of the images to reserve for tests.\n    validation_percentage: Integer percentage of images reserved for validation.\n\n  Returns:\n    A dictionary containing an entry for each label subfolder, with images split\n    into training, testing, and validation sets within each label.\n  """"""\n  if not gfile.Exists(image_dir):\n    print(""Image directory \'"" + image_dir + ""\' not found."")\n    return None\n  result = {}\n  sub_dirs = [x[0] for x in os.walk(image_dir)]\n  # The root directory comes first, so skip it.\n  is_root_dir = True\n  for sub_dir in sub_dirs:\n    if is_root_dir:\n      is_root_dir = False\n      continue\n    extensions = [\'jpg\', \'jpeg\', \'JPG\', \'JPEG\']\n    file_list = []\n    dir_name = os.path.basename(sub_dir)\n    if dir_name == image_dir:\n      continue\n    print(""Looking for images in \'"" + dir_name + ""\'"")\n    for extension in extensions:\n      file_glob = os.path.join(image_dir, dir_name, \'*.\' + extension)\n      file_list.extend(glob.glob(file_glob))\n    if not file_list:\n      print(\'No files found\')\n      continue\n    if len(file_list) < 20:\n      print(\'WARNING: Folder has less than 20 images, which may cause issues.\')\n    label_name = re.sub(r\'[^a-z0-9]+\', \' \', dir_name.lower())\n    training_images = []\n    testing_images = []\n    validation_images = []\n    for file_name in file_list:\n      base_name = os.path.basename(file_name)\n      # We want to ignore anything after \'_nohash_\' in the file name when\n      # deciding which set to put an image in, the data set creator has a way of\n      # grouping photos that are close variations of each other. For example\n      # this is used in the plant disease data set to group multiple pictures of\n      # the same leaf.\n      hash_name = re.sub(r\'_nohash_.*$\', \'\', file_name)\n      # This looks a bit magical, but we need to decide whether this file should\n      # go into the training, testing, or validation sets, and we want to keep\n      # existing files in the same set even if more files are subsequently\n      # added.\n      # To do that, we need a stable way of deciding based on just the file name\n      # itself, so we do a hash of that and then use that to generate a\n      # probability value that we use to assign it.\n      hash_name_hashed = hashlib.sha1(hash_name.encode(\'utf-8\')).hexdigest()\n      percentage_hash = (int(hash_name_hashed, 16) % (65536)) * (100 / 65535.0)\n      if percentage_hash < validation_percentage:\n        validation_images.append(base_name)\n      elif percentage_hash < (testing_percentage + validation_percentage):\n        testing_images.append(base_name)\n      else:\n        training_images.append(base_name)\n    result[label_name] = {\n        \'dir\': dir_name,\n        \'training\': training_images,\n        \'testing\': testing_images,\n        \'validation\': validation_images,\n    }\n  return result\n\n\ndef get_image_path(image_lists, label_name, index, image_dir, category):\n  """"""""Returns a path to an image for a label at the given index.\n\n  Args:\n    image_lists: Dictionary of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Int offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.\n\n  """"""\n  if label_name not in image_lists:\n    tf.logging.fatal(\'Label does not exist %s.\', label_name)\n  label_lists = image_lists[label_name]\n  if category not in label_lists:\n    tf.logging.fatal(\'Category does not exist %s.\', category)\n  category_list = label_lists[category]\n  if not category_list:\n    tf.logging.fatal(\'Category has no images - %s.\', category)\n  mod_index = index % len(category_list)\n  base_name = category_list[mod_index]\n  sub_dir = label_lists[\'dir\']\n  full_path = os.path.join(image_dir, sub_dir, base_name)\n  return full_path\n\n\ndef get_bottleneck_path(image_lists, label_name, index, bottleneck_dir,\n                        category):\n  """"""""Returns a path to a bottleneck file for a label at the given index.\n\n  Args:\n    image_lists: Dictionary of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Integer offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.\n  """"""\n  return get_image_path(image_lists, label_name, index, bottleneck_dir,\n                        category) + \'.txt\'\n\n\ndef create_inception_graph():\n  """"""""Creates a graph from saved GraphDef file and returns a Graph object.\n\n  Returns:\n    Graph holding the trained Inception network, and various tensors we\'ll be\n    manipulating.\n  """"""\n  with tf.Session() as sess:\n    model_filename = os.path.join(\n        FLAGS.model_dir, \'classify_image_graph_def.pb\')\n    with gfile.FastGFile(model_filename, \'rb\') as f:\n      graph_def = tf.GraphDef()\n      graph_def.ParseFromString(f.read())\n      bottleneck_tensor, jpeg_data_tensor, resized_input_tensor = (\n          tf.import_graph_def(graph_def, name=\'\', return_elements=[\n              BOTTLENECK_TENSOR_NAME, JPEG_DATA_TENSOR_NAME,\n              RESIZED_INPUT_TENSOR_NAME]))\n  return sess.graph, bottleneck_tensor, jpeg_data_tensor, resized_input_tensor\n\n\ndef run_bottleneck_on_image(sess, image_data, image_data_tensor,\n                            bottleneck_tensor):\n  """"""Runs inference on an image to extract the \'bottleneck\' summary layer.\n\n  Args:\n    sess: Current active TensorFlow Session.\n    image_data: Numpy array of image data.\n    image_data_tensor: Input data layer in the graph.\n    bottleneck_tensor: Layer before the final softmax.\n\n  Returns:\n    Numpy array of bottleneck values.\n  """"""\n  bottleneck_values = sess.run(\n      bottleneck_tensor,\n      {image_data_tensor: image_data})\n  bottleneck_values = np.squeeze(bottleneck_values)\n  return bottleneck_values\n\n\ndef maybe_download_and_extract():\n  """"""Download and extract model tar file.\n\n  If the pretrained model we\'re using doesn\'t already exist, this function\n  downloads it from the TensorFlow.org website and unpacks it into a directory.\n  """"""\n  dest_directory = FLAGS.model_dir\n  if not os.path.exists(dest_directory):\n    os.makedirs(dest_directory)\n  filename = DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dest_directory, filename)\n  if not os.path.exists(filepath):\n\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' %\n                       (filename,\n                        float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n\n    filepath, _ = urllib.request.urlretrieve(DATA_URL,\n                                             filepath,\n                                             _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dest_directory)\n\n\ndef ensure_dir_exists(dir_name):\n  """"""Makes sure the folder exists on disk.\n\n  Args:\n    dir_name: Path string to the folder we want to create.\n  """"""\n  if not os.path.exists(dir_name):\n    os.makedirs(dir_name)\n\n\ndef write_list_of_floats_to_file(list_of_floats , file_path):\n  """"""Writes a given list of floats to a binary file.\n\n  Args:\n    list_of_floats: List of floats we want to write to a file.\n    file_path: Path to a file where list of floats will be stored.\n\n  """"""\n\n  s = struct.pack(\'d\' * BOTTLENECK_TENSOR_SIZE, *list_of_floats)\n  with open(file_path, \'wb\') as f:\n    f.write(s)\n\n\ndef read_list_of_floats_from_file(file_path):\n  """"""Reads list of floats from a given file.\n\n  Args:\n    file_path: Path to a file where list of floats was stored.\n  Returns:\n    Array of bottleneck values (list of floats).\n\n  """"""\n\n  with open(file_path, \'rb\') as f:\n    s = struct.unpack(\'d\' * BOTTLENECK_TENSOR_SIZE, f.read())\n    return list(s)\n\n\nbottleneck_path_2_bottleneck_values = {}\n\n\ndef get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir,\n                             category, bottleneck_dir, jpeg_data_tensor,\n                             bottleneck_tensor):\n  """"""Retrieves or calculates bottleneck values for an image.\n\n  If a cached version of the bottleneck data exists on-disk, return that,\n  otherwise calculate the data and save it to disk for future use.\n\n  Args:\n    sess: The current active TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Integer offset of the image we want. This will be modulo-ed by the\n    available number of images for the label, so it can be arbitrarily large.\n    image_dir: Root folder string  of the subfolders containing the training\n    images.\n    category: Name string of which  set to pull images from - training, testing,\n    or validation.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    jpeg_data_tensor: The tensor to feed loaded jpeg data into.\n    bottleneck_tensor: The output tensor for the bottleneck values.\n\n  Returns:\n    Numpy array of values produced by the bottleneck layer for the image.\n  """"""\n  label_lists = image_lists[label_name]\n  sub_dir = label_lists[\'dir\']\n  sub_dir_path = os.path.join(bottleneck_dir, sub_dir)\n  ensure_dir_exists(sub_dir_path)\n  bottleneck_path = get_bottleneck_path(image_lists, label_name, index,\n                                        bottleneck_dir, category)\n  if not os.path.exists(bottleneck_path):\n    print(\'Creating bottleneck at \' + bottleneck_path)\n    image_path = get_image_path(image_lists, label_name, index, image_dir,\n                                category)\n    if not gfile.Exists(image_path):\n      tf.logging.fatal(\'File does not exist %s\', image_path)\n    image_data = gfile.FastGFile(image_path, \'rb\').read()\n    bottleneck_values = run_bottleneck_on_image(sess, image_data,\n                                                jpeg_data_tensor,\n                                                bottleneck_tensor)\n    bottleneck_string = \',\'.join(str(x) for x in bottleneck_values)\n    with open(bottleneck_path, \'w\') as bottleneck_file:\n      bottleneck_file.write(bottleneck_string)\n\n  with open(bottleneck_path, \'r\') as bottleneck_file:\n    bottleneck_string = bottleneck_file.read()\n  bottleneck_values = [float(x) for x in bottleneck_string.split(\',\')]\n  return bottleneck_values\n\n\ndef cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir,\n                      jpeg_data_tensor, bottleneck_tensor):\n  """"""Ensures all the training, testing, and validation bottlenecks are cached.\n\n  Because we\'re likely to read the same image multiple times (if there are no\n  distortions applied during training) it can speed things up a lot if we\n  calculate the bottleneck layer values once for each image during\n  preprocessing, and then just read those cached values repeatedly during\n  training. Here we go through all the images we\'ve found, calculate those\n  values, and save them off.\n\n  Args:\n    sess: The current active TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    jpeg_data_tensor: Input tensor for jpeg data from file.\n    bottleneck_tensor: The penultimate output layer of the graph.\n\n  Returns:\n    Nothing.\n  """"""\n  how_many_bottlenecks = 0\n  ensure_dir_exists(bottleneck_dir)\n  for label_name, label_lists in image_lists.items():\n    for category in [\'training\', \'testing\', \'validation\']:\n      category_list = label_lists[category]\n      for index, unused_base_name in enumerate(category_list):\n        get_or_create_bottleneck(sess, image_lists, label_name, index,\n                                 image_dir, category, bottleneck_dir,\n                                 jpeg_data_tensor, bottleneck_tensor)\n        how_many_bottlenecks += 1\n        if how_many_bottlenecks % 100 == 0:\n          print(str(how_many_bottlenecks) + \' bottleneck files created.\')\n\n\ndef get_random_cached_bottlenecks(sess, image_lists, how_many, category,\n                                  bottleneck_dir, image_dir, jpeg_data_tensor,\n                                  bottleneck_tensor):\n  """"""Retrieves bottleneck values for cached images.\n\n  If no distortions are being applied, this function can retrieve the cached\n  bottleneck values directly from disk for images. It picks a random set of\n  images from the specified category.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    how_many: The number of bottleneck values to return.\n    category: Name string of which set to pull from - training, testing, or\n    validation.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    jpeg_data_tensor: The layer to feed jpeg image data into.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n\n  Returns:\n    List of bottleneck arrays and their corresponding ground truths.\n  """"""\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  for unused_i in range(how_many):\n    label_index = random.randrange(class_count)\n    label_name = list(image_lists.keys())[label_index]\n    image_index = random.randrange(65536)\n    bottleneck = get_or_create_bottleneck(sess, image_lists, label_name,\n                                          image_index, image_dir, category,\n                                          bottleneck_dir, jpeg_data_tensor,\n                                          bottleneck_tensor)\n    ground_truth = np.zeros(class_count, dtype=np.float32)\n    ground_truth[label_index] = 1.0\n    bottlenecks.append(bottleneck)\n    ground_truths.append(ground_truth)\n  return bottlenecks, ground_truths\n\n\ndef get_random_distorted_bottlenecks(\n    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\n    distorted_image, resized_input_tensor, bottleneck_tensor):\n  """"""Retrieves bottleneck values for training images, after distortions.\n\n  If we\'re training with distortions like crops, scales, or flips, we have to\n  recalculate the full model for every image, and so we can\'t use cached\n  bottleneck values. Instead we find random images for the requested category,\n  run them through the distortion graph, and then the full graph to get the\n  bottleneck results for each.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    how_many: The integer number of bottleneck values to return.\n    category: Name string of which set of images to fetch - training, testing,\n    or validation.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    input_jpeg_tensor: The input layer we feed the image data to.\n    distorted_image: The output node of the distortion graph.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n\n  Returns:\n    List of bottleneck arrays and their corresponding ground truths.\n  """"""\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  for unused_i in range(how_many):\n    label_index = random.randrange(class_count)\n    label_name = list(image_lists.keys())[label_index]\n    image_index = random.randrange(65536)\n    image_path = get_image_path(image_lists, label_name, image_index, image_dir,\n                                category)\n    if not gfile.Exists(image_path):\n      tf.logging.fatal(\'File does not exist %s\', image_path)\n    jpeg_data = gfile.FastGFile(image_path, \'rb\').read()\n    # Note that we materialize the distorted_image_data as a numpy array before\n    # sending running inference on the image. This involves 2 memory copies and\n    # might be optimized in other implementations.\n    distorted_image_data = sess.run(distorted_image,\n                                    {input_jpeg_tensor: jpeg_data})\n    bottleneck = run_bottleneck_on_image(sess, distorted_image_data,\n                                         resized_input_tensor,\n                                         bottleneck_tensor)\n    ground_truth = np.zeros(class_count, dtype=np.float32)\n    ground_truth[label_index] = 1.0\n    bottlenecks.append(bottleneck)\n    ground_truths.append(ground_truth)\n  return bottlenecks, ground_truths\n\n\ndef should_distort_images(flip_left_right, random_crop, random_scale,\n                          random_brightness):\n  """"""Whether any distortions are enabled, from the input flags.\n\n  Args:\n    flip_left_right: Boolean whether to randomly mirror images horizontally.\n    random_crop: Integer percentage setting the total margin used around the\n    crop box.\n    random_scale: Integer percentage of how much to vary the scale by.\n    random_brightness: Integer range to randomly multiply the pixel values by.\n\n  Returns:\n    Boolean value indicating whether any distortions should be applied.\n  """"""\n  return (flip_left_right or (random_crop != 0) or (random_scale != 0) or\n          (random_brightness != 0))\n\n\ndef add_input_distortions(flip_left_right, random_crop, random_scale,\n                          random_brightness):\n  """"""Creates the operations to apply the specified distortions.\n\n  During training it can help to improve the results if we run the images\n  through simple distortions like crops, scales, and flips. These reflect the\n  kind of variations we expect in the real world, and so can help train the\n  model to cope with natural data more effectively. Here we take the supplied\n  parameters and construct a network of operations to apply them to an image.\n\n  Cropping\n  ~~~~~~~~\n\n  Cropping is done by placing a bounding box at a random position in the full\n  image. The cropping parameter controls the size of that box relative to the\n  input image. If it\'s zero, then the box is the same size as the input and no\n  cropping is performed. If the value is 50%, then the crop box will be half the\n  width and height of the input. In a diagram it looks like this:\n\n  <       width         >\n  +---------------------+\n  |                     |\n  |   width - crop%     |\n  |    <      >         |\n  |    +------+         |\n  |    |      |         |\n  |    |      |         |\n  |    |      |         |\n  |    +------+         |\n  |                     |\n  |                     |\n  +---------------------+\n\n  Scaling\n  ~~~~~~~\n\n  Scaling is a lot like cropping, except that the bounding box is always\n  centered and its size varies randomly within the given range. For example if\n  the scale percentage is zero, then the bounding box is the same size as the\n  input and no scaling is applied. If it\'s 50%, then the bounding box will be in\n  a random range between half the width and height and full size.\n\n  Args:\n    flip_left_right: Boolean whether to randomly mirror images horizontally.\n    random_crop: Integer percentage setting the total margin used around the\n    crop box.\n    random_scale: Integer percentage of how much to vary the scale by.\n    random_brightness: Integer range to randomly multiply the pixel values by.\n    graph.\n\n  Returns:\n    The jpeg input layer and the distorted result tensor.\n  """"""\n\n  jpeg_data = tf.placeholder(tf.string, name=\'DistortJPGInput\')\n  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=MODEL_INPUT_DEPTH)\n  decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\n  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n  margin_scale = 1.0 + (random_crop / 100.0)\n  resize_scale = 1.0 + (random_scale / 100.0)\n  margin_scale_value = tf.constant(margin_scale)\n  resize_scale_value = tf.random_uniform(tensor_shape.scalar(),\n                                         minval=1.0,\n                                         maxval=resize_scale)\n  scale_value = tf.mul(margin_scale_value, resize_scale_value)\n  precrop_width = tf.mul(scale_value, MODEL_INPUT_WIDTH)\n  precrop_height = tf.mul(scale_value, MODEL_INPUT_HEIGHT)\n  precrop_shape = tf.pack([precrop_height, precrop_width])\n  precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)\n  precropped_image = tf.image.resize_bilinear(decoded_image_4d,\n                                              precrop_shape_as_int)\n  precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[0])\n  cropped_image = tf.random_crop(precropped_image_3d,\n                                 [MODEL_INPUT_HEIGHT, MODEL_INPUT_WIDTH,\n                                  MODEL_INPUT_DEPTH])\n  if flip_left_right:\n    flipped_image = tf.image.random_flip_left_right(cropped_image)\n  else:\n    flipped_image = cropped_image\n  brightness_min = 1.0 - (random_brightness / 100.0)\n  brightness_max = 1.0 + (random_brightness / 100.0)\n  brightness_value = tf.random_uniform(tensor_shape.scalar(),\n                                       minval=brightness_min,\n                                       maxval=brightness_max)\n  brightened_image = tf.mul(flipped_image, brightness_value)\n  distort_result = tf.expand_dims(brightened_image, 0, name=\'DistortResult\')\n  return jpeg_data, distort_result\n\n\ndef variable_summaries(var, name):\n  """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n  with tf.name_scope(\'summaries\'):\n    mean = tf.reduce_mean(var)\n    scalar_summary(\'mean/\' + name, mean)\n    with tf.name_scope(\'stddev\'):\n      stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n    scalar_summary(\'sttdev/\' + name, stddev)\n    scalar_summary(\'max/\' + name, tf.reduce_max(var))\n    scalar_summary(\'min/\' + name, tf.reduce_min(var))\n    if(int(tf.__version__.split(""."")[0])<1): ### For tf v<1.0\n      tf.histogram_summary(name, var)\n    else: ### For tf v>=1.0 \n      tf.summary.histogram(name, var) \n\n\ndef add_final_training_ops(class_count, final_tensor_name, bottleneck_tensor):\n  """"""Adds a new softmax and fully-connected layer for training.\n\n  We need to retrain the top layer to identify our new classes, so this function\n  adds the right operations to the graph, along with some variables to hold the\n  weights, and then sets up all the gradients for the backward pass.\n\n  The set up for the softmax and fully-connected layers is based on:\n  https://tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\n\n  Args:\n    class_count: Integer of how many categories of things we\'re trying to\n    recognize.\n    final_tensor_name: Name string for the new final node that produces results.\n    bottleneck_tensor: The output of the main CNN graph.\n\n  Returns:\n    The tensors for the training and cross entropy results, and tensors for the\n    bottleneck input and ground truth input.\n  """"""\n  with tf.name_scope(\'input\'):\n    bottleneck_input = tf.placeholder_with_default(\n        bottleneck_tensor, shape=[None, BOTTLENECK_TENSOR_SIZE],\n        name=\'BottleneckInputPlaceholder\')\n\n    ground_truth_input = tf.placeholder(tf.float32,\n                                        [None, class_count],\n                                        name=\'GroundTruthInput\')\n\n  # Organizing the following ops as `final_training_ops` so they\'re easier\n  # to see in TensorBoard\n  layer_name = \'final_training_ops\'\n  with tf.name_scope(layer_name):\n    with tf.name_scope(\'weights\'):\n      layer_weights = tf.Variable(tf.truncated_normal([BOTTLENECK_TENSOR_SIZE, class_count], stddev=0.001), name=\'final_weights\')\n      variable_summaries(layer_weights, layer_name + \'/weights\')\n    with tf.name_scope(\'biases\'):\n      layer_biases = tf.Variable(tf.zeros([class_count]), name=\'final_biases\')\n      variable_summaries(layer_biases, layer_name + \'/biases\')\n    with tf.name_scope(\'Wx_plus_b\'):\n      logits = tf.matmul(bottleneck_input, layer_weights) + layer_biases\n      if(int(tf.__version__.split(""."")[0])<1): ### For tf v<1.0\n        tf.histogram_summary(layer_name + \'/pre_activations\', logits)\n      else: ### For tf v>=1.0 \n        tf.summary.histogram(layer_name + \'/pre_activations\', logits)\n\n  final_tensor = tf.nn.softmax(logits, name=final_tensor_name)\n  if(int(tf.__version__.split(""."")[0])<1): ### For tf v<1.0\n    tf.histogram_summary(final_tensor_name + \'/activations\', final_tensor)\n  else: ### For tf v>=1.0 \n    tf.summary.histogram(final_tensor_name + \'/activations\', final_tensor)\n \n  \n\n  with tf.name_scope(\'cross_entropy\'):\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n      logits, ground_truth_input)\n    with tf.name_scope(\'total\'):\n      cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    scalar_summary(\'cross entropy\', cross_entropy_mean)\n\n  with tf.name_scope(\'train\'):\n    train_step = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(\n        cross_entropy_mean)\n\n  return (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input,\n          final_tensor)\n\n\ndef add_evaluation_step(result_tensor, ground_truth_tensor):\n  """"""Inserts the operations we need to evaluate the accuracy of our results.\n\n  Args:\n    result_tensor: The new final node that produces results.\n    ground_truth_tensor: The node we feed ground truth data\n    into.\n\n  Returns:\n    Nothing.\n  """"""\n  with tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n      correct_prediction = tf.equal(tf.argmax(result_tensor, 1), \\\n        tf.argmax(ground_truth_tensor, 1))\n    with tf.name_scope(\'accuracy\'):\n      evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    scalar_summary(\'accuracy\', evaluation_step)\n  return evaluation_step\n\n\ndef main(_):\n  # Setup the directory we\'ll write summaries to for TensorBoard\n  if tf.gfile.Exists(FLAGS.summaries_dir):\n    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n\n  # Set up the pre-trained graph.\n  maybe_download_and_extract()\n  graph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor = (\n      create_inception_graph())\n\n  # Look at the folder structure, and create lists of all the images.\n  image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\n                                   FLAGS.validation_percentage)\n  class_count = len(image_lists.keys())\n  if class_count == 0:\n    print(\'No valid folders of images found at \' + FLAGS.image_dir)\n    return -1\n  if class_count == 1:\n    print(\'Only one valid folder of images found at \' + FLAGS.image_dir +\n          \' - multiple classes are needed for classification.\')\n    return -1\n\n  # See if the command-line flags mean we\'re applying any distortions.\n  do_distort_images = should_distort_images(\n      FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n      FLAGS.random_brightness)\n  sess = tf.Session()\n\n  if do_distort_images:\n    # We will be applying distortions, so setup the operations we\'ll need.\n    distorted_jpeg_data_tensor, distorted_image_tensor = add_input_distortions(\n        FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n        FLAGS.random_brightness)\n  else:\n    # We\'ll make sure we\'ve calculated the \'bottleneck\' image summaries and\n    # cached them on disk.\n    cache_bottlenecks(sess, image_lists, FLAGS.image_dir, FLAGS.bottleneck_dir,\n                      jpeg_data_tensor, bottleneck_tensor)\n\n  # Add the new layer that we\'ll be training.\n  (train_step, cross_entropy, bottleneck_input, ground_truth_input,\n   final_tensor) = add_final_training_ops(len(image_lists.keys()),\n                                          FLAGS.final_tensor_name,\n                                          bottleneck_tensor)\n\n  # Create the operations we need to evaluate the accuracy of our new layer.\n  evaluation_step = add_evaluation_step(final_tensor, ground_truth_input)\n\n  # Merge all the summaries and write them out to /tmp/retrain_logs (by default)\n  if(int(tf.__version__.split(""."")[0])<1): ### For tf v<1.0\n    merged = tf.merge_all_summaries()\n  else: ### For tf v>=1.0\n    merged = tf.summary.merge_all()\n  train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + \'/train\',\n                                        sess.graph)\n  validation_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + \'/validation\')\n\n  # Set up all our weights to their initial default values.\n  if(int(tf.__version__.split(""."")[0])==0 and int(tf.__version__.split(""."")[1])<12): ### for tf v<0.12.0\n    init = tf.initialize_all_variables()\n  else: ### for tf v>=0.12.0\n    init = tf.global_variables_initializer()\n  sess.run(init)\n\n  # Run the training for as many cycles as requested on the command line.\n  for i in range(FLAGS.how_many_training_steps):\n    # Get a catch of input bottleneck values, either calculated fresh every time\n    # with distortions applied, or from the cache stored on disk.\n    if do_distort_images:\n      train_bottlenecks, train_ground_truth = get_random_distorted_bottlenecks(\n          sess, image_lists, FLAGS.train_batch_size, \'training\',\n          FLAGS.image_dir, distorted_jpeg_data_tensor,\n          distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\n    else:\n      train_bottlenecks, train_ground_truth = get_random_cached_bottlenecks(\n          sess, image_lists, FLAGS.train_batch_size, \'training\',\n          FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n          bottleneck_tensor)\n    # Feed the bottlenecks and ground truth into the graph, and run a training\n    # step. Capture training summaries for TensorBoard with the `merged` op.\n    train_summary, _ = sess.run([merged, train_step],\n             feed_dict={bottleneck_input: train_bottlenecks,\n                        ground_truth_input: train_ground_truth})\n    train_writer.add_summary(train_summary, i)\n\n    # Every so often, print out how well the graph is training.\n    is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n    if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n      train_accuracy, cross_entropy_value = sess.run(\n          [evaluation_step, cross_entropy],\n          feed_dict={bottleneck_input: train_bottlenecks,\n                     ground_truth_input: train_ground_truth})\n      print(\'%s: Step %d: Train accuracy = %.1f%%\' % (datetime.now(), i,\n                                                      train_accuracy * 100))\n      print(\'%s: Step %d: Cross entropy = %f\' % (datetime.now(), i,\n                                                 cross_entropy_value))\n      validation_bottlenecks, validation_ground_truth = (\n          get_random_cached_bottlenecks(\n              sess, image_lists, FLAGS.validation_batch_size, \'validation\',\n              FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n              bottleneck_tensor))\n      # Run a validation step and capture training summaries for TensorBoard\n      # with the `merged` op.\n      validation_summary, validation_accuracy = sess.run(\n          [merged, evaluation_step],\n          feed_dict={bottleneck_input: validation_bottlenecks,\n                     ground_truth_input: validation_ground_truth})\n      validation_writer.add_summary(validation_summary, i)\n      print(\'%s: Step %d: Validation accuracy = %.1f%%\' %\n            (datetime.now(), i, validation_accuracy * 100))\n\n  # We\'ve completed all our training, so run a final test evaluation on\n  # some new images we haven\'t used before.\n  test_bottlenecks, test_ground_truth = get_random_cached_bottlenecks(\n      sess, image_lists, FLAGS.test_batch_size, \'testing\',\n      FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n      bottleneck_tensor)\n  test_accuracy = sess.run(\n      evaluation_step,\n      feed_dict={bottleneck_input: test_bottlenecks,\n                 ground_truth_input: test_ground_truth})\n  print(\'Final test accuracy = %.1f%%\' % (test_accuracy * 100))\n\n  # Write out the trained graph and labels with the weights stored as constants.\n  output_graph_def = graph_util.convert_variables_to_constants(\n      sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n  with gfile.FastGFile(FLAGS.output_graph, \'wb\') as f:\n    f.write(output_graph_def.SerializeToString())\n  with gfile.FastGFile(FLAGS.output_labels, \'w\') as f:\n    f.write(\'\\n\'.join(image_lists.keys()) + \'\\n\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
INCEPTION/image_retraining/retrain_test.py,10,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: disable=g-bad-import-order,unused-import\n""""""Tests the graph freezing tool.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.image_retraining import retrain\nfrom tensorflow.python.framework import test_util\n\n\nclass ImageRetrainingTest(test_util.TensorFlowTestCase):\n\n  def dummyImageLists(self):\n    return {\'label_one\': {\'dir\': \'somedir\', \'training\': [\'image_one.jpg\',\n                                                         \'image_two.jpg\'],\n                          \'testing\': [\'image_three.jpg\', \'image_four.jpg\'],\n                          \'validation\': [\'image_five.jpg\', \'image_six.jpg\']},\n            \'label_two\': {\'dir\': \'otherdir\', \'training\': [\'image_one.jpg\',\n                                                          \'image_two.jpg\'],\n                          \'testing\': [\'image_three.jpg\', \'image_four.jpg\'],\n                          \'validation\': [\'image_five.jpg\', \'image_six.jpg\']}}\n\n  def testGetImagePath(self):\n    image_lists = self.dummyImageLists()\n    self.assertEqual(\'image_dir/somedir/image_one.jpg\', retrain.get_image_path(\n        image_lists, \'label_one\', 0, \'image_dir\', \'training\'))\n    self.assertEqual(\'image_dir/otherdir/image_four.jpg\',\n                     retrain.get_image_path(image_lists, \'label_two\', 1,\n                                            \'image_dir\', \'testing\'))\n\n  def testGetBottleneckPath(self):\n    image_lists = self.dummyImageLists()\n    self.assertEqual(\'bottleneck_dir/somedir/image_five.jpg.txt\',\n                     retrain.get_bottleneck_path(\n                         image_lists, \'label_one\', 0, \'bottleneck_dir\',\n                         \'validation\'))\n\n  def testShouldDistortImage(self):\n    self.assertEqual(False, retrain.should_distort_images(False, 0, 0, 0))\n    self.assertEqual(True, retrain.should_distort_images(True, 0, 0, 0))\n    self.assertEqual(True, retrain.should_distort_images(False, 10, 0, 0))\n    self.assertEqual(True, retrain.should_distort_images(False, 0, 1, 0))\n    self.assertEqual(True, retrain.should_distort_images(False, 0, 0, 50))\n\n  def testAddInputDistortions(self):\n    with tf.Graph().as_default():\n      with tf.Session() as sess:\n        retrain.add_input_distortions(True, 10, 10, 10)\n        self.assertIsNotNone(sess.graph.get_tensor_by_name(\'DistortJPGInput:0\'))\n        self.assertIsNotNone(sess.graph.get_tensor_by_name(\'DistortResult:0\'))\n\n  def testAddFinalTrainingOps(self):\n    with tf.Graph().as_default():\n      with tf.Session() as sess:\n        bottleneck = tf.placeholder(\n            tf.float32, [1, retrain.BOTTLENECK_TENSOR_SIZE],\n            name=retrain.BOTTLENECK_TENSOR_NAME.split(\':\')[0])\n        retrain.add_final_training_ops(5, \'final\', bottleneck)\n        self.assertIsNotNone(sess.graph.get_tensor_by_name(\'final:0\'))\n\n  def testAddEvaluationStep(self):\n    with tf.Graph().as_default():\n      final = tf.placeholder(tf.float32, [1], name=\'final\')\n      gt = tf.placeholder(tf.float32, [1], name=\'gt\')\n      self.assertIsNotNone(retrain.add_evaluation_step(final, gt))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
TENSORBOX/utils/__init__.py,0,b''
TENSORBOX/utils/data_utils.py,0,"b""import os\nimport cv2\nimport re\nimport sys\nimport argparse\nimport numpy as np\nimport copy\nimport annolist.AnnotationLib as al\n\ndef annotation_to_h5(H, a, cell_width, cell_height, max_len):\n    region_size = H['region_size']\n    assert H['region_size'] == H['image_height'] / H['grid_height']\n    assert H['region_size'] == H['image_width'] / H['grid_width']\n    cell_regions = get_cell_grid(cell_width, cell_height, region_size)\n\n    cells_per_image = len(cell_regions)\n\n    box_list = [[] for idx in range(cells_per_image)]\n            \n    for cidx, c in enumerate(cell_regions):\n        box_list[cidx] = [r for r in a.rects if all(r.intersection(c))]\n\n    boxes = np.zeros((1, cells_per_image, 4, max_len, 1), dtype = np.float)\n    box_flags = np.zeros((1, cells_per_image, 1, max_len, 1), dtype = np.float)\n\n    for cidx in xrange(cells_per_image):\n        #assert(cur_num_boxes <= max_len)\n\n        cell_ox = 0.5 * (cell_regions[cidx].x1 + cell_regions[cidx].x2)\n        cell_oy = 0.5 * (cell_regions[cidx].y1 + cell_regions[cidx].y2)\n\n        unsorted_boxes = []\n        for bidx in xrange(min(len(box_list[cidx]), max_len)):\n\n            # relative box position with respect to cell\n            ox = 0.5 * (box_list[cidx][bidx].x1 + box_list[cidx][bidx].x2) - cell_ox\n            oy = 0.5 * (box_list[cidx][bidx].y1 + box_list[cidx][bidx].y2) - cell_oy\n\n            width = abs(box_list[cidx][bidx].x2 - box_list[cidx][bidx].x1)\n            height= abs(box_list[cidx][bidx].y2 - box_list[cidx][bidx].y1)\n            \n            if (abs(ox) < H['focus_size'] * region_size and abs(oy) < H['focus_size'] * region_size and\n                    width < H['biggest_box_px'] and height < H['biggest_box_px']):\n                unsorted_boxes.append(np.array([ox, oy, width, height], dtype=np.float))\n\n        for bidx, box in enumerate(sorted(unsorted_boxes, key=lambda x: x[0]**2 + x[1]**2)):\n            boxes[0, cidx, :, bidx, 0] = box\n            box_flags[0, cidx, 0, bidx, 0] = max(box_list[cidx][bidx].silhouetteID, 1)\n\n    return boxes, box_flags\n\ndef get_cell_grid(cell_width, cell_height, region_size):\n\n    cell_regions = []\n    for iy in xrange(cell_height):\n        for ix in xrange(cell_width):\n            cidx = iy * cell_width + ix\n            ox = (ix + 0.5) * region_size\n            oy = (iy + 0.5) * region_size\n\n            r = al.AnnoRect(ox - 0.5 * region_size, oy - 0.5 * region_size,\n                            ox + 0.5 * region_size, oy + 0.5 * region_size)\n            r.track_id = cidx\n\n            cell_regions.append(r)\n\n\n    return cell_regions\n\ndef annotation_jitter(I, a_in, min_box_width=20, jitter_scale_min=0.9, jitter_scale_max=1.1, jitter_offset=16, target_width=640, target_height=480):\n    a = copy.deepcopy(a_in)\n\n    # MA: sanity check\n    new_rects = []\n    for i in range(len(a.rects)):\n        r = a.rects[i]\n        try:\n            assert(r.x1 < r.x2 and r.y1 < r.y2)\n            new_rects.append(r)\n        except:\n            print('bad rectangle')\n    a.rects = new_rects\n\n\n    if a.rects:\n        cur_min_box_width = min([r.width() for r in a.rects])\n    else:\n        cur_min_box_width = min_box_width / jitter_scale_min\n\n    # don't downscale below min_box_width \n    jitter_scale_min = max(jitter_scale_min, float(min_box_width) / cur_min_box_width)\n\n    # it's always ok to upscale \n    jitter_scale_min = min(jitter_scale_min, 1.0)\n\n    jitter_scale_max = jitter_scale_max\n\n    jitter_scale = np.random.uniform(jitter_scale_min, jitter_scale_max)\n\n    jitter_flip = np.random.random_integers(0, 1)\n\n    if jitter_flip == 1:\n        I = np.fliplr(I)\n\n        for r in a:\n            r.x1 = I.shape[1] - r.x1\n            r.x2 = I.shape[1] - r.x2\n            r.x1, r.x2 = r.x2, r.x1\n\n            for p in r.point:\n                p.x = I.shape[1] - p.x\n\n    I1 = cv2.resize(I, None, fx=jitter_scale, fy=jitter_scale, interpolation = cv2.INTER_CUBIC)\n\n    jitter_offset_x = np.random.random_integers(-jitter_offset, jitter_offset)\n    jitter_offset_y = np.random.random_integers(-jitter_offset, jitter_offset)\n\n\n\n    rescaled_width = I1.shape[1]\n    rescaled_height = I1.shape[0]\n\n    px = round(0.5*(target_width)) - round(0.5*(rescaled_width)) + jitter_offset_x\n    py = round(0.5*(target_height)) - round(0.5*(rescaled_height)) + jitter_offset_y\n\n    I2 = np.zeros((target_height, target_width, 3), dtype=I1.dtype)\n\n    x1 = max(0, px)\n    y1 = max(0, py)\n    x2 = min(rescaled_width, target_width - x1)\n    y2 = min(rescaled_height, target_height - y1)\n\n    I2[0:(y2 - y1), 0:(x2 - x1), :] = I1[y1:y2, x1:x2, :]\n\n    ox1 = round(0.5*rescaled_width) + jitter_offset_x\n    oy1 = round(0.5*rescaled_height) + jitter_offset_y\n\n    ox2 = round(0.5*target_width)\n    oy2 = round(0.5*target_height)\n\n    for r in a:\n        r.x1 = round(jitter_scale*r.x1 - x1)\n        r.x2 = round(jitter_scale*r.x2 - x1)\n\n        r.y1 = round(jitter_scale*r.y1 - y1)\n        r.y2 = round(jitter_scale*r.y2 - y1)\n\n        if r.x1 < 0:\n            r.x1 = 0\n\n        if r.y1 < 0:\n            r.y1 = 0\n\n        if r.x2 >= I2.shape[1]:\n            r.x2 = I2.shape[1] - 1\n\n        if r.y2 >= I2.shape[0]:\n            r.y2 = I2.shape[0] - 1\n\n        for p in r.point:\n            p.x = round(jitter_scale*p.x - x1)\n            p.y = round(jitter_scale*p.y - y1)\n\n        # MA: make sure all points are inside the image\n        r.point = [p for p in r.point if p.x >=0 and p.y >=0 and p.x < I2.shape[1] and p.y < I2.shape[0]]\n\n    new_rects = []\n    for r in a.rects:\n        if r.x1 <= r.x2 and r.y1 <= r.y2:\n            new_rects.append(r)\n        else:\n            pass\n\n    a.rects = new_rects\n\n    return I2, a\n"""
TENSORBOX/utils/googlenet_load.py,17,"b'import tensorflow as tf\nfrom kaffe import mynet\nimport os\nimport numpy as np\n\ndef init(H, config=None):\n    if config is None:\n        gpu_options = tf.GPUOptions()\n        config = tf.ConfigProto(gpu_options=gpu_options)\n\n    k = H[\'num_classes\']\n    features_dim = 1024\n    input_layer = \'input\'\n\n    features_layers = [\'output/confidences\', \'output/boxes\']\n\n    graph_def_orig_file = \'%s/../data/googlenet.pb\' % os.path.dirname(os.path.realpath(__file__))\n\n    dense_layer_num_output = [k, 4]\n\n    googlenet_graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    tf.set_random_seed(0)\n    with open(graph_def_orig_file) as f:\n        tf.set_random_seed(0)\n        graph_def.MergeFromString(f.read())\n\n    with googlenet_graph.as_default():\n        tf.import_graph_def(graph_def, name=\'\')\n\n    input_op = googlenet_graph.get_operation_by_name(input_layer)\n\n    weights_ops = [\n        op for op in googlenet_graph.get_operations() \n        if any(op.name.endswith(x) for x in [ \'_w\', \'_b\'])\n        and op.type == \'Const\'\n    ]\n\n    reuse_ops = [\n        op for op in googlenet_graph.get_operations() \n        if op not in weights_ops + [input_op]\n        and op.name != \'output\'\n    ]\n\n    with tf.Session(graph=googlenet_graph, config=config):\n        weights_orig = {\n            op.name: op.outputs[0].eval()\n            for op in weights_ops\n        }\n\n    def weight_init(num_output):\n        return 0.001 * np.random.randn(features_dim, num_output).astype(np.float32)\n\n    def bias_init(num_output):\n        return 0.001 * np.random.randn(num_output).astype(np.float32)\n\n\n    W = [\n        tf.Variable(weight_init(dense_layer_num_output[i]), \n                    name=\'softmax/weights_{}\'.format(i)) \n        for i in range(len(features_layers))\n    ]\n\n    B = [\n        tf.Variable(bias_init(dense_layer_num_output[i]),\n                    name=\'softmax/biases_{}\'.format(i)) \n        for i in range(len(features_layers))\n    ]\n\n    weight_vars = {\n        name: tf.Variable(weight, name=name)\n        for name, weight in weights_orig.iteritems()\n    }\n\n    weight_tensors = {\n        name: tf.convert_to_tensor(weight)\n        for name, weight in weight_vars.iteritems()\n    }\n\n    W_norm = [tf.nn.l2_loss(weight) for weight in weight_vars.values() + W]\n    W_norm = tf.reduce_sum(tf.pack(W_norm), name=\'weights_norm\')\n    tf.scalar_summary(W_norm.op.name, W_norm)\n\n    googlenet = {\n        ""W"": W,\n        ""B"": B,\n        ""weight_tensors"": weight_tensors,\n        ""reuse_ops"": reuse_ops,\n        ""input_op"": input_op,\n        ""W_norm"": W_norm,\n        }\n    return googlenet\n\ndef model(x, googlenet, H):\n    weight_tensors = googlenet[""weight_tensors""]\n    input_op = googlenet[""input_op""]\n    reuse_ops = googlenet[""reuse_ops""]\n    def is_early_loss(name):\n        early_loss_layers = [\'head0\', \'nn0\', \'softmax0\', \'head1\', \'nn1\', \'softmax1\', \'output1\']\n        return any(name.startswith(prefix) for prefix in early_loss_layers)\n\n    T = weight_tensors\n    T[input_op.name] = x\n\n    for op in reuse_ops:\n        if is_early_loss(op.name):\n            continue\n        elif op.name == \'avgpool0\':\n            pool_op = tf.nn.avg_pool(T[\'mixed5b\'], ksize=[1,H[\'grid_height\'],H[\'grid_width\'],1], strides=[1,1,1,1], padding=\'VALID\', name=op.name)\n            T[op.name] = pool_op\n\n        else:\n            copied_op = x.graph.create_op(\n                op_type = op.type, \n                inputs = [T[t.op.name] for t in list(op.inputs)], \n                dtypes = [o.dtype for o in op.outputs], \n                name = op.name, \n                attrs =  op.node_def.attr\n            )\n\n            T[op.name] = copied_op.outputs[0]\n            #T[op.name] = tf.Print(copied_op.outputs[0], [tf.shape(copied_op.outputs[0]), tf.constant(op.name)], summarize=4)\n    \n\n    coarse_feat = T[\'mixed5b\']\n\n    # fine feat can be used to reinspect input\n    attention_lname = H.get(\'attention_lname\', \'mixed3b\')\n    early_feat = T[attention_lname]\n    early_feat_channels = 480\n\n    return coarse_feat, early_feat, early_feat_channels\n'"
TENSORBOX/utils/rect.py,0,"b'class Rect(object):\n    def __init__(self, cx, cy, width, height, confidence):\n        self.cx = cx\n        self.cy = cy\n        self.width = width\n        self.height = height\n        self.confidence = confidence\n        self.true_confidence = confidence\n    def overlaps(self, other):\n        if abs(self.cx - other.cx) > (self.width + other.width) / 1.5:\n            return False\n        elif abs(self.cy - other.cy) > (self.height + other.height) / 2.0:\n            return False\n        else:\n            return True\n    def distance(self, other):\n        return sum(map(abs, [self.cx - other.cx, self.cy - other.cy,\n                       self.width - other.width, self.height - other.height]))\n    def intersection(self, other):\n        left = max(self.cx - self.width/2., other.cx - other.width/2.)\n        right = min(self.cx + self.width/2., other.cx + other.width/2.)\n        width = max(right - left, 0)\n        top = max(self.cy - self.height/2., other.cy - other.height/2.)\n        bottom = min(self.cy + self.height/2., other.cy + other.height/2.)\n        height = max(bottom - top, 0)\n        return width * height\n    def area(self):\n        return self.height * self.width\n    def union(self, other):\n        return self.area() + other.area() - self.intersection(other)\n    def iou(self, other):\n        return self.intersection(other) / self.union(other)\n    def __eq__(self, other):\n        return (self.cx == other.cx and \n            self.cy == other.cy and\n            self.width == other.width and\n            self.height == other.height and\n            self.confidence == other.confidence)\n'"
TENSORBOX/utils/rect_multiclass.py,0,"b'class Rect_Multiclass(object):\n    def __init__(self, cx, cy, width, height, confidence, label_confidence,label):\n        \n        self.cx = cx\n        self.cy = cy\n        self.width = width\n        self.height = height\n        self.true_confidence = confidence\n        self.label_confidence = label_confidence\n        self.label= label\n        self.trackID=-1\n        self.x1 = self.cx - self.width/2.\n        self.x2 = self.cx + self.width/2.\n        self.y1 = self.cy - self.height/2.\n        self.y2 = self.cy + self.height/2.\n\n    def overlaps(self, other):\n        if abs(self.cx - other.cx) > (self.width + other.width) / 1.5:\n            return False\n        elif abs(self.cy - other.cy) > (self.height + other.height) / 2.0:\n            return False\n        else:\n            return True\n    def distance(self, other):\n        return sum(map(abs, [self.cx - other.cx, self.cy - other.cy,\n                       self.width - other.width, self.height - other.height]))\n    def intersection(self, other):\n        left = max(self.cx - self.width/2., other.cx - other.width/2.)\n        right = min(self.cx + self.width/2., other.cx + other.width/2.)\n        width = max(right - left, 0)\n        top = max(self.cy - self.height/2., other.cy - other.height/2.)\n        bottom = min(self.cy + self.height/2., other.cy + other.height/2.)\n        height = max(bottom - top, 0)\n        return width * height\n    def area(self):\n        return self.height * self.width\n    def union(self, other):\n        return self.area() + other.area() - self.intersection(other)\n    def iou(self, other):\n        return self.intersection(other) / self.union(other)\n    def __eq__(self, other):\n        return (self.cx == other.cx and \n            self.cy == other.cy and\n            self.width == other.width and\n            self.height == other.height and\n            self.confidence == other.confidence and\n            self.label_confidence == other.label_confidence and self.label == other.label and self.trackID == other.trackID)\n'"
TENSORBOX/utils/stitch_wrapper.py,0,"b""print('ERROR: stitck_wrapper not yet compiled. Please run `cd /path/to/tensorbox/utils && make`')\n"""
TENSORBOX/utils/train_utils.py,40,"b'import numpy as np\nimport random\nimport json\nimport os\nimport cv2\nimport itertools\nfrom scipy.misc import imread, imresize\nimport tensorflow as tf\n\nfrom data_utils import (annotation_jitter, annotation_to_h5)\nfrom utils.annolist import AnnotationLib as al\nfrom rect import Rect\n\ndef rescale_boxes(current_shape, anno, target_height, target_width):\n    x_scale = target_width / float(current_shape[1])\n    y_scale = target_height / float(current_shape[0])\n    for r in anno.rects:\n        assert r.x1 < r.x2\n        r.x1 *= x_scale\n        r.x2 *= x_scale\n        assert r.x1 < r.x2\n        r.y1 *= y_scale\n        r.y2 *= y_scale\n    return anno\n\ndef load_idl_tf(idlfile, H, jitter):\n    """"""Take the idlfile and net configuration and create a generator\n    that outputs a jittered version of a random image from the annolist\n    that is mean corrected.""""""\n\n    annolist = al.parse(idlfile)\n    annos = []\n    for anno in annolist:\n        anno.imageName = os.path.join(\n            os.path.dirname(os.path.realpath(idlfile)), anno.imageName)\n        annos.append(anno)\n    random.seed(0)\n    if H[\'data\'][\'truncate_data\']:\n        annos = annos[:10]\n    for epoch in itertools.count():\n        random.shuffle(annos)\n        for anno in annos:\n            I = imread(anno.imageName)\n            if I.shape[2] == 4:\n                I = I[:, :, :3]\n            if I.shape[0] != H[""image_height""] or I.shape[1] != H[""image_width""]:\n                if epoch == 0:\n                    anno = rescale_boxes(I.shape, anno, H[""image_height""], H[""image_width""])\n                I = imresize(I, (H[""image_height""], H[""image_width""]), interp=\'cubic\')\n            if jitter:\n                jitter_scale_min=0.9\n                jitter_scale_max=1.1\n                jitter_offset=16\n                I, anno = annotation_jitter(I,\n                                            anno, target_width=H[""image_width""],\n                                            target_height=H[""image_height""],\n                                            jitter_scale_min=jitter_scale_min,\n                                            jitter_scale_max=jitter_scale_max,\n                                            jitter_offset=jitter_offset)\n\n            boxes, flags = annotation_to_h5(H,\n                                            anno,\n                                            H[""grid_width""],\n                                            H[""grid_height""],\n                                            H[""rnn_len""])\n\n            yield {""image"": I, ""boxes"": boxes, ""flags"": flags}\n\ndef make_sparse(n, d):\n    v = np.zeros((d,), dtype=np.float32)\n    v[n] = 1.\n    return v\n\ndef load_data_gen(H, phase, jitter):\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n\n    data = load_idl_tf(H[""data""][\'%s_idl\' % phase], H, jitter={\'train\': jitter, \'test\': False}[phase])\n\n    for d in data:\n        output = {}\n        \n        rnn_len = H[""rnn_len""]\n        flags = d[\'flags\'][0, :, 0, 0:rnn_len, 0]\n        boxes = np.transpose(d[\'boxes\'][0, :, :, 0:rnn_len, 0], (0, 2, 1))\n        assert(flags.shape == (grid_size, rnn_len))\n        assert(boxes.shape == (grid_size, rnn_len, 4))\n\n        output[\'image\'] = d[\'image\']\n        output[\'confs\'] = np.array([[make_sparse(int(detection), d=H[\'num_classes\']) for detection in cell] for cell in flags])\n        output[\'boxes\'] = boxes\n        output[\'flags\'] = flags\n        \n        yield output\n\ndef add_rectangles(H, orig_image, confidences, boxes, use_stitching=False, rnn_len=1, min_conf=0.1, show_removed=True, tau=0.25):\n    image = np.copy(orig_image[0])\n    num_cells = H[""grid_height""] * H[""grid_width""]\n    boxes_r = np.reshape(boxes, (-1,\n                                 H[""grid_height""],\n                                 H[""grid_width""],\n                                 rnn_len,\n                                 4))\n    confidences_r = np.reshape(confidences, (-1,\n                                             H[""grid_height""],\n                                             H[""grid_width""],\n                                             rnn_len,\n                                             H[\'num_classes\']))\n    cell_pix_size = H[\'region_size\']\n    all_rects = [[[] for _ in range(H[""grid_width""])] for _ in range(H[""grid_height""])]\n    for n in range(rnn_len):\n        for y in range(H[""grid_height""]):\n            for x in range(H[""grid_width""]):\n                bbox = boxes_r[0, y, x, n, :]\n                abs_cx = int(bbox[0]) + cell_pix_size/2 + cell_pix_size * x\n                abs_cy = int(bbox[1]) + cell_pix_size/2 + cell_pix_size * y\n                w = bbox[2]\n                h = bbox[3]\n                conf = np.max(confidences_r[0, y, x, n, 1:])\n                all_rects[y][x].append(Rect(abs_cx,abs_cy,w,h,conf))\n\n    all_rects_r = [r for row in all_rects for cell in row for r in cell]\n    if use_stitching:\n        from stitch_wrapper import stitch_rects\n        acc_rects = stitch_rects(all_rects, tau)\n    else:\n        acc_rects = all_rects_r\n\n\n    pairs = [(all_rects_r, (255, 0, 0)), (acc_rects, (0, 255, 0))]\n    for rect_set, color in pairs:\n        for rect in rect_set:\n            if rect.confidence > min_conf:\n                cv2.rectangle(image,\n                    (rect.cx-int(rect.width/2), rect.cy-int(rect.height/2)),\n                    (rect.cx+int(rect.width/2), rect.cy+int(rect.height/2)),\n                    color,\n                    2)\n\n    rects = []\n    for rect in acc_rects:\n        r = al.AnnoRect()\n        r.x1 = rect.cx - rect.width/2.\n        r.x2 = rect.cx + rect.width/2.\n        r.y1 = rect.cy - rect.height/2.\n        r.y2 = rect.cy + rect.height/2.\n        r.score = rect.true_confidence\n        rects.append(r)\n\n    return image, rects\n\ndef to_x1y1x2y2(box):\n    w = tf.maximum(box[:, 2:3], 1)\n    h = tf.maximum(box[:, 3:4], 1)\n    x1 = box[:, 0:1] - w / 2\n    x2 = box[:, 0:1] + w / 2\n    y1 = box[:, 1:2] - h / 2\n    y2 = box[:, 1:2] + h / 2\n    return tf.concat(1, [x1, y1, x2, y2])\n\ndef intersection(box1, box2):\n    x1_max = tf.maximum(box1[:, 0], box2[:, 0])\n    y1_max = tf.maximum(box1[:, 1], box2[:, 1])\n    x2_min = tf.minimum(box1[:, 2], box2[:, 2])\n    y2_min = tf.minimum(box1[:, 3], box2[:, 3])\n   \n    x_diff = tf.maximum(x2_min - x1_max, 0)\n    y_diff = tf.maximum(y2_min - y1_max, 0)\n    \n    return x_diff * y_diff\n\ndef area(box):\n    x_diff = tf.maximum(box[:, 2] - box[:, 0], 0)\n    y_diff = tf.maximum(box[:, 3] - box[:, 1], 0)\n    return x_diff * y_diff\n\ndef union(box1, box2):\n    return area(box1) + area(box2) - intersection(box1, box2)\n\ndef iou(box1, box2):\n    return intersection(box1, box2) / union(box1, box2)\n\ndef to_idx(vec, w_shape):\n    \'\'\'\n    vec = (idn, idh, idw)\n    w_shape = [n, h, w, c]\n    \'\'\'\n    return vec[:, 2] + w_shape[2] * (vec[:, 1] + w_shape[1] * vec[:, 0])\n\ndef interp(w, i, channel_dim):\n    \'\'\'\n    Input:\n        w: A 4D block tensor of shape (n, h, w, c)\n        i: A list of 3-tuples [(x_1, y_1, z_1), (x_2, y_2, z_2), ...],\n            each having type (int, float, float)\n \n        The 4D block represents a batch of 3D image feature volumes with c channels.\n        The input i is a list of points  to index into w via interpolation. Direct\n        indexing is not possible due to y_1 and z_1 being float values.\n    Output:\n        A list of the values: [\n            w[x_1, y_1, z_1, :]\n            w[x_2, y_2, z_2, :]\n            ...\n            w[x_k, y_k, z_k, :]\n        ]\n        of the same length == len(i)\n    \'\'\'\n    w_as_vector = tf.reshape(w, [-1, channel_dim]) # gather expects w to be 1-d\n    if(int(tf.__version__.split(""."")[1])<13 and int(tf.__version__.split(""."")[0])<2): ### for tf version < 13\n        upper_l = tf.to_int32(tf.concat(1, [i[:, 0:1], tf.floor(i[:, 1:2]), tf.floor(i[:, 2:3])]))\n        upper_r = tf.to_int32(tf.concat(1, [i[:, 0:1], tf.floor(i[:, 1:2]), tf.ceil(i[:, 2:3])]))\n        lower_l = tf.to_int32(tf.concat(1, [i[:, 0:1], tf.ceil(i[:, 1:2]), tf.floor(i[:, 2:3])]))\n        lower_r = tf.to_int32(tf.concat(1, [i[:, 0:1], tf.ceil(i[:, 1:2]), tf.ceil(i[:, 2:3])]))\n    else:  ### for tf version >= 13\n        upper_l = tf.cast(tf.concat(1, [i[:, 0:1], tf.floor(i[:, 1:2]), tf.floor(i[:, 2:3])]),tf.int32)\n        upper_r = tf.cast(tf.concat(1, [i[:, 0:1], tf.floor(i[:, 1:2]), tf.ceil(i[:, 2:3])]),tf.int32)\n        lower_l = tf.cast(tf.concat(1, [i[:, 0:1], tf.ceil(i[:, 1:2]), tf.floor(i[:, 2:3])]),tf.int32)\n        lower_r = tf.cast(tf.concat(1, [i[:, 0:1], tf.ceil(i[:, 1:2]), tf.ceil(i[:, 2:3])]),tf.int32)\n    upper_l_idx = to_idx(upper_l, tf.shape(w))\n    upper_r_idx = to_idx(upper_r, tf.shape(w))\n    lower_l_idx = to_idx(lower_l, tf.shape(w))\n    lower_r_idx = to_idx(lower_r, tf.shape(w))\n \n    upper_l_value = tf.gather(w_as_vector, upper_l_idx)\n    upper_r_value = tf.gather(w_as_vector, upper_r_idx)\n    lower_l_value = tf.gather(w_as_vector, lower_l_idx)\n    lower_r_value = tf.gather(w_as_vector, lower_r_idx)\n \n    alpha_lr = tf.expand_dims(i[:, 2] - tf.floor(i[:, 2]), 1)\n    alpha_ud = tf.expand_dims(i[:, 1] - tf.floor(i[:, 1]), 1)\n \n    upper_value = (1 - alpha_lr) * upper_l_value + (alpha_lr) * upper_r_value\n    lower_value = (1 - alpha_lr) * lower_l_value + (alpha_lr) * lower_r_value\n    value = (1 - alpha_ud) * upper_value + (alpha_ud) * lower_value\n    return value\n\ndef bilinear_select(H, pred_boxes, early_feat, early_feat_channels, w_offset, h_offset):\n    \'\'\'\n    Function used for rezooming high level feature maps. Uses bilinear interpolation\n    to select all channels at index (x, y) for a high level feature map, where x and y are floats.\n    \'\'\'\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n\n    fine_stride = 8. # pixels per 60x80 grid cell in 480x640 image\n    coarse_stride = H[\'region_size\'] # pixels per 15x20 grid cell in 480x640 image\n    batch_ids = []\n    x_offsets = []\n    y_offsets = []\n    for n in range(H[\'batch_size\']):\n        for i in range(H[\'grid_height\']):\n            for j in range(H[\'grid_width\']):\n                for k in range(H[\'rnn_len\']):\n                    batch_ids.append([n])\n                    x_offsets.append([coarse_stride / 2. + coarse_stride * j])\n                    y_offsets.append([coarse_stride / 2. + coarse_stride * i])\n\n    batch_ids = tf.constant(batch_ids)\n    x_offsets = tf.constant(x_offsets)\n    y_offsets = tf.constant(y_offsets)\n\n    pred_boxes_r = tf.reshape(pred_boxes, [outer_size * H[\'rnn_len\'], 4])\n    scale_factor = coarse_stride / fine_stride # scale difference between 15x20 and 60x80 features\n\n    pred_x_center = (pred_boxes_r[:, 0:1] + w_offset * pred_boxes_r[:, 2:3] + x_offsets) / fine_stride\n    pred_x_center_clip = tf.clip_by_value(pred_x_center,\n                                     0,\n                                     scale_factor * H[\'grid_width\'] - 1)\n    pred_y_center = (pred_boxes_r[:, 1:2] + h_offset * pred_boxes_r[:, 3:4] + y_offsets) / fine_stride\n    pred_y_center_clip = tf.clip_by_value(pred_y_center,\n                                          0,\n                                          scale_factor * H[\'grid_height\'] - 1)\n    if(int(tf.__version__.split(""."")[1])<13 and int(tf.__version__.split(""."")[0])<2): ### for tf version < 1.13\n        interp_indices = tf.concat(1, [tf.to_float(batch_ids), pred_y_center_clip, pred_x_center_clip])\n    else: ### for tf version >= 1.13.0\n        interp_indices = tf.concat(1, [tf.cast(batch_ids,tf.float32), pred_y_center_clip, pred_x_center_clip]) \n    return interp_indices\n'"
dataset_scripts/inception_tensorflow/build_imagenet_data.py,31,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts ImageNet data to TFRecords file format with Example protos.\n\nThe raw ImageNet data set is expected to reside in JPEG files located in the\nfollowing directory structure.\n\n  data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n  data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n  ...\n\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThe training data set consists of 1000 sub-directories (i.e. labels)\neach containing 1200 JPEG images for a total of 1.2M JPEG images.\n\nThe evaluation data set consists of 1000 sub-directories (i.e. labels)\neach containing 50 JPEG images for a total of 50K JPEG images.\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\n  train_directory/train-00000-of-01024\n  train_directory/train-00001-of-01024\n  ...\n  train_directory/train-00127-of-01024\n\nand\n\n  validation_directory/validation-00000-of-00128\n  validation_directory/validation-00001-of-00128\n  ...\n  validation_directory/validation-00127-of-00128\n\nEach validation TFRecord file contains ~390 records. Each training TFREcord\nfile contains ~1250 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n  image/encoded: string containing JPEG encoded image in RGB colorspace\n  image/height: integer, image height in pixels\n  image/width: integer, image width in pixels\n  image/colorspace: string, specifying the colorspace, always \'RGB\'\n  image/channels: integer, specifying the number of channels, always 3\n  image/format: string, specifying the format, always\'JPEG\'\n\n  image/filename: string containing the basename of the image file\n            e.g. \'n01440764_10026.JPEG\' or \'ILSVRC2012_val_00000293.JPEG\'\n  image/class/label: integer specifying the index in a classification layer.\n    The label ranges from [1, 1000] where 0 is not used.\n  image/class/synset: string specifying the unique ID of the label,\n    e.g. \'n01440764\'\n  image/class/text: string specifying the human-readable version of the label\n    e.g. \'red fox, Vulpes vulpes\'\n\n  image/object/bbox/xmin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/xmax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/label: integer specifying the index in a classification\n    layer. The label ranges from [1, 1000] where 0 is not used. Note this is\n    always identical to the image label.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n\nRunning this script using 16 threads may take around ~2.5 hours on a HP Z420.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\n\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string(\'train_directory\', \'/tmp/\',\n                           \'Training data directory\')\ntf.app.flags.DEFINE_string(\'validation_directory\', \'/tmp/\',\n                           \'Validation data directory\')\ntf.app.flags.DEFINE_string(\'output_directory\', \'/tmp/\',\n                           \'Output data directory\')\n\ntf.app.flags.DEFINE_integer(\'train_shards\', 1024,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'validation_shards\', 128,\n                            \'Number of shards in validation TFRecord files.\')\n\ntf.app.flags.DEFINE_integer(\'num_threads\', 8,\n                            \'Number of threads to preprocess the images.\')\n\n# The labels file contains a list of valid labels are held in this file.\n# Assumes that the file contains entries as such:\n#   n01440764\n#   n01443537\n#   n01484850\n# where each line corresponds to a label expressed as a synset. We map\n# each synset contained in the file to an integer (based on the alphabetical\n# ordering). See below for details.\ntf.app.flags.DEFINE_string(\'labels_file\',\n                           \'map_vid.txt\',\n                           \'Labels file\')\n\n# This file containing mapping from synset to human-readable label.\n# Assumes each line of the file looks like:\n#\n#   n02119247    black fox\n#   n02119359    silver fox\n#   n02119477    red fox, Vulpes fulva\n#\n# where each line corresponds to a unique mapping. Note that each line is\n# formatted as <synset>\\t<human readable label>.\ntf.app.flags.DEFINE_string(\'imagenet_metadata_file\',\n                           \'meta_vid.txt\',\n                           \'ImageNet metadata file\')\n\n# This file is the output of process_bounding_box.py\n# Assumes each line of the file looks like:\n#\n#   n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n#\n# where each line corresponds to one bounding box annotation associated\n# with an image. Each line can be parsed as:\n#\n#   <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n#\n# Note that there might exist mulitple bounding box annotations associated\n# with an image file.\ntf.app.flags.DEFINE_string(\'bounding_box_file\',\n                           \'./dataset_summary.csv\',\n                           \'Bounding box file\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n  """"""Wrapper for inserting float features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, synset, human, bbox,\n                        height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    synset: string, unique WordNet ID specifying the label, e.g., \'n02323233\'\n    human: string, human-readable label, e.g., \'red fox, Vulpes vulpes\'\n    bbox: list of bounding boxes; each box is a list of integers\n      specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong to\n      the same label as the image label.\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  for b in bbox:\n    assert len(b) == 4\n    # pylint: disable=expression-not-assigned\n    [l.append(point) for l, point in zip([xmin, ymin, xmax, ymax], b)]\n    # pylint: enable=expression-not-assigned\n\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'image/colorspace\': _bytes_feature(colorspace),\n      \'image/channels\': _int64_feature(channels),\n      \'image/class/label\': _int64_feature(label),\n      \'image/class/synset\': _bytes_feature(synset),\n      \'image/class/text\': _bytes_feature(human),\n      \'image/object/bbox/xmin\': _float_feature(xmin),\n      \'image/object/bbox/xmax\': _float_feature(xmax),\n      \'image/object/bbox/ymin\': _float_feature(ymin),\n      \'image/object/bbox/ymax\': _float_feature(ymax),\n      \'image/object/bbox/label\': _int64_feature([label] * len(xmin)),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/filename\': _bytes_feature(os.path.basename(filename)),\n      \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  # File list from:\n  # https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU\n  return \'n02105855_2933.JPEG\' in filename\n\n\ndef _is_cmyk(filename):\n  """"""Determine if file contains a CMYK JPEG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a JPEG encoded with CMYK color space.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  blacklist = [\'n01739381_1309.JPEG\', \'n02077923_14822.JPEG\',\n               \'n02447366_23489.JPEG\', \'n02492035_15739.JPEG\',\n               \'n02747177_10752.JPEG\', \'n03018349_4028.JPEG\',\n               \'n03062245_4620.JPEG\', \'n03347037_9675.JPEG\',\n               \'n03467068_12171.JPEG\', \'n03529860_11437.JPEG\',\n               \'n03544143_17228.JPEG\', \'n03633091_5218.JPEG\',\n               \'n03710637_5125.JPEG\', \'n03961711_5286.JPEG\',\n               \'n04033995_2932.JPEG\', \'n04258138_17003.JPEG\',\n               \'n04264628_27969.JPEG\', \'n04336792_7448.JPEG\',\n               \'n04371774_5854.JPEG\', \'n04596742_4225.JPEG\',\n               \'n07583066_647.JPEG\', \'n13037406_4650.JPEG\']\n  return filename.split(\'/\')[-1] in blacklist\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  image_data = tf.gfile.FastGFile(filename, \'r\').read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    print(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    print(\'Converting CMYK to RGB for %s\' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               synsets, labels, humans, bboxes, num_shards):\n  """"""Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in xrange(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n      synset = synsets[i]\n      human = humans[i]\n      bbox = bboxes[i]\n\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, image_buffer, label,\n                                    synset, human, bbox,\n                                    height, width)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    print(\'%s [thread %d]: Wrote %d images to %s\' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()\n\n\ndef _process_image_files(name, filenames, synsets, labels, humans,\n                         bboxes, num_shards):\n  """"""Process and save list of images as TFRecord of Example protos.\n\n  Args:\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  assert len(filenames) == len(synsets)\n  assert len(filenames) == len(labels)\n  assert len(filenames) == len(humans)\n  assert len(filenames) == len(bboxes)\n\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  threads = []\n  for i in xrange(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i+1]])\n\n  # Launch a thread for each batch.\n  print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in xrange(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames,\n            synsets, labels, humans, bboxes, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(\'%s: Finished writing all %d images in data set.\' %\n        (datetime.now(), len(filenames)))\n  sys.stdout.flush()\n\n\ndef _find_image_files(data_dir, labels_file):\n  """"""Build a list of all images files and labels in the data set.\n\n  Args:\n    data_dir: string, path to the root directory of images.\n\n      Assumes that the ImageNet data set resides in JPEG files located in\n      the following directory structure.\n\n        data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n        data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n\n      where \'n01440764\' is the unique synset label associated with these images.\n\n    labels_file: string, path to the labels file.\n\n      The list of valid labels are held in this file. Assumes that the file\n      contains entries as such:\n        n01440764\n        n01443537\n        n01484850\n      where each line corresponds to a label expressed as a synset. We map\n      each synset contained in the file to an integer (based on the alphabetical\n      ordering) starting with the integer 1 corresponding to the synset\n      contained in the first line.\n\n      The reason we start the integer labels at 1 is to reserve label 0 as an\n      unused background class.\n\n  Returns:\n    filenames: list of strings; each string is a path to an image file.\n    synsets: list of strings; each string is a unique WordNet ID.\n    labels: list of integer; each integer identifies the ground truth.\n  """"""\n  print(\'Determining list of input files and labels from %s.\' % data_dir)\n  challenge_synsets = [l.strip() for l in\n                       tf.gfile.FastGFile(labels_file, \'r\').readlines()]\n\n  labels = []\n  filenames = []\n  synsets = []\n\n  # Leave label index 0 empty as a background class.\n  label_index = 1\n\n  # Construct the list of JPEG files and labels.\n  for synset in challenge_synsets:\n    jpeg_file_path = \'%s/%s/*.JPEG\' % (data_dir, synset)\n    matching_files = tf.gfile.Glob(jpeg_file_path)\n\n    labels.extend([label_index] * len(matching_files))\n    synsets.extend([synset] * len(matching_files))\n    filenames.extend(matching_files)\n\n    if not label_index % 100:\n      print(\'Finished finding files in %d of %d classes.\' % (\n          label_index, len(challenge_synsets)))\n    label_index += 1\n\n  # Shuffle the ordering of all image files in order to guarantee\n  # random ordering of the images with respect to label in the\n  # saved TFRecord files. Make the randomization repeatable.\n  shuffled_index = range(len(filenames))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n\n  filenames = [filenames[i] for i in shuffled_index]\n  synsets = [synsets[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  print(\'Found %d JPEG files across %d labels inside %s.\' %\n        (len(filenames), len(challenge_synsets), data_dir))\n  return filenames, synsets, labels\n\n\ndef _find_human_readable_labels(synsets, synset_to_human):\n  """"""Build a list of human-readable labels.\n\n  Args:\n    synsets: list of strings; each string is a unique WordNet ID.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n\n  Returns:\n    List of human-readable strings corresponding to each synset.\n  """"""\n  humans = []\n  for s in synsets:\n    assert s in synset_to_human, (\'Failed to find: %s\' % s)\n    humans.append(synset_to_human[s])\n  return humans\n\n\ndef _find_image_bounding_boxes(filenames, image_to_bboxes):\n  """"""Find the bounding boxes for a given image file.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file.\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  Returns:\n    List of bounding boxes for each image. Note that each entry in this\n    list might contain from 0+ entries corresponding to the number of bounding\n    box annotations for the image.\n  """"""\n  num_image_bbox = 0\n  bboxes = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in image_to_bboxes:\n      bboxes.append(image_to_bboxes[basename])\n      num_image_bbox += 1\n    else:\n      bboxes.append([])\n  print(\'Found %d images with bboxes out of %d images\' % (\n      num_image_bbox, len(filenames)))\n  return bboxes\n\n\ndef _process_dataset(name, directory, num_shards, synset_to_human,\n                     image_to_bboxes):\n  """"""Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  """"""\n  filenames, synsets, labels = _find_image_files(directory, FLAGS.labels_file)\n  humans = _find_human_readable_labels(synsets, synset_to_human)\n  bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes)\n  _process_image_files(name, filenames, synsets, labels,\n                       humans, bboxes, num_shards)\n\n\ndef _build_synset_lookup(imagenet_metadata_file):\n  """"""Build lookup for synset to human-readable label.\n\n  Args:\n    imagenet_metadata_file: string, path to file containing mapping from\n      synset to human-readable label.\n\n      Assumes each line of the file looks like:\n\n        n02119247    black fox\n        n02119359    silver fox\n        n02119477    red fox, Vulpes fulva\n\n      where each line corresponds to a unique mapping. Note that each line is\n      formatted as <synset>\\t<human readable label>.\n\n  Returns:\n    Dictionary of synset to human labels, such as:\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n  """"""\n  lines = tf.gfile.FastGFile(imagenet_metadata_file, \'r\').readlines()\n  synset_to_human = {}\n  for l in lines:\n    if l:\n      parts = l.strip().split(\'\\t\')\n      assert len(parts) == 2\n      synset = parts[0]\n      human = parts[1]\n      synset_to_human[synset] = human\n  return synset_to_human\n\n\ndef _build_bounding_box_lookup(bounding_box_file):\n  """"""Build a lookup from image file to bounding boxes.\n\n  Args:\n    bounding_box_file: string, path to file with bounding boxes annotations.\n\n      Assumes each line of the file looks like:\n\n        n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\n      where each line corresponds to one bounding box annotation associated\n      with an image. Each line can be parsed as:\n\n        <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\n      Note that there might exist mulitple bounding box annotations associated\n      with an image file. This file is the output of process_bounding_boxes.py.\n\n  Returns:\n    Dictionary mapping image file names to a list of bounding boxes. This list\n    contains 0+ bounding boxes.\n  """"""\n  lines = tf.gfile.FastGFile(bounding_box_file, \'r\').readlines()\n  images_to_bboxes = {}\n  num_bbox = 0\n  num_image = 0\n  for l in lines:\n    if l:\n      parts = l.split(\',\')\n      # print(\'Successfully splitted p1: %s p2: %s p3: %s p4: %s \' % (parts[1],parts[2],parts[3],parts[4]))\n      assert len(parts) == 5, (\'Failed to parse: %s\' % l)\n      filename = parts[0]\n      xmin = float(parts[1])\n      ymin = float(parts[2])\n      xmax = float(parts[3])\n      ymax = float(parts[4])\n      box = [xmin, ymin, xmax, ymax]\n\n      if filename not in images_to_bboxes:\n        images_to_bboxes[filename] = []\n        num_image += 1\n      images_to_bboxes[filename].append(box)\n      num_bbox += 1\n\n  print(\'Successfully read %d bounding boxes \'\n        \'across %d images.\' % (num_bbox, num_image))\n  return images_to_bboxes\n\n\ndef main(unused_argv):\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with \'\n      \'FLAGS.validation_shards\')\n  print(\'Saving results to %s\' % FLAGS.output_directory)\n\n  # Build a map from synset to human-readable label.\n  synset_to_human = _build_synset_lookup(FLAGS.imagenet_metadata_file)\n  print(synset_to_human)\n  image_to_bboxes = _build_bounding_box_lookup(FLAGS.bounding_box_file)\n\n  # Run it!\n  # _process_dataset(\'validation\', FLAGS.validation_directory,\n  #                  FLAGS.validation_shards, synset_to_human, image_to_bboxes)\n  _process_dataset(\'train\', \'./dataset/data/\', FLAGS.train_shards,\n                   synset_to_human, image_to_bboxes)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
dataset_scripts/inception_tensorflow/process_bounding_boxes.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nThis script is called as\n\nprocess_bounding_boxes.py <dir> [synsets-file]\n\nWhere <dir> is a directory containing the downloaded and unpacked bounding box\ndata. If [synsets-file] is supplied, then only the bounding boxes whose\nsynstes are contained within this file are returned. Note that the\n[synsets-file] file contains synset ids, one per line.\n\nThe script dumps out a CSV text file in which each line contains an entry.\n  n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\nThe entry can be read as:\n  <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\nThe bounding box for <JPEG file name> contains two points (xmin, ymin) and\n(xmax, ymax) specifying the lower-left corner and upper-right corner of a\nbounding box in *relative* coordinates.\n\nThe user supplies a directory where the XML files reside. The directory\nstructure in the directory <dir> is assumed to look like this:\n\n<dir>/nXXXXXXXX/nXXXXXXXX_YYYY.xml\n\nEach XML file contains a bounding box annotation. The script:\n\n (1) Parses the XML file and extracts the filename, label and bounding box info.\n\n (2) The bounding box is specified in the XML files as integer (xmin, ymin) and\n    (xmax, ymax) *relative* to image size displayed to the human annotator. The\n    size of the image displayed to the human annotator is stored in the XML file\n    as integer (height, width).\n\n    Note that the displayed size will differ from the actual size of the image\n    downloaded from image-net.org. To make the bounding box annotation useable,\n    we convert bounding box to floating point numbers relative to displayed\n    height and width of the image.\n\n    Note that each XML file might contain N bounding box annotations.\n\n    Note that the points are all clamped at a range of [0.0, 1.0] because some\n    human annotations extend outside the range of the supplied image.\n\n    See details here: http://image-net.org/download-bboxes\n\n(3) By default, the script outputs all valid bounding boxes. If a\n    [synsets-file] is supplied, only the subset of bounding boxes associated\n    with those synsets are outputted. Importantly, one can supply a list of\n    synsets in the ImageNet Challenge and output the list of bounding boxes\n    associated with the training images of the ILSVRC.\n\n    We use these bounding boxes to inform the random distortion of images\n    supplied to the network.\n\nIf you run this script successfully, you will see the following output\nto stderr:\n> Finished processing 544546 XML files.\n> Skipped 0 XML files not in ImageNet Challenge.\n> Skipped 0 bounding boxes not in ImageNet Challenge.\n> Wrote 615299 bounding boxes from 544546 annotated images.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\nimport sys\nimport xml.etree.ElementTree as ET\nimport csv\n\n\nclass BoundingBox(object):\n  pass\n\n\ndef GetItem(name, root, index=0):\n  count = 0\n  for item in root.iter(name):\n    if count == index:\n      return item.text\n    count += 1\n  # Failed to find ""index"" occurrence of item.\n  return -1\n\n\ndef GetInt(name, root, index=0):\n  return int(GetItem(name, root, index))\n\n\ndef FindNumberBoundingBoxes(root):\n  index = 0\n  while True:\n    if GetInt(\'xmin\', root, index) == -1:\n      break\n    index += 1\n  return index\n\n\ndef ProcessXMLAnnotation(xml_file):\n  """"""Process a single XML file containing a bounding box.""""""\n  # pylint: disable=broad-except\n  try:\n    tree = ET.parse(xml_file)\n  except Exception:\n    print(\'Failed to parse: \' + xml_file, file=sys.stderr)\n    return None\n  # pylint: enable=broad-except\n  root = tree.getroot()\n\n  num_boxes = FindNumberBoundingBoxes(root)\n  boxes = []\n\n  for index in xrange(num_boxes):\n    box = BoundingBox()\n    # Grab the \'index\' annotation.\n    box.xmin = GetInt(\'xmin\', root, index)\n    box.ymin = GetInt(\'ymin\', root, index)\n    box.xmax = GetInt(\'xmax\', root, index)\n    box.ymax = GetInt(\'ymax\', root, index)\n\n    box.width = GetInt(\'width\', root)\n    box.height = GetInt(\'height\', root)\n    box.filename = GetItem(\'filename\', root) + \'.JPEG\'\n    box.label = GetItem(\'name\', root)\n\n    xmin = float(box.xmin) / float(box.width)\n    xmax = float(box.xmax) / float(box.width)\n    ymin = float(box.ymin) / float(box.height)\n    ymax = float(box.ymax) / float(box.height)\n\n    # Some images contain bounding box annotations that\n    # extend outside of the supplied image. See, e.g.\n    # n03127925/n03127925_147.xml\n    # Additionally, for some bounding boxes, the min > max\n    # or the box is entirely outside of the image.\n    min_x = min(xmin, xmax)\n    max_x = max(xmin, xmax)\n    box.xmin_scaled = min(max(min_x, 0.0), 1.0)\n    box.xmax_scaled = min(max(max_x, 0.0), 1.0)\n\n    min_y = min(ymin, ymax)\n    max_y = max(ymin, ymax)\n    box.ymin_scaled = min(max(min_y, 0.0), 1.0)\n    box.ymax_scaled = min(max(max_y, 0.0), 1.0)\n\n    boxes.append(box)\n\n  return boxes\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 2 or len(sys.argv) > 3:\n    print(\'Invalid usage\\n\'\n          \'usage: process_bounding_boxes.py <dir> [synsets-file]\',\n          file=sys.stderr)\n    sys.exit(-1)\n\n  xml_files = glob.glob(sys.argv[1] + \'/*/*.xml\')\n  print(\'Identified %d XML files in %s\' % (len(xml_files), sys.argv[1]),\n        file=sys.stderr)\n\n  if len(sys.argv) == 3:\n    labels = set([l.strip() for l in open(sys.argv[2]).readlines()])\n    print(\'Identified %d synset IDs in %s\' % (len(labels), sys.argv[2]),\n          file=sys.stderr)\n  else:\n    labels = None\n\n  skipped_boxes = 0\n  skipped_files = 0\n  saved_boxes = 0\n  saved_files = 0\n  \n  with open(\'dataset_summary.csv\', \'w\') as summary:\n    summary_writer = csv.writer(summary)\n    #summary_writer.writerow([""JPEG file name"", ""xmin"", ""ymin"", ""xmax"", ""ymax""])\n    for file_index, one_file in enumerate(xml_files):\n        # Example: <...>/n06470073/n00141669_6790.xml\n        label = os.path.basename(os.path.dirname(one_file))\n\n        # Determine if the annotation is from an ImageNet Challenge label.\n        if labels is not None and label not in labels:\n          skipped_files += 1\n          continue\n\n        bboxes = ProcessXMLAnnotation(one_file)\n        assert bboxes is not None, \'No bounding boxes found in \' + one_file\n\n        found_box = False\n        for bbox in bboxes:\n          if labels is not None:\n            if bbox.label != label:\n              # Note: There is a slight bug in the bounding box annotation data.\n              # Many of the dog labels have the human label \'Scottish_deerhound\'\n              # instead of the synset ID \'n02092002\' in the bbox.label field. As a\n              # simple hack to overcome this issue, we only exclude bbox labels\n              # *which are synset ID\'s* that do not match original synset label for\n              # the XML file.\n              if bbox.label in labels:\n                skipped_boxes += 1\n                continue\n\n          # Guard against improperly specified boxes.\n          if (bbox.xmin_scaled >= bbox.xmax_scaled or\n              bbox.ymin_scaled >= bbox.ymax_scaled):\n            skipped_boxes += 1\n            continue\n\n          # Note bbox.filename occasionally contains \'%s\' in the name. This is\n          # data set noise that is fixed by just using the basename of the XML file.\n          image_filename = os.path.splitext(os.path.basename(one_file))[0]\n          print(\'%s.JPEG,%.4f,%.4f,%.4f,%.4f\' %\n                (image_filename,\n                 bbox.xmin_scaled, bbox.ymin_scaled,\n                 bbox.xmax_scaled, bbox.ymax_scaled))\n          summary_writer.writerow([image_filename+\'.JPEG\',  bbox.xmin_scaled, bbox.ymin_scaled,bbox.xmax_scaled, bbox.ymax_scaled])\n\n\n          saved_boxes += 1\n          found_box = True\n        if found_box:\n          saved_files += 1\n        else:\n          skipped_files += 1\n\n        if not file_index % 5000:\n          print(\'--> processed %d of %d XML files.\' %\n                (file_index + 1, len(xml_files)),\n                file=sys.stderr)\n          print(\'--> skipped %d boxes and %d XML files.\' %\n                (skipped_boxes, skipped_files), file=sys.stderr)\n\n  print(\'Finished processing %d XML files.\' % len(xml_files), file=sys.stderr)\n  print(\'Skipped %d XML files not in ImageNet Challenge.\' % skipped_files,\n        file=sys.stderr)\n  print(\'Skipped %d bounding boxes not in ImageNet Challenge.\' % skipped_boxes,\n        file=sys.stderr)\n  print(\'Wrote %d bounding boxes from %d annotated images.\' %\n        (saved_boxes, saved_files),\n        file=sys.stderr)\n  print(\'Finished.\', file=sys.stderr)\n'"
TENSORBOX/utils/annolist/AnnoList_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: AnnoList.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'AnnoList.proto\',\n  package=\'protobuf_annolist\',\n  serialized_pb=_b(\'\\n\\x0e\\x41nnoList.proto\\x12\\x11protobuf_annolist\\""B\\n\\tAttribute\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\x0b\\n\\x03val\\x18\\x02 \\x01(\\x05\\x12\\x0c\\n\\x04\\x66val\\x18\\x03 \\x01(\\x02\\x12\\x0e\\n\\x06strval\\x18\\x04 \\x01(\\t\\""\\""\\n\\tIdStrPair\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\t\\n\\x01s\\x18\\x02 \\x01(\\t\\""j\\n\\rAttributeDesc\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05\\x64type\\x18\\x03 \\x01(\\x05\\x12\\x30\\n\\nval_to_str\\x18\\x04 \\x03(\\x0b\\x32\\x1c.protobuf_annolist.IdStrPair\\"".\\n\\x11\\x41nnoRectAttribute\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0b\\n\\x03val\\x18\\x02 \\x01(\\t\\""\\x98\\x01\\n\\x08\\x41nnoRect\\x12\\n\\n\\x02x1\\x18\\x01 \\x01(\\x02\\x12\\n\\n\\x02y1\\x18\\x02 \\x01(\\x02\\x12\\n\\n\\x02x2\\x18\\x03 \\x01(\\x02\\x12\\n\\n\\x02y2\\x18\\x04 \\x01(\\x02\\x12\\r\\n\\x05score\\x18\\x05 \\x01(\\x02\\x12\\n\\n\\x02id\\x18\\x06 \\x01(\\x05\\x12\\x10\\n\\x08track_id\\x18\\x0b \\x01(\\x05\\x12/\\n\\tattribute\\x18\\x0c \\x03(\\x0b\\x32\\x1c.protobuf_annolist.Attribute\\""o\\n\\nAnnotation\\x12\\x11\\n\\timageName\\x18\\x01 \\x01(\\t\\x12)\\n\\x04rect\\x18\\x02 \\x03(\\x0b\\x32\\x1b.protobuf_annolist.AnnoRect\\x12\\x10\\n\\x08imgWidth\\x18\\x03 \\x01(\\x05\\x12\\x11\\n\\timgHeight\\x18\\x04 \\x01(\\x05\\""w\\n\\x08\\x41nnoList\\x12\\x31\\n\\nannotation\\x18\\x01 \\x03(\\x0b\\x32\\x1d.protobuf_annolist.Annotation\\x12\\x38\\n\\x0e\\x61ttribute_desc\\x18\\x02 \\x03(\\x0b\\x32 .protobuf_annolist.AttributeDescB\\x0c\\x42\\nAnnoListPb\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_ATTRIBUTE = _descriptor.Descriptor(\n  name=\'Attribute\',\n  full_name=\'protobuf_annolist.Attribute\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.Attribute.id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val\', full_name=\'protobuf_annolist.Attribute.val\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fval\', full_name=\'protobuf_annolist.Attribute.fval\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'strval\', full_name=\'protobuf_annolist.Attribute.strval\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=37,\n  serialized_end=103,\n)\n\n\n_IDSTRPAIR = _descriptor.Descriptor(\n  name=\'IdStrPair\',\n  full_name=\'protobuf_annolist.IdStrPair\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.IdStrPair.id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'s\', full_name=\'protobuf_annolist.IdStrPair.s\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=105,\n  serialized_end=139,\n)\n\n\n_ATTRIBUTEDESC = _descriptor.Descriptor(\n  name=\'AttributeDesc\',\n  full_name=\'protobuf_annolist.AttributeDesc\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'protobuf_annolist.AttributeDesc.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.AttributeDesc.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'protobuf_annolist.AttributeDesc.dtype\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val_to_str\', full_name=\'protobuf_annolist.AttributeDesc.val_to_str\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=141,\n  serialized_end=247,\n)\n\n\n_ANNORECTATTRIBUTE = _descriptor.Descriptor(\n  name=\'AnnoRectAttribute\',\n  full_name=\'protobuf_annolist.AnnoRectAttribute\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'protobuf_annolist.AnnoRectAttribute.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val\', full_name=\'protobuf_annolist.AnnoRectAttribute.val\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=249,\n  serialized_end=295,\n)\n\n\n_ANNORECT = _descriptor.Descriptor(\n  name=\'AnnoRect\',\n  full_name=\'protobuf_annolist.AnnoRect\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'x1\', full_name=\'protobuf_annolist.AnnoRect.x1\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y1\', full_name=\'protobuf_annolist.AnnoRect.y1\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x2\', full_name=\'protobuf_annolist.AnnoRect.x2\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y2\', full_name=\'protobuf_annolist.AnnoRect.y2\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'protobuf_annolist.AnnoRect.score\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.AnnoRect.id\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'track_id\', full_name=\'protobuf_annolist.AnnoRect.track_id\', index=6,\n      number=11, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attribute\', full_name=\'protobuf_annolist.AnnoRect.attribute\', index=7,\n      number=12, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=298,\n  serialized_end=450,\n)\n\n\n_ANNOTATION = _descriptor.Descriptor(\n  name=\'Annotation\',\n  full_name=\'protobuf_annolist.Annotation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'imageName\', full_name=\'protobuf_annolist.Annotation.imageName\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rect\', full_name=\'protobuf_annolist.Annotation.rect\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'imgWidth\', full_name=\'protobuf_annolist.Annotation.imgWidth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'imgHeight\', full_name=\'protobuf_annolist.Annotation.imgHeight\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=452,\n  serialized_end=563,\n)\n\n\n_ANNOLIST = _descriptor.Descriptor(\n  name=\'AnnoList\',\n  full_name=\'protobuf_annolist.AnnoList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'annotation\', full_name=\'protobuf_annolist.AnnoList.annotation\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attribute_desc\', full_name=\'protobuf_annolist.AnnoList.attribute_desc\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=565,\n  serialized_end=684,\n)\n\n_ATTRIBUTEDESC.fields_by_name[\'val_to_str\'].message_type = _IDSTRPAIR\n_ANNORECT.fields_by_name[\'attribute\'].message_type = _ATTRIBUTE\n_ANNOTATION.fields_by_name[\'rect\'].message_type = _ANNORECT\n_ANNOLIST.fields_by_name[\'annotation\'].message_type = _ANNOTATION\n_ANNOLIST.fields_by_name[\'attribute_desc\'].message_type = _ATTRIBUTEDESC\nDESCRIPTOR.message_types_by_name[\'Attribute\'] = _ATTRIBUTE\nDESCRIPTOR.message_types_by_name[\'IdStrPair\'] = _IDSTRPAIR\nDESCRIPTOR.message_types_by_name[\'AttributeDesc\'] = _ATTRIBUTEDESC\nDESCRIPTOR.message_types_by_name[\'AnnoRectAttribute\'] = _ANNORECTATTRIBUTE\nDESCRIPTOR.message_types_by_name[\'AnnoRect\'] = _ANNORECT\nDESCRIPTOR.message_types_by_name[\'Annotation\'] = _ANNOTATION\nDESCRIPTOR.message_types_by_name[\'AnnoList\'] = _ANNOLIST\n\nAttribute = _reflection.GeneratedProtocolMessageType(\'Attribute\', (_message.Message,), dict(\n  DESCRIPTOR = _ATTRIBUTE,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.Attribute)\n  ))\n_sym_db.RegisterMessage(Attribute)\n\nIdStrPair = _reflection.GeneratedProtocolMessageType(\'IdStrPair\', (_message.Message,), dict(\n  DESCRIPTOR = _IDSTRPAIR,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.IdStrPair)\n  ))\n_sym_db.RegisterMessage(IdStrPair)\n\nAttributeDesc = _reflection.GeneratedProtocolMessageType(\'AttributeDesc\', (_message.Message,), dict(\n  DESCRIPTOR = _ATTRIBUTEDESC,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AttributeDesc)\n  ))\n_sym_db.RegisterMessage(AttributeDesc)\n\nAnnoRectAttribute = _reflection.GeneratedProtocolMessageType(\'AnnoRectAttribute\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNORECTATTRIBUTE,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoRectAttribute)\n  ))\n_sym_db.RegisterMessage(AnnoRectAttribute)\n\nAnnoRect = _reflection.GeneratedProtocolMessageType(\'AnnoRect\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNORECT,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoRect)\n  ))\n_sym_db.RegisterMessage(AnnoRect)\n\nAnnotation = _reflection.GeneratedProtocolMessageType(\'Annotation\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNOTATION,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.Annotation)\n  ))\n_sym_db.RegisterMessage(Annotation)\n\nAnnoList = _reflection.GeneratedProtocolMessageType(\'AnnoList\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNOLIST,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoList)\n  ))\n_sym_db.RegisterMessage(AnnoList)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'B\\nAnnoListPb\'))\n# @@protoc_insertion_point(module_scope)\n'"
TENSORBOX/utils/annolist/AnnotationLib.py,0,"b'import os\r\n\r\nfrom math import sqrt\r\n\r\nimport gzip\r\nimport bz2\r\nimport sys\r\nimport numpy as np;\r\n\r\nfrom collections import MutableSequence\r\n\r\n#import AnnoList_pb2\r\nimport PalLib;\r\n\r\nimport xml.dom.minidom\r\nfrom xml.dom.minidom import Node\r\nxml_dom_ext_available=False\r\ntry:\r\n\timport xml.dom.ext\r\n\txml_dom_ext_available=True\r\nexcept ImportError:\r\n\tpass\r\n\r\n\r\n################################################\r\n#\r\n#  TODO: check distance function\r\n#\r\n################################################\r\n\r\n\r\ndef cmpAnnRectsByScore(r1, r2):\r\n\treturn cmp(r1.score, r2.score)\r\n\r\ndef cmpAnnoRectsByScoreDescending(r1, r2):\r\n\treturn (-1)*cmp(r1.score, r2.score)\r\n\r\ndef cmpDetAnnoRectsByScore(r1, r2):\r\n\treturn cmp(r1.rect.score, r2.rect.score);\r\n\r\n\r\ndef suffixMatch(fn1, fn2):\r\n\tl1 = len(fn1);\r\n\tl2 = len(fn2);\r\n\t\t\r\n\tif fn1[-l2:] == fn2:\r\n\t\treturn True\r\n\t\r\n\tif fn2[-l1:] == fn1:\r\n\t\treturn True\r\n\t\r\n\treturn False\t\t\r\n\r\nclass AnnoList(MutableSequence):\r\n\t""""""Define a list format, which I can customize""""""\r\n\tTYPE_INT32 = 5;\r\n\tTYPE_FLOAT = 2;\r\n\tTYPE_STRING = 9;\r\n\r\n\tdef __init__(self, data=None):\r\n\t\tsuper(AnnoList, self).__init__()\r\n\r\n\t\tself.attribute_desc = {};\r\n                self.attribute_val_to_str = {};\r\n\r\n\t\tif not (data is None):\r\n\t\t\tself._list = list(data)\r\n\t\telse:\r\n\t\t\tself._list = list()\t\r\n        \r\n\tdef add_attribute(self, name, dtype):\r\n\t\t_adesc = AnnoList_pb2.AttributeDesc();\r\n\t\t_adesc.name = name;\r\n\t\tif self.attribute_desc:\r\n\t\t\t_adesc.id = max((self.attribute_desc[d].id for d in self.attribute_desc)) + 1;\r\n\t\telse:\r\n\t\t\t_adesc.id = 0;\r\n\r\n\t\tif dtype == int:\r\n\t\t\t_adesc.dtype = AnnoList.TYPE_INT32;\r\n\t\telif dtype == float or dtype == np.float32:\r\n\t\t\t_adesc.dtype = AnnoList.TYPE_FLOAT;\r\n\t\telif dtype == str:\r\n\t\t\t_adesc.dtype = AnnoList.TYPE_STRING;\r\n\t\telse:\r\n                        print ""unknown attribute type: "", dtype\r\n\t\t\tassert(False);\r\n\t\t\r\n\t\t#print ""adding attribute: {}, id: {}, type: {}"".format(_adesc.name, _adesc.id, _adesc.dtype);\r\n\t\tself.attribute_desc[name] = _adesc;\r\n\r\n        def add_attribute_val(self, aname, vname, val):\r\n                # add attribute before adding string corresponding to integer value\r\n                assert(aname in self.attribute_desc);\r\n\r\n                # check and add if new \r\n                if all((val_desc.id != val for val_desc in self.attribute_desc[aname].val_to_str)):\r\n                        val_desc = self.attribute_desc[aname].val_to_str.add()\r\n                        val_desc.id = val;\r\n                        val_desc.s = vname;\r\n\r\n                # also add to map for quick access\r\n                if not aname in self.attribute_val_to_str:\r\n                        self.attribute_val_to_str[aname] = {};\r\n\r\n                assert(not val in self.attribute_val_to_str[aname]);\r\n                self.attribute_val_to_str[aname][val] = vname;\r\n\r\n\r\n        def attribute_get_value_str(self, aname, val):\r\n                if aname in self.attribute_val_to_str and val in self.attribute_val_to_str[aname]:\r\n                        return self.attribute_val_to_str[aname][val];\r\n                else:\r\n                        return str(val);\r\n\r\n        def save(self, fname):\r\n                save(fname, self);\r\n        \r\n\t#MA: list interface   \r\n\tdef __len__(self):\r\n\t\treturn len(self._list)\r\n\r\n\tdef __getitem__(self, ii):\r\n                if isinstance(ii, slice):\r\n                        res = AnnoList();\r\n                        res.attribute_desc = self.attribute_desc;\r\n                        res._list = self._list[ii]\r\n                        return res;\r\n                else:\r\n                        return self._list[ii]\r\n\r\n\tdef __delitem__(self, ii):\r\n\t\tdel self._list[ii]\r\n\r\n\tdef __setitem__(self, ii, val):\r\n\t\treturn self._list[ii]\r\n\r\n\tdef __str__(self):\r\n\t\treturn self.__repr__()\r\n\r\n\tdef __repr__(self):\r\n\t\treturn """"""<AnnoList %s>"""""" % self._list\r\n\r\n\tdef insert(self, ii, val):\r\n\t\tself._list.insert(ii, val)\r\n\r\n\tdef append(self, val):\r\n\t\tlist_idx = len(self._list)\r\n\t\tself.insert(list_idx, val)\r\n\r\n\r\ndef is_compatible_attr_type(protobuf_type, attr_type):\r\n        if protobuf_type == AnnoList.TYPE_INT32:\r\n                return (attr_type == int);\r\n        elif protobuf_type == AnnoList.TYPE_FLOAT:\r\n                return (attr_type == float or attr_type == np.float32);\r\n        elif protobuf_type == AnnoList.TYPE_STRING:\r\n                return (attr_type == str);\r\n        else:\r\n                assert(false);\r\n        \r\n\r\ndef protobuf_type_to_python(protobuf_type):\r\n        if protobuf_type == AnnoList.TYPE_INT32:\r\n                return int;\r\n        elif protobuf_type == AnnoList.TYPE_FLOAT:\r\n                return float;\r\n        elif protobuf_type == AnnoList.TYPE_STRING:\r\n                return str;\r\n        else:\r\n                assert(false);\r\n\r\n\r\nclass AnnoPoint(object):\r\n\tdef __init__(self, x=None, y=None, id=None):\r\n\t\tself.x = x;\r\n\t\tself.y = y;\r\n\t\tself.id = id;\r\n\r\nclass AnnoRect(object):\r\n\tdef __init__(self, x1=-1, y1=-1, x2=-1, y2=-1):\r\n\r\n\t\tself.x1 = x1\r\n\t\tself.y1 = y1\r\n\t\tself.x2 = x2\r\n\t\tself.y2 = y2\r\n\r\n\t\tself.score = -1.0\r\n\t\tself.scale = -1.0\r\n\t\tself.articulations =[]\r\n\t\tself.viewpoints =[]\r\n\t\tself.d3 = []\r\n\r\n\t\tself.silhouetteID = -1\r\n\t\tself.classID = -1\r\n\t\tself.track_id = -1\r\n\r\n\t\tself.point = [];\r\n                self.at = {};\r\n\r\n\tdef width(self):\r\n\t\treturn abs(self.x2-self.x1)\r\n\r\n\tdef height(self):\r\n\t\treturn abs(self.y2-self.y1)\r\n\r\n\tdef centerX(self):\r\n\t\treturn (self.x1+self.x2)/2.0\r\n\r\n\tdef centerY(self):\r\n\t\treturn (self.y1+self.y2)/2.0\r\n\r\n\tdef left(self):\r\n\t\treturn min(self.x1, self.x2)\r\n\r\n\tdef right(self):\r\n\t\treturn max(self.x1, self.x2)\r\n\r\n\tdef top(self):\r\n\t\treturn min(self.y1, self.y2)\r\n\r\n\tdef bottom(self):\r\n\t\treturn max(self.y1, self.y2)\r\n\r\n\tdef forceAspectRatio(self, ratio, KeepHeight = False, KeepWidth = False):\r\n\t\t""""""force the Aspect ratio""""""\r\n\t\tif KeepWidth or ((not KeepHeight) and self.width() * 1.0 / self.height() > ratio):\r\n\t\t\t# extend height\r\n\t\t\tnewHeight = self.width() * 1.0 / ratio\r\n\t\t\tself.y1 = (self.centerY() - newHeight / 2.0)\r\n\t\t\tself.y2 = (self.y1 + newHeight)\r\n\t\telse:\r\n\t\t\t# extend width\r\n\t\t\tnewWidth = self.height() * ratio\r\n\t\t\tself.x1 = (self.centerX() - newWidth / 2.0)\r\n\t\t\tself.x2 = (self.x1 + newWidth)\r\n\t\t\t\r\n\tdef clipToImage(self, min_x, max_x, min_y, max_y):\r\n\t\t\tself.x1 = max(min_x, self.x1)\r\n\t\t\tself.x2 = max(min_x, self.x2)\r\n\t\t\tself.y1 = max(min_y, self.y1)\r\n\t\t\tself.y2 = max(min_y, self.y2)\r\n\t\t\tself.x1 = min(max_x, self.x1)\r\n\t\t\tself.x2 = min(max_x, self.x2)\r\n\t\t\tself.y1 = min(max_y, self.y1)\r\n\t\t\tself.y2 = min(max_y, self.y2)\r\n\r\n\tdef printContent(self):\r\n\t\tprint ""Coords: "", self.x1, self.y1, self.x2, self.y2\r\n\t\tprint ""Score: "", self.score\r\n\t\tprint ""Articulations: "", self.articulations\r\n\t\tprint ""Viewpoints: "", self.viewpoints\r\n\t\tprint ""Silhouette: "", self.silhouetteID\r\n\r\n\tdef ascii(self):\r\n\t\tr = ""(""+str(self.x1)+"", ""+str(self.y1)+"", ""+str(self.x2)+"", ""+str(self.y2)+"")""\r\n\t\tif (self.score!=-1):\r\n\t\t\tr = r + "":""+str(self.score)\r\n\t\tif (self.silhouetteID !=-1):\r\n\t\t\tri = r + ""/""+str(self.silhouetteID)\r\n\t\treturn r\r\n\r\n\tdef writeIDL(self, file):\r\n\t\tfile.write("" (""+str(self.x1)+"", ""+str(self.y1)+"", ""+str(self.x2)+"", ""+str(self.y2)+"")"")\r\n\t\tif (self.score!=-1):\r\n\t\t\tfile.write("":""+str(self.score))\r\n\t\tif (self.silhouetteID !=-1):\r\n\t\t\tfile.write(""/""+str(self.silhouetteID))\r\n\r\n\tdef sortCoords(self):\r\n\t\tif (self.x1>self.x2):\r\n\t\t\tself.x1, self.x2 = self.x2, self.x1\r\n\t\tif (self.y1>self.y2):\r\n\t\t\tself.y1, self.y2 = self.y2, self.y1\r\n\r\n\tdef rescale(self, factor):\r\n\t\tself.x1=(self.x1*float(factor))\r\n\t\tself.y1=(self.y1*float(factor))\r\n\t\tself.x2=(self.x2*float(factor))\r\n\t\tself.y2=(self.y2*float(factor))\r\n\r\n\tdef resize(self, factor, factor_y = None):\r\n\t\tw = self.width()\r\n\t\th = self.height()\r\n\t\tif factor_y is None:\r\n\t\t\tfactor_y = factor\r\n\t\tcenterX = float(self.x1+self.x2)/2.0\r\n\t\tcenterY = float(self.y1+self.y2)/2.0\r\n\t\tself.x1 = (centerX - (w/2.0)*factor)\r\n\t\tself.y1 = (centerY - (h/2.0)*factor_y)\r\n\t\tself.x2 = (centerX + (w/2.0)*factor)\r\n\t\tself.y2 = (centerY + (h/2.0)*factor_y)\r\n\t\t\r\n\r\n\tdef intersection(self, other):\r\n\t\tself.sortCoords()\r\n\t\tother.sortCoords()\r\n\t\t\r\n\t\tif(self.x1 >= other.x2):\r\n\t\t\treturn (0, 0)\t\t\r\n\t\tif(self.x2 <= other.x1):\r\n\t\t\treturn (0, 0)\r\n\t\tif(self.y1 >= other.y2):\r\n\t\t\treturn (0, 0)\t\r\n\t\tif(self.y2 <= other.y1):\r\n\t\t\treturn (0, 0)\r\n\t\t\r\n\t\tl = max(self.x1, other.x1);\r\n\t\tt = max(self.y1, other.y1);\r\n\t\tr = min(self.x2, other.x2);\r\n\t\tb = min(self.y2, other.y2);\r\n\t\treturn (r - l, b - t)\r\n\t\t\r\n\t\t#Alternate implementation\r\n\t\t#nWidth  = self.x2 - self.x1\r\n\t\t#nHeight = self.y2 - self.y1\r\n\t\t#iWidth  = max(0,min(max(0,other.x2-self.x1),nWidth )-max(0,other.x1-self.x1))\r\n\t\t#iHeight = max(0,min(max(0,other.y2-self.y1),nHeight)-max(0,other.y1-self.y1))\r\n\t\t#return (iWidth, iHeight)\r\n\r\n\tdef cover(self, other):\r\n\t\tnWidth = self.width()\r\n\t\tnHeight = self.height()\r\n\t\tiWidth, iHeight = self.intersection(other)\t\t\r\n\t\treturn float(iWidth * iHeight) / float(nWidth * nHeight)\r\n\r\n\tdef overlap_pascal(self, other):\r\n\t\tself.sortCoords()\r\n\t\tother.sortCoords()\r\n\r\n\t\tnWidth  = self.x2 - self.x1\r\n\t\tnHeight = self.y2 - self.y1\r\n\t\tiWidth, iHeight = self.intersection(other)\r\n\t\tinterSection = iWidth * iHeight\r\n\t\t\t\r\n\t\tunion = self.width() * self.height() + other.width() * other.height() - interSection\r\n\t\t\t\r\n\t\toverlap = interSection * 1.0 / union\r\n\t\treturn overlap\r\n\r\n\tdef isMatchingPascal(self, other, minOverlap):\r\n\t\toverlap = self.overlap_pascal(other)\r\n\t\tif (overlap >= minOverlap and (self.classID == -1 or other.classID == -1 or self.classID == other.classID)):\r\n\t\t\treturn 1\r\n\t\telse:\r\n\t\t\treturn 0\r\n\r\n\tdef distance(self, other, aspectRatio=-1, fixWH=\'fixheight\'):\r\n\t\tif (aspectRatio!=-1):\r\n\t\t\tif (fixWH==\'fixwidth\'):\r\n\t\t\t\tdWidth  = float(self.x2 - self.x1)\r\n\t\t\t\tdHeight = dWidth / aspectRatio\r\n\t\t\telif (fixWH==\'fixheight\'):\r\n\t\t\t\tdHeight = float(self.y2 - self.y1)\r\n\t\t\t\tdWidth  = dHeight * aspectRatio\r\n\t\telse:\r\n\t\t\tdWidth  = float(self.x2 - self.x1)\r\n\t\t\tdHeight = float(self.y2 - self.y1)\r\n\r\n\t\txdist   = (self.x1 + self.x2 - other.x1 - other.x2) / dWidth\r\n\t\tydist   = (self.y1 + self.y2 - other.y1 - other.y2) / dHeight\r\n\r\n\t\treturn sqrt(xdist*xdist + ydist*ydist)\r\n\r\n\tdef isMatchingStd(self, other, coverThresh, overlapThresh, distThresh, aspectRatio=-1, fixWH=-1):\r\n\t\tcover = other.cover(self)\r\n\t\toverlap = self.cover(other)\r\n\t\tdist = self.distance(other, aspectRatio, fixWH)\r\n\r\n\t\t#if(self.width() == 24 ):\r\n\t\t#print cover, "" "", overlap, "" "", dist\r\n\t\t#print coverThresh, overlapThresh, distThresh\r\n\t\t#print (cover>=coverThresh and overlap>=overlapThresh and dist<=distThresh)\r\n\t\t\r\n\t\tif (cover>=coverThresh and overlap>=overlapThresh and dist<=distThresh and self.classID == other.classID):\r\n\t\t\treturn 1\r\n\t\telse:\r\n\t\t\treturn 0\r\n\r\n\tdef isMatching(self, other, style, coverThresh, overlapThresh, distThresh, minOverlap, aspectRatio=-1, fixWH=-1):\r\n\t\t#choose matching style\r\n\t\tif (style == 0):\r\n\t\t\treturn self.isMatchingStd(other, coverThresh, overlapThresh, distThresh, aspectRatio=-1, fixWH=-1)\r\n\r\n\t\tif (style == 1):\r\n\t\t\treturn self.isMatchingPascal(other, minOverlap)\r\n\r\n\tdef addToXML(self, node, doc): # no Silhouette yet\r\n\t\trect_el = doc.createElement(""annorect"")\r\n\t\tfor item in ""x1 y1 x2 y2 score scale track_id"".split():\r\n\t\t\tcoord_el = doc.createElement(item)\r\n\t\t\tcoord_val = doc.createTextNode(str(self.__getattribute__(item)))\r\n\t\t\tcoord_el.appendChild(coord_val)\r\n\t\t\trect_el.appendChild(coord_el)\r\n\t\t\t\r\n\t\tarticulation_el = doc.createElement(""articulation"")\r\n\t\tfor articulation in self.articulations:\r\n\t\t\tid_el = doc.createElement(""id"")\r\n\t\t\tid_val = doc.createTextNode(str(articulation))\r\n\t\t\tid_el.appendChild(id_val)\r\n\t\t\tarticulation_el.appendChild(id_el)\r\n\t\tif(len(self.articulations) > 0):\r\n\t\t\trect_el.appendChild(articulation_el)\r\n\t\t\t\r\n\t\tviewpoint_el    = doc.createElement(""viewpoint"")\r\n\t\tfor viewpoint in self.viewpoints:\r\n\t\t\tid_el = doc.createElement(""id"")\r\n\t\t\tid_val = doc.createTextNode(str(viewpoint))\r\n\t\t\tid_el.appendChild(id_val)\r\n\t\t\tviewpoint_el.appendChild(id_el)\r\n\t\tif(len(self.viewpoints) > 0):\r\n\t\t\trect_el.appendChild(viewpoint_el)\r\n\t\t\r\n\t\td3_el    = doc.createElement(""D3"")\t\t\t\t\t\r\n\t\tfor d in self.d3:\r\n\t\t\tid_el = doc.createElement(""id"")\r\n\t\t\tid_val = doc.createTextNode(str(d))\r\n\t\t\tid_el.appendChild(id_val)\r\n\t\t\td3_el.appendChild(id_el)\r\n\t\tif(len(self.d3) > 0):\r\n\t\t\trect_el.appendChild(d3_el)\r\n\t\t\t\t\t\r\n\t\tif self.silhouetteID != -1:\r\n\t\t\tsilhouette_el    = doc.createElement(""silhouette"")\r\n\t\t\tid_el = doc.createElement(""id"")\r\n\t\t\tid_val = doc.createTextNode(str(self.silhouetteID))\r\n\t\t\tid_el.appendChild(id_val)\r\n\t\t\tsilhouette_el.appendChild(id_el)\r\n\t\t\trect_el.appendChild(silhouette_el)\r\n\r\n\t\tif self.classID != -1:\r\n\t\t\tclass_el    = doc.createElement(""classID"")\r\n\t\t\tclass_val = doc.createTextNode(str(self.classID))\r\n\t\t\tclass_el.appendChild(class_val)\r\n\t\t\trect_el.appendChild(class_el)\r\n\r\n\t\tif len(self.point) > 0:\r\n\t\t\tannopoints_el = doc.createElement(""annopoints"")\r\n\r\n\t\t\tfor p in self.point:\r\n\t\t\t\tpoint_el = doc.createElement(""point"");\r\n\t\t\t\t\r\n\t\t\t\tpoint_id_el = doc.createElement(""id"");\r\n\t\t\t\tpoint_id_val = doc.createTextNode(str(p.id));\r\n\t\t\t\tpoint_id_el.appendChild(point_id_val);\r\n\t\t\t\tpoint_el.appendChild(point_id_el);\r\n\r\n\t\t\t\tpoint_x_el = doc.createElement(""x"");\r\n\t\t\t\tpoint_x_val = doc.createTextNode(str(p.x));\r\n\t\t\t\tpoint_x_el.appendChild(point_x_val);\r\n\t\t\t\tpoint_el.appendChild(point_x_el);\r\n\r\n\t\t\t\tpoint_y_el = doc.createElement(""y"");\r\n\t\t\t\tpoint_y_val = doc.createTextNode(str(p.y));\r\n\t\t\t\tpoint_y_el.appendChild(point_y_val);\r\n\t\t\t\tpoint_el.appendChild(point_y_el);\r\n\r\n\t\t\t\tannopoints_el.appendChild(point_el);\r\n\t\t\t\r\n\t\t\trect_el.appendChild(annopoints_el);\r\n\t\t\t\r\n\t\tnode.appendChild(rect_el)\r\n\r\n\r\n\r\nclass Annotation(object):\r\n\r\n\tdef __init__(self):\r\n\t\tself.imageName = """"\r\n\t\tself.imagePath = """"\r\n\t\tself.rects =[]\r\n\t\tself.frameNr = -1\r\n\t\r\n\tdef clone_empty(self):\r\n\t\tnew = Annotation()\r\n\t\tnew.imageName = self.imageName\r\n\t\tnew.imagePath = self.imagePath\r\n\t\tnew.frameNr   = self.frameNr\r\n\t\tnew.rects     = []\r\n\t\treturn new\r\n\r\n\tdef filename(self):\r\n\t\treturn os.path.join(self.imagePath, self.imageName)\r\n\r\n\tdef printContent(self):\r\n\t\tprint ""Name: "", self.imageName\r\n\t\tfor rect in self.rects:\r\n\t\t\trect.printContent()\r\n\r\n\tdef writeIDL(self, file):\r\n\t\tif (self.frameNr == -1):\r\n\t\t\tfile.write(""\\""""+os.path.join(self.imagePath, self.imageName)+""\\"""")\r\n\t\telse:\r\n\t\t\tfile.write(""\\""""+os.path.join(self.imagePath, self.imageName)+""@%d\\"""" % self.frameNr)\r\n\r\n\t\tif (len(self.rects)>0):\r\n\t\t\tfile.write("":"")\r\n\t\ti=0\r\n\t\tfor rect in self.rects:\r\n\t\t\trect.writeIDL(file)\r\n\t\t\tif (i+1<len(self.rects)):\r\n\t\t\t\tfile.write("","")\r\n\t\t\ti+=1\r\n\r\n\tdef addToXML(self, node, doc): # no frame# yet\r\n\t\tannotation_el = doc.createElement(""annotation"")\r\n\t\timg_el = doc.createElement(""image"")\r\n\t\tname_el = doc.createElement(""name"")\t\t\r\n\t\tname_val = doc.createTextNode(os.path.join(self.imagePath, self.imageName))\r\n\t\tname_el.appendChild(name_val)\r\n\t\timg_el.appendChild(name_el)\r\n\t\t\r\n\t\tif(self.frameNr != -1):\r\n\t\t\tframe_el = doc.createElement(""frameNr"")\r\n\t\t\tframe_val = doc.createTextNode(str(self.frameNr))\r\n\t\t\tframe_el.appendChild(frame_val)\r\n\t\t\timg_el.appendChild(frame_el)\r\n\t\t\r\n\t\tannotation_el.appendChild(img_el)\r\n\t\tfor rect in self.rects:\r\n\t\t\trect.addToXML(annotation_el, doc)\r\n\t\tnode.appendChild(annotation_el)\r\n\r\n\r\n\tdef sortByScore(self, dir=""ascending""):\r\n\t\tif (dir==""descending""):\r\n\t\t\tself.rects.sort(cmpAnnoRectsByScoreDescending)\r\n\t\telse:\r\n\t\t\tself.rects.sort(cmpAnnoRectsByScore)\r\n\r\n\tdef __getitem__(self, index):\r\n\t\treturn self.rects[index]\r\n\r\nclass detAnnoRect:\r\n\tdef __init(self):\r\n\t\tself.imageName = """"\r\n\t\tself.frameNr = -1\r\n\t\tself.rect = AnnoRect()\r\n\t\tself.imageIndex = -1\r\n\t\tself.boxIndex = -1\r\n\r\n#####################################################################\r\n### Parsing\r\n\r\ndef parseTii(filename):\r\n\r\n        # MA: this must be some really old code\r\n        assert(False);\r\n\tannotations = []\r\n\r\n\t#--- parse xml ---#\r\n\tdoc = xml.dom.minidom.parse(filename)\r\n\r\n\t#--- get tags ---#\r\n\tfor file in doc.getElementsByTagName(""file""):\r\n\r\n\t\tanno = Annotation()\r\n\r\n\t\tfor filename in file.getElementsByTagName(""filename""):\r\n\t\t\taNode = filename.getAttributeNode(""Src"")\r\n\t\t\tanno.imageName = aNode.firstChild.data[:-4]+"".png""\r\n\r\n\t\tfor objects in file.getElementsByTagName(""objects""):\r\n\r\n\t\t\tfor vehicle in objects.getElementsByTagName(""vehicle""):\r\n\r\n\t\t\t\taNode = vehicle.getAttributeNode(""Type"")\r\n\t\t\t\ttype = aNode.firstChild.data\r\n\r\n\t\t\t\tif (type==""pedestrian""):\r\n\r\n\t\t\t\t\trect = AnnoRect()\r\n\t\t\t\t\taNode = vehicle.getAttributeNode(""FR"")\r\n\t\t\t\t\tfrontrear = aNode.firstChild.data\r\n\t\t\t\t\taNode = vehicle.getAttributeNode(""SD"")\r\n\t\t\t\t\tside = aNode.firstChild.data\r\n\t\t\t\t\tif (frontrear == ""1""):\r\n\t\t\t\t\t\torientation=""FR""\r\n\t\t\t\t\telif (side == ""1""):\r\n\t\t\t\t\t\torientation=""SD""\r\n\t\t\t\t\taNode = vehicle.getAttributeNode( orientation+""_TopLeft_X"")\r\n\t\t\t\t\trect.x1 = float(aNode.firstChild.data)\r\n\t\t\t\t\taNode = vehicle.getAttributeNode( orientation+""_TopLeft_Y"")\r\n\t\t\t\t\trect.y1 = float(aNode.firstChild.data)\r\n\t\t\t\t\taNode = vehicle.getAttributeNode( orientation+""_BottomRight_X"")\r\n\t\t\t\t\trect.x2 = float(aNode.firstChild.data)\r\n\t\t\t\t\taNode = vehicle.getAttributeNode( orientation+""_BottomRight_Y"")\r\n\t\t\t\t\trect.y2 = float(aNode.firstChild.data)\r\n\t\t\t\t\tprint ""pedestrian:"", anno.imageName, rect.x1, rect.y1, rect.x2, rect.y2\r\n\t\t\t\t\tanno.rects.append(rect)\r\n\r\n\t\tannotations.append(anno)\r\n\r\n\treturn annotations\r\n\r\ndef parseXML(filename):\r\n\tfilename = os.path.realpath(filename)\r\n\r\n\tname, ext = os.path.splitext(filename)\r\n\r\n\tannotations = AnnoList([])\r\n\r\n\tif(ext == "".al""):\r\n\t\tfile = open(filename,\'r\')\r\n\t\tlines = file.read()\r\n\t\tfile.close()\r\n\t\r\n\tif(ext == "".gz""):\r\n\t\tzfile = gzip.GzipFile(filename)\r\n\t\tlines = zfile.read()\r\n\t\tzfile.close()\r\n\r\n\tif(ext == "".bz2""):\r\n\t\tbfile = bz2.BZ2File(filename)\r\n\t\tlines = bfile.read()\r\n\t\tbfile.close()\r\n\r\n\t#--- parse xml ---#\r\n\tdoc = xml.dom.minidom.parseString(lines)\r\n\r\n\t#--- get tags ---#\r\n\tfor annotation in doc.getElementsByTagName(""annotation""):\r\n\t\tanno = Annotation()\r\n\t\tfor image in annotation.getElementsByTagName(""image""):\r\n\t\t\tfor name in image.getElementsByTagName(""name""):\r\n\t\t\t\tanno.imageName = name.firstChild.data\r\n\r\n\t\t\tfor fn in image.getElementsByTagName(""frameNr""):\r\n\t\t\t\tanno.frameNr = int(fn.firstChild.data)\r\n\r\n\t\trects = []\r\n\t\tfor annoRect in annotation.getElementsByTagName(""annorect""):\r\n\t\t\trect = AnnoRect()\r\n\r\n\t\t\tfor x1 in annoRect.getElementsByTagName(""x1""):\r\n\t\t\t\trect.x1 = float(x1.firstChild.data)\r\n\r\n\t\t\tfor y1 in annoRect.getElementsByTagName(""y1""):\r\n\t\t\t\trect.y1 = float(y1.firstChild.data)\r\n\r\n\t\t\tfor x2 in annoRect.getElementsByTagName(""x2""):\r\n\t\t\t\trect.x2 = float(x2.firstChild.data)\r\n\r\n\t\t\tfor y2 in annoRect.getElementsByTagName(""y2""):\r\n\t\t\t\trect.y2 = float(y2.firstChild.data)\r\n\r\n\t\t\tfor scale in annoRect.getElementsByTagName(""scale""):\r\n\t\t\t\trect.scale = float(scale.firstChild.data)\r\n\r\n\t\t\tfor score in annoRect.getElementsByTagName(""score""):\r\n\t\t\t\trect.score = float(score.firstChild.data)\r\n\r\n\t\t\tfor classID in annoRect.getElementsByTagName(""classID""):\r\n\t\t\t\trect.classID = int(classID.firstChild.data)\r\n\r\n\t\t\tfor track_id in annoRect.getElementsByTagName(""track_id""):\r\n\t\t\t\trect.track_id = int(track_id.firstChild.data)\r\n\r\n\t\t\tfor articulation in annoRect.getElementsByTagName(""articulation""):\r\n\t\t\t\tfor id in articulation.getElementsByTagName(""id""):\r\n\t\t\t\t\trect.articulations.append(int(id.firstChild.data))\r\n\t\t\t\t#print ""Articulations: "", rect.articulations\r\n\r\n\t\t\tfor viewpoint in annoRect.getElementsByTagName(""viewpoint""):\r\n\t\t\t\tfor id in viewpoint.getElementsByTagName(""id""):\r\n\t\t\t\t\trect.viewpoints.append(int(id.firstChild.data))\r\n\t\t\t\t\t#print ""Viewpoints: "", rect.viewpoints\r\n\t\t\t\t\t\r\n\t\t\tfor d in annoRect.getElementsByTagName(""D3""):\r\n\t\t\t\tfor id in d.getElementsByTagName(""id""):\r\n\t\t\t\t\trect.d3.append(float(id.firstChild.data))\r\n\r\n\t\t\tfor silhouette in annoRect.getElementsByTagName(""silhouette""):\r\n\t\t\t\tfor id in silhouette.getElementsByTagName(""id""):\r\n\t\t\t\t\trect.silhouetteID = int(id.firstChild.data)\r\n\t\t\t\t#print ""SilhouetteID: "", rect.silhouetteID\r\n\r\n\t\t\tfor annoPoints in annoRect.getElementsByTagName(""annopoints""):\t\t\t\t\r\n\t\t\t\tfor annoPoint in annoPoints.getElementsByTagName(""point""):\r\n\r\n\t\t\t\t\tp = AnnoPoint();\r\n\t\t\t\t\tfor annoPointX in annoPoint.getElementsByTagName(""x""):\r\n\t\t\t\t\t\tp.x = int(float(annoPointX.firstChild.data));\r\n\r\n\t\t\t\t\tfor annoPointY in annoPoint.getElementsByTagName(""y""):\r\n\t\t\t\t\t\tp.y = int(float(annoPointY.firstChild.data));\r\n\t\t\t\t\t\t\r\n\t\t\t\t\tfor annoPointId in annoPoint.getElementsByTagName(""id""):\r\n\t\t\t\t\t\tp.id = int(annoPointId.firstChild.data);\r\n\r\n\t\t\t\t\tassert(p.x != None and p.y != None and p.id != None);\r\n\t\t\t\t\trect.point.append(p);\t\t\t\t\t\r\n\r\n\t\t\trects.append(rect)\r\n\r\n\t\tanno.rects = rects\r\n\t\tannotations.append(anno)\r\n\r\n\treturn annotations\r\n\r\n\r\ndef parse(filename, abs_path=False):\r\n\t#print ""Parsing: "", filename\r\n\tname, ext = os.path.splitext(filename)\r\n\t\r\n\tif (ext == "".gz"" or ext == "".bz2""):\r\n\t\tname, ext = os.path.splitext(name)\r\n\t\r\n\tif(ext == "".idl""):\r\n\t\tannolist = parseIDL(filename)\r\n        elif(ext == "".al""):\r\n\t\tannolist = parseXML(filename)\r\n        elif(ext == "".pal""):\r\n\t\tannolist = PalLib.pal2al(PalLib.loadPal(filename));\r\n        else:\r\n                annolist = AnnoList([]);\r\n\r\n        if abs_path:\r\n                basedir = os.path.dirname(os.path.abspath(filename))\r\n                for a in annolist:\r\n                        a.imageName = basedir + ""/"" + os.path.basename(a.imageName)\r\n\r\n\treturn annolist\r\n\r\n\r\ndef parseIDL(filename):\r\n\tfilename = os.path.realpath(filename)\r\n\r\n\tname, ext = os.path.splitext(filename)\r\n\r\n\tlines = []\r\n\tif(ext == "".idl""):\r\n\t\tfile = open(filename,\'r\')\r\n\t\tlines = file.readlines()\r\n\t\tfile.close()\r\n\t\r\n\tif(ext == "".gz""):\r\n\t\tzfile = gzip.GzipFile(filename)\r\n\t\tlines = zfile.readlines()\r\n\t\tzfile.close()\r\n\r\n\tif(ext == "".bz2""):\r\n\t\tbfile = bz2.BZ2File(filename)\r\n\t\tlines = bfile.readlines()\r\n\t\tbfile.close()\r\n\r\n        annotations = AnnoList([])\r\n\r\n\tfor line in lines:\r\n\t\tanno = Annotation()\r\n\r\n\t\t### remove line break\r\n\t\tif (line[-1]==\'\\n\'):\r\n\t\t\tline = line[:-1]; # remove \'\\n\'\r\n\t\tlineLen = len(line)\r\n\t\t#print line\r\n\r\n\t\t### get image name\r\n\t\tposImageEnd = line.find(\'\\"":\')\r\n\t\tif (posImageEnd==-1):\r\n\t\t\tposImageEnd = line.rfind(""\\"""")\r\n\t\tanno.imageName = line[1:posImageEnd]\r\n\t\t#print anno.imageName\r\n\r\n\t\tpos = anno.imageName.rfind(""@"")\r\n\t\tif (pos >= 0):\r\n\t\t\tanno.frameNr = int(anno.imageName[pos+1:])\r\n\t\t\tanno.imageName = anno.imageName[:pos]\r\n\t\t\tif anno.imageName[-1] == ""/"":\r\n\t\t\t\tanno.imageName = anno.imageName[:-1]\r\n\t\telse:\r\n\t\t\tanno.frameNr = -1\r\n\r\n\t\t### get rect list\r\n\t\t# we split by \',\'. there are 3 commas for each rect and 1 comma seperating the rects\r\n\t\trectSegs=[]\r\n\t\tif (posImageEnd!=-1 and posImageEnd+4<lineLen):\r\n\r\n\t\t\tline = line[posImageEnd+3:-1]; # remove ; or .\r\n\r\n\t\t\tsegments = line.split(\',\')\r\n\t\t\tif (len(segments)%4!=0):\r\n\t\t\t\tprint ""Parse Errror""\r\n\t\t\telse:\r\n\t\t\t\tfor i in range(0,len(segments),4):\r\n\t\t\t\t\trectSeg = segments[i]+"",""+segments[i+1]+"",""+segments[i+2]+"",""+segments[i+3]\r\n\t\t\t\t\trectSegs.append(rectSeg)\r\n\t\t\t\t\t#print rectSegs\r\n\r\n\t\t\t## parse rect segments\r\n\t\t\tfor rectSeg in rectSegs:\r\n\t\t\t\t#print ""RectSeg: "", rectSeg\r\n\t\t\t\trect = AnnoRect()\r\n\t\t\t\tposBracket1 = rectSeg.find(\'(\')\r\n\t\t\t\tposBracket2 = rectSeg.find(\')\')\r\n\t\t\t\tcoordinates = rectSeg[posBracket1+1:posBracket2].split(\',\')\r\n\t\t\t\t#print coordinates\r\n\t\t\t\t#print ""Coordinates: "",coordinates\t\t\t\t\r\n\t\t\t\trect.x1 = float(round(float(coordinates[0].strip())))\r\n\t\t\t\trect.y1 = float(round(float(coordinates[1].strip())))\r\n\t\t\t\trect.x2 = float(round(float(coordinates[2].strip())))\r\n\t\t\t\trect.y2 = float(round(float(coordinates[3].strip())))\r\n\t\t\t\tposColon = rectSeg.find(\':\')\r\n\t\t\t\tposSlash = rectSeg.find(\'/\')\r\n\t\t\t\tif (posSlash!=-1):\r\n\t\t\t\t\trect.silhouetteID = int(rectSeg[posSlash+1:])\r\n\t\t\t\telse:\r\n\t\t\t\t\trectSeg+=""\\n""\r\n\t\t\t\tif (posColon!=-1):\r\n\t\t\t\t\t#print rectSeg[posColon+1:posSlash]\r\n\t\t\t\t\trect.score = float(rectSeg[posColon+1:posSlash])\r\n\t\t\t\tanno.rects.append(rect)\r\n\r\n\t\tannotations.append(anno)\r\n\r\n\treturn annotations\r\n\r\n\r\n\r\n\t\r\n\r\n#####################################################################\r\n### Saving\r\n\r\ndef save(filename, annotations):\r\n\tprint ""saving: "", filename;\r\n\r\n\tname, ext = os.path.splitext(filename)\r\n\r\n\tif (ext == "".gz"" or ext == "".bz2""):\r\n\t\tname, ext = os.path.splitext(name)\r\n\t\r\n\tif(ext == "".idl""):\r\n\t\treturn saveIDL(filename, annotations)\t\t\r\n\r\n\telif(ext == "".al""):\r\n\t\treturn saveXML(filename, annotations)\r\n\r\n\telif(ext == "".pal""):\r\n\t\treturn PalLib.savePal(filename, PalLib.al2pal(annotations));\r\n\r\n\telse:\r\n\t\tassert(False);\r\n\t\treturn False;\r\n\r\ndef saveIDL(filename, annotations):\r\n\t[name, ext] = os.path.splitext(filename)\r\n\r\n\tif(ext == "".idl""):\r\n\t\tfile = open(filename,\'w\')\r\n\t\r\n\tif(ext == "".gz""):\r\n\t\tfile = gzip.GzipFile(filename, \'w\')\r\n\r\n\tif(ext == "".bz2""):\r\n\t\tfile = bz2.BZ2File(filename, \'w\')\r\n\r\n\ti=0\r\n\tfor annotation in annotations:\r\n\t\tannotation.writeIDL(file)\r\n\t\tif (i+1<len(annotations)):\r\n\t\t\tfile.write("";\\n"")\r\n\t\telse:\r\n\t\t\tfile.write("".\\n"")\r\n\t\ti+=1\r\n\r\n\tfile.close()\r\n\r\ndef idlBase(filename):\r\n\tif (filename.rfind("".pal"") == len(filename) - 4):\r\n\t\treturn (filename[:-4], "".pal"")\r\n\r\n\tif (filename.rfind("".idl"") == len(filename) - 4):\r\n\t\treturn (filename[:-4], "".idl"")\r\n\r\n\tif (filename.rfind("".al"") == len(filename) - 3):\r\n\t\treturn (filename[:-3], "".al"")\r\n\r\n\tif (filename.rfind("".idl.gz"") == len(filename) - 7):\r\n\t\treturn (filename[:-7], "".idl.gz"")\r\n\r\n\tif (filename.rfind("".idl.bz2"") == len(filename) - 8):\r\n\t\treturn (filename[:-8], "".idl.bz2"")\r\n\r\n\tif (filename.rfind("".al.gz"") == len(filename) - 6):\r\n\t\treturn (filename[:-6], "".al.gz"")\r\n\r\n\tif (filename.rfind("".al.bz2"") == len(filename) - 7):\r\n\t\treturn (filename[:-7], "".al.bz2"")\r\n\r\ndef saveXML(filename, annotations):\r\n\tdocument = xml.dom.minidom.Document()\r\n\trootnode = document.createElement(""annotationlist"")\r\n\tfor anno in annotations:\r\n\t\tanno.addToXML(rootnode, document)\r\n\tdocument.appendChild(rootnode)\r\n\t[name, ext] = os.path.splitext(filename)\r\n\tif(ext == "".al""):\r\n\t\twriter = open(filename,\'w\')\r\n\telif(ext == "".gz""):\r\n\t\twriter = gzip.GzipFile(filename, \'w\')\r\n\telif(ext == "".bz2""):\r\n\t\twriter = bz2.BZ2File(filename, \'w\')\r\n\telse:\r\n\t\tprint ""invalid filename - .al(.gz|.bz2) is accepted""\r\n\t\treturn\r\n\r\n\t\r\n\tif xml_dom_ext_available:\r\n\t\txml.dom.ext.PrettyPrint(document, writer)\r\n\telse:\r\n\t\t# MA: skip header (currently Matlab\'s loadannotations can\'t deal with the header)\r\n\t\tdocument.documentElement.writexml(writer);\r\n\r\n\t\t#document.writexml(writer)\r\n\r\n\tdocument.unlink()\r\n\r\n\r\n\r\n\r\n\r\n#####################################################################\r\n### Statistics\r\n\r\ndef getStats(annotations):\r\n\tno = 0\r\n\tnoTiny =0\r\n\tnoSmall =0\r\n\theights = []\r\n\twidths =[]\r\n\r\n\t###--- get all rects ---###\r\n\tfor anno in annotations:\r\n\t\tno = no + len(anno.rects)\r\n\t\tfor rect in anno.rects:\r\n\t\t\tif (rect.height()<36):\r\n\t\t\t\tnoTiny=noTiny+1\r\n\t\t\tif (rect.height()<128):\r\n\t\t\t\tnoSmall=noSmall+1\r\n\t\t\theights.append(rect.height())\r\n\t\t\tif (rect.width()==0):\r\n\t\t\t\tprint ""Warning: width=0 in image "", anno.imageName\r\n\t\t\t\twidths.append(1)\r\n\t\t\telse:\r\n\t\t\t\twidths.append(rect.width())\r\n\t\t\t\tif (float(rect.height())/float(rect.width())<1.5):\r\n\t\t\t\t\tprint ""Degenerated pedestrian annotation: "", anno.imageName\r\n\r\n\t###--- compute average height and variance ---###\r\n\tavgHeight = 0\r\n\tvarHeight = 0\r\n\r\n\r\n\tminHeight = 0\r\n\tmaxHeight = 0\r\n\tif len(heights) > 0:\r\n\t\tminHeight = heights[0]\r\n\t\tmaxHeight = heights[0]\r\n\r\n\tfor height in heights:\r\n\t\tavgHeight = avgHeight+height\r\n\t\tif (height > maxHeight):\r\n\t\t\tmaxHeight = height\r\n\t\tif (height < minHeight):\r\n\t\t\tminHeight = height\r\n\r\n\tif (no>0):\r\n\t\tavgHeight = avgHeight/no\r\n\tfor height in heights:\r\n\t\tvarHeight += (height-avgHeight)*(height-avgHeight)\r\n\tif (no>1):\r\n\t\tvarHeight=float(varHeight)/float(no-1)\r\n\r\n\t###--- compute average width and variance ---###\r\n\tavgWidth = 0\r\n\tvarWidth = 0\r\n\tfor width in widths:\r\n\t\tavgWidth = avgWidth+width\r\n\tif (no>0):\r\n\t\tavgWidth = avgWidth/no\r\n\tfor width in widths:\r\n\t\tvarWidth += (width-avgWidth)*(width-avgWidth)\r\n\r\n\tif (no>1):\r\n\t\tvarWidth=float(varWidth)/float(no-1)\r\n\r\n\t###--- write statistics ---###\r\n\tprint ""  Total # rects:"", no\r\n\tprint ""     avg. Width:"", avgWidth, "" ("", sqrt(varWidth), ""standard deviation )""\r\n\tprint ""    avg. Height:"", avgHeight, "" ("", sqrt(varHeight), ""standard deviation )""\r\n\tprint ""     tiny rects:"", noTiny, "" (< 36 pixels)""\r\n\tprint ""    small rects:"", noSmall, "" (< 128 pixels)""\r\n\tprint ""    minimum height:"", minHeight\r\n\tprint ""    maximum height:"", maxHeight\r\n\r\n\t###--- return ---###\r\n\treturn [widths, heights]\r\n\r\n############################################################\r\n##\r\n##  IDL merging\r\n##\r\n\r\ndef mergeIDL(detIDL, det2IDL, detectionFuse= True, minOverlap = 0.5):\r\n\tmergedIDL = []\r\n\r\n\tfor i,anno in enumerate(detIDL):\r\n\t\tmergedAnno = Annotation()\r\n\t\tmergedAnno.imageName = anno.imageName\r\n\t\tmergedAnno.frameNr = anno.frameNr\r\n\t\tmergedAnno.rects = anno.rects\r\n\r\n\t\timageFound = False\r\n\t\tfilterIndex = -1\r\n\t\tfor i,filterAnno in enumerate(det2IDL):\r\n\t\t\t\tif (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\r\n\t\t\t\t\tfilterIndex = i\r\n\t\t\t\t\timageFound = True\r\n\t\t\t\t\tbreak\r\n\r\n\t\tif(not imageFound):\r\n\t\t\tmergedIDL.append(mergedAnno)\r\n\t\t\tcontinue\r\n\r\n\t\tfor rect in det2IDL[filterIndex].rects:\r\n\t\t\tmatches = False\r\n\r\n\t\t\tfor frect in anno.rects:\r\n\t\t\t\tif rect.overlap_pascal(frect) > minOverlap:\r\n\t\t\t\t\tmatches = True\r\n\t\t\t\t\tbreak\r\n\r\n\t\t\tif (not matches or detectionFuse == False):\r\n\t\t\t\tmergedAnno.rects.append(rect)\r\n\r\n\t\tmergedIDL.append(mergedAnno)\r\n\r\n\treturn mergedIDL\r\n\r\n\r\n############################################################################33\r\n#\r\n# Function to force the aspect ratio of annotations to ratio = width / height\r\n#\r\n#\r\ndef forceAspectRatio(annotations, ratio, KeepHeight = False, KeepWidth = False):\r\n\tfor anno in annotations:\r\n\t\tfor rect in anno.rects:\r\n\t\t\trect.forceAspectRatio(ratio, KeepHeight, KeepWidth)\r\n\t\t\t#Determine which side needs to be extended\r\n#\t\t\tif (rect.width() * 1.0 / rect.height() > ratio):\r\n#\r\n#\t\t\t\t#Too wide -> extend height\r\n#\t\t\t\tnewHeight = rect.width() * 1.0 / ratio\r\n#\t\t\t\trect.y1 = int(rect.centerY() - newHeight / 2.0)\r\n#\t\t\t\trect.y2 = int(rect.y1 + newHeight)\r\n#\r\n#\t\t\telse:\r\n#\t\t\t\t#Too short -> extend width\r\n#\t\t\t\tnewWidth = rect.height() * ratio\r\n#\t\t\t\trect.x1 = int(rect.centerX() - newWidth / 2.0)\r\n#\t\t\t\trect.x2 = int(rect.x1 + newWidth)\r\n\r\n\r\n###################################################################\r\n# Function to greedyly remove subset detIDL from gtIDL\r\n#\r\n# returns two sets\r\n#\r\n# [filteredIDL, missingRecallIDL]\r\n#\r\n# filteredIDL == Rects that were present in both sets\r\n# missingRecallIDL == Rects that were only present in set gtIDL\r\n#\r\n###################################################################\r\ndef extractSubSet(gtIDL, detIDL):\r\n\tfilteredIDL = []\r\n\tmissingRecallIDL = []\r\n\r\n\tfor i,gtAnno in enumerate(gtIDL):\r\n\t\tfilteredAnno = Annotation()\r\n\t\tfilteredAnno.imageName = gtAnno.imageName\r\n\t\tfilteredAnno.frameNr = gtAnno.frameNr\r\n\r\n\t\tmissingRecallAnno = Annotation()\r\n\t\tmissingRecallAnno.imageName = gtAnno.imageName\r\n\t\tmissingRecallAnno.frameNr = gtAnno.frameNr\r\n\r\n\t\timageFound = False\r\n\t\tfilterIndex = -1\r\n\t\tfor i,anno in enumerate(detIDL):\r\n\t\t\t\tif (suffixMatch(anno.imageName, gtAnno.imageName) and anno.frameNr == gtAnno.frameNr):\r\n\t\t\t\t\tfilterIndex = i\r\n\t\t\t\t\timageFound = True\r\n\t\t\t\t\tbreak\r\n\r\n\t\tif(not imageFound):\r\n\t\t\tprint ""Image not found "" + gtAnno.imageName + "" !""\r\n\t\t\tmissingRecallIDL.append(gtAnno)\r\n\t\t\tfilteredIDL.append(filteredAnno)\r\n\t\t\tcontinue\r\n\r\n\t\tmatched = [-1] * len(detIDL[filterIndex].rects)\r\n\t\tfor j, rect in enumerate(gtAnno.rects):\r\n\t\t\tmatches = False\r\n\r\n\t\t\tmatchingID = -1\r\n\t\t\tminCenterPointDist = -1\r\n\t\t\tfor k,frect in enumerate(detIDL[filterIndex].rects):\r\n\t\t\t\tminCover = 0.5\r\n\t\t\t\tminOverlap = 0.5\r\n\t\t\t\tmaxDist = 0.5\r\n\r\n\t\t\t\tif rect.isMatchingStd(frect, minCover,minOverlap, maxDist):\r\n\t\t\t\t\tif (matchingID == -1 or rect.distance(frect) < minCenterPointDist):\r\n\t\t\t\t\t\tmatchingID = k\r\n\t\t\t\t\t\tminCenterPointDist = rect.distance(frect)\r\n\t\t\t\t\t\tmatches = True\r\n\r\n\t\t\tif (matches):\r\n\t\t\t\t#Already matched once check if you are the better match\r\n\t\t\t\tif(matched[matchingID] >= 0):\r\n\t\t\t\t\t#Take the match with the smaller center point distance\r\n\t\t\t\t\tif(gtAnno.rects[matched[matchingID]].distance(frect) > rect.distance(frect)):\r\n\t\t\t\t\t\tmissingRecallAnno.rects.append(gtAnno.rects[matched[matchingID]])\r\n\t\t\t\t\t\tfilteredAnno.rects.remove(gtAnno.rects[matched[matchingID]])\r\n\t\t\t\t\t\tfilteredAnno.rects.append(rect)\r\n\t\t\t\t\t\tmatched[matchingID] = j\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tmissingRecallAnno.rects.append(rect)\r\n\t\t\t\telse:\r\n\t\t\t\t\t#Not matched before.. go on and add the match\r\n\t\t\t\t\tfilteredAnno.rects.append(rect)\r\n\t\t\t\t\tmatched[matchingID] = j\r\n\t\t\telse:\r\n\t\t\t\tmissingRecallAnno.rects.append(rect)\r\n\r\n\t\tfilteredIDL.append(filteredAnno)\r\n\t\tmissingRecallIDL.append(missingRecallAnno)\r\n\r\n\treturn (filteredIDL\t, missingRecallIDL)\r\n\r\n###########################################################\r\n#\r\n#  Function to remove all detections with a too low score\r\n#\r\n#\r\ndef filterMinScore(detections, minScore):\r\n\tnewDetections = []\r\n\tfor anno in detections:\r\n\t\tnewAnno = Annotation()\r\n\t\tnewAnno.frameNr = anno.frameNr\r\n\t\tnewAnno.imageName = anno.imageName\r\n\t\tnewAnno.imagePath = anno.imagePath\r\n\t\tnewAnno.rects = []\r\n\r\n\t\tfor rect in anno.rects:\r\n\t\t\tif(rect.score >= minScore):\r\n\t\t\t\tnewAnno.rects.append(rect)\r\n\r\n\t\tnewDetections.append(newAnno)\r\n\treturn newDetections\r\n\r\n# foo.idl -> foo-suffix.idl, foo.idl.gz -> foo-suffix.idl.gz etc\r\ndef suffixIdlFileName(filename, suffix):\r\n\texts = ["".idl"", "".idl.gz"", "".idl.bz2""]\r\n\tfor ext in exts:\r\n\t\tif filename.endswith(ext):\r\n\t\t\treturn filename[0:-len(ext)] + ""-"" + suffix + ext\r\n\traise ValueError(""this does not seem to be a valid filename for an idl-file"")\r\n\r\nif __name__ == ""__main__"":\r\n# test output\r\n\tidl = parseIDL(""/tmp/asdf.idl"")\r\n\tidl[0].rects[0].articulations = [4,2]\r\n\tidl[0].rects[0].viewpoints = [2,3]\r\n\tsaveXML("""", idl)\r\n\r\n\r\ndef annoAnalyze(detIDL):\r\n\tallRects = []\r\n\t\r\n\tfor i,anno in enumerate(detIDL):\r\n\t\tfor j in anno.rects:\r\n\t\t\tnewRect = detAnnoRect()\r\n\t\t\tnewRect.imageName = anno.imageName\r\n\t\t\tnewRect.frameNr = anno.frameNr\r\n\t\t\tnewRect.rect = j\r\n\t\t\tallRects.append(newRect)\r\n\t\r\n\tallRects.sort(cmpDetAnnoRectsByScore)\r\n\t\r\n\tfilteredIDL = AnnoList([])\r\n\tfor i in allRects:\r\n\t\ta = Annotation()\r\n\t\ta.imageName = i.imageName\r\n\t\ta.frameNr = i.frameNr\r\n\t\ta.rects = []\r\n\t\ta.rects.append(i.rect)\r\n\t\tfilteredIDL.append(a)\r\n\t\t\r\n\treturn filteredIDL\r\n\r\n\r\n'"
TENSORBOX/utils/annolist/MatPlotter.py,0,"b'import os\nimport sys\nimport string\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom pylab import *\nimport numpy as np\n\nclass MatPlotter:\n\tfontsize=15\n\tcolor=0\n\tcolors=[""r-"", ""b-"", ""k-"", ""c-"", ""m-"", ""y-""]\n\tcolors+=[x + ""-"" for x in colors]\n\tcolors+=[""g-"", ""g--""]\n\tcurFigure=[]\n\tlegendNames=[]\n\tfontsizeLegend=14\n\tlegendPlace=\'lower right\'\n\tlegendborderpad = None\n\tlegendlabelsep = None\n\t\t\n\n\tdef __init__(self, fontsize=15):\n\t\t# self.newFigure()\n\t\tself.fontsize=fontsize\n\t\tself.fontsizeLegend=fontsize - 1\n\t\tpass\n\t\t\t\t\t\n\tdef formatLegend(self, newFontSize = 14, newPlace = \'lower right\', borderpad = None, labelsep = None):\n\t\tself.fontsizeLegend=newFontSize\n\t\tself.legendPlace=newPlace\n\t\tself.legendborderpad = borderpad\n\t\tself.legendlabelsep = labelsep\n\t\t\n\tdef newFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\treturn self.newRPCFigure(plotTitle, fsize)\n\n\tdef newRPCFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\t#subplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\t\tsubplots_adjust(right=0.975, top=0.975)\n\n\t\t#axis(\'equal\')\n\t\taxis([0, 1, 0, 1])\n\t\txticklocs, xticklabels = xticks(arange(0, 1.01, 0.1))\n\t\tsetp(xticklabels, size=self.fontsize)\n\t\tyticklocs, yticklabels = yticks(arange(0, 1.01, 0.1))\n\t\tsetp(yticklabels, size=self.fontsize)\n\t\tself.xlabel = xlabel(""1-precision"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""recall"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\t\t\n\tdef newFPPIFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\n\t\t#axis(\'equal\')\n\t\taxis([0, 100, 0, 1])\n\t\txticklocs, xticklabels = xticks(arange(0, 100.01, 0.5))\n\t\tsetp(xticklabels, size=self.fontsize)\n\t\tyticklocs, yticklabels = yticks(arange(0, 1.01, 0.1))\n\t\tsetp(yticklabels, size=self.fontsize)\n\t\tself.xlabel = xlabel(""false positives per image"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""recall"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\n\n\tdef newFreqFigure(self, plotTitle="""", maxX = 10, maxY = 10,fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.1)\n\t\t#axis(\'equal\')\n\n\t\taxis([0, maxX, 0, maxY])\n\t\txticklocs, xticklabels = xticks(arange(0, maxX + 0.01, maxX * 1.0/ 10))\n\t\tsetp(xticklabels, size=self.fontsize)\n\t\tyticklocs, yticklabels = yticks(arange(0, maxY + 0.01, maxY * 1.0/ 10))\n\t\tsetp(yticklabels, size=self.fontsize)\n\t\tself.xlabel = xlabel(""False positive / ground truth rect"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""True positives / ground truth rect"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\n\tdef newFPPWFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\n\t\tself.xlabel = xlabel(""false positive per windows (FPPW)"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""miss rate"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\t\n\t\tgrid()\n\t\thold(True)\n\n\tdef newLogFPPIFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n\t\tcurFigure = figure(figsize=fsize)\n\t\tself.title = title(plotTitle, fontsize=self.fontsize)\n\t\tsubplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.1)\n\n\t\t#axis(\'equal\')\n\n\t\tself.xlabel = xlabel(""false positives per image"")\n\t\tself.xlabel.set_size(self.fontsize+2)\n\t\tself.ylabel = ylabel(""miss rate"")\n\t\tself.ylabel.set_size(self.fontsize+4)\n\t\tgrid()\n\t\thold(True)\n\n\tdef loadRPCData(self, fname):\n\t\tself.filename = fname\n\t\tself.prec=[]\n\t\tself.rec=[]\n\t\tself.score=[]\n\t\tself.fppi=[]\n\t\tfile = open(fname)\n\n\t\tprecScores = []\n\t\tfor i in range(1,10,1):\n\t\t\tprecScores.append(100 - i * 10)\n\t\t\t\n\t\tfppiScores=[]\n\t\tfor i in range(0, 500, 5):\n\t\t\tfppiScores.append(i * 1.0 / 100.0)\n\t\t\t\n\t\n\n\t\tprecinfo = []\n\t\tfppiinfo = []\n\t\teerinfo = []\n\t\tlogAvInfo = []\n\t\t\n\t\tlogAvMR= []\n\t\tself.lamr = 0;\n\t\tself.eer = None;\n\t\tfirstLine = True\n\t\tleadingZeroCount = 0\n\n\t\tfor line in file.readlines():\n\t\t\tvals = line.split()\n\t\t\t#vals=line.split("" "")\n\t\t\t#for val in vals:\n\t\t\t#\tif val=="""":\n\t\t\t#\t\tvals.remove(val)\n\t\t\tself.prec.append(1-float(vals[0]))\n\t\t\tself.rec.append(float(vals[1]))\n\t\t\tself.score.append(float(vals[2]))\n\n\t\t\tif(len(vals)>3):\n\t\t\t\tself.fppi.append(float(vals[3]))\n\t\t\t\tif firstLine and not float(vals[3]) == 0:\n\t\t\t\t\tfirstLine = False\n\t\t\t\t\t\n\t\t\t\t\tlamrcount = 1\n\t\t\t\t\tself.lamr = 1 - float(vals[1])\n\t\t\t\t\t\n\t\t\t\t\tlowest_fppi = math.ceil( math.log(float(vals[3]))/ math.log(10) * 10 )  \n\t\t\t\t\tprint ""lowest_fppi: "",lowest_fppi;\n\n\t\t\t\t\t# MA: temporarily commented out \n\t\t\t\t\t# for i in range(lowest_fppi, 1, 1):\t\t\t\t\t\t\n\t\t\t\t\t# \tlogAvMR.append(10** (i * 1.0 / 10))\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t#self.score.append(float(vals[2][:-1]))\n\t\t\t#print 1-self.prec[-1], self.rec[-1], self.score[-1]\n\t\t\tif (len(self.prec)>1):\n\t\t\t\tdiff = (1-self.prec[-1]-self.rec[-1]) * (1-self.prec[-2]-self.rec[-2])\n\t\t\t\tif ( diff <0):\n\t\t\t\t\teerinfo.append( ""EER between: %.03f and %.03f\\tScore:%f"" % (self.rec[-1], self.rec[-2], self.score[-1]))\n\t\t\t\t\tself.eer = (self.rec[-1]+self.rec[-2]) * 0.5\n\t\t\t\tif ( diff == 0 and 1-self.prec[-1]-self.rec[-1]==0):\n\t\t\t\t\teerinfo.append( ""EER: %.03f\\tScore:%f"" % (self.rec[-1], self.score[-1]))\n\t\t\t\t\tself.eer = self.rec[-1]\n\n\t\t\t#Remove already passed precision\n\t\t\tif (len(precScores) > 0 and (float(vals[0])) < precScores[0] / 100.0):\n\t\t\t\tprecinfo.append(""%d percent precision score: %f, recall: %.03f"" % (precScores[0], float(vals[2]), float(vals[1])))\n\t\t\t\twhile(len(precScores) > 0 and precScores[0]/100.0 > float(vals[0])):\n\t\t\t\t\tprecScores.pop(0)\n\t\t\t\t\t\n\t\t\t#Remove already passed precision\n\t\t\tif(len(vals) > 3):\n\t\t\t\tif (len(fppiScores) > 0 and (float(vals[3])) > fppiScores[0]):\n\t\t\t\t\tfppiinfo.append(""%f fppi score: %f, recall: %.03f"" % (fppiScores[0], float(vals[2]), float(vals[1])))\n\t\t\t\t\twhile(len(fppiScores) > 0 and fppiScores[0] < float(vals[3])):\n\t\t\t\t\t\tfppiScores.pop(0)\n\t\t\t\t\n\t\t\t\tif (len(logAvMR) > 0 and (float(vals[3])) > logAvMR[0]):\n\t\t\t\t\twhile(len(logAvMR) > 0 and logAvMR[0] < float(vals[3])):\n\t\t\t\t\t\tlogAvInfo.append(""%f fppi, miss rate: %.03f, score: %f"" % (logAvMR[0], 1-float(vals[1]), float(vals[2])) )\n\t\t\t\t\t\tself.lamr += 1-float(vals[1])\n\t\t\t\t\t\tlamrcount += 1\t\t\t\t\t\t\n\t\t\t\t\t\tlogAvMR.pop(0)\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tlastMR = 1-float(vals[1])\n\t\t\t\t\n\t\t\n\t\tif(len(vals)>3):\n\t\t\tfor i in logAvMR:\n\t\t\t\tlogAvInfo.append(""%f fppi, miss rate: %.03f, extended"" % (i, lastMR) )\n\t\t\t\tself.lamr += lastMR\n\t\t\t\tlamrcount += 1\n\t\t\n\t\tfor i in precinfo:\n\t\t\tprint i;\n\t\tprint;\n\t\tfor i in fppiinfo:\n\t\t\tprint i;\n\t\tprint\n\t\tfor i in eerinfo:\n\t\t\tprint i;\n\t\tprint\n\t\tprint ""Recall at first false positive: %.03f"" % self.rec[0]\n\t\tif(len(vals)>3):\n\t\t\tprint\n\t\t\tfor i in logAvInfo:\n\t\t\t\tprint i;\n\t\t\tself.lamr =  self.lamr * 1.0 / lamrcount\n\t\t\tprint ""Log average miss rate in [10^%.01f, 10^0]: %.03f"" % (lowest_fppi / 10.0, self.lamr )\n\t\n\t\t\n\t\t\n\t\tprint; print\n\t\tfile.close()\n\n\tdef loadFreqData(self, fname):\n\t\tself.filename = fname\n\t\tself.prec=[]\n\t\tself.rec=[]\n\t\tself.score=[]\n\t\tfile = open(fname)\n\n\t\tfor line in file.readlines():\n\t\t\tvals = line.split()\n\n\t\t\tself.prec.append(float(vals[0]))\n\t\t\tself.rec.append(float(vals[1]))\n\t\t\tself.score.append(float(vals[2]))\n\n\t\tfile.close()\n\n\tdef loadFPPWData(self, fname):\n\t\tself.loadFreqData(fname)\n\n\tdef finishPlot(self, axlimits = [0,1.0,0,1.0]):\n\t\t# MA:\n\t\t#self.legend = legend(self.legendNames, self.legendPlace, pad = self.legendborderpad, labelsep = self.legendlabelsep)\n\t\tself.legend = legend(self.legendNames, self.legendPlace)\n\t\t\n\t\tlstrings = self.legend.get_texts()\n\t\tsetp(lstrings, fontsize=self.fontsizeLegend)\t\t\n\t\t#line= plot( [1 - axlimits[0], 0], [axlimits[3], 1 - axlimits[3] ] , \'k\')\n\t\tline= plot( [1, 0], [0, 1] , \'k\')\n\n\tdef finishFreqPlot(self):\n\t\tself.legend = legend(self.legendNames, self.legendPlace, pad = self.legendborderpad, labelsep = self.legendlabelsep)\n\t\tlstrings = self.legend.get_texts()\t\t\n\t\tsetp(lstrings, fontsize=self.fontsizeLegend)\n\t\t\n\n\tdef show(self, plotEER = True, axlimits = [0,1.0,0,1.0]):\n\t\tif (plotEER):\t\t\t\n\t\t\tself.finishPlot(axlimits)\n\t\t\taxis(axlimits)\n\t\telse:\n\t\t\tself.finishFreqPlot()\n\n\t\tshow()\n\n\tdef saveCurrentFigure(self, plotEER, filename, axlimits = [0,1.0,0,1.0]):\n\t\tif (plotEER):\n\t\t\tself.finishPlot(axlimits)\n\t\t\taxis(axlimits)\n\t\telse:\n\t\t\tself.finishFreqPlot()\t\t\t\n\t\t\n\t\tprint ""Saving: "" + filename\t\t\n\t\tsavefig(filename)\n\n\tdef plotRFP(self, numImages, fname, line=""r-""):\n\t\tprint \'NOT YET IMPLEMENTED\'\n\n\tdef plotRPC(self, fname, descr=""line"", style=""-1"", axlimits = [0,1.0,0,1.0], linewidth = 2, dashstyle = [], addEER = False ):\n\t\tself.loadRPCData(fname)\n\t\t\n\t\t#axis(axlimits);\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color])\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, style)\n\t\t\n\t\taxis(axlimits)\n\t\t\n\t\tif addEER and self.eer != None:\n\t\t\tdescr += "" (%.01f%%)"" % (self.eer * 100)\n\t\t\t\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\tself.legendNames= self.legendNames+[descr]\n\n\tdef plotFPPI(self, fname, descr=""line"", style=""-1"", axlimits = [0,2,0,1], linewidth = 2, dashstyle = []):\n\t\tself.loadRPCData(fname)\n\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.fppi, self.rec, self.colors[self.color], dashes = dashstyle)\t\t\t\t\n\t\t\telse:\n\t\t\t\tline = plot(self.fppi, self.rec, self.colors[self.color])\t\t\t\t\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.fppi, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.fppi, self.rec, style)\n\t\t\n\t\taxis(axlimits);\n\t\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\tself.legendNames= self.legendNames+[descr]\n\t\t\t\n\t\t\n\tdef plotFreq(self, fname, descr=""line"", style=""-1"", linewidth = 2, dashstyle = []):\n\t\tself.loadFreqData(fname)\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, self.colors[self.color])\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = plot(self.prec, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = plot(self.prec, self.rec, style)\n\n\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\tself.legendNames= self.legendNames+[descr]\n\n\tdef plotFPPW(self, fname, descr=""line"", style=""-1"", axlimits = [5e-6, 1e0, 1e-2, 0.5], linewidth = 2, dashstyle = []):\n\t\tself.loadFPPWData(fname)\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = loglog(self.prec, self.rec, self.colors[self.color])\n\t\t\tself.color=self.color+1\n\t\t\tself.color=self.color % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.prec, self.rec, style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = loglog(self.prec, self.rec, style)\n\t\t\n \t\txticklocs, xticklabels = xticks([1e-5, 1e-4,1e-3, 1e-2, 1e-1, 1e0])\n \t\tsetp(xticklabels, size=self.fontsize)\n \t\tyticklocs, yticklabels = yticks(array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5]), \n\t\t\t\t\t\t\t\t\t\t(""0.01"", ""0.02"", ""0.03"", ""0.04"", ""0.05"", ""0.06"", ""0.07"", ""0.08"",""0.09"", ""0.1"", ""0.2"", ""0.3"", ""0.4"", ""0.5""))\n \t\tsetp(yticklabels, size=self.fontsize)\n \t\t\n \t\taxis(axlimits)\n\n\t\tgca().yaxis.grid(True, \'minor\')\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\t\t\t\n\t\tself.legendNames= self.legendNames+[descr]\n\n\tdef plotLogFPPI(self, fname, descr=""line"", style=""-1"", axlimits = [5e-3, 1e1, 1e-1, 1], linewidth = 2, dashstyle = [], addlamr = False):\n\t\tself.loadRPCData(fname)\n\t\tif (style==""-1""):\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], self.colors[self.color], dashes = dashstyle)\t\t\t\t\n\t\t\telse:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], self.colors[self.color])\t\t\t\t\n\n\t\t\tself.color=(self.color+1) % len(self.colors)\n\t\telse:\n\t\t\tif dashstyle != []:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], style, dashes = dashstyle)\n\t\t\telse:\n\t\t\t\tline = loglog(self.fppi, [1 - x for x in self.rec], style)\n\t\t\t\t\n\t\tgca().yaxis.grid(True, \'minor\')\n\t\t\t\t\n\t\tm = min(self.fppi)\n\t\tlax = axlimits[0]\n\t\tfor i in self.fppi:\n\t\t\tif(i != m):\t\t\t\t\n\t\t\t\tlax = math.floor(log(i)/math.log(10))\n\t\t\t\tleftlabel = math.pow(10, lax)\t\t\t\t \n\t\t\t\tbreak\n\t\t\t\n\t\tm = max(self.fppi)\n\t\trightlabel = math.pow(10, math.ceil(log(m)/math.log(10))) + 0.01\t\t\t\t \n\t\t\t\t\n\t\tk = leftlabel\n\t\tticks = [k]\n\t\twhile k < rightlabel:\n\t\t\tk = k * 10\n\t\t\tticks.append(k)\n\t\t\t\t\n \t\txticklocs, xticklabels = xticks(ticks)\n \t\tsetp(xticklabels, size=self.fontsize)\n \t\tyticklocs, yticklabels = yticks(arange(0.1, 1.01, 0.1), (""0.1"", ""0.2"", ""0.3"", ""0.4"", ""0.5"", ""0.6"", ""0.7"", ""0.8"", ""0.9"", ""1.0""))\n \t\tsetp(yticklabels, size=self.fontsize)\n \t\t\n \t\taxlimits[0] = lax\t\t\n \t\taxis(axlimits)\t\t\n\t\t\n\t\tsetp(line, \'linewidth\', linewidth)\n\t\t\n\t\tif addlamr:\n\t\t\tdescr += "" (%.01f%%)"" % (self.lamr * 100)\n\t\t\t\n\t\tself.legendNames= self.legendNames+[descr]\n'"
TENSORBOX/utils/annolist/PalLib.py,0,"b'import sys\n#import AnnoList_pb2\nimport AnnotationLib;\n\nfrom ma_utils import is_number;\n\ndef loadPal(filename):\n    _annolist = AnnoList_pb2.AnnoList();\n\n    f = open(filename, ""rb"");\n    _annolist.ParseFromString(f.read());\n    f.close();\n\n    return _annolist;\n\ndef savePal(filename, _annolist):\n    f = open(filename, ""wb"");\n    f.write(_annolist.SerializeToString());\n    f.close();\n\ndef al2pal(annotations):\n    _annolist = AnnoList_pb2.AnnoList();\n\n    #assert(isinstance(annotations, AnnotationLib.AnnoList));\n\n    # check type of attributes, add missing attributes \n    for a in annotations:\n        for r in a.rects:\n            for k, v in r.at.iteritems():\n                if not k in annotations.attribute_desc:\n                    annotations.add_attribute(k, type(v));\n                else:\n                    assert(AnnotationLib.is_compatible_attr_type(annotations.attribute_desc[k].dtype, type(v)));\n\n    # check attributes values\n    for a in annotations:\n        for r in a.rects:\n            for k, v in r.at.iteritems():\n                if k in annotations.attribute_val_to_str:\n                    # don\'t allow undefined values\n                    if not v in annotations.attribute_val_to_str[k]:\n                        print ""attribute: {}, undefined value: {}"".format(k, v);\n                        assert(False);\n\n    # store attribute descriptions in pal structure\n    for aname, adesc in annotations.attribute_desc.iteritems():\n        _annolist.attribute_desc.extend([adesc]);\n\n    for a in annotations:\n        _a = _annolist.annotation.add();\n        _a.imageName = a.imageName;\n\t\t\n        for r in a.rects:\n            _r = _a.rect.add();\n\n            _r.x1 = r.x1;\n            _r.y1 = r.y1;\n            _r.x2 = r.x2;\n            _r.y2 = r.y2;\n            \n            _r.score = float(r.score);\n\n            if hasattr(r, \'id\'):\n                _r.id = r.id;\n\n            if hasattr(r, \'track_id\'):\n                _r.track_id = r.track_id;\n\n            if hasattr(r, \'at\'):\n                for k, v in r.at.items():\n                    _at = _r.attribute.add();\n\n                    _at.id = annotations.attribute_desc[k].id;\n\n                    if annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_INT32:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_INT32, type(v)));\n                        _at.val = int(v);\n                    elif annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_FLOAT:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_FLOAT, type(v)));\n                        _at.fval = float(v);\n                    elif annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_STRING:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_STRING, type(v)));\n                        _at.strval = str(v);\n                    else:\n                        assert(false);            \n\n    return _annolist;\n\ndef pal2al(_annolist):\n    #annotations = [];\n    annotations = AnnotationLib.AnnoList();\n\n    for adesc in _annolist.attribute_desc:\n        annotations.attribute_desc[adesc.name] = adesc;\n        print ""attribute: "", adesc.name, adesc.id\n\n        for valdesc in adesc.val_to_str:\n            annotations.add_attribute_val(adesc.name, valdesc.s, valdesc.id);\n\n    attribute_name_from_id = {adesc.id: aname for aname, adesc in annotations.attribute_desc.iteritems()}\n    attribute_dtype_from_id = {adesc.id: adesc.dtype for aname, adesc in annotations.attribute_desc.iteritems()}\n    \n    for _a in _annolist.annotation:\n        anno = AnnotationLib.Annotation()\n\n        anno.imageName = _a.imageName;\n\n        anno.rects = [];\n\n        for _r in _a.rect:\n            rect = AnnotationLib.AnnoRect()\n\n            rect.x1 = _r.x1;\n            rect.x2 = _r.x2;\n            rect.y1 = _r.y1;\n            rect.y2 = _r.y2;\n\n            if _r.HasField(""id""):\n                rect.id = _r.id;\n\n            if _r.HasField(""track_id""):\n                rect.track_id = _r.track_id;\n\n            if _r.HasField(""score""):\n                rect.score = _r.score;\n\n            for _at in _r.attribute:\n                try:\n                    cur_aname = attribute_name_from_id[_at.id];\n                    cur_dtype = attribute_dtype_from_id[_at.id];\n                except KeyError as e:\n                    print ""attribute: "", _at.id\n                    print e\n                    assert(False);\n\n                if cur_dtype == AnnotationLib.AnnoList.TYPE_INT32:\n                    rect.at[cur_aname] = _at.val;\n                elif cur_dtype == AnnotationLib.AnnoList.TYPE_FLOAT:\n                    rect.at[cur_aname] = _at.fval;\n                elif cur_dtype == AnnotationLib.AnnoList.TYPE_STRING:\n                    rect.at[cur_aname] = _at.strval;\n                else:\n                    assert(False);            \n\n            anno.rects.append(rect);\n\n        annotations.append(anno);\n\n    return annotations;\n'"
TENSORBOX/utils/annolist/__init__.py,0,b''
TENSORBOX/utils/annolist/doRPC.py,0,"b'#!/usr/bin/env python\n\nimport os, sys\nfrom AnnotationLib import *\nfrom optparse import OptionParser\nimport copy\nimport math\n\n# BASED ON WIKIPEDIA VERSION\n# n - number of nodes\n# C - capacity matrix\n# F - flow matrix\n# s - source\n# t - sink\n# sumC - sum over rows of C (too speed up computation)\n\ndef edmonds_karp(n, C, s, t, sumC):\n\n\t# Residual capacity from u to v is C[u][v] - F[u][v]\n\tF = [[0] * n for i in xrange(n)]\n\twhile True:\n\t\tP = [-1] * n # Parent table\n\t\tP[s] = s\n\t\tM = [0] * n  # Capacity of path to node\n\t\tM[s] = float(\'infinity\')\n\t\tQ = [s]      # BFS queue\n\t\twhile Q:\n\t\t\tu = Q.pop(0)\t\t\t\n\t\t\tfor v in xrange(n):\n\t\t\t\t# There is available capacity, \n               \t# and v is not seen before in search\n\t\t\t\tif C[u][v] - F[u][v] > 0 and P[v] == -1:\n\t\t\t\t\tP[v] = u\n\t\t\t\t\tM[v] = min(M[u], C[u][v] - F[u][v])\n\t\t\t\t\tif v != t:\n\t\t\t\t\t\tif(sumC[u] > 0):\n\t\t\t\t\t\t\tQ.append(v)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Backtrack search, and write flow\n\t\t\t\t\t\twhile P[v] != v:\n\t\t\t\t\t\t\tu = P[v]\n\t\t\t\t\t\t\tF[u][v] += M[t]\n\t\t\t\t\t\t\tF[v][u] -= M[t]\n\t\t\t\t\t\t\tv = u\n\t\t\t\t\t\tQ = None\n\t\t\t\t\t\tbreak\n\t\tif P[t] == -1: # We did not find a path to t\t\t\t\n\t\t\treturn (F)\n\nclass AnnoGraph:\n\n\tdef __init__(self, anno, det, ignore, style, minCover, minOverlap, maxDistance, ignoreOverlap):\n\t\t\n\t\t# setting rects\n\t\t#print anno.imageName\n\t\tself.anno = anno\n\t\tself.det = det\n\t\tself.det.sortByScore(""descending"")\n\t\t\n\t\t# generate initial graph\n\t\tself.n = len(det.rects)\n\t\tself.m = len(anno.rects)\n\t\t\n\t\t# Number of nodes = number of detections + number of GT + source + sink\n\t\tself.a = self.n + self.m + 2\n\t\n\t\t# Flow matrix\n\t\tself.F = [[0] * self.a for i in xrange(self.a)]\n\t\t# Capacity matrix\n\t\tself.C = [[0] * self.a for i in xrange(self.a)]\n\t\t\n\t\t# Connect source to all detections\n\t\tfor i in range(1, self.n + 1):\n\t\t\tself.C[0][i] = 1\n\t\t\tself.C[i][0] = 1\n\t\t\t\n\t\t# Connect sink to all GT\t\n\t\tfor i in range(self.n + 1, self.a - 1):\n\t\t\tself.C[i][self.a - 1] = 1\n\t\t\tself.C[self.a - 1][i] = 1\n\t\t\n\t\t# Overall flow\n\t\tself.full_flow = 0\n\t\tself.ignore_flow = 0\n\t\n\t\t# match rects / Adjacency matrix\n\t\tself.M = [[] for i in xrange(self.n)]\n\t\tself.match(style, minCover, minOverlap, maxDistance)\n\t\tself.nextN = 0\n\t\t\n\t\t# Deactivate All Non Matching detections\n\t\t# Save row sums for capacity matrix\n\t\tself.sumC = []\n\t\tself.sumC.append(self.n)\n\t\tfor q in [len(self.M[j]) for j in xrange(len(self.M))]: \n\t\t\tself.sumC.append(q)\n\t\tfor q in [1] * self.m:\n\t\t\tself.sumC.append(q)\n\t\t\n\t\t# Initially no links are active\n\t\tself.sumC_active = []\n\t\tself.sumC_active.append(self.n)\t\n\t\tfor q in [len(self.M[j]) for j in xrange(len(self.M))]: \n\t\t\tself.sumC_active.append(0)\t\n\t\tfor q in [1] * self.m:\n\t\t\tself.sumC_active.append(q)\n\t\t\t\n\t\t#\n\t\tself.ignore = [ 0 ] * self.m\n\t\tfor ig in ignore.rects:\n\t\t\tfor i, r in enumerate(anno.rects):\n\t\t\t\tif(ig.overlap_pascal(r) > ignoreOverlap):\n\t\t\t\t\tself.ignore[i] = 1\n\t\t\t\t \n\n\tdef match(self, style, minCover, minOverlap, maxDistance):\n\t\tfor i in xrange(self.n):\n\t\t\tdetRect = self.det.rects[i]\n\t\t\tfor j in xrange(self.m):\n\t\t\t\tannoRect = self.anno.rects[j]\n\t\t\t\t\n\t\t\t\t# Bastian Leibe\'s matching style\t\t\t\t\n\t\t\t\tif(style == 0):\t\t\t\t\t\n\t\t\t\t\tassert False;\n\t\t\t\t\tif detRect.isMatchingStd(annoRect, minCover, minOverlap, maxDistance):\n\t\t\t\t\t\tself.M[i].append(self.n + 1 + j)\n\t\t\t\t\t\n\t\t\t\t# Pascal Matching style\t\n\t\t\t\tif(style == 1):\t\n\t\t\t\t\tif (detRect.isMatchingPascal(annoRect, minOverlap)):\n\t\t\t\t\t\tself.M[i].append(self.n + 1 + j)\n\t\t\t\t\t\n\tdef decreaseScore(self, score):\n\t\tcapacity_change = False\n\t\tfor i in xrange(self.nextN, self.n):\n\t\t\tif (self.det.rects[i].score >= score):\t\t\t\t\n\t\t\t\tcapacity_change = self.insertIntoC(i + 1) or capacity_change\n\t\t\t\tself.nextN += 1\n\t\t\telse:\n\t\t\t\tbreak\n\t\t\t\n\t\tif capacity_change:\n\t\t\tself.F = edmonds_karp(self.a, self.C, 0, self.a - 1, self.sumC_active)\n\t\t\tself.full_flow = sum([self.F[0][i] for i in xrange(self.a)])\n\t\t\tself.ignore_flow = sum([self.F[i][self.a - 1] * self.ignore[i - 1 - self.n] for i in range(1 + self.n, 1 + self.n + self.m )])\n\t\t\t\t\t\n\t\treturn capacity_change\n\t\n\tdef addBB(self, rect):\n\t\tself.nextN += 1\n\t\t\n\t\tcapacity_change = self.insertIntoC(rect.boxIndex + 1)\n\t\t\t\n\t\tif capacity_change:\n\t\t\tself.F = edmonds_karp(self.a, self.C, 0, self.a - 1, self.sumC_active)\n\t\t\tself.full_flow = sum([self.F[0][i] for i in xrange(self.a)])\n\t\t\tself.ignore_flow = sum([self.F[i][self.a - 1] * self.ignore[i - 1 - self.n] for i in range(1 + self.n, 1 + self.n + self.m )])\n\t\t\t\n\t\treturn capacity_change\n\n\tdef\tinsertIntoC(self, i):\n\t\t#print ""Inserting node"", i, self.det.rects[i-1].score, ""of image"", self.anno.imageName\n\t\t\t\n\t\tfor match in self.M[i - 1]:\n\t\t\t#print ""  match: "", match\n\t\t\tself.C[i][match] = 1\n\t\t\tself.C[match][i] = 1\n\t\t\t\t\t\t\t\t\n\t\tself.sumC_active[i] = self.sumC[i]\n\t\t\n\t\treturn self.sumC[i] > 0\t\t\n\n\tdef maxflow(self):\n\t\treturn self.full_flow - self.ignore_flow\n\t\n\tdef consideredDets(self):\t\t\t\t\n\t\treturn self.nextN - self.ignore_flow\n\t\n\tdef ignoredFlow(self):\n\t\treturn self.ignore_flow\n\t\t\n\tdef getTruePositives(self):\n\t\tret = copy.copy(self.anno)\n\t\tret.rects = []\n\t\t#iterate over GT\n\t\tfor i in xrange(self.n + 1, self.a - 1):\n\t\t\t#Flow to sink > 0\n\t\t\tif(self.F[i][self.a - 1] > 0 and self.ignore[i - self.n - 1] == 0):\n\t\t\t\t#Find associated det\n\t\t\t\tfor j in xrange(1, self.n + 1):\n\t\t\t\t\tif(self.F[j][i] > 0):\n\t\t\t\t\t\tret.rects.append(self.det[j - 1])\n\t\t\t\t\t\tbreak\t\n\t\treturn ret\n\t\n\tdef getIgnoredTruePositives(self):\n\t\tret = copy.copy(self.anno)\n\t\tret.rects = []\n\t\t#iterate over GT\n\t\tfor i in xrange(self.n + 1, self.a - 1):\n\t\t\t#Flow to sink > 0\n\t\t\tif(self.F[i][self.a - 1] > 0 and self.ignore[i - self.n - 1] == 1):\n\t\t\t\t#Find associated det\n\t\t\t\tfor j in xrange(1, self.n + 1):\n\t\t\t\t\tif(self.F[j][i] > 0):\n\t\t\t\t\t\tret.rects.append(self.det[j - 1])\n\t\t\t\t\t\tbreak\t\n\t\treturn ret\n\t\t\n\tdef getMissingRecall(self):\n\t\tret = copy.copy(self.anno)\n\t\tret.rects = []\n\t\tfor i in xrange(self.n + 1, self.a - 1):\n\t\t\tif(self.F[i][self.a - 1] == 0 and self.ignore[i - self.n - 1] == 0):\n\t\t\t\tret.rects.append(self.anno.rects[i - self.n - 1])\n\t\treturn ret\n\t\t\n\tdef getFalsePositives(self):\t\t\t\t\n\t\tret = copy.copy(self.det)\n\t\tret.rects = []\n\t\tfor i in xrange(1, self.n + 1):\n\t\t\tif(self.F[0][i] == 0):\t\t\t\t\t\n\t\t\t\tret.rects.append(self.det[i - 1])\t\t\t\n\t\treturn ret\n\t\ndef asort(idlGT, idlDet, minWidth, minHeight, style, minCover, minOverlap, maxDistance, maxWidth=float(\'inf\'), maxHeight=float(\'inf\')):\n\t#Asort too small object\tin ground truth\n\t\t\n\tfor x,anno in enumerate(idlGT):\n\t\t\t\t\n\t\timageFound = False\n\t\tfilterIndex = -1\n\t\tfor i,filterAnno in enumerate(idlDet):\n\t\t\t\tif (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\n\t\t\t\t\tfilterIndex = i\n\t\t\t\t\timageFound = True\n\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\n\t\tif(not imageFound):\n\t\t\tcontinue\n\t\t\t\t\n\t\tvalidGTRects = []\t\t\n\t\tfor j in anno.rects:\n\t\t\tif (j.width() >= minWidth) and (j.height() >= minHeight) and (j.width() <= maxWidth) and (j.height() <= maxHeight):\n\t\t\t\tvalidGTRects.append(j)\n\t\t\telse:\n\t\t\t\t# Sort out detections that would have matched\t\t\t\t\n\t\t\t\tmatchingIndexes = []\n\t\t\t\t\t\t\t\t\t\n\t\t\t\tfor m,frect in enumerate(idlDet[filterIndex].rects):\n\t\t\t\t\tif(style == 0):\n\t\t\t\t\t\tif (j.isMatchingStd(frect, minCover,minOverlap, maxDistance)):\n\t\t\t\t\t\t\toverlap = j.overlap_pascal(frect)\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tmatchingIndexes.append((m,overlap))\n\t\t\t\t\t\t\t\n\t\t\t\t\tif(style == 1):\n\t\t\t\t\t\tif(j.isMatchingPascal(frect, minOverlap)):\n\t\t\t\t\t\t\toverlap = j.overlap_pascal(frect)\n\t\t\t\t\t\t\tmatchingIndexes.append((m, overlap))\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tfor m in xrange(len(matchingIndexes) - 1, -1, -1):\n\t\t\t\t\tmatching_rect = idlDet[filterIndex].rects[matchingIndexes[m][0]]\n\t\t\t\t\tmatching_overlap = matchingIndexes[m][1]\n\t\t\t\t\tbetter_overlap_found = False\n\t\t\t\t\tfor l in anno.rects:\n\t\t\t\t\t\tif l.overlap_pascal(matching_rect) > matching_overlap:\n\t\t\t\t\t\t\tbetter_overlap_found = True\n\t\t\t\t\t\t\t\n\t\t\t\t\tif better_overlap_found:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\tdel idlDet[filterIndex].rects[matchingIndexes[m][0]]\n\t\t\t\t\t\t\t\n\t\tidlGT[x].rects = validGTRects\n\t\n\t#Sort out too small false positives\t\n\tfor x,anno in enumerate(idlDet):\n\t\t\t\t\t\t\n\t\timageFound = False\n\t\tfilterIndex = -1\n\t\tfor i,filterAnno in enumerate(idlGT):\n\t\t\t\tif (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\n\t\t\t\t\tfilterIndex = i\n\t\t\t\t\timageFound = True\n\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\n\t\tif(not imageFound):\n\t\t\tcontinue\n\t\t\n\t\tvalidDetRects = []\t\t\n\t\tfor j in anno.rects:\n\t\t\tif (j.width() >= minWidth) and (j.height() >= minHeight) and (j.width() <= maxWidth) and (j.height() <= maxHeight):\n\t\t\t\tvalidDetRects.append(j)\n\t\t\telse:\t\t\t\t\n\t\t\t\tfor frect in idlGT[filterIndex].rects:\n\t\t\t\t\t\n\t\t\t\t\tif(style == 0):\n\t\t\t\t\t\tif j.isMatchingStd(frect, minCover,minOverlap, maxDistance):\n\t\t\t\t\t\t\tvalidDetRects.append(j)\n\t\t\t\t\t\t\n\t\t\t\t\tif(style == 1):\t\t\n\t\t\t\t\t\tif(j.isMatchingPascal(frect, minOverlap)):\n\t\t\t\t\t\t\tvalidDetRects.append(j)\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\tidlDet[x].rects = validDetRects\n\n\n# MA: simplified version that does Pascal style matching with one parameter controlling ""intersection-over-union"" matching threshold \ndef comp_prec_recall(annoIDL, detIDL, minOverlap):\n        ignoreIDL = copy.deepcopy(annoIDL)\n        for anno in ignoreIDL:\n                anno.rects = []\n\n        precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minOverlap=minOverlap);\n        return precs, recalls, scores, fppi\n\ndef comp_prec_recall_graphs(annoIDL, detIDL, minOverlap):\n        ignoreIDL = copy.deepcopy(annoIDL)\n        for anno in ignoreIDL:\n                anno.rects = []\n\n        precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minOverlap=minOverlap);\n        return graphs\n\n\n# MA: full version\ndef comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minWidth=0, minHeight=0, maxWidth=float(\'inf\'), maxHeight=float(\'inf\'), \n                                matchingStyle=1, minCover=0.5, minOverlap=0.5, maxDistance=0.5, ignoreOverlap=0.9, verbose=False):\n\n\t# Asort detections which are too small/too big\n        if verbose: \n                print ""Asorting too large/ too small detections""\n                print ""minWidth:"", minWidth\n                print ""minHeight:"", minHeight\n                print ""maxWidth: "", maxWidth\n                print ""maxHeight: "", maxHeight\n\n\tasort(annoIDL, detIDL, minWidth, minHeight, matchingStyle, minCover, minOverlap, maxDistance, maxWidth, maxHeight)\n\t\n\t#Debugging asort\n\t#saveIDL(""testGT.idl"", annoIDL)\n\t#saveIDL(""testDET.idl"", detIDL)\n\t\n\n\n\tnoAnnotations = 0\n\tfor anno in annoIDL:\n\t\tfor j,detAnno in enumerate(detIDL):\n\t\t\t\tif (suffixMatch(anno.imageName, detIDL[j].imageName) and anno.frameNr == detIDL[j].frameNr):\n\t\t\t\t\tnoAnnotations = noAnnotations + len(anno.rects)\n\t\t\t\t\tbreak\n\n        if verbose:\n                print ""#Annotations:"", noAnnotations\n\t\n                ###--- set up graphs ---###\n                print ""Setting up graphs ...""\n\n\tgraphs = []\n\tallRects = []\n\tmissingFrames = 0\n\tfor i in xrange(len(annoIDL)):\n\t\t\t\t\n\t\timageFound = False\n\t\tfilterIndex = -1\n\n                for j, detAnno in enumerate(detIDL):\t\t\t\n                        if (suffixMatch(annoIDL[i].imageName, detIDL[j].imageName) and annoIDL[i].frameNr == detIDL[j].frameNr):\n                                filterIndex = j\n                                imageFound = True\n                                break\n\t\t\t\t\t\t\t\n\t\tif(not imageFound):\n\t\t\tprint ""No annotation/detection pair found for: "" + annoIDL[i].imageName + "" frame: "" + str(annoIDL[i].frameNr)\n\t\t\tmissingFrames += 1\n\t\t\tcontinue;\n\t\t\t\t\n\t\tgraphs.append(AnnoGraph(annoIDL[i], detIDL[filterIndex], ignoreIDL[i], matchingStyle, minCover, minOverlap, maxDistance, ignoreOverlap))\n\t\t\t\t\t\n\t\tfor j,rect in enumerate(detIDL[filterIndex]):\n\t\t\tnewRect = detAnnoRect()\n\t\t\tnewRect.imageName = anno.imageName\n\t\t\tnewRect.frameNr = anno.frameNr\n\t\t\tnewRect.rect = rect\n\t\t\tnewRect.imageIndex = i - missingFrames\n\t\t\tnewRect.boxIndex = j\n\t\t\tallRects.append(newRect)\t\n\t\n        if verbose: \n                print ""missingFrames: "", missingFrames\n                print ""Number of detections on annotated frames: "" , len(allRects)\n\t\n                ###--- get scores from all rects ---###\n                print ""Sorting scores ...""\n\n\tallRects.sort(cmpDetAnnoRectsByScore)\n\tallRects.reverse()\n\n\t###--- gradually decrease score ---###\n        if verbose: \n                print ""Gradually decrease score ...""\n\n\tlastScore = float(\'infinity\')\n\t\n\tprecs = [1.0] \n\trecalls = [0.0] \n\t#fppi = [ 10**(math.floor(math.log(1.0 / float(len(annoIDL)))/math.log(10) * 10.0) / 10.0) ]\n\tfppi = [ 1.0 / float(len(annoIDL)) ]\n\tscores = [lastScore]  \n\t\n\tnumDet = len(allRects)\n\tsf = lastsf = 0\n\tcd = lastcd = 0\t\n\tiflow = lastiflow = 0\n\t\n\tchanged = False\t\n\tfirstFP = True\n\tfor i,nextrect in enumerate(allRects):\n\t\tscore = nextrect.rect.score;\n\t\t\n\t\t# updating true and false positive counts\t\t\n\t\tsf = sf - graphs[nextrect.imageIndex].maxflow()\n\t\tcd = cd - graphs[nextrect.imageIndex].consideredDets()\n\t\tiflow = iflow -\tgraphs[nextrect.imageIndex].ignoredFlow()\t\t\n\t\t\n\t\t#changed = changed or graphs[nextrect.imageIndex].decreaseScore(score)\n\t\tchanged = graphs[nextrect.imageIndex].addBB(nextrect) or changed\t\t\t\t\t\n\t\tsf = sf + graphs[nextrect.imageIndex].maxflow()\n\t\tcd = cd + graphs[nextrect.imageIndex].consideredDets()\n\t\tiflow = iflow + graphs[nextrect.imageIndex].ignoredFlow()\n\t\t\n\t\tif(firstFP and cd - sf != 0):\n\t\t\tfirstFP = False\n\t\t\tchanged = True\n\t\t\t\t\t\t\n\t\tif (i == numDet - 1 or score != allRects[i + 1].rect.score or firstFP or i == len(allRects)):\t\n\t\t\tif(changed or i == numDet - 1 or i == len(allRects)):\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\tif(lastcd > 0):\n\t\t\t\t\tscores.append(lastScore)\n\t\t\t\t\trecalls.append(float(lastsf) / float(noAnnotations - lastiflow))\n\t\t\t\t\tprecs.append(float(lastsf) / float(lastcd))\n\t\t\t\t\tfppi.append(float(lastcd - lastsf) / float(len(annoIDL)))\n\t\t\t\t\t\t\t\t\n\t\t\t\tif (cd > 0):\n\t\t\t\t\tscores.append(score)\n\t\t\t\t\trecalls.append(float(sf) / float(noAnnotations - iflow))\t\t\t\t\t\t\t\t\n\t\t\t\t\tprecs.append(float(sf) / float(cd))\t\t\t\t\t\n\t\t\t\t\tfppi.append(float(cd - sf) / float(len(annoIDL)))\n\t\t\t\t\t\n\t\t\t\t\n\t\t\tchanged = False\n\t\t\n\t\tlastScore = score\n\t\tlastsf = sf\n\t\tlastcd = cd\n\t\tlastiflow = iflow\n        \n        return precs, recalls, scores, fppi, graphs\n\ndef main():\n\t\n\tparser = OptionParser(usage=""usage: %prog [options] <groundTruthIdl> <detectionIdl>"")\n\t\t             \n\tparser.add_option(""-o"", ""--outFile"",\n                  action=""store"", type=""string"", dest=""outFile"")\n\tparser.add_option(""-a"", ""--analysisFiles"",\n                  action=""store"", type=""string"", dest=""analysisFile"")\n\t\n\tparser.add_option(""-s"", ""--minScore"",\n                  action=""store"", type=""float"", dest=""minScore"")\n                  \n\tparser.add_option(""-w"", ""--minWidth"",\n\t\t\t\taction=""store"", type=""int"", dest=""minWidth"", default=0)\n\t\t\t\t  \n\tparser.add_option(""-u"", ""--minHeight"",\n\t\t\t\taction=""store"", type=""int"", dest=""minHeight"",default=0)\n\tparser.add_option(""--maxWidth"", action=""store"", type=""float"", dest=""maxWidth"", default=float(\'inf\'))\n\tparser.add_option(""--maxHeight"", action=""store"", type=""float"", dest=""maxHeight"", default=float(\'inf\'))\n\n\tparser.add_option(""-r"", ""--fixAspectRatio"",\n\t\t\taction=""store"", type=""float"", dest=""aspectRatio"")\n\n\tparser.add_option(""-p"", ""--Pascal-Style"", action=""store_true"", dest=""pascalStyle"")\n\tparser.add_option(""-l"", ""--Leibe-Seemann-Matching-Style"", action=""store_true"", dest=""leibeStyle"")\n\t\n\tparser.add_option(""--minCover"", action=""store"", type=""float"", dest=""minCover"", default=0.5)\n\tparser.add_option(""--maxDistance"", action=""store"", type=""float"", dest=""maxDistance"", default=0.5)\n\tparser.add_option(""--minOverlap"", action=""store"", type=""float"", dest=""minOverlap"", default=0.5)\n\t\n\t\n\tparser.add_option(""--clipToImageWidth"", action=""store"", type=""float"", dest=""clipWidth"", default= None)\n\tparser.add_option(""--clipToImageHeight"", action=""store"", type=""float"", dest=""clipHeight"", default= None)\n\t\t\n\tparser.add_option(""-d"", ""--dropFirst"", action=""store_true"", dest=""dropFirst"")\n\t\n\t#parser.add_option(""-c"", ""--class"", action=""store"", type=""int"", dest=""classID"", default=-1)\n\tparser.add_option(""-c"", ""--class"", action=""store"", type=""int"", dest=""classID"", default = None)\n\t\n\tparser.add_option(""-i"", ""--ignore"", action=""store"", type=""string"", dest=""ignoreFile"")\n\tparser.add_option(""--ignoreOverlap"", action=""store"", type=""float"", dest=""ignoreOverlap"", default = 0.9)\n\t\n\t(options, args) = parser.parse_args()\n\t\n\tif (len(args) < 2):\n\t\tprint ""Please specify annotation and detection as arguments!""\n\t\tparser.print_help()\n\t\tsys.exit(1)\n\t\n\tannoFile = args[0]\n\t\t\n\t# First figure out the minimum height and width we are dealing with\n\tminWidth =  options.minWidth\n\tminHeight = options.minHeight\n\tmaxWidth =  options.maxWidth\n\tmaxHeight = options.maxHeight\n\t\t\n\tprint ""Minimum width: %d height: %d"" % (minWidth, minHeight)\n\t\n\t# Load files\t\n\tannoIDL = parse(annoFile)\t\n\tdetIDL = []\n\tfor dets in args[1:]:\n\t\tdetIDL += parse(dets)\n\n\t\n\tif options.ignoreFile != None:\n\t\tignoreIDL = parse(options.ignoreFile)\n\telse:\n\t\tignoreIDL = copy.deepcopy(annoIDL)\n\t\tfor anno in ignoreIDL:\n\t\t\tanno.rects = []\n\t\t\t\n\tif(options.classID is not None):\n\t\tfor anno in annoIDL:\n\t\t\tanno.rects = [rect for rect in anno.rects if (rect.classID == options.classID  or rect.classID == -1)]\n\t\tfor anno in detIDL:\n\t\t\tanno.rects = [rect for rect in anno.rects if (rect.classID == options.classID or rect.classID == -1)]\n\t\tfor anno in ignoreIDL:\n\t\t\tanno.rects = [rect for rect in anno.rects if (rect.classID == options.classID or rect.classID == -1)]\n\n\t# prevent division by zero when fixing aspect ratio\n\tfor anno in annoIDL:\n\t\tanno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n\tfor anno in detIDL:\n\t\tanno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n\tfor anno in ignoreIDL:\n\t\tanno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n\n\t\n\t# Fix aspect ratio \n\tif (not options.aspectRatio == None):\n\t\tforceAspectRatio(annoIDL, options.aspectRatio)\n\t\tforceAspectRatio(detIDL, options.aspectRatio)\n\t\tforceAspectRatio(ignoreIDL, options.aspectRatio)\n\t\t\t\n\t# Deselect detections with too low score\n\tif (not options.minScore == None):\n\t\tfor i,anno in enumerate(detIDL):\n\t\t\tvalidRects = []\n\t\t\tfor rect in anno.rects:\t\t\t\t\n\t\t\t\tif (rect.score >= options.minScore):\n\t\t\t\t\tvalidRects.append(rect)\n\t\t\tanno.rects = validRects\n\t\n\t# Clip detections to the image dimensions\n\tif(options.clipWidth != None or options.clipHeight != None):\n\t\tmin_x = -float(\'inf\')\n\t\tmin_y = -float(\'inf\')\n\t\tmax_x = float(\'inf\')\n\t\tmax_y = float(\'inf\')\n\t\t\n\t\tif(options.clipWidth != None):\n\t\t\tmin_x = 0\n\t\t\tmax_x = options.clipWidth\n\t\tif(options.clipHeight != None):\n\t\t\tmin_y = 0\n\t\t\tmax_y = options.clipHeight\n\t\t\t\n\t\tprint ""Clipping width: (%.02f-%.02f); clipping height: (%.02f-%.02f)"" % (min_x, max_x, min_y, max_y)\n\t\tfor anno in annoIDL:\n\t\t\tfor rect in anno:\n\t\t\t\trect.clipToImage(min_x, max_x, min_y, max_y)\n\t\tfor anno in detIDL:\n\t\t\tfor rect in anno:\n\t\t\t\trect.clipToImage(min_x, max_x, min_y, max_y)\t\t\n\t\n\t# Setup matching style; standard is Pascal\n\t# style\n\tmatchingStyle = 1\n\t\n\t# Pascal style\n\tif (options.pascalStyle == True):\n\t\tmatchingStyle = 1\n\t\t\n\tif (options.leibeStyle == True):\n\t\tmatchingStyle = 0\n\t\t\n\tif (options.pascalStyle and options.leibeStyle):\n\t\tprint ""Conflicting matching styles!""\n\t\tsys.exit(1)\n\t\t\n\tif (options.dropFirst == True):\n\t\tprint ""Drop first frame of each sequence...""\n\t\tnewIDL = []\n\t\tfor i, anno in enumerate(detIDL):\n\t\t\tif (i > 1 and detIDL[i].frameNr == detIDL[i-1].frameNr + 1 and detIDL[i].frameNr == detIDL[i-2].frameNr + 2 and  detIDL[i].frameNr == detIDL[i-3].frameNr + 3  and detIDL[i].frameNr == detIDL[i-4].frameNr + 4):\n\t\t\t\tnewIDL.append(anno)\n\t\tdetIDL = newIDL\n        \n        verbose = True;\n        precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL,\n                                                                   minWidth=options.minWidth, minHeight=options.minHeight, \n                                                                   maxWidth=options.maxWidth, maxHeight=options.maxHeight,\n                                                                   matchingStyle=matchingStyle, \n                                                                   minCover=options.minCover, minOverlap=options.minOverlap, \n                                                                   maxDistance=options.maxDistance, ignoreOverlap=options.ignoreOverlap, \n                                                                   verbose=verbose);\n\n\t###--- output to file ---###\n\toutfilename = options.outFile\n\tif outfilename is None:\n\t\toutputDir = os.path.dirname(os.path.abspath(args[1]))\n\t\toutputFile = os.path.basename(os.path.abspath(args[1]))\n\t\t[base, ext] = idlBase(outputFile)\n\t\t#outfilename = outputDir + ""/rpc-"" + base + "".txt""\n\n                outfilename = outputDir + ""/rpc-"" + base + ""_overlap"" + str(options.minOverlap) + "".txt""\n\n\tprint ""saving:\\n"" + outfilename;\n\n\tfile = open(outfilename, \'w\')\n\tfor i in xrange(len(precs)):\n\t\tfile.write(str(precs[i])+"" ""+str(recalls[i])+"" ""+str(scores[i])+ "" "" + str(fppi[i])+ ""\\n"")\n\tfile.close()\n\t\n\t# Extracting failure cases\n\tif(options.analysisFile != None):\n\t\t\t\t\n\t\tanaPrefix = options.analysisFile\n\t\t\t\n\t\tfalsePositives = AnnoList([])\n\t\ttruePositives = AnnoList([])\n\t\tmissingRecall = AnnoList([])\n\t\tignoredTruePositives = AnnoList([])\n\n\t\tfor i in xrange(len(graphs)):\t\t\t\t\n\t\t\tfalsePositives.append(graphs[i].getFalsePositives())\n\t\t\ttruePositives.append(graphs[i].getTruePositives())\n\t\t\ttruePositives[-1].imageName = falsePositives[-1].imageName\n\t\t\ttruePositives[-1].imagePath = falsePositives[-1].imagePath\n\t\t\tmissingRecall.append(graphs[i].getMissingRecall())\n\t\t\tmissingRecall[-1].imageName = falsePositives[-1].imageName\n\t\t\tmissingRecall[-1].imagePath = falsePositives[-1].imagePath\n\t\t\tif options.ignoreFile != None:\n\t\t\t\tignoredTruePositives.append(graphs[i].getIgnoredTruePositives())\n\t\t\n\t\t#saveIDL(anaPrefix + ""-falsePositives.idl.gz"", falsePositives);\n                falsePositives.save(anaPrefix + ""-falsePositives.pal"")\n\t\n\t\tsortedFP = annoAnalyze(falsePositives);\n\t\t#saveIDL(anaPrefix + ""-falsePositives-sortedByScore.idl.gz"", sortedFP);\n\t\t#saveIDL(anaPrefix + ""-truePositives.idl.gz"", truePositives);\n\n\t\t# saveIDL(anaPrefix + ""-falsePositives-sortedByScore.idl"", sortedFP);\n\t\t# saveIDL(anaPrefix + ""-truePositives.idl"", truePositives);\n\n                sortedFP.save(anaPrefix + ""-falsePositives-sortedByScore.pal"")\n                truePositives.save(anaPrefix + ""-truePositives.pal"")\n\n\t\tsortedFP = annoAnalyze(truePositives);\n\t\t#saveIDL(anaPrefix + ""-truePositives-sortedByScore.idl.gz"", sortedFP);\n                #saveIDL(anaPrefix + ""-truePositives-sortedByScore.idl"", sortedFP);\n                sortedFP.save(anaPrefix + ""-truePositives-sortedByScore.pal"")\n\n\t\tif options.ignoreFile != None:\n\t\t\t#saveIDL(anaPrefix + ""-ignoredTruePositives.idl.gz"", ignoredTruePositives)\n                        #saveIDL(anaPrefix + ""-ignoredTruePositives.idl"", ignoredTruePositives)\n                        ignoredTruePositives.save(anaPrefix + ""-ignoredTruePositives.pal"")\n\t\t\n\t\t#saveIDL(anaPrefix + ""-missingRecall.idl.gz"", missingRecall);\n                #saveIDL(anaPrefix + ""-missingRecall.idl"", missingRecall);\n                missingRecall.save(anaPrefix + ""-missingRecall.pal"")\n\n\nif __name__ == ""__main__"":\n\tmain()\n\n\n'"
TENSORBOX/utils/annolist/ma_utils.py,0,b'def is_number(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n'
TENSORBOX/utils/annolist/plotSimple.py,0,"b'#!/usr/bin/env python\n\nimport sys\nimport os\nimport random\nimport re\nfrom AnnotationLib import *\nfrom MatPlotter import *\nfrom optparse import OptionParser\nfrom copy import deepcopy\nfrom math import sqrt\n\n\ndef main(argv):\n\tparser = OptionParser(usage=""usage: %prog [options] <datafile> [...]"")\n\tparser.add_option(""-o"", ""--output-file"",        action=""store"",\n\t\t\tdest=""output"", type=""str"", help=""outfile. mandatory"")\n\tparser.add_option(""--fppw"", action=""store_true"", dest=""fppw"", help=""False Positives Per Window"")\n\tparser.add_option(""--colors"", action=""store"", dest=""colors"", help=""colors"")\n\tparser.add_option(""--fppi"", action=""store_true"", dest=""fppi"", help=""False Positives Per Image"")\n\tparser.add_option(""--lfppi"", action=""store_true"", dest=""lfppi"", help=""False Positives Per Image(log)"")\n\tparser.add_option(""-c"", ""--components"", action=""store"", dest=""ncomponents"", type=""int"", help=""show n trailing components of the part"", default=3)\n\tparser.add_option(""--cut-trailing"", action=""store"", dest=""cutcomponents"", type=""int"", help=""cut n trailing components of the part (applied after --components)"", default=-1)\n\tparser.add_option(""-t"", ""--title"", action=""store"", dest=""title"", type=""str"", default="""")\n\tparser.add_option(""-f"", ""--fontsize"", action=""store"", dest=""fontsize"", type=""int"", default=12)\n\tparser.add_option(""-l"", ""--legend\'"", action=""store"", dest=""legend"", type=""string"", default=""lr"")\n\t(options, args) = parser.parse_args()\n\tplotter = MatPlotter(options.fontsize)\n\t\n\tposition = ""lower right""\n\tif(options.legend == ""ur""):\n\t\tposition = ""upper right""\n\tif(options.legend == ""ul""):\n\t\tposition = ""upper left""\n\tif(options.legend == ""ll""):\n\t\tposition = ""lower left""\t\n\tplotter.formatLegend(options.fontsize, newPlace = position)\n\t\n\ttitle = options.title\n\tcolors = None\n\tif (options.colors):\n\t\tcolors = options.colors.split()\n\tif (options.fppw):\n\t\tplotter.newFPPWFigure(title)\n\telif (options.lfppi):\n\t\tplotter.newLogFPPIFigure(title)\n\telif (options.fppi):\n\t\tplotter.newFPPIFigure(title)\n\telse:\n\t\tplotter.newFigure(title)\t\t\n\t\t\n\tfor i, filename in enumerate(args):\n\t\tif (os.path.isdir(filename)):\n\t\t\tfilename = os.path.join(filename, ""rpc"", ""result-minh-48"")\n\t\tdisplayname = filename\n\t\tif (options.ncomponents > 0):\n\t\t\tsuffix = None\n\t\t\tfor idx in xrange(options.ncomponents):\n\t\t\t\tdisplayname, last = os.path.split(displayname)\n\t\t\t\tif (suffix):\n\t\t\t\t\tsuffix = os.path.join(last, suffix)\n\t\t\t\telse:\n\t\t\t\t\tsuffix = last\n\t\t\tdisplayname = suffix\n\t\tif (options.cutcomponents > 0):\n\t\t\tfor idx in xrange(options.cutcomponents):\n\t\t\t\tdisplayname, last = os.path.split(displayname)\n#\t\tplusidx = displayname.index(""+"")\n#\t\tdisplayname = displayname[plusidx:]\n\t\tprint ""Plotting: ""+displayname\n\t\tif (options.fppw):\n\t\t\tplotter.plotFPPW(filename, displayname)\n\t\telif (options.lfppi):\n\t\t\tif colors:\n\t\t\t\tplotter.plotLogFPPI(filename, displayname, colors[i])\n\t\t\telse:\n\t\t\t\tplotter.plotLogFPPI(filename, displayname)\n\t\telif (options.fppi):\n\t\t\tplotter.plotFPPI(filename, displayname)\n\t\telse:\t\t\n\t\t\tplotter.plotRPC(filename, displayname)\n\t\n\tplotLine = not (options.fppw or options.lfppi or options.fppi);\t\t\n\t\n\tif (options.output is None):\n\t\tplotter.show(plotLine)\n\telse:\n\t\tplotter.saveCurrentFigure(plotLine, options.output)\n\treturn 0\n\nif __name__ == ""__main__"":\n\tsys.exit(main(sys.argv))\n'"
TENSORBOX/utils/kaffe/__init__.py,0,b''
TENSORBOX/utils/kaffe/mynet.py,0,"b""from network import Network\n\nclass VGG(Network):\n    def setup(self):\n        (self.feed('data')\n             .conv(3, 3, 64, 1, 1, name='conv1_1')\n             .conv(3, 3, 64, 1, 1, name='conv1_2')\n             .max_pool(2, 2, 2, 2, name='pool1')\n             .conv(3, 3, 128, 1, 1, name='conv2_1')\n             .conv(3, 3, 128, 1, 1, name='conv2_2')\n             .max_pool(2, 2, 2, 2, name='pool2')\n             .conv(3, 3, 256, 1, 1, name='conv3_1')\n             .conv(3, 3, 256, 1, 1, name='conv3_2')\n             .conv(3, 3, 256, 1, 1, name='conv3_3')\n             .max_pool(2, 2, 2, 2, name='pool3')\n             .conv(3, 3, 512, 1, 1, name='conv4_1')\n             .conv(3, 3, 512, 1, 1, name='conv4_2')\n             .conv(3, 3, 512, 1, 1, name='conv4_3')\n             .max_pool(2, 2, 2, 2, name='pool4')\n             .conv(3, 3, 512, 1, 1, name='conv5_1')\n             .conv(3, 3, 512, 1, 1, name='conv5_2')\n             .conv(3, 3, 512, 1, 1, name='conv5_3')\n             .max_pool(2, 2, 2, 2, name='pool5')\n             )\n             #.fc(4096, name='fc6')\n             #.fc(4096, name='fc7')\n             #.fc(1000, relu=False, name='fc8')\n             #.softmax(name='prob'))\n"""
TENSORBOX/utils/kaffe/network.py,21,"b""import numpy as np\nimport tensorflow as tf\n\nDEFAULT_PADDING = 'SAME'\n\ndef layer(op):\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.inputs)==0:\n            raise RuntimeError('No input variables found for layer %s.'%name)\n        elif len(self.inputs)==1:\n            layer_input = self.inputs[0]\n        else:\n            layer_input = list(self.inputs)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n    return layer_decorated\n\nclass Network(object):\n    def __init__(self, inputs, trainable=True):\n        self.inputs = []\n        self.layers = dict(inputs)\n        self.trainable = trainable\n        self.setup()\n\n    def setup(self):\n        raise NotImplementedError('Must be subclassed.')\n\n    def load(self, data_path, session, ignore_missing=False):\n        data_dict = np.load(data_path).item()\n        for key in data_dict:\n            with tf.variable_scope(key, reuse=True):\n                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n                    try:\n                        var = tf.get_variable(subkey)\n                        session.run(var.assign(data))\n                    except ValueError:\n                        if not ignore_missing:\n                            raise\n\n    def feed(self, *args):\n        assert len(args)!=0\n        self.inputs = []\n        for layer in args:\n            if isinstance(layer, basestring):\n                try:\n                    layer = self.layers[layer]\n                except KeyError:\n                    print self.layers.keys()\n                    raise KeyError('Unknown layer name fed: %s'%layer)\n            self.inputs.append(layer)\n        return self\n\n    def get_output(self):\n        return self.inputs[-1]\n\n    def get_unique_name(self, prefix):\n        id = sum(t.startswith(prefix) for t,_ in self.layers.items())+1\n        return '%s_%d'%(prefix, id)\n\n    def make_var(self, name, shape):\n        return tf.get_variable(name, shape, trainable=self.trainable)\n\n    def validate_padding(self, padding):\n        assert padding in ('SAME', 'VALID')\n\n    @layer\n    def conv(self, input, k_h, k_w, c_o, s_h, s_w, name, relu=True, padding=DEFAULT_PADDING, group=1):\n        self.validate_padding(padding)\n        c_i = input.get_shape()[-1]\n        assert c_i%group==0\n        assert c_o%group==0\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_var('weights', shape=[k_h, k_w, c_i/group, c_o])\n            biases = self.make_var('biases', [c_o])\n            if group==1:\n                conv = convolve(input, kernel)\n            else:\n                input_groups = tf.split(3, group, input)\n                kernel_groups = tf.split(3, group, kernel)\n                output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n                conv = tf.concat(3, output_groups)\n            if relu:\n                bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list())\n                return tf.nn.relu(bias, name=scope.name)\n            return tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list(), name=scope.name)\n\n    @layer\n    def relu(self, input, name):\n        return tf.nn.relu(input, name=name)\n\n    @layer\n    def max_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def avg_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.avg_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def lrn(self, input, radius, alpha, beta, name, bias=1.0):\n        return tf.nn.local_response_normalization(input,\n                                                  depth_radius=radius,\n                                                  alpha=alpha,\n                                                  beta=beta,\n                                                  bias=bias,\n                                                  name=name)\n\n    @layer\n    def concat(self, inputs, axis, name):\n        return tf.concat(concat_dim=axis, values=inputs, name=name)\n\n    @layer\n    def fc(self, input, num_out, name, relu=True):\n        with tf.variable_scope(name) as scope:\n            input_shape = input.get_shape()\n            if input_shape.ndims==4:\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= d\n                feed_in = tf.reshape(input, [int(input_shape[0]), dim])\n            else:\n                feed_in, dim = (input, int(input_shape[-1]))\n            weights = self.make_var('weights', shape=[dim, num_out])\n            biases = self.make_var('biases', [num_out])\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=scope.name)\n            return fc\n\n    @layer\n    def softmax(self, input, name):\n        return tf.nn.softmax(input, name)\n\n    @layer\n    def dropout(self, input, keep_prob, name):\n        return tf.nn.dropout(input, keep_prob, name=name)\n"""
