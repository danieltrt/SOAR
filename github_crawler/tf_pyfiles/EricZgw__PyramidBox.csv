file_path,api_count,code
AnchorSampling.py,0,b'Todo:\n'
check_data_io.py,4,"b'#coding=utf-8\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nfrom preparedata import PrepareData\nfrom nets.ssd import g_ssd_model\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\nobj= PrepareData()\n\nimage, filename,glabels,gbboxes,gdifficults,gclasses_face, localizations_face, gscores_face,\\\ngclasses_head, localizations_head, gscores_head,gclasses_body, localizations_body,\\\ngscores_body=obj.get_voc_2007_2012_train_data()\n\nssd_anchors = g_ssd_model.ssd_anchors_all_layers()\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n    \n    for i in range(5):\n        img,picname,label,bbox,gclass,glocal,gscore=sess.run([image, filename,glabels,gbboxes,gclasses_face, localizations_face, gscores_face])\n        \n        b=np.zeros_like(img[0])\n        b[:,:,1]=img[0][:,:,1]\n        b[:,:,0]=img[0][:,:,2]\n        b[:,:,2]=img[0][:,:,0]\n        box=bbox[0]\n        hh,ww=b.shape[:2]\n        \n        gboxes=[]\n        for u in range(len(gclass)):\n            gbox=glocal[u][0].reshape([-1,4])\n            gcls=gclass[u][0].reshape([-1])\n            gsc=gscore[u][0].reshape([-1])\n            anchor_bboxes=ssd_anchors[u]\n            yref, xref, href_src, wref_src = anchor_bboxes\n            href=href_src/2.\n            wref=wref_src/2.\n            xref = np.reshape(xref, [-1])\n            yref = np.reshape(yref, [-1])\n            cx = gbox[:,  0] * wref * 0.1 + xref\n            cy = gbox[:,  1] * href * 0.1 + yref\n            w = wref * np.exp(gbox[:,  2] * 0.2)\n            h = href * np.exp(gbox[:,  3] * 0.2)\n            bboxes = np.zeros_like(gbox)\n            bboxes[:,  0] = cy - h / 2.\n            bboxes[:,  1] = cx - w / 2.\n            bboxes[:,  2] = cy + h / 2.\n            bboxes[:,  3] = cx + w / 2.\n            \n            for f in range(len(gcls)):\n                if gcls[f]==1:\n                    gboxes.append(gbox[f])\n                    cv2.rectangle(b,(int(bboxes[f][1]*640),int(bboxes[f][0]*640)),(int(bboxes[f][3]*640),int(bboxes[f][2]*640)),(0,0,255),3)        \n        \n        for j in range(len(box)):\n            cv2.rectangle(b,(int(box[j][1]*ww),int(box[j][0]*hh)),(int(box[j][3]*ww),int(box[j][2]*hh)),(0,255,0),3)\n        cv2.imshow(\'test\',b.astype(np.uint8))\n        cv2.waitKey(0)\n    \n    coord.request_stop()\n    coord.join(threads)\n'"
demo.py,8,"b'import os\nimport math\nimport random\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nfrom PIL import Image\n\nfrom tensorflow.contrib import slim\n\n# slim = tf.contrib.slim\n\nimport matplotlib.pyplot as plt\n\n\nimport sys\nsys.path.append(\'../\')\n\n\nfrom preprocessing import ssd_vgg_preprocessing\nfrom utility import visualization\nfrom nets.ssd import g_ssd_model\nimport nets.np_methods as np_methods\n\n\n# TensorFlow session: grow memory when needed. \nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\ngpu_options = tf.GPUOptions(allow_growth=True)\nconfig = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\nisess = tf.InteractiveSession(config=config)\n\n\n# Input placeholder.\ndata_format = \'NHWC\'\nimg_input = tf.placeholder(tf.uint8, shape=(None, None, 3))\n# Evaluation pre-processing: resize to SSD net shape.\nimage_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(\n    img_input, None, None, data_format, resize=ssd_vgg_preprocessing.Resize.NONE)\nimage_4d = tf.expand_dims(image_pre, 0)\n\n# Define the SSD model.\n\npredictions, localisations, _, end_points = g_ssd_model.get_model(image_4d)\n\n# Restore SSD model.\nckpt_filename = \'model/pyramidbox.ckpt\'\n\nisess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver()\nsaver.restore(isess, ckpt_filename)\n\n\n\n\n# Main image processing routine.\ndef process_image(img, select_threshold=0.35, nms_threshold=0.1):\n    # Run SSD network.\n    h,w=img.shape[:2]\n    if h<w and h<640:\n        scale=640./h\n        h=640\n        w=int(w*scale)\n    elif h>=w and w<640:\n        scale=640./w\n        w=640\n        h=int(h*scale)\n    img=Image.fromarray(np.uint8(img))\n    resized_img=img.resize((w,h))    \n    net_shape=np.array(resized_img).shape[:2]\n    rimg, rpredictions, rlocalisations, rbbox_img,e_ps = isess.run([image_4d, predictions, localisations, bbox_img,end_points],feed_dict={img_input: resized_img})\n    \n    layer_shape=[e_ps[\'block3\'].shape[1:3],e_ps[\'block4\'].shape[1:3],e_ps[\'block5\'].shape[1:3],e_ps[\'block7\'].shape[1:3],e_ps[\'block8\'].shape[1:3],e_ps[\'block9\'].shape[1:3]]\n\n    # SSD default anchor boxes.\n    ssd_anchors = g_ssd_model.ssd_anchors_all_layers(feat_shapes=layer_shape,img_shape=net_shape)\n\n    # Get classes and bboxes from the net outputs.\n    rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(\n            rpredictions, rlocalisations[0], ssd_anchors,\n            select_threshold=select_threshold, img_shape=net_shape, num_classes=2, decode=True)\n    \n    rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)\n    rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=1200)\n    rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, nms_threshold=nms_threshold)\n    # Resize bboxes to original image shape. Note: useless for Resize.WARP!\n    rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)\n\n    return rclasses, rscores, rbboxes\n\n\n# Test on some demo image and visualize output.\npath = \'demo/images/\'\nimage_names = sorted(os.listdir(path))\nfor i in range(len(image_names)):\n    img = np.array(Image.open(path + image_names[i]))\n    rclasses, rscores, rbboxes =  process_image(img)\n    visualization.plt_bboxes(img, rclasses, rscores, rbboxes)\n    plt.show()\n\n'"
makedir.py,0,"b""import os\nif not os.path.isdir('checkpoints'):\n    os.mkdir('checkpoints')\nif not os.path.isdir('model'):\n    os.mkdir('model')\nif not os.path.isdir('tfrecords'):\n    os.mkdir('tfrecords')\nif not os.path.isdir('datasets/widerface'):\n    os.mkdir('datasets/widerface')\n"""
preparedata.py,7,"b'from datasets import pascalvoc_datasets\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow.contrib.slim as slim\n# from nets import nets_factory\nfrom preprocessing import preprocessing_factory\nimport numpy as np\nimport cv2\nfrom utility import visualization\nfrom nets.ssd import g_ssd_model\nfrom preprocessing.ssd_vgg_preprocessing import np_image_unwhitened\nfrom preprocessing.ssd_vgg_preprocessing import preprocess_for_train\nfrom preprocessing.ssd_vgg_preprocessing import preprocess_for_eval\nimport tf_utils\nimport math\nimport tf_extended as tfe\n\n\nclass PrepareData():\n    def __init__(self):\n        \n        self.batch_size = 1\n        self.labels_offset = 0\n        \n        \n        self.matched_thresholds = 0.35 #threshold for anchor matching strategy\n      \n        \n       \n        \n        \n        return\n    def __preprocess_data(self, image, labels, bboxes,shape):\n        out_shape = g_ssd_model.img_shape\n        if self.is_training_data:\n            image, labels, bboxes = preprocess_for_train(image, labels, bboxes, out_shape = out_shape)\n        else:\n            image, labels, bboxes, _ = preprocess_for_eval(image, labels, bboxes)\n        return image, labels, bboxes\n    def __get_images_labels_bboxes(self,data_sources, num_samples,is_training_data):\n        \n        self.dataset = pascalvoc_datasets.get_dataset_info(data_sources, num_samples)\n        self.is_training_data = is_training_data\n        if self.is_training_data:\n            \n            shuffle = True\n            #make sure most samples can be fetched in one epoch\n            self.num_readers = 2\n        else:\n            #make sure data is fetchd in sequence\n            shuffle = False\n            self.num_readers = 1\n            \n        \n        provider = slim.dataset_data_provider.DatasetDataProvider(\n                    self.dataset,\n                    shuffle=shuffle,\n                    num_readers=self.num_readers,\n                    common_queue_capacity=20 * self.batch_size,\n                    common_queue_min=10 * self.batch_size)\n        \n        # Get for SSD network: image, labels, bboxes.\n        [image, shape, format, filename, glabels, gbboxes,gdifficults] = provider.get([\'image\', \'shape\', \'format\',\'filename\',\n                                                         \'object/label\',\n                                                         \'object/bbox\',\n                                                         \'object/difficult\'])\n        glabels -= self.labels_offset\n        \n        # Pre-processing image, labels and bboxes.\n        image, glabels, gbboxes = self.__preprocess_data(image, glabels, gbboxes,shape)\n       \n        gclasses_list, glocalisations_list, gscores_list = g_ssd_model.match_achors(glabels, gbboxes, matching_threshold=self.matched_thresholds)\n        \n        \n        return self.__batching_data(image, glabels, format, filename, gbboxes, gdifficults, gclasses_list, glocalisations_list, gscores_list)\n    def __batching_data(self,image, glabels, format, filename, gbboxes, gdifficults,gclasses, glocalisations, gscores):\n        \n        #we will want to batch original glabels and gbboxes\n        #this information is still useful even if they are padded after dequeuing\n        dynamic_pad = True\n        batch_shape = [1,1,1,1,1] + [len(gclasses[0]), len(glocalisations[0]), len(gscores[0])] + [len(gclasses[1]), len(glocalisations[1]), len(gscores[1])] + [len(gclasses[2]), len(glocalisations[2]), len(gscores[2])]\n        tensors = [image, filename,glabels,gbboxes,gdifficults,gclasses[0], glocalisations[0], gscores[0],gclasses[1], glocalisations[1], gscores[1],gclasses[2], glocalisations[2], gscores[2]]\n        #Batch the samples\n        if self.is_training_data:\n            self.num_preprocessing_threads = 1\n        else:\n            # to make sure data is fectched in sequence during evaluation\n            self.num_preprocessing_threads = 1\n            \n        #tf.train.batch accepts only list of tensors, this batch shape can used to\n        #flatten the list in list, and later on convet it back to list in list.\n        batch = tf.train.batch(\n                tf_utils.reshape_list(tensors),\n                batch_size=self.batch_size,\n                num_threads=self.num_preprocessing_threads,\n                dynamic_pad=dynamic_pad,\n                capacity=5 * self.batch_size)\n            \n        #convert it back to the list in list format which allows us to easily use later on\n        batch= tf_utils.reshape_list(batch, batch_shape)\n        return batch\n    def __disp_image(self, img, classes, bboxes):\n        bvalid = (classes !=0)\n        classes = classes[bvalid]\n        bboxes = bboxes[bvalid]\n        scores =np.full(classes.shape, 1.0)\n        visualization.plt_bboxes(img, classes, scores, bboxes,title=\'Ground Truth\')\n        return\n    def __disp_matched_anchors(self,img, target_labels_data, target_localizations_data, target_scores_data):\n        found_matched = False\n        all_anchors = g_ssd_model.get_allanchors()\n        for i, target_score_data in enumerate(target_scores_data):\n\n            num_pos = (target_labels_data[i] != 0).sum()\n            if (num_pos == 0):\n                continue\n            print(\'Found  {} matched default boxes in layer {}\'.format(num_pos,g_ssd_model.feat_layers[i]))\n#             pos_sample_inds = ((target_labels_data[i] != 0) & (target_score_data <=self.matched_thresholds)).nonzero()\n            pos_sample_inds = (target_labels_data[i] != 0).nonzero()\n\n\n            classes = target_labels_data[i][pos_sample_inds]\n            scores = target_scores_data[i][pos_sample_inds]\n            print(""matched scores :{}"".format(scores))\n            print(""matched labels: {}"".format(classes))\n            bboxes_default= g_ssd_model.get_allanchors(minmaxformat=True)[i][pos_sample_inds]\n            \n            \n            \n            bboxes_gt = g_ssd_model.decode_bboxes_layer(target_localizations_data[i][pos_sample_inds], \n                                       all_anchors[i][pos_sample_inds])\n            \n#             print(""default box minimum, {} gt box minimum, {}"".format(bboxes_default.min(), bboxes_gt.min()))\n            \n            marks_default = np.full(classes.shape, True)\n            marks_gt = np.full(classes.shape, False)\n            scores_gt = np.full(scores.shape, 1.0)\n            \n            bboxes = bboxes_default\n            neg_marks = marks_default\n            add_gt = True\n            if add_gt :\n                bboxes = np.vstack([bboxes_default,bboxes_gt])\n                neg_marks = np.hstack([marks_default,marks_gt])\n                classes = np.tile(classes, 2)\n                scores = np.hstack([scores, scores_gt])\n            \n            title = ""Default boxes: Layer {}"".format(g_ssd_model.feat_layers[i])\n            visualization.plt_bboxes(img, classes, scores, bboxes,neg_marks=neg_marks,title=title)\n            found_matched = True  \n            \n        return found_matched\n    def get_voc_2007_train_data(self,is_training_data=True):\n        data_sources = ""tfrecords_val/voc_2007_val*.tfrecord""\n        num_samples = pascalvoc_datasets.DATASET_SIZE[\'wider_face_val\']\n       \n        return self.__get_images_labels_bboxes(data_sources, num_samples, is_training_data)\n    \n    def get_voc_2012_train_data(self,is_training_data=True):\n        data_sources = ""../data/voc/tfrecords/voc_train_2012*.tfrecord""\n        num_samples = pascalvoc_datasets.DATASET_SIZE[\'2012_train\']\n        \n        return self.__get_images_labels_bboxes(data_sources, num_samples, is_training_data)\n    \n    def get_voc_2007_2012_train_data(self,is_training_data=True):\n        data_sources = ""tfrecords/voc_train*.tfrecord""\n        num_samples = pascalvoc_datasets.DATASET_SIZE[\'wider_face_train\'] \n        \n        return self.__get_images_labels_bboxes(data_sources, num_samples, is_training_data)\n    def get_voc_2007_test_data(self):\n        data_sources = ""../data/voc/tfrecords/voc_test_2007*.tfrecord""\n        num_samples = pascalvoc_datasets.DATASET_SIZE[\'2007_test\']\n        \n        return self.__get_images_labels_bboxes(data_sources, num_samples, False)\n    \n        \n    def iterate_file_name(self, batch_data):\n        \n        num_batches = 1*math.ceil(self.dataset.num_samples / float(self.batch_size))\n        all_files = []\n        with tf.Session(\'\') as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            with slim.queues.QueueRunners(sess):\n                for i in range(num_batches):\n                    \n                    image, filename,glabels,gbboxes,gdifficults,gclasses, glocalisations, gscores = sess.run(list(batch_data))\n                    print(filename)\n                    all_files.append(filename)\n                all_files = np.concatenate(all_files)\n                \n                \n                all_files_unique = np.unique(all_files)\n                print(len(all_files_unique))\n            \n        return\n    def check_match_statistics(self,filename,gclasses, gscores):\n        #flatten the array into Batch_num x bbox_num\n        gt_anchor_labels = []\n        gt_anchor_scores = []\n        for i in range(len(gclasses)):\n            gt_anchor_labels.append(np.reshape(gclasses[i], [self.batch_size, -1]))\n            gt_anchor_scores.append(np.reshape(gscores[i], [self.batch_size,-1]))\n        gt_anchor_labels = np.concatenate(gt_anchor_labels, axis = 1)\n        gt_anchor_scores = np.concatenate(gt_anchor_scores, axis = 1)\n        \n        #find out missed match\n        inds = (gt_anchor_scores <= self.matched_thresholds) & (gt_anchor_labels != 0)\n        \n        real_inds = inds.nonzero()\n\n        print(""missed match: {}"".format(filename[real_inds[0]]))\n        print(""missed match scores: {}"".format(gt_anchor_scores[real_inds]))\n        print(""missed match labels: {}"".format(gt_anchor_labels[real_inds]))\n        return\n    def run(self):\n        \n        \n        with tf.Graph().as_default():\n#             batch_data= self.get_voc_2007_train_data(is_training_data=True)\n#             batch_data = self.get_voc_2007_test_data()\n#             batch_data = self.get_voc_2012_train_data()\n            batch_data = self.get_voc_2007_2012_train_data(is_training_data = True)\n\n\n            return self.iterate_file_name(batch_data)\n           \n            with tf.Session(\'\') as sess:\n                init = tf.global_variables_initializer()\n                sess.run(init)\n                with slim.queues.QueueRunners(sess):  \n                    while True:  \n                         \n                        image, filename,glabels,gbboxes,gdifficults,gclasses, glocalisations, gscores = sess.run(list(batch_data))\n                        \n                        \n                        \n#                         print(""min: {}, max: {}"".format(gbboxes.min(), gbboxes.max()))\n#                         return\n                        \n#                         print(glabels)\n#                         print(""number of zero label patch {}"".format((glabels.sum(axis=1)  == 0).sum()))\n#                         return\n                       \n#                         \n                        \n                         \n                        print(filename)\n                        selected_file =  b\'000050\'\n                        picked_inds = None\n                        #selet the first image in the batch\n                        if selected_file is None:\n                            picked_inds = 0\n                        else:\n                            picked_inds = (selected_file == filename).nonzero()\n                            if len(picked_inds[0]) == 0:\n                                picked_inds = None\n                            else:\n                                picked_inds = picked_inds[0][0]\n                        \n                        if picked_inds is None:\n                            continue\n                        \n                        self.check_match_statistics(filename, gclasses, gscores)\n                        target_labels_data = [item[picked_inds] for item in gclasses]\n                        target_localizations_data = [item[picked_inds] for item in glocalisations]\n                        target_scores_data = [item[picked_inds] for item in gscores]\n                        image_data = image[picked_inds]\n                        print(""picked file {}"".format(filename[picked_inds]))\n \n                        image_data = np_image_unwhitened(image_data)\n                        self.__disp_image(image_data, glabels[picked_inds], gbboxes[picked_inds])\n                        found_matched = self.__disp_matched_anchors(image_data,target_labels_data, target_localizations_data, target_scores_data)\n                        plt.show()\n                        break;\n                        #exit the batch data testing right after a successful match have been found\n#                         if found_matched:\n                        #this could be a potential issue to be solved since sometime not all grouth truth bboxes are encoded.\n                        \n                            \n                        \n        \n        \n        \n        return\n    \n    \n\n\nif __name__ == ""__main__"":   \n    obj= PrepareData()\n    obj.run()\n'"
tf_utils.py,20,"b'# Copyright 2016 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Diverse TensorFlow utils, for training, evaluation and so on!\n""""""\nimport os\nfrom pprint import pprint\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.data import parallel_reader\n\nslim = tf.contrib.slim\n\n\n# =========================================================================== #\n# General tools.\n# =========================================================================== #\ndef reshape_list(l, shape=None):\n    """"""Reshape list of (list): 1D to 2D or the other way around.\n\n    Args:\n      l: List or List of list.\n      shape: 1D or 2D shape.\n    Return\n      Reshaped list.\n    """"""\n    r = []\n    if shape is None:\n        # Flatten everything.\n        for a in l:\n            if isinstance(a, (list, tuple)):\n                r = r + list(a)\n            else:\n                r.append(a)\n    else:\n        # Reshape to list of list.\n        i = 0\n        for s in shape:\n            if s == 1:\n                r.append(l[i])\n            else:\n                r.append(l[i:i+s])\n            i += s\n    return r\n\n\n# =========================================================================== #\n# Training utils.\n# =========================================================================== #\ndef print_configuration(flags, ssd_params, data_sources, save_dir=None):\n    """"""Print the training configuration.\n    """"""\n    def print_config(stream=None):\n        print(\'\\n# =========================================================================== #\', file=stream)\n        print(\'# Training | Evaluation flags:\', file=stream)\n        print(\'# =========================================================================== #\', file=stream)\n        pprint(flags, stream=stream)\n\n        print(\'\\n# =========================================================================== #\', file=stream)\n        print(\'# SSD net parameters:\', file=stream)\n        print(\'# =========================================================================== #\', file=stream)\n        pprint(dict(ssd_params._asdict()), stream=stream)\n\n        print(\'\\n# =========================================================================== #\', file=stream)\n        print(\'# Training | Evaluation dataset files:\', file=stream)\n        print(\'# =========================================================================== #\', file=stream)\n        data_files = parallel_reader.get_data_files(data_sources)\n        pprint(data_files, stream=stream)\n        print(\'\', file=stream)\n\n    print_config(None)\n    # Save to a text file as well.\n    if save_dir is not None:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        path = os.path.join(save_dir, \'training_config.txt\')\n        with open(path, ""w"") as out:\n            print_config(out)\n\n\ndef configure_learning_rate(flags, num_samples_per_epoch, global_step):\n    """"""Configures the learning rate.\n\n    Args:\n      num_samples_per_epoch: The number of samples in each epoch of training.\n      global_step: The global_step tensor.\n    Returns:\n      A `Tensor` representing the learning rate.\n    """"""\n    decay_steps = int(num_samples_per_epoch / flags.batch_size *\n                      flags.num_epochs_per_decay)\n\n    if flags.learning_rate_decay_type == \'exponential\':\n        return tf.train.exponential_decay(flags.learning_rate,\n                                          global_step,\n                                          decay_steps,\n                                          flags.learning_rate_decay_factor,\n                                          staircase=True,\n                                          name=\'exponential_decay_learning_rate\')\n    elif flags.learning_rate_decay_type == \'fixed\':\n        return tf.constant(flags.learning_rate, name=\'fixed_learning_rate\')\n    elif flags.learning_rate_decay_type == \'polynomial\':\n        return tf.train.polynomial_decay(flags.learning_rate,\n                                         global_step,\n                                         decay_steps,\n                                         flags.end_learning_rate,\n                                         power=1.0,\n                                         cycle=False,\n                                         name=\'polynomial_decay_learning_rate\')\n    else:\n        raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                         flags.learning_rate_decay_type)\n\n\ndef configure_optimizer(flags, learning_rate):\n    """"""Configures the optimizer used for training.\n\n    Args:\n      learning_rate: A scalar or `Tensor` learning rate.\n    Returns:\n      An instance of an optimizer.\n    """"""\n    if flags.optimizer == \'adadelta\':\n        optimizer = tf.train.AdadeltaOptimizer(\n            learning_rate,\n            rho=flags.adadelta_rho,\n            epsilon=flags.opt_epsilon)\n    elif flags.optimizer == \'adagrad\':\n        optimizer = tf.train.AdagradOptimizer(\n            learning_rate,\n            initial_accumulator_value=flags.adagrad_initial_accumulator_value)\n    elif flags.optimizer == \'adam\':\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate,\n            beta1=flags.adam_beta1,\n            beta2=flags.adam_beta2,\n            epsilon=flags.opt_epsilon)\n    elif flags.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(\n            learning_rate,\n            learning_rate_power=flags.ftrl_learning_rate_power,\n            initial_accumulator_value=flags.ftrl_initial_accumulator_value,\n            l1_regularization_strength=flags.ftrl_l1,\n            l2_regularization_strength=flags.ftrl_l2)\n    elif flags.optimizer == \'momentum\':\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate,\n            momentum=flags.momentum,\n            name=\'Momentum\')\n    elif flags.optimizer == \'rmsprop\':\n        optimizer = tf.train.RMSPropOptimizer(\n            learning_rate,\n            decay=flags.rmsprop_decay,\n            momentum=flags.rmsprop_momentum,\n            epsilon=flags.opt_epsilon)\n    elif flags.optimizer == \'sgd\':\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    else:\n        raise ValueError(\'Optimizer [%s] was not recognized\', flags.optimizer)\n    return optimizer\n\n\ndef add_variables_summaries(learning_rate):\n    summaries = []\n    for variable in slim.get_model_variables():\n        summaries.append(tf.summary.histogram(variable.op.name, variable))\n    summaries.append(tf.summary.scalar(\'training/Learning Rate\', learning_rate))\n    return summaries\n\n\ndef update_model_scope(var, ckpt_scope, new_scope):\n    return var.op.name.replace(new_scope,\'vgg_16\')\n\n\ndef get_init_fn(flags):\n    """"""Returns a function run by the chief worker to warm-start the training.\n    Note that the init_fn is only run when initializing the model during the very\n    first global step.\n\n    Returns:\n      An init function run by the supervisor.\n    """"""\n    if flags.checkpoint_path is None:\n        return None\n    # Warn the user if a checkpoint exists in the train_dir. Then ignore.\n    if tf.train.latest_checkpoint(flags.train_dir):\n        tf.logging.info(\n            \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n            % flags.train_dir)\n        return None\n\n    exclusions = []\n    if flags.checkpoint_exclude_scopes:\n        exclusions = [scope.strip()\n                      for scope in flags.checkpoint_exclude_scopes.split(\',\')]\n\n    # TODO(sguada) variables.filter_variables()\n    variables_to_restore = []\n    for var in slim.get_model_variables():\n        excluded = False\n        for exclusion in exclusions:\n            if var.op.name.startswith(exclusion):\n                excluded = True\n                break\n        if not excluded:\n            variables_to_restore.append(var)\n    # Change model scope if necessary.\n    if flags.checkpoint_model_scope is not None:\n        variables_to_restore = \\\n            {var.op.name.replace(flags.model_name,\n                                 flags.checkpoint_model_scope): var\n             for var in variables_to_restore}\n\n\n    if tf.gfile.IsDirectory(flags.checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags.checkpoint_path)\n    else:\n        checkpoint_path = flags.checkpoint_path\n    tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n    return slim.assign_from_checkpoint_fn(\n        checkpoint_path,\n        variables_to_restore,\n        ignore_missing_vars=flags.ignore_missing_vars)\n\n\ndef get_variables_to_train(flags):\n    """"""Returns a list of variables to train.\n\n    Returns:\n      A list of variables to train by the optimizer.\n    """"""\n    if flags.trainable_scopes is None:\n        return tf.trainable_variables()\n    else:\n        scopes = [scope.strip() for scope in flags.trainable_scopes.split(\',\')]\n\n    variables_to_train = []\n    for scope in scopes:\n        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n        variables_to_train.extend(variables)\n    return variables_to_train\n\n\n# =========================================================================== #\n# Evaluation utils.\n# =========================================================================== #\n'"
train_model.py,27,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom preparedata import PrepareData\nfrom nets.ssd import g_ssd_model\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import timeline\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training import saver as tf_saver\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\nclass TrainModel(PrepareData):\n    def __init__(self):\n        PrepareData.__init__(self)\n        \n        self.num_epochs_per_decay = 8.0\n        self.learning_rate_decay_type = \'exponential\'\n        self.end_learning_rate =  0.0001\n        self.learning_rate = 0.1\n        \n        #optimiser\n        self.optimizer = \'rmsprop\'\n        \n        \n        self.adadelta_rho = 0.95\n        self.opt_epsilon= 1.0\n        self.adagrad_initial_accumulator_value= 0.1\n        self.adam_beta1= 0.9\n        self.adam_beta2= 0.999\n        self.ftrl_learning_rate_power = -0.5\n        self.ftrl_initial_accumulator_value = 0.1\n        self.ftrl_l1= 0.0\n        self.ftrl_l2 = 0.0\n        self.momentum= 0.9\n        \n        self.rmsprop_decay = 0.9\n        self.rmsprop_momentum = 0.9\n        \n        self.train_dir = \'/tmp/tfmodel/\'\n        self.max_number_of_steps = None\n\n        \n        self.checkpoint_path = None\n        self.checkpoint_exclude_scopes = None\n        self.ignore_missing_vars = False\n        \n        self.batch_size= 1\n        \n        self.save_interval_secs = 60*60*1#one hour\n        self.save_summaries_secs= 30\n\n        self.learning_rate_decay_factor=0.5   \n        \n        \n        \n        self.label_smoothing = 0\n        return\n    \n    def __configure_learning_rate(self, num_samples_per_epoch, global_step):\n        """"""Configures the learning rate.\n    \n        Args:\n            num_samples_per_epoch: The number of samples in each epoch of training.\n            global_step: The global_step tensor.\n    \n        Returns:\n            A `Tensor` representing the learning rate.\n    \n        Raises:\n            ValueError: if\n        """"""\n        decay_steps = int(num_samples_per_epoch / self.batch_size * self.num_epochs_per_decay)\n       \n    \n        if self.learning_rate_decay_type == \'exponential\':\n            return tf.train.exponential_decay(self.learning_rate,\n                                                                                global_step,\n                                                                                decay_steps,\n                                                                                self.learning_rate_decay_factor,\n                                                                                staircase=True,\n                                                                                name=\'exponential_decay_learning_rate\')\n        elif self.learning_rate_decay_type == \'fixed\':\n            return tf.constant(self.learning_rate, name=\'fixed_learning_rate\')\n        elif self.learning_rate_decay_type == \'polynomial\':\n            return tf.train.polynomial_decay(self.learning_rate,\n                                                                             global_step,\n                                                                             decay_steps,\n                                                                             self.end_learning_rate,\n                                                                             power=1.0,\n                                                                             cycle=False,\n                                                                             name=\'polynomial_decay_learning_rate\')\n        else:\n            raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                                         self.learning_rate_decay_type)\n        return\n    def __configure_optimizer(self, learning_rate):\n        """"""Configures the optimizer used for training.\n    \n        Args:\n            learning_rate: A scalar or `Tensor` learning rate.\n    \n        Returns:\n            An instance of an optimizer.\n    \n        Raises:\n            ValueError: if FLAGS.optimizer is not recognized.\n        """"""\n        if self.optimizer == \'adadelta\':\n            optimizer = tf.train.AdadeltaOptimizer(\n                    learning_rate,\n                    rho=self.adadelta_rho,\n                    epsilon=self.opt_epsilon)\n        elif self.optimizer == \'adagrad\':\n            optimizer = tf.train.AdagradOptimizer(\n                    learning_rate,\n                    initial_accumulator_value=self.adagrad_initial_accumulator_value)\n        elif self.optimizer == \'adam\':\n            optimizer = tf.train.AdamOptimizer(\n                    learning_rate,\n                    beta1=self.adam_beta1,\n                    beta2=self.adam_beta2,\n                    epsilon=self.opt_epsilon)\n        elif self.optimizer == \'ftrl\':\n            optimizer = tf.train.FtrlOptimizer(\n                    learning_rate,\n                    learning_rate_power=self.ftrl_learning_rate_power,\n                    initial_accumulator_value=self.ftrl_initial_accumulator_value,\n                    l1_regularization_strength=self.ftrl_l1,\n                    l2_regularization_strength=self.ftrl_l2)\n        elif self.optimizer == \'momentum\':\n            optimizer = tf.train.MomentumOptimizer(\n                    learning_rate,\n                    momentum=self.momentum,\n                    name=\'Momentum\')\n        elif self.optimizer == \'rmsprop\':\n            optimizer = tf.train.RMSPropOptimizer(\n                    learning_rate,\n                    decay=self.rmsprop_decay,\n                    momentum=self.rmsprop_momentum,\n                    epsilon=self.opt_epsilon)\n        elif self.optimizer == \'sgd\':\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n        else:\n            raise ValueError(\'Optimizer [%s] was not recognized\', self.optimizer)\n        return optimizer\n    def __get_variables_to_train(self):\n        """"""Returns a list of variables to train.\n    \n        Returns:\n            A list of variables to train by the optimizer.\n        """"""\n        if self.trainable_scopes is None:\n            return tf.trainable_variables()\n        else:\n            scopes = [scope.strip() for scope in self.trainable_scopes.split(\',\')]\n    \n        variables_to_train = []\n        for scope in scopes:\n            variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n            variables_to_train.extend(variables)\n        return variables_to_train\n    \n    \n    def __start_training(self):\n        tf.logging.set_verbosity(tf.logging.INFO)\n        \n        #get batched training training data \n        image, filename,glabels,gbboxes,gdifficults,gclasses_face, localizations_face, gscores_face,\\\n        gclasses_head, localizations_head, gscores_head,gclasses_body, localizations_body, gscores_body= self.get_voc_2007_2012_train_data()\n        \n        #get model outputs\n        localisations, logits, end_points = g_ssd_model.get_model(image, weight_decay=self.weight_decay, is_training=True)\n        \n        #get model training losss\n        gclasses=[gclasses_face,gclasses_head,gclasses_body]\n        localizations=[localizations_face,localizations_head,localizations_body]\n        gscores=[gscores_face,gscores_head,gscores_body]\n        total_loss = g_ssd_model.get_losses(logits, localisations, gclasses, localizations, gscores)\n\n        \n        \n        global_step = slim.create_global_step()\n        \n        # Variables to train.\n        variables_to_train = self.__get_variables_to_train()\n        \n        learning_rate = self.__configure_learning_rate(self.dataset.num_samples, global_step)\n        optimizer = self.__configure_optimizer(learning_rate)\n        \n        \n        train_op = slim.learning.create_train_op(total_loss, optimizer, variables_to_train=variables_to_train)\n        \n        self.__add_summaries(end_points, learning_rate, total_loss)\n        \n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n        config = tf.ConfigProto(log_device_placement=False,\n                                gpu_options=gpu_options)\n        \n        ###########################\n        # Kicks off the training. #\n        ###########################\n       \n        slim.learning.train(\n                train_op,\n                self.train_dir,\n                train_step_fn=self.train_step,\n                saver=tf_saver.Saver(max_to_keep=5),\n                init_fn=self.__get_init_fn(),\n                number_of_steps=self.max_number_of_steps,\n                log_every_n_steps=self.log_every_n_steps,\n                save_summaries_secs=self.save_summaries_secs,\n#                 session_config=config,\n                save_interval_secs=self.save_interval_secs)\n        \n        \n        return\n   \n    def debug_training(self,sess,global_step):\n        np_global_step = sess.run(global_step)\n        if np_global_step % self.log_every_n_steps != 0:\n            return\n        \n       \n        m_AP_12 = sess.run(self.mAP_12_op_train)\n        logging.info(""step {}/{}, m_AP_12 {}"".format(np_global_step, self.max_number_of_steps, m_AP_12))\n        \n        return\n    def train_step(self, sess, train_op, global_step, train_step_kwargs):\n        """"""Function that takes a gradient step and specifies whether to stop.\n    \n        Args:\n            sess: The current session.\n            train_op: An `Operation` that evaluates the gradients and returns the\n                total loss.\n            global_step: A `Tensor` representing the global training step.\n            train_step_kwargs: A dictionary of keyword arguments.\n    \n        Returns:\n            The total loss and a boolean indicating whether or not to stop training.\n    \n        Raises:\n            ValueError: if \'should_trace\' is in `train_step_kwargs` but `logdir` is not.\n        """"""\n        \n    \n        start_time = time.time()\n        trace_run_options = None\n        run_metadata = None\n        if \'should_trace\' in train_step_kwargs:\n            if \'logdir\' not in train_step_kwargs:\n                raise ValueError(\'logdir must be present in train_step_kwargs when \'\n                                                 \'should_trace is present\')\n            if sess.run(train_step_kwargs[\'should_trace\']):\n                trace_run_options = config_pb2.RunOptions(\n                        trace_level=config_pb2.RunOptions.FULL_TRACE)\n                run_metadata = config_pb2.RunMetadata()\n    \n        total_loss, np_global_step = sess.run([train_op, global_step],\n                                                                                    options=trace_run_options,\n                                                                                    run_metadata=run_metadata)\n        time_elapsed = time.time() - start_time\n        \n#         self.debug_training(sess,global_step)\n        \n    \n        if run_metadata is not None:\n            tl = timeline.Timeline(run_metadata.step_stats)\n            trace = tl.generate_chrome_trace_format()\n            trace_filename = os.path.join(train_step_kwargs[\'logdir\'],\n                                                                        \'tf_trace-%d.json\' % np_global_step)\n            logging.info(\'Writing trace to %s\', trace_filename)\n            file_io.write_string_to_file(trace_filename, trace)\n            if \'summary_writer\' in train_step_kwargs:\n                train_step_kwargs[\'summary_writer\'].add_run_metadata(run_metadata,\n                                                                                                                         \'run_metadata-%d\' %\n                                                                                                                         np_global_step)\n    \n        if \'should_log\' in train_step_kwargs:\n            if sess.run(train_step_kwargs[\'should_log\']):\n                logging.info(\'global step %d: loss = %.4f (%.2f sec/step)\',\n                                         np_global_step, total_loss, time_elapsed)\n    \n        # TODO(nsilberman): figure out why we can\'t put this into sess.run. The\n        # issue right now is that the stop check depends on the global step. The\n        # increment of global step often happens via the train op, which used\n        # created using optimizer.apply_gradients.\n        #\n        # Since running `train_op` causes the global step to be incremented, one\n        # would expected that using a control dependency would allow the\n        # should_stop check to be run in the same session.run call:\n        #\n        #     with ops.control_dependencies([train_op]):\n        #         should_stop_op = ...\n        #\n        # However, this actually seems not to work on certain platforms.\n        if \'should_stop\' in train_step_kwargs:\n            should_stop = sess.run(train_step_kwargs[\'should_stop\'])\n        else:\n            should_stop = False\n    \n        return total_loss, should_stop\n    def __add_summaries(self,end_points,learning_rate,total_loss):\n        # Add summaries for end_points (activations).\n\n        for end_point in end_points:\n            x = end_points[end_point]\n            tf.summary.histogram(\'activations/\' + end_point, x)\n            tf.summary.scalar(\'sparsity/\' + end_point,\n                                            tf.nn.zero_fraction(x))\n        # Add summaries for losses and extra losses.\n        \n        tf.summary.scalar(\'total_loss\', total_loss)\n        for loss in tf.get_collection(\'EXTRA_LOSSES\'):\n            tf.summary.scalar(loss.op.name, loss)\n\n        # Add summaries for variables.\n        for variable in slim.get_model_variables():\n            tf.summary.histogram(variable.op.name, variable)\n\n        return\n    def __get_init_fn(self):\n        """"""Returns a function run by the chief worker to warm-start the training.\n    \n        Note that the init_fn is only run when initializing the model during the very\n        first global step.\n    \n        Returns:\n            An init function run by the supervisor.\n        """"""  \n        \n        if self.checkpoint_path is None:\n            return None\n    \n        # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n        # ignoring the checkpoint anyway.\n        \n        \n        if tf.train.latest_checkpoint(self.train_dir):\n            tf.logging.info(\n                    \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n                    % self.train_dir)\n            return None\n    \n        exclusions = []\n        if self.checkpoint_exclude_scopes:\n            exclusions = [scope.strip()\n                                        for scope in self.checkpoint_exclude_scopes.split(\',\')]\n    \n        # TODO(sguada) variables.filter_variables()\n        variables_to_restore = []\n        all_variables = slim.get_model_variables()\n        if self.fine_tune_vgg16:\n            global_step = slim.get_or_create_global_step()\n            all_variables.append(global_step)\n        for var in all_variables:\n            excluded = False\n            \n            for exclusion in exclusions:\n                if var.op.name.startswith(exclusion):\n                    excluded = True\n                    break\n            if not excluded:\n                variables_to_restore.append(var)\n    \n        if tf.gfile.IsDirectory(self.checkpoint_path):\n            checkpoint_path = tf.train.latest_checkpoint(self.checkpoint_path)\n        else:\n            checkpoint_path = self.checkpoint_path\n    \n        tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n    \n        return slim.assign_from_checkpoint_fn(\n                checkpoint_path,\n                variables_to_restore,\n                ignore_missing_vars=self.ignore_missing_vars)\n    \n    def run(self):\n        \n        #fine tune the new parameters\n        self.train_dir = \'./logs\'\n        \n        \n        self.checkpoint_path = \'checkpoints/vgg_16.ckpt\'\n        self.checkpoint_exclude_scopes = g_ssd_model.model_name\n        self.trainable_scopes = g_ssd_model.model_name\n        \n        \n        self.max_number_of_steps = 60000\n        self.log_every_n_steps = 10\n        \n        self.learning_rate = 0.001\n        self.learning_rate_decay_type = \'exponential\'\n        \n        \n        self.optimizer = \'adam\'\n        self.weight_decay = 0.0005 # for model regularization\n        \n        self.fine_tune_vgg16 = False\n        \n        if self.fine_tune_vgg16:  \n            #fine tune all parameters\n            self.train_dir = \'./logs/finetune\'\n            self.checkpoint_path =  \'./logs\'\n            self.checkpoint_exclude_scopes = None\n            self.trainable_scopes = ""{},vgg_16"".format(g_ssd_model.model_name)\n            self.max_number_of_steps = 250000\n            self.learning_rate=0.001\n\n       \n        \n        \n        self.__start_training()\n        return\n    \n    \n\n\nif __name__ == ""__main__"":   \n    obj= TrainModel()\n    obj.run()\n'"
widerface_eval.py,8,"b'#coding=utf-8\r\nimport os\r\nimport math\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport cv2\r\nfrom PIL import Image\r\nimport scipy.io as sio\r\nfrom tensorflow.contrib import slim\r\nimport matplotlib.pyplot as plt\r\nimport sys\r\nsys.path.append(\'../\')\r\n\r\nfrom preprocessing import ssd_vgg_preprocessing\r\nfrom utility import visualization\r\nfrom nets.ssd import g_ssd_model\r\nimport nets.np_methods as np_methods\r\n\r\n# TensorFlow session: grow memory when needed.\r\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\r\ngpu_options = tf.GPUOptions(allow_growth=True)\r\nconfig = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\r\nisess = tf.InteractiveSession(config=config)\r\n\r\n# Input placeholder.\r\ndata_format = \'NHWC\'\r\nimg_input = tf.placeholder(tf.uint8, shape=(None, None, 3))\r\n# Evaluation pre-processing: resize to SSD net shape.\r\nimage_pre, labels_pre, bboxes_pre, bbox_img = ssd_vgg_preprocessing.preprocess_for_eval(\r\n    img_input, None, None, data_format, resize=ssd_vgg_preprocessing.Resize.NONE)#WARP_RESIZE\r\nimage_4d = tf.expand_dims(image_pre, 0)\r\n\r\n# Define the PyramidBox model.\r\npredictions, localisations, _, end_points = g_ssd_model.get_model(image_4d)\r\n\r\n# Restore PyramidBox model.\r\nckpt_filename = tf.train.latest_checkpoint(\'logs/finetune/\')\r\n\r\nisess.run(tf.global_variables_initializer())\r\nsaver = tf.train.Saver()\r\nsaver.restore(isess, ckpt_filename)\r\n\r\n\r\n# Main image processing routine.\r\ndef process_image(img, select_threshold=0.05, nms_threshold=0.3):\r\n    # Run PyramidBox network.\r\n    h,w=img.shape[:2]\r\n    if h<w and h<640:\r\n        scale=640./h\r\n        h=640\r\n        w=int(w*scale)\r\n    elif h>=w and w<640:\r\n        scale=640./w\r\n        w=640\r\n        h=int(h*scale)\r\n    img_tmp=Image.fromarray(np.uint8(img))\r\n    resized_img=img_tmp.resize((w,h))    \r\n    net_shape=np.array(resized_img).shape[:2]\r\n    rimg, rpredictions, rlocalisations, rbbox_img,e_ps = isess.run([image_4d, predictions, localisations, bbox_img,end_points],feed_dict={img_input: resized_img})\r\n    \r\n    layer_shape=[e_ps[\'block3\'].shape[1:3],e_ps[\'block4\'].shape[1:3],e_ps[\'block5\'].shape[1:3],e_ps[\'block7\'].shape[1:3],e_ps[\'block8\'].shape[1:3],e_ps[\'block9\'].shape[1:3]]\r\n    \r\n    # SSD default anchor boxes.\r\n    ssd_anchors = g_ssd_model.ssd_anchors_all_layers(feat_shapes=layer_shape,img_shape=net_shape)\r\n    # Get classes and bboxes from the net outputs.\r\n    rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(\r\n            rpredictions, rlocalisations[0], ssd_anchors,\r\n            select_threshold=select_threshold, img_shape=net_shape, num_classes=2, decode=True)\r\n    \r\n    rbboxes = np_methods.bboxes_clip(rbbox_img, rbboxes)\r\n    rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=1000)\r\n\r\n    return rclasses, rscores, rbboxes\r\n\r\n\r\n# Test on some demo image and visualize output.\r\nwider_face_mat = sio.loadmat(\'eval/eval_tools/ground_truth/wider_face_val.mat\')\r\nevent_list = wider_face_mat[\'event_list\']\r\nfile_list = wider_face_mat[\'file_list\']\r\nsave_path = r\'eval/pyramidbox_val/\'\r\n\r\nfor ind, event in enumerate(event_list):\r\n    filelist = file_list[ind][0]\r\n    im_dir = event[0][0]\r\n    if not os.path.exists(save_path + im_dir): os.makedirs(save_path + im_dir)\r\n\r\n    for num, file in enumerate(filelist):\r\n        im_name = file[0][0]\r\n        impath = \'datasets/widerface/WIDER_val/images/\'+\'%s/%s.jpg\' % (im_dir,im_name)\r\n        if os.path.exists(save_path + im_dir + \'/\' + im_name + \'.txt\'):\r\n            continue\r\n        img = Image.open(impath)\r\n        h,w=np.array(img).shape[:2]\r\n        rclasses1, rscores1, rbboxes1=process_image(np.array(img))\r\n\r\n        img_flip=img.transpose(Image.FLIP_LEFT_RIGHT) \r\n        rclasses2, rscores2, rbboxes2=process_image(np.array(img_flip))\r\n        temp=rbboxes2[:,1].copy()\r\n        rbboxes2[:,1]=1.-rbboxes2[:,3]\r\n        rbboxes2[:,3]=1.-temp\r\n\r\n        flag1=0\r\n        if h*w*4<2048*2048:\r\n            img_scale1=img.resize((w*2,h*2))\r\n            rclasses3, rscores3, rbboxes3=process_image(np.array(img_scale1))\r\n            index = np.where(np.minimum((rbboxes3[:,2]-rbboxes3[:,0])*h,(rbboxes3[:,3]-rbboxes3[:,1])*w)<=96)[0] # only detect small face\r\n            rclasses3=rclasses3[index]\r\n            rscores3=rscores3[index]\r\n            rbboxes3=rbboxes3[index]\r\n            flag1=1\r\n        flag2=0    \r\n        if h*w/4>960*960:\r\n            img_scale2=img.resize((int(w*0.5),int(h*0.5)))\r\n            rclasses4, rscores4, rbboxes4=process_image(np.array(img_scale2))\r\n            index = np.where(np.minimum((rbboxes4[:,2]-rbboxes4[:,0])*h,(rbboxes4[:,3]-rbboxes4[:,1])*w)>96)[0] # only detect large face\r\n            rclasses4=rclasses4[index]\r\n            rscores4=rscores4[index]\r\n            rbboxes4=rbboxes4[index]\r\n            flag2=1\r\n\r\n        if flag1==1 and flag2==1:\r\n            rclasses=np.concatenate((rclasses1,rclasses2,rclasses3,rclasses4))\r\n            rscores=np.concatenate((rscores1,rscores2,rscores3,rscores4))\r\n            rbboxes=np.concatenate((rbboxes1,rbboxes2,rbboxes3,rbboxes4))\r\n        elif flag1==1 and flag2==0:\r\n            rclasses=np.concatenate((rclasses1,rclasses2,rclasses3))\r\n            rscores=np.concatenate((rscores1,rscores2,rscores3))\r\n            rbboxes=np.concatenate((rbboxes1,rbboxes2,rbboxes3))\r\n        elif flag2==1 and flag1==0:\r\n            rclasses=np.concatenate((rclasses1,rclasses2,rclasses4))\r\n            rscores=np.concatenate((rscores1,rscores2,rscores4))\r\n            rbboxes=np.concatenate((rbboxes1,rbboxes2,rbboxes4))\r\n        else:\r\n            rclasses=np.concatenate((rclasses1,rclasses2))\r\n            rscores=np.concatenate((rscores1,rscores2))\r\n            rbboxes=np.concatenate((rbboxes1,rbboxes2))\r\n\r\n        rclasses, rscores, rbboxes = np_methods.bboxes_nms_fast(rclasses, rscores, rbboxes, nms_threshold=0.1)\r\n        rbbox_img=[0.,0.,1.,1.]\r\n        rbboxes = np_methods.bboxes_resize(rbbox_img, rbboxes)\r\n        \r\n        f = open(save_path + im_dir + \'/\' + im_name + \'.txt\', \'w\')\r\n        f.write(\'{:s}\\n\'.format(\'%s/%s.jpg\' % (im_dir,im_name)))\r\n        f.write(\'{:d}\\n\'.format(len(rbboxes)))\r\n        for ii in range(len(rbboxes)):\r\n            y1,x1,y2,x2 = rbboxes[ii]\r\n            y1*=h\r\n            x1*=w\r\n            y2*=h\r\n            x2*=w\r\n            f.write(\'{:.1f} {:.1f} {:.1f} {:.1f} {:.3f}\\n\'.format(int(x1),int(y1),int(x2-x1+1),int(y2-y1+1),rscores[ii]))\r\n        f.close()\r\n        print(\'event:%d num:%d\' % (ind + 1, num + 1))\r\n\r\n\r\n    \r\n        \r\n'"
datasets/__init__.py,0,b'\n'
datasets/dataset_utils.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(value):\n    """"""Wrapper for inserting int64 features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef float_feature(value):\n    """"""Wrapper for inserting float features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef bytes_feature(value):\n    """"""Wrapper for inserting bytes features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n    return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n    }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n    """"""Downloads the `tarball_url` and uncompresses it locally.\n\n    Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n    """"""\n    filename = tarball_url.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n\n    def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n            filename, float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n    """"""Writes a file with the list of class names.\n\n    Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n    """"""\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'w\') as f:\n        for label in labels_to_class_names:\n            class_name = labels_to_class_names[label]\n            f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n    """"""Specifies whether or not the dataset directory contains a label map file.\n\n    Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n    Returns:\n    `True` if the labels file exists and `False` otherwise.\n    """"""\n    return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n    """"""Reads the labels file and returns a mapping from ID to class name.\n\n    Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n    Returns:\n    A map from a label (integer) to class name.\n    """"""\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'rb\') as f:\n        lines = f.read()\n    lines = lines.split(b\'\\n\')\n    lines = filter(None, lines)\n\n    labels_to_class_names = {}\n    for line in lines:\n        index = line.index(b\':\')\n        labels_to_class_names[int(line[:index])] = line[index+1:]\n    return labels_to_class_names\n'"
datasets/pascalvoc_datasets.py,16,"b'\n""""""Provides data for the Pascal VOC Dataset (images + annotations).\n""""""\nimport os\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\nVOC_LABELS = {\n    \'none\': (0, \'Background\'),\n    \'face\': (1, \'face\') ,\n}\nITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'shape\': \'Shape of the image\',\n    \'object/bbox\': \'A list of bounding boxes, one per each object.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\nDATASET_SIZE = {\n    \'wider_face_val\': 3226,\n    \'wider_face_test\': 5011,\n    \'wider_face_train\': 12880\n}\nNUM_CLASSES = 1\ndef get_dataset_info(data_sources, num_samples):\n    """"""Gets a dataset tuple with instructions for reading Pascal VOC dataset.\n\n    Args:\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a \'%s\' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    """"""\n   \n    # Allowing None in the signature so that dataset_factory can use the default.\n    \n    reader = tf.TFRecordReader\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/filename\': tf.FixedLenFeature((), tf.string, default_value=\'000000\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/shape\': tf.FixedLenFeature([3], tf.int64),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/label\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/difficult\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/truncated\': tf.VarLenFeature(dtype=tf.int64)\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'shape\': slim.tfexample_decoder.Tensor(\'image/shape\'),\n        \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n        \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/bbox/label\'),\n        \'object/difficult\': slim.tfexample_decoder.Tensor(\'image/object/bbox/difficult\'),\n        \'object/truncated\': slim.tfexample_decoder.Tensor(\'image/object/bbox/truncated\'),\n        \'format\': slim.tfexample_decoder.Tensor(\'image/format\'),\n        \'filename\': slim.tfexample_decoder.Tensor(\'image/filename\')\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(\n        keys_to_features, items_to_handlers)\n\n    labels_to_names = None\n\n\n    return slim.dataset.Dataset(\n            data_sources=data_sources,\n            reader=reader,\n            decoder=decoder,\n            num_samples=num_samples,\n            items_to_descriptions=ITEMS_TO_DESCRIPTIONS,\n            num_classes=NUM_CLASSES,\n            labels_to_names=labels_to_names)\n\n'"
datasets/pascalvoc_to_tfrecords.py,4,"b'\n""""""Converts Pascal VOC data to TFRecords file format with Example protos.\n\nThe raw Pascal VOC data set is expected to reside in JPEG files located in the\ndirectory \'JPEGImages\'. Similarly, bounding box annotations are supposed to be\nstored in the \'Annotation directory\'\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\nEach validation TFRecord file contains ~500 records. Each training TFREcord\nfile contains ~1000 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n    image/encoded: string containing JPEG encoded image in RGB colorspace\n    image/height: integer, image height in pixels\n    image/width: integer, image width in pixels\n    image/channels: integer, specifying the number of channels, always 3\n    image/format: string, specifying the format, always\'JPEG\'\n\n\n    image/object/bbox/xmin: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/xmax: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/ymin: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/ymax: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/label: list of integer specifying the classification index.\n    image/object/bbox/label_text: list of string descriptions.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n""""""\nimport os\nimport sys\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\nimport xml.etree.ElementTree as ET\n\nfrom dataset_utils import int64_feature, float_feature, bytes_feature\nfrom pascalvoc_datasets import VOC_LABELS\nimport math\n\nDIRECTORY_ANNOTATIONS = \'Annotations/\'\nDIRECTORY_IMAGES = \'JPEGImages/\'\n\n\ndef _process_image(directory, name):\n    """"""Process a image and annotation file.\n\n    Args:\n      filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n      coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    Returns:\n      image_buffer: string, JPEG encoding of RGB image.\n      height: integer, image height in pixels.\n      width: integer, image width in pixels.\n    """"""\n    # Read the image file.\n    #filename = directory + DIRECTORY_IMAGES + name + \'.jpg\'\n    filename = os.path.join(directory, DIRECTORY_IMAGES, \'{}.jpg\'.format(name))\n    image_data = tf.gfile.FastGFile(filename, \'rb\').read()\n\n    # Read the XML annotation file.\n    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + \'.xml\')\n    tree = ET.parse(filename)\n    root = tree.getroot()\n\n    # Image shape.\n    size = root.find(\'size\')\n    shape = [int(size.find(\'height\').text),\n             int(size.find(\'width\').text),\n             int(size.find(\'depth\').text)]\n    # Find annotations.\n    bboxes = []\n    labels = []\n    labels_text = []\n    difficult = []\n    truncated = []\n    for obj in root.findall(\'object\'):\n        label = obj.find(\'name\').text\n        labels.append(int(VOC_LABELS[label][0]))\n        if(int(VOC_LABELS[label][0]) == 0):\n            print(filename)\n            raise \n        \n        labels_text.append(label.encode(\'ascii\'))\n\n        if obj.find(\'difficult\'):\n            difficult.append(int(obj.find(\'difficult\').text))\n        else:\n            difficult.append(0)\n        if obj.find(\'truncated\'):\n            truncated.append(int(obj.find(\'truncated\').text))\n        else:\n            truncated.append(0)\n\n        bbox = obj.find(\'bndbox\')\n        bboxes.append((float(bbox.find(\'ymin\').text) / shape[0],\n                       float(bbox.find(\'xmin\').text) / shape[1],\n                       float(bbox.find(\'ymax\').text) / shape[0],\n                       float(bbox.find(\'xmax\').text) / shape[1]\n                       ))\n    return image_data, shape, bboxes, labels, labels_text, difficult, truncated\n\n\ndef _convert_to_example(image_data, labels, labels_text, bboxes, shape,\n                        difficult, truncated,name):\n    """"""Build an Example proto for an image example.\n\n    Args:\n      image_data: string, JPEG encoding of RGB image;\n      labels: list of integers, identifier for the ground truth;\n      labels_text: list of strings, human-readable labels;\n      bboxes: list of bounding boxes; each box is a list of integers;\n          specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong\n          to the same label as the image label.\n      shape: 3 integers, image shapes in pixels.\n    Returns:\n      Example proto\n    """"""\n    xmin = []\n    ymin = []\n    xmax = []\n    ymax = []\n    for b in bboxes:\n        assert len(b) == 4\n        # pylint: disable=expression-not-assigned\n        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n        # pylint: enable=expression-not-assigned\n\n    image_format = b\'JPEG\'\n    example = tf.train.Example(features=tf.train.Features(feature={\n            \'image/height\': int64_feature(shape[0]),\n            \'image/width\': int64_feature(shape[1]),\n            \'image/channels\': int64_feature(shape[2]),\n            \'image/shape\': int64_feature(shape),\n            \'image/object/bbox/xmin\': float_feature(xmin),\n            \'image/object/bbox/xmax\': float_feature(xmax),\n            \'image/object/bbox/ymin\': float_feature(ymin),\n            \'image/object/bbox/ymax\': float_feature(ymax),\n            \'image/object/bbox/label\': int64_feature(labels),\n            \'image/object/bbox/label_text\': bytes_feature(labels_text),\n            \'image/object/bbox/difficult\': int64_feature(difficult),\n            \'image/object/bbox/truncated\': int64_feature(truncated),\n            \'image/format\': bytes_feature(image_format),\n            \'image/filename\': bytes_feature(name.encode(\'utf-8\')),\n            \'image/encoded\': bytes_feature(image_data)}))\n    return example\n\n\ndef _add_to_tfrecord(dataset_dir, name, tfrecord_writer):\n    """"""Loads data from image and annotations files and add them to a TFRecord.\n\n    Args:\n      dataset_dir: Dataset directory;\n      name: Image name to add to the TFRecord;\n      tfrecord_writer: The TFRecord writer to use for writing.\n    """"""\n    image_data, shape, bboxes, labels, labels_text, difficult, truncated = \\\n        _process_image(dataset_dir, name)\n    example = _convert_to_example(image_data, labels, labels_text,\n                                  bboxes, shape, difficult, truncated,name)\n    tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(output_dir, name):\n    return \'%s/%s.tfrecord\' % (output_dir, name)\n\ndef _get_dataset_filename(dataset_dir, name, shard_id, num_shard, records_num):\n    output_filename = \'%s_%05d-of-%05d-total%05d.tfrecord\' % (name, shard_id + 1, num_shard,records_num)\n    return os.path.join(dataset_dir, output_filename)\n\ndef run(dataset_dir, output_dir, name, shuffling=False):\n    """"""Runs the conversion operation.\n\n    Args:\n      dataset_dir: The dataset directory where the dataset is stored.\n      output_dir: Output directory.\n    """"""\n    \n\n    # Dataset filenames, and shuffling.\n    path = os.path.join(dataset_dir, DIRECTORY_ANNOTATIONS)\n    if not tf.gfile.Exists(path):\n        raise Exception(""{} does not exist"".format(path))\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    filenames = sorted(os.listdir(path))\n    if shuffling:\n        random.seed(12345)\n        random.shuffle(filenames)\n\n    # Process dataset files.\n    num_per_shard = 200\n    num_shard = int(math.ceil(len(filenames) / float(num_per_shard)))\n    \n    for shard_id in range(num_shard):\n        start_ndx = shard_id * num_per_shard\n        end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n        records_num = end_ndx - start_ndx\n        tf_filename = _get_dataset_filename(output_dir, name, shard_id, num_shard, records_num)\n        with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:\n            for i in range(start_ndx, end_ndx):\n                filename = filenames[i]\n                \n                print(\'Converting image %d/%d %s shard %d\' % (i+1, len(filenames), filename[:-4], shard_id+1))\n                #save the file to tfrecords\n                _add_to_tfrecord(dataset_dir, filename[:-4], tfrecord_writer)\n\n    # Finally, write the labels file:\n    # labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n    # dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n    print(\'\\nFinished converting the Pascal VOC dataset!\')\n    \n    \nif __name__ == ""__main__"": \n    \n    dataset_dir = \'datasets/widerface/\'\n    output_dir = ""tfrecords/""\n    name=\'voc_train_2007\'\n    \n    run(dataset_dir, output_dir, name=name, shuffling=True)\n    \n    \n    \n    \n    \n'"
nets/__init__.py,0,b''
nets/custom_layers.py,20,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implement some custom layers, not provided by TensorFlow.\n\nTrying to follow as much as possible the style/standards used in\ntf.contrib.layers\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom tensorflow.contrib.framework.python.ops import variables\nfrom tensorflow.contrib.layers.python.layers import utils\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\n\n\ndef abs_smooth(x):\n    """"""Smoothed absolute function. Useful to compute an L1 smooth error.\n\n    Define as:\n        x^2 / 2         if abs(x) < 1\n        abs(x) - 0.5    if abs(x) > 1\n    We use here a differentiable definition using min(x) and abs(x). Clearly\n    not optimal, but good enough for our purpose!\n    """"""\n    absx = tf.abs(x)\n    minx = tf.minimum(absx, 1)\n    r = 0.5 * ((absx - 1) * minx + absx)\n    return r\n\ndef abs_smooth_2(x):\n    """"""Smoothed absolute function. Useful to compute an L1 smooth error.\n\n    Define as:\n        x^2 / 2         if abs(x) < 1\n        abs(x) - 0.5    if abs(x) > 1\n    an implementation that strictly stick to the formula\n    """"""\n    absx = tf.abs(x)\n    r = array_ops.where(absx < 1, math_ops.square(x)/2.0, absx-0.5)\n    return r\n\n\n@add_arg_scope\ndef l2_normalization(\n        inputs,\n        scaling=False,\n        scale_initializer=init_ops.ones_initializer(),\n        reuse=None,\n        variables_collections=None,\n        outputs_collections=None,\n        data_format=\'NHWC\',\n        trainable=True,\n        scope=None):\n    """"""Implement L2 normalization on every feature (i.e. spatial normalization).\n\n    Should be extended in some near future to other dimensions, providing a more\n    flexible normalization framework.\n\n    Args:\n      inputs: a 4-D tensor with dimensions [batch_size, height, width, channels].\n      scaling: whether or not to add a post scaling operation along the dimensions\n        which have been normalized.\n      scale_initializer: An initializer for the weights.\n      reuse: whether or not the layer and its variables should be reused. To be\n        able to reuse the layer scope must be given.\n      variables_collections: optional list of collections for all the variables or\n        a dictionary containing a different list of collection per variable.\n      outputs_collections: collection to add the outputs.\n      data_format:  NHWC or NCHW data format.\n      trainable: If `True` also add variables to the graph collection\n        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n      scope: Optional scope for `variable_scope`.\n    Returns:\n      A `Tensor` representing the output of the operation.\n    """"""\n\n    with variable_scope.variable_scope(\n            scope, \'L2Normalization\', [inputs], reuse=reuse) as sc:\n        inputs_shape = inputs.get_shape()\n        inputs_rank = inputs_shape.ndims\n        dtype = inputs.dtype.base_dtype\n        if data_format == \'NHWC\':\n            # norm_dim = tf.range(1, inputs_rank-1)\n            norm_dim = tf.range(inputs_rank-1, inputs_rank)\n            params_shape = inputs_shape[-1:]\n        elif data_format == \'NCHW\':\n            # norm_dim = tf.range(2, inputs_rank)\n            norm_dim = tf.range(1, 2)\n            params_shape = (inputs_shape[1])\n\n        # Normalize along spatial dimensions.\n        outputs = nn.l2_normalize(inputs, norm_dim, epsilon=1e-12)\n        # Additional scaling.\n        if scaling:\n            scale_collections = utils.get_variable_collections(\n                variables_collections, \'scale\')\n            scale = variables.model_variable(\'gamma\',\n                                             shape=params_shape,\n                                             dtype=dtype,\n                                             initializer=scale_initializer,\n                                             collections=scale_collections,\n                                             trainable=trainable)\n            if data_format == \'NHWC\':\n                outputs = tf.multiply(outputs, scale)\n            elif data_format == \'NCHW\':\n                scale = tf.expand_dims(scale, axis=-1)\n                scale = tf.expand_dims(scale, axis=-1)\n                outputs = tf.multiply(outputs, scale)\n                # outputs = tf.transpose(outputs, perm=(0, 2, 3, 1))\n\n        return utils.collect_named_outputs(outputs_collections,\n                                           sc.original_name_scope, outputs)\n\n\n@add_arg_scope\ndef pad2d(inputs,\n          pad=(0, 0),\n          mode=\'CONSTANT\',\n          data_format=\'NHWC\',\n          trainable=True,\n          scope=None):\n    """"""2D Padding layer, adding a symmetric padding to H and W dimensions.\n\n    Aims to mimic padding in Caffe and MXNet, helping the port of models to\n    TensorFlow. Tries to follow the naming convention of `tf.contrib.layers`.\n\n    Args:\n      inputs: 4D input Tensor;\n      pad: 2-Tuple with padding values for H and W dimensions;\n      mode: Padding mode. C.f. `tf.pad`\n      data_format:  NHWC or NCHW data format.\n    """"""\n    with tf.name_scope(scope, \'pad2d\', [inputs]):\n        # Padding shape.\n        if data_format == \'NHWC\':\n            paddings = [[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]]\n        elif data_format == \'NCHW\':\n            paddings = [[0, 0], [0, 0], [pad[0], pad[0]], [pad[1], pad[1]]]\n        net = tf.pad(inputs, paddings, mode=mode)\n        return net\n\n\n@add_arg_scope\ndef channel_to_last(inputs,\n                    data_format=\'NHWC\',\n                    scope=None):\n    """"""Move the channel axis to the last dimension. Allows to\n    provide a single output format whatever the input data format.\n\n    Args:\n      inputs: Input Tensor;\n      data_format: NHWC or NCHW.\n    Return:\n      Input in NHWC format.\n    """"""\n    with tf.name_scope(scope, \'channel_to_last\', [inputs]):\n        if data_format == \'NHWC\':\n            net = inputs\n        elif data_format == \'NCHW\':\n            net = tf.transpose(inputs, perm=(0, 2, 3, 1))\n        return net\n'"
nets/np_methods.py,0,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional Numpy methods. Big mess of many things!\n""""""\nimport numpy as np\n\n\n# =========================================================================== #\n# Numpy implementations of SSD boxes functions.\n# =========================================================================== #\ndef ssd_bboxes_decode(feat_localizations,\n                      anchor_bboxes,\n                      prior_scaling=[0.1, 0.1, 0.2, 0.2]):\n    """"""Compute the relative bounding boxes from the layer features and\n    reference anchor bounding boxes.\n\n    Return:\n      numpy array Nx4: ymin, xmin, ymax, xmax\n    """"""\n    # Reshape for easier broadcasting.\n    l_shape = feat_localizations.shape\n    feat_localizations = np.reshape(feat_localizations,\n                                    (-1,  l_shape[-1]))\n    yref, xref, href, wref = anchor_bboxes\n    xref = np.reshape(xref, [-1])\n    yref = np.reshape(yref, [-1])\n    \n    # Compute center, height and width\n    cx = feat_localizations[:,  0] * wref * prior_scaling[0] + xref\n    cy = feat_localizations[:,  1] * href * prior_scaling[1] + yref\n    w = wref * np.exp(feat_localizations[:,  2] * prior_scaling[2])\n    h = href * np.exp(feat_localizations[:,  3] * prior_scaling[3])\n    # bboxes: ymin, xmin, xmax, ymax.\n    bboxes = np.zeros_like(feat_localizations)\n    bboxes[:,  0] = cy - h / 2.\n    bboxes[:,  1] = cx - w / 2.\n    bboxes[:,  2] = cy + h / 2.\n    bboxes[:,  3] = cx + w / 2.\n    # Back to original shape.\n    bboxes = np.reshape(bboxes, l_shape)\n    return bboxes\n\n\ndef ssd_bboxes_select_layer(predictions_layer,\n                            localizations_layer,\n                            anchors_layer,\n                            select_threshold=0.5,\n                            img_shape=(300, 300),\n                            num_classes=2,\n                            decode=True):\n    """"""Extract classes, scores and bounding boxes from features in one layer.\n\n    Return:\n      classes, scores, bboxes: Numpy arrays...\n    """"""\n    # First decode localizations features if necessary.\n    if decode:\n        localizations_layer = ssd_bboxes_decode(localizations_layer, anchors_layer)\n\n    # Reshape features to: Batches x N x N_labels | 4.\n    p_shape = predictions_layer.shape\n    batch_size = p_shape[0] if len(p_shape) == 4 else 1\n    predictions_layer = np.reshape(predictions_layer,\n                                   (batch_size, -1, p_shape[-1]))\n    l_shape = localizations_layer.shape\n    localizations_layer = np.reshape(localizations_layer,\n                                     (batch_size, -1, l_shape[-1]))\n\n    # Boxes selection: use threshold or score > no-label criteria.\n    if select_threshold is None or select_threshold == 0:\n        # Class prediction and scores: assign 0. to 0-class\n        classes = np.argmax(predictions_layer, axis=2)\n        scores = np.amax(predictions_layer, axis=2)\n        mask = (classes > 0)\n        classes = classes[mask]\n        scores = scores[mask]\n        bboxes = localizations_layer[mask]\n    else:\n        sub_predictions = predictions_layer[:, :, 1:]\n        #print(np.mean(predictions_layer[0,:,1]))\n        idxes = np.where(sub_predictions > select_threshold)\n        classes = idxes[-1]+1\n        scores = sub_predictions[idxes]\n        bboxes = localizations_layer[idxes[:-1]]\n    return classes, scores, bboxes\n\n\ndef ssd_bboxes_select(predictions_net,\n                      localizations_net,\n                      anchors_net,\n                      select_threshold=0.5,\n                      img_shape=(300, 300),\n                      num_classes=2,\n                      decode=True):\n    """"""Extract classes, scores and bounding boxes from network output layers.\n\n    Return:\n      classes, scores, bboxes: Numpy arrays...\n    """"""\n    l_classes = []\n    l_scores = []\n    l_bboxes = []\n\n    for i in range(len(predictions_net)):#len(predictions_net)\n        classes, scores, bboxes = ssd_bboxes_select_layer(\n            predictions_net[i], localizations_net[i], anchors_net[i],\n            select_threshold, img_shape, num_classes, decode)\n        l_classes.append(classes)\n        l_scores.append(scores)\n        l_bboxes.append(bboxes)\n\n    classes = np.concatenate(l_classes, 0)\n    scores = np.concatenate(l_scores, 0)\n    bboxes = np.concatenate(l_bboxes, 0)\n    return classes, scores, bboxes\n\n\n# =========================================================================== #\n# Common functions for bboxes handling and selection.\n# =========================================================================== #\ndef bboxes_sort(classes, scores, bboxes, top_k=400):\n    """"""Sort bounding boxes by decreasing order and keep only the top_k\n    """"""\n    idxes = np.argsort(-scores)\n    classes = classes[idxes][:top_k]\n    scores = scores[idxes][:top_k]\n    bboxes = bboxes[idxes][:top_k]\n    return classes, scores, bboxes\n\n\ndef bboxes_clip(bbox_ref, bboxes):\n    """"""Clip bounding boxes with respect to reference bbox.\n    """"""\n    bboxes = np.copy(bboxes)\n    bboxes = np.transpose(bboxes)\n    bbox_ref = np.transpose(bbox_ref)\n    bboxes[0] = np.maximum(bboxes[0], bbox_ref[0])\n    bboxes[1] = np.maximum(bboxes[1], bbox_ref[1])\n    bboxes[2] = np.minimum(bboxes[2], bbox_ref[2])\n    bboxes[3] = np.minimum(bboxes[3], bbox_ref[3])\n    bboxes = np.transpose(bboxes)\n    return bboxes\n\n\ndef bboxes_resize(bbox_ref, bboxes):\n    """"""Resize bounding boxes based on a reference bounding box,\n    assuming that the latter is [0, 0, 1, 1] after transform.\n    """"""\n    bboxes = np.copy(bboxes)\n    # Translate.\n    bboxes[:, 0] -= bbox_ref[0]\n    bboxes[:, 1] -= bbox_ref[1]\n    bboxes[:, 2] -= bbox_ref[0]\n    bboxes[:, 3] -= bbox_ref[1]\n    # Resize.\n    resize = [bbox_ref[2] - bbox_ref[0], bbox_ref[3] - bbox_ref[1]]\n    bboxes[:, 0] /= resize[0]\n    bboxes[:, 1] /= resize[1]\n    bboxes[:, 2] /= resize[0]\n    bboxes[:, 3] /= resize[1]\n    return bboxes\n\n\ndef bboxes_jaccard(bboxes1, bboxes2):\n    """"""Computing jaccard index between bboxes1 and bboxes2.\n    Note: bboxes1 and bboxes2 can be multi-dimensional, but should broacastable.\n    """"""\n    bboxes1 = np.transpose(bboxes1)\n    bboxes2 = np.transpose(bboxes2)\n    # Intersection bbox and volume.\n    int_ymin = np.maximum(bboxes1[0], bboxes2[0])\n    int_xmin = np.maximum(bboxes1[1], bboxes2[1])\n    int_ymax = np.minimum(bboxes1[2], bboxes2[2])\n    int_xmax = np.minimum(bboxes1[3], bboxes2[3])\n\n    int_h = np.maximum(int_ymax - int_ymin, 0.)\n    int_w = np.maximum(int_xmax - int_xmin, 0.)\n    int_vol = int_h * int_w\n    # Union volume.\n    vol1 = (bboxes1[2] - bboxes1[0]) * (bboxes1[3] - bboxes1[1])\n    vol2 = (bboxes2[2] - bboxes2[0]) * (bboxes2[3] - bboxes2[1])\n    jaccard = int_vol / (vol1 + vol2 - int_vol)\n    return jaccard\n\n\ndef bboxes_intersection(bboxes_ref, bboxes2):\n    """"""Computing jaccard index between bboxes1 and bboxes2.\n    Note: bboxes1 and bboxes2 can be multi-dimensional, but should broacastable.\n    """"""\n    bboxes_ref = np.transpose(bboxes_ref)\n    bboxes2 = np.transpose(bboxes2)\n    # Intersection bbox and volume.\n    int_ymin = np.maximum(bboxes_ref[0], bboxes2[0])\n    int_xmin = np.maximum(bboxes_ref[1], bboxes2[1])\n    int_ymax = np.minimum(bboxes_ref[2], bboxes2[2])\n    int_xmax = np.minimum(bboxes_ref[3], bboxes2[3])\n\n    int_h = np.maximum(int_ymax - int_ymin, 0.)\n    int_w = np.maximum(int_xmax - int_xmin, 0.)\n    int_vol = int_h * int_w\n    # Union volume.\n    vol = (bboxes_ref[2] - bboxes_ref[0]) * (bboxes_ref[3] - bboxes_ref[1])\n    score = int_vol / vol\n    return score\n\n\ndef bboxes_nms(classes, scores, bboxes, nms_threshold=0.45):\n    """"""Apply non-maximum selection to bounding boxes.\n    """"""\n    keep_bboxes = np.ones(scores.shape, dtype=np.bool)\n    for i in range(scores.size-1):\n        if keep_bboxes[i]:\n            # Computer overlap with bboxes which are following.\n            overlap = bboxes_jaccard(bboxes[i], bboxes[(i+1):])\n            # Overlap threshold for keeping + checking part of the same class\n            keep_overlap = np.logical_or(overlap < nms_threshold, classes[(i+1):] != classes[i])\n            keep_bboxes[(i+1):] = np.logical_and(keep_bboxes[(i+1):], keep_overlap)\n\n    idxes = np.where(keep_bboxes)\n    return classes[idxes], scores[idxes], bboxes[idxes]\n\n\ndef bboxes_nms_fast(classes, scores, bboxes, nms_threshold=0.45):\n    """"""Apply non-maximum selection to bounding boxes.\n    """"""\n    y1 = bboxes[:, 0]  \n    x1 = bboxes[:, 1]  \n    y2 = bboxes[:, 2]  \n    x2 = bboxes[:, 3]    \n\n    areas = (x2 - x1 ) * (y2 - y1 )  \n\n    order = scores.argsort()[::-1]  \n\n    keep = []  \n    while order.size > 0:  \n        i = order[0]  \n        keep.append(i)  \n        xx1 = np.maximum(x1[i], x1[order[1:]])  \n        yy1 = np.maximum(y1[i], y1[order[1:]])  \n        xx2 = np.minimum(x2[i], x2[order[1:]])  \n        yy2 = np.minimum(y2[i], y2[order[1:]])  \n\n        w = np.maximum(0.0, xx2 - xx1)  \n        h = np.maximum(0.0, yy2 - yy1)  \n        inter = w * h  \n        ovr = inter / (areas[i] + areas[order[1:]] - inter)  \n        inds = np.where(ovr <= nms_threshold)[0]  \n        order = order[inds + 1]  \n    keep = keep[0:200]\n    return classes[keep], scores[keep], bboxes[keep]\n\n\n\n\n'"
nets/ssd.py,135,"b'\nimport tensorflow as tf\n\nimport tensorflow.contrib.slim as slim\n\nimport numpy as np\n\nimport math\nfrom numpy import newaxis\nfrom nets import custom_layers\nimport tf_extended as tfe\nfrom nets import ssd_common\nfrom tensorflow.python.ops import array_ops\n\n\nclass PyramidBoxModel():\n    """"""Implementation of the PyramidBox vgg-based 640 network.\n\n    The default features layers with 640x640 image input are:\n      block3 ==> 160 x 160\n      block4 ==> 80 x 80\n      block5 ==> 40 x 40\n      block7 ==> 20 x 20\n      block8 ==> 10 x 10\n      block9 ==> 5 x 5\n    The default image size used to train this network is 640x640.\n    """"""\n    def __init__(self):\n        \n        self.img_shape=(640, 640)\n        self.num_classes=2\n        self.no_annotation_label=2\n        self.feat_layers=[\'block3\', \'block4\', \'block5\', \'block7\', \'block8\', \'block9\']\n        self.feat_shapes=[(160, 160), (80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n        self.layer_shape=[]\n        self.anchor_sizes=[16.,32.,64.,128.,256.,512.]\n        self.anchor_ratios=1. \n        self.anchor_steps=[4, 8, 16, 32, 64, 128] \n        self.anchor_offset=0.5\n        #Scaling of encoded coordinates.\n        #For the scaling, the idea is try to scale such that all error terms (classification + position + size) \n        #have roughly the same scaling. Otherwise, the training would tend to over-optimise one component and not the others.\n        self.prior_scaling=[0.1, 0.1, 0.2, 0.2] \n        \n        #normalization for conv4 3\n        self.normalizations=[1, 1, 1, -1, -1, -1]\n        \n        #thresholding for ignoring ""no annotation label""\n        self.ignore_threshold = 0.5 \n\n        self.np__anchors = None\n\n        self.np_anchors_minmax = None\n        self.model_name = \'ssd_300_vgg\'\n        \n        #post processing\n        self.select_threshold = 0.01\n        self.nms_threshold = 0.1\n        self.select_top_k = 600\n        self.keep_top_k = 400\n        \n        return\n    def __dropout(self,net):\n        net_shape = net.get_shape().as_list() \n        noise_shape = [net_shape[0],1,1,net_shape[-1]]\n        return slim.dropout(net, noise_shape=noise_shape)\n    def __additional_ssd_block(self, end_points,channels, net, is_training=False):\n        # Additional SSD blocks.\n        # Block 6: let\'s dilate the hell out of it!\n        \n        net = slim.conv2d(net, 1024, [3, 3], rate=6, scope=\'conv6\')\n        #net = slim.batch_norm(net)\n        #net = self.__dropout(net)\n        end_points[\'block6\'] = net\n        # Block 7: 1x1 conv. Because the fuck.\n        net = slim.conv2d(net, 1024, [1, 1], scope=\'conv7\')\n        #net = slim.batch_norm(net)\n        #net = self.__dropout(net)\n        end_points[\'block7\'] = net\n        channels[\'block7\']=1024\n        self.layer_shape.append(tfe.get_shape(net)[1:3])\n\n        # Block 8/9: 1x1 and 3x3 convolutions stride 2 (except lasts).\n        end_point = \'block8\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 256, [1, 1], scope=\'conv1x1\')\n            #net = slim.batch_norm(net)\n            #net = self.__dropout(net)\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 512, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n            #net = slim.batch_norm(net)\n            #net = self.__dropout(net)\n        end_points[end_point] = net\n        channels[end_point]=512\n        self.layer_shape.append(tfe.get_shape(net)[1:3])\n        end_point = \'block9\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            #net = slim.batch_norm(net)\n            #net = self.__dropout(net)\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 256, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n            #net = slim.batch_norm(net)\n            #net = self.__dropout(net)\n        end_points[end_point] = net\n        channels[end_point]=256\n        self.layer_shape.append(tfe.get_shape(net)[1:3])\n\n        # Prediction and localisations layers.\n        predictions = []\n\n        logits, localisations = self.ssd_multibox_layer(end_points,\n                                  channels,\n                                  self.feat_layers,\n                                  self.normalizations,\n                                  is_training=is_training)\n        \n        if is_training==True:\n            return localisations, logits, end_points\n        else:\n            predictions = []\n            for l in range(len(logits[0])):\n                predictions.append(slim.softmax(logits[0][l]))\n            return predictions, localisations, logits, end_points\n       \n    \n    def __arg_scope(self, weight_decay=0.0005, data_format=\'NHWC\'):\n        """"""Defines the VGG arg scope.\n    \n        Args:\n          weight_decay: The l2 regularization coefficient.\n    \n        Returns:\n          An arg_scope.\n        """"""\n        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                            activation_fn=tf.nn.relu,\n                            weights_regularizer=slim.l2_regularizer(weight_decay),\n                            weights_initializer=tf.contrib.layers.xavier_initializer(),\n                            biases_initializer=tf.zeros_initializer()):\n            with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                                padding=\'SAME\',\n                                data_format=data_format):\n                with slim.arg_scope([custom_layers.pad2d,\n                                     custom_layers.l2_normalization,\n                                     custom_layers.channel_to_last],\n                                    data_format=data_format) as sc:\n                    return sc\n\n    \n    def get_model(self,inputs, weight_decay=0.0005,is_training=False):\n        # End_points collect relevant activations for external use.\n        arg_scope = self.__arg_scope(weight_decay=weight_decay)\n        self.img_shape=tfe.get_shape(inputs)[1:3]\n        with slim.arg_scope(arg_scope):\n            end_points = {}\n            channels={}\n            with tf.variable_scope(\'vgg_16\', [inputs]):\n                # Original VGG-16 blocks.\n                net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n                end_points[\'block1\'] = net\n                net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n                # Block 2.\n                net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n                end_points[\'block2\'] = net\n                net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n                # Block 3.\n                net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n                end_points[\'block3\'] = net\n                channels[\'block3\']=256\n                self.layer_shape.append(tfe.get_shape(net)[1:3])\n                net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n                # Block 4.\n                net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n                end_points[\'block4\'] = net\n                channels[\'block4\']=512\n                self.layer_shape.append(tfe.get_shape(net)[1:3])\n                net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n                # Block 5.\n                net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n                end_points[\'block5\'] = net\n                channels[\'block5\']=512\n                self.layer_shape.append(tfe.get_shape(net)[1:3])\n                net = slim.max_pool2d(net, [2, 2],  scope=\'pool5\')\n        \n            # Additional SSD blocks.\n            #with slim.arg_scope([slim.conv2d],\n                            #activation_fn=None):\n                #with slim.arg_scope([slim.batch_norm],\n                            #activation_fn=tf.nn.relu, is_training=is_training,updates_collections=None):\n                    #with slim.arg_scope([slim.dropout],\n                            #is_training=is_training,keep_prob=0.8):\n            with tf.variable_scope(self.model_name):\n                return self.__additional_ssd_block(end_points, channels,net,is_training=is_training)\n                        \n    def cpm(self,inputs):\n        residual1=slim.conv2d(inputs, 256, [1, 1], activation_fn=None,scope=\'cpm_conv1x1_branch1\')\n        #residual1=slim.batch_norm(residual1)\n        inputs1=slim.conv2d(inputs, 1024, [1, 1], scope=\'cpm_conv_1x1_branch1a\')\n        #inputs1=slim.batch_norm(inputs1)\n        inputs1=slim.conv2d(inputs1, 256, [1, 1], scope=\'cpm_conv_1x1_branch1b\')\n        #inputs1=slim.batch_norm(inputs1)\n        inputs1=slim.conv2d(inputs1, 256, [1, 1],activation_fn=None, scope=\'cpm_conv_1x1_branch1c\')\n        #inputs1=slim.batch_norm(inputs1)\n        branch1 = tf.add(inputs1, residual1, name=\'cpm_addition_branch1\')\n        branch1=tf.nn.relu(branch1)        \n\n        residual2=slim.conv2d(inputs, 256, [1, 1], activation_fn=None,scope=\'cpm_conv1x1_branch2\')\n        #residual2=slim.batch_norm(residual2)\n        inputs2=slim.conv2d(inputs, 1024, [1, 1], scope=\'cpm_conv_1x1_branch2a\')\n        #inputs2=slim.batch_norm(inputs2)\n        inputs2=slim.conv2d(inputs2, 256, [1, 1], scope=\'cpm_conv_1x1_branch2b\')\n        #inputs2=slim.batch_norm(inputs2)\n        inputs2=slim.conv2d(inputs2, 256, [1, 1],activation_fn=None, scope=\'cpm_conv_1x1_branch2c\')\n        #inputs2=slim.batch_norm(inputs2)\n        branch2 = tf.add(inputs2, residual2, name=\'cpm_addition_branch2\')\n        branch2=tf.nn.relu(branch2) \n        \n        residual2_1=slim.conv2d(branch2, 128, [1, 1], activation_fn=None,scope=\'cpm_conv1x1_branch2_1\')\n        #residual2_1=slim.batch_norm(residual2_1)\n        inputs2_1=slim.conv2d(branch2, 1024, [1, 1], scope=\'cpm_conv_1x1_branch2_1a\')\n        #inputs2_1=slim.batch_norm(inputs2_1)\n        inputs2_1=slim.conv2d(inputs2_1, 256, [1, 1], scope=\'cpm_conv_1x1_branch2_1b\')\n        #inputs2_1=slim.batch_norm(inputs2_1)\n        inputs2_1=slim.conv2d(inputs2_1, 128, [1, 1],activation_fn=None, scope=\'cpm_conv_1x1_branch2_1c\')\n        #inputs2_1=slim.batch_norm(inputs2_1)\n        branch2_1= tf.add(inputs2_1, residual2_1, name=\'cpm_addition_branch2_1\')\n        branch2_1=tf.nn.relu(branch2_1) \n        \n        residual2_2=slim.conv2d(branch2, 128, [1, 1], activation_fn=None,scope=\'cpm_conv1x1_branch2_2\')\n        #residual2_2=slim.batch_norm(residual2_2)\n        inputs2_2=slim.conv2d(branch2, 1024, [1, 1], scope=\'cpm_conv_1x1_branch2_2a\')\n        #inputs2_2=slim.batch_norm(inputs2_2)\n        inputs2_2=slim.conv2d(inputs2_2, 256, [1, 1], scope=\'cpm_conv_1x1_branch2_2b\')\n        #inputs2_2=slim.batch_norm(inputs2_2)\n        inputs2_2=slim.conv2d(inputs2_2, 128, [1, 1], activation_fn=None,scope=\'cpm_conv_1x1_branch2_2c\')\n        #inputs2_2=slim.batch_norm(inputs2_2)\n        branch2_2= tf.add(inputs2_2, residual2_2, name=\'cpm_addition_branch2_2\')\n        branch2_2=tf.nn.relu(branch2_2)\n        \n        residual2_2_1=slim.conv2d(branch2_2, 128, [1, 1], activation_fn=None,scope=\'cpm_conv1x1_branch2_2_1\')\n        #residual2_2_1=slim.batch_norm(residual2_2_1)\n        inputs2_2_1=slim.conv2d(branch2_2, 1024, [1, 1], scope=\'cpm_conv_1x1_branch2_2_1a\')\n        #inputs2_2_1=slim.batch_norm(inputs2_2_1)\n        inputs2_2_1=slim.conv2d(inputs2_2_1, 256, [1, 1], scope=\'cpm_conv_1x1_branch2_2_1b\')\n        #inputs2_2_1=slim.batch_norm(inputs2_2_1)\n        inputs2_2_1=slim.conv2d(inputs2_2_1, 128, [1, 1], activation_fn=None,scope=\'cpm_conv_1x1_branch2_2_1c\')\n        #inputs2_2_1=slim.batch_norm(inputs2_2_1)\n        branch2_2_1= tf.add(inputs2_2_1, residual2_2_1, name=\'cpm_addition_branch2_2_1\')\n        branch2_2_1=tf.nn.relu(branch2_2_1)\n\t\n        return tf.concat(values=[branch1, branch2_1,branch2_2_1], axis=3)\n    def ssd_multibox_layer(self, end_points,\n                       channels,\n                       feat_layers,\n                       normalization,\n                       bn_normalization=False,\n                       is_training=True):\n        """"""Construct a multibox layer, return a class and localization predictions.\n        """"""\n        face_logits = []\n        face_localisations = []\n        head_logits = []\n        head_localisations = []\n        body_logits = []\n        body_localisations = []\n        pyramid_layer=end_points.copy()\n        feat_layers.reverse()\n        for i, layer in enumerate(feat_layers):\n            with tf.variable_scope(layer + \'_box\'):\n                if int(layer[-1])<7:\n                        u=pyramid_layer[layer]\n                        sz = tf.shape(u)\n                        d=pyramid_layer[feat_layers[i-1]]\n                        d_=slim.conv2d(d, channels[layer], [1, 1], scope=\'conv_1x1\')\n                        #d_=slim.batch_norm(d_)\n                        d_=tf.image.resize_bilinear(d_,(sz[1],sz[2]),name=\'2xup\')\n                        u_ = tf.add(d_, u, name=\'addition\')\n                        pyramid_layer[layer]=u_\n                        net_=u_\n                else:\n                        net=pyramid_layer[layer]\n                if normalization[5-i] > 0:\n                    net = custom_layers.l2_normalization(net_, scaling=True,scope=\'L2Norm%d\'%(5-i))\t\t\n                net=self.cpm(net)\n                # Location.\n                num_loc_pred =  4\n                loc_pred_face = slim.conv2d(net, num_loc_pred, [3, 3], activation_fn=None,\n                                       scope=\'conv_loc_face\')\n                face_localisations.append(loc_pred_face)\n                \n                loc_pred_head = slim.conv2d(net, num_loc_pred, [3, 3], activation_fn=None,\n                                       scope=\'conv_loc_head\')\n                head_localisations.append(loc_pred_head)\n                \n                loc_pred_body = slim.conv2d(net, num_loc_pred, [3, 3], activation_fn=None,\n                                       scope=\'conv_loc_body\')\n                body_localisations.append(loc_pred_body)\t\n                \n                # Class prediction.\n                num_cls_pred =  4\n                cls_pred_face = slim.conv2d(net, num_cls_pred, [1, 1], activation_fn=None,\n                                       scope=\'conv_cls_face\')\n                if i==5:\n                    cn1,cn2,cn3,cp=tf.split(cls_pred_face,4,3)\n                    cn=tf.maximum(cn1,tf.maximum(cn2,cn3))\n                else:\n                    cn,cp1,cp2,cp3=tf.split(cls_pred_face,4,3)\n                    cp=tf.maximum(cp1,tf.maximum(cp2,cp3))\n                cls_pred_face=tf.concat([cn,cp],3)\n                face_logits.append(cls_pred_face)\n                \n                num_cls_pred =  2\t\t\t\t\t  \n                cls_pred_head = slim.conv2d(net, num_cls_pred, [1, 1], activation_fn=None,\n                                       scope=\'conv_cls_head\')\n                head_logits.append(cls_pred_head)\t\n                \n                num_cls_pred =  2\t\t\t\t\t  \n                cls_pred_body = slim.conv2d(net, num_cls_pred, [1, 1], activation_fn=None,\n                                       scope=\'conv_cls_body\')\t\t\t  \n                body_logits.append(cls_pred_body)\n        face_logits.reverse()\n        head_logits.reverse()\n        body_logits.reverse()\n        face_localisations.reverse()\n        head_localisations.reverse()\n        body_localisations.reverse()\n        feat_layers.reverse()\n        if is_training==True:\n            logits=[face_logits,head_logits,body_logits]\n            localisations=[face_localisations,head_localisations,body_localisations]\n        else:\n            logits=[face_logits]\n            localisations=[face_localisations]\n        return logits,localisations\n    \n    \n    def tensor_shape(self, x, rank=3):\n        """"""Returns the dimensions of a tensor.\n        Args:\n          image: A N-D Tensor of shape.\n        Returns:\n          A list of dimensions. Dimensions that are statically known are python\n            integers,otherwise they are integer scalar tensors.\n        """"""\n        if x.get_shape().is_fully_defined():\n            return x.get_shape().as_list()\n        else:\n            static_shape = x.get_shape().with_rank(rank).as_list()\n            dynamic_shape = tf.unstack(tf.shape(x), rank)\n            return [s if s is not None else d\n                    for s, d in zip(static_shape, dynamic_shape)]\n                \n    def get_allanchors(self, minmaxformat=False,layer_shape=None,img_shape=None):\n        \n        if self.np__anchors is None:\n            \n            anchors = self.ssd_anchors_all_layers(feat_shapes=layer_shape,img_shape=img_shape)\n            self.np_anchors_minmax_face =[]\n            self.np_anchors_minmax_head =[]\n            self.np_anchors_minmax_body =[]\n            self.np__anchors_face = []\n            self.np__anchors_head = []\n            self.np__anchors_body = []\n            for i, anchors_layer in enumerate(anchors):\n                \n                yref, xref, href, wref = anchors_layer\n                href=href\n                wref=wref\n                ymin = yref - href / 2.\n                xmin = xref - wref / 2.\n                ymax = yref + href / 2\n                xmax = xref + wref / 2.\n                temp_achors = np.concatenate([ymin[...,np.newaxis],xmin[...,np.newaxis],ymax[...,np.newaxis],xmax[...,np.newaxis]], axis = -1)\n                self.np_anchors_minmax_face.append(temp_achors)\n                cy = (ymax + ymin) / 2.\n                cx = (xmax + xmin) / 2.\n                h = ymax - ymin\n                w = xmax - xmin\n                temp_achors = np.concatenate([cx[...,np.newaxis],cy[...,np.newaxis],w[...,np.newaxis],h[...,np.newaxis]], axis = -1)\n                self.np__anchors_face.append(temp_achors)\n                if i>0:\n                    yref, xref, href_src, wref_src = anchors_layer\n                    href=href_src/2\n                    wref=wref_src/2\n                    ymin = yref - href / 2.\n                    xmin = xref - wref / 2.\n                    ymax = yref + href / 2\n                    xmax = xref + wref / 2.\n                    temp_achors = np.concatenate([ymin[...,np.newaxis],xmin[...,np.newaxis],ymax[...,np.newaxis],xmax[...,np.newaxis]], axis = -1)\n                    self.np_anchors_minmax_head.append(temp_achors)\n                    cy = (ymax + ymin) / 2.\n                    cx = (xmax + xmin) / 2.\n                    h = ymax - ymin\n                    w = xmax - xmin\n                    temp_achors = np.concatenate([cx[...,np.newaxis],cy[...,np.newaxis],w[...,np.newaxis],h[...,np.newaxis]], axis = -1)\n                    self.np__anchors_head.append(temp_achors)\n                if i>1:\n                    yref, xref, href_src, wref_src = anchors_layer\n                    href=href_src/4\n                    wref=wref_src/4\n                    ymin = yref - href / 2.\n                    xmin = xref - wref / 2.\n                    ymax = yref + href / 2\n                    xmax = xref + wref / 2.\n                    temp_achors = np.concatenate([ymin[...,np.newaxis],xmin[...,np.newaxis],ymax[...,np.newaxis],xmax[...,np.newaxis]], axis = -1)\n                    self.np_anchors_minmax_body.append(temp_achors)\n                    cy = (ymax + ymin) / 2.\n                    cx = (xmax + xmin) / 2.\n                    h = ymax - ymin\n                    w = xmax - xmin\n                    temp_achors = np.concatenate([cx[...,np.newaxis],cy[...,np.newaxis],w[...,np.newaxis],h[...,np.newaxis]], axis = -1)\n                    self.np__anchors_body.append(temp_achors)\n            self.np_anchors_minmax=[self.np_anchors_minmax_face,self.np_anchors_minmax_head,self.np_anchors_minmax_body]\n            self.np__anchors=[self.np__anchors_face,self.np__anchors_head,self.np__anchors_body]\n        if  minmaxformat:\n            return self.np_anchors_minmax\n        else:\n            return self.np__anchors\n        \n    def detected_bboxes(self, predictions, localisations,\n                        clipping_bbox=None):\n        """"""Get the detected bounding boxes from the SSD network output.\n        """"""\n        # Select top_k bboxes from predictions, and clip\n        rscores, rbboxes = \\\n            ssd_common.tf_ssd_bboxes_select(predictions, localisations,\n                                            select_threshold=self.select_threshold,\n                                            num_classes=self.num_classes)\n        rscores, rbboxes = \\\n            tfe.bboxes_sort(rscores, rbboxes, top_k=self.select_top_k)\n        # Apply NMS algorithm.\n        rscores, rbboxes = \\\n            tfe.bboxes_nms_batch(rscores, rbboxes,\n                                 nms_threshold=self.nms_threshold,\n                                 keep_top_k=self.keep_top_k)\n        if clipping_bbox is not None:\n            rbboxes = tfe.bboxes_clip(clipping_bbox, rbboxes)\n        return rscores, rbboxes\n    def decode_bboxes_all_ayers_tf(self, feat_localizations):\n        """"""convert ssd boxes from relative to input image anchors to relative to input width/height\n    \n        Return:\n          numpy array NlayersxNx4: ymin, xmin, ymax, xmax\n        """"""\n        anchors = self.ssd_anchors_all_layers(feat_shapes=self.layer_shape,img_shape=self.img_shape)\n        return ssd_common.tf_ssd_bboxes_decode(\n            feat_localizations, anchors,\n            prior_scaling=self.prior_scaling)\n    def compute_jaccard(self, gt_bboxes, anchors):\n        with tf.device(\'/cpu:0\'):\n            gt_bboxes = tf.reshape(gt_bboxes, (-1,1,4))\n            anchors = tf.reshape(anchors, (1,-1,4))\n            \n            inter_ymin = tf.maximum(gt_bboxes[:,:,0], anchors[:,:,0])\n            inter_xmin = tf.maximum(gt_bboxes[:,:,1], anchors[:,:,1])\n            inter_ymax = tf.minimum(gt_bboxes[:,:,2], anchors[:,:,2])\n            inter_xmax = tf.minimum(gt_bboxes[:,:,3], anchors[:,:,3])\n            \n            h = tf.maximum(inter_ymax - inter_ymin, 0.)\n            w = tf.maximum(inter_xmax - inter_xmin, 0.)\n            \n            inter_area = h * w\n            anchors_area = (anchors[:,:,3] - anchors[:,:,1]) * (anchors[:,:,2] - anchors[:,:,0])\n            gt_bboxes_area = (gt_bboxes[:,:,3] - gt_bboxes[:,:,1]) * (gt_bboxes[:,:,2] - gt_bboxes[:,:,0])\n            union_area = anchors_area - inter_area + gt_bboxes_area\n            jaccard = inter_area/union_area\n            \n            return jaccard\n    \n    def __match_no_miss(self,gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores,jaccard,gt_labels,gt_bboxes, num_anchors):\n        #make sure every ground truth box can be matched to at least one anchor box\n        max_inds = tf.cast(tf.argmax(jaccard, axis=1),tf.int32)\n        def cond(i,gt_anchors_labels,gt_anchors_bboxes,gt_anchors_scores):\n            r = tf.less(i, tf.shape(gt_labels)[0])\n            return r\n        def body(i,gt_anchors_labels,gt_anchors_bboxes,gt_anchors_scores):\n            \n            #upate gt_anchors_labels\n            updates = tf.reshape(gt_labels[i], [-1])\n            indices = tf.reshape(max_inds[i],[1,-1])\n            shape = tf.reshape(num_anchors,[-1])\n            \n            \n            new_labels = tf.scatter_nd(indices, updates, shape)\n            new_mask = tf.cast(new_labels, tf.bool)\n            gt_anchors_labels = tf.where(new_mask, new_labels, gt_anchors_labels)\n            \n            #update gt_anchors_bboxes\n            updates = tf.reshape(gt_bboxes[i], [1,-1])\n            indices = tf.reshape(max_inds[i],[1,-1])\n            shape = tf.shape(gt_anchors_bboxes)\n            new_bboxes = tf.scatter_nd(indices, updates, shape)\n            gt_anchors_bboxes = tf.where(new_mask, new_bboxes, gt_anchors_bboxes)\n            \n            #update gt_anchors_scores\n            updates = tf.reshape(jaccard[i, max_inds[i]], [-1])\n            indices = tf.reshape(max_inds[i],[1,-1])\n            shape = tf.reshape(num_anchors,[-1])\n            new_scores = tf.scatter_nd(indices, updates, shape)\n            gt_anchors_scores = tf.where(new_mask, new_scores, gt_anchors_scores)\n            \n    \n            \n            return [i+1,gt_anchors_labels,gt_anchors_bboxes,gt_anchors_scores]\n        \n        \n        i = 0\n        [i,gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores] = tf.while_loop(cond, body,[i,gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores])\n        \n        return gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores\n    \n    def __match_no_labels(self,gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores,jaccard,matching_threshold,gt_labels,gt_bboxes,num_anchors):\n        #For images without labels, just return all zero tensors\n        \n        return gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores\n    def __match_with_labels(self,u,gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores,jaccard,matching_threshold,gt_labels,gt_bboxes,num_anchors):\n       \n        mask = tf.reduce_max (jaccard, axis = 0) > matching_threshold\n        mask_inds = tf.argmax(jaccard, axis = 0)\n        matched_labels = tf.gather(gt_labels, mask_inds)\n        gt_anchor_labels = tf.where(mask, matched_labels, gt_anchor_labels)\n        gt_anchor_bboxes = tf.where(mask, tf.gather(gt_bboxes, mask_inds),gt_anchor_bboxes)\n        gt_anchor_scores = tf.reduce_max(jaccard, axis= 0)\n    \n        #matching each ground truth box to the default box with the best jaccard overlap\n        if u==0:\n            use_no_miss = True\n        else:\n            use_no_miss = False\n        if use_no_miss:\n            gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores = self.__match_no_miss(gt_anchor_labels, \\\n                                                                                      gt_anchor_bboxes, gt_anchor_scores, jaccard, \\\n                                                                                      gt_labels, gt_bboxes, num_anchors)\n        \n        return gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores\n    \n    def match_achors(self, gt_labels, gt_bboxes, matching_threshold = 0.35,layer_shape=None,img_shape=None):\n        \n        anchors_src = self.get_allanchors(minmaxformat=True,layer_shape=layer_shape,img_shape=img_shape)\n        #flattent the anchors\n        gt_anchor_bboxes_list=[]\n        gt_anchor_labels_list=[]\n        gt_anchor_scores_list=[]\n        for u in range(3):\n            temp_anchors = []\n            for i in range(len(anchors_src[u])):\n                temp_anchors.append(tf.reshape(anchors_src[u][i], [-1, 4]))\n            anchors = tf.concat(temp_anchors, axis=0)\n            \n            jaccard = self.compute_jaccard(gt_bboxes, anchors)\n            num_anchors= jaccard.shape[1]\n            \n            #initialize output\n            gt_anchor_labels = tf.zeros(num_anchors, dtype=tf.int64)\n            gt_anchor_scores = tf.zeros(num_anchors, dtype=tf.float32)\n            gt_anchor_ymins = tf.zeros(num_anchors)\n            gt_anchor_xmins = tf.zeros(num_anchors)\n            gt_anchor_ymaxs = tf.ones(num_anchors)\n            gt_anchor_xmaxs = tf.ones(num_anchors)\n            gt_anchor_bboxes = tf.stack([gt_anchor_ymins,gt_anchor_xmins,gt_anchor_ymaxs,gt_anchor_xmaxs], axis=-1)\n            \n            n__glabels = tf.size(gt_labels)\n            gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores = tf.cond(tf.equal(n__glabels, 0), \\\n                                                                         lambda: self.__match_no_labels(gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores,jaccard,matching_threshold,gt_labels,gt_bboxes,num_anchors), \\\n                                                                         lambda: self.__match_with_labels(u,gt_anchor_labels,gt_anchor_bboxes,gt_anchor_scores,jaccard,matching_threshold,gt_labels,gt_bboxes,num_anchors))\n            \n            \n            \n            # Transform to center / size.\n            feat_cx = (gt_anchor_bboxes[:,3] + gt_anchor_bboxes[:,1]) / 2.\n            feat_cy = (gt_anchor_bboxes[:,2] + gt_anchor_bboxes[:,0]) / 2.\n            feat_w = gt_anchor_bboxes[:,3] - gt_anchor_bboxes[:,1]\n            feat_h = gt_anchor_bboxes[:,2] - gt_anchor_bboxes[:,0]\n            \n            xref = (anchors[:,3] + anchors[:,1]) / 2.\n            yref = (anchors[:,2] + anchors[:,0]) / 2.\n            wref = anchors[:,3] - anchors[:,1]\n            href = anchors[:,2] - anchors[:,0]\n            \n            \n            # Encode features, convert ground truth bboxes to  shape offset relative to default boxes \n            feat_cx = (feat_cx - xref) / wref\n            feat_cy = (feat_cy - yref) / href \n            feat_w = tf.log(feat_w / wref) \n            feat_h = tf.log(feat_h / href) \n            if u!=0:\n                feat_cy=feat_cy-(1-2**u)/2.0*feat_h\n                feat_cx=feat_cx-(1-2**u)/2.0*feat_w\n                feat_h=(2**u)*feat_h\n                feat_w=(2**u)*feat_w\t\n            feat_cy/=self.prior_scaling[0]\n            feat_cx/=self.prior_scaling[1]\n            feat_h/=self.prior_scaling[2]\n            feat_w/=self.prior_scaling[3]\n            \n            gt_anchor_bboxes = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=-1)\n            \n            gt_anchor_bboxes_list.append(gt_anchor_bboxes)\n            gt_anchor_labels_list.append(gt_anchor_labels)\n            gt_anchor_scores_list.append(gt_anchor_scores)\n        gt_anchor_labels_list, gt_anchor_bboxes_list,gt_anchor_scores_list = self.__convert2layers(gt_anchor_labels_list, gt_anchor_bboxes_list,gt_anchor_scores_list)\n\n        return gt_anchor_labels_list, gt_anchor_bboxes_list, gt_anchor_scores_list\n    def __convert2layers(self,gclasses, glocalisations, gscores):\n        gt_anchor_labels_list = []\n        gt_anchor_bboxes_list = []\n        gt_anchor_scores_list = []\n        \n        anchors = self.get_allanchors(minmaxformat = False)\n        for u in range(3):\n            start = 0\n            end = 0\n            gt_anchor_labels = []\n            gt_anchor_bboxes = []\n            gt_anchor_scores = []\n            for i in range(len(anchors[u])):\n                anchor_shape = anchors[u][i].shape[:-1]\n                anchor_shape = list(anchor_shape)\n                anchor_num = np.array(anchor_shape).prod()\n                start = end\n                end = start + anchor_num\n\n                gt_anchor_labels.append(tf.reshape(gclasses[u][start:end],anchor_shape))\n                gt_anchor_scores.append(tf.reshape(gscores[u][start:end],anchor_shape))\n                gt_anchor_bboxes.append(tf.reshape(glocalisations[u][start:end],anchor_shape + [4]))\n            \n            gt_anchor_labels_list.append(gt_anchor_labels)\n            gt_anchor_scores_list.append(gt_anchor_scores)\n            gt_anchor_bboxes_list.append(gt_anchor_bboxes)\n            \n        return gt_anchor_labels_list, gt_anchor_bboxes_list,gt_anchor_scores_list\n   \n    def ssd_anchors_all_layers(self,\n                           dtype=np.float32,feat_shapes=None,img_shape=None):\n        """"""Compute anchor boxes for all feature layers.\n        """"""\n        layers_anchors = []\n        if feat_shapes==None and img_shape==None:\n            for i, s in enumerate(self.feat_shapes):\n                anchor_bboxes = self.__ssd_anchor_one_layer(self.img_shape, s,\n                                                     self.anchor_sizes[i],        \n                                                     self.anchor_steps[i],\n                                                     offset=self.anchor_offset, dtype=dtype)\n                layers_anchors.append(anchor_bboxes)\n        else:\n            for i, s in enumerate(feat_shapes):\n                anchor_bboxes = self.__ssd_anchor_one_layer(img_shape, s,\n                                                     self.anchor_sizes[i],        \n                                                     self.anchor_steps[i],\n                                                     offset=self.anchor_offset, dtype=dtype)\n                layers_anchors.append(anchor_bboxes)\n        return layers_anchors\n    def __ssd_anchor_one_layer(self,img_shape,\n                         feat_shape,\n                         sizes,\n                         step,\n                         offset=0.5,\n                         dtype=np.float32):\n        """"""Computer SSD default anchor boxes for one feature layer.\n    \n        Determine the relative position grid of the centers, and the relative\n        width and height.\n    \n        Arguments:\n          feat_shape: Feature shape, used for computing relative position grids;\n          size: Absolute reference sizes;\n          ratios: Ratios to use on these features;\n          img_shape: Image shape, used for computing height, width relatively to the\n            former;\n          offset: Grid offset.\n    \n        Return:\n          y, x, h, w: Relative x and y grids, and height and width.\n        """"""\n        # Weird SSD-Caffe computation using steps values...\n        y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\n        y = (y.astype(dtype) + offset) * step / img_shape[0]\n        x = (x.astype(dtype) + offset) * step / img_shape[1]\n    \n        # Expand dims to support easy broadcasting.\n        y = np.expand_dims(y, axis=-1)\n        x = np.expand_dims(x, axis=-1)\n    \n        # Compute relative height and width.\n        # Tries to follow the original implementation of SSD for the order.\n\n        h = np.zeros((1, ), dtype=dtype)\n        w = np.zeros((1, ), dtype=dtype)\n        # Add first anchor boxes with ratio=1.\n        h[0] = sizes / img_shape[0]\n        w[0] = sizes / img_shape[1]\n        \n        return y, x, h, w\n   \n    \n    def get_losses(self, logits3, localisations3,\n               gclasses3, glocalisations3, gscores3,\n               match_threshold=0.5,\n               negative_ratio=2.,\n               alpha=1.,\n               label_smoothing=0.,\n               scope=None):\n        """"""Loss functions for training the SSD 300 VGG network.\n    \n        This function defines the different loss components of the SSD, and\n        adds them to the TF loss collection.\n    \n        Arguments:\n          logits: (list of) predictions logits Tensors;\n          localisations: (list of) localisations Tensors;\n          gclasses: (list of) groundtruth labels Tensors;\n          glocalisations: (list of) groundtruth localisations Tensors;\n          gscores: (list of) groundtruth score Tensors;\n        """"""\n        with tf.name_scope(scope, \'ssd_losses\'):\n            train_or_eval_test=len(logits3)\n            all_pmask=[]\n            apmask=[]\n            for u in range(train_or_eval_test):\n                gclasses=gclasses3[u]\n                fgclasses = []\n                for i in range(len(gclasses)):\n                    fgclasses.append(tf.reshape(gclasses[i], [-1]))\n                gclasses = tf.concat(fgclasses, axis=0)\n                pmask = gclasses > 0\n                all_pmask.append(pmask)\n            part1=all_pmask[0][0:25600]\n            part2_temp=tf.logical_or(all_pmask[0][25600:],all_pmask[1][:],name=\'or1\')\n            part2=part2_temp[0:6400]\n            part3=tf.logical_or(part2_temp[6400:],all_pmask[2][:],name=\'or2\')\n            apmask.append(tf.concat([part1,part2,part3],axis=0))\n            apmask.append(tf.concat([part2,part3],axis=0))\n            apmask.append(part3)\n            for u in range(train_or_eval_test):\n                logits=logits3[u]\n                localisations=localisations3[u]\n                gclasses=gclasses3[u]\n                glocalisations=glocalisations3[u]\n                gscores=gscores3[u]\n                lshape = tfe.get_shape(logits[0], 4)\n                num_classes = 2\n                batch_size = lshape[0]\n                # Flatten out all vectors!\n                flogits = []\n                fgclasses = []\n                fgscores = []\n                flocalisations = []\n                fglocalisations = []\n                for i in range(len(logits)-u):\n                    flogits.append(tf.reshape(logits[i+u], [-1, num_classes]))\n                    fgclasses.append(tf.reshape(gclasses[i], [-1]))\n                    fgscores.append(tf.reshape(gscores[i], [-1]))\n                    flocalisations.append(tf.reshape(localisations[i+u], [-1, 4]))\n                    fglocalisations.append(tf.reshape(glocalisations[i], [-1, 4]))\n                # And concat the crap!\n                logits = tf.concat(flogits, axis=0)\n                gclasses = tf.concat(fgclasses, axis=0)\n                gscores = tf.concat(fgscores, axis=0)\n                localisations = tf.concat(flocalisations, axis=0)\n                glocalisations = tf.concat(fglocalisations, axis=0)\n                dtype = logits.dtype\n    \n                # Compute positive matching mask...\n                pmask = gclasses > 0\n                fpmask = tf.cast(pmask, dtype)\n                n_positives = tf.reduce_sum(fpmask)\n        \n                # Hard negative mining...\n                #for no_classes, we only care that false positive\'s label is 0\n                #this is why pmask sufice our needs\n                no_classes = tf.cast(apmask[u], tf.int32)\n                \n                nmask = tf.logical_not(apmask[u])\n                \n                fnmask = tf.cast(nmask, dtype)\n            \n                # Number of negative entries to select.\n                max_neg_entries = tf.cast(tf.reduce_sum(fnmask), tf.int32)\n               \n                n_neg = tf.cast(negative_ratio * n_positives, tf.int32)\n                n_neg = tf.minimum(n_neg, max_neg_entries)\n                #avoid n_neg is zero, and cause error when doing top_k later on\n                n_neg = tf.maximum(n_neg, 1)\n        \n                \n                extend_weight=1.0\n                if u==1:\n                    extend_weight=0.5\n                elif u==2:\n                    extend_weight=0.25\n                # Add cross-entropy loss.\n                with tf.name_scope(\'cross_entropy_pos%d\' % u):\n                    total_cross_pos = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n                                                                          labels=gclasses)\n                    total_cross_pos = tf.reduce_sum(total_cross_pos * fpmask, name=""cross_entropy_pos"")\n                    total_cross_pos=tf.cond(n_positives>0,lambda:tf.div(total_cross_pos,n_positives),lambda:0.)\n                    tf.losses.add_loss(total_cross_pos)\n        \n                with tf.name_scope(\'cross_entropy_neg%d\' % u):\n                    total_cross_neg = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n                                                                          labels=no_classes)\n                    val, idxes = tf.nn.top_k(total_cross_neg* fnmask,k=n_neg)   \n                    total_cross_neg=tf.reduce_sum(val,name=""cross_entropy_neg"")                                                   \n                    total_cross_neg=tf.cond(n_positives>0,lambda:tf.div(total_cross_neg,n_positives),lambda:0.)\n                    tf.losses.add_loss(total_cross_neg)\n        \n                # Add localization loss: smooth L1, L2, ...\n                with tf.name_scope(\'localization%d\' % u):\n                    # Weights Tensor: positive mask + random negative.\n                    weights = tf.expand_dims(alpha * fpmask, axis=-1)\n                    total_loc = custom_layers.abs_smooth_2(localisations - glocalisations)\n                    total_loc = tf.reduce_sum(total_loc * weights*extend_weight, name=""localization"")\n                    total_loc=tf.cond(n_positives>0,lambda:tf.div(total_loc,n_positives),lambda:0.)\n                    tf.losses.add_loss(total_loc)\n            \n                total_cross = tf.add(total_cross_pos, total_cross_neg, \'cross_entropy%d\' % u)\n            \n            \n                # Add to EXTRA LOSSES TF.collection\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_cross_pos)\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_cross_neg)\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_cross)\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_loc)\n           \n                tf.summary.scalar(\'postive_num%d\' % u, n_positives)\n                tf.summary.scalar(\'negative_num%d\' % u, n_neg)\n            \n            model_loss=tf.get_collection(tf.GraphKeys.LOSSES)\n            model_loss = tf.add_n(model_loss)\n            regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            regularization_loss = tf.add_n(regularization_losses,name=\'regularization_loss\')\n            tf.summary.scalar(\'regularization_loss\', regularization_loss)\n            total_loss=tf.add(model_loss, regularization_loss)\n            return total_loss\n   \n    \n    \n    def run(self):\n        \n        \n        return\n    \n    \ng_ssd_model = PyramidBoxModel()\n\nif __name__ == ""__main__"":   \n    obj= PyramidBoxModel()\n    obj.run()\n'"
nets/ssd_common.py,68,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Shared function between different SSD implementations.\n""""""\nimport numpy as np\nimport tensorflow as tf\nimport tf_extended as tfe\n\n\n# =========================================================================== #\n# TensorFlow implementation of boxes SSD encoding / decoding.\n# =========================================================================== #\ndef tf_ssd_bboxes_encode_layer(labels,\n                               bboxes,\n                               anchors_layer,\n                               num_classes,\n                               no_annotation_label,\n                               ignore_threshold=0.5,\n                               prior_scaling=[0.1, 0.1, 0.2, 0.2],\n                               dtype=tf.float32):\n    """"""Encode groundtruth labels and bounding boxes using SSD anchors from\n    one layer.\n\n    Arguments:\n      labels: 1D Tensor(int64) containing groundtruth labels;\n      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;\n      anchors_layer: Numpy array with layer anchors;\n      matching_threshold: Threshold for positive match with groundtruth bboxes;\n      prior_scaling: Scaling of encoded coordinates.\n\n    Return:\n      (target_labels, target_localizations, target_scores): Target Tensors.\n    """"""\n    # Anchors coordinates and volume.\n    yref, xref, href, wref = anchors_layer\n    ymin = yref - href / 2.\n    xmin = xref - wref / 2.\n    ymax = yref + href / 2.\n    xmax = xref + wref / 2.\n    vol_anchors = (xmax - xmin) * (ymax - ymin)\n\n    # Initialize tensors...\n    shape = (yref.shape[0], yref.shape[1], href.size)\n    feat_labels = tf.zeros(shape, dtype=tf.int64)\n    feat_scores = tf.zeros(shape, dtype=dtype)\n\n    feat_ymin = tf.zeros(shape, dtype=dtype)\n    feat_xmin = tf.zeros(shape, dtype=dtype)\n    feat_ymax = tf.ones(shape, dtype=dtype)\n    feat_xmax = tf.ones(shape, dtype=dtype)\n\n    def jaccard_with_anchors(bbox):\n        """"""Compute jaccard score between a box and the anchors.\n        """"""\n        int_ymin = tf.maximum(ymin, bbox[0])\n        int_xmin = tf.maximum(xmin, bbox[1])\n        int_ymax = tf.minimum(ymax, bbox[2])\n        int_xmax = tf.minimum(xmax, bbox[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        # Volumes.\n        inter_vol = h * w\n        union_vol = vol_anchors - inter_vol \\\n            + (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n        jaccard = tf.div(inter_vol, union_vol)\n        return jaccard\n\n    def intersection_with_anchors(bbox):\n        """"""Compute intersection between score a box and the anchors.\n        """"""\n        int_ymin = tf.maximum(ymin, bbox[0])\n        int_xmin = tf.maximum(xmin, bbox[1])\n        int_ymax = tf.minimum(ymax, bbox[2])\n        int_xmax = tf.minimum(xmax, bbox[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        inter_vol = h * w\n        scores = tf.div(inter_vol, vol_anchors)\n        return scores\n\n    def condition(i, feat_labels, feat_scores,\n                  feat_ymin, feat_xmin, feat_ymax, feat_xmax):\n        """"""Condition: check label index.\n        """"""\n        r = tf.less(i, tf.shape(labels))\n        return r[0]\n\n    def body(i, feat_labels, feat_scores,\n             feat_ymin, feat_xmin, feat_ymax, feat_xmax):\n        """"""Body: update feature labels, scores and bboxes.\n        Follow the original SSD paper for that purpose:\n          - assign values when jaccard > 0.5;\n          - only update if beat the score of other bboxes.\n        """"""\n        # Jaccard score.\n        label = labels[i]\n        bbox = bboxes[i]\n        jaccard = jaccard_with_anchors(bbox)\n        # Mask: check threshold + scores + no annotations + num_classes.\n        mask = tf.greater(jaccard, feat_scores)\n        # mask = tf.logical_and(mask, tf.greater(jaccard, matching_threshold))\n        mask = tf.logical_and(mask, feat_scores > -0.5)\n        mask = tf.logical_and(mask, label < num_classes)\n        imask = tf.cast(mask, tf.int64)\n        fmask = tf.cast(mask, dtype)\n        # Update values using mask.\n        feat_labels = imask * label + (1 - imask) * feat_labels\n        feat_scores = tf.where(mask, jaccard, feat_scores)\n\n        feat_ymin = fmask * bbox[0] + (1 - fmask) * feat_ymin\n        feat_xmin = fmask * bbox[1] + (1 - fmask) * feat_xmin\n        feat_ymax = fmask * bbox[2] + (1 - fmask) * feat_ymax\n        feat_xmax = fmask * bbox[3] + (1 - fmask) * feat_xmax\n\n        # Check no annotation label: ignore these anchors...\n        interscts = intersection_with_anchors(bbox)\n        mask = tf.logical_and(interscts > ignore_threshold,\n                              label == no_annotation_label)\n        # Replace scores by -1.\n        feat_scores = tf.where(mask, -tf.cast(mask, dtype), feat_scores)\n\n        return [i+1, feat_labels, feat_scores,\n                feat_ymin, feat_xmin, feat_ymax, feat_xmax]\n    # Main loop definition.\n    i = 0\n    [i, feat_labels, feat_scores,\n     feat_ymin, feat_xmin,\n     feat_ymax, feat_xmax] = tf.while_loop(condition, body,\n                                           [i, feat_labels, feat_scores,\n                                            feat_ymin, feat_xmin,\n                                            feat_ymax, feat_xmax])\n    # Transform to center / size.\n    feat_cy = (feat_ymax + feat_ymin) / 2.\n    feat_cx = (feat_xmax + feat_xmin) / 2.\n    feat_h = feat_ymax - feat_ymin\n    feat_w = feat_xmax - feat_xmin\n    # Encode features.\n    feat_cy = (feat_cy - yref) / href / prior_scaling[0]\n    feat_cx = (feat_cx - xref) / wref / prior_scaling[1]\n    feat_h = tf.log(feat_h / href) / prior_scaling[2]\n    feat_w = tf.log(feat_w / wref) / prior_scaling[3]\n    # Use SSD ordering: x / y / w / h instead of ours.\n    feat_localizations = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=-1)\n    return feat_labels, feat_localizations, feat_scores\n\n\ndef tf_ssd_bboxes_encode(labels,\n                         bboxes,\n                         anchors,\n                         num_classes,\n                         no_annotation_label,\n                         ignore_threshold=0.5,\n                         prior_scaling=[0.1, 0.1, 0.2, 0.2],\n                         dtype=tf.float32,\n                         scope=\'ssd_bboxes_encode\'):\n    """"""Encode groundtruth labels and bounding boxes using SSD net anchors.\n    Encoding boxes for all feature layers.\n\n    Arguments:\n      labels: 1D Tensor(int64) containing groundtruth labels;\n      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;\n      anchors: List of Numpy array with layer anchors;\n      matching_threshold: Threshold for positive match with groundtruth bboxes;\n      prior_scaling: Scaling of encoded coordinates.\n\n    Return:\n      (target_labels, target_localizations, target_scores):\n        Each element is a list of target Tensors.\n    """"""\n    with tf.name_scope(scope):\n        target_labels = []\n        target_localizations = []\n        target_scores = []\n        for i, anchors_layer in enumerate(anchors):\n            with tf.name_scope(\'bboxes_encode_block_%i\' % i):\n                t_labels, t_loc, t_scores = \\\n                    tf_ssd_bboxes_encode_layer(labels, bboxes, anchors_layer,\n                                               num_classes, no_annotation_label,\n                                               ignore_threshold,\n                                               prior_scaling, dtype)\n                target_labels.append(t_labels)\n                target_localizations.append(t_loc)\n                target_scores.append(t_scores)\n        return target_labels, target_localizations, target_scores\n\n\ndef tf_ssd_bboxes_decode_layer(feat_localizations,\n                               anchors_layer,\n                               prior_scaling=[0.1, 0.1, 0.2, 0.2]):\n    """"""Compute the relative bounding boxes from the layer features and\n    reference anchor bounding boxes.\n\n    Arguments:\n      feat_localizations: Tensor containing localization features.\n      anchors: List of numpy array containing anchor boxes.\n\n    Return:\n      Tensor Nx4: ymin, xmin, ymax, xmax\n    """"""\n    yref, xref, href, wref = anchors_layer\n    \n    # Compute center, height and width\n    cx = feat_localizations[:, :,  :, 0:1] * wref * prior_scaling[0] + xref\n    cy = feat_localizations[:, :,  :, 1:2] * href * prior_scaling[1] + yref\n    w = wref * tf.exp(feat_localizations[:, :,  :, 2:3] * prior_scaling[2])\n    h = href * tf.exp(feat_localizations[:, :,  :, 3:] * prior_scaling[3])\n    # Boxes coordinates.\n    ymin = cy - h / 2.\n    xmin = cx - w / 2.\n    ymax = cy + h / 2.\n    xmax = cx + w / 2.\n    bboxes = tf.stack([ymin, xmin, ymax, xmax], axis=-1)\n    \n    return bboxes\n\n\ndef tf_ssd_bboxes_decode(feat_localizations,\n                         anchors,\n                         prior_scaling=[0.1, 0.1, 0.2, 0.2],\n                         scope=\'ssd_bboxes_decode\'):\n    """"""Compute the relative bounding boxes from the SSD net features and\n    reference anchors bounding boxes.\n\n    Arguments:\n      feat_localizations: List of Tensors containing localization features.\n      anchors: List of numpy array containing anchor boxes.\n\n    Return:\n      List of Tensors Nx4: ymin, xmin, ymax, xmax\n    """"""\n    with tf.name_scope(scope):\n        bboxes = []\n        for i, anchors_layer in enumerate(anchors):\n            bboxes.append(\n                tf_ssd_bboxes_decode_layer(feat_localizations[i],\n                                           anchors_layer,\n                                           prior_scaling))\n        return bboxes\n\n\n# =========================================================================== #\n# SSD boxes selection.\n# =========================================================================== #\ndef tf_ssd_bboxes_select_layer(predictions_layer, localizations_layer,\n                               select_threshold=None,\n                               num_classes=21,\n                               ignore_class=0,\n                               scope=None):\n    """"""Extract classes, scores and bounding boxes from features in one layer.\n    Batch-compatible: inputs are supposed to have batch-type shapes.\n\n    Args:\n      predictions_layer: A SSD prediction layer;\n      localizations_layer: A SSD localization layer;\n      select_threshold: Classification threshold for selecting a box. All boxes\n        under the threshold are set to \'zero\'. If None, no threshold applied.\n    Return:\n      d_scores, d_bboxes: Dictionary of scores and bboxes Tensors of\n        size Batches X N x 1 | 4. Each key corresponding to a class.\n    """"""\n    select_threshold = 0.0 if select_threshold is None else select_threshold\n    with tf.name_scope(scope, \'ssd_bboxes_select_layer\',\n                       [predictions_layer, localizations_layer]):\n        # Reshape features: Batches x N x N_labels | 4\n        p_shape = tfe.get_shape(predictions_layer)\n        predictions_layer = tf.reshape(predictions_layer,\n                                       tf.stack([p_shape[0], -1, p_shape[-1]]))\n        l_shape = tfe.get_shape(localizations_layer)\n        localizations_layer = tf.reshape(localizations_layer,\n                                         tf.stack([l_shape[0], -1, l_shape[-1]]))\n        #print(l_shape)\n        #print(tfe.get_shape(predictions_layer))\n        d_scores = {}\n        d_bboxes = {}\n        for c in range(0, num_classes):\n            if c != ignore_class:\n                # Remove boxes under the threshold.\n                scores = predictions_layer[:, :, c]\n                fmask = tf.cast(tf.greater_equal(scores, select_threshold), scores.dtype)\n                scores = scores * fmask\n                bboxes = localizations_layer * tf.expand_dims(fmask, axis=-1)\n                # Append to dictionary.\n                d_scores[c] = scores\n                d_bboxes[c] = bboxes\n\n        return d_scores, d_bboxes\n\n\ndef tf_ssd_bboxes_select(predictions_net, localizations_net,\n                         select_threshold=None,\n                         num_classes=21,\n                         ignore_class=0,\n                         scope=None):\n    """"""Extract classes, scores and bounding boxes from network output layers.\n    Batch-compatible: inputs are supposed to have batch-type shapes.\n\n    Args:\n      predictions_net: List of SSD prediction layers;\n      localizations_net: List of localization layers;\n      select_threshold: Classification threshold for selecting a box. All boxes\n        under the threshold are set to \'zero\'. If None, no threshold applied.\n    Return:\n      d_scores, d_bboxes: Dictionary of scores and bboxes Tensors of\n        size Batches X N x 1 | 4. Each key corresponding to a class.\n    """"""\n    with tf.name_scope(scope, \'ssd_bboxes_select\',\n                       [predictions_net, localizations_net]):\n        l_scores = []\n        l_bboxes = []\n        for i in range(len(predictions_net)):\n            scores, bboxes = tf_ssd_bboxes_select_layer(predictions_net[i],\n                                                        localizations_net[i],\n                                                        select_threshold,\n                                                        num_classes,\n                                                        ignore_class)\n            l_scores.append(scores)\n            l_bboxes.append(bboxes)\n        # Concat results.\n        d_scores = {}\n        d_bboxes = {}\n        for c in l_scores[0].keys():\n            ls = [s[c] for s in l_scores]\n            lb = [b[c] for b in l_bboxes]\n            d_scores[c] = tf.concat(ls, axis=1)\n            d_bboxes[c] = tf.concat(lb, axis=1)\n        return d_scores, d_bboxes\n\n\ndef tf_ssd_bboxes_select_layer_all_classes(predictions_layer, localizations_layer,\n                                           select_threshold=None):\n    """"""Extract classes, scores and bounding boxes from features in one layer.\n     Batch-compatible: inputs are supposed to have batch-type shapes.\n\n     Args:\n       predictions_layer: A SSD prediction layer;\n       localizations_layer: A SSD localization layer;\n      select_threshold: Classification threshold for selecting a box. If None,\n        select boxes whose classification score is higher than \'no class\'.\n     Return:\n      classes, scores, bboxes: Input Tensors.\n     """"""\n    # Reshape features: Batches x N x N_labels | 4\n    p_shape = tfe.get_shape(predictions_layer)\n    predictions_layer = tf.reshape(predictions_layer,\n                                   tf.stack([p_shape[0], -1, p_shape[-1]]))\n    l_shape = tfe.get_shape(localizations_layer)\n    localizations_layer = tf.reshape(localizations_layer,\n                                     tf.stack([l_shape[0], -1, l_shape[-1]]))\n    # Boxes selection: use threshold or score > no-label criteria.\n    if select_threshold is None or select_threshold == 0:\n        # Class prediction and scores: assign 0. to 0-class\n        classes = tf.argmax(predictions_layer, axis=2)\n        scores = tf.reduce_max(predictions_layer, axis=2)\n        scores = scores * tf.cast(classes > 0, scores.dtype)\n    else:\n        sub_predictions = predictions_layer[:, :, 1:]\n        classes = tf.argmax(sub_predictions, axis=2) + 1\n        scores = tf.reduce_max(sub_predictions, axis=2)\n        # Only keep predictions higher than threshold.\n        mask = tf.greater(scores, select_threshold)\n        classes = classes * tf.cast(mask, classes.dtype)\n        scores = scores * tf.cast(mask, scores.dtype)\n    # Assume localization layer already decoded.\n    bboxes = localizations_layer\n    return classes, scores, bboxes\n\n\ndef tf_ssd_bboxes_select_all_classes(predictions_net, localizations_net,\n                                     select_threshold=None,\n                                     scope=None):\n    """"""Extract classes, scores and bounding boxes from network output layers.\n    Batch-compatible: inputs are supposed to have batch-type shapes.\n\n    Args:\n      predictions_net: List of SSD prediction layers;\n      localizations_net: List of localization layers;\n      select_threshold: Classification threshold for selecting a box. If None,\n        select boxes whose classification score is higher than \'no class\'.\n    Return:\n      classes, scores, bboxes: Tensors.\n    """"""\n    with tf.name_scope(scope, \'ssd_bboxes_select\',\n                       [predictions_net, localizations_net]):\n        l_classes = []\n        l_scores = []\n        l_bboxes = []\n        for i in range(len(predictions_net)):\n            classes, scores, bboxes = \\\n                tf_ssd_bboxes_select_layer_all_classes(predictions_net[i],\n                                                       localizations_net[i],\n                                                       select_threshold)\n            l_classes.append(classes)\n            l_scores.append(scores)\n            l_bboxes.append(bboxes)\n\n        classes = tf.concat(l_classes, axis=1)\n        scores = tf.concat(l_scores, axis=1)\n        bboxes = tf.concat(l_bboxes, axis=1)\n        return classes, scores, bboxes\n\n'"
preprocessing/__init__.py,0,b'\n'
preprocessing/inception_preprocessing.py,62,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n    Args:\n        x: input Tensor.\n        func: Python function to apply.\n        num_cases: Python int32, number of cases to sample sel from.\n\n    Returns:\n        The result of func(x, sel), where func receives the value of the\n        selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n            func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n            for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n\n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n\n    Args:\n        image: 3-D Tensor containing single image in [0, 1].\n        color_ordering: Python int, a type of distortion (valid values: 0-3).\n        fast_mode: Avoids slower ops (random_hue and random_contrast)\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n        ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n        image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged\n            as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n            image.\n        min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n            area of the image must contain at least this fraction of any bounding box\n            supplied.\n        aspect_ratio_range: An optional list of `floats`. The cropped area of the\n            image must have an aspect ratio = width / height within this range.\n        area_range: An optional list of `floats`. The cropped area of the image\n            must contain a fraction of the supplied image within in this range.\n        max_attempts: An optional `int`. Number of attempts at generating a cropped\n            region of the image of the specified constraints. After `max_attempts`\n            failures, return the entire image.\n        scope: Optional scope for name_scope.\n    Returns:\n        A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n        # A large fraction of image datasets contain a human-annotated bounding\n        # box delineating the region of the image containing the object of interest.\n        # We choose to create a new bounding box for the object which is a randomly\n        # distorted version of the human-annotated bounding box that obeys an\n        # allowed range of aspect ratios, sizes and overlap with the human-annotated\n        # bounding box. If no box is supplied, then we assume the bounding box is\n        # the entire image.\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n                tf.shape(image),\n                bounding_boxes=bbox,\n                min_object_covered=min_object_covered,\n                aspect_ratio_range=aspect_ratio_range,\n                area_range=area_range,\n                max_attempts=max_attempts,\n                use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True, scope=None):\n    """"""Distort one image for training a network.\n\n    Distorting images provides a useful technique for augmenting the data\n    set during training in order to make the network invariant to aspects\n    of the image that do not effect the label.\n\n    Additionally it would create image_summaries to display the different\n    transformations applied to the image.\n\n    Args:\n        image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n            [0, 1], otherwise it would converted to tf.float32 assuming that the range\n            is [0, MAX], where MAX is largest positive representable number for\n            int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n        height: integer\n        width: integer\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged\n            as [ymin, xmin, ymax, xmax].\n        fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n            bi-cubic resizing, random_hue or random_contrast).\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D float Tensor of distorted image used for training with range [-1, 1].\n    """"""\n    with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n        if bbox is None:\n            bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                               dtype=tf.float32,\n                               shape=[1, 1, 4])\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                      bbox)\n        tf.image_summary(\'image_with_bounding_boxes\', image_with_box)\n\n        distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n        # Restore the shape since the dynamic slice based upon the bbox_size loses\n        # the third dimension.\n        distorted_image.set_shape([None, None, 3])\n        image_with_distorted_box = tf.image.draw_bounding_boxes(\n                tf.expand_dims(image, 0), distorted_bbox)\n        tf.image_summary(\'images_with_distorted_bounding_box\',\n                         image_with_distorted_box)\n\n        # This resizing operation may distort the images because the aspect\n        # ratio is not respected. We select a resize method in a round robin\n        # fashion based on the thread number.\n        # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n        # We select only 1 case for fast_mode bilinear.\n        num_resize_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n                distorted_image,\n                lambda x, method: tf.image.resize_images(x, [height, width], method),\n                num_cases=num_resize_cases)\n\n        tf.image_summary(\'cropped_resized_image\',\n                         tf.expand_dims(distorted_image, 0))\n\n        # Randomly flip the image horizontally.\n        distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n        # Randomly distort the colors. There are 4 ways to do it.\n        distorted_image = apply_with_random_selector(\n                distorted_image,\n                lambda x, ordering: distort_color(x, ordering, fast_mode),\n                num_cases=4)\n\n        tf.image_summary(\'final_distorted_image\',\n                         tf.expand_dims(distorted_image, 0))\n        distorted_image = tf.sub(distorted_image, 0.5)\n        distorted_image = tf.mul(distorted_image, 2.0)\n        return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n    """"""Prepare one image for evaluation.\n\n    If height and width are specified it would output an image with that size by\n    applying resize_bilinear.\n\n    If central_fraction is specified it would cropt the central fraction of the\n    input image.\n\n    Args:\n        image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n            [0, 1], otherwise it would converted to tf.float32 assuming that the range\n            is [0, MAX], where MAX is largest positive representable number for\n            int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n        height: integer\n        width: integer\n        central_fraction: Optional Float, fraction of the image to crop.\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D float Tensor of prepared image.\n    """"""\n    with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Crop the central region of the image with an area containing 87.5% of\n        # the original image.\n        if central_fraction:\n            image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n        if height and width:\n            # Resize the image to the specified height and width.\n            image = tf.expand_dims(image, 0)\n            image = tf.image.resize_bilinear(image, [height, width],\n                                             align_corners=False)\n            image = tf.squeeze(image, [0])\n        image = tf.sub(image, 0.5)\n        image = tf.mul(image, 2.0)\n        return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False, bbox=None, fast_mode=True):\n    """"""Pre-process one image for training or evaluation.\n\n    Args:\n        image: 3-D Tensor [height, width, channels] with the image.\n        height: integer, image expected height.\n        width: integer, image expected width.\n        is_training: Boolean. If true it would transform an image for train,\n            otherwise it would transform it for evaluation.\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged as\n            [ymin, xmin, ymax, xmax].\n        fast_mode: Optional boolean, if True avoids slower transformations.\n\n    Returns:\n        3-D float Tensor containing an appropriately scaled image\n\n    Raises:\n        ValueError: if user does not provide bounding box\n    """"""\n    if is_training:\n        return preprocess_for_train(image, height, width, bbox, fast_mode)\n    else:\n        return preprocess_for_eval(image, height, width)\n'"
preprocessing/preprocessing_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# from preprocessing import cifarnet_preprocessing\n# from preprocessing import inception_preprocessing\n# from preprocessing import vgg_preprocessing\n\nfrom preprocessing import ssd_vgg_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n    """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n    Args:\n      name: The name of the preprocessing function.\n      is_training: `True` if the model is being used for training.\n\n    Returns:\n      preprocessing_fn: A function that preprocessing a single image (pre-batch).\n        It has the following signature:\n          image = preprocessing_fn(image, output_height, output_width, ...).\n\n    Raises:\n      ValueError: If Preprocessing `name` is not recognized.\n    """"""\n    preprocessing_fn_map = {\n        \'ssd_300_vgg\': ssd_vgg_preprocessing,\n        \'ssd_512_vgg\': ssd_vgg_preprocessing,\n    }\n\n    if name not in preprocessing_fn_map:\n        raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n    def preprocessing_fn(image, labels, bboxes,\n                         out_shape, data_format=\'NHWC\', **kwargs):\n        return preprocessing_fn_map[name].preprocess_image(\n            image, labels, bboxes, out_shape, data_format=data_format,\n            is_training=is_training, **kwargs)\n    return preprocessing_fn\n'"
preprocessing/ssd_vgg_preprocessing.py,71,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Pre-processing images for SSD-type networks.\n""""""\nfrom enum import Enum, IntEnum\nimport numpy as np\n\nimport tensorflow as tf\nimport tf_extended as tfe\n\nfrom tensorflow.python.ops import control_flow_ops\n\nfrom preprocessing import tf_image\n# from nets import ssd_common\n\nimport random\n\nslim = tf.contrib.slim\n\n# Resizing strategies.\nResize = IntEnum(\'Resize\', (\'NONE\',                # Nothing!\n                            \'CENTRAL_CROP\',        # Crop (and pad if necessary).\n                            \'PAD_AND_RESIZE\',      # Pad, and resize to output shape.\n                            \'WARP_RESIZE\'))        # Warp resize.\n\n# VGG mean parameters.\n_R_MEAN = 123.\n_G_MEAN = 117.\n_B_MEAN = 104.\n\n# Some training pre-processing parameters.\nBBOX_CROP_OVERLAP = 0.5        # Minimum overlap to keep a bbox after cropping.\nCROP_RATIO_RANGE = (0.8, 1.2)  # Distortion ratio during cropping.\nEVAL_SIZE = (1280, 1280)\n\n\ndef tf_image_whitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN]):\n    """"""Subtracts the given means from each image channel.\n\n    Returns:\n        the centered image.\n    """"""\n    if image.get_shape().ndims != 3:\n        raise ValueError(\'Input must be of size [height, width, C>0]\')\n    num_channels = image.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n        raise ValueError(\'len(means) must match the number of channels\')\n\n    mean = tf.constant(means, dtype=image.dtype)\n    image = image - mean\n    return image\n\n\ndef tf_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n    """"""Re-convert to original image distribution, and convert to int if\n    necessary.\n\n    Returns:\n      Centered image.\n    """"""\n    mean = tf.constant(means, dtype=image.dtype)\n    image = image + mean\n    if to_int:\n        image = tf.cast(image, tf.int32)\n    return image\n\n\ndef np_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n    """"""Re-convert to original image distribution, and convert to int if\n    necessary. Numpy version.\n\n    Returns:\n      Centered image.\n    """"""\n    img = np.copy(image)\n    img += np.array(means, dtype=img.dtype)\n    if to_int:\n        img = img.astype(np.uint8)\n    return img\n\n\ndef tf_summary_image(image, bboxes, name=\'image\', unwhitened=False):\n    """"""Add image with bounding boxes to summary.\n    """"""\n    if unwhitened:\n        image = tf_image_unwhitened(image)\n    image = tf.expand_dims(image, 0)\n    bboxes = tf.expand_dims(bboxes, 0)\n    image_with_box = tf.image.draw_bounding_boxes(image, bboxes)\n    tf.summary.image(name, image_with_box)\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n    Args:\n        x: input Tensor.\n        func: Python function to apply.\n        num_cases: Python int32, number of cases to sample sel from.\n\n    Returns:\n        The result of func(x, sel), where func receives the value of the\n        selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n            func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n            for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n\n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n\n    Args:\n        image: 3-D Tensor containing single image in [0, 1].\n        color_ordering: Python int, a type of distortion (valid values: 0-3).\n        fast_mode: Avoids slower ops (random_hue and random_contrast)\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n        ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                labels,\n                                bboxes,\n                                min_object_covered=0.5,#0.95,\n                                aspect_ratio_range=(0.9, 1.1),\n                                area_range=(0.75, 1.0),#(0.95, 1.0),\n                                max_attempts=200,\n                                bbox_crop_overlap = 0.5,#0.95,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n        image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged\n            as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n            image.\n        min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n            area of the image must contain at least this fraction of any bounding box\n            supplied.\n        aspect_ratio_range: An optional list of `floats`. The cropped area of the\n            image must have an aspect ratio = width / height within this range.\n        area_range: An optional list of `floats`. The cropped area of the image\n            must contain a fraction of the supplied image within in this range.\n        max_attempts: An optional `int`. Number of attempts at generating a cropped\n            region of the image of the specified constraints. After `max_attempts`\n            failures, return the entire image.\n        scope: Optional scope for name_scope.\n    Returns:\n        A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bboxes]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n                tf.shape(image),\n                bounding_boxes=tf.expand_dims(bboxes, 0),\n                min_object_covered=min_object_covered,\n                aspect_ratio_range=aspect_ratio_range,\n                area_range=area_range,\n                max_attempts=max_attempts,\n                use_image_if_no_bounding_boxes=True)\n        distort_bbox = distort_bbox[0, 0]\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        # Restore the shape since the dynamic slice loses 3rd dimension.\n        cropped_image.set_shape([None, None, 3])\n\n        # Update bounding boxes: resize and filter out.\n        bboxes = tfe.bboxes_resize(distort_bbox, bboxes)\n        labels, bboxes = tfe.bboxes_filter_overlap(labels, bboxes,\n                                                   bbox_crop_overlap)\n        \n        #adjust bbbox coordinates so that they go back to [0,1]\n        #this is essentially keeping only the overlapped of the bbox\n        \n        ymin = tf.maximum(bboxes[:,0], 0.0)\n        xmin = tf.maximum(bboxes[:,1], 0.0)\n        ymax = tf.minimum(bboxes[:,2], 1.0)\n        xmax = tf.minimum(bboxes[:,3], 1.0)\n        bboxes = tf.stack([ymin,xmin,ymax,xmax], axis = -1)\n\n        return cropped_image, labels, bboxes, distort_bbox\n\n\ndef preprocess_for_train(image, labels, bboxes,\n                         out_shape=EVAL_SIZE, data_format=\'NHWC\',\n                         scope=\'ssd_preprocessing_train\'):\n    """"""Preprocesses the given image for training.\n\n    Note that the actual resizing scale is sampled from\n        [`resize_size_min`, `resize_size_max`].\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        output_height: The height of the image after preprocessing.\n        output_width: The width of the image after preprocessing.\n        resize_side_min: The lower bound for the smallest side of the image for\n            aspect-preserving resizing.\n        resize_side_max: The upper bound for the smallest side of the image for\n            aspect-preserving resizing.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    fast_mode = False\n    with tf.name_scope(scope, \'ssd_preprocessing_train\', [image, labels, bboxes]):\n        if image.get_shape().ndims != 3:\n            raise ValueError(\'Input must be of size [height, width, C>0]\')\n        # Convert to float scaled [0, 1].\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        tf_summary_image(image, bboxes, \'original_image\')\n        #print(tfe.get_shape(image),image.get_shape().as_list(),\'debug\')\n        # # Remove DontCare labels.\n        # labels, bboxes = ssd_common.tf_bboxes_filter_labels(out_label,\n        #                                                     labels,\n        #                                                     bboxes)\n\n        # Distort image and bounding boxes.\n        dst_image = image\n        debug_image=image\n        bboxes=tf.clip_by_value(bboxes,0.,1.)\n        dst_image, labels, bboxes, distort_bbox = distorted_bounding_box_crop(dst_image, labels, bboxes)\n     \n        #tf_summary_image(image, tf.reshape(distort_bbox, (1,-1)), \'cropped_position\')\n        \n        # Resize image to output size.\n        dst_image = tf_image.resize_image(dst_image, out_shape, method=tf.image.ResizeMethod.BILINEAR,align_corners=False)\n        # Randomly flip the image horizontally.\n        dst_image, bboxes = tf_image.random_flip_left_right(dst_image, bboxes)\n        tf_summary_image(dst_image, bboxes, \'resized_image\')\n\n        # Randomly distort the colors. There are 4 ways to do it.\n        dst_image = apply_with_random_selector(dst_image,lambda x, ordering: distort_color(x, ordering, fast_mode), num_cases=4)\n        tf_summary_image(dst_image, bboxes, \'color_distorted_image\')\n\n        # Rescale to VGG input scale.\n        image = dst_image * 255.\n        image = tf_image_whitened(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n        # Image data format.\n        if data_format == \'NCHW\':\n            image = tf.transpose(image, perm=(2, 0, 1))\n        return image, labels, bboxes,\n\n\ndef preprocess_for_eval(image, labels, bboxes,\n                        out_shape=EVAL_SIZE, data_format=\'NHWC\',\n                        difficults=None, resize=Resize.WARP_RESIZE,\n                        scope=\'ssd_preprocessing_train\'):\n    """"""Preprocess an image for evaluation.\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        out_shape: Output shape after pre-processing (if resize != None)\n        resize: Resize strategy.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    with tf.name_scope(scope):\n        if image.get_shape().ndims != 3:\n            raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n        image = tf.to_float(image)\n        image = tf_image_whitened(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n        # Add image rectangle to bboxes.\n        bbox_img = tf.constant([[0., 0., 1., 1.]])\n        if bboxes is None:\n            bboxes = bbox_img\n        else:\n            bboxes = tf.concat([bbox_img, bboxes], axis=0)\n\n        if resize == Resize.NONE:\n            # No resizing...\n            #shape = tf.shape(image)\n            #factor =tf.minimum(tf.to_double(1.0), tf.minimum(tf.to_double(src_shape[0] / out_shape[0]),tf.to_double(src_shape[1] / out_shape[1])))\n            #resize_shape = tf.to_double(src_shape[0:2])/factor\n            #resize_shape = tf.cast(tf.floor(resize_shape), tf.int32)\n\n            #image = tf_image.resize_image(image, resize_shape,method=tf.image.ResizeMethod.BILINEAR,align_corners=False)\n            pass\n        elif resize == Resize.CENTRAL_CROP:\n            # Central cropping of the image.\n            image, bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(\n                image, bboxes, out_shape[0], out_shape[1])\n        elif resize == Resize.PAD_AND_RESIZE:\n            # Resize image first: find the correct factor...\n            shape = tf.shape(image)\n            factor = tf.minimum(tf.to_double(1.0),\n                                tf.minimum(tf.to_double(out_shape[0] / shape[0]),\n                                           tf.to_double(out_shape[1] / shape[1])))\n            resize_shape = factor * tf.to_double(shape[0:2])\n            resize_shape = tf.cast(tf.floor(resize_shape), tf.int32)\n\n            image = tf_image.resize_image(image, resize_shape,\n                                          method=tf.image.ResizeMethod.BILINEAR,\n                                          align_corners=False)\n            # Pad to expected size.\n            image, bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(\n                image, bboxes, out_shape[0], out_shape[1])\n        elif resize == Resize.WARP_RESIZE:\n            # Warp resize of the image.\n            image = tf_image.resize_image(image, out_shape,\n                                          method=tf.image.ResizeMethod.BILINEAR,\n                                          align_corners=False)\n\n        # Split back bounding boxes.\n        bbox_img = bboxes[0]\n        bboxes = bboxes[1:]\n        # Remove difficult boxes.\n        if difficults is not None:\n            mask = tf.logical_not(tf.cast(difficults, tf.bool))\n            labels = tf.boolean_mask(labels, mask)\n            bboxes = tf.boolean_mask(bboxes, mask)\n        # Image data format.\n        if data_format == \'NCHW\':\n            image = tf.transpose(image, perm=(2, 0, 1))\n        return image, labels, bboxes, bbox_img\n\n\ndef preprocess_image(image,\n                     labels,\n                     bboxes,\n                     out_shape,\n                     data_format,\n                     is_training=False,\n                     **kwargs):\n    """"""Pre-process an given image.\n\n    Args:\n      image: A `Tensor` representing an image of arbitrary size.\n      output_height: The height of the image after preprocessing.\n      output_width: The width of the image after preprocessing.\n      is_training: `True` if we\'re preprocessing the image for training and\n        `False` otherwise.\n      resize_side_min: The lower bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, then this value\n        is used for rescaling.\n      resize_side_max: The upper bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, this value is\n         ignored. Otherwise, the resize side is sampled from\n         [resize_size_min, resize_size_max].\n\n    Returns:\n      A preprocessed image.\n    """"""\n    if is_training:\n        return preprocess_for_train(image, labels, bboxes,\n                                    out_shape=out_shape,\n                                    data_format=data_format)\n    else:\n        return preprocess_for_eval(image, labels, bboxes,\n                                   out_shape=out_shape,\n                                   data_format=data_format,\n                                   **kwargs)\n'"
preprocessing/tf_image.py,16,"b'# Copyright 2015 The TensorFlow Authors and Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Custom image operations.\nMost of the following methods extend TensorFlow image library, and part of\nthe code is shameless copy-paste of the former!\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_image_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\n\n\n# =========================================================================== #\n# Modification of TensorFlow image routines.\n# =========================================================================== #\ndef _assert(cond, ex_type, msg):\n    """"""A polymorphic assert, works with tensors and boolean expressions.\n    If `cond` is not a tensor, behave like an ordinary assert statement, except\n    that a empty list is returned. If `cond` is a tensor, return a list\n    containing a single TensorFlow assert op.\n    Args:\n      cond: Something evaluates to a boolean value. May be a tensor.\n      ex_type: The exception class to use.\n      msg: The error message.\n    Returns:\n      A list, containing at most one assert op.\n    """"""\n    if _is_tensor(cond):\n        return [control_flow_ops.Assert(cond, [msg])]\n    else:\n        if not cond:\n            raise ex_type(msg)\n        else:\n            return []\n\n\ndef _is_tensor(x):\n    """"""Returns `True` if `x` is a symbolic tensor-like object.\n    Args:\n      x: A python object to check.\n    Returns:\n      `True` if `x` is a `tf.Tensor` or `tf.Variable`, otherwise `False`.\n    """"""\n    return isinstance(x, (ops.Tensor, variables.Variable))\n\n\ndef _ImageDimensions(image):\n    """"""Returns the dimensions of an image tensor.\n    Args:\n      image: A 3-D Tensor of shape `[height, width, channels]`.\n    Returns:\n      A list of `[height, width, channels]` corresponding to the dimensions of the\n        input image.  Dimensions that are statically known are python integers,\n        otherwise they are integer scalar tensors.\n    """"""\n    if image.get_shape().is_fully_defined():\n        return image.get_shape().as_list()\n    else:\n        static_shape = image.get_shape().with_rank(3).as_list()\n        dynamic_shape = array_ops.unstack(array_ops.shape(image), 3)\n        return [s if s is not None else d\n                for s, d in zip(static_shape, dynamic_shape)]\n\n\ndef _Check3DImage(image, require_static=True):\n    """"""Assert that we are working with properly shaped image.\n    Args:\n      image: 3-D Tensor of shape [height, width, channels]\n        require_static: If `True`, requires that all dimensions of `image` are\n        known and non-zero.\n    Raises:\n      ValueError: if `image.shape` is not a 3-vector.\n    Returns:\n      An empty list, if `image` has fully defined dimensions. Otherwise, a list\n        containing an assert op is returned.\n    """"""\n    try:\n        image_shape = image.get_shape().with_rank(3)\n    except ValueError:\n        raise ValueError(""\'image\' must be three-dimensional."")\n    if require_static and not image_shape.is_fully_defined():\n        raise ValueError(""\'image\' must be fully defined."")\n    if any(x == 0 for x in image_shape):\n        raise ValueError(""all dims of \'image.shape\' must be > 0: %s"" %\n                         image_shape)\n    if not image_shape.is_fully_defined():\n        return [check_ops.assert_positive(array_ops.shape(image),\n                                          [""all dims of \'image.shape\' ""\n                                           ""must be > 0.""])]\n    else:\n        return []\n\n\ndef fix_image_flip_shape(image, result):\n    """"""Set the shape to 3 dimensional if we don\'t know anything else.\n    Args:\n      image: original image size\n      result: flipped or transformed image\n    Returns:\n      An image whose shape is at least None,None,None.\n    """"""\n    image_shape = image.get_shape()\n    if image_shape == tensor_shape.unknown_shape():\n        result.set_shape([None, None, None])\n    else:\n        result.set_shape(image_shape)\n    return result\n\n\n# =========================================================================== #\n# Image + BBoxes methods: cropping, resizing, flipping, ...\n# =========================================================================== #\ndef bboxes_crop_or_pad(bboxes,\n                       height, width,\n                       offset_y, offset_x,\n                       target_height, target_width):\n    """"""Adapt bounding boxes to crop or pad operations.\n    Coordinates are always supposed to be relative to the image.\n\n    Arguments:\n      bboxes: Tensor Nx4 with bboxes coordinates [y_min, x_min, y_max, x_max];\n      height, width: Original image dimension;\n      offset_y, offset_x: Offset to apply,\n        negative if cropping, positive if padding;\n      target_height, target_width: Target dimension after cropping / padding.\n    """"""\n    with tf.name_scope(\'bboxes_crop_or_pad\'):\n        # Rescale bounding boxes in pixels.\n        scale = tf.cast(tf.stack([height, width, height, width]), bboxes.dtype)\n        bboxes = bboxes * scale\n        # Add offset.\n        offset = tf.cast(tf.stack([offset_y, offset_x, offset_y, offset_x]), bboxes.dtype)\n        bboxes = bboxes + offset\n        # Rescale to target dimension.\n        scale = tf.cast(tf.stack([target_height, target_width,\n                                  target_height, target_width]), bboxes.dtype)\n        bboxes = bboxes / scale\n        return bboxes\n\n\ndef resize_image_bboxes_with_crop_or_pad(image, bboxes,\n                                         target_height, target_width):\n    """"""Crops and/or pads an image to a target width and height.\n    Resizes an image to a target width and height by either centrally\n    cropping the image or padding it evenly with zeros.\n\n    If `width` or `height` is greater than the specified `target_width` or\n    `target_height` respectively, this op centrally crops along that dimension.\n    If `width` or `height` is smaller than the specified `target_width` or\n    `target_height` respectively, this op centrally pads with 0 along that\n    dimension.\n    Args:\n      image: 3-D tensor of shape `[height, width, channels]`\n      target_height: Target height.\n      target_width: Target width.\n    Raises:\n      ValueError: if `target_height` or `target_width` are zero or negative.\n    Returns:\n      Cropped and/or padded image of shape\n        `[target_height, target_width, channels]`\n    """"""\n    with tf.name_scope(\'resize_with_crop_or_pad\'):\n        image = ops.convert_to_tensor(image, name=\'image\')\n\n        assert_ops = []\n        assert_ops += _Check3DImage(image, require_static=False)\n        assert_ops += _assert(target_width > 0, ValueError,\n                              \'target_width must be > 0.\')\n        assert_ops += _assert(target_height > 0, ValueError,\n                              \'target_height must be > 0.\')\n\n        image = control_flow_ops.with_dependencies(assert_ops, image)\n        # `crop_to_bounding_box` and `pad_to_bounding_box` have their own checks.\n        # Make sure our checks come first, so that error messages are clearer.\n        if _is_tensor(target_height):\n            target_height = control_flow_ops.with_dependencies(\n                assert_ops, target_height)\n        if _is_tensor(target_width):\n            target_width = control_flow_ops.with_dependencies(assert_ops, target_width)\n\n        def max_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.maximum(x, y)\n            else:\n                return max(x, y)\n\n        def min_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.minimum(x, y)\n            else:\n                return min(x, y)\n\n        def equal_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.equal(x, y)\n            else:\n                return x == y\n\n        height, width, _ = _ImageDimensions(image)\n        width_diff = target_width - width\n        offset_crop_width = max_(-width_diff // 2, 0)\n        offset_pad_width = max_(width_diff // 2, 0)\n\n        height_diff = target_height - height\n        offset_crop_height = max_(-height_diff // 2, 0)\n        offset_pad_height = max_(height_diff // 2, 0)\n\n        # Maybe crop if needed.\n        height_crop = min_(target_height, height)\n        width_crop = min_(target_width, width)\n        cropped = tf.image.crop_to_bounding_box(image, offset_crop_height, offset_crop_width,\n                                                height_crop, width_crop)\n        bboxes = bboxes_crop_or_pad(bboxes,\n                                    height, width,\n                                    -offset_crop_height, -offset_crop_width,\n                                    height_crop, width_crop)\n        # Maybe pad if needed.\n        resized = tf.image.pad_to_bounding_box(cropped, offset_pad_height, offset_pad_width,\n                                               target_height, target_width)\n        bboxes = bboxes_crop_or_pad(bboxes,\n                                    height_crop, width_crop,\n                                    offset_pad_height, offset_pad_width,\n                                    target_height, target_width)\n\n        # In theory all the checks below are redundant.\n        if resized.get_shape().ndims is None:\n            raise ValueError(\'resized contains no shape.\')\n\n        resized_height, resized_width, _ = _ImageDimensions(resized)\n\n        assert_ops = []\n        assert_ops += _assert(equal_(resized_height, target_height), ValueError,\n                              \'resized height is not correct.\')\n        assert_ops += _assert(equal_(resized_width, target_width), ValueError,\n                              \'resized width is not correct.\')\n\n        resized = control_flow_ops.with_dependencies(assert_ops, resized)\n        return resized, bboxes\n\n\ndef resize_image(image, size,\n                 method=tf.image.ResizeMethod.BILINEAR,\n                 align_corners=False):\n    """"""Resize an image and bounding boxes.\n    """"""\n    # Resize image.\n    with tf.name_scope(\'resize_image\'):\n        height, width, channels = _ImageDimensions(image)\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize_images(image, size,\n                                       method, align_corners)\n        image = tf.reshape(image, tf.stack([size[0], size[1], channels]))\n        return image\n\n\ndef random_flip_left_right(image, bboxes, seed=None):\n    """"""Random flip left-right of an image and its bounding boxes.\n    """"""\n    def flip_bboxes(bboxes):\n        """"""Flip bounding boxes coordinates.\n        """"""\n        bboxes = tf.stack([bboxes[:, 0], 1 - bboxes[:, 3],\n                           bboxes[:, 2], 1 - bboxes[:, 1]], axis=-1)\n        return bboxes\n\n    # Random flip. Tensorflow implementation.\n    with tf.name_scope(\'random_flip_left_right\'):\n        image = ops.convert_to_tensor(image, name=\'image\')\n        _Check3DImage(image, require_static=False)\n        uniform_random = random_ops.random_uniform([], 0, 1.0, seed=seed)\n        mirror_cond = math_ops.less(uniform_random, .5)\n        \n        #debugging info\n#         mirror_cond = tf.Print(mirror_cond, [mirror_cond], \'flipped image\')\n        # Flip image.\n        result = control_flow_ops.cond(mirror_cond,\n                                       lambda: array_ops.reverse_v2(image, [1]),\n                                       lambda: image)\n        # Flip bboxes.\n        bboxes = control_flow_ops.cond(mirror_cond,\n                                       lambda: flip_bboxes(bboxes),\n                                       lambda: bboxes)\n        return fix_image_flip_shape(image, result), bboxes\n\n'"
preprocessing/vgg_preprocessing.py,54,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n    Very Deep Convolutional Networks For Large-Scale Image Recognition\n    Karen Simonyan and Andrew Zisserman\n    arXiv technical report, 2015\n    PDF: http://arxiv.org/pdf/1409.1556.pdf\n    ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n    CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n    """"""Crops the given image using the provided offsets and sizes.\n\n    Note that the method doesn\'t assume we know the input image size but it does\n    assume we know the input image rank.\n\n    Args:\n        image: an image of shape [height, width, channels].\n        offset_height: a scalar tensor indicating the height offset.\n        offset_width: a scalar tensor indicating the width offset.\n        crop_height: the height of the cropped image.\n        crop_width: the width of the cropped image.\n\n    Returns:\n        the cropped (and resized) image.\n\n    Raises:\n        InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n            less than the crop size.\n    """"""\n    original_shape = tf.shape(image)\n\n    rank_assertion = tf.Assert(\n            tf.equal(tf.rank(image), 3),\n            [\'Rank of image must be equal to 3.\'])\n    cropped_shape = control_flow_ops.with_dependencies(\n            [rank_assertion],\n            tf.pack([crop_height, crop_width, original_shape[2]]))\n\n    size_assertion = tf.Assert(\n            tf.logical_and(\n                    tf.greater_equal(original_shape[0], crop_height),\n                    tf.greater_equal(original_shape[1], crop_width)),\n            [\'Crop size greater than the image size.\'])\n\n    offsets = tf.to_int32(tf.pack([offset_height, offset_width, 0]))\n\n    # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n    # define the crop size.\n    image = control_flow_ops.with_dependencies(\n            [size_assertion],\n            tf.slice(image, offsets, cropped_shape))\n    return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n    """"""Crops the given list of images.\n\n    The function applies the same crop to each image in the list. This can be\n    effectively applied when there are multiple image inputs of the same\n    dimension such as:\n\n        image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n    Args:\n        image_list: a list of image tensors of the same dimension but possibly\n            varying channel.\n        crop_height: the new height.\n        crop_width: the new width.\n\n    Returns:\n        the image_list with cropped images.\n\n    Raises:\n        ValueError: if there are multiple image inputs provided with different size\n            or the images are smaller than the crop dimensions.\n    """"""\n    if not image_list:\n        raise ValueError(\'Empty image_list.\')\n\n    # Compute the rank assertions.\n    rank_assertions = []\n    for i in range(len(image_list)):\n        image_rank = tf.rank(image_list[i])\n        rank_assert = tf.Assert(\n                tf.equal(image_rank, 3),\n                [\'Wrong rank for tensor  %s [expected] [actual]\',\n                 image_list[i].name, 3, image_rank])\n        rank_assertions.append(rank_assert)\n\n    image_shape = control_flow_ops.with_dependencies(\n            [rank_assertions[0]],\n            tf.shape(image_list[0]))\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    crop_size_assert = tf.Assert(\n            tf.logical_and(\n                    tf.greater_equal(image_height, crop_height),\n                    tf.greater_equal(image_width, crop_width)),\n            [\'Crop size greater than the image size.\'])\n\n    asserts = [rank_assertions[0], crop_size_assert]\n\n    for i in range(1, len(image_list)):\n        image = image_list[i]\n        asserts.append(rank_assertions[i])\n        shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                                   tf.shape(image))\n        height = shape[0]\n        width = shape[1]\n\n        height_assert = tf.Assert(\n                tf.equal(height, image_height),\n                [\'Wrong height for tensor %s [expected][actual]\',\n                 image.name, height, image_height])\n        width_assert = tf.Assert(\n                tf.equal(width, image_width),\n                [\'Wrong width for tensor %s [expected][actual]\',\n                 image.name, width, image_width])\n        asserts.extend([height_assert, width_assert])\n\n    # Create a random bounding box.\n    #\n    # Use tf.random_uniform and not numpy.random.rand as doing the former would\n    # generate random numbers at graph eval time, unlike the latter which\n    # generates random numbers at graph definition time.\n    max_offset_height = control_flow_ops.with_dependencies(\n            asserts, tf.reshape(image_height - crop_height + 1, []))\n    max_offset_width = control_flow_ops.with_dependencies(\n            asserts, tf.reshape(image_width - crop_width + 1, []))\n    offset_height = tf.random_uniform(\n            [], maxval=max_offset_height, dtype=tf.int32)\n    offset_width = tf.random_uniform(\n            [], maxval=max_offset_width, dtype=tf.int32)\n\n    return [_crop(image, offset_height, offset_width,\n                  crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n    """"""Performs central crops of the given image list.\n\n    Args:\n        image_list: a list of image tensors of the same dimension but possibly\n            varying channel.\n        crop_height: the height of the image following the crop.\n        crop_width: the width of the image following the crop.\n\n    Returns:\n        the list of cropped images.\n    """"""\n    outputs = []\n    for image in image_list:\n        image_height = tf.shape(image)[0]\n        image_width = tf.shape(image)[1]\n\n        offset_height = (image_height - crop_height) / 2\n        offset_width = (image_width - crop_width) / 2\n\n        outputs.append(_crop(image, offset_height, offset_width,\n                             crop_height, crop_width))\n    return outputs\n\n\ndef _mean_image_subtraction(image, means):\n    """"""Subtracts the given means from each image channel.\n\n    For example:\n        means = [123.68, 116.779, 103.939]\n        image = _mean_image_subtraction(image, means)\n\n    Note that the rank of `image` must be known.\n\n    Args:\n        image: a tensor of size [height, width, C].\n        means: a C-vector of values to subtract from each channel.\n\n    Returns:\n        the centered image.\n\n    Raises:\n        ValueError: If the rank of `image` is unknown, if `image` has a rank other\n            than three or if the number of channels in `image` doesn\'t match the\n            number of values in `means`.\n    """"""\n    if image.get_shape().ndims != 3:\n        raise ValueError(\'Input must be of size [height, width, C>0]\')\n    num_channels = image.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n        raise ValueError(\'len(means) must match the number of channels\')\n\n    channels = tf.split(2, num_channels, image)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(channels, axis=2)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n    """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n    Computes new shape with the smallest side equal to `smallest_side` while\n    preserving the original aspect ratio.\n\n    Args:\n        height: an int32 scalar tensor indicating the current height.\n        width: an int32 scalar tensor indicating the current width.\n        smallest_side: A python integer or scalar `Tensor` indicating the size of\n            the smallest side after resize.\n\n    Returns:\n        new_height: an int32 scalar tensor indicating the new height.\n        new_width: and int32 scalar tensor indicating the new width.\n    """"""\n    smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n    height = tf.to_float(height)\n    width = tf.to_float(width)\n    smallest_side = tf.to_float(smallest_side)\n\n    scale = tf.cond(tf.greater(height, width),\n                    lambda: smallest_side / width,\n                    lambda: smallest_side / height)\n    new_height = tf.to_int32(height * scale)\n    new_width = tf.to_int32(width * scale)\n    return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n    """"""Resize images preserving the original aspect ratio.\n\n    Args:\n        image: A 3-D image `Tensor`.\n        smallest_side: A python integer or scalar `Tensor` indicating the size of\n            the smallest side after resize.\n\n    Returns:\n        resized_image: A 3-D tensor containing the resized image.\n    """"""\n    smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n    shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n    new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n    image = tf.expand_dims(image, 0)\n    resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                             align_corners=False)\n    resized_image = tf.squeeze(resized_image)\n    resized_image.set_shape([None, None, 3])\n    return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n    """"""Preprocesses the given image for training.\n\n    Note that the actual resizing scale is sampled from\n        [`resize_size_min`, `resize_size_max`].\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        output_height: The height of the image after preprocessing.\n        output_width: The width of the image after preprocessing.\n        resize_side_min: The lower bound for the smallest side of the image for\n            aspect-preserving resizing.\n        resize_side_max: The upper bound for the smallest side of the image for\n            aspect-preserving resizing.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    resize_side = tf.random_uniform(\n            [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n    image = _aspect_preserving_resize(image, resize_side)\n    image = _random_crop([image], output_height, output_width)[0]\n    image.set_shape([output_height, output_width, 3])\n    image = tf.to_float(image)\n    image = tf.image.random_flip_left_right(image)\n    return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n    """"""Preprocesses the given image for evaluation.\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        output_height: The height of the image after preprocessing.\n        output_width: The width of the image after preprocessing.\n        resize_side: The smallest side of the image for aspect-preserving resizing.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    image = _aspect_preserving_resize(image, resize_side)\n    image = _central_crop([image], output_height, output_width)[0]\n    image.set_shape([output_height, output_width, 3])\n    image = tf.to_float(image)\n    return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n    """"""Preprocesses the given image.\n\n    Args:\n      image: A `Tensor` representing an image of arbitrary size.\n      output_height: The height of the image after preprocessing.\n      output_width: The width of the image after preprocessing.\n      is_training: `True` if we\'re preprocessing the image for training and\n        `False` otherwise.\n      resize_side_min: The lower bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, then this value\n        is used for rescaling.\n      resize_side_max: The upper bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, this value is\n         ignored. Otherwise, the resize side is sampled from\n         [resize_size_min, resize_size_max].\n\n    Returns:\n        A preprocessed image.\n    """"""\n    if is_training:\n        return preprocess_for_train(image, output_height, output_width,\n                                    resize_side_min, resize_side_max)\n    else:\n        return preprocess_for_eval(image, output_height, output_width,\n                                   resize_side_min)\n'"
tf_extended/__init__.py,0,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional metrics.\n""""""\n\n# pylint: disable=unused-import,line-too-long,g-importing-member,wildcard-import\nfrom tf_extended.metrics import *\nfrom tf_extended.tensors import *\nfrom tf_extended.bboxes import *\nfrom tf_extended.image import *\nfrom tf_extended.math import *\n\n'"
tf_extended/bboxes.py,108,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional bounding boxes methods.\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_extended import tensors as tfe_tensors\nfrom tf_extended import math as tfe_math\n\n\n# =========================================================================== #\n# Standard boxes algorithms.\n# =========================================================================== #\ndef bboxes_sort_all_classes(classes, scores, bboxes, top_k=400, scope=None):\n    """"""Sort bounding boxes by decreasing order and keep only the top_k.\n    Assume the input Tensors mix-up objects with different classes.\n    Assume a batch-type input.\n\n    Args:\n      classes: Batch x N Tensor containing integer classes.\n      scores: Batch x N Tensor containing float scores.\n      bboxes: Batch x N x 4 Tensor containing boxes coordinates.\n      top_k: Top_k boxes to keep.\n    Return:\n      classes, scores, bboxes: Sorted tensors of shape Batch x Top_k.\n    """"""\n    with tf.name_scope(scope, \'bboxes_sort\', [classes, scores, bboxes]):\n        scores, idxes = tf.nn.top_k(scores, k=top_k, sorted=True)\n\n        # Trick to be able to use tf.gather: map for each element in the batch.\n        def fn_gather(classes, bboxes, idxes):\n            cl = tf.gather(classes, idxes)\n            bb = tf.gather(bboxes, idxes)\n            return [cl, bb]\n        r = tf.map_fn(lambda x: fn_gather(x[0], x[1], x[2]),\n                      [classes, bboxes, idxes],\n                      dtype=[classes.dtype, bboxes.dtype],\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=False,\n                      infer_shape=True)\n        classes = r[0]\n        bboxes = r[1]\n        return classes, scores, bboxes\n\n\ndef bboxes_sort(scores, bboxes, top_k=400, scope=None):\n    """"""Sort bounding boxes by decreasing order and keep only the top_k.\n    If inputs are dictionnaries, assume every key is a different class.\n    Assume a batch-type input.\n\n    Args:\n      scores: Batch x N Tensor/Dictionary containing float scores.\n      bboxes: Batch x N x 4 Tensor/Dictionary containing boxes coordinates.\n      top_k: Top_k boxes to keep.\n    Return:\n      scores, bboxes: Sorted Tensors/Dictionaries of shape Batch x Top_k x 1|4.\n    """"""\n    # Dictionaries as inputs.\n    if isinstance(scores, dict) or isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_sort_dict\'):\n            d_scores = {}\n            d_bboxes = {}\n            for c in scores.keys():\n                s, b = bboxes_sort(scores[c], bboxes[c], top_k=top_k)\n                d_scores[c] = s\n                d_bboxes[c] = b\n            return d_scores, d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(scope, \'bboxes_sort\', [scores, bboxes]):\n        # Sort scores...\n        scores, idxes = tf.nn.top_k(scores, k=top_k, sorted=True)\n\n        # Trick to be able to use tf.gather: map for each element in the first dim.\n        def fn_gather(bboxes, idxes):\n            bb = tf.gather(bboxes, idxes)\n            return [bb]\n        r = tf.map_fn(lambda x: fn_gather(x[0], x[1]),\n                      [bboxes, idxes],\n                      dtype=[bboxes.dtype],\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=False,\n                      infer_shape=True)\n        bboxes = r[0]\n        return scores, bboxes\n\n\ndef bboxes_clip(bbox_ref, bboxes, scope=None):\n    """"""Clip bounding boxes to a reference box.\n    Batch-compatible if the first dimension of `bbox_ref` and `bboxes`\n    can be broadcasted.\n\n    Args:\n      bbox_ref: Reference bounding box. Nx4 or 4 shaped-Tensor;\n      bboxes: Bounding boxes to clip. Nx4 or 4 shaped-Tensor or dictionary.\n    Return:\n      Clipped bboxes.\n    """"""\n    # Bboxes is dictionary.\n    if isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_clip_dict\'):\n            d_bboxes = {}\n            for c in bboxes.keys():\n                d_bboxes[c] = bboxes_clip(bbox_ref, bboxes[c])\n            return d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(scope, \'bboxes_clip\'):\n        # Easier with transposed bboxes. Especially for broadcasting.\n        bbox_ref = tf.transpose(bbox_ref)\n        bboxes = tf.transpose(bboxes)\n        # Intersection bboxes and reference bbox.\n        ymin = tf.maximum(bboxes[0], bbox_ref[0])\n        xmin = tf.maximum(bboxes[1], bbox_ref[1])\n        ymax = tf.minimum(bboxes[2], bbox_ref[2])\n        xmax = tf.minimum(bboxes[3], bbox_ref[3])\n        bboxes = tf.transpose(tf.stack([ymin, xmin, ymax, xmax], axis=0))\n        return bboxes\n\n\ndef bboxes_resize(bbox_ref, bboxes, name=None):\n    """"""Resize bounding boxes based on a reference bounding box,\n    assuming that the latter is [0, 0, 1, 1] after transform. Useful for\n    updating a collection of boxes after cropping an image.\n    """"""\n    # Bboxes is dictionary.\n    if isinstance(bboxes, dict):\n        with tf.name_scope(name, \'bboxes_resize_dict\'):\n            d_bboxes = {}\n            for c in bboxes.keys():\n                d_bboxes[c] = bboxes_resize(bbox_ref, bboxes[c])\n            return d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(name, \'bboxes_resize\'):\n        # Translate.\n        v = tf.stack([bbox_ref[0], bbox_ref[1], bbox_ref[0], bbox_ref[1]])\n        bboxes = bboxes - v\n        # Scale.\n        s = tf.stack([bbox_ref[2] - bbox_ref[0],\n                      bbox_ref[3] - bbox_ref[1],\n                      bbox_ref[2] - bbox_ref[0],\n                      bbox_ref[3] - bbox_ref[1]])\n        bboxes = bboxes / s\n        return bboxes\n\n\ndef bboxes_nms(scores, bboxes, nms_threshold=0.5, keep_top_k=200, scope=None):\n    """"""Apply non-maximum selection to bounding boxes. In comparison to TF\n    implementation, use classes information for matching.\n    Should only be used on single-entries. Use batch version otherwise.\n\n    Args:\n      scores: N Tensor containing float scores.\n      bboxes: N x 4 Tensor containing boxes coordinates.\n      nms_threshold: Matching threshold in NMS algorithm;\n      keep_top_k: Number of total object to keep after NMS.\n    Return:\n      classes, scores, bboxes Tensors, sorted by score.\n        Padded with zero if necessary.\n    """"""\n    with tf.name_scope(scope, \'bboxes_nms_single\', [scores, bboxes]):\n        # Apply NMS algorithm.\n        idxes = tf.image.non_max_suppression(bboxes, scores,\n                                             keep_top_k, nms_threshold)\n        scores = tf.gather(scores, idxes)\n        bboxes = tf.gather(bboxes, idxes)\n        # Pad results.\n        scores = tfe_tensors.pad_axis(scores, 0, keep_top_k, axis=0)\n        bboxes = tfe_tensors.pad_axis(bboxes, 0, keep_top_k, axis=0)\n        return scores, bboxes\n\n\ndef bboxes_nms_batch(scores, bboxes, nms_threshold=0.5, keep_top_k=200,\n                     scope=None):\n    """"""Apply non-maximum selection to bounding boxes. In comparison to TF\n    implementation, use classes information for matching.\n    Use only on batched-inputs. Use zero-padding in order to batch output\n    results.\n\n    Args:\n      scores: Batch x N Tensor/Dictionary containing float scores.\n      bboxes: Batch x N x 4 Tensor/Dictionary containing boxes coordinates.\n      nms_threshold: Matching threshold in NMS algorithm;\n      keep_top_k: Number of total object to keep after NMS.\n    Return:\n      scores, bboxes Tensors/Dictionaries, sorted by score.\n        Padded with zero if necessary.\n    """"""\n    # Dictionaries as inputs.\n    if isinstance(scores, dict) or isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_nms_batch_dict\'):\n            d_scores = {}\n            d_bboxes = {}\n            for c in scores.keys():\n                s, b = bboxes_nms_batch(scores[c], bboxes[c],\n                                        nms_threshold=nms_threshold,\n                                        keep_top_k=keep_top_k)\n                d_scores[c] = s\n                d_bboxes[c] = b\n            return d_scores, d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(scope, \'bboxes_nms_batch\'):\n        r = tf.map_fn(lambda x: bboxes_nms(x[0], x[1],\n                                           nms_threshold, keep_top_k),\n                      (scores, bboxes),\n                      dtype=(scores.dtype, bboxes.dtype),\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=False,\n                      infer_shape=True)\n        scores, bboxes = r\n        return scores, bboxes\n\n\n# def bboxes_fast_nms(classes, scores, bboxes,\n#                     nms_threshold=0.5, eta=3., num_classes=21,\n#                     pad_output=True, scope=None):\n#     with tf.name_scope(scope, \'bboxes_fast_nms\',\n#                        [classes, scores, bboxes]):\n\n#         nms_classes = tf.zeros((0,), dtype=classes.dtype)\n#         nms_scores = tf.zeros((0,), dtype=scores.dtype)\n#         nms_bboxes = tf.zeros((0, 4), dtype=bboxes.dtype)\n\n\ndef bboxes_matching(label, scores, bboxes,\n                    glabels, gbboxes, gdifficults,\n                    matching_threshold=0.5, scope=None):\n    """"""Matching a collection of detected boxes with groundtruth values.\n    Does not accept batched-inputs.\n    The algorithm goes as follows: for every detected box, check\n    if one grountruth box is matching. If none, then considered as False Positive.\n    If the grountruth box is already matched with another one, it also counts\n    as a False Positive. We refer the Pascal VOC documentation for the details.\n\n    Args:\n      rclasses, rscores, rbboxes: N(x4) Tensors. Detected objects, sorted by score;\n      glabels, gbboxes: Groundtruth bounding boxes. May be zero padded, hence\n        zero-class objects are ignored.\n      matching_threshold: Threshold for a positive match.\n    Return: Tuple of:\n       n_gbboxes: Scalar Tensor with number of groundtruth boxes (may difer from\n         size because of zero padding).\n       tp_match: (N,)-shaped boolean Tensor containing with True Positives.\n       fp_match: (N,)-shaped boolean Tensor containing with False Positives.\n    """"""\n    with tf.name_scope(scope, \'bboxes_matching_single\',\n                       [scores, bboxes, glabels, gbboxes]):\n        rsize = tf.size(scores)\n        rshape = tf.shape(scores)\n        rlabel = tf.cast(label, glabels.dtype)\n        # Number of groundtruth boxes.\n        gdifficults = tf.cast(gdifficults, tf.bool)\n        \n        #At the preprocessing stage, some groud truth bbox and lables may be cliped/sampled\n        #we need to do the same gdifficults\n        #Also we might have added padding during data queuing, we remove the padding here as well\n        n_valid_glabels = tf.count_nonzero(glabels,dtype=tf.int32)\n#         gdifficults = tf.Print(gdifficults, [tf.size(gdifficults), glabels, tf.size(glabels)], ""value size"")\n#         n_valid_glabels = tf.Print(n_valid_glabels, [n_valid_glabels], ""valid size"")\n        gdifficults = gdifficults[:n_valid_glabels]\n        glabels = glabels[:n_valid_glabels]\n        gbboxes = gbboxes[:n_valid_glabels]\n        \n        \n        n_gbboxes = tf.count_nonzero(tf.logical_and(tf.equal(glabels, label),\n                                                    tf.logical_not(gdifficults)))\n        # Grountruth matching arrays.\n        gmatch = tf.zeros(tf.shape(glabels), dtype=tf.bool)\n        grange = tf.range(tf.size(glabels), dtype=tf.int32)\n        # True/False positive matching TensorArrays.\n        sdtype = tf.bool\n        ta_tp_bool = tf.TensorArray(sdtype, size=rsize, dynamic_size=False, infer_shape=True)\n        ta_fp_bool = tf.TensorArray(sdtype, size=rsize, dynamic_size=False, infer_shape=True)\n        \n        \n\n        # Loop over returned objects.\n        def m_condition(i, ta_tp, ta_fp, gmatch):\n            r = tf.less(i, rsize)\n            return r\n        def m_body_no_lables(i, ta_tp, ta_fp, gmatch):\n            \n            ta_tp = ta_tp.write(i, False)\n            ta_fp = ta_fp.write(i, True)\n           \n            return [i+1, ta_tp, ta_fp, gmatch]\n        def m_body_normal(i, ta_tp, ta_fp, gmatch):\n            # Jaccard score with groundtruth bboxes.\n            rbbox = bboxes[i]\n            jaccard = bboxes_jaccard(rbbox, gbboxes)\n            jaccard = jaccard * tf.cast(tf.equal(glabels, rlabel), dtype=jaccard.dtype)\n\n            # Best fit, checking it\'s above threshold.\n            idxmax = tf.cast(tf.argmax(jaccard, axis=0), tf.int32)\n            jcdmax = jaccard[idxmax]\n            match = jcdmax > matching_threshold\n            existing_match = gmatch[idxmax]\n            not_difficult = tf.logical_not(gdifficults[idxmax])\n\n            # TP: match & no previous match and FP: previous match | no match.\n            # If difficult: no record, i.e FP=False and TP=False.\n            tp = tf.logical_and(not_difficult,\n                                tf.logical_and(match, tf.logical_not(existing_match)))\n            ta_tp = ta_tp.write(i, tp)\n            fp = tf.logical_and(not_difficult,\n                                tf.logical_or(existing_match, tf.logical_not(match)))\n            ta_fp = ta_fp.write(i, fp)\n            # Update grountruth match.\n            mask = tf.logical_and(tf.equal(grange, idxmax),\n                                  tf.logical_and(not_difficult, match))\n            gmatch = tf.logical_or(gmatch, mask)\n            return [i+1, ta_tp, ta_fp, gmatch]\n        def m_body(i, ta_tp, ta_fp, gmatch):\n            \n\n            return tf.cond(tf.equal(n_valid_glabels, 0), lambda: m_body_no_lables(i, ta_tp, ta_fp, gmatch), lambda: m_body_normal(i, ta_tp, ta_fp, gmatch))\n        # Main loop definition.\n        i = 0\n        [i, ta_tp_bool, ta_fp_bool, gmatch] = \\\n            tf.while_loop(m_condition, m_body,\n                          [i, ta_tp_bool, ta_fp_bool, gmatch],\n                          parallel_iterations=1,\n                          back_prop=False)\n        # TensorArrays to Tensors and reshape.\n        tp_match = tf.reshape(ta_tp_bool.stack(), rshape)\n        fp_match = tf.reshape(ta_fp_bool.stack(), rshape)\n\n        # Some debugging information...\n        # tp_match = tf.Print(tp_match,\n        #                     [n_gbboxes,\n        #                      tf.reduce_sum(tf.cast(tp_match, tf.int64)),\n        #                      tf.reduce_sum(tf.cast(fp_match, tf.int64)),\n        #                      tf.reduce_sum(tf.cast(gmatch, tf.int64))],\n        #                     \'Matching (NG, TP, FP, GM): \')\n        return n_gbboxes, tp_match, fp_match\n\n\ndef bboxes_matching_batch(labels, scores, bboxes,\n                          glabels, gbboxes, gdifficults,\n                          matching_threshold=0.5, scope=None):\n    """"""Matching a collection of detected boxes with groundtruth values.\n    Batched-inputs version.\n\n    Args:\n      rclasses, rscores, rbboxes: BxN(x4) Tensors. Detected objects, sorted by score;\n      glabels, gbboxes: Groundtruth bounding boxes. May be zero padded, hence\n        zero-class objects are ignored.\n      matching_threshold: Threshold for a positive match.\n    Return: Tuple or Dictionaries with:\n       n_gbboxes: Scalar Tensor with number of groundtruth boxes (may difer from\n         size because of zero padding).\n       tp: (B, N)-shaped boolean Tensor containing with True Positives.\n       fp: (B, N)-shaped boolean Tensor containing with False Positives.\n    """"""\n    # Dictionaries as inputs.\n    if isinstance(scores, dict) or isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_matching_batch_dict\'):\n            d_n_gbboxes = {}\n            d_tp = {}\n            d_fp = {}\n            for c in labels:\n                n, tp, fp, _ = bboxes_matching_batch(c, scores[c], bboxes[c],\n                                                     glabels, gbboxes, gdifficults,\n                                                     matching_threshold)\n                d_n_gbboxes[c] = n\n                d_tp[c] = tp\n                d_fp[c] = fp\n            return d_n_gbboxes, d_tp, d_fp, scores\n\n    with tf.name_scope(scope, \'bboxes_matching_batch\',\n                       [scores, bboxes, glabels, gbboxes]):\n        r = tf.map_fn(lambda x: bboxes_matching(labels, x[0], x[1],\n                                                x[2], x[3], x[4],\n                                                matching_threshold),\n                      (scores, bboxes, glabels, gbboxes, gdifficults),\n                      dtype=(tf.int64, tf.bool, tf.bool),\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=True,\n                      infer_shape=True)\n        return r[0], r[1], r[2], scores\n\n\n# =========================================================================== #\n# Some filteting methods.\n# =========================================================================== #\ndef bboxes_filter_center(labels, bboxes, margins=[0., 0., 0., 0.],\n                         scope=None):\n    """"""Filter out bounding boxes whose center are not in\n    the rectangle [0, 0, 1, 1] + margins. The margin Tensor\n    can be used to enforce or loosen this condition.\n\n    Return:\n      labels, bboxes: Filtered elements.\n    """"""\n    with tf.name_scope(scope, \'bboxes_filter\', [labels, bboxes]):\n        cy = (bboxes[:, 0] + bboxes[:, 2]) / 2.\n        cx = (bboxes[:, 1] + bboxes[:, 3]) / 2.\n        mask = tf.greater(cy, margins[0])\n        mask = tf.logical_and(mask, tf.greater(cx, margins[1]))\n        mask = tf.logical_and(mask, tf.less(cx, 1. + margins[2]))\n        mask = tf.logical_and(mask, tf.less(cx, 1. + margins[3]))\n        # Boolean masking...\n        labels = tf.boolean_mask(labels, mask)\n        bboxes = tf.boolean_mask(bboxes, mask)\n        return labels, bboxes\n\n\ndef bboxes_filter_overlap(labels, bboxes, threshold=0.5,\n                          scope=None):\n    """"""Filter out bounding boxes based on overlap with reference\n    box [0, 0, 1, 1].\n\n    Return:\n      labels, bboxes: Filtered elements.\n    """"""\n    with tf.name_scope(scope, \'bboxes_filter\', [labels, bboxes]):\n        scores = bboxes_intersection(tf.constant([0, 0, 1, 1], bboxes.dtype),\n                                     bboxes)\n        mask = scores > threshold\n        labels = tf.boolean_mask(labels, mask)\n        bboxes = tf.boolean_mask(bboxes, mask)\n        return labels, bboxes\n\n\ndef bboxes_filter_labels(labels, bboxes,\n                         out_labels=[], num_classes=np.inf,\n                         scope=None):\n    """"""Filter out labels from a collection. Typically used to get\n    of DontCare elements. Also remove elements based on the number of classes.\n\n    Return:\n      labels, bboxes: Filtered elements.\n    """"""\n    with tf.name_scope(scope, \'bboxes_filter_labels\', [labels, bboxes]):\n        mask = tf.greater_equal(labels, num_classes)\n        for l in labels:\n            mask = tf.logical_and(mask, tf.not_equal(labels, l))\n        labels = tf.boolean_mask(labels, mask)\n        bboxes = tf.boolean_mask(bboxes, mask)\n        return labels, bboxes\n\n\n# =========================================================================== #\n# Standard boxes computation.\n# =========================================================================== #\ndef bboxes_jaccard(bbox_ref, bboxes, name=None):\n    """"""Compute jaccard score between a reference box and a collection\n    of bounding boxes.\n\n    Args:\n      bbox_ref: (N, 4) or (4,) Tensor with reference bounding box(es).\n      bboxes: (N, 4) Tensor, collection of bounding boxes.\n    Return:\n      (N,) Tensor with Jaccard scores.\n    """"""\n    with tf.name_scope(name, \'bboxes_jaccard\'):\n        # Should be more efficient to first transpose.\n        bboxes = tf.transpose(bboxes)\n        bbox_ref = tf.transpose(bbox_ref)\n        # Intersection bbox and volume.\n        int_ymin = tf.maximum(bboxes[0], bbox_ref[0])\n        int_xmin = tf.maximum(bboxes[1], bbox_ref[1])\n        int_ymax = tf.minimum(bboxes[2], bbox_ref[2])\n        int_xmax = tf.minimum(bboxes[3], bbox_ref[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        # Volumes.\n        inter_vol = h * w\n        union_vol = -inter_vol \\\n            + (bboxes[2] - bboxes[0]) * (bboxes[3] - bboxes[1]) \\\n            + (bbox_ref[2] - bbox_ref[0]) * (bbox_ref[3] - bbox_ref[1])\n        jaccard = tfe_math.safe_divide(inter_vol, union_vol, \'jaccard\')\n        return jaccard\n\n\ndef bboxes_intersection(bbox_ref, bboxes, name=None):\n    """"""Compute relative intersection between a reference box and a\n    collection of bounding boxes. Namely, compute the quotient between\n    intersection area and box area.\n\n    Args:\n      bbox_ref: (N, 4) or (4,) Tensor with reference bounding box(es).\n      bboxes: (N, 4) Tensor, collection of bounding boxes.\n    Return:\n      (N,) Tensor with relative intersection.\n    """"""\n    with tf.name_scope(name, \'bboxes_intersection\'):\n        # Should be more efficient to first transpose.\n        bboxes = tf.transpose(bboxes)\n        bbox_ref = tf.transpose(bbox_ref)\n        # Intersection bbox and volume.\n        int_ymin = tf.maximum(bboxes[0], bbox_ref[0])\n        int_xmin = tf.maximum(bboxes[1], bbox_ref[1])\n        int_ymax = tf.minimum(bboxes[2], bbox_ref[2])\n        int_xmax = tf.minimum(bboxes[3], bbox_ref[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        # Volumes.\n        inter_vol = h * w\n        bboxes_vol = (bboxes[2] - bboxes[0]) * (bboxes[3] - bboxes[1])\n        scores = tfe_math.safe_divide(inter_vol, bboxes_vol, \'intersection\')\n        return scores\n'"
tf_extended/image.py,0,b''
tf_extended/math.py,6,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional math functions.\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\n\n\ndef safe_divide(numerator, denominator, name):\n    """"""Divides two values, returning 0 if the denominator is <= 0.\n    Args:\n      numerator: A real `Tensor`.\n      denominator: A real `Tensor`, with dtype matching `numerator`.\n      name: Name for the returned op.\n    Returns:\n      0 if `denominator` <= 0, else `numerator` / `denominator`\n    """"""\n    return tf.where(\n        math_ops.greater(denominator, 0),\n        math_ops.divide(numerator, denominator),\n        tf.zeros_like(numerator),\n        name=name)\n\n\ndef cummax(x, reverse=False, name=None):\n    """"""Compute the cumulative maximum of the tensor `x` along `axis`. This\n    operation is similar to the more classic `cumsum`. Only support 1D Tensor\n    for now.\n\n    Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,\n       `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,\n       `complex128`, `qint8`, `quint8`, `qint32`, `half`.\n       axis: A `Tensor` of type `int32` (default: 0).\n       reverse: A `bool` (default: False).\n       name: A name for the operation (optional).\n    Returns:\n    A `Tensor`. Has the same type as `x`.\n    """"""\n    with ops.name_scope(name, ""Cummax"", [x]) as name:\n        x = ops.convert_to_tensor(x, name=""x"")\n        # Not very optimal: should directly integrate reverse into tf.scan.\n        if reverse:\n            x = tf.reverse(x, axis=[0])\n        # \'Accumlating\' maximum: ensure it is always increasing.\n        cmax = tf.scan(lambda a, y: tf.maximum(a, y), x,\n                       initializer=None, parallel_iterations=1,\n                       back_prop=False, swap_memory=False)\n        if reverse:\n            cmax = tf.reverse(cmax, axis=[0])\n        return cmax\n'"
tf_extended/metrics.py,91,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional metrics.\n""""""\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.contrib.framework.python.ops import variables as contrib_variables\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\n\nfrom tf_extended import math as tfe_math\n\n\n# =========================================================================== #\n# TensorFlow utils\n# =========================================================================== #\ndef _create_local(name, shape, collections=None, validate_shape=True,\n                  dtype=dtypes.float32):\n    """"""Creates a new local variable.\n    Args:\n        name: The name of the new or existing variable.\n        shape: Shape of the new or existing variable.\n        collections: A list of collection names to which the Variable will be added.\n        validate_shape: Whether to validate the shape of the variable.\n        dtype: Data type of the variables.\n    Returns:\n        The created variable.\n    """"""\n    # Make sure local variables are added to tf.GraphKeys.LOCAL_VARIABLES\n    collections = list(collections or [])\n    collections += [ops.GraphKeys.LOCAL_VARIABLES]\n    return variables.Variable(\n            initial_value=array_ops.zeros(shape, dtype=dtype),\n            name=name,\n            trainable=False,\n            collections=collections,\n            validate_shape=validate_shape)\n\n\ndef _safe_div(numerator, denominator, name):\n    """"""Divides two values, returning 0 if the denominator is <= 0.\n    Args:\n      numerator: A real `Tensor`.\n      denominator: A real `Tensor`, with dtype matching `numerator`.\n      name: Name for the returned op.\n    Returns:\n      0 if `denominator` <= 0, else `numerator` / `denominator`\n    """"""\n    return tf.where(\n        math_ops.greater(denominator, 0),\n        math_ops.divide(numerator, denominator),\n        tf.zeros_like(numerator),\n        name=name)\n\n\ndef _broadcast_weights(weights, values):\n    """"""Broadcast `weights` to the same shape as `values`.\n    This returns a version of `weights` following the same broadcast rules as\n    `mul(weights, values)`. When computing a weighted average, use this function\n    to broadcast `weights` before summing them; e.g.,\n    `reduce_sum(w * v) / reduce_sum(_broadcast_weights(w, v))`.\n    Args:\n      weights: `Tensor` whose shape is broadcastable to `values`.\n      values: `Tensor` of any shape.\n    Returns:\n      `weights` broadcast to `values` shape.\n    """"""\n    weights_shape = weights.get_shape()\n    values_shape = values.get_shape()\n    if(weights_shape.is_fully_defined() and\n       values_shape.is_fully_defined() and\n       weights_shape.is_compatible_with(values_shape)):\n        return weights\n    return math_ops.mul(\n        weights, array_ops.ones_like(values), name=\'broadcast_weights\')\n\n\n# =========================================================================== #\n# TF Extended metrics: TP and FP arrays.\n# =========================================================================== #\ndef precision_recall(num_gbboxes, num_detections, tp, fp, scores,\n                     dtype=tf.float64, scope=None):\n    """"""Compute precision and recall from scores, true positives and false\n    positives booleans arrays\n    """"""\n    # Input dictionaries: dict outputs as streaming metrics.\n    if isinstance(scores, dict):\n        d_precision = {}\n        d_recall = {}\n        for c in num_gbboxes.keys():\n            scope = \'precision_recall_%s\' % c\n            p, r = precision_recall(num_gbboxes[c], num_detections[c],\n                                    tp[c], fp[c], scores[c],\n                                    dtype, scope)\n            d_precision[c] = p\n            d_recall[c] = r\n        return d_precision, d_recall\n\n    # Sort by score.\n    with tf.name_scope(scope, \'precision_recall\',\n                       [num_gbboxes, num_detections, tp, fp, scores]):\n        # Sort detections by score.\n        scores, idxes = tf.nn.top_k(scores, k=num_detections, sorted=True)\n        tp = tf.gather(tp, idxes)\n        fp = tf.gather(fp, idxes)\n        # Computer recall and precision.\n        tp = tf.cumsum(tf.cast(tp, dtype), axis=0)\n        fp = tf.cumsum(tf.cast(fp, dtype), axis=0)\n        recall = _safe_div(tp, tf.cast(num_gbboxes, dtype), \'recall\')\n        precision = _safe_div(tp, tp + fp, \'precision\')\n        return tf.tuple([precision, recall])\n\n\ndef streaming_tp_fp_arrays(num_gbboxes, tp, fp, scores,\n                           remove_zero_scores=True,\n                           metrics_collections=None,\n                           updates_collections=None,\n                           name=None):\n    """"""Streaming computation of True and False Positive arrays. This metrics\n    also keeps track of scores and number of grountruth objects.\n    """"""\n    # Input dictionaries: dict outputs as streaming metrics.\n    if isinstance(scores, dict) or isinstance(fp, dict):\n        d_values = {}\n        d_update_ops = {}\n        for c in num_gbboxes.keys():\n            scope = \'streaming_tp_fp_%s\' % c\n            v, up = streaming_tp_fp_arrays(num_gbboxes[c], tp[c], fp[c], scores[c],\n                                           remove_zero_scores,\n                                           metrics_collections,\n                                           updates_collections,\n                                           name=scope)\n            d_values[c] = v\n            d_update_ops[c] = up\n        return d_values, d_update_ops\n\n    # Input Tensors...\n    with variable_scope.variable_scope(name, \'streaming_tp_fp\',\n                                       [num_gbboxes, tp, fp, scores]):\n        num_gbboxes = math_ops.to_int64(num_gbboxes)\n        scores = math_ops.to_float(scores)\n        stype = tf.bool\n        tp = tf.cast(tp, stype)\n        fp = tf.cast(fp, stype)\n        # Reshape TP and FP tensors and clean away 0 class values.(difficult bboxes)\n        scores = tf.reshape(scores, [-1])\n        tp = tf.reshape(tp, [-1])\n        fp = tf.reshape(fp, [-1])\n        # Remove TP and FP both false.\n        mask = tf.logical_or(tp, fp)\n        if remove_zero_scores:\n            rm_threshold = 1e-4\n            mask = tf.logical_and(mask, tf.greater(scores, rm_threshold))\n            scores = tf.boolean_mask(scores, mask)\n            tp = tf.boolean_mask(tp, mask)\n            fp = tf.boolean_mask(fp, mask)\n\n        # Local variables accumlating information over batches.\n        v_nobjects = _create_local(\'v_num_gbboxes\', shape=[], dtype=tf.int64)\n        v_ndetections = _create_local(\'v_num_detections\', shape=[], dtype=tf.int32)\n        v_scores = _create_local(\'v_scores\', shape=[0, ])\n        v_tp = _create_local(\'v_tp\', shape=[0, ], dtype=stype)\n        v_fp = _create_local(\'v_fp\', shape=[0, ], dtype=stype)\n\n        # Update operations.\n        nobjects_op = state_ops.assign_add(v_nobjects,\n                                           tf.reduce_sum(num_gbboxes))\n        ndetections_op = state_ops.assign_add(v_ndetections,\n                                              tf.size(scores, out_type=tf.int32))\n        scores_op = state_ops.assign(v_scores, tf.concat([v_scores, scores], axis=0),\n                                     validate_shape=False)\n        tp_op = state_ops.assign(v_tp, tf.concat([v_tp, tp], axis=0),\n                                 validate_shape=False)\n        fp_op = state_ops.assign(v_fp, tf.concat([v_fp, fp], axis=0),\n                                 validate_shape=False)\n\n        # Value and update ops.\n        val = (v_nobjects, v_ndetections, v_tp, v_fp, v_scores)\n        with ops.control_dependencies([nobjects_op, ndetections_op,\n                                       scores_op, tp_op, fp_op]):\n            update_op = (nobjects_op, ndetections_op, tp_op, fp_op, scores_op)\n\n        if metrics_collections:\n            ops.add_to_collections(metrics_collections, val)\n        if updates_collections:\n            ops.add_to_collections(updates_collections, update_op)\n        return val, update_op\n\n\n# =========================================================================== #\n# Average precision computations.\n# =========================================================================== #\ndef average_precision_voc12(precision, recall, name=None):\n    """"""Compute (interpolated) average precision from precision and recall Tensors.\n\n    The implementation follows Pascal 2012 and ILSVRC guidelines.\n    See also: https://sanchom.wordpress.com/tag/average-precision/\n    """"""\n    with tf.name_scope(name, \'average_precision_voc12\', [precision, recall]):\n        # Convert to float64 to decrease error on Riemann sums.\n        precision = tf.cast(precision, dtype=tf.float64)\n        recall = tf.cast(recall, dtype=tf.float64)\n\n        # Add bounds values to precision and recall.\n        precision = tf.concat([[0.], precision, [0.]], axis=0)\n        recall = tf.concat([[0.], recall, [1.]], axis=0)\n        # Ensures precision is increasing in reverse order.\n        precision = tfe_math.cummax(precision, reverse=True)\n\n        # Riemann sums for estimating the integral.\n        # mean_pre = (precision[1:] + precision[:-1]) / 2.\n        mean_pre = precision[1:]\n        diff_rec = recall[1:] - recall[:-1]\n        ap = tf.reduce_sum(mean_pre * diff_rec)\n        return ap\n\n\ndef average_precision_voc07(precision, recall, name=None):\n    """"""Compute (interpolated) average precision from precision and recall Tensors.\n\n    The implementation follows Pascal 2007 guidelines.\n    See also: https://sanchom.wordpress.com/tag/average-precision/\n    """"""\n    with tf.name_scope(name, \'average_precision_voc07\', [precision, recall]):\n        # Convert to float64 to decrease error on cumulated sums.\n        precision = tf.cast(precision, dtype=tf.float64)\n        recall = tf.cast(recall, dtype=tf.float64)\n        # Add zero-limit value to avoid any boundary problem...\n        precision = tf.concat([precision, [0.]], axis=0)\n        recall = tf.concat([recall, [np.inf]], axis=0)\n\n        # Split the integral into 10 bins.\n        l_aps = []\n        for t in np.arange(0., 1.1, 0.1):\n            mask = tf.greater_equal(recall, t)\n            v = tf.reduce_max(tf.boolean_mask(precision, mask))\n            l_aps.append(v / 11.)\n        ap = tf.add_n(l_aps)\n        return ap\n\n\ndef precision_recall_values(xvals, precision, recall, name=None):\n    """"""Compute values on the precision/recall curve.\n\n    Args:\n      x: Python list of floats;\n      precision: 1D Tensor decreasing.\n      recall: 1D Tensor increasing.\n    Return:\n      list of precision values.\n    """"""\n    with ops.name_scope(name, ""precision_recall_values"",\n                        [precision, recall]) as name:\n        # Add bounds values to precision and recall.\n        precision = tf.concat([[0.], precision, [0.]], axis=0)\n        recall = tf.concat([[0.], recall, [1.]], axis=0)\n        precision = tfe_math.cummax(precision, reverse=True)\n\n        prec_values = []\n        for x in xvals:\n            mask = tf.less_equal(recall, x)\n            val = tf.reduce_min(tf.boolean_mask(precision, mask))\n            prec_values.append(val)\n        return tf.tuple(prec_values)\n\n\n# =========================================================================== #\n# TF Extended metrics: old stuff!\n# =========================================================================== #\ndef _precision_recall(n_gbboxes, n_detections, scores, tp, fp, scope=None):\n    """"""Compute precision and recall from scores, true positives and false\n    positives booleans arrays\n    """"""\n    # Sort by score.\n    with tf.name_scope(scope, \'prec_rec\', [n_gbboxes, scores, tp, fp]):\n        # Sort detections by score.\n        scores, idxes = tf.nn.top_k(scores, k=n_detections, sorted=True)\n        tp = tf.gather(tp, idxes)\n        fp = tf.gather(fp, idxes)\n        # Computer recall and precision.\n        dtype = tf.float64\n        tp = tf.cumsum(tf.cast(tp, dtype), axis=0)\n        fp = tf.cumsum(tf.cast(fp, dtype), axis=0)\n        recall = _safe_div(tp, tf.cast(n_gbboxes, dtype), \'recall\')\n        precision = _safe_div(tp, tp + fp, \'precision\')\n\n        return tf.tuple([precision, recall])\n\n\ndef streaming_precision_recall_arrays(n_gbboxes, rclasses, rscores,\n                                      tp_tensor, fp_tensor,\n                                      remove_zero_labels=True,\n                                      metrics_collections=None,\n                                      updates_collections=None,\n                                      name=None):\n    """"""Streaming computation of precision / recall arrays. This metrics\n    keeps tracks of boolean True positives and False positives arrays.\n    """"""\n    with variable_scope.variable_scope(name, \'stream_precision_recall\',\n                                       [n_gbboxes, rclasses, tp_tensor, fp_tensor]):\n        n_gbboxes = math_ops.to_int64(n_gbboxes)\n        rclasses = math_ops.to_int64(rclasses)\n        rscores = math_ops.to_float(rscores)\n\n        stype = tf.int32\n        tp_tensor = tf.cast(tp_tensor, stype)\n        fp_tensor = tf.cast(fp_tensor, stype)\n\n        # Reshape TP and FP tensors and clean away 0 class values.\n        rclasses = tf.reshape(rclasses, [-1])\n        rscores = tf.reshape(rscores, [-1])\n        tp_tensor = tf.reshape(tp_tensor, [-1])\n        fp_tensor = tf.reshape(fp_tensor, [-1])\n        if remove_zero_labels:\n            mask = tf.greater(rclasses, 0)\n            rclasses = tf.boolean_mask(rclasses, mask)\n            rscores = tf.boolean_mask(rscores, mask)\n            tp_tensor = tf.boolean_mask(tp_tensor, mask)\n            fp_tensor = tf.boolean_mask(fp_tensor, mask)\n\n        # Local variables accumlating information over batches.\n        v_nobjects = _create_local(\'v_nobjects\', shape=[], dtype=tf.int64)\n        v_ndetections = _create_local(\'v_ndetections\', shape=[], dtype=tf.int32)\n        v_scores = _create_local(\'v_scores\', shape=[0, ])\n        v_tp = _create_local(\'v_tp\', shape=[0, ], dtype=stype)\n        v_fp = _create_local(\'v_fp\', shape=[0, ], dtype=stype)\n\n        # Update operations.\n        nobjects_op = state_ops.assign_add(v_nobjects,\n                                           tf.reduce_sum(n_gbboxes))\n        ndetections_op = state_ops.assign_add(v_ndetections,\n                                              tf.size(rscores, out_type=tf.int32))\n        scores_op = state_ops.assign(v_scores, tf.concat([v_scores, rscores], axis=0),\n                                     validate_shape=False)\n        tp_op = state_ops.assign(v_tp, tf.concat([v_tp, tp_tensor], axis=0),\n                                 validate_shape=False)\n        fp_op = state_ops.assign(v_fp, tf.concat([v_fp, fp_tensor], axis=0),\n                                 validate_shape=False)\n\n        # Precision and recall computations.\n        # r = _precision_recall(nobjects_op, scores_op, tp_op, fp_op, \'value\')\n        r = _precision_recall(v_nobjects, v_ndetections, v_scores,\n                              v_tp, v_fp, \'value\')\n\n        with ops.control_dependencies([nobjects_op, ndetections_op,\n                                       scores_op, tp_op, fp_op]):\n            update_op = _precision_recall(nobjects_op, ndetections_op,\n                                          scores_op, tp_op, fp_op, \'update_op\')\n\n            # update_op = tf.Print(update_op,\n            #                      [tf.reduce_sum(tf.cast(mask, tf.int64)),\n            #                       tf.reduce_sum(tf.cast(mask2, tf.int64)),\n            #                       tf.reduce_min(rscores),\n            #                       tf.reduce_sum(n_gbboxes)],\n            #                      \'Metric: \')\n            # Some debugging stuff!\n            # update_op = tf.Print(update_op,\n            #                      [tf.shape(tp_op),\n            #                       tf.reduce_sum(tf.cast(tp_op, tf.int64), axis=0)],\n            #                      \'TP and FP shape: \')\n            # update_op[0] = tf.Print(update_op,\n            #                      [nobjects_op],\n            #                      \'# Groundtruth bboxes: \')\n            # update_op = tf.Print(update_op,\n            #                      [update_op[0][0],\n            #                       update_op[0][-1],\n            #                       tf.reduce_min(update_op[0]),\n            #                       tf.reduce_max(update_op[0]),\n            #                       tf.reduce_min(update_op[1]),\n            #                       tf.reduce_max(update_op[1])],\n            #                      \'Precision and recall :\')\n\n        if metrics_collections:\n            ops.add_to_collections(metrics_collections, r)\n        if updates_collections:\n            ops.add_to_collections(updates_collections, update_op)\n        return r, update_op\n\n'"
tf_extended/tensors.py,12,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional tensors operations.\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.contrib.framework.python.ops import variables as contrib_variables\nfrom tensorflow.contrib.metrics.python.ops import set_ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\n\n\ndef get_shape(x, rank=None):\n    """"""Returns the dimensions of a Tensor as list of integers or scale tensors.\n\n    Args:\n      x: N-d Tensor;\n      rank: Rank of the Tensor. If None, will try to guess it.\n    Returns:\n      A list of `[d1, d2, ..., dN]` corresponding to the dimensions of the\n        input tensor.  Dimensions that are statically known are python integers,\n        otherwise they are integer scalar tensors.\n    """"""\n    if x.get_shape().is_fully_defined():\n        return x.get_shape().as_list()\n    else:\n        static_shape = x.get_shape()\n        if rank is None:\n            static_shape = static_shape.as_list()\n            rank = len(static_shape)\n        else:\n            static_shape = x.get_shape().with_rank(rank).as_list()\n        dynamic_shape = tf.unstack(tf.shape(x), rank)\n        return [s if s is not None else d\n                for s, d in zip(static_shape, dynamic_shape)]\n\n\ndef pad_axis(x, offset, size, axis=0, name=None):\n    """"""Pad a tensor on an axis, with a given offset and output size.\n    The tensor is padded with zero (i.e. CONSTANT mode). Note that the if the\n    `size` is smaller than existing size + `offset`, the output tensor\n    was the latter dimension.\n\n    Args:\n      x: Tensor to pad;\n      offset: Offset to add on the dimension chosen;\n      size: Final size of the dimension.\n    Return:\n      Padded tensor whose dimension on `axis` is `size`, or greater if\n      the input vector was larger.\n    """"""\n    with tf.name_scope(name, \'pad_axis\'):\n        shape = get_shape(x)\n        rank = len(shape)\n        # Padding description.\n        new_size = tf.maximum(size-offset-shape[axis], 0)\n        pad1 = tf.stack([0]*axis + [offset] + [0]*(rank-axis-1))\n        pad2 = tf.stack([0]*axis + [new_size] + [0]*(rank-axis-1))\n        paddings = tf.stack([pad1, pad2], axis=1)\n        x = tf.pad(x, paddings, mode=\'CONSTANT\')\n        # Reshape, to get fully defined shape if possible.\n        # TODO: fix with tf.slice\n        shape[axis] = size\n        x = tf.reshape(x, tf.stack(shape))\n        return x\n\n\n# def select_at_index(idx, val, t):\n#     """"""Return a tensor.\n#     """"""\n#     idx = tf.expand_dims(tf.expand_dims(idx, 0), 0)\n#     val = tf.expand_dims(val, 0)\n#     t = t + tf.scatter_nd(idx, val, tf.shape(t))\n#     return t\n'"
utility/__init__.py,0,b''
utility/visualization.py,0,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport cv2\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.cm as mpcm\n\n\n# =========================================================================== #\n# Some colormaps.\n# =========================================================================== #\ndef colors_subselect(colors, num_classes=21):\n    dt = len(colors) // num_classes\n    sub_colors = []\n    for i in range(num_classes):\n        color = colors[i*dt]\n        if isinstance(color[0], float):\n            sub_colors.append([int(c * 255) for c in color])\n        else:\n            sub_colors.append([c for c in color])\n    return sub_colors\n\ncolors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=21)\ncolors_tableau = [(255, 255, 255), (31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\n                  (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),\n                  (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),\n                  (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),\n                  (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]\n\n\n# =========================================================================== #\n# OpenCV drawing.\n# =========================================================================== #\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """"""Draw a collection of lines on an image.\n    """"""\n    for line in lines:\n        for x1, y1, x2, y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\n\ndef draw_rectangle(img, p1, p2, color=[255, 0, 0], thickness=2):\n    cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n\n\ndef draw_bbox(img, bbox, shape, label, color=[255, 0, 0], thickness=2):\n    p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))\n    p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))\n    cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n    p1 = (p1[0]+15, p1[1])\n    cv2.putText(img, str(label), p1[::-1], cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)\n\n\ndef bboxes_draw_on_img(img, classes, scores, bboxes, colors, thickness=2):\n    shape = img.shape\n    for i in range(bboxes.shape[0]):\n        bbox = bboxes[i]\n        color = colors[classes[i]]\n        # Draw bounding box...\n        p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))\n        p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))\n        cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n        # Draw text...\n        s = \'%s/%.3f\' % (classes[i], scores[i])\n        p1 = (p1[0]-5, p1[1])\n        cv2.putText(img, s, p1[::-1], cv2.FONT_HERSHEY_DUPLEX, 0.4, color, 1)\n\n\n# =========================================================================== #\n# Matplotlib show...\n# =========================================================================== #\ndef plt_bboxes(img, classes, scores, bboxes, figsize=(10,10), linewidth=1.5,neg_marks=None, title=None):\n    """"""Visualize bounding boxes. Largely inspired by SSD-MXNET!\n    """"""\n    fig = plt.figure(figsize=figsize)\n    plt.imshow(img)\n    if title is not None:\n        plt.suptitle(title)\n    height = img.shape[0]\n    width = img.shape[1]\n    colors = dict()\n    for i in range(classes.shape[0]):\n        cls_id = int(classes[i])\n        if cls_id >= 0:\n            score = scores[i]\n            if cls_id not in colors:\n                colors[cls_id] = (0, 1, 0)\n            ymin = int(bboxes[i, 0] * height)\n            xmin = int(bboxes[i, 1] * width)\n            ymax = int(bboxes[i, 2] * height)\n            xmax = int(bboxes[i, 3] * width)\n            if (neg_marks is not None) and neg_marks[i]:\n                #it\'s negative sample default box\n                rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n                                     ymax - ymin, fill=False,\n                                     edgecolor=colors[cls_id],\n                                     linestyle = \'--\',\n                                     linewidth=linewidth/2)\n            else:\n                rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n                                     ymax - ymin, fill=False,\n                                     edgecolor=colors[cls_id],\n                                     linestyle = \'-\',\n                                     linewidth=linewidth)\n            plt.gca().add_patch(rect)\n            class_name = str(cls_id)\n            #plt.gca().text(xmin, ymin - 2,\'{:s} | {:.3f}\'.format(class_name, score),bbox=dict(facecolor=colors[cls_id], alpha=0.5),fontsize=12, color=\'white\')\n    \n'"
