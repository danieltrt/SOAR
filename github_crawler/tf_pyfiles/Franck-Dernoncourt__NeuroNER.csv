file_path,api_count,code
setup.py,0,"b'# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\n# import glob\n# for directory in glob.glob(\'./**/\', recursive=True):\n#     print(directory)\n\nsetup(\n    name=\'pyneuroner\',\n\n    # Versions should comply with PEP440. For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=\'1.0.8\',\n\n    description=\'NeuroNER\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n\n    # The project\'s main homepage.\n    url=\'https://github.com/Franck-Dernoncourt/NeuroNER\',\n\n    # Author details\n    # author=\'\',\n    # author_email=\'\',\n\n    # Choose your license\n    # license=\'MIT\',\n\n    # What does your project relate to?\n    keywords=\'Named-entity recognition using neural networks\',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    # packages=find_packages(exclude=[\'contrib\', \'docs\', \'tests\',\'env\',\n    #     \'output\']),\n    packages=[\'neuroner\'],\n\n    # Alternatively, if you want to distribute just a my_module.py, uncomment\n    # this:\n    # py_modules=[\'\'],\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\n        \'matplotlib>=3.0.2\',\n        \'networkx>=2.2\',\n        \'pycorenlp>=0.3.0\',\n        \'scikit-learn>=0.20.2\',\n        \'scipy>=1.2.0\',\n        \'spacy>=2.0.18\',\n        ],\n\n    # allow user to select flavour of TensorFlow \n    # https://github.com/tensorflow/tensorflow/issues/7166\n    extras_require={\n        ""cpu"": [""tensorflow>=1.12.0""],\n        ""gpu"": [""tensorflow-gpu>=1.0.0""],\n    },\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]\n    # extras_require={\n    #     \'dev\': [\'check-manifest\'],\n    #     \'test\': [\'coverage\'],\n    # },\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.  If using Python 2.6 or less, then these\n    # have to be included in MANIFEST.in as well.\n    zip_safe=False,\n    # package_dir={\'neuroner\': \'neuroner\'},\n    include_package_data = True,\n    package_data={\'data\': [\'data/**\'], \n        \'trained_models\': [\'trained_models/**\']\n    },\n\n    # Although \'package_data\' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files # noqa\n    # In this case, \'data_file\' will be installed into \'<sys.prefix>/my_data\'\n    # data_files=[(\'my_data\', [\'data/data_file\'])],\n    # data_files=[(\'neuroner\', [\'conlleval\'])],\n\n    # To provide executable scripts, use entry points in preference to the\n    # ""scripts"" keyword. Entry points provide cross-platform support and allow\n    # pip to create the appropriate form of executable for the target platform.\n    entry_points={\n        \'console_scripts\': [\n            \'neuroner = neuroner.__main__:main\',\n        ],\n    },\n\n)'"
neuroner/__init__.py,0,b''
neuroner/__main__.py,0,"b'\'\'\'\nTo run:\nCUDA_VISIBLE_DEVICES="""" python3.5 main.py &\nCUDA_VISIBLE_DEVICES=1 python3.5 main.py &\nCUDA_VISIBLE_DEVICES=2 python3.5 main.py &\nCUDA_VISIBLE_DEVICES=3 python3.5 main.py &\n\'\'\'\nfrom __future__ import print_function\nimport argparse\nfrom argparse import RawTextHelpFormatter\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nfrom neuroner import neuromodel\n\ndef parse_arguments(arguments=None):\n    """"""\n    Parse the NeuroNER arguments\n\n    arguments:\n        arguments the arguments, optionally given as argument\n    """"""\n\n    parser = argparse.ArgumentParser(description=\'\'\'NeuroNER CLI\'\'\', formatter_class=RawTextHelpFormatter)\n\n    parser.add_argument(\'--parameters_filepath\', required=False, default=None, help=\'The parameters file\')\n    parser.add_argument(\'--character_embedding_dimension\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--character_lstm_hidden_state_dimension\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--check_for_digits_replaced_with_zeros\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--check_for_lowercase\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--dataset_text_folder\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--debug\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--dropout_rate\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--experiment_name\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--freeze_token_embeddings\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--gradient_clipping_value\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--learning_rate\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--load_only_pretrained_token_embeddings\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--load_all_pretrained_token_embeddings\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--main_evaluation_mode\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--maximum_number_of_epochs\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--number_of_cpu_threads\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--number_of_gpus\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--optimizer\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--output_folder\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--output_scores\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--patience\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--plot_format\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--pretrained_model_folder\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--reload_character_embeddings\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--reload_character_lstm\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--reload_crf\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--reload_feedforward\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--reload_token_embeddings\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--reload_token_lstm\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--remap_unknown_tokens_to_unk\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--spacylanguage\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--tagging_format\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--token_embedding_dimension\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--token_lstm_hidden_state_dimension\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--token_pretrained_embedding_filepath\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--tokenizer\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--train_model\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--use_character_lstm\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--use_crf\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--use_pretrained_model\', required=False, default=None, help=\'\')\n    parser.add_argument(\'--verbose\', required=False, default=None, help=\'\')\n\n    # load data to local folder\n    parser.add_argument(\'--fetch_data\', required=False, default=\'\', help=\'\')\n    parser.add_argument(\'--fetch_trained_model\', required=False, default=\'\', help=\'\')\n\n    try:\n        arguments = parser.parse_args(args=arguments)\n    except:\n        parser.print_help()\n        sys.exit(0)\n\n    # http://stackoverflow.com/questions/16878315/what-is-the-right-way-to-treat-python-argparse-namespace-as-a-dictionary\n    arguments = vars(arguments)\n\n    return {k: v for k, v in arguments.items() if v is not None}\n\ndef main(argv=sys.argv):\n    \'\'\' NeuroNER main method\n\n    Args:\n        parameters_filepath the path to the parameters file\n        output_folder the path to the output folder\n    \'\'\'\n    arguments = parse_arguments(argv[1:])\n\n    # fetch data and models from the package\n    if arguments[\'fetch_data\'] or arguments[\'fetch_trained_model\']:\n\n        if arguments[\'fetch_data\']:\n            neuromodel.fetch_data(arguments[\'fetch_data\'])\n        if arguments[\'fetch_trained_model\']:\n            neuromodel.fetch_model(arguments[\'fetch_trained_model\'])\n\n        msg = """"""When the fetch_data and fetch_trained_model arguments are specified, other\n            arguments are ignored. Remove these arguments to train or apply a model.""""""\n        print(msg)\n        sys.exit(0)\n\n    # create the model\n    nn = neuromodel.NeuroNER(**arguments)\n    nn.fit()\n    nn.close()\n\nif __name__ == ""__main__"":\n    main()\n'"
neuroner/brat_to_conll.py,0,"b'# -*- coding: utf-8 -*-\nimport codecs\nimport glob\nimport json\nimport os\n\nfrom pycorenlp import StanfordCoreNLP\nimport spacy\n\nfrom neuroner import utils_nlp\n\n\ndef get_start_and_end_offset_of_token_from_spacy(token):\n    start = token.idx\n    end = start + len(token)\n    return start, end\n\ndef get_sentences_and_tokens_from_spacy(text, spacy_nlp):\n    document = spacy_nlp(text)\n    # sentences\n    sentences = []\n    for span in document.sents:\n        sentence = [document[i] for i in range(span.start, span.end)]\n        sentence_tokens = []\n        for token in sentence:\n            token_dict = {}\n            token_dict[\'start\'], token_dict[\'end\'] = get_start_and_end_offset_of_token_from_spacy(token)\n            token_dict[\'text\'] = text[token_dict[\'start\']:token_dict[\'end\']]\n            if token_dict[\'text\'].strip() in [\'\\n\', \'\\t\', \' \', \'\']:\n                continue\n            # Make sure that the token text does not contain any space\n            if len(token_dict[\'text\'].split(\' \')) != 1:\n                print(""WARNING: the text of the token contains space character, replaced with hyphen\\n\\t{0}\\n\\t{1}"".format(token_dict[\'text\'], \n                                                                                                                           token_dict[\'text\'].replace(\' \', \'-\')))\n                token_dict[\'text\'] = token_dict[\'text\'].replace(\' \', \'-\')\n            sentence_tokens.append(token_dict)\n        sentences.append(sentence_tokens)\n    return sentences\n\ndef get_stanford_annotations(text, core_nlp, port=9000, annotators=\'tokenize,ssplit,pos,lemma\'):\n    output = core_nlp.annotate(text, properties={\n        ""timeout"": ""10000"",\n        ""ssplit.newlineIsSentenceBreak"": ""two"",\n        \'annotators\': annotators,\n        \'outputFormat\': \'json\'\n    })\n    if type(output) is str:\n        output = json.loads(output, strict=False)\n    return output\n\ndef get_sentences_and_tokens_from_stanford(text, core_nlp):\n    stanford_output = get_stanford_annotations(text, core_nlp)\n    sentences = []\n    for sentence in stanford_output[\'sentences\']:\n        tokens = []\n        for token in sentence[\'tokens\']:\n            token[\'start\'] = int(token[\'characterOffsetBegin\'])\n            token[\'end\'] = int(token[\'characterOffsetEnd\'])\n            token[\'text\'] = text[token[\'start\']:token[\'end\']]\n            if token[\'text\'].strip() in [\'\\n\', \'\\t\', \' \', \'\']:\n                continue\n            # Make sure that the token text does not contain any space\n            if len(token[\'text\'].split(\' \')) != 1:\n                print(""WARNING: the text of the token contains space character, replaced with hyphen\\n\\t{0}\\n\\t{1}"".format(token[\'text\'], \n                                                                                                                           token[\'text\'].replace(\' \', \'-\')))\n                token[\'text\'] = token[\'text\'].replace(\' \', \'-\')\n            tokens.append(token)\n        sentences.append(tokens)\n    return sentences\n\ndef get_entities_from_brat(text_filepath, annotation_filepath, verbose=False):\n    # load text\n    with codecs.open(text_filepath, \'r\', \'UTF-8\') as f:\n        text =f.read()\n    if verbose: print(""\\ntext:\\n{0}\\n"".format(text))\n\n    # parse annotation file\n    entities = []\n    with codecs.open(annotation_filepath, \'r\', \'UTF-8\') as f:\n        for line in f.read().splitlines():\n            anno = line.split()\n            id_anno = anno[0]\n            # parse entity\n            if id_anno[0] == \'T\':\n                entity = {}\n                entity[\'id\'] = id_anno\n                entity[\'type\'] = anno[1]\n                entity[\'start\'] = int(anno[2])\n                entity[\'end\'] = int(anno[3])\n                entity[\'text\'] = \' \'.join(anno[4:])\n                if verbose:\n                    print(""entity: {0}"".format(entity))\n                # Check compatibility between brat text and anootation\n                if utils_nlp.replace_unicode_whitespaces_with_ascii_whitespace(text[entity[\'start\']:entity[\'end\']]) != \\\n                    utils_nlp.replace_unicode_whitespaces_with_ascii_whitespace(entity[\'text\']):\n                    print(""Warning: brat text and annotation do not match."")\n                    print(""\\ttext: {0}"".format(text[entity[\'start\']:entity[\'end\']]))\n                    print(""\\tanno: {0}"".format(entity[\'text\']))\n                # add to entitys data\n                entities.append(entity)\n    if verbose: print(""\\n\\n"")\n    \n    return text, entities\n\ndef check_brat_annotation_and_text_compatibility(brat_folder):\n    \'\'\'\n    Check if brat annotation and text files are compatible.\n    \'\'\'\n    dataset_type =  os.path.basename(brat_folder)\n    print(""Checking the validity of BRAT-formatted {0} set... "".format(dataset_type), end=\'\')\n    text_filepaths = sorted(glob.glob(os.path.join(brat_folder, \'*.txt\')))\n    for text_filepath in text_filepaths:\n        base_filename = os.path.splitext(os.path.basename(text_filepath))[0]\n        annotation_filepath = os.path.join(os.path.dirname(text_filepath), base_filename + \'.ann\')\n        # check if annotation file exists\n        if not os.path.exists(annotation_filepath):\n            raise IOError(""Annotation file does not exist: {0}"".format(annotation_filepath))\n        text, entities = get_entities_from_brat(text_filepath, annotation_filepath)\n    print(""Done."")\n\ndef brat_to_conll(input_folder, output_filepath, tokenizer, language):\n    \'\'\'\n    Assumes \'.txt\' and \'.ann\' files are in the input_folder.\n    Checks for the compatibility between .txt and .ann at the same time.\n    \'\'\'\n    if tokenizer == \'spacy\':\n        spacy_nlp = spacy.load(language)\n    elif tokenizer == \'stanford\':\n        core_nlp = StanfordCoreNLP(\'http://localhost:{0}\'.format(9000))\n    else:\n        raise ValueError(""tokenizer should be either \'spacy\' or \'stanford\'."")\n    verbose = False\n    dataset_type =  os.path.basename(input_folder)\n    print(""Formatting {0} set from BRAT to CONLL... "".format(dataset_type), end=\'\')\n    text_filepaths = sorted(glob.glob(os.path.join(input_folder, \'*.txt\')))\n    output_file = codecs.open(output_filepath, \'w\', \'utf-8\')\n    for text_filepath in text_filepaths:\n        base_filename = os.path.splitext(os.path.basename(text_filepath))[0]\n        annotation_filepath = os.path.join(os.path.dirname(text_filepath), base_filename + \'.ann\')\n        # create annotation file if it does not exist\n        if not os.path.exists(annotation_filepath):\n            codecs.open(annotation_filepath, \'w\', \'UTF-8\').close()\n\n        text, entities = get_entities_from_brat(text_filepath, annotation_filepath)\n        entities = sorted(entities, key=lambda entity:entity[""start""])\n        \n        if tokenizer == \'spacy\':\n            sentences = get_sentences_and_tokens_from_spacy(text, spacy_nlp)\n        elif tokenizer == \'stanford\':\n            sentences = get_sentences_and_tokens_from_stanford(text, core_nlp)\n        \n        for sentence in sentences:\n            inside = False\n            previous_token_label = \'O\'\n            for token in sentence:\n                token[\'label\'] = \'O\'\n                for entity in entities:\n                    if entity[\'start\'] <= token[\'start\'] < entity[\'end\'] or \\\n                       entity[\'start\'] < token[\'end\'] <= entity[\'end\'] or \\\n                       token[\'start\'] < entity[\'start\'] < entity[\'end\'] < token[\'end\']:\n\n                        token[\'label\'] = entity[\'type\'].replace(\'-\', \'_\') # Because the ANN doesn\'t support tag with \'-\' in it\n\n                        break\n                    elif token[\'end\'] < entity[\'start\']:\n                        break\n                        \n                if len(entities) == 0:\n                    entity={\'end\':0}\n                if token[\'label\'] == \'O\':\n                    gold_label = \'O\'\n                    inside = False\n                elif inside and token[\'label\'] == previous_token_label:\n                    gold_label = \'I-{0}\'.format(token[\'label\'])\n                else:\n                    inside = True\n                    gold_label = \'B-{0}\'.format(token[\'label\'])\n                if token[\'end\'] == entity[\'end\']:\n                    inside = False\n                previous_token_label = token[\'label\']\n                if verbose: print(\'{0} {1} {2} {3} {4}\\n\'.format(token[\'text\'], base_filename, token[\'start\'], token[\'end\'], gold_label))\n                output_file.write(\'{0} {1} {2} {3} {4}\\n\'.format(token[\'text\'], base_filename, token[\'start\'], token[\'end\'], gold_label))\n            if verbose: print(\'\\n\')\n            output_file.write(\'\\n\')\n\n    output_file.close()\n    print(\'Done.\')\n    if tokenizer == \'spacy\':\n        del spacy_nlp\n    elif tokenizer == \'stanford\':\n        del core_nlp\n'"
neuroner/conll_to_brat.py,0,"b'import codecs\nimport glob\nimport os\nimport shutil\n\nfrom neuroner import utils\nfrom neuroner import utils_nlp\n\ndef generate_reference_text_file_for_conll(conll_input_filepath, conll_output_filepath, text_folder):\n    \'\'\'\n    generates reference text files and adds the corresponding filename and token offsets to conll file.\n    \n    conll_input_filepath: path to a conll-formatted file without filename and token offsets\n    text_folder: folder to write the reference text file to\n    \'\'\'\n    dataset_type =  utils.get_basename_without_extension(conll_input_filepath)\n    conll_file = codecs.open(conll_input_filepath, \'r\', \'UTF-8\')   \n    utils.create_folder_if_not_exists(text_folder)\n    text = \'\'\n    new_conll_string = \'\'\n    character_index = 0\n    document_count = 0\n    text_base_filename = \'{0}_text_{1}\'.format(dataset_type, str(document_count).zfill(5))\n    for line in conll_file:\n        split_line = line.strip().split(\' \')\n        # New document\n        if \'-DOCSTART-\' in split_line[0]:\n            new_conll_string += line\n            if len(text) != 0:\n                with codecs.open(os.path.join(text_folder, \'{0}.txt\'.format(text_base_filename)), \'w\', \'UTF-8\') as f:\n                    f.write(text)\n            text = \'\'\n            character_index = 0\n            document_count += 1\n            text_base_filename = \'{0}_text_{1}\'.format(dataset_type, str(document_count).zfill(5))\n            continue            \n        # New sentence\n        elif len(split_line) == 0 or len(split_line[0]) == 0:\n            new_conll_string += \'\\n\'\n            if text != \'\':\n                text += \'\\n\'\n                character_index += 1\n            continue\n        token = split_line[0]\n        start = character_index\n        end = start + len(token)\n        text += token + \' \'\n        character_index += len(token) + 1\n        new_conll_string += \' \'.join([token, text_base_filename, str(start), str(end)] + split_line[1:]) + \'\\n\' \n    if len(text) != 0:\n        with codecs.open(os.path.join(text_folder, \'{0}.txt\'.format(text_base_filename)), \'w\', \'UTF-8\') as f:\n            f.write(text)\n    conll_file.close()\n    \n    with codecs.open(conll_output_filepath, \'w\', \'UTF-8\') as f:\n        f.write(new_conll_string)\n\ndef check_compatibility_between_conll_and_brat_text(conll_filepath, brat_folder):\n    \'\'\'\n    check if token offsets match between conll and brat .txt files. \n\n    conll_filepath: path to conll file\n    brat_folder: folder that contains the .txt (and .ann) files that are formatted according to brat.\n                                \n    \'\'\'\n    verbose = False\n    dataset_type = utils.get_basename_without_extension(conll_filepath)\n    print(""Checking compatibility between CONLL and BRAT for {0} set ... "".format(dataset_type), end=\'\')\n    conll_file = codecs.open(conll_filepath, \'r\', \'UTF-8\')\n\n    previous_filename = \'\'\n    for line in conll_file:\n        line = line.strip().split(\' \')\n        # New sentence\n        if len(line) == 0 or len(line[0]) == 0 or \'-DOCSTART-\' in line[0]:\n            continue\n        \n        filename = str(line[1])\n        # New file\n        if filename != previous_filename:\n            text_filepath = os.path.join(brat_folder, \'{0}.txt\'.format(filename))\n            with codecs.open(text_filepath, \'r\', \'UTF-8\') as f:\n                text = f.read()\n            previous_filename = filename \n            \n        label = str(line[-1]).replace(\'_\', \'-\') # For LOCATION-OTHER\n        \n        token = {}\n        token[\'text\'] = str(line[0])\n        token[\'start\'] = int(line[2])\n        token[\'end\'] = int(line[3])\n\n        # check that the token text matches the original\n        if token[\'text\'] != text[token[\'start\']:token[\'end\']]:\n            print(""Warning: conll and brat text do not match."")\n            print(""\\tCONLL: {0}"".format(token[\'text\']))\n            print(""\\tBRAT : {0}"".format(text[token[\'start\']:token[\'end\']]))\n            if token[\'text\'] != text[token[\'start\']:token[\'end\']].replace(\' \', \'-\'):\n                raise AssertionError(""CONLL and BRAT files are incompatible."")\n    \n    print(""Done."")\n\ndef output_entities(brat_output_folder, previous_filename, entities, text_filepath, text, overwrite=False):\n    if previous_filename == \'\':\n        return\n    output_filepath = os.path.join(brat_output_folder, \'{0}.ann\'.format(previous_filename))\n    if not overwrite:\n        # Avoid overriding existing annotation\n        if os.path.exists(output_filepath) and os.path.getsize(output_filepath) > 0:\n            raise AssertionError(""The annotation already exists at: {0}"".format(output_filepath))\n    # Write the entities to the annotation file\n    with codecs.open(output_filepath, \'w\', \'utf-8\') as output_file:\n        for i, entity in enumerate(entities):\n            output_file.write(\'T{0}\\t{1} {2} {3}\\t{4}\\n\'.format(i+1, entity[\'label\'], entity[\'start\'], entity[\'end\'], \n                                                           utils_nlp.replace_unicode_whitespaces_with_ascii_whitespace(text[entity[\'start\']:entity[\'end\']])))\n    # Copy the corresponding text file\n    if text_filepath != os.path.join(brat_output_folder, os.path.basename(text_filepath)):\n        shutil.copy(text_filepath, brat_output_folder)\n\ndef conll_to_brat(conll_input_filepath, conll_output_filepath, brat_original_folder, brat_output_folder, overwrite=False):\n    \'\'\'\n    convert conll file in conll-filepath to brat annotations and output to brat_output_folder, \n    with reference to the existing text files in brat_original_folder \n    if brat_original_folder does not exist or contain any text file, then the text files are generated from conll files,\n    and conll file is updated with filenames and token offsets accordingly. \n    \n    conll_input_filepath: path to conll file to convert to brat annotations\n    conll_output_filepath: path to output conll file with filename and offsets that are compatible with brat annotations\n    brat_original_folder: folder that contains the original .txt (and .ann) files that are formatted according to brat.\n                          .txt files are used to check if the token offsets match and generate the annotation from conll.                      \n    brat_output_folder: folder to output the text and brat annotations \n                        .txt files are copied from brat_original_folder to brat_output_folder\n    \'\'\'\n    verbose = False\n    dataset_type = utils.get_basename_without_extension(conll_input_filepath)\n    print(""Formatting {0} set from CONLL to BRAT... "".format(dataset_type), end=\'\')\n    \n    # if brat_original_folder does not exist or have any text file\n    if not os.path.exists(brat_original_folder) or len(glob.glob(os.path.join(brat_original_folder, \'*.txt\'))) == 0:\n        assert(conll_input_filepath != conll_output_filepath)\n        generate_reference_text_file_for_conll(conll_input_filepath, conll_output_filepath, brat_original_folder)\n\n    utils.create_folder_if_not_exists(brat_output_folder)\n    conll_file = codecs.open(conll_output_filepath, \'r\', \'UTF-8\')\n\n    previous_token_label = \'O\'\n    previous_filename = \'\'\n    text_filepath = \'\'\n    text = \'\'\n    entity_id = 1\n    entities = []\n    entity = {}\n    for line in conll_file:\n        line = line.strip().split(\' \')\n        # New sentence\n        if len(line) == 0 or len(line[0]) == 0 or \'-DOCSTART-\' in line[0]:\n            # Add the last entity \n            if entity != {}:\n                if verbose: print(""entity: {0}"".format(entity))\n                entities.append(entity)\n                entity_id += 1\n                entity = {}\n            previous_token_label = \'O\'\n            continue\n        \n        filename = str(line[1])\n        # New file\n        if filename != previous_filename:    \n            output_entities(brat_output_folder, previous_filename, entities, text_filepath, text, overwrite=overwrite)\n            text_filepath = os.path.join(brat_original_folder, \'{0}.txt\'.format(filename))\n            with codecs.open(text_filepath, \'r\', \'UTF-8\') as f:\n                text = f.read()\n            previous_token_label = \'O\'\n            previous_filename = filename \n            entity_id = 1\n            entities = []\n            entity = {}\n            \n        label = str(line[-1]).replace(\'_\', \'-\') # For LOCATION-OTHER\n        if label == \'O\':\n            # Previous entity ended\n            if previous_token_label != \'O\':\n                if verbose: print(""entity: {0}"".format(entity))\n                entities.append(entity)\n                entity_id += 1\n                entity = {}\n            previous_token_label = \'O\'\n            continue\n        \n        token = {}\n        token[\'text\'] = str(line[0])\n        token[\'start\'] = int(line[2])\n        token[\'end\'] = int(line[3])\n        # check that the token text matches the original\n        if token[\'text\'] != text[token[\'start\']:token[\'end\']].replace(\' \', \'-\'):\n            print(""Warning: conll and brat text do not match."")\n            print(""\\tCONLL: {0}"".format(token[\'text\']))\n            print(""\\tBRAT : {0}"".format(text[token[\'start\']:token[\'end\']]))\n        token[\'label\'] = label[2:]\n    \n        if label[:2] == \'B-\':\n            if previous_token_label != \'O\':\n                # End the previous entity\n                if verbose: print(""entity: {0}"".format(entity))\n                entities.append(entity)\n                entity_id += 1\n            # Start a new entity\n            entity = token\n        elif label[:2] == \'I-\':\n            # Entity continued\n            if previous_token_label == token[\'label\']:\n                # if there is no newline between the entity and the token\n                if \'\\n\' not in text[entity[\'end\']:token[\'start\']]:\n                    # Update entity \n                    entity[\'text\'] = entity[\'text\'] + \' \' + token[\'text\']\n                    entity[\'end\'] = token[\'end\']\n                else: # newline between the entity and the token\n                    # End the previous entity\n                    if verbose: print(""entity: {0}"".format(entity))\n                    entities.append(entity)\n                    entity_id += 1\n                    # Start a new entity\n                    entity = token\n            elif previous_token_label != \'O\':\n                # TODO: count BI or II incompatibility\n                # End the previous entity\n                if verbose: print(""entity: {0}"".format(entity))\n                entities.append(entity)\n                entity_id += 1\n                # Start new entity\n                entity = token\n            else: # previous_token_label == \'O\'\n                # TODO: count  OI incompatibility\n                # Start new entity\n                entity = token\n        previous_token_label = token[\'label\']\n    output_entities(brat_output_folder, previous_filename, entities, text_filepath, text, overwrite=overwrite)\n    conll_file.close()\n    print(\'Done.\')\n\ndef output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder, overwrite=False):\n    # Output brat files\n    for dataset_type in [\'train\', \'valid\', \'test\', \'deploy\']:\n        if dataset_type not in output_filepaths.keys():\n            continue\n        brat_output_folder = os.path.join(stats_graph_folder, \'brat\', dataset_type)\n        utils.create_folder_if_not_exists(brat_output_folder)\n        conll_to_brat(output_filepaths[dataset_type], output_filepaths[dataset_type], dataset_brat_folders[dataset_type], brat_output_folder, overwrite=overwrite)\n'"
neuroner/dataset.py,0,"b'import sklearn.preprocessing\n\nimport collections\nimport codecs\nimport os\nimport pickle\nimport random\nimport re\nimport time\nimport token\n\nfrom neuroner import utils\nfrom neuroner import utils_nlp\n\n\nclass Dataset(object):\n    """"""A class for handling data sets.""""""\n\n    def __init__(self, name=\'\', verbose=False, debug=False):\n        self.name = name\n        self.verbose = verbose\n        self.debug = debug\n\n    def _parse_dataset(self, dataset_filepath):\n        token_count = collections.defaultdict(lambda: 0)\n        label_count = collections.defaultdict(lambda: 0)\n        character_count = collections.defaultdict(lambda: 0)\n\n        line_count = -1\n        tokens = []\n        labels = []\n        new_token_sequence = []\n        new_label_sequence = []\n        if dataset_filepath:\n            f = codecs.open(dataset_filepath, \'r\', \'UTF-8\')\n            for line in f:\n                line_count += 1\n                line = line.strip().split(\' \')\n                if len(line) == 0 or len(line[0]) == 0 or \'-DOCSTART-\' in line[0]:\n                    if len(new_token_sequence) > 0:\n                        labels.append(new_label_sequence)\n                        tokens.append(new_token_sequence)\n                        new_token_sequence = []\n                        new_label_sequence = []\n                    continue\n                token = str(line[0])\n                label = str(line[-1])\n                token_count[token] += 1\n                label_count[label] += 1\n\n                new_token_sequence.append(token)\n                new_label_sequence.append(label)\n\n                for character in token:\n                    character_count[character] += 1\n\n                if self.debug and line_count > 200: break# for debugging purposes\n\n            if len(new_token_sequence) > 0:\n                labels.append(new_label_sequence)\n                tokens.append(new_token_sequence)\n            f.close()\n        return labels, tokens, token_count, label_count, character_count\n\n\n    def _convert_to_indices(self, dataset_types):\n        tokens = self.tokens\n        labels = self.labels\n        token_to_index = self.token_to_index\n        character_to_index = self.character_to_index\n        label_to_index = self.label_to_index\n        index_to_label = self.index_to_label\n        \n        # Map tokens and labels to their indices\n        token_indices = {}\n        label_indices = {}\n        characters = {}\n        token_lengths = {}\n        character_indices = {}\n        character_indices_padded = {}\n        for dataset_type in dataset_types:\n            token_indices[dataset_type] = []\n            characters[dataset_type] = []\n            character_indices[dataset_type] = []\n            token_lengths[dataset_type] = []\n            character_indices_padded[dataset_type] = []\n            for token_sequence in tokens[dataset_type]:\n                token_indices[dataset_type].append([token_to_index.get(token, \n                    self.UNK_TOKEN_INDEX) for token in token_sequence])\n                characters[dataset_type].append([list(token) for token in token_sequence])\n                character_indices[dataset_type].append([[character_to_index.get(character, \n                    random.randint(1, max(self.index_to_character.keys()))) for character in token] for token in token_sequence])\n                token_lengths[dataset_type].append([len(token) for token in token_sequence])\n                longest_token_length_in_sequence = max(token_lengths[dataset_type][-1])\n                character_indices_padded[dataset_type].append([utils.pad_list(temp_token_indices, \n                    longest_token_length_in_sequence, self.PADDING_CHARACTER_INDEX) for temp_token_indices in character_indices[dataset_type][-1]])\n            \n            label_indices[dataset_type] = []\n            for label_sequence in labels[dataset_type]:\n                label_indices[dataset_type].append([label_to_index[label] for label in label_sequence])\n        \n        if self.verbose:\n            print(\'token_lengths[\\\'train\\\'][0][0:10]: {0}\'.format(token_lengths[\'train\'][0][0:10]))\n        if self.verbose:\n            print(\'characters[\\\'train\\\'][0][0:10]: {0}\'.format(characters[\'train\'][0][0:10]))\n        if self.verbose:\n            print(\'token_indices[\\\'train\\\'][0:10]: {0}\'.format(token_indices[\'train\'][0:10]))\n        if self.verbose:\n            print(\'label_indices[\\\'train\\\'][0:10]: {0}\'.format(label_indices[\'train\'][0:10]))\n        if self.verbose:\n            print(\'character_indices[\\\'train\\\'][0][0:10]: {0}\'.format(character_indices[\'train\'][0][0:10]))\n        if self.verbose:\n            # Vectorize the labels\n            print(\'character_indices_padded[\\\'train\\\'][0][0:10]: {0}\'.format(character_indices_padded[\'train\'][0][0:10])) \n        # [Numpy 1-hot array](http://stackoverflow.com/a/42263603/395857)\n        label_binarizer = sklearn.preprocessing.LabelBinarizer()\n        label_binarizer.fit(range(max(index_to_label.keys()) + 1))\n        label_vector_indices = {}\n        for dataset_type in dataset_types:\n            label_vector_indices[dataset_type] = []\n            for label_indices_sequence in label_indices[dataset_type]:\n                label_vector_indices[dataset_type].append(label_binarizer.transform(label_indices_sequence))\n        \n        if self.verbose:\n            print(\'label_vector_indices[\\\'train\\\'][0:2]: {0}\'.format(label_vector_indices[\'train\'][0:2]))\n        if self.verbose:\n            print(\'len(label_vector_indices[\\\'train\\\']): {0}\'.format(len(label_vector_indices[\'train\'])))\n            \n        return token_indices, label_indices, character_indices_padded, character_indices, token_lengths, characters, label_vector_indices\n\n    def update_dataset(self, dataset_filepaths, dataset_types):\n        \'\'\'\n        dataset_filepaths : dictionary with keys \'train\', \'valid\', \'test\', \'deploy\'\n        Overwrites the data of type specified in dataset_types using the existing token_to_index, character_to_index, and label_to_index mappings. \n        \'\'\'\n        for dataset_type in dataset_types:\n            self.labels[dataset_type], self.tokens[dataset_type], _, _, _ = self._parse_dataset(dataset_filepaths.get(dataset_type, None))\n        \n        token_indices, label_indices, character_indices_padded, character_indices, token_lengths, characters, label_vector_indices = self._convert_to_indices(dataset_types)\n        \n        self.token_indices.update(token_indices)\n        self.label_indices.update(label_indices)\n        self.character_indices_padded.update(character_indices_padded)\n        self.character_indices.update(character_indices)\n        self.token_lengths.update(token_lengths)\n        self.characters.update(characters)\n        self.label_vector_indices.update(label_vector_indices)\n\n    def load_dataset(self, dataset_filepaths, parameters, token_to_vector=None):\n        \'\'\'\n        dataset_filepaths : dictionary with keys \'train\', \'valid\', \'test\', \'deploy\'\n        \'\'\'\n        start_time = time.time()\n        print(\'Load dataset... \', end=\'\', flush=True)\n        if parameters[\'token_pretrained_embedding_filepath\'] != \'\':\n            if token_to_vector==None:\n                token_to_vector = utils_nlp.load_pretrained_token_embeddings(parameters)\n        else:\n            token_to_vector = {}\n        if self.verbose: \n            print(""len(token_to_vector): {0}"".format(len(token_to_vector)))\n\n        # Load pretraining dataset to ensure that index to label is compatible to the pretrained model,\n        #   and that token embeddings that are learned in the pretrained model are loaded properly.\n        all_tokens_in_pretraining_dataset = []\n        all_characters_in_pretraining_dataset = []\n        if parameters[\'use_pretrained_model\']:\n            try: \n                pretraining_dataset = pickle.load(open(os.path.join(parameters[\'pretrained_model_folder\'], \n                    \'dataset.pickle\'), \'rb\'))\n            except:\n                pretraining_dataset = utils.renamed_load(open(os.path.join(parameters[\'pretrained_model_folder\'], \n                    \'dataset.pickle\'), \'rb\'))\n            all_tokens_in_pretraining_dataset = pretraining_dataset.index_to_token.values()\n            all_characters_in_pretraining_dataset = pretraining_dataset.index_to_character.values()\n\n        remap_to_unk_count_threshold = 1\n        self.UNK_TOKEN_INDEX = 0\n        self.PADDING_CHARACTER_INDEX = 0\n        self.tokens_mapped_to_unk = []\n        self.UNK = \'UNK\'\n        self.unique_labels = []\n        labels = {}\n        tokens = {}\n        label_count = {}\n        token_count = {}\n        character_count = {}\n        for dataset_type in [\'train\', \'valid\', \'test\', \'deploy\']:\n            labels[dataset_type], tokens[dataset_type], token_count[dataset_type], label_count[dataset_type], character_count[dataset_type] \\\n                = self._parse_dataset(dataset_filepaths.get(dataset_type, None))\n\n            if self.verbose: \n                print(""dataset_type: {0}"".format(dataset_type))\n            if self.verbose: \n                print(""len(token_count[dataset_type]): {0}"".format(len(token_count[dataset_type])))\n\n        token_count[\'all\'] = {}\n        for token in list(token_count[\'train\'].keys()) + list(token_count[\'valid\'].keys()) + list(token_count[\'test\'].keys()) + list(token_count[\'deploy\'].keys()):\n            token_count[\'all\'][token] = token_count[\'train\'][token] + token_count[\'valid\'][token] + token_count[\'test\'][token] + token_count[\'deploy\'][token]\n        \n        if parameters[\'load_all_pretrained_token_embeddings\']:\n            for token in token_to_vector:\n                if token not in token_count[\'all\']:\n                    token_count[\'all\'][token] = -1\n                    token_count[\'train\'][token] = -1\n            for token in all_tokens_in_pretraining_dataset:\n                if token not in token_count[\'all\']:\n                    token_count[\'all\'][token] = -1\n                    token_count[\'train\'][token] = -1\n\n        character_count[\'all\'] = {}\n        for character in list(character_count[\'train\'].keys()) + list(character_count[\'valid\'].keys()) + list(character_count[\'test\'].keys()) + list(character_count[\'deploy\'].keys()):\n            character_count[\'all\'][character] = character_count[\'train\'][character] + character_count[\'valid\'][character] + character_count[\'test\'][character] + character_count[\'deploy\'][character]\n\n        for character in all_characters_in_pretraining_dataset:\n            if character not in character_count[\'all\']:\n                character_count[\'all\'][character] = -1\n                character_count[\'train\'][character] = -1\n\n        for dataset_type in dataset_filepaths.keys():\n            if self.verbose: print(""dataset_type: {0}"".format(dataset_type))\n            if self.verbose: print(""len(token_count[dataset_type]): {0}"".format(len(token_count[dataset_type])))\n\n        label_count[\'all\'] = {}\n        for character in list(label_count[\'train\'].keys()) + list(label_count[\'valid\'].keys()) + list(label_count[\'test\'].keys()) + list(label_count[\'deploy\'].keys()):\n            label_count[\'all\'][character] = label_count[\'train\'][character] + label_count[\'valid\'][character] + label_count[\'test\'][character] + label_count[\'deploy\'][character]\n\n        token_count[\'all\'] = utils.order_dictionary(token_count[\'all\'], \'value_key\', reverse = True)\n        label_count[\'all\'] = utils.order_dictionary(label_count[\'all\'], \'key\', reverse = False)\n        character_count[\'all\'] = utils.order_dictionary(character_count[\'all\'], \'value\', reverse = True)\n        if self.verbose: \n            print(\'character_count[\\\'all\\\']: {0}\'.format(character_count[\'all\']))\n\n        token_to_index = {}\n        token_to_index[self.UNK] = self.UNK_TOKEN_INDEX\n        iteration_number = 0\n        number_of_unknown_tokens = 0\n        if self.verbose: \n            print(""parameters[\'remap_unknown_tokens_to_unk\']: {0}"".format(parameters[\'remap_unknown_tokens_to_unk\']))\n        if self.verbose: \n            print(""len(token_count[\'train\'].keys()): {0}"".format(len(token_count[\'train\'].keys())))\n        for token, count in token_count[\'all\'].items():\n            if iteration_number == self.UNK_TOKEN_INDEX: iteration_number += 1\n\n            if parameters[\'remap_unknown_tokens_to_unk\'] == 1 and \\\n                (token_count[\'train\'][token] == 0 or \\\n                parameters[\'load_only_pretrained_token_embeddings\']) and \\\n                not utils_nlp.is_token_in_pretrained_embeddings(token, token_to_vector, parameters) and \\\n                token not in all_tokens_in_pretraining_dataset:\n                if self.verbose: \n                    print(""token: {0}"".format(token))\n                if self.verbose: \n                    print(""token.lower(): {0}"".format(token.lower()))\n                if self.verbose: \n                    print(""re.sub(\'\\d\', \'0\', token.lower()): {0}"".format(re.sub(\'\\d\', \'0\', token.lower())))\n                token_to_index[token] =  self.UNK_TOKEN_INDEX\n                number_of_unknown_tokens += 1\n                self.tokens_mapped_to_unk.append(token)\n            else:\n                token_to_index[token] = iteration_number\n                iteration_number += 1\n        if self.verbose: print(""number_of_unknown_tokens: {0}"".format(number_of_unknown_tokens))\n\n        infrequent_token_indices = []\n        for token, count in token_count[\'train\'].items():\n            if 0 < count <= remap_to_unk_count_threshold:\n                infrequent_token_indices.append(token_to_index[token])\n        if self.verbose: \n            print(""len(token_count[\'train\']): {0}"".format(len(token_count[\'train\'])))\n        if self.verbose: \n            print(""len(infrequent_token_indices): {0}"".format(len(infrequent_token_indices)))\n\n        # Ensure that both B- and I- versions exist for each label\n        labels_without_bio = set()\n        for label in label_count[\'all\'].keys():\n            new_label = utils_nlp.remove_bio_from_label_name(label)\n            labels_without_bio.add(new_label)\n        for label in labels_without_bio:\n            if label == \'O\':\n                continue\n            if parameters[\'tagging_format\'] == \'bioes\':\n                prefixes = [\'B-\', \'I-\', \'E-\', \'S-\']\n            else:\n                prefixes = [\'B-\', \'I-\']\n            for prefix in prefixes:\n                l = prefix + label\n                if l not in label_count[\'all\']:\n                    label_count[\'all\'][l] = 0\n        label_count[\'all\'] = utils.order_dictionary(label_count[\'all\'], \'key\', reverse = False)\n\n        if parameters[\'use_pretrained_model\']:\n            self.unique_labels = sorted(list(pretraining_dataset.label_to_index.keys()))\n            # Make sure labels are compatible with the pretraining dataset.\n            for label in label_count[\'all\']:\n                if label not in pretraining_dataset.label_to_index:\n                    raise AssertionError(""The label {0} does not exist in the pretraining dataset. "".format(label) +\n                                         ""Please ensure that only the following labels exist in the dataset: {0}"".format(\', \'.join(self.unique_labels)))\n            label_to_index = pretraining_dataset.label_to_index.copy()\n        else:\n            label_to_index = {}\n            iteration_number = 0\n            for label, count in label_count[\'all\'].items():\n                label_to_index[label] = iteration_number\n                iteration_number += 1\n                self.unique_labels.append(label)\n\n        if self.verbose: \n            print(\'self.unique_labels: {0}\'.format(self.unique_labels))\n\n        character_to_index = {}\n        iteration_number = 0\n        for character, count in character_count[\'all\'].items():\n            if iteration_number == self.PADDING_CHARACTER_INDEX: iteration_number += 1\n            character_to_index[character] = iteration_number\n            iteration_number += 1\n\n        if self.verbose: \n            print(\'token_count[\\\'train\\\'][0:10]: {0}\'.format(list(token_count[\'train\'].items())[0:10]))\n        token_to_index = utils.order_dictionary(token_to_index, \'value\', reverse = False)\n        if self.verbose: \n            print(\'token_to_index: {0}\'.format(token_to_index))\n        index_to_token = utils.reverse_dictionary(token_to_index)\n        if parameters[\'remap_unknown_tokens_to_unk\'] == 1: \n            index_to_token[self.UNK_TOKEN_INDEX] = self.UNK\n        if self.verbose: \n            print(\'index_to_token: {0}\'.format(index_to_token))\n\n        if self.verbose: \n            print(\'label_count[\\\'train\\\']: {0}\'.format(label_count[\'train\']))\n        label_to_index = utils.order_dictionary(label_to_index, \'value\', reverse = False)\n        if self.verbose: \n            print(\'label_to_index: {0}\'.format(label_to_index))\n        index_to_label = utils.reverse_dictionary(label_to_index)\n        if self.verbose: \n            print(\'index_to_label: {0}\'.format(index_to_label))\n\n        character_to_index = utils.order_dictionary(character_to_index, \'value\', reverse = False)\n        index_to_character = utils.reverse_dictionary(character_to_index)\n        if self.verbose: \n            print(\'character_to_index: {0}\'.format(character_to_index))\n        if self.verbose: \n            print(\'index_to_character: {0}\'.format(index_to_character))\n\n\n        if self.verbose: \n            print(\'labels[\\\'train\\\'][0:10]: {0}\'.format(labels[\'train\'][0:10]))\n        if self.verbose: \n            print(\'tokens[\\\'train\\\'][0:10]: {0}\'.format(tokens[\'train\'][0:10]))\n\n        if self.verbose:\n            # Print sequences of length 1 in train set\n            for token_sequence, label_sequence in zip(tokens[\'train\'], labels[\'train\']):\n                if len(label_sequence) == 1 and label_sequence[0] != \'O\':\n                    print(""{0}\\t{1}"".format(token_sequence[0], label_sequence[0]))\n\n        self.token_to_index = token_to_index\n        self.index_to_token = index_to_token\n        self.index_to_character = index_to_character\n        self.character_to_index = character_to_index\n        self.index_to_label = index_to_label\n        self.label_to_index = label_to_index\n        if self.verbose: \n            print(""len(self.token_to_index): {0}"".format(len(self.token_to_index)))\n        if self.verbose: \n            print(""len(self.index_to_token): {0}"".format(len(self.index_to_token)))\n        self.tokens = tokens\n        self.labels = labels\n\n        token_indices, label_indices, character_indices_padded, character_indices, token_lengths, characters, label_vector_indices = self._convert_to_indices(dataset_filepaths.keys())\n        \n        self.token_indices = token_indices\n        self.label_indices = label_indices\n        self.character_indices_padded = character_indices_padded\n        self.character_indices = character_indices\n        self.token_lengths = token_lengths\n        self.characters = characters\n        self.label_vector_indices = label_vector_indices\n\n        self.number_of_classes = max(self.index_to_label.keys()) + 1\n        self.vocabulary_size = max(self.index_to_token.keys()) + 1\n        self.alphabet_size = max(self.index_to_character.keys()) + 1\n        if self.verbose: \n            print(""self.number_of_classes: {0}"".format(self.number_of_classes))\n        if self.verbose: \n            print(""self.alphabet_size: {0}"".format(self.alphabet_size))\n        if self.verbose: \n            print(""self.vocabulary_size: {0}"".format(self.vocabulary_size))\n\n        # unique_labels_of_interest is used to compute F1-scores.\n        self.unique_labels_of_interest = list(self.unique_labels)\n        self.unique_labels_of_interest.remove(\'O\')\n\n        self.unique_label_indices_of_interest = []\n        for lab in self.unique_labels_of_interest:\n            self.unique_label_indices_of_interest.append(label_to_index[lab])\n\n        self.infrequent_token_indices = infrequent_token_indices\n\n        if self.verbose: \n            print(\'self.unique_labels_of_interest: {0}\'.format(self.unique_labels_of_interest))\n        if self.verbose: \n            print(\'self.unique_label_indices_of_interest: {0}\'.format(self.unique_label_indices_of_interest))\n\n        elapsed_time = time.time() - start_time\n        print(\'done ({0:.2f} seconds)\'.format(elapsed_time))\n        \n        return token_to_vector\n'"
neuroner/entity_lstm.py,107,"b'import os\nimport pickle\nimport re\nimport time\n\nimport tensorflow as tf\n\nfrom neuroner import utils\nfrom neuroner import utils_tf\nfrom neuroner import utils_nlp\n\ndef bidirectional_LSTM(inputs, hidden_state_dimension, initializer, \n    sequence_length=None, output_sequence=True):\n    """"""\n    """"""\n    with tf.variable_scope(""bidirectional_LSTM""):\n        if sequence_length == None:\n            batch_size = 1\n            sequence_length = tf.shape(inputs)[1]\n            sequence_length = tf.expand_dims(sequence_length, axis=0, \n                name=\'sequence_length\')\n        else:\n            batch_size = tf.shape(sequence_length)[0]\n\n        lstm_cell = {}\n        initial_state = {}\n        \n        for direction in [""forward"", ""backward""]:\n            with tf.variable_scope(direction):\n                # LSTM cell\n                lstm_cell[direction] = tf.contrib.rnn.CoupledInputForgetGateLSTMCell(hidden_state_dimension, \n                    forget_bias=1.0, initializer=initializer, state_is_tuple=True)\n                # initial state: http://stackoverflow.com/questions/38441589/tensorflow-rnn-initial-state\n                initial_cell_state = tf.get_variable(""initial_cell_state"", shape=[1, hidden_state_dimension], \n                    dtype=tf.float32, initializer=initializer)\n                initial_output_state = tf.get_variable(""initial_output_state"", shape=[1, \n                    hidden_state_dimension], dtype=tf.float32, initializer=initializer)\n                c_states = tf.tile(initial_cell_state, tf.stack([batch_size, 1]))\n                h_states = tf.tile(initial_output_state, tf.stack([batch_size, 1]))\n                initial_state[direction] = tf.contrib.rnn.LSTMStateTuple(c_states, \n                    h_states)\n\n        # sequence_length must be provided for tf.nn.bidirectional_dynamic_rnn due to internal bug\n        outputs, final_states = tf.nn.bidirectional_dynamic_rnn(lstm_cell[""forward""],\n            lstm_cell[""backward""],inputs = inputs, dtype=tf.float32, sequence_length=sequence_length,\n            initial_state_fw=initial_state[""forward""], initial_state_bw=initial_state[""backward""])\n\n        if output_sequence == True:\n            outputs_forward, outputs_backward = outputs\n            output = tf.concat([outputs_forward, outputs_backward], axis=2, name=\'output_sequence\')\n        else:\n            # # max pooling\n            # outputs_forward, outputs_backward = outputs\n            # output = tf.concat([outputs_forward, outputs_backward], axis=2, name=\'output_sequence\')\n            # output = tf.reduce_max(output, axis=1, name=\'output\')\n            # # last pooling\n            final_states_forward, final_states_backward = final_states\n            output = tf.concat([final_states_forward[1], final_states_backward[1]], axis=1, name=\'output\')\n\n    return output\n\n\nclass EntityLSTM(object):\n    """"""\n    An LSTM architecture for named entity recognition.\n    Uses a character embedding layer followed by an LSTM to generate vector \n    representation from characters for each token.\n    Then the character vector is concatenated with token embedding vector, \n    which is input to another LSTM  followed by a CRF layer.\n    """"""\n    def __init__(self, dataset, parameters):\n\n        self.verbose = False\n\n        # Placeholders for input, output and dropout\n        self.input_token_indices = tf.placeholder(tf.int32, [None], \n            name=""input_token_indices"")\n        self.input_label_indices_vector = tf.placeholder(tf.float32, [None, \n            dataset.number_of_classes], name=""input_label_indices_vector"")\n        self.input_label_indices_flat = tf.placeholder(tf.int32, [None], \n            name=""input_label_indices_flat"")\n        self.input_token_character_indices = tf.placeholder(tf.int32, [None, None], \n            name=""input_token_indices"")\n        self.input_token_lengths = tf.placeholder(tf.int32, [None], \n            name=""input_token_lengths"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, \n            name=""dropout_keep_prob"")\n\n        # Internal parameters\n        initializer = tf.contrib.layers.xavier_initializer()\n\n        if parameters[\'use_character_lstm\']:\n            # Character-level LSTM\n            # Idea: reshape so that we have a tensor [number_of_token, max_token_length, \n            # token_embeddings_size], which we pass to the LSTM\n\n            # Character embedding layer\n            with tf.variable_scope(""character_embedding""):\n                self.character_embedding_weights = tf.get_variable(\n                    ""character_embedding_weights"",\n                    shape=[dataset.alphabet_size, parameters[\'character_embedding_dimension\']],\n                    initializer=initializer)\n                embedded_characters = tf.nn.embedding_lookup(self.character_embedding_weights, \n                    self.input_token_character_indices, name=\'embedded_characters\')\n                if self.verbose: \n                    print(""embedded_characters: {0}"".format(embedded_characters))\n                utils_tf.variable_summaries(self.character_embedding_weights)\n\n            # Character LSTM layer\n            with tf.variable_scope(\'character_lstm\') as vs:\n                character_lstm_output = bidirectional_LSTM(embedded_characters, \n                    parameters[\'character_lstm_hidden_state_dimension\'], initializer,\n                    sequence_length=self.input_token_lengths, output_sequence=False)\n                self.character_lstm_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                    scope=vs.name)\n\n        # Token embedding layer\n        with tf.variable_scope(""token_embedding""):\n            self.token_embedding_weights = tf.get_variable(\n                ""token_embedding_weights"",\n                shape=[dataset.vocabulary_size, parameters[\'token_embedding_dimension\']],\n                initializer=initializer,\n                trainable=not parameters[\'freeze_token_embeddings\'])\n            embedded_tokens = tf.nn.embedding_lookup(self.token_embedding_weights, \n                self.input_token_indices)\n            utils_tf.variable_summaries(self.token_embedding_weights)\n\n        # Concatenate character LSTM outputs and token embeddings\n        if parameters[\'use_character_lstm\']:\n            with tf.variable_scope(""concatenate_token_and_character_vectors""):\n\n                if self.verbose: \n                    print(\'embedded_tokens: {0}\'.format(embedded_tokens))\n                \n                token_lstm_input = tf.concat([character_lstm_output, embedded_tokens], \n                    axis=1, name=\'token_lstm_input\')\n\n                if self.verbose: \n                    print(""token_lstm_input: {0}"".format(token_lstm_input))\n        else:\n            token_lstm_input = embedded_tokens\n\n        # Add dropout\n        with tf.variable_scope(""dropout""):\n            token_lstm_input_drop = tf.nn.dropout(token_lstm_input, self.dropout_keep_prob, \n                name=\'token_lstm_input_drop\')\n\n            if self.verbose: \n                print(""token_lstm_input_drop: {0}"".format(token_lstm_input_drop))\n            \n            # https://www.tensorflow.org/api_guides/python/contrib.rnn\n            # Prepare data shape to match `rnn` function requirements\n            # Current data input shape: (batch_size, n_steps, n_input)\n            # Required shape: \'n_steps\' tensors list of shape (batch_size, n_input)\n            token_lstm_input_drop_expanded = tf.expand_dims(token_lstm_input_drop, \n                axis=0, name=\'token_lstm_input_drop_expanded\')\n\n            if self.verbose: \n                print(""token_lstm_input_drop_expanded: {0}"".format(token_lstm_input_drop_expanded))\n\n        # Token LSTM layer\n        with tf.variable_scope(\'token_lstm\') as vs:\n            token_lstm_output = bidirectional_LSTM(token_lstm_input_drop_expanded, \n                parameters[\'token_lstm_hidden_state_dimension\'], initializer, output_sequence=True)\n\n            token_lstm_output_squeezed = tf.squeeze(token_lstm_output, axis=0)\n\n            self.token_lstm_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                scope=vs.name)\n\n        # Needed only if Bidirectional LSTM is used for token level\n        with tf.variable_scope(""feedforward_after_lstm"") as vs:\n\n            W = tf.get_variable(\n                ""W"",\n                shape=[2 * parameters[\'token_lstm_hidden_state_dimension\'], \n                parameters[\'token_lstm_hidden_state_dimension\']],\n                initializer=initializer)\n\n            b = tf.Variable(tf.constant(0.0, shape=[parameters[\'token_lstm_hidden_state_dimension\']]), \n                name=""bias"")\n\n            outputs = tf.nn.xw_plus_b(token_lstm_output_squeezed, W, b, name=""output_before_tanh"")\n            outputs = tf.nn.tanh(outputs, name=""output_after_tanh"")\n            utils_tf.variable_summaries(W)\n            utils_tf.variable_summaries(b)\n            self.token_lstm_variables += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=vs.name)\n\n        with tf.variable_scope(""feedforward_before_crf"") as vs:\n            W = tf.get_variable(\n                ""W"",\n                shape=[parameters[\'token_lstm_hidden_state_dimension\'], \n                dataset.number_of_classes],\n                initializer=initializer)\n            \n            b = tf.Variable(tf.constant(0.0, shape=[dataset.number_of_classes]), \n                name=""bias"")\n            \n            scores = tf.nn.xw_plus_b(outputs, W, b, name=""scores"")\n            self.unary_scores = scores\n            self.predictions = tf.argmax(self.unary_scores, 1, name=""predictions"")\n            utils_tf.variable_summaries(W)\n            utils_tf.variable_summaries(b)\n            self.feedforward_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                scope=vs.name)\n\n        # CRF layer\n        if parameters[\'use_crf\']:\n            with tf.variable_scope(""crf"") as vs:\n                # Add start and end tokens\n                small_score = -1000.0\n                large_score = 0.0\n                sequence_length = tf.shape(self.unary_scores)[0]\n                unary_scores_with_start_and_end = tf.concat([self.unary_scores, \n                    tf.tile( tf.constant(small_score, shape=[1, 2]) , [sequence_length, 1])], 1)\n                start_unary_scores = [[small_score] * dataset.number_of_classes + [large_score, small_score]]\n                end_unary_scores = [[small_score] * dataset.number_of_classes + [small_score, large_score]]\n                self.unary_scores = tf.concat([start_unary_scores, unary_scores_with_start_and_end, \n                    end_unary_scores], 0)\n                start_index = dataset.number_of_classes\n                end_index = dataset.number_of_classes + 1\n                input_label_indices_flat_with_start_and_end = tf.concat([tf.constant(start_index, \n                    shape=[1]), self.input_label_indices_flat, tf.constant(end_index, \n                    shape=[1]) ], 0)\n\n                # Apply CRF layer\n                sequence_length = tf.shape(self.unary_scores)[0]\n                sequence_lengths = tf.expand_dims(sequence_length, axis=0, \n                    name=\'sequence_lengths\')\n                unary_scores_expanded = tf.expand_dims(self.unary_scores, axis=0, \n                    name=\'unary_scores_expanded\')\n                input_label_indices_flat_batch = tf.expand_dims(input_label_indices_flat_with_start_and_end, \n                    axis=0, name=\'input_label_indices_flat_batch\')\n                \n                if self.verbose: \n                    print(\'unary_scores_expanded: {0}\'.format(unary_scores_expanded))\n                    print(\'input_label_indices_flat_batch: {0}\'.format(input_label_indices_flat_batch))\n                    print(""sequence_lengths: {0}"".format(sequence_lengths))\n\n                # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf\n                # Compute the log-likelihood of the gold sequences and keep the \n                # transition params for inference at test time.\n                self.transition_parameters=tf.get_variable(\n                    ""transitions"",\n                    shape=[dataset.number_of_classes+2, dataset.number_of_classes+2],\n                    initializer=initializer)\n                utils_tf.variable_summaries(self.transition_parameters)\n                log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n                    unary_scores_expanded, input_label_indices_flat_batch, sequence_lengths, \n                    transition_params=self.transition_parameters)\n\n                self.loss =  tf.reduce_mean(-log_likelihood, name=\'cross_entropy_mean_loss\')\n                self.accuracy = tf.constant(1)\n                self.crf_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                    scope=vs.name)\n\n        # Do not use CRF layer\n        else:\n            with tf.variable_scope(""crf"") as vs:\n                self.transition_parameters = tf.get_variable(\n                    ""transitions"",\n                    shape=[dataset.number_of_classes+2, dataset.number_of_classes+2],\n                    initializer=initializer)\n                utils_tf.variable_summaries(self.transition_parameters)\n                self.crf_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                    scope=vs.name)\n\n            # Calculate mean cross-entropy loss\n            with tf.variable_scope(""loss""):\n                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.unary_scores, \n                    labels=self.input_label_indices_vector, name=\'softmax\')\n                self.loss =  tf.reduce_mean(losses, name=\'cross_entropy_mean_loss\')\n            with tf.variable_scope(""accuracy""):\n                correct_predictions = tf.equal(self.predictions, \n                    tf.argmax(self.input_label_indices_vector, 1))\n                self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \'float\'), \n                    name=\'accuracy\')\n\n        self.define_training_procedure(parameters)\n        self.summary_op = tf.summary.merge_all()\n        # defaults to saving all variables\n        self.saver = tf.train.Saver(max_to_keep=parameters[\'maximum_number_of_epochs\']) \n\n    def define_training_procedure(self, parameters):\n        """"""\n        Define training procedure\n        """"""\n        self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\n\n        if parameters[\'optimizer\'] == \'adam\':\n            self.optimizer = tf.train.AdamOptimizer(parameters[\'learning_rate\'])\n        elif parameters[\'optimizer\'] == \'sgd\':\n            self.optimizer = tf.train.GradientDescentOptimizer(parameters[\'learning_rate\'])\n        elif parameters[\'optimizer\'] == \'adadelta\':\n            self.optimizer = tf.train.AdadeltaOptimizer(parameters[\'learning_rate\'])\n        else:\n            raise ValueError(\'The lr_method parameter must be either adadelta, adam or sgd.\')\n\n        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n\n        if parameters[\'gradient_clipping_value\']:\n            grads_and_vars = [(tf.clip_by_value(grad, -parameters[\'gradient_clipping_value\'], \n                parameters[\'gradient_clipping_value\']), var) for grad, var in grads_and_vars]\n\n        # By defining a global_step variable and passing it to the optimizer \n        # we allow TensorFlow handle the counting of training steps for us.\n        # The global step will be automatically incremented by one every time \n        # you execute train_op.\n        self.train_op = self.optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n\n    def load_pretrained_token_embeddings(self, sess, dataset, parameters, \n        token_to_vector=None):\n        """"""\n        """"""\n        if parameters[\'token_pretrained_embedding_filepath\'] == \'\':\n            return\n\n        # Load embeddings\n        start_time = time.time()\n        print(\'Load token embeddings... \', end=\'\', flush=True)\n\n        if token_to_vector == None:\n            token_to_vector = utils_nlp.load_pretrained_token_embeddings(parameters)\n\n        initial_weights = sess.run(self.token_embedding_weights.read_value())\n        number_of_loaded_word_vectors = 0\n        number_of_token_original_case_found = 0\n        number_of_token_lowercase_found = 0\n        number_of_token_digits_replaced_with_zeros_found = 0\n        number_of_token_lowercase_and_digits_replaced_with_zeros_found = 0\n\n        for token in dataset.token_to_index.keys():\n            if token in token_to_vector.keys():\n                initial_weights[dataset.token_to_index[token]] = token_to_vector[token]\n                number_of_token_original_case_found += 1\n            elif parameters[\'check_for_lowercase\'] and token.lower() in token_to_vector.keys():\n                initial_weights[dataset.token_to_index[token]] = token_to_vector[token.lower()]\n                number_of_token_lowercase_found += 1\n            elif parameters[\'check_for_digits_replaced_with_zeros\'] and re.sub(r\'\\d\', \n                \'0\', token) in token_to_vector.keys():\n                initial_weights[dataset.token_to_index[token]] = token_to_vector[re.sub(r\'\\d\', \n                    \'0\', token)]\n                number_of_token_digits_replaced_with_zeros_found += 1\n            elif parameters[\'check_for_lowercase\'] and parameters[\'check_for_digits_replaced_with_zeros\'] \\\n            and re.sub(\'\\d\', \'0\', token.lower()) in token_to_vector.keys():\n                initial_weights[dataset.token_to_index[token]] = token_to_vector[re.sub(r\'\\d\', \n                    \'0\', token.lower())]\n                number_of_token_lowercase_and_digits_replaced_with_zeros_found += 1\n            else:\n                continue\n            number_of_loaded_word_vectors += 1\n        elapsed_time = time.time() - start_time\n        \n        print(\'done ({0:.2f} seconds)\'.format(elapsed_time))\n        print(""number_of_token_original_case_found: {0}"".format(number_of_token_original_case_found))\n        print(""number_of_token_lowercase_found: {0}"".format(number_of_token_lowercase_found))\n        print(""number_of_token_digits_replaced_with_zeros_found: {0}"".format(number_of_token_digits_replaced_with_zeros_found))\n        print(""number_of_token_lowercase_and_digits_replaced_with_zeros_found: {0}"".format(number_of_token_lowercase_and_digits_replaced_with_zeros_found))\n        print(\'number_of_loaded_word_vectors: {0}\'.format(number_of_loaded_word_vectors))\n        print(""dataset.vocabulary_size: {0}"".format(dataset.vocabulary_size))\n        sess.run(self.token_embedding_weights.assign(initial_weights))\n\n    def load_embeddings_from_pretrained_model(self, sess, dataset, pretraining_dataset, \n        pretrained_embedding_weights, embedding_type=\'token\'):\n        """"""\n        """"""\n        if embedding_type == \'token\':\n            embedding_weights = self.token_embedding_weights\n            index_to_string = dataset.index_to_token\n            pretraining_string_to_index = pretraining_dataset.token_to_index\n        elif embedding_type == \'character\':\n            embedding_weights = self.character_embedding_weights\n            index_to_string = dataset.index_to_character\n            pretraining_string_to_index = pretraining_dataset.character_to_index\n        # Load embeddings\n        start_time = time.time()\n        print(\'Load {0} embeddings from pretrained model... \'.format(embedding_type), \n            end=\'\', flush=True)\n        initial_weights = sess.run(embedding_weights.read_value())\n\n        if embedding_type == \'token\':\n            initial_weights[dataset.UNK_TOKEN_INDEX] = pretrained_embedding_weights[pretraining_dataset.UNK_TOKEN_INDEX]\n        elif embedding_type == \'character\':\n            initial_weights[dataset.PADDING_CHARACTER_INDEX] = pretrained_embedding_weights[pretraining_dataset.PADDING_CHARACTER_INDEX]\n\n        number_of_loaded_vectors = 1\n        for index, string in index_to_string.items():\n            if index == dataset.UNK_TOKEN_INDEX:\n                continue\n            if string in pretraining_string_to_index.keys():\n                initial_weights[index] = pretrained_embedding_weights[pretraining_string_to_index[string]]\n                number_of_loaded_vectors += 1\n        elapsed_time = time.time() - start_time\n\n        print(\'done ({0:.2f} seconds)\'.format(elapsed_time))\n        print(""number_of_loaded_vectors: {0}"".format(number_of_loaded_vectors))\n\n        if embedding_type == \'token\':\n            print(""dataset.vocabulary_size: {0}"".format(dataset.vocabulary_size))\n        elif embedding_type == \'character\':\n            print(""dataset.alphabet_size: {0}"".format(dataset.alphabet_size))\n        \n        sess.run(embedding_weights.assign(initial_weights))\n\n    def restore_from_pretrained_model(self, parameters, dataset, sess, \n        token_to_vector=None):\n        """"""\n        """"""\n        try:\n            pretraining_dataset = pickle.load(open(os.path.join(parameters[\'pretrained_model_folder\'], \n                \'dataset.pickle\'), \'rb\')) \n        except:\n            pretraining_dataset = utils.renamed_load(open(os.path.join(parameters[\'pretrained_model_folder\'], \n                \'dataset.pickle\'), \'rb\'))\n\n        pretrained_model_checkpoint_filepath = os.path.join(parameters[\'pretrained_model_folder\'], \n            \'model.ckpt\')\n        \n        # Assert that the label sets are the same\n        # Test set should have the same label set as the pretrained dataset\n        assert pretraining_dataset.index_to_label == dataset.index_to_label\n    \n        # If the token and character mappings are exactly the same\n        if pretraining_dataset.index_to_token == dataset.index_to_token and \\\n        pretraining_dataset.index_to_character == dataset.index_to_character:\n            \n            # Restore the pretrained model\n            # Works only when the dimensions of tensor variables are matched.\n            self.saver.restore(sess, pretrained_model_checkpoint_filepath) \n        \n        # If the token and character mappings are different between the pretrained model \n        # and the current model\n        else:\n            \n            # Resize the token and character embedding weights to match them with \n            # the pretrained model (required in order to restore the pretrained model)\n            utils_tf.resize_tensor_variable(sess, self.character_embedding_weights, \n                [pretraining_dataset.alphabet_size, parameters[\'character_embedding_dimension\']])\n            utils_tf.resize_tensor_variable(sess, self.token_embedding_weights, \n                [pretraining_dataset.vocabulary_size, parameters[\'token_embedding_dimension\']])\n        \n            # Restore the pretrained model\n            # Works only when the dimensions of tensor variables are matched.\n            self.saver.restore(sess, pretrained_model_checkpoint_filepath) \n            \n            # Get pretrained embeddings\n            character_embedding_weights, token_embedding_weights = sess.run([self.character_embedding_weights, \n                self.token_embedding_weights]) \n            \n            # Restore the sizes of token and character embedding weights\n            utils_tf.resize_tensor_variable(sess, self.character_embedding_weights, \n                [dataset.alphabet_size, parameters[\'character_embedding_dimension\']])\n            utils_tf.resize_tensor_variable(sess, self.token_embedding_weights, \n                [dataset.vocabulary_size, parameters[\'token_embedding_dimension\']]) \n            \n            # Re-initialize the token and character embedding weights\n            sess.run(tf.variables_initializer([self.character_embedding_weights, \n                self.token_embedding_weights]))\n            \n            # Load embedding weights from pretrained token embeddings first\n            self.load_pretrained_token_embeddings(sess, dataset, parameters, \n                token_to_vector=token_to_vector) \n            \n            # Load embedding weights from pretrained model\n            self.load_embeddings_from_pretrained_model(sess, dataset, pretraining_dataset, \n                token_embedding_weights, embedding_type=\'token\')\n            self.load_embeddings_from_pretrained_model(sess, dataset, pretraining_dataset, \n                character_embedding_weights, embedding_type=\'character\') \n            \n            del pretraining_dataset\n            del character_embedding_weights\n            del token_embedding_weights\n        \n        # Get transition parameters\n        transition_params_trained = sess.run(self.transition_parameters)\n        \n        if not parameters[\'reload_character_embeddings\']:\n            sess.run(tf.variables_initializer([self.character_embedding_weights]))\n        if not parameters[\'reload_character_lstm\']:\n            sess.run(tf.variables_initializer(self.character_lstm_variables))\n        if not parameters[\'reload_token_embeddings\']:\n            sess.run(tf.variables_initializer([self.token_embedding_weights]))\n        if not parameters[\'reload_token_lstm\']:\n            sess.run(tf.variables_initializer(self.token_lstm_variables))\n        if not parameters[\'reload_feedforward\']:\n            sess.run(tf.variables_initializer(self.feedforward_variables))\n        if not parameters[\'reload_crf\']:\n            sess.run(tf.variables_initializer(self.crf_variables))\n    \n        return transition_params_trained\n'"
neuroner/evaluate.py,0,"b'import json\nimport os\nimport pkg_resources\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.metrics\n\nfrom neuroner import utils_plots\nfrom neuroner import utils_nlp\n\ndef assess_model(y_pred, y_true, labels, target_names, labels_with_o, target_names_with_o, dataset_type, stats_graph_folder, epoch_number, parameters,\n                 evaluation_mode=\'bio\', verbose=False):\n    results = {}\n    assert len(y_true) == len(y_pred)\n\n    # Classification report\n    classification_report = sklearn.metrics.classification_report(y_true, y_pred, labels=labels, target_names=target_names, sample_weight=None, digits=4)\n\n    utils_plots.plot_classification_report(classification_report,\n                                           title=\'Classification report for epoch {0} in {1} ({2} evaluation)\\n\'.format(epoch_number, dataset_type,\n                                                                                                                        evaluation_mode),\n                                           cmap=\'RdBu\')\n    plt.savefig(os.path.join(stats_graph_folder, \'classification_report_for_epoch_{0:04d}_in_{1}_{2}_evaluation.{3}\'.format(epoch_number, dataset_type,\n                                                                                                                            evaluation_mode, parameters[\'plot_format\'])),\n                dpi=300, format=parameters[\'plot_format\'], bbox_inches=\'tight\')\n    plt.close()\n    results[\'classification_report\'] = classification_report\n\n    # F1 scores\n    results[\'f1_score\'] = {}\n    for f1_average_style in [\'weighted\', \'micro\', \'macro\']:\n        results[\'f1_score\'][f1_average_style] = sklearn.metrics.f1_score(y_true, y_pred, average=f1_average_style, labels=labels)*100\n    results[\'f1_score\'][\'per_label\'] = [x*100 for x in sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, average=None, labels=labels)[2].tolist()]\n\n    confusion_matrix = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=labels_with_o)\n    results[\'confusion_matrix\'] = confusion_matrix.tolist()\n    title = \'Confusion matrix for epoch {0} in {1} ({2} evaluation)\\n\'.format(epoch_number, dataset_type, evaluation_mode)\n    xlabel = \'Predicted\'\n    ylabel = \'True\'\n    xticklabels = yticklabels = target_names_with_o\n    utils_plots.heatmap(confusion_matrix, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=True, fmt=""%d"", \n                        remove_diagonal=True)\n    plt.savefig(os.path.join(stats_graph_folder, \'confusion_matrix_for_epoch_{0:04d}_in_{1}_{2}_evaluation.{3}\'.format(epoch_number, dataset_type,\n                                                                                                                       evaluation_mode, parameters[\'plot_format\'])),\n                dpi=300, format=parameters[\'plot_format\'], bbox_inches=\'tight\')\n    plt.close()\n\n    # Accuracy\n    results[\'accuracy_score\'] = sklearn.metrics.accuracy_score(y_true, y_pred)*100\n\n    return results\n\n\ndef save_results(results, stats_graph_folder):\n    \'\'\'\n    Save results\n    \'\'\'\n    json.dump(results, open(os.path.join(stats_graph_folder, \'results.json\'), \'w\'), indent = 4, sort_keys=True)\n\n\ndef plot_f1_vs_epoch(results, stats_graph_folder, metric, parameters, from_json=False):\n    \'\'\'\n    Takes results dictionary and saves the f1 vs epoch plot in stats_graph_folder.\n    from_json indicates if the results dictionary was loaded from results.json file.\n    In this case, dictionary indexes are mapped from string to int.\n\n    metric can be f1_score or accuracy\n    \'\'\'\n\n    assert(metric in [\'f1_score\', \'accuracy_score\', \'f1_conll\'])\n\n    if not from_json:\n        epoch_idxs = sorted(results[\'epoch\'].keys())\n    else:\n        epoch_idxs = sorted(map(int, results[\'epoch\'].keys()))    # when loading json file\n\n    dataset_types = []\n    for dataset_type in [\'train\', \'valid\', \'test\']:\n        if dataset_type in results[\'epoch\'][epoch_idxs[0]][-1]:\n            dataset_types.append(dataset_type)\n    if len(dataset_type) < 2:\n        return\n\n    f1_dict_all = {}\n    for dataset_type in dataset_types:\n        f1_dict_all[dataset_type] = []\n    for eidx in epoch_idxs:\n        if not from_json:\n            result_epoch = results[\'epoch\'][eidx][-1]\n        else:\n            result_epoch = results[\'epoch\'][str(eidx)][-1]    # when loading json file\n        for dataset_type in dataset_types:\n            f1_dict_all[dataset_type].append(result_epoch[dataset_type][metric])\n\n\n    # Plot micro f1 vs epoch for all classes\n    plt.figure()\n    plot_handles = []\n    f1_all = {}\n    for dataset_type in dataset_types:\n        if dataset_type not in results: results[dataset_type] = {}\n        if metric in [\'f1_score\', \'f1_conll\']:\n            f1 = [f1_dict[\'micro\'] for f1_dict in f1_dict_all[dataset_type]]\n        else:\n            f1 = [score_value for score_value in f1_dict_all[dataset_type]]\n        results[dataset_type][\'best_{0}\'.format(metric)] = max(f1)\n        results[dataset_type][\'epoch_for_best_{0}\'.format(metric)] = int(np.asarray(f1).argmax())\n        f1_all[dataset_type] = f1\n        plot_handles.extend(plt.plot(epoch_idxs, f1, \'-\', label=dataset_type + \' (max: {0:.4f})\'.format(results[dataset_type][\'best_{0}\'.format(metric)])))\n    # Record the best values according to the best epoch for valid\n    best_epoch = results[\'valid\'][\'epoch_for_best_{0}\'.format(metric)]\n    plt.axvline(x=best_epoch, color=\'k\', linestyle=\':\')   # Add a vertical line at best epoch for valid\n    for dataset_type in dataset_types:\n        best_score_based_on_valid = f1_all[dataset_type][best_epoch]\n        results[dataset_type][\'best_{0}_based_on_valid\'.format(metric)] = best_score_based_on_valid\n        if dataset_type == \'test\':\n            plot_handles.append(plt.axhline(y=best_score_based_on_valid, label=dataset_type + \' (best: {0:.4f})\'.format(best_score_based_on_valid),\n                                            color=\'k\', linestyle=\':\'))\n        else:\n            plt.axhline(y=best_score_based_on_valid, label=\'{0:.4f}\'.format(best_score_based_on_valid), color=\'k\', linestyle=\':\')\n    title = \'{0} vs epoch number for all classes\\n\'.format(metric)\n    xlabel = \'epoch number\'\n    ylabel = metric\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.legend(handles=plot_handles, loc=0)\n    plt.savefig(os.path.join(stats_graph_folder, \'{0}_vs_epoch_for_all_classes.{1}\'.format(metric, parameters[\'plot_format\'])))\n    plt.close()\n\n\ndef result_to_plot(folder_name=None):\n    \'\'\'\n    Loads results.json file in the ../stats_graphs/folder_name, and plot f1 vs epoch.\n    Use for debugging purposes, or in case the program stopped due to error in plot_f1_vs_epoch.\n    \'\'\'\n    stats_graph_folder=os.path.join(\'.\', \'stats_graphs\')\n    if folder_name == None:\n        # Getting a list of all subdirectories in the current directory. Not recursive.\n        subfolders = os.listdir(stats_graph_folder)\n    else:\n        subfolders = [folder_name]\n\n    for subfolder in subfolders:\n        subfolder_filepath = os.path.join(stats_graph_folder, subfolder)\n        result_filepath = os.path.join(stats_graph_folder, subfolder, \'results.json\')\n        if not os.path.isfile(result_filepath): continue\n        result = json.load(open(result_filepath, \'r\'))\n        for metric in [\'accuracy_score\', \'f1_score\']:\n            plot_f1_vs_epoch(result, subfolder_filepath, metric, from_json=True)\n\n\ndef remap_labels(y_pred, y_true, dataset, evaluation_mode=\'bio\'):\n    \'\'\'\n    y_pred: list of predicted labels\n    y_true: list of gold labels\n    evaluation_mode: \'bio\', \'token\', or \'binary\'\n\n    Both y_pred and y_true must use label indices and names specified in the dataset\n#     (dataset.unique_label_indices_of_interest, dataset.unique_label_indices_of_interest).\n    \'\'\'\n    all_unique_labels = dataset.unique_labels\n    if evaluation_mode == \'bio\':\n        # sort label to index\n        new_label_names = all_unique_labels[:]\n        new_label_names.remove(\'O\')\n        new_label_names.sort(key=lambda x: (utils_nlp.remove_bio_from_label_name(x), x))\n        new_label_names.append(\'O\')\n        new_label_indices = list(range(len(new_label_names)))\n        new_label_to_index = dict(zip(new_label_names, new_label_indices))\n\n        remap_index = {}\n        for i, label_name in enumerate(new_label_names):\n            label_index = dataset.label_to_index[label_name]\n            remap_index[label_index] = i\n\n    elif evaluation_mode == \'token\':\n        new_label_names = set()\n        for label_name in all_unique_labels:\n            if label_name == \'O\':\n                continue\n            new_label_name = utils_nlp.remove_bio_from_label_name(label_name)\n            new_label_names.add(new_label_name)\n        new_label_names = sorted(list(new_label_names)) + [\'O\']\n        new_label_indices = list(range(len(new_label_names)))\n        new_label_to_index = dict(zip(new_label_names, new_label_indices))\n\n        remap_index = {}\n        for label_name in all_unique_labels:\n            new_label_name = utils_nlp.remove_bio_from_label_name(label_name)\n            label_index = dataset.label_to_index[label_name]\n            remap_index[label_index] = new_label_to_index[new_label_name]\n\n    elif evaluation_mode == \'binary\':\n        new_label_names = [\'NAMED_ENTITY\', \'O\']\n        new_label_indices = [0, 1]\n        new_label_to_index = dict(zip(new_label_names, new_label_indices))\n\n        remap_index = {}\n        for label_name in all_unique_labels:\n            new_label_name = \'O\'\n            if label_name != \'O\':\n                new_label_name = \'NAMED_ENTITY\'\n            label_index = dataset.label_to_index[label_name]\n            remap_index[label_index] = new_label_to_index[new_label_name]\n\n    else:\n        raise ValueError(""evaluation_mode must be either \'bio\', \'token\', or \'binary\'."")\n\n    new_y_pred = [ remap_index[label_index] for label_index in y_pred ]\n    new_y_true = [ remap_index[label_index] for label_index in y_true ]\n\n    new_label_indices_with_o = new_label_indices[:]\n    new_label_names_with_o = new_label_names[:]\n    new_label_names.remove(\'O\')\n    new_label_indices.remove(new_label_to_index[\'O\'])\n\n    return new_y_pred, new_y_true, new_label_indices, new_label_names, new_label_indices_with_o, new_label_names_with_o\n\n\ndef evaluate_model(results, dataset, y_pred_all, y_true_all, stats_graph_folder, epoch_number, epoch_start_time, output_filepaths, parameters, verbose=False):\n    results[\'execution_details\'][\'num_epochs\'] = epoch_number\n    results[\'epoch\'][epoch_number] = []\n    result_update = {}\n\n    for dataset_type in [\'train\', \'valid\', \'test\']:\n        if dataset_type not in output_filepaths.keys():\n            continue\n        print(\'Generating plots for the {0} set\'.format(dataset_type))\n        result_update[dataset_type] = {}\n        y_pred_original = y_pred_all[dataset_type]\n        y_true_original = y_true_all[dataset_type]\n\n        for evaluation_mode in [\'bio\', \'token\', \'binary\']:\n            y_pred, y_true, label_indices, label_names, label_indices_with_o, label_names_with_o = remap_labels(y_pred_original, y_true_original, dataset,\n                                                                                                                evaluation_mode=evaluation_mode)\n            result_update[dataset_type][evaluation_mode] = assess_model(y_pred, y_true, label_indices, label_names, label_indices_with_o, label_names_with_o,\n                                                                        dataset_type, stats_graph_folder, epoch_number,  parameters, evaluation_mode=evaluation_mode,\n                                                                        verbose=verbose)\n            if parameters[\'main_evaluation_mode\'] == evaluation_mode:\n                result_update[dataset_type].update(result_update[dataset_type][evaluation_mode])\n\n    result_update[\'time_elapsed_since_epoch_start\'] = time.time() - epoch_start_time\n    result_update[\'time_elapsed_since_train_start\'] = time.time() - results[\'execution_details\'][\'train_start\']\n    results[\'epoch\'][epoch_number].append(result_update)\n\n    # CoNLL evaluation script\n    for dataset_type in [\'train\', \'valid\', \'test\']:\n        if dataset_type not in output_filepaths.keys():\n            continue\n\n        # run perl evaluation script in python package\n        # conll_evaluation_script = os.path.join(\'.\', \'conlleval\')\n        package_name = \'neuroner\'\n        root_dir = os.path.dirname(pkg_resources.resource_filename(package_name, \n            \'__init__.py\'))\n        print(root_dir)\n        conll_evaluation_script = os.path.join(root_dir, \'conlleval\')\n\n        conll_output_filepath = \'{0}_conll_evaluation.txt\'.format(output_filepaths[dataset_type])\n        shell_command = \'perl {0} < {1} > {2}\'.format(conll_evaluation_script, output_filepaths[dataset_type], conll_output_filepath)\n        print(\'shell_command: {0}\'.format(shell_command))\n        os.system(shell_command)\n        conll_parsed_output = utils_nlp.get_parsed_conll_output(conll_output_filepath)\n        results[\'epoch\'][epoch_number][0][dataset_type][\'conll\'] = conll_parsed_output\n        results[\'epoch\'][epoch_number][0][dataset_type][\'f1_conll\'] = {}\n        results[\'epoch\'][epoch_number][0][dataset_type][\'f1_conll\'][\'micro\'] = results[\'epoch\'][epoch_number][0][dataset_type][\'conll\'][\'all\'][\'f1\']\n        if parameters[\'main_evaluation_mode\'] == \'conll\':\n            results[\'epoch\'][epoch_number][0][dataset_type][\'f1_score\'] = {}\n            results[\'epoch\'][epoch_number][0][dataset_type][\'f1_score\'][\'micro\'] = results[\'epoch\'][epoch_number][0][dataset_type][\'conll\'][\'all\'][\'f1\']\n            results[\'epoch\'][epoch_number][0][dataset_type][\'accuracy_score\'] = results[\'epoch\'][epoch_number][0][dataset_type][\'conll\'][\'all\'][\'accuracy\']\n            utils_plots.plot_classification_report(results[\'epoch\'][epoch_number][0][dataset_type][\'conll\'],\n                title=\'Classification report for epoch {0} in {1} ({2} evaluation)\\n\'.format(epoch_number, dataset_type, \'conll\'),\n                cmap=\'RdBu\', from_conll_json=True)\n            plt.savefig(os.path.join(stats_graph_folder, \'classification_report_for_epoch_{0:04d}_in_{1}_conll_evaluation.{3}\'.format(epoch_number, dataset_type,\n                                                                                                                                    evaluation_mode, parameters[\'plot_format\'])),\n                        dpi=300, format=parameters[\'plot_format\'], bbox_inches=\'tight\')\n            plt.close()\n\n    if  parameters[\'train_model\'] and \'train\' in output_filepaths.keys() and \'valid\' in output_filepaths.keys():\n        plot_f1_vs_epoch(results, stats_graph_folder, \'f1_score\', parameters)\n        plot_f1_vs_epoch(results, stats_graph_folder, \'accuracy_score\', parameters)\n        plot_f1_vs_epoch(results, stats_graph_folder, \'f1_conll\', parameters)\n\n    results[\'execution_details\'][\'train_duration\'] = time.time() - results[\'execution_details\'][\'train_start\']\n    save_results(results, stats_graph_folder)'"
neuroner/neuromodel.py,6,"b'import codecs\nimport configparser\nimport copy\nimport distutils.util\nimport glob\nimport os\nimport pickle\nfrom pprint import pprint\nimport random\nimport shutil\nimport sys\nimport time\nimport warnings\nimport pkg_resources\n\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\nfrom neuroner import train\nfrom neuroner import dataset\nfrom neuroner.entity_lstm import EntityLSTM\nfrom neuroner import utils\nfrom neuroner import conll_to_brat\nfrom neuroner import evaluate\nfrom neuroner import brat_to_conll\nfrom neuroner import utils_nlp\n\n# http://stackoverflow.com/questions/42217532/tensorflow-version-1-0-0-rc2-on-windows-opkernel-op-bestsplits-device-typ\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n# print(\'NeuroNER version: {0}\'.format(\'1.0.0\'))\n# print(\'TensorFlow version: {0}\'.format(tf.__version__))\nwarnings.filterwarnings(\'ignore\')\n\ndef fetch_model(name):\n    """"""\n    Fetch a pre-trained model and copy to a local ""trained_models"" folder\n     If name is provided, fetch from the package folder.\n\n    Args:\n        name (str): Name of a model folder.\n    """"""\n    # get content from package and write to local dir\n    # model comprises of:\n    # dataset.pickle\n    # model.ckpt.data-00000-of-00001\n    # model.ckpt.index\n    # model.ckpt.meta\n    # parameters.ini\n    _fetch(name, content_type=""trained_models"") \n\n\ndef fetch_data(name):\n    """"""\n    Fetch a dataset. If name is provided, fetch from the package folder. If url\n    is provided, fetch from a remote location.\n\n    Args:\n        name (str): Name of a dataset.\n        url (str): URL of a model folder.\n    """"""\n    # get content from package and write to local dir\n    _fetch(name, content_type=""data"")\n\n\ndef _fetch(name, content_type=None):\n    """"""\n    Load data or models from the package folder.\n\n    Args:\n        name (str): name of the resource\n        content_type (str): either ""data"" or ""trained_models""\n\n    Returns:\n        fileset (dict): dictionary containing the file content\n    """"""\n    package_name = \'neuroner\'\n    resource_path = \'/\'.join((content_type, name))\n\n    # get dirs\n    root_dir = os.path.dirname(pkg_resources.resource_filename(package_name,\n        \'__init__.py\'))\n    src_dir = os.path.join(root_dir, resource_path)\n    dest_dir = os.path.join(\'.\', content_type, name)\n\n    if pkg_resources.resource_isdir(package_name, resource_path):\n\n        # copy from package to dest dir\n        if os.path.isdir(dest_dir):\n            msg = ""Directory \'{}\' already exists."".format(dest_dir)\n            print(msg)\n        else:\n            shutil.copytree(src_dir, dest_dir)\n            msg = ""Directory created: \'{}\'."".format(dest_dir)\n            print(msg)\n    else:\n        msg = ""{} not found in {} package."".format(name,package_name)\n        print(msg)\n\n\ndef _get_default_param():\n    """"""\n    Get the default parameters.\n\n    """"""\n    param = {\'pretrained_model_folder\':\'./trained_models/conll_2003_en\',\n             \'dataset_text_folder\':\'./data/conll2003/en\',\n             \'character_embedding_dimension\':25,\n             \'character_lstm_hidden_state_dimension\':25,\n             \'check_for_digits_replaced_with_zeros\':True,\n             \'check_for_lowercase\':True,\n             \'debug\':False,\n             \'dropout_rate\':0.5,\n             \'experiment_name\':\'experiment\',\n             \'freeze_token_embeddings\':False,\n             \'gradient_clipping_value\':5.0,\n             \'learning_rate\':0.005,\n             \'load_only_pretrained_token_embeddings\':False,\n             \'load_all_pretrained_token_embeddings\':False,\n             \'main_evaluation_mode\':\'conll\',\n             \'maximum_number_of_epochs\':100,\n             \'number_of_cpu_threads\':8,\n             \'number_of_gpus\':0,\n             \'optimizer\':\'sgd\',\n             \'output_folder\':\'./output\',\n             \'output_scores\':False,\n             \'patience\':10,\n             \'parameters_filepath\': os.path.join(\'.\',\'parameters.ini\'),\n             \'plot_format\':\'pdf\',\n             \'reload_character_embeddings\':True,\n             \'reload_character_lstm\':True,\n             \'reload_crf\':True,\n             \'reload_feedforward\':True,\n             \'reload_token_embeddings\':True,\n             \'reload_token_lstm\':True,\n             \'remap_unknown_tokens_to_unk\':True,\n             \'spacylanguage\':\'en\',\n             \'tagging_format\':\'bioes\',\n             \'token_embedding_dimension\':100,\n             \'token_lstm_hidden_state_dimension\':100,\n             \'token_pretrained_embedding_filepath\':\'./data/word_vectors/glove.6B.100d.txt\',\n             \'tokenizer\':\'spacy\',\n             \'train_model\':True,\n             \'use_character_lstm\':True,\n             \'use_crf\':True,\n             \'use_pretrained_model\':False,\n             \'verbose\':False}\n\n    return param\n\n\ndef _get_config_param(param_filepath=None):\n    """"""\n    Get the parameters from the config file.\n    """"""\n    param = {}\n\n    # If a parameter file is specified, load it\n    if param_filepath:\n        param_file_txt = configparser.ConfigParser()\n        param_file_txt.read(param_filepath, encoding=""UTF-8"")\n        nested_parameters = utils.convert_configparser_to_dictionary(param_file_txt)\n\n        for k, v in nested_parameters.items():\n            param.update(v)\n\n    return param, param_file_txt\n\n\ndef _clean_param_dtypes(param):\n    """"""\n    Ensure data types are correct in the parameter dictionary.\n\n    Args:\n        param (dict): dictionary of parameter settings.\n    """"""\n\n    # Set the data type\n    for k, v in param.items():\n        v = str(v)\n        # If the value is a list delimited with a comma, choose one element at random.\n        # NOTE: review this behaviour.\n        if \',\' in v:\n            v = random.choice(v.split(\',\'))\n            param[k] = v\n\n        # Ensure that each parameter is cast to the correct type\n        if k in [\'character_embedding_dimension\',\n            \'character_lstm_hidden_state_dimension\', \'token_embedding_dimension\',\n            \'token_lstm_hidden_state_dimension\', \'patience\',\n            \'maximum_number_of_epochs\', \'maximum_training_time\',\n            \'number_of_cpu_threads\', \'number_of_gpus\']:\n            param[k] = int(v)\n        elif k in [\'dropout_rate\', \'learning_rate\', \'gradient_clipping_value\']:\n            param[k] = float(v)\n        elif k in [\'remap_unknown_tokens_to_unk\', \'use_character_lstm\',\n            \'use_crf\', \'train_model\', \'use_pretrained_model\', \'debug\', \'verbose\',\n            \'reload_character_embeddings\', \'reload_character_lstm\',\n            \'reload_token_embeddings\', \'reload_token_lstm\',\n            \'reload_feedforward\', \'reload_crf\', \'check_for_lowercase\',\n            \'check_for_digits_replaced_with_zeros\', \'output_scores\',\n            \'freeze_token_embeddings\', \'load_only_pretrained_token_embeddings\',\n            \'load_all_pretrained_token_embeddings\']:\n            param[k] = distutils.util.strtobool(v)\n\n    return param\n\n\ndef load_parameters(**kwargs):\n    \'\'\'\n    Load parameters from the ini file if specified, take into account any\n    command line argument, and ensure that each parameter is cast to the\n    correct type.\n\n    Command line arguments take precedence over parameters specified in the\n    parameter file.\n    \'\'\'\n    param = {}\n    param_default = _get_default_param()\n\n    # use parameter path if provided, otherwise use default\n    try:\n        if kwargs[\'parameters_filepath\']:\n            parameters_filepath = kwargs[\'parameters_filepath\']\n    except:\n        parameters_filepath = param_default[\'parameters_filepath\']\n\n    param_config, param_file_txt = _get_config_param(parameters_filepath)\n\n    # Parameter file settings should overwrite default settings\n    for k, v in param_config.items():\n        param[k] = v\n\n    # Command line args should overwrite settings in the parameter file\n    for k, v in kwargs.items():\n        param[k] = v\n\n    # Any missing args can be set to default\n    for k, v in param_default.items():\n        if k not in param:\n            param[k] = param_default[k]\n\n    # clean the data types\n    param = _clean_param_dtypes(param)\n\n    # if loading a pretrained model, set to pretrain hyperparameters\n    if param[\'use_pretrained_model\']:\n\n        pretrain_path = os.path.join(param[\'pretrained_model_folder\'],\n            \'parameters.ini\')\n\n        if os.path.isfile(pretrain_path):\n            pretrain_param, _ = _get_config_param(pretrain_path)\n            pretrain_param = _clean_param_dtypes(pretrain_param)\n\n            pretrain_list = [\'use_character_lstm\', \'character_embedding_dimension\',\n                \'character_lstm_hidden_state_dimension\', \'token_embedding_dimension\',\n                \'token_lstm_hidden_state_dimension\', \'use_crf\']\n\n            for name in pretrain_list:\n                if param[name] != pretrain_param[name]:\n                    msg = """"""WARNING: parameter \'{0}\' was overwritten from \'{1}\' to \'{2}\'\n                        for consistency with the pretrained model"""""".format(name,\n                            param[name], pretrain_param[name])\n                    print(msg)\n                    param[name] = pretrain_param[name]\n        else:\n            msg = """"""Warning: pretraining parameter file not found.""""""\n            print(msg)\n\n    # update param_file_txt to reflect the overriding\n    param_to_section = utils.get_parameter_to_section_of_configparser(param_file_txt)\n    for k, v in param.items():\n        try:\n            param_file_txt.set(param_to_section[k], k, str(v))\n        except:\n            pass\n\n    pprint(param)\n\n    return param, param_file_txt\n\n\ndef get_valid_dataset_filepaths(parameters):\n    """"""\n    Get valid filepaths for the datasets.\n    """"""\n    dataset_filepaths = {}\n    dataset_brat_folders = {}\n\n    for dataset_type in [\'train\', \'valid\', \'test\', \'deploy\']:\n        dataset_filepaths[dataset_type] = os.path.join(parameters[\'dataset_text_folder\'], \n            \'{0}.txt\'.format(dataset_type))\n        dataset_brat_folders[dataset_type] = os.path.join(parameters[\'dataset_text_folder\'], \n            dataset_type)\n        dataset_compatible_with_brat_filepath = os.path.join(parameters[\'dataset_text_folder\'], \n            \'{0}_compatible_with_brat.txt\'.format(dataset_type))\n\n        # Conll file exists\n        if os.path.isfile(dataset_filepaths[dataset_type]) \\\n        and os.path.getsize(dataset_filepaths[dataset_type]) > 0:\n\n            # Brat text files exist\n            if os.path.exists(dataset_brat_folders[dataset_type]) \\\n            and len(glob.glob(os.path.join(dataset_brat_folders[dataset_type], \'*.txt\'))) > 0:\n\n                # Check compatibility between conll and brat files\n                brat_to_conll.check_brat_annotation_and_text_compatibility(dataset_brat_folders[dataset_type])\n                if os.path.exists(dataset_compatible_with_brat_filepath):\n                    dataset_filepaths[dataset_type] = dataset_compatible_with_brat_filepath\n\n                conll_to_brat.check_compatibility_between_conll_and_brat_text(dataset_filepaths[dataset_type], \n                    dataset_brat_folders[dataset_type])\n\n            # Brat text files do not exist\n            else:\n\n                # Populate brat text and annotation files based on conll file\n                conll_to_brat.conll_to_brat(dataset_filepaths[dataset_type], dataset_compatible_with_brat_filepath, \n                    dataset_brat_folders[dataset_type], dataset_brat_folders[dataset_type])\n                dataset_filepaths[dataset_type] = dataset_compatible_with_brat_filepath\n\n        # Conll file does not exist\n        else:\n            # Brat text files exist\n            if os.path.exists(dataset_brat_folders[dataset_type]) \\\n            and len(glob.glob(os.path.join(dataset_brat_folders[dataset_type], \'*.txt\'))) > 0:\n                dataset_filepath_for_tokenizer = os.path.join(parameters[\'dataset_text_folder\'], \n                    \'{0}_{1}.txt\'.format(dataset_type, parameters[\'tokenizer\']))\n                if os.path.exists(dataset_filepath_for_tokenizer):\n                    conll_to_brat.check_compatibility_between_conll_and_brat_text(dataset_filepath_for_tokenizer, \n                        dataset_brat_folders[dataset_type])\n                else:\n                    # Populate conll file based on brat files\n                    brat_to_conll.brat_to_conll(dataset_brat_folders[dataset_type], \n                        dataset_filepath_for_tokenizer, parameters[\'tokenizer\'], \n                        parameters[\'spacylanguage\'])\n                dataset_filepaths[dataset_type] = dataset_filepath_for_tokenizer\n\n            # Brat text files do not exist\n            else:\n                del dataset_filepaths[dataset_type]\n                del dataset_brat_folders[dataset_type]\n                continue\n\n        if parameters[\'tagging_format\'] == \'bioes\':\n            # Generate conll file with BIOES format\n            bioes_filepath = os.path.join(parameters[\'dataset_text_folder\'], \n                \'{0}_bioes.txt\'.format(utils.get_basename_without_extension(dataset_filepaths[dataset_type])))\n            utils_nlp.convert_conll_from_bio_to_bioes(dataset_filepaths[dataset_type], \n                bioes_filepath)\n            dataset_filepaths[dataset_type] = bioes_filepath\n\n    return dataset_filepaths, dataset_brat_folders\n\n\ndef check_param_compatibility(parameters, dataset_filepaths):\n    """"""\n    Check parameters are compatible.\n    """"""\n    # Check mode of operation\n    if parameters[\'train_model\']:\n        if \'train\' not in dataset_filepaths or \'valid\' not in dataset_filepaths:\n            msg = """"""If train_model is set to True, both train and valid set must exist \n                in the specified dataset folder: {0}"""""".format(parameters[\'dataset_text_folder\'])\n            raise IOError(msg)\n    elif parameters[\'use_pretrained_model\']:\n        if \'train\' in dataset_filepaths and \'valid\' in dataset_filepaths:\n            msg = """"""WARNING: train and valid set exist in the specified dataset folder, \n                but train_model is set to FALSE: {0}"""""".format(parameters[\'dataset_text_folder\'])\n            print(msg)\n        if \'test\' not in dataset_filepaths and \'deploy\' not in dataset_filepaths:\n            msg = """"""For prediction mode, either test set and deploy set must exist \n                in the specified dataset folder: {0}"""""".format(parameters[\'dataset_text_folder\'])\n            raise IOError(msg)\n    # if not parameters[\'train_model\'] and not parameters[\'use_pretrained_model\']:\n    else:\n        raise ValueError(""At least one of train_model and use_pretrained_model must be set to True."")\n\n    if parameters[\'use_pretrained_model\']:\n        if all([not parameters[s] for s in [\'reload_character_embeddings\', \'reload_character_lstm\', \n            \'reload_token_embeddings\', \'reload_token_lstm\', \'reload_feedforward\', \'reload_crf\']]):\n            msg = """"""If use_pretrained_model is set to True, at least one of reload_character_embeddings, \n                reload_character_lstm, reload_token_embeddings, reload_token_lstm, reload_feedforward, \n                reload_crf must be set to True.""""""\n            raise ValueError(msg)\n\n    if parameters[\'gradient_clipping_value\'] < 0:\n        parameters[\'gradient_clipping_value\'] = abs(parameters[\'gradient_clipping_value\'])\n\n    try:\n        if parameters[\'output_scores\'] and parameters[\'use_crf\']:\n            warn_msg = """"""Warning when use_crf is True, scores are decoded\n            using the crf. As a result, the scores cannot be directly interpreted\n            in terms of class prediction.\n            """"""\n            warnings.warn(warn_msg)\n    except KeyError:\n        parameters[\'output_scores\'] = False\n\n\n\nclass NeuroNER(object):\n    """"""\n    NeuroNER model.\n\n    Args:\n        param_filepath (type): description\n        pretrained_model_folder (type): description\n        dataset_text_folder (type): description\n        character_embedding_dimension (type): description\n        character_lstm_hidden_state_dimension (type): description\n        check_for_digits_replaced_with_zeros (type): description\n        check_for_lowercase (type): description\n        debug (type): description\n        dropout_rate (type): description\n        experiment_name (type): description\n        freeze_token_embeddings (type): description\n        gradient_clipping_value (type): description\n        learning_rate (type): description\n        load_only_pretrained_token_embeddings (type): description\n        load_all_pretrained_token_embeddings (type): description\n        main_evaluation_mode (type): description\n        maximum_number_of_epochs (type): description\n        number_of_cpu_threads (type): description\n        number_of_gpus (type): description\n        optimizer (type): description\n        output_folder (type): description\n        output_scores (bool): description\n        patience (type): description\n        plot_format (type): description\n        reload_character_embeddings (type): description\n        reload_character_lstm (type): description\n        reload_crf (type): description\n        reload_feedforward (type): description\n        reload_token_embeddings (type): description\n        reload_token_lstm (type): description\n        remap_unknown_tokens_to_unk (type): description\n        spacylanguage (type): description\n        tagging_format (type): description\n        token_embedding_dimension (type): description\n        token_lstm_hidden_state_dimension (type): description\n        token_pretrained_embedding_filepath (type): description\n        tokenizer (type): description\n        train_model (type): description\n        use_character_lstm (type): description\n        use_crf (type): description\n        use_pretrained_model (type): description\n        verbose (type): description\n    """"""\n\n    prediction_count = 0\n\n    def __init__(self, **kwargs):\n\n        # Set parameters\n        self.parameters, self.conf_parameters = load_parameters(**kwargs)\n\n        self.dataset_filepaths, self.dataset_brat_folders = self._get_valid_dataset_filepaths(self.parameters)\n        self._check_param_compatibility(self.parameters, self.dataset_filepaths)\n\n        # Load dataset\n        self.modeldata = dataset.Dataset(verbose=self.parameters[\'verbose\'], debug=self.parameters[\'debug\'])\n        token_to_vector = self.modeldata.load_dataset(self.dataset_filepaths, self.parameters)\n\n        # Launch session. Automatically choose a device\n        # if the specified one doesn\'t exist\n        session_conf = tf.ConfigProto(\n            intra_op_parallelism_threads=self.parameters[\'number_of_cpu_threads\'],\n            inter_op_parallelism_threads=self.parameters[\'number_of_cpu_threads\'],\n            device_count={\'CPU\': 1, \'GPU\': self.parameters[\'number_of_gpus\']},\n            allow_soft_placement=True,\n            log_device_placement=False)\n\n        self.sess = tf.Session(config=session_conf)\n        with self.sess.as_default():\n\n            # Initialize or load pretrained model\n            self.model = EntityLSTM(self.modeldata, self.parameters)\n            self.sess.run(tf.global_variables_initializer())\n\n            if self.parameters[\'use_pretrained_model\']:\n                self.transition_params_trained = self.model.restore_from_pretrained_model(self.parameters,\n                    self.modeldata, self.sess, token_to_vector=token_to_vector)\n            else:\n                self.model.load_pretrained_token_embeddings(self.sess, self.modeldata,\n                    self.parameters, token_to_vector)\n                self.transition_params_trained = np.random.rand(len(self.modeldata.unique_labels)+2,\n                    len(self.modeldata.unique_labels)+2)\n\n    def _create_stats_graph_folder(self, parameters):\n        """"""\n        Initialize stats_graph_folder.\n\n        Args:\n            parameters (type): description.\n        """"""\n        experiment_timestamp = utils.get_current_time_in_miliseconds()\n        dataset_name = utils.get_basename_without_extension(parameters[\'dataset_text_folder\'])\n        model_name = \'{0}_{1}\'.format(dataset_name, experiment_timestamp)\n        utils.create_folder_if_not_exists(parameters[\'output_folder\'])\n\n        # Folder where to save graphs\n        stats_graph_folder = os.path.join(parameters[\'output_folder\'], model_name) \n        utils.create_folder_if_not_exists(stats_graph_folder)\n        return stats_graph_folder, experiment_timestamp\n\n    def _get_valid_dataset_filepaths(self, parameters, dataset_types=[\'train\', \'valid\', \'test\', \'deploy\']):\n        """"""\n        Get paths for the datasets.\n\n        Args:\n            parameters (type): description.\n            dataset_types (type): description.\n        """"""\n        dataset_filepaths = {}\n        dataset_brat_folders = {}\n\n        for dataset_type in dataset_types:\n            dataset_filepaths[dataset_type] = os.path.join(parameters[\'dataset_text_folder\'], \n                \'{0}.txt\'.format(dataset_type))\n            dataset_brat_folders[dataset_type] = os.path.join(parameters[\'dataset_text_folder\'], \n                dataset_type)\n            dataset_compatible_with_brat_filepath = os.path.join(parameters[\'dataset_text_folder\'], \n                \'{0}_compatible_with_brat.txt\'.format(dataset_type))\n\n            # Conll file exists\n            if os.path.isfile(dataset_filepaths[dataset_type]) \\\n            and os.path.getsize(dataset_filepaths[dataset_type]) > 0:\n                # Brat text files exist\n                if os.path.exists(dataset_brat_folders[dataset_type]) and \\\n                len(glob.glob(os.path.join(dataset_brat_folders[dataset_type], \'*.txt\'))) > 0:\n\n                    # Check compatibility between conll and brat files\n                    brat_to_conll.check_brat_annotation_and_text_compatibility(dataset_brat_folders[dataset_type])\n                    if os.path.exists(dataset_compatible_with_brat_filepath):\n                        dataset_filepaths[dataset_type] = dataset_compatible_with_brat_filepath\n                    conll_to_brat.check_compatibility_between_conll_and_brat_text(dataset_filepaths[dataset_type], \n                        dataset_brat_folders[dataset_type])\n\n                # Brat text files do not exist\n                else:\n                    # Populate brat text and annotation files based on conll file\n                    conll_to_brat.conll_to_brat(dataset_filepaths[dataset_type], \n                        dataset_compatible_with_brat_filepath, dataset_brat_folders[dataset_type], \n                        dataset_brat_folders[dataset_type])\n                    dataset_filepaths[dataset_type] = dataset_compatible_with_brat_filepath\n\n            # Conll file does not exist\n            else:\n                # Brat text files exist\n                if os.path.exists(dataset_brat_folders[dataset_type]) \\\n                and len(glob.glob(os.path.join(dataset_brat_folders[dataset_type], \'*.txt\'))) > 0:\n                    dataset_filepath_for_tokenizer = os.path.join(parameters[\'dataset_text_folder\'], \n                        \'{0}_{1}.txt\'.format(dataset_type, parameters[\'tokenizer\']))\n                    if os.path.exists(dataset_filepath_for_tokenizer):\n                        conll_to_brat.check_compatibility_between_conll_and_brat_text(dataset_filepath_for_tokenizer, \n                            dataset_brat_folders[dataset_type])\n                    else:\n                        # Populate conll file based on brat files\n                        brat_to_conll.brat_to_conll(dataset_brat_folders[dataset_type], \n                            dataset_filepath_for_tokenizer, parameters[\'tokenizer\'], parameters[\'spacylanguage\'])\n                    dataset_filepaths[dataset_type] = dataset_filepath_for_tokenizer\n\n                # Brat text files do not exist\n                else:\n                    del dataset_filepaths[dataset_type]\n                    del dataset_brat_folders[dataset_type]\n                    continue\n\n            if parameters[\'tagging_format\'] == \'bioes\':\n                # Generate conll file with BIOES format\n                bioes_filepath = os.path.join(parameters[\'dataset_text_folder\'],\n                    \'{0}_bioes.txt\'.format(utils.get_basename_without_extension(dataset_filepaths[dataset_type])))\n                utils_nlp.convert_conll_from_bio_to_bioes(dataset_filepaths[dataset_type],\n                    bioes_filepath)\n                dataset_filepaths[dataset_type] = bioes_filepath\n\n        return dataset_filepaths, dataset_brat_folders\n\n    def _check_param_compatibility(self, parameters, dataset_filepaths):\n        """"""\n        Check parameters are compatible.\n\n        Args:\n            parameters (type): description.\n            dataset_filepaths (type): description.\n        """"""\n        check_param_compatibility(parameters, dataset_filepaths)\n\n    def fit(self):\n        """"""\n        Fit the model.\n        """"""\n        parameters = self.parameters\n        conf_parameters = self.conf_parameters\n        dataset_filepaths = self.dataset_filepaths\n        modeldata = self.modeldata\n        dataset_brat_folders = self.dataset_brat_folders\n        sess = self.sess\n        model = self.model\n        transition_params_trained = self.transition_params_trained\n        stats_graph_folder, experiment_timestamp = self._create_stats_graph_folder(parameters)\n\n        # Initialize and save execution details\n        start_time = time.time()\n        results = {}\n        results[\'epoch\'] = {}\n        results[\'execution_details\'] = {}\n        results[\'execution_details\'][\'train_start\'] = start_time\n        results[\'execution_details\'][\'time_stamp\'] = experiment_timestamp\n        results[\'execution_details\'][\'early_stop\'] = False\n        results[\'execution_details\'][\'keyboard_interrupt\'] = False\n        results[\'execution_details\'][\'num_epochs\'] = 0\n        results[\'model_options\'] = copy.copy(parameters)\n\n        model_folder = os.path.join(stats_graph_folder, \'model\')\n        utils.create_folder_if_not_exists(model_folder)\n        with open(os.path.join(model_folder, \'parameters.ini\'), \'w\') as parameters_file:\n            conf_parameters.write(parameters_file)\n        pickle.dump(modeldata, open(os.path.join(model_folder, \'dataset.pickle\'), \'wb\'))\n\n        tensorboard_log_folder = os.path.join(stats_graph_folder, \'tensorboard_logs\')\n        utils.create_folder_if_not_exists(tensorboard_log_folder)\n        tensorboard_log_folders = {}\n        for dataset_type in dataset_filepaths.keys():\n            tensorboard_log_folders[dataset_type] = os.path.join(stats_graph_folder, \n                \'tensorboard_logs\', dataset_type)\n            utils.create_folder_if_not_exists(tensorboard_log_folders[dataset_type])\n\n        # Instantiate the writers for TensorBoard\n        writers = {}\n        for dataset_type in dataset_filepaths.keys():\n            writers[dataset_type] = tf.summary.FileWriter(tensorboard_log_folders[dataset_type], \n                graph=sess.graph)\n\n        # embedding_writer has to write in model_folder, otherwise TensorBoard won\'t be able to view embeddings\n        embedding_writer = tf.summary.FileWriter(model_folder)\n\n        embeddings_projector_config = projector.ProjectorConfig()\n        tensorboard_token_embeddings = embeddings_projector_config.embeddings.add()\n        tensorboard_token_embeddings.tensor_name = model.token_embedding_weights.name\n        token_list_file_path = os.path.join(model_folder, \'tensorboard_metadata_tokens.tsv\')\n        tensorboard_token_embeddings.metadata_path = os.path.relpath(token_list_file_path, \'.\')\n\n        tensorboard_character_embeddings = embeddings_projector_config.embeddings.add()\n        tensorboard_character_embeddings.tensor_name = model.character_embedding_weights.name\n        character_list_file_path = os.path.join(model_folder, \'tensorboard_metadata_characters.tsv\')\n        tensorboard_character_embeddings.metadata_path = os.path.relpath(character_list_file_path, \'.\')\n\n        projector.visualize_embeddings(embedding_writer, embeddings_projector_config)\n\n        # Write metadata for TensorBoard embeddings\n        token_list_file = codecs.open(token_list_file_path,\'w\', \'UTF-8\')\n        for token_index in range(modeldata.vocabulary_size):\n            token_list_file.write(\'{0}\\n\'.format(modeldata.index_to_token[token_index]))\n        token_list_file.close()\n\n        character_list_file = codecs.open(character_list_file_path,\'w\', \'UTF-8\')\n        for character_index in range(modeldata.alphabet_size):\n            if character_index == modeldata.PADDING_CHARACTER_INDEX:\n                character_list_file.write(\'PADDING\\n\')\n            else:\n                character_list_file.write(\'{0}\\n\'.format(modeldata.index_to_character[character_index]))\n        character_list_file.close()\n\n\n        # Start training + evaluation loop. Each iteration corresponds to 1 epoch.\n        # number of epochs with no improvement on the validation test in terms of F1-score\n        bad_counter = 0\n        previous_best_valid_f1_score = 0\n        epoch_number = -1\n        try:\n            while True:\n                step = 0\n                epoch_number += 1\n                print(\'\\nStarting epoch {0}\'.format(epoch_number))\n\n                epoch_start_time = time.time()\n\n                if epoch_number != 0:\n                    # Train model: loop over all sequences of training set with shuffling\n                    sequence_numbers=list(range(len(modeldata.token_indices[\'train\'])))\n                    random.shuffle(sequence_numbers)\n                    for sequence_number in sequence_numbers:\n                        transition_params_trained = train.train_step(sess, modeldata, \n                            sequence_number, model, parameters)\n                        step += 1\n                        if step % 10 == 0:\n                            print(\'Training {0:.2f}% done\'.format(step/len(sequence_numbers)*100),\n                                end=\'\\r\', flush=True)\n\n                epoch_elapsed_training_time = time.time() - epoch_start_time\n                print(\'Training completed in {0:.2f} seconds\'.format(epoch_elapsed_training_time),\n                    flush=True)\n\n                y_pred, y_true, output_filepaths = train.predict_labels(sess, model,\n                    transition_params_trained, parameters, modeldata, epoch_number,\n                    stats_graph_folder, dataset_filepaths)\n\n                # Evaluate model: save and plot results\n                evaluate.evaluate_model(results, modeldata, y_pred, y_true, stats_graph_folder,\n                    epoch_number, epoch_start_time, output_filepaths, parameters)\n\n                if parameters[\'use_pretrained_model\'] and not parameters[\'train_model\']:\n                    conll_to_brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder)\n                    break\n\n                # Save model\n                model.saver.save(sess, os.path.join(model_folder, \'model_{0:05d}.ckpt\'.format(epoch_number)))\n\n                # Save TensorBoard logs\n                summary = sess.run(model.summary_op, feed_dict=None)\n                writers[\'train\'].add_summary(summary, epoch_number)\n                writers[\'train\'].flush()\n                utils.copytree(writers[\'train\'].get_logdir(), model_folder)\n\n\n                # Early stop\n                valid_f1_score = results[\'epoch\'][epoch_number][0][\'valid\'][\'f1_score\'][\'micro\']\n                if  valid_f1_score > previous_best_valid_f1_score:\n                    bad_counter = 0\n                    previous_best_valid_f1_score = valid_f1_score\n                    conll_to_brat.output_brat(output_filepaths, dataset_brat_folders,\n                        stats_graph_folder, overwrite=True)\n                    self.transition_params_trained = transition_params_trained\n                else:\n                    bad_counter += 1\n                print(""The last {0} epochs have not shown improvements on the validation set."".format(bad_counter))\n\n                if bad_counter >= parameters[\'patience\']:\n                    print(\'Early Stop!\')\n                    results[\'execution_details\'][\'early_stop\'] = True\n                    break\n\n                if epoch_number >= parameters[\'maximum_number_of_epochs\']:\n                    break\n\n        except KeyboardInterrupt:\n            results[\'execution_details\'][\'keyboard_interrupt\'] = True\n            print(\'Training interrupted\')\n\n        print(\'Finishing the experiment\')\n        end_time = time.time()\n        results[\'execution_details\'][\'train_duration\'] = end_time - start_time\n        results[\'execution_details\'][\'train_end\'] = end_time\n        evaluate.save_results(results, stats_graph_folder)\n        for dataset_type in dataset_filepaths.keys():\n            writers[dataset_type].close()\n\n    def predict(self, text):\n        """"""\n        Predict\n\n        Args:\n            text (str): Description.\n        """"""\n        self.prediction_count += 1        \n\n        if self.prediction_count == 1:\n            self.parameters[\'dataset_text_folder\'] = os.path.join(\'.\', \'data\', \'temp\')\n            self.stats_graph_folder, _ = self._create_stats_graph_folder(self.parameters)\n\n        # Update the deploy folder, file, and modeldata \n        dataset_type = \'deploy\'\n\n        # Delete all deployment data    \n        for filepath in glob.glob(os.path.join(self.parameters[\'dataset_text_folder\'], \n            \'{0}*\'.format(dataset_type))):\n            if os.path.isdir(filepath): \n                shutil.rmtree(filepath)\n            else:\n                os.remove(filepath)\n\n        # Create brat folder and file\n        dataset_brat_deploy_folder = os.path.join(self.parameters[\'dataset_text_folder\'], \n            dataset_type)\n        utils.create_folder_if_not_exists(dataset_brat_deploy_folder)\n        dataset_brat_deploy_filepath = os.path.join(dataset_brat_deploy_folder, \n            \'temp_{0}.txt\'.format(str(self.prediction_count).zfill(5)))\n            #self._get_dataset_brat_deploy_filepath(dataset_brat_deploy_folder) \n        with codecs.open(dataset_brat_deploy_filepath, \'w\', \'UTF-8\') as f:\n            f.write(text)\n\n        # Update deploy filepaths\n        dataset_filepaths, dataset_brat_folders = self._get_valid_dataset_filepaths(self.parameters,\n            dataset_types=[dataset_type])\n        self.dataset_filepaths.update(dataset_filepaths)\n        self.dataset_brat_folders.update(dataset_brat_folders)    \n\n        # Update the dataset for the new deploy set\n        self.modeldata.update_dataset(self.dataset_filepaths, [dataset_type])\n\n        # Predict labels and output brat\n        output_filepaths = {}\n        prediction_output = train.prediction_step(self.sess, self.modeldata,\n            dataset_type, self.model, self.transition_params_trained,\n            self.stats_graph_folder, self.prediction_count, self.parameters,\n            self.dataset_filepaths)\n\n        _, _, output_filepaths[dataset_type] = prediction_output\n        conll_to_brat.output_brat(output_filepaths, self.dataset_brat_folders, \n            self.stats_graph_folder, overwrite=True)\n\n        # Print and output result\n        text_filepath = os.path.join(self.stats_graph_folder, \'brat\', \'deploy\', \n            os.path.basename(dataset_brat_deploy_filepath))\n        annotation_filepath = os.path.join(self.stats_graph_folder, \'brat\', \n            \'deploy\', \'{0}.ann\'.format(utils.get_basename_without_extension(dataset_brat_deploy_filepath)))\n        text2, entities = brat_to_conll.get_entities_from_brat(text_filepath, \n            annotation_filepath, verbose=True)\n        assert(text == text2)\n        return entities\n\n    def get_params(self):\n        return self.parameters\n\n    def close(self):\n        self.__del__()\n\n    def __del__(self):\n        self.sess.close()\n'"
neuroner/prepare_pretrained_model.py,5,"b'\'\'\'\nThis script prepares a pretrained model to be shared without exposing the data used for training.\n\'\'\'\nimport glob\nimport os\nimport pickle\nfrom pprint import pprint\nimport shutil\nimport utils\n\nfrom entity_lstm import EntityLSTM\nimport tensorflow as tf\nfrom tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n\nfrom neuroner import utils_tf\nfrom neuroner import neuromodel\n\ndef trim_dataset_pickle(input_dataset_filepath, output_dataset_filepath=None, delete_token_mappings=False):\n    \'\'\'\n    Remove the dataset and labels from dataset.pickle. \n    If delete_token_mappings = True, then also remove token_to_index and index_to_token except for UNK.\n    \'\'\'\n    print(""Trimming dataset.pickle.."")\n    if output_dataset_filepath == None:\n        output_dataset_filepath = os.path.join(os.path.dirname(input_dataset_filepath), \n            \'dataset_trimmed.pickle\')\n    dataset = pickle.load(open(input_dataset_filepath, \'rb\'))\n    count = 0\n    print(""Keys removed:"")\n    keys_to_remove = [\'character_indices\', \'character_indices_padded\', \'characters\', \n        \'label_indices\', \'label_vector_indices\', \'labels\', \'token_indices\', \n        \'token_lengths\', \'tokens\', \'infrequent_token_indices\', \'tokens_mapped_to_unk\']\n    for key in keys_to_remove:\n        if key in dataset.__dict__:\n            del dataset.__dict__[key]\n            print(\'\\t\' + key)\n            count += 1            \n    if delete_token_mappings:\n        dataset.__dict__[\'token_to_index\'] = {dataset.__dict__[\'UNK\']:dataset.__dict__[\'UNK_TOKEN_INDEX\']}\n        dataset.__dict__[\'index_to_token\'] = {dataset.__dict__[\'UNK_TOKEN_INDEX\']:dataset.__dict__[\'UNK\']}\n    print(""Number of keys removed: {0}"".format(count))\n    pprint(dataset.__dict__)\n    pickle.dump(dataset, open(output_dataset_filepath, \'wb\'))\n    print(""Done!"")\n\n\ndef trim_model_checkpoint(parameters_filepath, dataset_filepath, input_checkpoint_filepath, \n    output_checkpoint_filepath):\n    \'\'\'\n    Remove all token embeddings except UNK.\n    \'\'\'\n    parameters, _ = neuromodel.load_parameters(parameters_filepath=parameters_filepath)\n    dataset = pickle.load(open(dataset_filepath, \'rb\'))\n    model = EntityLSTM(dataset, parameters) \n    with tf.Session() as sess:\n        model_saver = tf.train.Saver()  # defaults to saving all variables\n        \n        # Restore the pretrained model\n        model_saver.restore(sess, input_checkpoint_filepath) # Works only when the dimensions of tensor variables are matched.\n        \n        # Get pretrained embeddings\n        token_embedding_weights = sess.run(model.token_embedding_weights) \n    \n        # Restore the sizes of token embedding weights\n        utils_tf.resize_tensor_variable(sess, model.token_embedding_weights, \n            [1, parameters[\'token_embedding_dimension\']]) \n            \n        initial_weights = sess.run(model.token_embedding_weights)\n        initial_weights[dataset.UNK_TOKEN_INDEX] = token_embedding_weights[dataset.UNK_TOKEN_INDEX]\n        sess.run(tf.assign(model.token_embedding_weights, initial_weights, validate_shape=False))\n    \n        token_embedding_weights = sess.run(model.token_embedding_weights) \n        print(""token_embedding_weights: {0}"".format(token_embedding_weights))\n        \n        model_saver.save(sess, output_checkpoint_filepath)\n            \n    dataset.__dict__[\'vocabulary_size\'] = 1\n    pickle.dump(dataset, open(dataset_filepath, \'wb\'))\n    pprint(dataset.__dict__)\n\n\ndef prepare_pretrained_model_for_restoring(output_folder_name, epoch_number, \n    model_name, delete_token_mappings=False):\n    \'\'\'\n    Copy the dataset.pickle, parameters.ini, and model checkpoint files after \n    removing the data used for training.\n    \n    The dataset and labels are deleted from dataset.pickle by default. The only \n    information about the dataset that remain in the pretrained model\n    is the list of tokens that appears in the dataset and the corresponding token \n    embeddings learned from the dataset.\n    \n    If delete_token_mappings is set to True, index_to_token and token_to_index \n    mappings are deleted from dataset.pickle additionally,\n    and the corresponding token embeddings are deleted from the model checkpoint \n    files. In this case, the pretrained model would not contain\n    any information about the dataset used for training the model. \n    \n    If you wish to share a pretrained model with delete_token_mappings = True, \n    it is highly recommended to use some external pre-trained token \n    embeddings and freeze them while training the model to obtain high performance. \n    This can be done by specifying the token_pretrained_embedding_filepath \n    and setting freeze_token_embeddings = True in parameters.ini for training.\n    \'\'\'\n    input_model_folder = os.path.join(\'.\', \'output\', output_folder_name, \'model\')\n    output_model_folder = os.path.join(\'.\', \'trained_models\', model_name)\n    utils.create_folder_if_not_exists(output_model_folder)\n\n    # trim and copy dataset.pickle\n    input_dataset_filepath = os.path.join(input_model_folder, \'dataset.pickle\')\n    output_dataset_filepath = os.path.join(output_model_folder, \'dataset.pickle\')\n    trim_dataset_pickle(input_dataset_filepath, output_dataset_filepath, \n        delete_token_mappings=delete_token_mappings)\n    \n    # copy parameters.ini\n    parameters_filepath = os.path.join(input_model_folder, \'parameters.ini\')\n    shutil.copy(parameters_filepath, output_model_folder)\n    \n    # (trim and) copy checkpoint files\n    epoch_number_string = str(epoch_number).zfill(5)\n    if delete_token_mappings:\n        input_checkpoint_filepath = os.path.join(input_model_folder, \n            \'model_{0}.ckpt\'.format(epoch_number_string))\n        output_checkpoint_filepath = os.path.join(output_model_folder, \'model.ckpt\')\n        trim_model_checkpoint(parameters_filepath, output_dataset_filepath, \n            input_checkpoint_filepath, output_checkpoint_filepath)\n    else:\n        for filepath in glob.glob(os.path.join(input_model_folder, \n            \'model_{0}.ckpt*\'.format(epoch_number_string))):\n            shutil.copyfile(filepath, os.path.join(output_model_folder, \n                os.path.basename(filepath).replace(\'_\' + epoch_number_string, \'\')))\n\n \ndef check_contents_of_dataset_and_model_checkpoint(model_folder):\n    \'\'\'\n    Check the contents of dataset.pickle and model_xxx.ckpt.\n    model_folder: folder containing dataset.pickle and model_xxx.ckpt to be checked. \n    \'\'\'\n    dataset_filepath = os.path.join(model_folder, \'dataset.pickle\')\n    dataset = pickle.load(open(dataset_filepath, \'rb\'))\n    pprint(dataset.__dict__)\n    pprint(list(dataset.__dict__.keys()))\n\n    checkpoint_filepath = os.path.join(model_folder, \'model.ckpt\')\n    with tf.Session() as sess:\n        print_tensors_in_checkpoint_file(checkpoint_filepath, \n            tensor_name=\'token_embedding/token_embedding_weights\', all_tensors=True)\n        print_tensors_in_checkpoint_file(checkpoint_filepath, \n            tensor_name=\'token_embedding/token_embedding_weights\', all_tensors=False)\n\n\nif __name__ == \'__main__\':\n    output_folder_name = \'en_2017-05-05_08-58-32-633799\'\n    epoch_number = 30\n    model_name = \'conll_2003_en\'\n    delete_token_mappings = False\n    prepare_pretrained_model_for_restoring(output_folder_name, epoch_number, \n        model_name, delete_token_mappings)\n    \n#     model_name = \'mimic_glove_spacy_iobes\'\n#     model_folder = os.path.join(\'..\', \'trained_models\', model_name)\n#     check_contents_of_dataset_and_model_checkpoint(model_folder)\n'"
neuroner/train.py,1,"b'import codecs\nimport os\nimport pkg_resources\nimport pickle\nimport warnings\n\nimport numpy as np\nimport sklearn.metrics\nimport tensorflow as tf\n# from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n\nfrom neuroner.evaluate import remap_labels\nfrom neuroner import utils_tf\nfrom neuroner import utils_nlp\n\ndef train_step(sess, dataset, sequence_number, model, parameters):\n    """"""\n    Train.\n    """"""\n    # Perform one iteration\n    token_indices_sequence = dataset.token_indices[\'train\'][sequence_number]\n\n    for i, token_index in enumerate(token_indices_sequence):\n        if token_index in dataset.infrequent_token_indices and np.random.uniform() < 0.5:\n            token_indices_sequence[i] = dataset.UNK_TOKEN_INDEX\n\n    feed_dict = {\n      model.input_token_indices: token_indices_sequence,\n      model.input_label_indices_vector: dataset.label_vector_indices[\'train\'][sequence_number],\n      model.input_token_character_indices: dataset.character_indices_padded[\'train\'][sequence_number],\n      model.input_token_lengths: dataset.token_lengths[\'train\'][sequence_number],\n      model.input_label_indices_flat: dataset.label_indices[\'train\'][sequence_number],\n      model.dropout_keep_prob: 1-parameters[\'dropout_rate\']}\n\n    _, _, loss, accuracy, transition_params_trained = sess.run(\n                    [model.train_op, model.global_step, model.loss, model.accuracy,\n                    model.transition_parameters],feed_dict)\n\n    return transition_params_trained\n\ndef prediction_step(sess, dataset, dataset_type, model, transition_params_trained,\n    stats_graph_folder, epoch_number, parameters, dataset_filepaths):\n    """"""\n    Predict.\n    """"""\n    if dataset_type == \'deploy\':\n        print(\'Predict labels for the {0} set\'.format(dataset_type))\n    else:\n        print(\'Evaluate model on the {0} set\'.format(dataset_type))\n\n    all_predictions = []\n    all_y_true = []\n    output_filepath = os.path.join(stats_graph_folder, \'{1:03d}_{0}.txt\'.format(dataset_type,\n        epoch_number))\n    output_file = codecs.open(output_filepath, \'w\', \'UTF-8\')\n    original_conll_file = codecs.open(dataset_filepaths[dataset_type], \'r\', \'UTF-8\')\n\n    for i in range(len(dataset.token_indices[dataset_type])):\n        feed_dict = {\n          model.input_token_indices: dataset.token_indices[dataset_type][i],\n          model.input_token_character_indices: dataset.character_indices_padded[dataset_type][i],\n          model.input_token_lengths: dataset.token_lengths[dataset_type][i],\n          model.input_label_indices_vector: dataset.label_vector_indices[dataset_type][i],\n          model.dropout_keep_prob: 1.\n        }\n\n        unary_scores, predictions = sess.run([model.unary_scores,\n            model.predictions], feed_dict)\n\n        if parameters[\'use_crf\']:\n            predictions, _ = tf.contrib.crf.viterbi_decode(unary_scores,\n                transition_params_trained)\n            predictions = predictions[1:-1]\n        else:\n            predictions = predictions.tolist()\n\n        assert(len(predictions) == len(dataset.tokens[dataset_type][i]))\n\n        output_string = \'\'\n        prediction_labels = [dataset.index_to_label[prediction] for prediction in predictions]\n        unary_score_list = unary_scores.tolist()[1:-1]\n\n        gold_labels = dataset.labels[dataset_type][i]\n\n        if parameters[\'tagging_format\'] == \'bioes\':\n            prediction_labels = utils_nlp.bioes_to_bio(prediction_labels)\n            gold_labels = utils_nlp.bioes_to_bio(gold_labels)\n\n        for prediction, token, gold_label, scores in zip(prediction_labels,\n            dataset.tokens[dataset_type][i], gold_labels, unary_score_list):\n\n            while True:\n                line = original_conll_file.readline()\n                split_line = line.strip().split(\' \')\n\n                if \'-DOCSTART-\' in split_line[0] or len(split_line) == 0 \\\n                or len(split_line[0]) == 0:\n                    continue\n                else:\n                    token_original = split_line[0]\n\n                    if parameters[\'tagging_format\'] == \'bioes\':\n                        split_line.pop()\n\n                    gold_label_original = split_line[-1]\n\n                    assert(token == token_original and gold_label == gold_label_original)\n                    break\n\n            split_line.append(prediction)\n            try:\n                if parameters[\'output_scores\']:\n                    # space separated scores\n                    scores = \' \'.join([str(i) for i in scores])\n                    split_line.append(\'{}\'.format(scores))\n            except KeyError:\n                pass\n\n            output_string += \' \'.join(split_line) + \'\\n\'\n\n        output_file.write(output_string+\'\\n\')\n\n        all_predictions.extend(predictions)\n        all_y_true.extend(dataset.label_indices[dataset_type][i])\n\n    output_file.close()\n    original_conll_file.close()\n\n    if dataset_type != \'deploy\':\n\n        if parameters[\'main_evaluation_mode\'] == \'conll\':\n\n            # run perl evaluation script in python package\n            # conll_evaluation_script = os.path.join(\'.\', \'conlleval\')\n            package_name = \'neuroner\'\n            root_dir = os.path.dirname(pkg_resources.resource_filename(package_name,\n                \'__init__.py\'))\n            conll_evaluation_script = os.path.join(root_dir, \'conlleval\')\n\n            conll_output_filepath = \'{0}_conll_evaluation.txt\'.format(output_filepath)\n            shell_command = \'perl {0} < {1} > {2}\'.format(conll_evaluation_script,\n                output_filepath, conll_output_filepath)\n            os.system(shell_command)\n\n            with open(conll_output_filepath, \'r\') as f:\n                classification_report = f.read()\n                print(classification_report)\n\n        else:\n            new_y_pred, new_y_true, new_label_indices, new_label_names, _, _ = remap_labels(all_predictions,\n                all_y_true, dataset, parameters[\'main_evaluation_mode\'])\n\n            print(sklearn.metrics.classification_report(new_y_true, new_y_pred, \n                digits=4, labels=new_label_indices, target_names=new_label_names))\n\n    return all_predictions, all_y_true, output_filepath\n\n\ndef predict_labels(sess, model, transition_params_trained, parameters, dataset,\n    epoch_number, stats_graph_folder, dataset_filepaths):\n    """"""\n    Predict labels using trained model\n    """"""\n    y_pred = {}\n    y_true = {}\n    output_filepaths = {}\n\n    for dataset_type in [\'train\', \'valid\', \'test\', \'deploy\']:\n        if dataset_type not in dataset_filepaths.keys():\n            continue\n\n        prediction_output = prediction_step(sess, dataset, dataset_type, model,\n            transition_params_trained, stats_graph_folder, epoch_number,\n            parameters, dataset_filepaths)\n        y_pred[dataset_type], y_true[dataset_type], output_filepaths[dataset_type] = prediction_output\n\n    return y_pred, y_true, output_filepaths\n'"
neuroner/utils.py,0,"b'\'\'\'\nMiscellaneous utility functions\n\'\'\'\nimport collections\nimport datetime\nimport os\nimport operator\nimport shutil\nimport time\nimport pickle\n\n# https://stackoverflow.com/questions/2121874/python-pickling-after-changing-a-modules-directory\nclass RenameUnpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        renamed_module = module\n        if module == ""dataset"":\n            renamed_module = ""neuroner.dataset""\n\n        return super(RenameUnpickler, self).find_class(renamed_module, name)\n\n\ndef renamed_load(file_obj):\n    return RenameUnpickler(file_obj).load()\n\ndef order_dictionary(dictionary, mode, reverse=False):\n    \'\'\'\n    Order a dictionary by \'key\' or \'value\'.\n    mode should be either \'key\' or \'value\'\n    http://stackoverflow.com/questions/613183/sort-a-python-dictionary-by-value\n    \'\'\'\n\n    if mode ==\'key\':\n        return collections.OrderedDict(sorted(dictionary.items(),\n                                              key=operator.itemgetter(0),\n                                              reverse=reverse))\n    elif mode ==\'value\':\n        return collections.OrderedDict(sorted(dictionary.items(),\n                                              key=operator.itemgetter(1),\n                                              reverse=reverse))\n    elif mode ==\'key_value\':\n        return collections.OrderedDict(sorted(dictionary.items(),\n                                              reverse=reverse))\n    elif mode ==\'value_key\':\n        return collections.OrderedDict(sorted(dictionary.items(),\n                                              key=lambda x: (x[1], x[0]),\n                                              reverse=reverse))\n    else:\n        raise ValueError(""Unknown mode. Should be \'key\' or \'value\'"")\n\ndef reverse_dictionary(dictionary):\n    \'\'\'\n    http://stackoverflow.com/questions/483666/python-reverse-inverse-a-mapping\n    http://stackoverflow.com/questions/25480089/right-way-to-initialize-an-ordereddict-using-its-constructor-such-that-it-retain\n    \'\'\'\n    #print(\'type(dictionary): {0}\'.format(type(dictionary)))\n    if type(dictionary) is collections.OrderedDict:\n        #print(type(dictionary))\n        return collections.OrderedDict([(v, k) for k, v in dictionary.items()])\n    else:\n        return {v: k for k, v in dictionary.items()}\n\ndef merge_dictionaries(*dict_args):\n    \'\'\'\n    http://stackoverflow.com/questions/38987/how-can-i-merge-two-python-dictionaries-in-a-single-expression\n    Given any number of dicts, shallow copy and merge into a new dict,\n    precedence goes to key value pairs in latter dicts.\n    \'\'\'\n    result = {}\n    for dictionary in dict_args:\n        result.update(dictionary)\n    return result\n\ndef pad_list(old_list, padding_size, padding_value):\n    \'\'\'\n    http://stackoverflow.com/questions/3438756/some-built-in-to-pad-a-list-in-python\n    Example: pad_list([6,2,3], 5, 0) returns [6,2,3,0,0]\n    \'\'\'\n    assert padding_size >= len(old_list)\n    return old_list + [padding_value] * (padding_size-len(old_list))\n\ndef get_basename_without_extension(filepath):\n    \'\'\'\n    Getting the basename of the filepath without the extension\n    E.g. \'data/formatted/movie_reviews.pickle\' -> \'movie_reviews\'\n    \'\'\'\n    return os.path.basename(os.path.splitext(filepath)[0])\n\ndef create_folder_if_not_exists(directory):\n    \'\'\'\n    Create the folder if it doesn\'t exist already.\n    \'\'\'\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\ndef get_current_milliseconds():\n    \'\'\'\n    http://stackoverflow.com/questions/5998245/get-current-time-in-milliseconds-in-python\n    \'\'\'\n    return(int(round(time.time() * 1000)))\n\n\ndef get_current_time_in_seconds():\n    \'\'\'\n    http://stackoverflow.com/questions/415511/how-to-get-current-time-in-python\n    \'\'\'\n    return(time.strftime(""%Y-%m-%d_%H-%M-%S"", time.localtime()))\n\ndef get_current_time_in_miliseconds():\n    \'\'\'\n    http://stackoverflow.com/questions/5998245/get-current-time-in-milliseconds-in-python\n    \'\'\'\n    return(get_current_time_in_seconds() + \'-\' + str(datetime.datetime.now().microsecond))\n\n\ndef convert_configparser_to_dictionary(config):\n    \'\'\'\n    http://stackoverflow.com/questions/1773793/convert-configparser-items-to-dictionary\n    \'\'\'\n    my_config_parser_dict = {s:dict(config.items(s)) for s in config.sections()}\n    return my_config_parser_dict\n\ndef get_parameter_to_section_of_configparser(config):\n    parameter_to_section = {}\n    for s in config.sections():\n        for p, _ in config.items(s):\n            parameter_to_section[p] = s\n    return parameter_to_section\n\n\ndef copytree(src, dst, symlinks=False, ignore=None):\n    \'\'\'\n    http://stackoverflow.com/questions/1868714/how-do-i-copy-an-entire-directory-of-files-into-an-existing-directory-using-pyth\n    \'\'\'\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if os.path.isdir(s):\n            shutil.copytree(s, d, symlinks, ignore)\n        else:\n            shutil.copy2(s, d)'"
neuroner/utils_nlp.py,0,"b'\'\'\'\nMiscellaneous utility functions for natural language processing\n\'\'\'\nimport codecs\nimport os\nimport re\n\nimport numpy as np\n\nfrom neuroner import utils\n\ndef load_tokens_from_pretrained_token_embeddings(parameters):\n    file_input = codecs.open(parameters[\'token_pretrained_embedding_filepath\'], \'r\', \'UTF-8\')\n    count = -1\n    tokens = set()\n    number_of_loaded_word_vectors = 0\n    for cur_line in file_input:\n        count += 1\n        cur_line = cur_line.strip()\n        cur_line = cur_line.split(\' \')\n        if len(cur_line)==0:continue\n        token=cur_line[0]\n        tokens.add(token)\n        number_of_loaded_word_vectors += 1\n    file_input.close()\n    return tokens\n\n\ndef load_pretrained_token_embeddings(parameters):\n    file_input = codecs.open(parameters[\'token_pretrained_embedding_filepath\'], \'r\', \'UTF-8\')\n    count = -1\n    token_to_vector = {}\n    for cur_line in file_input:\n        count += 1\n        #if count > 1000:break\n        cur_line = cur_line.strip()\n        cur_line = cur_line.split(\' \')\n        if len(cur_line)==0:continue\n        token = cur_line[0]\n        vector = np.array([float(x) for x in cur_line[1:]])\n        token_to_vector[token] = vector\n    file_input.close()\n    return token_to_vector\n\n\ndef is_token_in_pretrained_embeddings(token, all_pretrained_tokens, parameters):\n    return token in all_pretrained_tokens or \\\n        parameters[\'check_for_lowercase\'] and token.lower() in all_pretrained_tokens or \\\n        parameters[\'check_for_digits_replaced_with_zeros\'] and re.sub(\'\\d\', \'0\', token) in all_pretrained_tokens or \\\n        parameters[\'check_for_lowercase\'] and parameters[\'check_for_digits_replaced_with_zeros\'] and re.sub(\'\\d\', \'0\', token.lower()) in all_pretrained_tokens\n\n\ndef get_parsed_conll_output(conll_output_filepath):\n    conll_output = [l.rstrip().replace(\'%\',\'\').replace(\';\',\'\').replace(\':\', \'\').strip() for l in codecs.open(conll_output_filepath, \'r\', \'utf8\')]\n    parsed_output = {}\n    line = conll_output[1].split()\n    parsed_output[\'all\'] = {\'accuracy\': float(line[1]),\n                            \'precision\': float(line[3]),\n                            \'recall\':float(line[5]),\n                            \'f1\':float(line[7])}\n    total_support = 0\n    for line in conll_output[2:]:\n        line = line.split()\n        phi_type = line[0].replace(\'_\', \'-\')\n        support = int(line[7])\n        total_support += support\n        parsed_output[phi_type] = {\'precision\': float(line[2]),\n                            \'recall\':float(line[4]),\n                            \'f1\':float(line[6]),\n                            \'support\':support}\n    parsed_output[\'all\'][\'support\'] = total_support\n    return parsed_output\n\ndef remove_bio_from_label_name(label_name):\n    if label_name[:2] in [\'B-\', \'I-\', \'E-\', \'S-\']:\n        new_label_name = label_name[2:]\n    else:\n        assert(label_name == \'O\')\n        new_label_name = label_name\n    return new_label_name\n\ndef replace_unicode_whitespaces_with_ascii_whitespace(string):\n    return \' \'.join(string.split())\n\n\ndef end_current_entity(previous_label_without_bio, current_entity_length, new_labels, i):\n    \'\'\'\n    Helper function for bio_to_bioes\n    \'\'\'\n    if current_entity_length == 0:\n        return\n    if current_entity_length == 1:\n        new_labels[i - 1] = \'S-\' + previous_label_without_bio\n    else: #elif current_entity_length > 1\n        new_labels[i - 1] = \'E-\' + previous_label_without_bio \n\ndef bio_to_bioes(labels):\n    previous_label_without_bio = \'O\'\n    current_entity_length = 0\n    new_labels = labels.copy()\n    for i, label in enumerate(labels):\n        label_without_bio = remove_bio_from_label_name(label)\n        # end the entity\n        if current_entity_length > 0 and (label[:2] in [\'B-\', \'O\'] or label[:2] == \'I-\' and previous_label_without_bio != label_without_bio):\n            end_current_entity(previous_label_without_bio, current_entity_length, new_labels, i)\n            current_entity_length = 0\n        if label[:2] == \'B-\':\n            current_entity_length = 1\n        elif label[:2] == \'I-\':\n            if current_entity_length == 0:\n                new_labels[i] = \'B-\' + label_without_bio\n            current_entity_length += 1\n        previous_label_without_bio = label_without_bio    \n    end_current_entity(previous_label_without_bio, current_entity_length, new_labels, i + 1)\n    return new_labels\n\ndef bioes_to_bio(labels):\n    previous_label_without_bio = \'O\'\n    new_labels = labels.copy()\n    for i, label in enumerate(labels):\n        label_without_bio = remove_bio_from_label_name(label)\n        if label[:2] in [\'I-\', \'E-\']:\n            if previous_label_without_bio == label_without_bio:\n                new_labels[i] = \'I-\' + label_without_bio\n            else:\n                new_labels[i] = \'B-\' + label_without_bio\n        elif label[:2] in [\'S-\']:\n            new_labels[i] = \'B-\' + label_without_bio\n        previous_label_without_bio = label_without_bio\n    return new_labels\n                \n\ndef check_bio_bioes_compatibility(labels_bio, labels_bioes):\n    if labels_bioes == []:\n        return True\n    new_labels_bio = bioes_to_bio(labels_bioes)\n    flag = True\n    if new_labels_bio != labels_bio:\n        print(""Not valid."")\n        flag = False \n    del labels_bio[:]\n    del labels_bioes[:]\n    return flag\n\ndef check_validity_of_conll_bioes(bioes_filepath):\n    dataset_type = utils.get_basename_without_extension(bioes_filepath).split(\'_\')[0]\n    print(""Checking validity of CONLL BIOES format... "".format(dataset_type), end=\'\')\n\n    input_conll_file = codecs.open(bioes_filepath, \'r\', \'UTF-8\')\n    labels_bioes = []\n    labels_bio = []\n    for line in input_conll_file:\n        split_line = line.strip().split(\' \')\n        # New sentence\n        if len(split_line) == 0 or len(split_line[0]) == 0 or \'-DOCSTART-\' in split_line[0]:\n            if check_bio_bioes_compatibility(labels_bio, labels_bioes):\n                continue\n            return False\n        label_bioes = split_line[-1]    \n        label_bio = split_line[-2]    \n        labels_bioes.append(label_bioes)\n        labels_bio.append(label_bio)\n    input_conll_file.close()\n    if check_bio_bioes_compatibility(labels_bio, labels_bioes):\n        print(""Done."")\n        return True\n    return False\n             \ndef output_conll_lines_with_bioes(split_lines, labels, output_conll_file):\n    \'\'\'\n    Helper function for convert_conll_from_bio_to_bioes\n    \'\'\'\n    if labels == []:\n        return\n    new_labels = bio_to_bioes(labels)\n    assert(len(new_labels) == len(split_lines))\n    for split_line, new_label in zip(split_lines, new_labels):\n        output_conll_file.write(\' \'.join(split_line + [new_label]) + \'\\n\')\n    del labels[:]\n    del split_lines[:]\n\n\ndef convert_conll_from_bio_to_bioes(input_conll_filepath, output_conll_filepath):\n    if os.path.exists(output_conll_filepath):\n        if check_validity_of_conll_bioes(output_conll_filepath):\n            return\n    dataset_type = utils.get_basename_without_extension(input_conll_filepath).split(\'_\')[0]\n    print(""Converting CONLL from BIO to BIOES format... "".format(dataset_type), end=\'\')\n    input_conll_file = codecs.open(input_conll_filepath, \'r\', \'UTF-8\')\n    output_conll_file = codecs.open(output_conll_filepath, \'w\', \'UTF-8\')\n\n    labels = []\n    split_lines = []\n    for line in input_conll_file:\n        split_line = line.strip().split(\' \')\n        # New sentence\n        if len(split_line) == 0 or len(split_line[0]) == 0 or \'-DOCSTART-\' in split_line[0]:\n            output_conll_lines_with_bioes(split_lines, labels, output_conll_file)\n            output_conll_file.write(line)\n            continue\n        label = split_line[-1]    \n        labels.append(label)\n        split_lines.append(split_line)\n    output_conll_lines_with_bioes(split_lines, labels, output_conll_file)\n    \n    input_conll_file.close()\n    output_conll_file.close()\n    print(""Done."")\n    \n'"
neuroner/utils_plots.py,0,"b'\'\'\'\nMiscellaneous functions for plots\n\'\'\'\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport matplotlib\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\n# http://stackoverflow.com/questions/2801882/generating-a-png-with-matplotlib-when-display-is-undefined\nmatplotlib.use(\'Agg\') \nimport numpy as np\nimport sklearn.preprocessing\n\ndef get_cmap():\n    \'\'\'\n    http://stackoverflow.com/questions/37517587/how-can-i-change-the-intensity-of-a-colormap-in-matplotlib\n    \'\'\'\n    cmap = cm.get_cmap(\'RdBu\', 256) # set how many colors you want in color map\n    # modify colormap\n    alpha = 1.0\n    colors = []\n    for ind in range(cmap.N):\n        c = []\n        if ind<128 or ind> 210: continue\n        for x in cmap(ind)[:3]: c.append(min(1,x*alpha))\n        colors.append(tuple(c))\n    my_cmap = matplotlib.colors.ListedColormap(colors, name = \'my_name\')\n    return my_cmap\n\ndef show_values(pc, fmt=""%.2f"", **kw):\n    \'\'\'\n    Heatmap with text in each cell with matplotlib\'s pyplot\n    Source: http://stackoverflow.com/a/25074150/395857\n    By HYRY\n    \'\'\'\n    pc.update_scalarmappable()\n    ax = pc.axes\n    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.all(color[:3] > 0.5):\n            color = (0.0, 0.0, 0.0)\n        else:\n            color = (1.0, 1.0, 1.0)\n        ax.text(x, y, fmt % value, ha=""center"", va=""center"", color=color, **kw)\n\ndef cm2inch(*tupl):\n    \'\'\'\n    Specify figure size in centimeter in matplotlib\n    Source: http://stackoverflow.com/a/22787457/395857\n    By gns-ank\n    \'\'\'\n    inch = 2.54\n    if type(tupl[0]) == tuple:\n        return tuple(i/inch for i in tupl[0])\n    else:\n        return tuple(i/inch for i in tupl)\n\ndef heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap=\'RdBu\', fmt=""%.2f"", graph_filepath=\'\', normalize=False, remove_diagonal=False):\n    \'\'\'\n    Inspired by:\n    - http://stackoverflow.com/a/16124677/395857\n    - http://stackoverflow.com/a/25074150/395857\n    \'\'\'\n    if normalize:\n        AUC = sklearn.preprocessing.normalize(AUC, norm=\'l1\', axis=1)\n        \n    if remove_diagonal:\n        matrix = np.copy(AUC)\n        np.fill_diagonal(matrix, 0)\n        if len(xticklabels)>2:\n            matrix[:,-1] = 0\n            matrix[-1, :] = 0\n        values= matrix.flatten()\n    else:\n        values = AUC.flatten()\n    vmin = values.min()\n    vmax = values.max()\n\n    # Plot it out\n    fig, ax = plt.subplots()\n    #c = ax.pcolor(AUC, edgecolors=\'k\', linestyle= \'dashed\', linewidths=0.2, cmap=\'RdBu\', vmin=0.0, vmax=1.0)\n    c = ax.pcolor(AUC, edgecolors=\'k\', linestyle= \'dashed\', linewidths=0.2, cmap=get_cmap(), vmin=vmin, vmax=vmax)\n\n    # put the major ticks at the middle of each cell\n    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n\n    # set tick labels\n    ax.set_xticklabels(xticklabels, minor=False)\n    ax.set_yticklabels(yticklabels, minor=False)\n\n    # set title and x/y labels\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n\n    # Remove last blank column\n    plt.xlim( (0, AUC.shape[1]) )\n\n    # Turn off all the ticks\n    ax = plt.gca()\n    for t in ax.xaxis.get_major_ticks():\n        t.tick1On = False\n        t.tick2On = False\n    for t in ax.yaxis.get_major_ticks():\n        t.tick1On = False\n        t.tick2On = False\n\n    # Add color bar\n    plt.colorbar(c)\n\n    # Add text in each cell\n    show_values(c, fmt=fmt)\n\n    # Proper orientation (origin at the top left instead of bottom left)\n    if correct_orientation:\n        ax.invert_yaxis()\n        ax.xaxis.tick_top()\n\n    # resize\n    fig = plt.gcf()\n    fig.set_size_inches(cm2inch(figure_width, figure_height))\n\n    if graph_filepath != \'\':\n        plt.savefig(graph_filepath, dpi=300, format=\'png\', bbox_inches=\'tight\')\n        plt.close()\n\n\ndef plot_classification_report(classification_report, title=\'Classification report \', cmap=\'RdBu\', from_conll_json=False):\n    \'\'\'\n    Plot scikit-learn classification report.\n    Extension based on http://stackoverflow.com/a/31689645/395857\n    \'\'\'\n    classes = []\n    plotMat = []\n    support = []\n    class_names = []\n    if from_conll_json:\n        for label in sorted(classification_report.keys()):\n            support.append(classification_report[label][""support""])\n            classes.append(\'micro-avg\' if label==\'all\' else label)\n            class_names.append(\'micro-avg\' if label==\'all\' else label)\n            plotMat.append([float(classification_report[label][x]) for x in [""precision"", ""recall"", ""f1""]]) \n    else:\n        lines = classification_report.split(\'\\n\')\n        for line in lines[2 : (len(lines) - 1)]:\n            t = line.strip().replace(\' avg\', \'-avg\').split()\n            if len(t) < 2: continue\n            classes.append(t[0])\n            v = [float(x)*100 for x in t[1: len(t) - 1]]\n            support.append(int(t[-1]))\n            class_names.append(t[0])\n            plotMat.append(v)\n    \n    xlabel = \'Metrics\'\n    ylabel = \'Classes\'\n    xticklabels = [\'Precision\', \'Recall\', \'F1-score\']\n    yticklabels = [\'{0} ({1})\'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n    figure_width = 25\n    figure_height = len(class_names) + 7\n    correct_orientation = True\n\n    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)\n\n\ndef plot_hist(sequence, xlabel, ylabel, title, graph_path):\n    xmin = min(sequence)\n    xmax = max(sequence)\n    step = 1\n    y, x = np.histogram(sequence, bins=np.linspace(xmin, xmax, (xmax-xmin+1)/step))\n\n    plt.bar(x[:-1], y, width=x[1]-x[0], color=\'red\', alpha=0.5)\n    plt.grid(True)\n    plt.xlabel(xlabel, fontsize=8)\n    plt.title(title, fontsize=12)\n    plt.ylabel(ylabel, fontsize=8)\n    plt.savefig(graph_path, dpi=300, format=\'png\', bbox_inches=\'tight\')\n    plt.close()\n\n\ndef plot_barh(x, y, xlabel, ylabel, title, graph_path):\n    width = 1\n    fig, ax = plt.subplots()\n    ind = np.arange(len(y))  # the x locations for the groups\n    ax.barh(ind, y, color=""blue"")\n    ax.set_yticks(ind+width/2)\n    ax.set_yticklabels(x, minor=False)\n    # http://stackoverflow.com/questions/30228069/how-to-display-the-value-of-the-bar-on-each-bar-with-pyplot-barh/30229062#30229062\n    for i, v in enumerate(y):\n        ax.text(v + 3, i + .25, str(v), color=\'blue\', fontweight=\'bold\')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.savefig(graph_path, dpi=300, format=\'png\', bbox_inches=\'tight\') # use format=\'svg\' or \'pdf\' for vectorial pictures\n    plt.clf()\n    plt.close()\n\n\ndef plot_precision_recall_curve(recall, precision, graph_path, title):\n    plt.clf()\n    plt.plot(recall, precision, label=\'Precision-Recall curve\')\n    plt.xlabel(\'Recall\')\n    plt.ylabel(\'Precision\')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title(title)\n    plt.legend(loc=""upper right"")\n    plt.savefig(graph_path, dpi=600, format=\'pdf\', bbox_inches=\'tight\') # use format=\'svg\' or \'pdf\' for vectorial pictures\n    plt.close()\n\n\ndef plot_roc_curve(fpr, tpr, graph_path, title):\n    plt.clf()\n    plt.plot(fpr, tpr, label=\'ROC curve\')\n    plt.xlabel(\'FPR\')\n    plt.ylabel(\'TPR\')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title(title)\n    plt.legend(loc=""lower left"")\n    plt.savefig(graph_path, dpi=600, format=\'pdf\', bbox_inches=\'tight\') # use format=\'svg\' or \'pdf\' for vectorial pictures\n    plt.close()\n\n\ndef plot_threshold_vs_accuracy_curve(accuracies, thresholds, graph_path, title):\n    plt.clf()\n    plt.plot(thresholds, accuracies, label=\'ROC curve\')\n    plt.xlabel(\'Threshold\')\n    plt.ylabel(\'Accuracy\')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title(title)\n    plt.legend(loc=""lower left"")\n    plt.savefig(graph_path, dpi=600, format=\'pdf\', bbox_inches=\'tight\') # use format=\'svg\' or \'pdf\' for vectorial pictures\n    plt.close()\n\n\n'"
neuroner/utils_tf.py,10,"b""import tensorflow as tf\n\ndef variable_summaries(var):\n    '''\n    Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n    From https://www.tensorflow.org/get_started/summaries_and_tensorboard\n    '''\n    with tf.name_scope('summaries'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar('mean', mean)\n        with tf.name_scope('stddev'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar('stddev', stddev)\n        tf.summary.scalar('max', tf.reduce_max(var))\n        tf.summary.scalar('min', tf.reduce_min(var))\n        tf.summary.histogram('histogram', var)\n    \ndef resize_tensor_variable(sess, tensor_variable, shape):\n    sess.run(tf.assign(tensor_variable, tf.zeros(shape), validate_shape=False))\n"""
test/test_main.py,0,"b'\'\'\'\nTests for main.py\n\'\'\'\n\nimport os\nfrom shutil import rmtree\nimport unittest\n\nfrom neuroner import neuromodel\n\nclass TestMain(unittest.TestCase):\n\n    outputFolder = os.path.join(\'.\', ""output"")\n    test_param_file = os.path.join(\'.\', \'test-parameters-training.ini\')\n\n    def setUp(self):\n        # delete the outputFolder\n        if os.path.isdir(self.outputFolder):\n            rmtree(self.outputFolder)\n\n    def tearDown(self):\n        # delete the outputFolder\n        if os.path.isdir(self.outputFolder):\n            rmtree(self.outputFolder)\n\n    def test_ProvideOutputDir_CorrectlyOutputsToDir(self):\n        """"""\n        Sanity test to check if all proper model output files are created in the output folder\n        """"""\n        nn = neuromodel.NeuroNER(output_folder=self.outputFolder, parameters_filepath=self.test_param_file)\n        nn.fit()\n\n        # find the newest dir, from: http://stackoverflow.com/questions/2014554/find-the-newest-folder-in-a-directory-in-python\n        run_outputdir = max([os.path.join(self.outputFolder,d) for d in os.listdir(self.outputFolder)], key=os.path.getmtime)\n\n        # assert the model has been written to files\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'checkpoint\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'dataset.pickle\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'model_00001.ckpt.data-00000-of-00001\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'model_00001.ckpt.index\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'model_00001.ckpt.meta\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'model_00002.ckpt.data-00000-of-00001\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'model_00002.ckpt.index\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'model_00002.ckpt.meta\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'parameters.ini\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'projector_config.pbtxt\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'tensorboard_metadata_characters.tsv\')))\n        self.assertTrue(os.path.isfile(os.path.join(run_outputdir, \'model\', \'tensorboard_metadata_tokens.tsv\')))'"
neuroner/data/i2b2_2014_deid/xml_to_brat.py,0,"b'\'\'\'\nThis script convert the i2b2 2014 dataset into BRAT format.\n\'\'\'\nimport xml.etree.ElementTree\nimport os\nimport sys\nimport time\nimport glob\nimport codecs\nimport shutil\n\nfrom neuroner.conll_to_brat import output_entities\nfrom neuroner import utils\n\n\ndef xml_to_brat(input_folder, output_folder, overwrite=True):\n    print(\'input_folder: {0}\'.format(input_folder))\n    start_time = time.time()\n    if overwrite:\n        shutil.rmtree(output_folder, ignore_errors=True)\n    utils.create_folder_if_not_exists(output_folder)\n\n    for input_filepath in sorted(glob.glob(os.path.join(input_folder, \'*.xml\'))):\n        filename = utils.get_basename_without_extension(input_filepath)\n        output_text_filepath = os.path.join(output_folder, \'{0}.txt\'.format(filename))\n        xmldoc = xml.etree.ElementTree.parse(input_filepath).getroot()\n        # Get text\n        text = xmldoc.findtext(\'TEXT\')\n        with codecs.open(output_text_filepath, \'w\', \'UTF-8\') as f:\n            f.write(text)\n\n        # Get PHI tags\n        tags = xmldoc.findall(\'TAGS\')[0] # [0] because there is only one <TAGS>...</TAGS>\n        entities = []\n        for tag in tags:\n            entity = {}\n            entity[\'label\'] = tag.get(\'TYPE\')\n            entity[\'text\'] = tag.get(\'text\')\n            entity[\'start\'] = int(tag.get(\'start\'))\n            entity[\'end\'] = int(tag.get(\'end\'))\n            entities.append(entity)\n        output_entities(output_folder, filename, entities, output_text_filepath, text, overwrite=overwrite)\n\n    time_spent = time.time() - start_time\n    print(""Time spent formatting: {0:.2f} seconds"".format(time_spent))\n\nif __name__ == \'__main__\':\n    print(""Started formatting i2b2_2014_deid\'s XML files to BRAT"")\n    dataset_base_filename = \'.\'\n    split = \'.\'\n    dataset_folder_original = os.path.join(\'.\')\n    input_folder = os.path.join(dataset_folder_original, dataset_base_filename, split, \'training-PHI-Gold-Set1\')\n    output_base_folder = os.path.join(\'.\')\n    output_folder = os.path.join(output_base_folder, \'train\')\n    xml_to_brat(input_folder, output_folder)\n\n    input_folder = os.path.join(dataset_folder_original, dataset_base_filename, split, \'training-PHI-Gold-Set2\')\n    output_folder = os.path.join(output_base_folder, \'valid\')\n    xml_to_brat(input_folder, output_folder)\n\n    input_folder = os.path.join(dataset_folder_original, dataset_base_filename, split, \'testing-PHI-Gold-fixed\')\n    output_folder = os.path.join(output_base_folder, \'test\')\n    xml_to_brat(input_folder, output_folder)'"
