file_path,api_count,code
lab1/01_hello_world.py,2,"b""from __future__ import print_function\n\n\nimport tensorflow as tf\n\nhello = tf.constant('Hello, TensorFlow!')\n\n# Start tf session\nsess = tf.Session()\n\nprint(sess.run(hello))\n"""
lab1/02_operations.py,3,b'from __future__ import print_function\n\nimport tensorflow as tf\n\n# Start tf session\nsess = tf.Session()\n\na = tf.constant(2)\nb = tf.constant(3)\n\nc = a+b\n\n# Print out operation everything is operation\nprint(a)\nprint(b)\nprint(c)\n\nprint(a+b)\n\n\n# Print out the result of operation\nprint(sess.run(a))\nprint(sess.run(b))\nprint(sess.run(c))\nprint(sess.run(a+b))\n'
lab1/03_placeholder.py,5,"b""from __future__ import print_function\n\n\nimport tensorflow as tf\n\na = tf.placeholder(tf.int16)\nb = tf.placeholder(tf.int16)\n\nadd = tf.add(a, b)\nmul = tf.multiply(a, b)\n\n# Same op?\nprint(add)\nprint(a + b)\nprint(mul)\nprint(a * b)\n\n# Launch the default graph\nwith tf.Session() as sess:\n    print(sess.run(add, feed_dict={a: 2, b: 3}))\n\n    # it's work!\n    feed = {a: 3, b: 5}\n    print(sess.run(mul, feed_dict=feed))\n"""
lab2/01_linear_regression.py,7,"b'from __future__ import print_function\n\n\nimport tensorflow as tf\n\nx_data = [1, 2, 3]\ny_data = [1, 2, 3]\n\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n\nhypothesis = W * x_data + b\n\ncost = tf.reduce_mean(tf.square(hypothesis - y_data))\n\na = tf.Variable(0.1)\noptimizer = tf.train.GradientDescentOptimizer(a)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(cost), sess.run(W), sess.run(b))\n'"
lab2/02_linear_regression_placeholder.py,9,"b'from __future__ import print_function\n\nimport tensorflow as tf\n\nx_data = [1, 2, 3]\ny_data = [1, 2, 3]\n\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nhypothesis = W * X + b\n\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\na = tf.Variable(0.1)\noptimizer = tf.train.GradientDescentOptimizer(a)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train, feed_dict={X: x_data, Y: y_data})\n    if step % 20 == 0:\n        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W), sess.run(b))\n\nprint(sess.run(hypothesis, feed_dict={X: 5}))\nprint(sess.run(hypothesis, feed_dict={X: 2.5}))\n'"
lab3/01_show_cost.py,5,"b""from __future__ import print_function\n\n\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n# Graph Input\nX = [1., 2., 3.]\nY = [1., 2., 3.]\nm = n_smaples = len(X)\n\n# model weight\nW = tf.placeholder(tf.float32)\n\n# Construct a linear model\nhypothesis = tf.multiply(X, W)\n\n# Cost function\ncost = tf.reduce_sum(tf.pow(hypothesis - Y, 2)) / m\n\ninit = tf.global_variables_initializer()\n\n# for graphs\nW_val = []\ncost_val = []\n\n# Launch the graphs\nsess = tf.Session()\nsess.run(init)\n\nfor i in range(-30, 50):\n    print(i * -0.1, sess.run(cost, feed_dict={W: i * 0.1}))\n    W_val.append(i * 0.1)\n    cost_val.append(sess.run(cost, feed_dict={W: i * 0.1}))\n\nplt.plot(W_val, cost_val, 'ro')\nplt.ylabel('cost')\nplt.xlabel('W')\nplt.show()\n"""
lab3/02_hand_made_descent.py,7,"b'from __future__ import print_function\n\n\nimport tensorflow as tf\n\nx_data = [1, 2, 3]\ny_data = [1, 2, 3]\n\nW = tf.Variable(tf.random_uniform([1], -10.0, 10.0))\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nhypothesis = W * X\n\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\nlr = 0.1\ndescent = W - tf.multiply(lr, tf.reduce_mean(tf.multiply((tf.multiply(W, X) - Y), X)))\ntrain = W.assign(descent)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train, feed_dict={X: x_data, Y: y_data})\n    if step % 20 == 0:\n        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n\nprint(sess.run(hypothesis, feed_dict={X: 5}))\nprint(sess.run(hypothesis, feed_dict={X: 2.5}))\n'"
lab4/01_multi-variable_linear_regression.py,7,"b'from __future__ import print_function\n\nimport tensorflow as tf\n\nx1_data = [1, 0, 3, 0, 5]\nx2_data = [0, 2, 0, 4, 0]\ny_data = [1, 2, 3, 4, 5]\n\nW1 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nW2 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n\nb = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n\nhypothesis = W1 * x1_data + W2 * x2_data + b\n\ncost = tf.reduce_mean(tf.square(hypothesis - y_data))\n\nlearning_rate = 0.1\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(cost), sess.run(W1), sess.run(W2), sess.run(b))\n'"
lab4/02_with_matrix.py,7,"b'from __future__ import print_function\n\nimport tensorflow as tf\n\nx_data = [[1., 0., 3., 0., 5.],\n          [0., 2., 0., 4., 0.]]\n\ny_data = [1, 2, 3, 4, 5]\n\nW = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\nb = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n\nhypothesis = tf.matmul(W, x_data) + b\n\ncost = tf.reduce_mean(tf.square(hypothesis - y_data))\n\nlearning_rate = 0.1\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(cost), sess.run(W), sess.run(b))\n'"
lab4/03_edit_bias.py,6,"b'from __future__ import print_function\n\nimport tensorflow as tf\n\nx_data = [[1., 1., 1., 1., 1.],\n          [1., 0., 3., 0., 5.],\n          [0., 2., 0., 4., 0.]]\n\ny_data = [1, 2, 3, 4, 5]\n\nW = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0))\n\nhypothesis = tf.matmul(W, x_data)\n\ncost = tf.reduce_mean(tf.square(hypothesis - y_data))\n\nlearning_rate = 0.1\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(cost), sess.run(W))\n'"
lab4/04_loading_data_from_file.py,6,"b""from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nxy = np.loadtxt('train.txt', unpack=True, dtype='float32')\nx_data = xy[0:-1]\ny_data = xy[-1]\n\nW = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0))\n\nhypothesis = tf.matmul(W, x_data)\n\ncost = tf.reduce_mean(tf.square(hypothesis - y_data))\n\nlearning_rate = 0.1\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(cost), sess.run(W))\n"""
lab5/01_logistic_regression.py,9,"b""from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nxy = np.loadtxt('train.txt', unpack=True, dtype='float32')\nx_data = xy[0:-1]\ny_data = xy[-1]\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n\nh = tf.matmul(W, X)\nhypothesis = tf.div(1., 1. + tf.exp(-h))\n\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n\nlearning_rate = 0.1\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train, feed_dict={X: x_data, Y: y_data})\n\n    if step % 20 == 0:\n        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n"""
lab5/02_ask_to_ml.py,9,"b""from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nxy = np.loadtxt('train.txt', unpack=True, dtype='float32')\nx_data = xy[0:-1]\ny_data = xy[-1]\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n\nh = tf.matmul(W, X)\nhypothesis = tf.div(1., 1. + tf.exp(-h))\n\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n\nlearning_rate = 0.1\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntrain = optimizer.minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init)\n\nfor step in range(2001):\n    sess.run(train, feed_dict={X: x_data, Y: y_data})\n\n    if step % 20 == 0:\n        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n\nprint('---------------------------')\n\nprint(sess.run(hypothesis, feed_dict={X: [[1], [2], [2]]}) > 0.5)\nprint(sess.run(hypothesis, feed_dict={X: [[1], [5], [5]]}) > 0.5)\nprint(sess.run(hypothesis, feed_dict={X: [[1, 1], [4, 3], [3, 5]]}) > 0.5)\n"""
lab6/01_softmax_classification.py,8,"b'from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nxy = np.loadtxt(\'train.txt\', unpack=True, dtype=\'float32\')\nx_data = np.transpose(xy[0:3])\ny_data = np.transpose(xy[3:])\n\n\nX = tf.placeholder(""float"", [None, 3])\nY = tf.placeholder(""float"", [None, 3])\n\nW = tf.Variable(tf.zeros([3, 3]))\n\n# matrix shape X=[8, 3], W=[3, 3]\nhypothesis = tf.nn.softmax(tf.matmul(X, W))\n\nlearning_rate = 0.001\n\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for step in range(2001):\n        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n        if step % 200 == 0:\n            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n'"
lab6/02_test_ont-hot_encoding.py,12,"b'from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nxy = np.loadtxt(\'train.txt\', unpack=True, dtype=\'float32\')\nx_data = np.transpose(xy[0:3])\ny_data = np.transpose(xy[3:])\n\nX = tf.placeholder(""float"", [None, 3])\nY = tf.placeholder(""float"", [None, 3])\n\nW = tf.Variable(tf.zeros([3, 3]))\n\n# matrix shape X=[8, 3], W=[3, 3]\nhypothesis = tf.nn.softmax(tf.matmul(X, W))\n\nlearning_rate = 0.001\n\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for step in range(2001):\n        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n        if step % 200 == 0:\n            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n\n    print(\'--------------------\')\n\n    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7]]})\n    print(a, sess.run(tf.argmax(a, 1)))\n\n    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4]]})\n    print(b, sess.run(tf.argmax(b, 1)))\n\n    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0]]})\n    print(c, sess.run(tf.argmax(c, 1)))\n\n    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7], [1, 3, 4], [1, 1, 0]]})\n    print(all, sess.run(tf.argmax(all, 1)))\n'"
