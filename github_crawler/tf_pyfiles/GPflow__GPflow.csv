file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# pylint: skip-file\n\nimport os\nimport sys\n\nfrom setuptools import find_packages, setup\n\n\n# Dependencies of GPflow\nrequirements = [\n    ""numpy>=1.10.0"",\n    ""scipy>=0.18.0"",\n    ""multipledispatch>=0.6"",\n    ""tabulate"",\n    ""typing_extensions"",\n]\n\nif sys.version_info < (3, 7):\n    # became part of stdlib in python 3.7\n    requirements.append(""dataclasses"")\n\n# We do not want to install tensorflow in the readthedocs environment, where we\n# use autodoc_mock_imports instead. Hence we use this flag to decide whether or\n# not to append tensorflow and tensorflow_probability to the requirements:\nif os.environ.get(""READTHEDOCS"") != ""True"":\n    requirements.extend([""tensorflow>=2.1.0"", ""tensorflow-probability>=0.9""])\n\n\ndef read_file(filename):\n    with open(filename, encoding=""utf-8"") as f:\n        return f.read().strip()\n\n\nversion = read_file(""VERSION"")\nreadme_text = read_file(""README.md"")\n\npackages = find_packages(""."", exclude=[""tests""])\n\nsetup(\n    name=""gpflow"",\n    version=version,\n    author=""James Hensman, Alex Matthews"",\n    author_email=""james.hensman@gmail.com"",\n    description=""Gaussian process methods in TensorFlow"",\n    long_description=readme_text,\n    long_description_content_type=""text/markdown"",\n    license=""Apache License 2.0"",\n    keywords=""machine-learning gaussian-processes kernels tensorflow"",\n    url=""https://www.gpflow.org"",\n    project_urls={\n        ""Source on GitHub"": ""https://github.com/GPflow/GPflow"",\n        ""Documentation"": ""https://gpflow.readthedocs.io"",\n    },\n    packages=packages,\n    include_package_data=True,\n    install_requires=requirements,\n    extras_require={""ImageToTensorBoard"": [""matplotlib""]},\n    python_requires="">=3.6"",\n    classifiers=[\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Natural Language :: English"",\n        ""Operating System :: MacOS :: MacOS X"",\n        ""Operating System :: Microsoft :: Windows"",\n        ""Operating System :: POSIX :: Linux"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n)\n'"
gpflow/__init__.py,0,"b'# Copyright 2016 alexggmatthews, James Hensman\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# flake8: noqa\n\nfrom .base import Module, Parameter\nfrom .config import default_int, default_float, default_jitter\nfrom .utilities import set_trainable\nfrom . import (\n    conditionals,\n    config,\n    covariances,\n    expectations,\n    inducing_variables,\n    kernels,\n    kullback_leiblers,\n    likelihoods,\n    logdensities,\n    mean_functions,\n    models,\n    monitor,\n    optimizers,\n    probability_distributions,\n    quadrature,\n    utilities,\n)\nfrom .versions import __version__\n\n__all__ = [export for export in dir()]\n'"
gpflow/base.py,35,"b'import functools\nfrom enum import Enum\nfrom typing import Any, List, Optional, Sequence, TYPE_CHECKING, Tuple, Union\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow.python.ops import array_ops\nfrom typing_extensions import Final\n\nfrom .config import default_float, default_summary_fmt\n\nif TYPE_CHECKING:\n    from IPython.lib import pretty\n\nDType = Union[np.dtype, tf.DType]\nVariableData = Union[List, Tuple, np.ndarray, int, float]  # deprecated\nTransform = Union[tfp.bijectors.Bijector]\nPrior = Union[tfp.distributions.Distribution]\n\n\nTensorType = Union[tf.Tensor, tf.Variable, ""Parameter""]\n""""""\nType alias for tensor-like types that are supported by most TensorFlow and GPflow operations.\n\nNOTE: Union types like this do not work with the `register` method of `multipledispatch`\'s\n`Dispatcher` class. Instead use `TensorLike`.\n""""""\n\n\n# We\'ve left this as object until we\'ve tested the performance consequences of using the full set\n# (np.ndarray, tf.Tensor, tf.Variable, Parameter), see https://github.com/GPflow/GPflow/issues/1434\nTensorLike: Final[Tuple[type, ...]] = (object,)\n""""""\n:var TensorLike: Collection of tensor-like types for registering implementations with\n    `multipledispatch` dispatchers.\n""""""\n\n\n_NativeScalar = Union[int, float]\n_Array = Sequence[Any]  # a nested array of int, float, bool etc. kept simple for readability\nTensorData = Union[_NativeScalar, _Array, TensorType]\n\n\ndef _IS_PARAMETER(o: Any) -> bool:\n    return isinstance(o, Parameter)\n\n\ndef _IS_TRAINABLE_PARAMETER(o: Any) -> bool:\n    return _IS_PARAMETER(o) and o.trainable\n\n\nclass Module(tf.Module):\n    @property\n    def parameters(self) -> Tuple[""Parameter"", ...]:\n        return tuple(self._flatten(predicate=_IS_PARAMETER))\n\n    @property\n    def trainable_parameters(self) -> Tuple[""Parameter"", ...]:\n        return tuple(self._flatten(predicate=_IS_TRAINABLE_PARAMETER))\n\n    def _representation_table(self, object_name: str, tablefmt: Optional[str]) -> str:\n        from .utilities import leaf_components, tabulate_module_summary\n\n        repr_components = [object_name]\n        if leaf_components(self):\n            repr_components.append(tabulate_module_summary(self, tablefmt=tablefmt))\n        return ""\\n"".join(repr_components)\n\n    def _repr_html_(self) -> str:\n        """""" Nice representation of GPflow objects in IPython/Jupyter notebooks """"""\n        from html import escape\n\n        return self._representation_table(escape(repr(self)), ""html"")\n\n    def _repr_pretty_(self, p: ""pretty.RepresentationPrinter"", cycle: bool) -> None:\n        """""" Nice representation of GPflow objects in the IPython shell """"""\n        repr_str = self._representation_table(repr(self), default_summary_fmt())\n        p.text(repr_str)\n\n\nclass PriorOn(Enum):\n    CONSTRAINED = ""constrained""\n    UNCONSTRAINED = ""unconstrained""\n\n\nclass Parameter(tf.Module):\n    def __init__(\n        self,\n        value: TensorData,\n        *,\n        transform: Optional[Transform] = None,\n        prior: Optional[Prior] = None,\n        prior_on: Union[str, PriorOn] = PriorOn.CONSTRAINED,\n        trainable: bool = True,\n        dtype: Optional[DType] = None,\n        name: Optional[str] = None,\n    ):\n        """"""\n        A parameter retains both constrained and unconstrained\n        representations. If no transform is provided, these two values will be the same.\n        It is often challenging to operate with unconstrained parameters. For example, a variance cannot be negative,\n        therefore we need a positive constraint and it is natural to use constrained values.\n        A prior can be imposed either on the constrained version (default) or on the unconstrained version of the parameter.\n        """"""\n        super().__init__()\n\n        self._transform = transform\n        self.prior = prior\n        self.prior_on = prior_on  # type: ignore  # see https://github.com/python/mypy/issues/3004\n\n        if isinstance(value, tf.Variable):\n            self._unconstrained = value\n        else:\n            unconstrained_value = self.validate_unconstrained_value(value, dtype)\n            self._unconstrained = tf.Variable(\n                unconstrained_value, dtype=dtype, name=name, trainable=trainable\n            )\n\n    def log_prior_density(self) -> tf.Tensor:\n        """""" Log of the prior probability density of the constrained variable. """"""\n\n        if self.prior is None:\n            return tf.convert_to_tensor(0.0, dtype=self.dtype)\n\n        y = self.read_value()\n\n        if self.prior_on == PriorOn.CONSTRAINED:\n            # evaluation is in same space as prior\n            return tf.reduce_sum(self.prior.log_prob(y))\n\n        else:\n            # prior on unconstrained, but evaluating log-prior in constrained space\n            x = self._unconstrained\n            log_p = tf.reduce_sum(self.prior.log_prob(x))\n\n            if self.transform is not None:\n                # need to include log|Jacobian| to account for coordinate transform\n                log_det_jacobian = self.transform.inverse_log_det_jacobian(y, y.shape.ndims)\n                log_p += tf.reduce_sum(log_det_jacobian)\n\n            return log_p\n\n    @property\n    def prior_on(self) -> PriorOn:\n        return self._prior_on\n\n    @prior_on.setter\n    def prior_on(self, value: Union[str, PriorOn]) -> None:\n        self._prior_on = PriorOn(value)\n\n    def value(self) -> tf.Tensor:\n        return _to_constrained(self._unconstrained.value(), self.transform)  # type: ignore  # assumes _to_constrained returns a tf.Tensor\n\n    def read_value(self) -> tf.Tensor:\n        return _to_constrained(self._unconstrained.read_value(), self.transform)  # type: ignore  # assumes _to_constrained returns a tf.Tensor\n\n    def experimental_ref(self) -> ""Parameter"":\n        return self\n\n    def deref(self) -> ""Parameter"":\n        return self\n\n    @property\n    def unconstrained_variable(self) -> tf.Variable:\n        return self._unconstrained\n\n    @property\n    def transform(self) -> Optional[Transform]:\n        return self._transform\n\n    @transform.setter\n    def transform(self, new_transform: Optional[Transform]) -> None:\n        constrained_value = self.read_value()\n        self._transform = new_transform\n        self.assign(constrained_value)\n\n    @property\n    def trainable(self) -> bool:\n        """"""\n        `True` if this instance is trainable, else `False`.\n\n        This attribute cannot be set directly. Use :func:`gpflow.set_trainable`.\n        """"""\n        return self._unconstrained.trainable\n\n    @property\n    def initial_value(self) -> tf.Tensor:\n        return self._unconstrained.initial_value\n\n    def validate_unconstrained_value(self, value: TensorData, dtype: DType) -> tf.Tensor:\n        value = _cast_to_dtype(value, dtype)\n        unconstrained_value = _to_unconstrained(value, self.transform)\n        message = (\n            ""gpflow.Parameter: the value to be assigned is incompatible with this parameter\'s ""\n            ""transform (the corresponding unconstrained value has NaN or Inf) and hence cannot be ""\n            ""assigned.""\n        )\n        return tf.debugging.assert_all_finite(unconstrained_value, message=message)\n\n    def assign(\n        self,\n        value: TensorData,\n        use_locking: bool = False,\n        name: Optional[str] = None,\n        read_value: bool = True,\n    ) -> tf.Tensor:\n        """"""\n        Assigns constrained `value` to the unconstrained parameter\'s variable.\n        It passes constrained value through parameter\'s transform first.\n\n        Example:\n            ```\n            a = Parameter(2.0, transform=tfp.bijectors.Softplus())\n            b = Parameter(3.0)\n\n            a.assign(4.0)               # `a` parameter to `2.0` value.\n            a.assign(tf.constant(5.0))  # `a` parameter to `5.0` value.\n            a.assign(b)                 # `a` parameter to constrained value of `b`.\n            ```\n\n        :param value: Constrained tensor-like value.\n        :param use_locking: If `True`, use locking during the assignment.\n        :param name: The name of the operation to be created.\n        :param read_value: if True, will return something which evaluates to the new\n            value of the variable; if False will return the assign op.\n        """"""\n        unconstrained_value = self.validate_unconstrained_value(value, self.dtype)\n        return self._unconstrained.assign(\n            unconstrained_value, use_locking=use_locking, name=name, read_value=read_value\n        )\n\n    @property\n    def is_tensor_like(self) -> bool:\n        """"""\n        This method means that TensorFlow\'s `tensor_util.is_tensor` function\n        will return `True`\n        """"""\n        return True\n\n    @property\n    def name(self) -> str:\n        return self._unconstrained.name\n\n    @property\n    def initializer(self):  # type unknown\n        return self._unconstrained.initializer\n\n    @property\n    def device(self) -> Optional[str]:\n        return self._unconstrained.device\n\n    @property\n    def dtype(self) -> tf.DType:\n        return self._unconstrained.dtype\n\n    @property\n    def op(self) -> tf.Operation:\n        return self._unconstrained.op\n\n    @property\n    def shape(self) -> tf.TensorShape:\n        if self.transform is not None:\n            return self.transform.forward_event_shape(self._unconstrained.shape)\n        return self._unconstrained.shape\n\n    def numpy(self) -> np.ndarray:\n        return self.read_value().numpy()\n\n    def get_shape(self) -> tf.TensorShape:\n        return self.shape\n\n    def _should_act_as_resource_variable(self):  # type unknown\n        # needed so that Parameters are correctly identified by TensorFlow\'s\n        # is_resource_variable() in resource_variable_ops.py\n        pass  # only checked by TensorFlow using hasattr()\n\n    @property\n    def handle(self):  # type unknown\n        return self._unconstrained.handle\n\n    def __repr__(self) -> str:\n        unconstrained = self.unconstrained_variable\n        constrained = self.read_value()\n        if tf.executing_eagerly():\n            info = (\n                f""unconstrained-shape={unconstrained.shape} ""\n                f""unconstrained-value={unconstrained.numpy()} ""\n                f""constrained-shape={constrained.shape} ""\n                f""constrained-value={constrained.numpy()}""\n            )\n        else:\n            if unconstrained.shape == constrained.shape:\n                info = f""shape={constrained.shape}""\n            else:\n                info = (\n                    f""unconstrained-shape={unconstrained.shape} ""\n                    f""constrained-shape={constrained.shape}""\n                )\n\n        return f""<gpflow.Parameter {self.name!r} dtype={self.dtype.name} {info}>""\n\n    # Below\n    # TensorFlow copy-paste code to make variable-like object to work\n\n    @classmethod\n    def _OverloadAllOperators(cls):  # pylint: disable=invalid-name\n        """"""Register overloads for all operators.""""""\n        for operator in tf.Tensor.OVERLOADABLE_OPERATORS:\n            cls._OverloadOperator(operator)\n        # For slicing, bind getitem differently than a tensor (use SliceHelperVar\n        # instead)\n        # pylint: disable=protected-access\n        setattr(cls, ""__getitem__"", array_ops._SliceHelperVar)\n\n    @classmethod\n    def _OverloadOperator(cls, operator):  # pylint: disable=invalid-name\n        """"""Defer an operator overload to `ops.Tensor`.\n\n        We pull the operator out of ops.Tensor dynamically to avoid ordering issues.\n\n        Args:\n            operator: string. The operator name.\n        """"""\n        tensor_oper = getattr(tf.Tensor, operator)\n\n        def _run_op(a, *args, **kwargs):\n            # pylint: disable=protected-access\n            return tensor_oper(a.read_value(), *args, **kwargs)\n\n        functools.update_wrapper(_run_op, tensor_oper)\n        setattr(cls, operator, _run_op)\n\n    # NOTE(mrry): This enables the Variable\'s overloaded ""right"" binary\n    # operators to run when the left operand is an ndarray, because it\n    # accords the Variable class higher priority than an ndarray, or a\n    # numpy matrix.\n    # TODO(mrry): Convert this to using numpy\'s __numpy_ufunc__\n    # mechanism, which allows more control over how Variables interact\n    # with ndarrays.\n    __array_priority__ = 100\n\n\nParameter._OverloadAllOperators()\ntf.register_tensor_conversion_function(Parameter, lambda x, *args, **kwds: x.read_value())\n\n\ndef _cast_to_dtype(\n    value: TensorData, dtype: Optional[DType] = None\n) -> Union[tf.Tensor, tf.Variable]:\n    if dtype is None:\n        dtype = default_float()\n\n    if tf.is_tensor(value):\n        # NOTE(awav) TF2.2 resolves issue with cast.\n        # From TF2.2, `tf.cast` can be used alone instead of this auxiliary function.\n        # workaround for https://github.com/tensorflow/tensorflow/issues/35938\n        return tf.cast(value, dtype)\n    else:\n        return tf.convert_to_tensor(value, dtype=dtype)\n\n\ndef _to_constrained(value: TensorType, transform: Optional[Transform]) -> TensorType:\n    if transform is not None:\n        return transform.forward(value)\n    return value\n\n\ndef _to_unconstrained(value: TensorType, transform: Optional[Transform]) -> TensorType:\n    if transform is not None:\n        return transform.inverse(value)\n    return value\n'"
gpflow/ci_utils.py,0,"b'# Copyright 2017-2019 GPflow\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\nimport os\nfrom typing import Sequence, Type, Any, List, Iterable, TypeVar\n\n\ndef is_continuous_integration() -> bool:\n    """"""\n    Determines whether we are running on the Continuous Integration system for\n    notebook integration tests. This is used to speed up notebook integration\n    tests (built on every pull request commit) by capping all expensive loops\n    at a small number, rather than running until convergence. When building the\n    docs (indicated by the presence of the `DOCS` environment variable), we\n    need to run notebooks to completion, and this function returns `False`.\n    Whether we are running on CI is determined by the presence of the `CI`\n    environment variable.\n    """"""\n    if ""DOCS"" in os.environ:\n        return False\n\n    return ""CI"" in os.environ\n\n\ndef ci_niter(n: int, test_n: int = 2) -> int:\n    return test_n if is_continuous_integration() else n\n\n\ndef ci_range(n: int, test_n: int = 2) -> Sequence[int]:\n    return range(ci_niter(n, test_n))\n\n\nT = TypeVar(""T"")\n\n\ndef ci_list(lst: List[T], test_n: int = 2) -> List[T]:\n    return lst[:test_n] if is_continuous_integration() else lst\n\n\ndef subclasses(cls: Type[Any]) -> Iterable[Type[Any]]:\n    """"""\n    Generator that returns all (not just direct) subclasses of `cls`\n    """"""\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n        yield subclass\n'"
gpflow/kullback_leiblers.py,23,"b'# Copyright 2016 James Hensman, alexggmatthews\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nfrom .config import default_float, default_jitter\nfrom .covariances.kuus import Kuu\nfrom .inducing_variables import InducingVariables\nfrom .kernels import Kernel\nfrom .utilities import Dispatcher, to_default_float\n\nprior_kl = Dispatcher(""prior_kl"")\n\n\n@prior_kl.register(InducingVariables, Kernel, object, object)\ndef _(inducing_variable, kernel, q_mu, q_sqrt, whiten=False):\n    if whiten:\n        return gauss_kl(q_mu, q_sqrt, None)\n    else:\n        K = Kuu(inducing_variable, kernel, jitter=default_jitter())  # [P, M, M] or [M, M]\n        return gauss_kl(q_mu, q_sqrt, K)\n\n\ndef gauss_kl(q_mu, q_sqrt, K=None, *, K_cholesky=None):\n    """"""\n    Compute the KL divergence KL[q || p] between\n\n          q(x) = N(q_mu, q_sqrt^2)\n    and\n          p(x) = N(0, K)    if K is not None\n          p(x) = N(0, I)    if K is None\n\n    We assume L multiple independent distributions, given by the columns of\n    q_mu and the first or last dimension of q_sqrt. Returns the *sum* of the\n    divergences.\n\n    q_mu is a matrix ([M, L]), each column contains a mean.\n\n    q_sqrt can be a 3D tensor ([L, M, M]), each matrix within is a lower\n        triangular square-root matrix of the covariance of q.\n    q_sqrt can be a matrix ([M, L]), each column represents the diagonal of a\n        square-root matrix of the covariance of q.\n\n    K is the covariance of p (positive-definite matrix).  The K matrix can be\n    passed either directly as `K`, or as its Cholesky factor, `K_cholesky`.  In\n    either case, it can be a single matrix [M, M], in which case the sum of the\n    L KL divergences is computed by broadcasting, or L different covariances\n    [L, M, M].\n\n    Note: if no K matrix is given (both `K` and `K_cholesky` are None),\n    `gauss_kl` computes the KL divergence from p(x) = N(0, I) instead.\n    """"""\n\n    if (K is not None) and (K_cholesky is not None):\n        raise ValueError(\n            ""Ambiguous arguments: gauss_kl() must only be passed one of `K` or `K_cholesky`.""\n        )\n\n    is_white = (K is None) and (K_cholesky is None)\n    is_diag = len(q_sqrt.shape) == 2\n\n    shape_constraints = [\n        (q_mu, [""M"", ""L""]),\n        (q_sqrt, ([""M"", ""L""] if is_diag else [""L"", ""M"", ""M""])),\n    ]\n    if not is_white:\n        if K is not None:\n            shape_constraints.append((K, ([""L"", ""M"", ""M""] if len(K.shape) == 3 else [""M"", ""M""])))\n        else:\n            shape_constraints.append(\n                (K_cholesky, ([""L"", ""M"", ""M""] if len(K_cholesky.shape) == 3 else [""M"", ""M""]))\n            )\n    tf.debugging.assert_shapes(shape_constraints, message=""gauss_kl() arguments"")\n\n    M, L = tf.shape(q_mu)[0], tf.shape(q_mu)[1]\n\n    if is_white:\n        alpha = q_mu  # [M, L]\n    else:\n        if K is not None:\n            Lp = tf.linalg.cholesky(K)  # [L, M, M] or [M, M]\n        elif K_cholesky is not None:\n            Lp = K_cholesky  # [L, M, M] or [M, M]\n\n        is_batched = len(Lp.shape) == 3\n\n        q_mu = tf.transpose(q_mu)[:, :, None] if is_batched else q_mu  # [L, M, 1] or [M, L]\n        alpha = tf.linalg.triangular_solve(Lp, q_mu, lower=True)  # [L, M, 1] or [M, L]\n\n    if is_diag:\n        Lq = Lq_diag = q_sqrt\n        Lq_full = tf.linalg.diag(tf.transpose(q_sqrt))  # [L, M, M]\n    else:\n        Lq = Lq_full = tf.linalg.band_part(q_sqrt, -1, 0)  # force lower triangle # [L, M, M]\n        Lq_diag = tf.linalg.diag_part(Lq)  # [M, L]\n\n    # Mahalanobis term: \xce\xbcq\xe1\xb5\x80 \xce\xa3p\xe2\x81\xbb\xc2\xb9 \xce\xbcq\n    mahalanobis = tf.reduce_sum(tf.square(alpha))\n\n    # Constant term: - L * M\n    constant = -to_default_float(tf.size(q_mu, out_type=tf.int64))\n\n    # Log-determinant of the covariance of q(x):\n    logdet_qcov = tf.reduce_sum(tf.math.log(tf.square(Lq_diag)))\n\n    # Trace term: tr(\xce\xa3p\xe2\x81\xbb\xc2\xb9 \xce\xa3q)\n    if is_white:\n        trace = tf.reduce_sum(tf.square(Lq))\n    else:\n        if is_diag and not is_batched:\n            # K is [M, M] and q_sqrt is [M, L]: fast specialisation\n            LpT = tf.transpose(Lp)  # [M, M]\n            Lp_inv = tf.linalg.triangular_solve(\n                Lp, tf.eye(M, dtype=default_float()), lower=True\n            )  # [M, M]\n            K_inv = tf.linalg.diag_part(tf.linalg.triangular_solve(LpT, Lp_inv, lower=False))[\n                :, None\n            ]  # [M, M] -> [M, 1]\n            trace = tf.reduce_sum(K_inv * tf.square(q_sqrt))\n        else:\n            # TODO: broadcast instead of tile when tf allows -- tf2.1 segfaults\n            # (https://github.com/tensorflow/tensorflow/issues/37584).\n            # See # https://github.com/GPflow/GPflow/issues/1321\n            Lp_full = Lp if is_batched else tf.tile(tf.expand_dims(Lp, 0), [L, 1, 1])\n            LpiLq = tf.linalg.triangular_solve(Lp_full, Lq_full, lower=True)\n            trace = tf.reduce_sum(tf.square(LpiLq))\n\n    twoKL = mahalanobis + constant - logdet_qcov + trace\n\n    # Log-determinant of the covariance of p(x):\n    if not is_white:\n        log_sqdiag_Lp = tf.math.log(tf.square(tf.linalg.diag_part(Lp)))\n        sum_log_sqdiag_Lp = tf.reduce_sum(log_sqdiag_Lp)\n        # If K is [L, M, M], num_latent_gps is no longer implicit, no need to multiply the single kernel logdet\n        scale = 1.0 if is_batched else to_default_float(L)\n        twoKL += scale * sum_log_sqdiag_Lp\n\n    tf.debugging.assert_shapes([(twoKL, ())], message=""gauss_kl() return value"")  # returns scalar\n    return 0.5 * twoKL\n'"
gpflow/logdensities.py,25,"b'# Copyright 2016 James Hensman, alexggmatthews\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\nfrom .config import default_float\nfrom .utilities import to_default_float\n\n\ndef gaussian(x, mu, var):\n    return -0.5 * (np.log(2 * np.pi) + tf.math.log(var) + tf.square(mu - x) / var)\n\n\ndef lognormal(x, mu, var):\n    lnx = tf.math.log(x)\n    return gaussian(lnx, mu, var) - lnx\n\n\ndef bernoulli(x, p):\n    return tf.math.log(tf.where(tf.equal(x, 1), p, 1 - p))\n\n\ndef poisson(x, lam):\n    return x * tf.math.log(lam) - lam - tf.math.lgamma(x + 1.0)\n\n\ndef exponential(x, scale):\n    return -x / scale - tf.math.log(scale)\n\n\ndef gamma(x, shape, scale):\n    return (\n        -shape * tf.math.log(scale)\n        - tf.math.lgamma(shape)\n        + (shape - 1.0) * tf.math.log(x)\n        - x / scale\n    )\n\n\ndef student_t(x, mean, scale, df):\n    df = to_default_float(df)\n    const = (\n        tf.math.lgamma((df + 1.0) * 0.5)\n        - tf.math.lgamma(df * 0.5)\n        - 0.5 * (tf.math.log(tf.square(scale)) + tf.math.log(df) + np.log(np.pi))\n    )\n    return const - 0.5 * (df + 1.0) * tf.math.log(\n        1.0 + (1.0 / df) * (tf.square((x - mean) / scale))\n    )\n\n\ndef beta(x, alpha, beta):\n    # need to clip x, since log of 0 is nan...\n    x = tf.clip_by_value(x, 1e-6, 1 - 1e-6)\n    return (\n        (alpha - 1.0) * tf.math.log(x)\n        + (beta - 1.0) * tf.math.log(1.0 - x)\n        + tf.math.lgamma(alpha + beta)\n        - tf.math.lgamma(alpha)\n        - tf.math.lgamma(beta)\n    )\n\n\ndef laplace(x, mu, sigma):\n    return -tf.abs(mu - x) / sigma - tf.math.log(2.0 * sigma)\n\n\ndef multivariate_normal(x, mu, L):\n    """"""\n    Computes the log-density of a multivariate normal.\n    :param x  : Dx1 or DxN sample(s) for which we want the density\n    :param mu : Dx1 or DxN mean(s) of the normal distribution\n    :param L  : DxD Cholesky decomposition of the covariance matrix\n    :return p : (1,) or (N,) vector of log densities for each of the N x\'s and/or mu\'s\n\n    x and mu are either vectors or matrices. If both are vectors (N,1):\n    p[0] = log pdf(x) where x ~ N(mu, LL^T)\n    If at least one is a matrix, we assume independence over the *columns*:\n    the number of rows must match the size of L. Broadcasting behaviour:\n    p[n] = log pdf of:\n    x[n] ~ N(mu, LL^T) or x ~ N(mu[n], LL^T) or x[n] ~ N(mu[n], LL^T)\n    """"""\n\n    d = x - mu\n    alpha = tf.linalg.triangular_solve(L, d, lower=True)\n    num_dims = tf.cast(tf.shape(d)[0], L.dtype)\n    p = -0.5 * tf.reduce_sum(tf.square(alpha), 0)\n    p -= 0.5 * num_dims * np.log(2 * np.pi)\n    p -= tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)))\n\n    shape_constraints = [\n        (d, [""D"", ""N""]),\n        (L, [""D"", ""D""]),\n        (p, [""N""]),\n    ]\n    tf.debugging.assert_shapes(shape_constraints, message=""multivariate_normal()"")\n\n    return p\n'"
gpflow/mean_functions.py,15,"b'# Copyright 2016 James Hensman, alexggmatthews, PabloLeon, Valentine Svensson\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThroughout GPflow, by default, latent functions being modelled with Gaussian\nprocesses are assumed to have zero mean, f ~ GP(0, k(x,x\')).\n\nIn some cases we may wish to model only the deviation from a fixed function\nwith a Gaussian process.  For flexibility this fixed function could be both\ninput dependent and parameterised function, \xce\xbc(x; \xce\xb8),\nwith some unknown parameters \xce\xb8, resulting in f ~ GP(\xce\xbc(x;\xce\xb8), k(x,x\')).\n\nThe GPflow :class:`MeanFunction <gpflow.mean_functions.MeanFunction>` class\nallows this to be done whilst additionally learning parameters of the\nparametric function.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .base import Module, Parameter\nfrom .config import default_float\n\n\nclass MeanFunction(Module):\n    """"""\n    The base mean function class.\n    To implement a mean function, write the __call__ method. This takes a\n    tensor X and returns a tensor m(X). In accordance with the GPflow\n    standard, each row of X represents one datum, and each row of Y is computed\n    independently for each row of X.\n\n    MeanFunction classes can have parameters, see the Linear class for an\n    example.\n    """"""\n\n    def __call__(self, X):\n        raise NotImplementedError(""Implement the __call__ method for this mean function"")\n\n    def __add__(self, other):\n        return Additive(self, other)\n\n    def __mul__(self, other):\n        return Product(self, other)\n\n\nclass Linear(MeanFunction):\n    """"""\n    y_i = A x_i + b\n    """"""\n\n    def __init__(self, A=None, b=None):\n        """"""\n        A is a matrix which maps each element of X to Y, b is an additive\n        constant.\n\n        If X has N rows and D columns, and Y is intended to have Q columns,\n        then A must be [D, Q], b must be a vector of length Q.\n        """"""\n        MeanFunction.__init__(self)\n        A = np.ones((1, 1), dtype=default_float()) if A is None else A\n        b = np.zeros(1, dtype=default_float()) if b is None else b\n        self.A = Parameter(np.atleast_2d(A))\n        self.b = Parameter(b)\n\n    def __call__(self, X):\n        return tf.tensordot(X, self.A, [[-1], [0]]) + self.b\n\n\nclass Identity(Linear):\n    """"""\n    y_i = x_i\n    """"""\n\n    def __init__(self, input_dim=None):\n        Linear.__init__(self)\n        self.input_dim = input_dim\n\n    def __call__(self, X):\n        return X\n\n    @property\n    def A(self):\n        if self.input_dim is None:\n            raise ValueError(\n                ""An input_dim needs to be specified when using the ""\n                ""`Identity` mean function in combination with expectations.""\n            )\n        return tf.eye(self.input_dim, dtype=default_float())\n\n    @property\n    def b(self):\n        if self.input_dim is None:\n            raise ValueError(\n                ""An input_dim needs to be specified when using the ""\n                ""`Identity` mean function in combination with expectations.""\n            )\n\n        return tf.zeros(self.input_dim, dtype=default_float())\n\n    @A.setter\n    def A(self, A):\n        pass\n\n    @b.setter\n    def b(self, b):\n        pass\n\n\nclass Constant(MeanFunction):\n    def __init__(self, c=None):\n        super().__init__()\n        c = np.zeros(1) if c is None else c\n        self.c = Parameter(c)\n\n    def __call__(self, X):\n        shape = [tf.shape(X)[0], 1]\n        return tf.tile(tf.reshape(self.c, (1, -1)), shape)\n\n\nclass Zero(Constant):\n    def __init__(self, output_dim=1):\n        Constant.__init__(self)\n        self.output_dim = output_dim\n        del self.c\n\n    def __call__(self, X):\n        return tf.zeros((tf.shape(X)[0], self.output_dim), dtype=X.dtype)\n\n\nclass SwitchedMeanFunction(MeanFunction):\n    """"""\n    This class enables to use different (independent) mean_functions respective\n    to the data \'label\'.\n    We assume the \'label\' is stored in the extra column of X.\n    """"""\n\n    def __init__(self, meanfunction_list):\n        super().__init__()\n        for m in meanfunction_list:\n            assert isinstance(m, MeanFunction)\n        self.meanfunctions = meanfunction_list\n\n    def __call__(self, X):\n        ind = tf.gather(tf.transpose(X), tf.shape(X)[1] - 1)  # ind = X[:,-1]\n        ind = tf.cast(ind, tf.int32)\n        X = tf.transpose(\n            tf.gather(tf.transpose(X), tf.range(0, tf.shape(X)[1] - 1))\n        )  # X = X[:,:-1]\n\n        # split up X into chunks corresponding to the relevant likelihoods\n        x_list = tf.dynamic_partition(X, ind, len(self.meanfunctions))\n        # apply the likelihood-function to each section of the data\n        results = [m(x) for x, m in zip(x_list, self.meanfunctions)]\n        # stitch the results back together\n        partitions = tf.dynamic_partition(tf.range(0, tf.size(ind)), ind, len(self.meanfunctions))\n        return tf.dynamic_stitch(partitions, results)\n\n\nclass Additive(MeanFunction):\n    def __init__(self, first_part, second_part):\n        MeanFunction.__init__(self)\n        self.add_1 = first_part\n        self.add_2 = second_part\n\n    def __call__(self, X):\n        return tf.add(self.add_1(X), self.add_2(X))\n\n\nclass Product(MeanFunction):\n    def __init__(self, first_part, second_part):\n        MeanFunction.__init__(self)\n\n        self.prod_1 = first_part\n        self.prod_2 = second_part\n\n    def __call__(self, X):\n        return tf.multiply(self.prod_1(X), self.prod_2(X))\n'"
gpflow/probability_distributions.py,0,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Eventually, it would be nice to not have to have our own classes for\n# proability distributions. The TensorFlow ""distributions"" framework would\n# be a good replacement.\nfrom .base import TensorType\n\n\nclass ProbabilityDistribution:\n    """"""\n    This is the base class for a probability distributions,\n    over which we take the expectations in the expectations framework.\n    """"""\n\n\nclass Gaussian(ProbabilityDistribution):\n    def __init__(self, mu: TensorType, cov: TensorType):\n        self.mu = mu  # [N, D]\n        self.cov = cov  # [N, D, D]\n\n\nclass DiagonalGaussian(ProbabilityDistribution):\n    def __init__(self, mu: TensorType, cov: TensorType):\n        self.mu = mu  # [N, D]\n        self.cov = cov  # [N, D]\n\n\nclass MarkovGaussian(ProbabilityDistribution):\n    """"""\n    Gaussian distribution with Markov structure.\n    Only covariances and covariances between t and t+1 need to be\n    parameterised. We use the solution proposed by Carl Rasmussen, i.e. to\n    represent\n    Var[x_t] = cov[x_t, :, :] * cov[x_t, :, :].T\n    Cov[x_t, x_{t+1}] = cov[t, :, :] * cov[t+1, :, :]\n    """"""\n\n    def __init__(self, mu: TensorType, cov: TensorType):\n        self.mu = mu  # N+[1, D]\n        self.cov = cov  # 2 x (N+1)[, D, D]\n'"
gpflow/quadrature.py,28,"b'# Copyright 2017-2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nfrom collections.abc import Iterable\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .config import default_float\nfrom .utilities import to_default_float\n\n\ndef hermgauss(n: int):\n    x, w = np.polynomial.hermite.hermgauss(n)\n    x, w = x.astype(default_float()), w.astype(default_float())\n    return x, w\n\n\ndef mvhermgauss(H: int, D: int):\n    """"""\n    Return the evaluation locations \'xn\', and weights \'wn\' for a multivariate\n    Gauss-Hermite quadrature.\n\n    The outputs can be used to approximate the following type of integral:\n    int exp(-x)*f(x) dx ~ sum_i w[i,:]*f(x[i,:])\n\n    :param H: Number of Gauss-Hermite evaluation points.\n    :param D: Number of input dimensions. Needs to be known at call-time.\n    :return: eval_locations \'x\' (H**DxD), weights \'w\' (H**D)\n    """"""\n    gh_x, gh_w = hermgauss(H)\n    x = np.array(list(itertools.product(*(gh_x,) * D)))  # H**DxD\n    w = np.prod(np.array(list(itertools.product(*(gh_w,) * D))), 1)  # H**D\n    return x, w\n\n\ndef mvnquad(func, means, covs, H: int, Din: int = None, Dout=None):\n    """"""\n    Computes N Gaussian expectation integrals of a single function \'f\'\n    using Gauss-Hermite quadrature.\n    :param f: integrand function. Takes one input of shape ?xD.\n    :param means: NxD\n    :param covs: NxDxD\n    :param H: Number of Gauss-Hermite evaluation points.\n    :param Din: Number of input dimensions. Needs to be known at call-time.\n    :param Dout: Number of output dimensions. Defaults to (). Dout is assumed\n    to leave out the item index, i.e. f actually maps (?xD)->(?x*Dout).\n    :return: quadratures (N,*Dout)\n    """"""\n    # Figure out input shape information\n    if Din is None:\n        Din = means.shape[1]\n\n    if Din is None:\n        raise ValueError(\n            ""If `Din` is passed as `None`, `means` must have a known shape. ""\n            ""Running mvnquad in `autoflow` without specifying `Din` and `Dout` ""\n            ""is problematic. Consider using your own session.""\n        )  # pragma: no cover\n\n    xn, wn = mvhermgauss(H, Din)\n    N = means.shape[0]\n\n    # transform points based on Gaussian parameters\n    cholXcov = tf.linalg.cholesky(covs)  # NxDxD\n    Xt = tf.linalg.matmul(\n        cholXcov, tf.tile(xn[None, :, :], (N, 1, 1)), transpose_b=True\n    )  # NxDxH**D\n    X = 2.0 ** 0.5 * Xt + tf.expand_dims(means, 2)  # NxDxH**D\n    Xr = tf.reshape(tf.transpose(X, [2, 0, 1]), (-1, Din))  # (H**D*N)xD\n\n    # perform quadrature\n    fevals = func(Xr)\n    if Dout is None:\n        Dout = tuple((d if type(d) is int else d.value) for d in fevals.shape[1:])\n\n    if any([d is None for d in Dout]):\n        raise ValueError(\n            ""If `Dout` is passed as `None`, the output of `func` must have known ""\n            ""shape. Running mvnquad in `autoflow` without specifying `Din` and `Dout` ""\n            ""is problematic. Consider using your own session.""\n        )  # pragma: no cover\n    fX = tf.reshape(fevals, (H ** Din, N,) + Dout)\n    wr = np.reshape(wn * np.pi ** (-Din * 0.5), (-1,) + (1,) * (1 + len(Dout)))\n    return tf.reduce_sum(fX * wr, 0)\n\n\ndef ndiagquad(funcs, H: int, Fmu, Fvar, logspace: bool = False, **Ys):\n    """"""\n    Computes N Gaussian expectation integrals of one or more functions\n    using Gauss-Hermite quadrature. The Gaussians must be independent.\n\n    The means and variances of the Gaussians are specified by Fmu and Fvar.\n    The N-integrals are assumed to be taken wrt the last dimensions of Fmu, Fvar.\n\n    :param funcs: the integrand(s):\n        Callable or Iterable of Callables that operates elementwise\n    :param H: number of Gauss-Hermite quadrature points\n    :param Fmu: array/tensor or `Din`-tuple/list thereof\n    :param Fvar: array/tensor or `Din`-tuple/list thereof\n    :param logspace: if True, funcs are the log-integrands and this calculates\n        the log-expectation of exp(funcs)\n    :param **Ys: arrays/tensors; deterministic arguments to be passed by name\n\n    Fmu, Fvar, Ys should all have same shape, with overall size `N`\n    :return: shape is the same as that of the first Fmu\n    """"""\n    if isinstance(Fmu, (tuple, list)):\n        Din = len(Fmu)\n\n        def unify(f_list):\n            """"""Stack a list of means/vars into a full block.""""""\n            return tf.reshape(\n                tensor=tf.concat([tf.reshape(f, shape=(-1, 1)) for f in f_list], axis=1),\n                shape=(-1, 1, Din),\n            )\n\n        shape = tf.shape(Fmu[0])\n        Fmu, Fvar = map(unify, [Fmu, Fvar])  # both [N, 1, Din]\n    else:\n        Din = 1\n        shape = tf.shape(Fmu)\n        Fmu, Fvar = [tf.reshape(f, (-1, 1, 1)) for f in [Fmu, Fvar]]\n\n    xn, wn = mvhermgauss(H, Din)\n    # xn: H**Din x Din, wn: H**Din\n\n    gh_x = xn.reshape(1, -1, Din)  # [1, H]**Din x Din\n    Xall = gh_x * tf.sqrt(2.0 * Fvar) + Fmu  # [N, H]**Din x Din\n    Xs = [Xall[:, :, i] for i in range(Din)]  # [N, H]**Din  each\n\n    gh_w = wn * np.pi ** (-0.5 * Din)  # H**Din x 1\n\n    for name, Y in Ys.items():\n        Y = tf.reshape(Y, (-1, 1))\n        Y = tf.tile(Y, [1, H ** Din])  # broadcast Y to match X\n        # without the tiling, some calls such as tf.where() (in bernoulli) fail\n        Ys[name] = Y  # now [N, H]**Din\n\n    def eval_func(f):\n        feval = f(*Xs, **Ys)  # f should be elementwise: return shape [N, H]**Din\n        if logspace:\n            log_gh_w = np.log(gh_w.reshape(1, -1))\n            result = tf.reduce_logsumexp(feval + log_gh_w, axis=1)\n        else:\n            result = tf.linalg.matmul(feval, gh_w.reshape(-1, 1))\n        return tf.reshape(result, shape)\n\n    if isinstance(funcs, Iterable):\n        return [eval_func(f) for f in funcs]\n\n    return eval_func(funcs)\n\n\ndef ndiag_mc(funcs, S: int, Fmu, Fvar, logspace: bool = False, epsilon=None, **Ys):\n    """"""\n    Computes N Gaussian expectation integrals of one or more functions\n    using Monte Carlo samples. The Gaussians must be independent.\n\n    :param funcs: the integrand(s):\n        Callable or Iterable of Callables that operates elementwise\n    :param S: number of Monte Carlo sampling points\n    :param Fmu: array/tensor\n    :param Fvar: array/tensor\n    :param logspace: if True, funcs are the log-integrands and this calculates\n        the log-expectation of exp(funcs)\n    :param **Ys: arrays/tensors; deterministic arguments to be passed by name\n\n    Fmu, Fvar, Ys should all have same shape, with overall size `N`\n    :return: shape is the same as that of the first Fmu\n    """"""\n    N, D = Fmu.shape[0], Fvar.shape[1]\n\n    if epsilon is None:\n        epsilon = tf.random.normal((S, N, D), dtype=default_float())\n\n    mc_x = Fmu[None, :, :] + tf.sqrt(Fvar[None, :, :]) * epsilon\n    mc_Xr = tf.reshape(mc_x, (S * N, D))\n\n    for name, Y in Ys.items():\n        D_out = Y.shape[1]\n        # we can\'t rely on broadcasting and need tiling\n        mc_Yr = tf.tile(Y[None, ...], [S, 1, 1])  # [S, N, D]_out\n        Ys[name] = tf.reshape(mc_Yr, (S * N, D_out))  # S * [N, _]out\n\n    def eval_func(func):\n        feval = func(mc_Xr, **Ys)\n        feval = tf.reshape(feval, (S, N, -1))\n        if logspace:\n            log_S = tf.math.log(to_default_float(S))\n            return tf.reduce_logsumexp(feval, axis=0) - log_S  # [N, D]\n        else:\n            return tf.reduce_mean(feval, axis=0)\n\n    if isinstance(funcs, Iterable):\n        return [eval_func(f) for f in funcs]\n    else:\n        return eval_func(funcs)\n'"
gpflow/versions.py,0,"b'import pkg_resources\n\ntry:\n    __version__ = str(pkg_resources.get_distribution(""gpflow"").parsed_version)\nexcept pkg_resources.DistributionNotFound:\n    __version__ = ""develop""\n'"
tests/__init__.py,0,"b'import warnings\n\nimport numpy as np\nimport tensorflow as tf\n\nimport gpflow\n\nwarnings.filterwarnings(""ignore"")\ngpflow.config.set_default_float(np.float64)\n'"
doc/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# GPflow documentation build configuration file, created by\n# sphinx-quickstart on Mon Jul 25 12:37:37 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport types\n\n# When we install GPflow on readthedocs we omit installing Tensorflow\n# and Tensorflow Probability. We make up for it by mocking them here.\nautodoc_mock_imports = [""tensorflow"", ""tensorflow_probability""]\n\n\n# on_rtd is whether we are on readthedocs.org, this line of code grabbed from docs.readthedocs.org\non_rtd = os.environ.get(""READTHEDOCS"", None) == ""True""\n\nif not on_rtd:  # only import and set the theme if we\'re building docs locally\n    import sphinx_rtd_theme\n\n    html_theme = ""sphinx_rtd_theme""\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# otherwise, readthedocs.org uses their theme by default, so no need to specify it\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    # builtin extansions\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""numpydoc"",\n    ""nbsphinx"",\n    ""sphinx_autodoc_typehints"",\n    ""IPython.sphinxext.ipython_console_highlighting"",\n]\n\nset_type_checking_flag = True\ntypehints_fully_qualified = False\nalways_document_param_types = True\n# autoclass_content = \'both\'\n\n# numpydoc_show_class_members = True\nnumpydoc_class_members_toctree = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = "".rst""\n\n# The encoding of source files.\n#\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# General information about the project.\nproject = ""GPflow""\ncopyright = ""2016-2020, James Hensman, Alexander G. de G. Matthews and the GPflow contributors""\nauthor = ""James Hensman and Alexander G. de G. Matthews and others""\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = ""2.0.4""\n# The full version, including alpha/beta/rc tags.\nrelease = ""2.0.4""\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#\n# today = \'\'\n#\n# Else, today_fmt is used as the format for a strftime call.\n#\n# today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = ["".ipynb_checkpoints""]\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_css_files = [\n    ""green_theme.css"",\n]\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#\n# html_title = u\'GPflow v0.2.0\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#\nhtml_logo = ""_static/GPflow_Logos_White.png""\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#\n# html_favicon = None\nhtml_theme_options = {""logo_only"": True}\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#\n# html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n#\n# html_domain_indices = True\n\n# If false, no index is generated.\n#\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""GPflowdoc""\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc,\n        ""gpflow.tex"",\n        ""GPflow Documentation"",\n        ""James Hensman and Alexander G. de G. Matthews"",\n        ""manual"",\n    ),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n#\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#\n# latex_appendices = []\n\n# It false, will not define \\strong, \\code, \titleref, \\crossref ... but only\n# \\sphinxstrong, ..., \\sphinxtitleref, ... To help avoid clash with user added\n# packages.\n#\n# latex_keep_old_macro_names = True\n\n# If false, no module index is generated.\n#\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""GPflow"", ""GPflow Documentation"", [author], 1)]\n\n# If true, show URL addresses after external links.\n#\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""GPflow"",\n        ""GPflow Documentation"",\n        author,\n        ""GPflow"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    ),\n]\n\n# Documents to append as an appendix to all manuals.\n#\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n#\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#\n# texinfo_no_detailmenu = False\n\n\ndef setup(app):\n    """""" Entry point to sphinx build customisation. """"""\n    app.connect(""autodoc-skip-member"", autodoc_skip_member_callback)\n\n\ndef autodoc_skip_member_callback(app, what, name, obj, skip, options):\n    """"""\n    Only skip special methods and functions, including `__init__`, if they have no docstring.\n    """"""\n    if isinstance(obj, (types.FunctionType, types.MethodType)):\n        if getattr(obj, ""__doc__"", None) is not None:\n            return False  # never skip methods containing a docstring\n\n    return skip\n'"
doc/source/generate_module_rst.py,0,"b'# Copyright 2019 GPflow Authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Script to autogenerate .rst files for autodocumentation of classes and modules in GPflow.\nTo be run by the CI system to update docs.\n""""""\nimport inspect\nimport os\n\nfrom datetime import datetime\nfrom typing import Any, Callable, List, Set, Tuple\nfrom types import ModuleType\n\nimport gpflow\n\nRST_PATH = ""source/""\nRST_LEVEL_SYMBOLS = [""="", ""-"", ""~"", \'""\', ""\'"", ""^""]\n\nSPHINX_CLASS_STRING = """"""\n{object_name}\n{level}\n\n.. autoclass:: {object_name}\n   :show-inheritance:\n   :members:\n""""""\n\nSPHINX_MULTIDISPATCH_STRING = """"""\n{object_name}\n{level}\n\nThis function uses multiple dispatch, which will depend on the type of argument passed in:\n\n{content}\n""""""\n\nSPHINX_MULTIDISPATCH_COMPONENT_STRING = """"""\n.. code-block:: python\n\n    {dispatch_name}( {args} )\n    # dispatch to -> {true_name}(...)\n\n\n.. autofunction:: {true_name}\n""""""\n\nSPHINX_FUNC_STRING = """"""\n{object_name}\n{level}\n\n.. autofunction:: {object_name}\n""""""\n\nSPHINX_INCLUDE_MODULE_STRING = """"""\n{name}\n{level}\n.. automodule:: {name}\n.. toctree::\n   :maxdepth: 1\n\n   {module}/index\n""""""\n\nSPHINX_FILE_STRING = """"""{headerline}\n{title}\n{headerline}\n\n.. THIS IS AN AUTOGENERATED RST FILE\n.. GENERATED BY `generate_rst.py`\n.. DATE: {date}\n\n\n{content}\n""""""\n\n\nIGNORE_MODULES = {\n    ""gpflow.covariances.dispatch"",\n    ""gpflow.conditionals.dispatch"",\n    ""gpflow.expectations.dispatch"",\n    ""gpflow.kullback_leiblers.dispatch"",\n    ""gpflow.versions"",\n}\n\nDATE_STRING = datetime.strftime(datetime.now(), ""%d/%m/%y"")\n\n\ndef set_global_path(path):\n    global RST_PATH\n    RST_PATH = path\n\n\ndef is_documentable_module(m: Any) -> bool:\n    """"""Return `True` if m is module to be documented automatically, `False` otherwise.\n    """"""\n    return inspect.ismodule(m) and ""gpflow"" in m.__name__ and m.__name__ not in IGNORE_MODULES\n\n\ndef is_documentable_component(m: Any) -> bool:\n    """"""Return `True` if a function or class to be documented automatically, `False` otherwise.\n    """"""\n    if inspect.isfunction(m):\n        return ""gpflow"" in m.__module__ and m.__module__ not in IGNORE_MODULES\n    elif inspect.isclass(m):\n        return ""gpflow"" in m.__module__ and m.__module__ not in IGNORE_MODULES\n    elif type(m).__name__ == ""Dispatcher"":\n        return True\n\n    return False\n\n\ndef is_documentable(m: Any) -> bool:\n    """"""Return `True` if a function, class, or module to be documented automatically, else `False`.\n    """"""\n    return is_documentable_component(m) or is_documentable_module(m)\n\n\ndef get_component_rst_string(module: ModuleType, component: Callable, level: int) -> str:\n    """"""Get a rst string, to autogenerate documentation for a component (class or function)\n\n    :param module: the module containing the component\n    :param component: the component (class or function)\n    :param level: the level in nested directory structure\n    """"""\n    object_name = f""{module.__name__}.{component.__name__}""\n\n    rst_documentation = """"\n    level_underline = RST_LEVEL_SYMBOLS[level] * len(object_name)\n    if inspect.isclass(component):\n        rst_documentation = SPHINX_CLASS_STRING.format(\n            object_name=object_name, var=component.__name__, level=level_underline\n        )\n    elif inspect.isfunction(component):\n        rst_documentation = SPHINX_FUNC_STRING.format(\n            object_name=object_name, var=component.__name__, level=level_underline\n        )\n    elif type(component).__name__ == ""Dispatcher"":\n        rst_documentation = get_multidispatch_string(component, module, level)\n\n    return rst_documentation\n\n\ndef get_multidispatch_string(md_component: Callable, module: ModuleType, level: int) -> str:\n    """"""Get the string for a multiple dispatch component. This involves iterating through the\n    possible functions and arguments and creating strings for each of these items.\n\n    :param md_component: the multidispatch component (wrapped around functions)\n    :param module: the module containing the component\n    :param level: the level in nested directory structure\n    """"""\n    content_list = []\n    dispatch_name = f""{module.__name__}.{md_component.name}""\n    level_underline = RST_LEVEL_SYMBOLS[level] * len(dispatch_name)\n    for args, fname in md_component.funcs.items():\n\n        arg_names = "", "".join([a.__name__ for a in args])\n        alias_name = f""{fname.__module__}.{fname.__name__}""\n\n        string = SPHINX_MULTIDISPATCH_COMPONENT_STRING.format(\n            dispatch_name=dispatch_name, args=arg_names, true_name=alias_name\n        )\n        content_list.append(string)\n    content = ""\\n"".join(content_list)\n    return SPHINX_MULTIDISPATCH_STRING.format(\n        object_name=dispatch_name, level=level_underline, content=content\n    )\n\n\ndef get_module_rst_string(module: ModuleType, level: int) -> str:\n    """"""Get an rst string, used to autogenerate documentation for a module\n\n    :param module: the module containing the component\n    :param level: the level in nested directory structure\n    """"""\n    level_underline = RST_LEVEL_SYMBOLS[level] * len(module.__name__)\n    return SPHINX_INCLUDE_MODULE_STRING.format(\n        name=module.__name__, module=module.__name__.split(""."")[-1], level=level_underline\n    )\n\n\ndef get_public_attributes(node: Any) -> Any:\n    """"""Get the public attributes (\'children\') of the current node, accessible from this node.\n    """"""\n    return [getattr(node, a) for a in dir(node) if not a.startswith(""_"")]\n\n\ndef write_to_rst_file(node_name: str, rst_content: List[str]) -> None:\n    """"""Write rst_content to a file, for a certain node.\n\n    :param node_name: name of the node to write to file\n    :param rst_content: List of rst strings to write to file\n    """"""\n    path = f""{RST_PATH}/{node_name.replace(\'.\', \'/\')}""\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    level_underline = RST_LEVEL_SYMBOLS[0] * len(node_name)\n    rst_file = SPHINX_FILE_STRING.format(\n        title=node_name, content="""".join(rst_content), date=DATE_STRING, headerline=level_underline,\n    )\n\n    path_to_file = path + ""/index.rst""\n    with open(path_to_file, ""w"") as f:\n        f.write(rst_file)\n\n\ndef do_visit_module(module: ModuleType, enqueued_items: Set[int]) -> bool:\n    """"""Decide whether to document this module or not, by checking its attributes and deciding\n    if there is something worth documenting there\n\n    :param module: module to document\n    :param enqueued_items: enqueue the items\n    """"""\n    for child in get_public_attributes(module):\n        if is_documentable_module(child) and id(child) not in enqueued_items:\n            # There is a module we have not visited\n            return True\n        elif (\n            is_documentable_component(child)\n            and id(child) not in enqueued_items\n            and (""__init__"" in child.__module__ or module.__name__ in child.__module__)\n        ):\n            # There is a class or function (or alias of them) we have not visited\n            return True\n    return False\n\n\ndef traverse_module_bfs(queue: List[Tuple[Any, int]], enqueued_items: Set[int]):\n    """"""\n    We will traverse the module in the queue to generate .rst files, that will be used by sphinx.\n    We do this to avoid having to add new classes or modules to the documentation.\n    We traverse the module breadth-first, and check `id` of modules to prevent double documentation\n    of same items. We traverse breadth first so that when an alias has been created:\n        ie - gpflow.kernels.Matern52 == gpflow.kernels.stationaries.Matern52\n    we take the path closest to the root (in this case: goflow.kernels.Matern52)\n\n    :param queue: The queue which contains the module and the starting depth. Usually: [(gpflow, 0)]\n    :param enqueued_items: The set tracks objects already in the queue, with `id`: set([id(gpflow)])\n    :return: None\n    """"""\n    while queue:\n        node, level = queue.pop(0)  # currently using a list as a queue (not great)\n\n        if not hasattr(node, ""__name__""):\n            continue\n\n        if is_documentable_module(node):\n\n            rst_components, rst_modules = [], []\n            for child in get_public_attributes(node):\n\n                if id(child) in enqueued_items:\n                    continue\n\n                if is_documentable_component(child):\n                    rst_components.append(get_component_rst_string(node, child, level))\n                    enqueued_items.add(id(child))\n\n                elif is_documentable_module(child):\n                    if do_visit_module(child, enqueued_items):\n                        rst_modules.append(get_module_rst_string(child, level))\n                        queue.append((child, level + 1))\n                        enqueued_items.add(id(child))\n\n            rst_content = ""\\n"".join(rst_components + rst_modules)\n            if rst_content:\n                write_to_rst_file(node.__name__, rst_content)\n\n\nif __name__ == ""__main__"":\n    set_global_path(os.path.dirname(os.path.realpath(__file__)))\n    traverse_module_bfs([(gpflow, 0)], set([id(gpflow)]))\n'"
gpflow/conditionals/__init__.py,0,"b'# noqa: F401\n\nfrom .dispatch import conditional, sample_conditional\n\nfrom . import conditionals\nfrom . import sample_conditionals\n\nfrom . import multioutput\n\n\nfrom .util import base_conditional\n\nfrom .uncertain_conditionals import uncertain_conditional\n'"
gpflow/conditionals/conditionals.py,6,"b'# noqa: F811\n\nimport tensorflow as tf\n\nfrom ..covariances import Kuf, Kuu\nfrom ..inducing_variables import InducingVariables\nfrom ..kernels import Kernel\nfrom ..utilities.ops import eye\nfrom ..config import default_jitter\nfrom .dispatch import conditional\nfrom .util import base_conditional, expand_independent_outputs\n\n\n@conditional.register(object, InducingVariables, Kernel, object)\ndef _conditional(\n    Xnew: tf.Tensor,\n    inducing_variable: InducingVariables,\n    kernel: Kernel,\n    f: tf.Tensor,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""\n    Single-output GP conditional.\n\n    The covariance matrices used to calculate the conditional have the following shape:\n    - Kuu: [M, M]\n    - Kuf: [M, N]\n    - Kff: [N, N]\n\n    Further reference\n    -----------------\n    - See `gpflow.conditionals._conditional` (below) for a detailed explanation of\n      conditional in the single-output case.\n    - See the multiouput notebook for more information about the multiouput framework.\n\n    Parameters\n    ----------\n    :param Xnew: data matrix, size [N, D].\n    :param f: data matrix, [M, R]\n    :param full_cov: return the covariance between the datapoints\n    :param full_output_cov: return the covariance between the outputs.\n           NOTE: as we are using a single-output kernel with repetitions\n                 these covariances will be zero.\n    :param q_sqrt: matrix of standard-deviations or Cholesky matrices,\n        size [M, R] or [R, M, M].\n    :param white: boolean of whether to use the whitened representation\n    :return:\n        - mean:     [N, R]\n        - variance: [N, R], [R, N, N], [N, R, R] or [N, R, N, R]\n        Please see `gpflow.conditional._expand_independent_outputs` for more information\n        about the shape of the variance, depending on `full_cov` and `full_output_cov`.\n    """"""\n    Kmm = Kuu(inducing_variable, kernel, jitter=default_jitter())  # [M, M]\n    Kmn = Kuf(inducing_variable, kernel, Xnew)  # [M, N]\n    Knn = kernel(Xnew, full_cov=full_cov)\n    fmean, fvar = base_conditional(\n        Kmn, Kmm, Knn, f, full_cov=full_cov, q_sqrt=q_sqrt, white=white\n    )  # [N, R],  [R, N, N] or [N, R]\n    return fmean, expand_independent_outputs(fvar, full_cov, full_output_cov)\n\n\n@conditional.register(object, object, Kernel, object)\ndef _conditional(\n    Xnew: tf.Tensor,\n    X: tf.Tensor,\n    kernel: Kernel,\n    f: tf.Tensor,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""\n    Given f, representing the GP at the points X, produce the mean and\n    (co-)variance of the GP at the points Xnew.\n\n    Additionally, there may be Gaussian uncertainty about f as represented by\n    q_sqrt. In this case `f` represents the mean of the distribution and\n    q_sqrt the square-root of the covariance.\n\n    Additionally, the GP may have been centered (whitened) so that\n        p(v) = \xf0\x9d\x92\xa9(\xf0\x9d\x9f\x8e, \xf0\x9d\x90\x88)\n        f = \xf0\x9d\x90\x8bv\n    thus\n        p(f) = \xf0\x9d\x92\xa9(\xf0\x9d\x9f\x8e, \xf0\x9d\x90\x8b\xf0\x9d\x90\x8b\xe1\xb5\x80) = \xf0\x9d\x92\xa9(\xf0\x9d\x9f\x8e, \xf0\x9d\x90\x8a).\n    In this case `f` represents the values taken by v.\n\n    The method can either return the diagonals of the covariance matrix for\n    each output (default) or the full covariance matrix (full_cov=True).\n\n    We assume R independent GPs, represented by the columns of f (and the\n    first dimension of q_sqrt).\n\n    :param Xnew: data matrix, size [N, D]. Evaluate the GP at these new points\n    :param X: data points, size [M, D].\n    :param kernel: GPflow kernel.\n    :param f: data matrix, [M, R], representing the function values at X,\n        for R functions.\n    :param q_sqrt: matrix of standard-deviations or Cholesky matrices,\n        size [M, R] or [R, M, M].\n    :param white: boolean of whether to use the whitened representation as\n        described above.\n    :return:\n        - mean:     [N, R]\n        - variance: [N, R] (full_cov = False), [R, N, N] (full_cov = True)\n    """"""\n    Kmm = kernel(X) + eye(tf.shape(X)[-2], value=default_jitter(), dtype=X.dtype)  # [..., M, M]\n    Kmn = kernel(X, Xnew)  # [M, ..., N]\n    Knn = kernel(Xnew, full_cov=full_cov)  # [..., N] (full_cov = False) or [..., N, N] (True)\n    mean, var = base_conditional(Kmn, Kmm, Knn, f, full_cov=full_cov, q_sqrt=q_sqrt, white=white)\n\n    return mean, var  # [N, R], [N, R] or [R, N, N]\n'"
gpflow/conditionals/dispatch.py,0,"b'from ..utilities import Dispatcher\n\nconditional = Dispatcher(""conditional"")\nsample_conditional = Dispatcher(""sample_conditional"")\n'"
gpflow/conditionals/sample_conditionals.py,4,"b'import tensorflow as tf\n\nfrom ..inducing_variables import InducingVariables\nfrom ..kernels import Kernel\nfrom .dispatch import conditional, sample_conditional\nfrom .util import sample_mvn\n\n\n@sample_conditional.register(object, object, Kernel, object)\n@sample_conditional.register(object, InducingVariables, Kernel, object)\ndef _sample_conditional(\n    Xnew: tf.Tensor,\n    inducing_variable: InducingVariables,\n    kernel: Kernel,\n    f: tf.Tensor,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n    num_samples=None,\n):\n    """"""\n    `sample_conditional` will return a sample from the conditional distribution.\n    In most cases this means calculating the conditional mean m and variance v and then\n    returning m + sqrt(v) * eps, with eps ~ N(0, 1).\n    However, for some combinations of Mok and Mof more efficient sampling routines exists.\n    The dispatcher will make sure that we use the most efficient one.\n    :return: samples, mean, cov\n        samples has shape [num_samples, N, P] or [N, P] if num_samples is None\n        mean and cov as for conditional()\n    """"""\n\n    if full_cov and full_output_cov:\n        msg = ""The combination of both `full_cov` and `full_output_cov` is not permitted.""\n        raise NotImplementedError(msg)\n\n    mean, cov = conditional(\n        Xnew,\n        inducing_variable,\n        kernel,\n        f,\n        q_sqrt=q_sqrt,\n        white=white,\n        full_cov=full_cov,\n        full_output_cov=full_output_cov,\n    )\n    if full_cov:\n        # mean: [..., N, P]\n        # cov: [..., P, N, N]\n        mean_for_sample = tf.linalg.adjoint(mean)  # [..., P, N]\n        samples = sample_mvn(\n            mean_for_sample, cov, full_cov=True, num_samples=num_samples\n        )  # [..., (S), P, N]\n        samples = tf.linalg.adjoint(samples)  # [..., (S), N, P]\n    else:\n        # mean: [..., N, P]\n        # cov: [..., N, P] or [..., N, P, P]\n        samples = sample_mvn(\n            mean, cov, full_cov=full_output_cov, num_samples=num_samples\n        )  # [..., (S), N, P]\n\n    return samples, mean, cov\n'"
gpflow/conditionals/uncertain_conditionals.py,32,"b'import tensorflow as tf\n\nfrom .. import mean_functions\nfrom .. import covariances\nfrom ..expectations import expectation\nfrom ..inducing_variables import InducingVariables, InducingPoints\nfrom ..kernels import Kernel\nfrom ..probability_distributions import Gaussian\nfrom ..config import default_float, default_jitter\n\n\ndef uncertain_conditional(\n    Xnew_mu: tf.Tensor,\n    Xnew_var: tf.Tensor,\n    inducing_variable: InducingVariables,\n    kernel: Kernel,\n    q_mu,\n    q_sqrt,\n    *,\n    mean_function=None,\n    full_output_cov=False,\n    full_cov=False,\n    white=False,\n):\n    """"""\n    Calculates the conditional for uncertain inputs Xnew, p(Xnew) = N(Xnew_mu, Xnew_var).\n    See ``conditional`` documentation for further reference.\n    :param Xnew_mu: mean of the inputs, size [N, D]in\n    :param Xnew_var: covariance matrix of the inputs, size [N, n, n]\n    :param inducing_variable: gpflow.InducingVariable object, only InducingPoints is supported\n    :param kernel: gpflow kernel object.\n    :param q_mu: mean inducing points, size [M, Dout]\n    :param q_sqrt: cholesky of the covariance matrix of the inducing points, size [t, M, M]\n    :param full_output_cov: boolean wheter to compute covariance between output dimension.\n                            Influences the shape of return value ``fvar``. Default is False\n    :param white: boolean whether to use whitened representation. Default is False.\n    :return fmean, fvar: mean and covariance of the conditional, size ``fmean`` is [N, Dout],\n            size ``fvar`` depends on ``full_output_cov``: if True ``f_var`` is [N, t, t],\n            if False then ``f_var`` is [N, Dout]\n    """"""\n\n    if not isinstance(inducing_variable, InducingPoints):\n        raise NotImplementedError\n\n    if full_cov:\n        raise NotImplementedError(\n            ""uncertain_conditional() currently does not support full_cov=True""\n        )\n\n    pXnew = Gaussian(Xnew_mu, Xnew_var)\n\n    num_data = tf.shape(Xnew_mu)[0]  # number of new inputs (N)\n    num_ind, num_func = tf.unstack(\n        tf.shape(q_mu), num=2, axis=0\n    )  # number of inducing points (M), output dimension (D)\n    q_sqrt_r = tf.linalg.band_part(q_sqrt, -1, 0)  # [D, M, M]\n\n    eKuf = tf.transpose(expectation(pXnew, (kernel, inducing_variable)))  # [M, N] (psi1)\n    Kuu = covariances.Kuu(inducing_variable, kernel, jitter=default_jitter())  # [M, M]\n    Luu = tf.linalg.cholesky(Kuu)  # [M, M]\n\n    if not white:\n        q_mu = tf.linalg.triangular_solve(Luu, q_mu, lower=True)\n        Luu_tiled = tf.tile(\n            Luu[None, :, :], [num_func, 1, 1]\n        )  # remove line once issue 216 is fixed\n        q_sqrt_r = tf.linalg.triangular_solve(Luu_tiled, q_sqrt_r, lower=True)\n\n    Li_eKuf = tf.linalg.triangular_solve(Luu, eKuf, lower=True)  # [M, N]\n    fmean = tf.linalg.matmul(Li_eKuf, q_mu, transpose_a=True)\n\n    eKff = expectation(pXnew, kernel)  # N (psi0)\n    eKuffu = expectation(\n        pXnew, (kernel, inducing_variable), (kernel, inducing_variable)\n    )  # [N, M, M] (psi2)\n    Luu_tiled = tf.tile(\n        Luu[None, :, :], [num_data, 1, 1]\n    )  # remove this line, once issue 216 is fixed\n    Li_eKuffu = tf.linalg.triangular_solve(Luu_tiled, eKuffu, lower=True)\n    Li_eKuffu_Lit = tf.linalg.triangular_solve(\n        Luu_tiled, tf.linalg.adjoint(Li_eKuffu), lower=True\n    )  # [N, M, M]\n    cov = tf.linalg.matmul(q_sqrt_r, q_sqrt_r, transpose_b=True)  # [D, M, M]\n\n    if mean_function is None or isinstance(mean_function, mean_functions.Zero):\n        e_related_to_mean = tf.zeros((num_data, num_func, num_func), dtype=default_float())\n    else:\n        # Update mean: \\mu(x) + m(x)\n        fmean = fmean + expectation(pXnew, mean_function)\n\n        # Calculate: m(x) m(x)^T + m(x) \\mu(x)^T + \\mu(x) m(x)^T,\n        # where m(x) is the mean_function and \\mu(x) is fmean\n        e_mean_mean = expectation(pXnew, mean_function, mean_function)  # [N, D, D]\n        Lit_q_mu = tf.linalg.triangular_solve(Luu, q_mu, adjoint=True)\n        e_mean_Kuf = expectation(pXnew, mean_function, (kernel, inducing_variable))  # [N, D, M]\n        # einsum isn\'t able to infer the rank of e_mean_Kuf, hence we explicitly set the rank of the tensor:\n        e_mean_Kuf = tf.reshape(e_mean_Kuf, [num_data, num_func, num_ind])\n        e_fmean_mean = tf.einsum(""nqm,mz->nqz"", e_mean_Kuf, Lit_q_mu)  # [N, D, D]\n        e_related_to_mean = e_fmean_mean + tf.linalg.adjoint(e_fmean_mean) + e_mean_mean\n\n    if full_output_cov:\n        fvar = (\n            tf.linalg.diag(tf.tile((eKff - tf.linalg.trace(Li_eKuffu_Lit))[:, None], [1, num_func]))\n            + tf.linalg.diag(tf.einsum(""nij,dji->nd"", Li_eKuffu_Lit, cov))\n            +\n            # tf.linalg.diag(tf.linalg.trace(tf.linalg.matmul(Li_eKuffu_Lit, cov))) +\n            tf.einsum(""ig,nij,jh->ngh"", q_mu, Li_eKuffu_Lit, q_mu)\n            -\n            # tf.linalg.matmul(q_mu, tf.linalg.matmul(Li_eKuffu_Lit, q_mu), transpose_a=True) -\n            fmean[:, :, None] * fmean[:, None, :]\n            + e_related_to_mean\n        )\n    else:\n        fvar = (\n            (eKff - tf.linalg.trace(Li_eKuffu_Lit))[:, None]\n            + tf.einsum(""nij,dji->nd"", Li_eKuffu_Lit, cov)\n            + tf.einsum(""ig,nij,jg->ng"", q_mu, Li_eKuffu_Lit, q_mu)\n            - fmean ** 2\n            + tf.linalg.diag_part(e_related_to_mean)\n        )\n\n    return fmean, fvar\n'"
gpflow/conditionals/util.py,124,"b'from typing import Optional\nimport tensorflow as tf\n\nfrom ..config import default_float, default_jitter\nfrom ..utilities.ops import leading_transpose\n\n\ndef base_conditional(\n    Kmn: tf.Tensor,\n    Kmm: tf.Tensor,\n    Knn: tf.Tensor,\n    f: tf.Tensor,\n    *,\n    full_cov=False,\n    q_sqrt: Optional[tf.Tensor] = None,\n    white=False,\n):\n    r""""""\n    Given a g1 and g2, and distribution p and q such that\n      p(g2) = N(g2; 0, Kmm)\n\n      p(g1) = N(g1; 0, Knn)\n      p(g1 | g2) = N(g1; Knm (Kmm\xe2\x81\xbb\xc2\xb9) g2, Knn - Knm (Kmm\xe2\x81\xbb\xc2\xb9) Kmn)\n\n    And\n      q(g2) = N(g2; f, q_sqrt q_sqrt\xe1\xb5\x80)\n\n    This method computes the mean and (co)variance of\n      q(g1) = \xe2\x88\xab q(g2) p(g1 | g2)\n\n    :param Kmn: [M, ..., N]\n    :param Kmm: [M, M]\n    :param Knn: [..., N, N]  or  N\n    :param f: [M, R]\n    :param full_cov: bool\n    :param q_sqrt: If this is a Tensor, it must have shape [R, M, M] (lower\n        triangular) or [M, R] (diagonal)\n    :param white: bool\n    :return: [N, R]  or [R, N, N]\n    """"""\n    # compute kernel stuff\n    num_func = tf.shape(f)[-1]  # R\n    N = tf.shape(Kmn)[-1]\n    M = tf.shape(f)[-2]\n\n    # get the leadings dims in Kmn to the front of the tensor\n    # if Kmn has rank two, i.e. [M, N], this is the identity op.\n    K = tf.rank(Kmn)\n    perm = tf.concat(\n        [\n            tf.reshape(tf.range(1, K - 1), [K - 2]),  # leading dims (...)\n            tf.reshape(0, [1]),  # [M]\n            tf.reshape(K - 1, [1]),\n        ],\n        0,\n    )  # [N]\n    Kmn = tf.transpose(Kmn, perm)  # [..., M, N]\n\n    shape_constraints = [\n        (Kmn, [..., ""M"", ""N""]),\n        (Kmm, [""M"", ""M""]),\n        (Knn, [..., ""N"", ""N""] if full_cov else [..., ""N""]),\n        (f, [""M"", ""R""]),\n    ]\n    if q_sqrt is not None:\n        shape_constraints.append(\n            (q_sqrt, ([""M"", ""R""] if q_sqrt.shape.ndims == 2 else [""R"", ""M"", ""M""]))\n        )\n    tf.debugging.assert_shapes(\n        shape_constraints,\n        message=""base_conditional() arguments ""\n        ""[Note that this check verifies the shape of an alternative ""\n        ""representation of Kmn. See the docs for the actual expected ""\n        ""shape.]"",\n    )\n\n    leading_dims = tf.shape(Kmn)[:-2]\n    Lm = tf.linalg.cholesky(Kmm)  # [M, M]\n\n    # Compute the projection matrix A\n    Lm = tf.broadcast_to(Lm, tf.concat([leading_dims, tf.shape(Lm)], 0))  # [..., M, M]\n    A = tf.linalg.triangular_solve(Lm, Kmn, lower=True)  # [..., M, N]\n\n    # compute the covariance due to the conditioning\n    if full_cov:\n        fvar = Knn - tf.linalg.matmul(A, A, transpose_a=True)  # [..., N, N]\n        cov_shape = tf.concat([leading_dims, [num_func, N, N]], 0)\n        fvar = tf.broadcast_to(tf.expand_dims(fvar, -3), cov_shape)  # [..., R, N, N]\n    else:\n        fvar = Knn - tf.reduce_sum(tf.square(A), -2)  # [..., N]\n        cov_shape = tf.concat([leading_dims, [num_func, N]], 0)  # [..., R, N]\n        fvar = tf.broadcast_to(tf.expand_dims(fvar, -2), cov_shape)  # [..., R, N]\n\n    # another backsubstitution in the unwhitened case\n    if not white:\n        A = tf.linalg.triangular_solve(tf.linalg.adjoint(Lm), A, lower=False)\n\n    # construct the conditional mean\n    f_shape = tf.concat([leading_dims, [M, num_func]], 0)  # [..., M, R]\n    f = tf.broadcast_to(f, f_shape)  # [..., M, R]\n    fmean = tf.linalg.matmul(A, f, transpose_a=True)  # [..., N, R]\n\n    if q_sqrt is not None:\n        q_sqrt_dims = q_sqrt.shape.ndims\n        if q_sqrt_dims == 2:\n            LTA = A * tf.expand_dims(tf.transpose(q_sqrt), 2)  # [R, M, N]\n        elif q_sqrt_dims == 3:\n            L = tf.linalg.band_part(q_sqrt, -1, 0)  # force lower triangle # [R, M, M]\n            L_shape = tf.shape(L)\n            L = tf.broadcast_to(L, tf.concat([leading_dims, L_shape], 0))\n\n            shape = tf.concat([leading_dims, [num_func, M, N]], axis=0)\n            A_tiled = tf.broadcast_to(tf.expand_dims(A, -3), shape)\n            LTA = tf.linalg.matmul(L, A_tiled, transpose_a=True)  # [R, M, N]\n        else:  # pragma: no cover\n            raise ValueError(""Bad dimension for q_sqrt: %s"" % str(q_sqrt.shape.ndims))\n\n        if full_cov:\n            fvar = fvar + tf.linalg.matmul(LTA, LTA, transpose_a=True)  # [R, N, N]\n        else:\n            fvar = fvar + tf.reduce_sum(tf.square(LTA), -2)  # [R, N]\n\n    if not full_cov:\n        fvar = tf.linalg.adjoint(fvar)  # [N, R]\n\n    shape_constraints = [\n        (Kmn, [..., ""M"", ""N""]),  # tensor included again for N dimension\n        (f, [..., ""M"", ""R""]),  # tensor included again for R dimension\n        (fmean, [..., ""N"", ""R""]),\n        (fvar, [..., ""R"", ""N"", ""N""] if full_cov else [..., ""N"", ""R""]),\n    ]\n    tf.debugging.assert_shapes(shape_constraints, message=""base_conditional() return values"")\n\n    return fmean, fvar\n\n\ndef sample_mvn(mean, cov, full_cov, num_samples=None):\n    """"""\n    Returns a sample from a D-dimensional Multivariate Normal distribution\n    :param mean: [..., N, D]\n    :param cov: [..., N, D] or [..., N, D, D]\n    :param full_cov: if `True` return a ""full"" covariance matrix, otherwise a ""diag"":\n    - ""full"": cov holds the full covariance matrix (without jitter)\n    - ""diag"": cov holds the diagonal elements of the covariance matrix\n    :return: sample from the MVN of shape [..., (S), N, D], S = num_samples\n    """"""\n    shape_constraints = [\n        (mean, [..., ""N"", ""D""]),\n        (cov, [..., ""N"", ""D"", ""D""] if full_cov else [..., ""N"", ""D""]),\n    ]\n    tf.debugging.assert_shapes(shape_constraints, message=""sample_mvn() arguments"")\n\n    mean_shape = tf.shape(mean)\n    S = num_samples if num_samples is not None else 1\n    D = mean_shape[-1]\n    leading_dims = mean_shape[:-2]\n\n    if not full_cov:\n        # mean: [..., N, D] and cov [..., N, D]\n        eps_shape = tf.concat([leading_dims, [S], mean_shape[-2:]], 0)\n        eps = tf.random.normal(eps_shape, dtype=default_float())  # [..., S, N, D]\n        samples = mean[..., None, :, :] + tf.sqrt(cov)[..., None, :, :] * eps  # [..., S, N, D]\n\n    else:\n        # mean: [..., N, D] and cov [..., N, D, D]\n        jittermat = (\n            tf.eye(D, batch_shape=mean_shape[:-1], dtype=default_float()) * default_jitter()\n        )  # [..., N, D, D]\n        eps_shape = tf.concat([mean_shape, [S]], 0)\n        eps = tf.random.normal(eps_shape, dtype=default_float())  # [..., N, D, S]\n        chol = tf.linalg.cholesky(cov + jittermat)  # [..., N, D, D]\n        samples = mean[..., None] + tf.linalg.matmul(chol, eps)  # [..., N, D, S]\n        samples = leading_transpose(samples, [..., -1, -3, -2])  # [..., S, N, D]\n\n    shape_constraints = [\n        (mean, [..., ""N"", ""D""]),\n        (samples, [..., ""S"", ""N"", ""D""]),\n    ]\n    tf.debugging.assert_shapes(shape_constraints, message=""sample_mvn() return values"")\n\n    if num_samples is None:\n        return tf.squeeze(samples, axis=-3)  # [..., N, D]\n    return samples  # [..., S, N, D]\n\n\ndef expand_independent_outputs(fvar, full_cov, full_output_cov):\n    """"""\n    Reshapes fvar to the correct shape, specified by `full_cov` and `full_output_cov`.\n\n    :param fvar: has shape [N, P] (full_cov = False) or [P, N, N] (full_cov = True).\n    :return:\n    1. full_cov: True and full_output_cov: True\n       fvar [N, P, N, P]\n    2. full_cov: True and full_output_cov: False\n       fvar [P, N, N]\n    3. full_cov: False and full_output_cov: True\n       fvar [N, P, P]\n    4. full_cov: False and full_output_cov: False\n       fvar [N, P]\n    """"""\n    if full_cov and full_output_cov:\n        fvar = tf.linalg.diag(tf.transpose(fvar))  # [N, N, P, P]\n        fvar = tf.transpose(fvar, [0, 2, 1, 3])  # [N, P, N, P]\n    if not full_cov and full_output_cov:\n        fvar = tf.linalg.diag(fvar)  # [N, P, P]\n    if full_cov and not full_output_cov:\n        pass  # [P, N, N]\n    if not full_cov and not full_output_cov:\n        pass  # [N, P]\n\n    return fvar\n\n\ndef independent_interdomain_conditional(\n    Kmn, Kmm, Knn, f, *, full_cov=False, full_output_cov=False, q_sqrt=None, white=False\n):\n    """"""\n    The inducing outputs live in the g-space (R^L).\n    Interdomain conditional calculation.\n    :param Kmn: [M, L, N, P]\n    :param Kmm: [L, M, M]\n    :param Knn: [N, P]  or  [N, P, P]  or  [P, N, N]  or  [N, P, N, P]\n    :param f: data matrix, [M, L]\n    :param q_sqrt: [L, M, M]  or  [M, L]\n    :param full_cov: calculate covariance between inputs\n    :param full_output_cov: calculate covariance between outputs\n    :param white: use whitened representation\n    :return:\n        - mean: [N, P]\n        - variance: [N, P], [N, P, P], [P, N, N], [N, P, N, P]\n    """"""\n    M, L, N, P = tf.unstack(tf.shape(Kmn), num=Kmn.shape.ndims, axis=0)\n\n    shape_constraints = [\n        (Kmn, [""M"", ""L"", ""N"", ""P""]),\n        (Kmm, [""L"", ""M"", ""M""]),\n        (f, [""M"", ""L""]),\n    ]\n    if q_sqrt is not None:\n        shape_constraints.append((q_sqrt, ""ML"" if q_sqrt.shape.ndims == 2 else ""LMM""))\n\n    Lm = tf.linalg.cholesky(Kmm)  # [L, M, M]\n\n    # Compute the projection matrix A\n    Kmn = tf.reshape(tf.transpose(Kmn, (1, 0, 2, 3)), (L, M, N * P))\n    A = tf.linalg.triangular_solve(Lm, Kmn, lower=True)  # [L, M, M]  *  [L, M, P]  ->  [L, M, P]\n    Ar = tf.reshape(A, (L, M, N, P))\n\n    # compute the covariance due to the conditioning\n    if full_cov and full_output_cov:\n        fvar = Knn - tf.tensordot(Ar, Ar, [[0, 1], [0, 1]])  # [N, P, N, P]\n        intended_cov_shape = [""N"", ""P"", ""N"", ""P""]\n    elif full_cov and not full_output_cov:\n        At = tf.reshape(tf.transpose(Ar), (P, N, M * L))  # [P, N, L]\n        fvar = Knn - tf.linalg.matmul(At, At, transpose_b=True)  # [P, N, N]\n        intended_cov_shape = [""P"", ""N"", ""N""]\n    elif not full_cov and full_output_cov:\n        At = tf.reshape(tf.transpose(Ar, [2, 3, 1, 0]), (N, P, M * L))  # [N, P, L]\n        fvar = Knn - tf.linalg.matmul(At, At, transpose_b=True)  # [N, P, P]\n        intended_cov_shape = [""N"", ""P"", ""P""]\n    elif not full_cov and not full_output_cov:\n        fvar = Knn - tf.reshape(tf.reduce_sum(tf.square(A), [0, 1]), (N, P))  # Knn: [N, P]\n        intended_cov_shape = [""N"", ""P""]\n\n    # another backsubstitution in the unwhitened case\n    if not white:\n        A = tf.linalg.triangular_solve(Lm, Ar)  # [L, M, M]  *  [L, M, P]  ->  [L, M, P]\n        Ar = tf.reshape(A, (L, M, N, P))\n\n    fmean = tf.tensordot(Ar, f, [[1, 0], [0, 1]])  # [N, P]\n\n    if q_sqrt is not None:\n        if q_sqrt.shape.ndims == 3:\n            Lf = tf.linalg.band_part(q_sqrt, -1, 0)  # [L, M, M]\n            LTA = tf.linalg.matmul(\n                Lf, A, transpose_a=True\n            )  # [L, M, M]  *  [L, M, P]  ->  [L, M, P]\n        else:  # q_sqrt [M, L]\n            LTA = A * tf.transpose(q_sqrt)[..., None]  # [L, M, P]\n\n        if full_cov and full_output_cov:\n            LTAr = tf.reshape(LTA, (L * M, N * P))\n            fvar = fvar + tf.reshape(tf.linalg.matmul(LTAr, LTAr, transpose_a=True), (N, P, N, P))\n        elif full_cov and not full_output_cov:\n            LTAr = tf.transpose(tf.reshape(LTA, (L * M, N, P)), [2, 0, 1])  # [P, M, N]\n            fvar = fvar + tf.linalg.matmul(LTAr, LTAr, transpose_a=True)  # [P, N, N]\n        elif not full_cov and full_output_cov:\n            LTAr = tf.transpose(tf.reshape(LTA, (L * M, N, P)), [1, 0, 2])  # [N, M, P]\n            fvar = fvar + tf.linalg.matmul(LTAr, LTAr, transpose_a=True)  # [N, P, P]\n        elif not full_cov and not full_output_cov:\n            fvar = fvar + tf.reshape(tf.reduce_sum(tf.square(LTA), (0, 1)), (N, P))\n\n    shape_constraints.extend(\n        [(Knn, intended_cov_shape), (fmean, [""N"", ""P""]), (fvar, intended_cov_shape),]\n    )\n    tf.debugging.assert_shapes(shape_constraints, message=""independent_interdomain_conditional()"")\n\n    return fmean, fvar\n\n\ndef fully_correlated_conditional(\n    Kmn, Kmm, Knn, f, *, full_cov=False, full_output_cov=False, q_sqrt=None, white=False\n):\n    """"""\n    This function handles conditioning of multi-output GPs in the case where the conditioning\n    points are all fully correlated, in both the prior and posterior.\n    :param Kmn: [M, N, P]\n    :param Kmm: [M, M]\n    :param Knn: [N, P] or [N, P, N, P]\n    :param f: data matrix, [M, 1]\n    :param q_sqrt: [1, M, M] or [1, L]\n    :param full_cov: calculate covariance between inputs\n    :param full_output_cov: calculate covariance between outputs\n    :param white: use whitened representation\n    :return:\n        - mean: [N, P]\n        - variance: [N, P], [N, P, P], [P, N, N], [N, P, N, P]\n    """"""\n    mean, var = fully_correlated_conditional_repeat(\n        Kmn,\n        Kmm,\n        Knn,\n        f,\n        full_cov=full_cov,\n        full_output_cov=full_output_cov,\n        q_sqrt=q_sqrt,\n        white=white,\n    )\n    return tf.squeeze(mean, axis=0), tf.squeeze(var, axis=0)\n\n\ndef fully_correlated_conditional_repeat(\n    Kmn, Kmm, Knn, f, *, full_cov=False, full_output_cov=False, q_sqrt=None, white=False\n):\n    """"""\n    This function handles conditioning of multi-output GPs in the case where the conditioning\n    points are all fully correlated, in both the prior and posterior.\n    Note: This conditional can handle \'repetitions\' R, given in `f` and `q_sqrt`.\n    :param Kmn: [M, N, P]\n    :param Kmm: [M, M]\n    :param Knn: [N, P] or [N, P, P] or [P, N, N] or [N, P, N, P]\n    :param f: data matrix, [M, R]\n    :param q_sqrt: [R, M, M] or [M, R]\n    :param full_cov: calculate covariance between inputs\n    :param full_output_cov: calculate covariance between outputs\n    :param white: use whitened representation\n    :return:\n        - mean: [R, N, P]\n        - variance: [R, N, P], [R, N, P, P], [R, P, N, N], [R, N, P, N, P]\n    """"""\n    R = tf.shape(f)[1]\n    M, N, P = tf.unstack(tf.shape(Kmn), num=Kmn.shape.ndims, axis=0)\n\n    shape_constraints = [\n        (Kmn, [""M"", ""N"", ""P""]),\n        (Kmm, [""M"", ""M""]),\n        (f, [""M"", ""R""]),\n    ]\n    if q_sqrt is not None:\n        shape_constraints.append(\n            (q_sqrt, [""M"", ""R""] if q_sqrt.shape.ndims == 2 else [""R"", ""M"", ""M""])\n        )\n\n    Lm = tf.linalg.cholesky(Kmm)\n\n    # Compute the projection matrix A\n    # Lm: [M, M]    Kmn: [M, P]\n    Kmn = tf.reshape(Kmn, (M, N * P))  # [M, P]\n    A = tf.linalg.triangular_solve(Lm, Kmn, lower=True)  # [M, P]\n    Ar = tf.reshape(A, (M, N, P))\n\n    # compute the covariance due to the conditioning\n    if full_cov and full_output_cov:\n        # fvar = Knn - tf.linalg.matmul(Ar, Ar, transpose_a=True)  # [P, P], then reshape?\n        fvar = Knn - tf.tensordot(Ar, Ar, [[0], [0]])  # [N, P, N, P]\n        intended_cov_shape = [""N"", ""P"", ""N"", ""P""]\n    elif full_cov and not full_output_cov:\n        At = tf.transpose(Ar)  # [P, N, M]\n        fvar = Knn - tf.linalg.matmul(At, At, transpose_b=True)  # [P, N, N]\n        intended_cov_shape = [""P"", ""N"", ""N""]\n    elif not full_cov and full_output_cov:\n        # This transpose is annoying\n        At = tf.transpose(Ar, [1, 0, 2])  # [N, M, P]\n        # fvar = Knn - tf.einsum(\'mnk,mnl->nkl\', Ar, Ar)\n        fvar = Knn - tf.linalg.matmul(At, At, transpose_a=True)  # [N, P, P]\n        intended_cov_shape = [""N"", ""P"", ""P""]\n    elif not full_cov and not full_output_cov:\n        # Knn: [N, P]\n        # Can also do this with a matmul\n        fvar = Knn - tf.reshape(tf.reduce_sum(tf.square(A), [0]), (N, P))\n        intended_cov_shape = [""N"", ""P""]\n\n    # another backsubstitution in the unwhitened case\n    if not white:\n        # A = tf.linalg.triangular_solve(tf.linalg.adjoint(Lm), A, lower=False)  # [M, P]\n        raise NotImplementedError(""Need to verify this."")  # pragma: no cover\n\n    # f: [M, R]\n    fmean = tf.linalg.matmul(f, A, transpose_a=True)  # [R, M]  *  [M, P]  ->  [R, P]\n    fmean = tf.reshape(fmean, (R, N, P))  # [R, N, P]\n\n    if q_sqrt is not None:\n        Lf = tf.linalg.band_part(q_sqrt, -1, 0)  # [R, M, M]\n        if q_sqrt.shape.ndims == 3:\n            A_tiled = tf.tile(A[None, :, :], tf.stack([R, 1, 1]))  # [R, M, P]\n            LTA = tf.linalg.matmul(Lf, A_tiled, transpose_a=True)  # [R, M, P]\n        elif q_sqrt.shape.ndims == 2:  # pragma: no cover\n            raise NotImplementedError(""Does not support diagonal q_sqrt yet..."")\n        else:  # pragma: no cover\n            raise ValueError(f""Bad dimension for q_sqrt: {q_sqrt.shape.ndims}"")\n\n        if full_cov and full_output_cov:\n            addvar = tf.linalg.matmul(LTA, LTA, transpose_a=True)  # [R, P, P]\n            fvar = fvar[None, :, :, :, :] + tf.reshape(addvar, (R, N, P, N, P))\n        elif full_cov and not full_output_cov:\n            LTAr = tf.transpose(tf.reshape(LTA, [R, M, N, P]), [0, 3, 1, 2])  # [R, P, M, N]\n            addvar = tf.linalg.matmul(LTAr, LTAr, transpose_a=True)  # [R, P, N, N]\n            fvar = fvar[None, ...] + addvar  # [R, P, N, N]\n        elif not full_cov and full_output_cov:\n            LTAr = tf.transpose(tf.reshape(LTA, (R, M, N, P)), [0, 2, 3, 1])  # [R, N, P, M]\n            fvar = fvar[None, ...] + tf.linalg.matmul(LTAr, LTAr, transpose_b=True)  # [R, N, P, P]\n        elif not full_cov and not full_output_cov:\n            addvar = tf.reshape(tf.reduce_sum(tf.square(LTA), axis=1), (R, N, P))  # [R, N, P]\n            fvar = fvar[None, ...] + addvar  # [R, N, P]\n    else:\n        fvar = tf.broadcast_to(fvar[None], tf.shape(fmean))\n\n    shape_constraints.extend(\n        [(Knn, intended_cov_shape), (fmean, [""R"", ""N"", ""P""]), (fvar, [""R""] + intended_cov_shape),]\n    )\n    tf.debugging.assert_shapes(shape_constraints, message=""fully_correlated_conditional_repeat()"")\n\n    return fmean, fvar\n\n\ndef rollaxis_left(A, num_rolls):\n    """"""Roll the tensor `A` backwards `num_rolls` times.""""""\n    assert num_rolls > 0\n    rank = tf.rank(A)\n    perm = tf.concat([num_rolls + tf.range(rank - num_rolls), tf.range(num_rolls)], 0)\n    return tf.transpose(A, perm)\n\n\ndef rollaxis_right(A, num_rolls):\n    """"""Roll the tensor `A` forward `num_rolls` times.""""""\n    assert num_rolls > 0\n    rank = tf.rank(A)\n    perm = tf.concat([rank - num_rolls + tf.range(num_rolls), tf.range(rank - num_rolls)], 0)\n    return tf.transpose(A, perm)\n\n\ndef mix_latent_gp(W, g_mean, g_var, full_cov, full_output_cov):\n    r""""""Takes the mean and variance of an uncorrelated L-dimensional latent GP\n    and returns the mean and the variance of the mixed GP, `f = W g`,\n    where both f and g are GPs, with W having a shape [P, L]\n\n    :param W: [P, L]\n    :param g_mean: [..., N, L]\n    :param g_var: [..., N, L] (full_cov = False) or [L, ..., N, N] (full_cov = True)\n    :return: f_mean and f_var, shape depends on `full_cov` and `full_output_cov`\n    """"""\n    shape_constraints = [\n        (W, [""P"", ""L""]),\n        (g_mean, [..., ""N"", ""L""]),\n    ]\n    if not full_cov:\n        shape_constraints.append((g_var, [..., ""N"", ""L""]))\n    else:\n        # NOTE(awav) cannot assert g_var shape here because of the inner ""leading""\n        # dimensions, see https://github.com/GPflow/GPflow/issues/1296\n        pass\n\n    f_mean = tf.tensordot(g_mean, W, [[-1], [-1]])  # [..., N, P]\n\n    if full_cov and full_output_cov:  # g_var is [L, ..., N, N]\n        # this branch is practically never taken\n        g_var = rollaxis_left(g_var, 1)  # [..., N, N, L]\n        shape_constraints.append((g_var, [..., ""N"", ""N"", ""L""]))\n\n        g_var = tf.expand_dims(g_var, axis=-2)  # [..., N, N, 1, L]\n        g_var_W = g_var * W  # [..., N, P, L]\n        f_var = tf.tensordot(g_var_W, W, [[-1], [-1]])  # [..., N, N, P, P]\n        f_var = leading_transpose(f_var, [..., -4, -2, -3, -1])  # [..., N, P, N, P]\n        intended_cov_shape = [..., ""N"", ""P"", ""N"", ""P""]\n\n    elif full_cov and not full_output_cov:  # g_var is [L, ..., N, N]\n        # this branch is practically never taken\n        f_var = tf.tensordot(g_var, W ** 2, [[0], [-1]])  # [..., N, N, P]\n        f_var = leading_transpose(f_var, [..., -1, -3, -2])  # [..., P, N, N]\n        intended_cov_shape = [..., ""P"", ""N"", ""N""]\n\n    elif not full_cov and full_output_cov:  # g_var is [..., N, L]\n        g_var = tf.expand_dims(g_var, axis=-2)  # [..., N, 1, L]\n        g_var_W = g_var * W  # [..., N, P, L]\n        f_var = tf.tensordot(g_var_W, W, [[-1], [-1]])  # [..., N, P, P]\n        intended_cov_shape = [..., ""N"", ""P"", ""P""]\n\n    elif not full_cov and not full_output_cov:  # g_var is [..., N, L]\n        W_squared = W ** 2  # [P, L]\n        f_var = tf.tensordot(g_var, W_squared, [[-1], [-1]])  # [..., N, P]\n        intended_cov_shape = [..., ""N"", ""P""]\n\n    shape_constraints.extend(\n        [(f_mean, [..., ""N"", ""P""]), (f_var, intended_cov_shape),]\n    )\n    tf.debugging.assert_shapes(shape_constraints, message=""mix_latent_gp()"")\n\n    return f_mean, f_var\n'"
gpflow/config/__config__.py,4,"b'""""""\nThis is a private module that manages GPflow configuration.\n\nThe module provides functions to modify default settings of GPflow, such as:\n- the standard float precision and integer type\n- the type of positive transformation\n- a value for a minimum shift from zero for the positive transformation\n- an output format for `gpflow.utilities.print_summary`\n\nThe module holds global configuration :class:`Config` variable that stores all\nsetting values.\n\nEnvironment variables are an alternative way for changing the default GPflow\nconfiguration.\n\n.. warning::\n    The user has to set environment variables before running python\n    interpreter to modify the configuration.\n\nFull set of environment variables and available options:\n\n* ``GPFLOW_INT``: ""int16"", ""int32"", or ""int64""\n* ``GPFLOW_FLOAT``: ""float16"", ""float32"", or ""float64""\n* ``GPFLOW_POSITIVE_BIJECTOR``: ""exp"" or ""softplus""\n* ``GPFLOW_POSITIVE_MINIMUM``: Any positive float number\n* ``GPFLOW_SUMMARY_FMT``: ""notebook"" or any other format that :mod:`tabulate` can handle.\n* ``GPFLOW_JITTER``: Any positive float number\n\nThe user can also change the GPflow configuration temporarily with a context\nmanager :func:`as_context`:\n\n>>> config = Config(jitter=1e-5)\n>>> with as_context(config):\n>>>     # ...code here sees new config\n""""""\n\nimport contextlib\nimport enum\nimport os\nfrom dataclasses import dataclass, field, replace\nfrom typing import Dict, Optional, Union\n\nimport numpy as np\nimport tabulate\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n__all__ = [\n    ""Config"",\n    ""as_context"",\n    ""config"",\n    ""set_config"",\n    ""default_float"",\n    ""set_default_float"",\n    ""default_int"",\n    ""set_default_int"",\n    ""default_jitter"",\n    ""set_default_jitter"",\n    ""default_positive_bijector"",\n    ""set_default_positive_bijector"",\n    ""default_positive_minimum"",\n    ""set_default_positive_minimum"",\n    ""default_summary_fmt"",\n    ""set_default_summary_fmt"",\n    ""positive_bijector_type_map"",\n]\n\n__config = None\n\n\nclass _Values(enum.Enum):\n    """"""Setting\'s names collection with default values. The `name` method returns name\n    of the environment variable. E.g. for `SUMMARY_FMT` field the environment variable\n    will be `GPFLOW_SUMMARY_FMT`.""""""\n\n    INT = np.int32\n    FLOAT = np.float64\n    POSITIVE_BIJECTOR = ""softplus""\n    POSITIVE_MINIMUM = 0.0\n    SUMMARY_FMT = ""fancy_grid""\n    JITTER = 1e-6\n\n    @property\n    def name(self):\n        return f""GPFLOW_{super().name}""\n\n\ndef _default(value: _Values):\n    """"""Checks if value is set in the environment.""""""\n    return os.getenv(value.name, default=value.value)\n\n\ndef _default_numeric_type_factory(valid_types, enum_key, type_name):\n    value = _default(enum_key)\n    if value in valid_types.values():\n        return value\n    if value not in valid_types:\n        raise TypeError(f""Config cannot recognize {type_name} type."")\n    return valid_types[value]\n\n\ndef _default_int_factory():\n    valid_types = dict(int16=np.int16, int32=np.int32, int64=np.int64)\n    return _default_numeric_type_factory(valid_types, _Values.INT, ""int"")\n\n\ndef _default_float_factory():\n    valid_types = dict(float16=np.float16, float32=np.float32, float64=np.float64)\n    return _default_numeric_type_factory(valid_types, _Values.FLOAT, ""float"")\n\n\ndef _default_jitter_factory():\n    value = _default(_Values.JITTER)\n    try:\n        value = float(value)\n    except ValueError:\n        raise TypeError(""Config cannot set the jitter value with non float type."")\n    return value\n\n\ndef _default_positive_bijector_factory():\n    bijector_type = _default(_Values.POSITIVE_BIJECTOR)\n    if bijector_type not in positive_bijector_type_map().keys():\n        raise TypeError(\n            ""Config cannot set the passed value as a default positive bijector.""\n            f""Available options: {set(positive_bijector_type_map().keys())}""\n        )\n    return bijector_type\n\n\ndef _default_positive_minimum_factory():\n    value = _default(_Values.POSITIVE_MINIMUM)\n    try:\n        value = float(value)\n    except ValueError:\n        raise TypeError(""Config cannot set the positive_minimum value with non float type."")\n    return value\n\n\ndef _default_summary_fmt_factory():\n    return _default(_Values.SUMMARY_FMT)\n\n\n# The following type alias is for the Config class, to help a static analyser distinguish\n# between the built-in \'float\' type and the \'float\' type defined in the that class.\nFloat = Union[float]\n\n\n@dataclass(frozen=True)\nclass Config:\n    """"""\n    Immutable object for storing global GPflow settings\n\n    Args:\n        int: Integer data type, int32 or int64.\n        float: Float data type, float32 or float64\n        jitter: Jitter value. Mainly used for for making badly conditioned matrices more stable.\n            Default value is `1e-6`.\n        positive_bijector: Method for positive bijector, either ""softplus"" or ""exp"".\n            Default is ""softplus"".\n        positive_minimum: Lower bound for the positive transformation.\n        summary_fmt: Summary format for module printing.\n    """"""\n\n    int: type = field(default_factory=_default_int_factory)\n    float: type = field(default_factory=_default_float_factory)\n    jitter: Float = field(default_factory=_default_jitter_factory)\n    positive_bijector: str = field(default_factory=_default_positive_bijector_factory)\n    positive_minimum: Float = field(default_factory=_default_positive_minimum_factory)\n    summary_fmt: str = field(default_factory=_default_summary_fmt_factory)\n\n\ndef config() -> Config:\n    """"""Returns current active config.""""""\n    return __config\n\n\ndef default_int():\n    """"""Returns default integer type""""""\n    return config().int\n\n\ndef default_float():\n    """"""Returns default float type""""""\n    return config().float\n\n\ndef default_jitter():\n    """"""\n    The jitter is a constant that GPflow adds to the diagonal of matrices\n    to achieve numerical stability of the system when the condition number \n    of the associated matrices is large, and therefore the matrices nearly singular.\n    """"""\n    return config().jitter\n\n\ndef default_positive_bijector():\n    """"""Type of bijector used for positive constraints: exp or softplus.""""""\n    return config().positive_bijector\n\n\ndef default_positive_minimum():\n    """"""Shift constant that GPflow adds to all positive constraints.""""""\n    return config().positive_minimum\n\n\ndef default_summary_fmt():\n    """"""Summary printing format as understood by :mod:`tabulate` or a special case ""notebook"".""""""\n    return config().summary_fmt\n\n\ndef set_config(new_config: Config):\n    """"""Update GPflow config with new settings from `new_config`.""""""\n    global __config\n    __config = new_config\n\n\ndef set_default_int(value_type):\n    """"""\n    Sets default integer type. Available options are ``np.int16``, ``np.int32``,\n    or ``np.int64``.\n    """"""\n    try:\n        tf_dtype = tf.as_dtype(value_type)  # Test that it\'s a tensorflow-valid dtype\n    except TypeError:\n        raise TypeError(f""{value_type} is not a valid tf or np dtype"")\n\n    if not tf_dtype.is_integer:\n        raise TypeError(f""{value_type} is not an integer dtype"")\n\n    set_config(replace(config(), int=tf_dtype.as_numpy_dtype))\n\n\ndef set_default_float(value_type):\n    """"""\n    Sets default float type. Available options are `np.float16`, `np.float32`,\n    or `np.float64`.\n    """"""\n    try:\n        tf_dtype = tf.as_dtype(value_type)  # Test that it\'s a tensorflow-valid dtype\n    except TypeError:\n        raise TypeError(f""{value_type} is not a valid tf or np dtype"")\n\n    if not tf_dtype.is_floating:\n        raise TypeError(f""{value_type} is not a float dtype"")\n\n    set_config(replace(config(), float=tf_dtype.as_numpy_dtype))\n\n\ndef set_default_jitter(value: float):\n    """"""\n    Sets constant jitter value.\n    The jitter is a constant that GPflow adds to the diagonal of matrices\n    to achieve numerical stability of the system when the condition number \n    of the associated matrices is large, and therefore the matrices nearly singular.\n    """"""\n    if not (\n        isinstance(value, (tf.Tensor, np.ndarray)) and len(value.shape) == 0\n    ) and not isinstance(value, float):\n        raise TypeError(""Expected float32 or float64 scalar value"")\n\n    if value < 0:\n        raise ValueError(""Jitter must be non-negative"")\n\n    set_config(replace(config(), jitter=value))\n\n\ndef set_default_positive_bijector(value: str):\n    """"""\n    Sets positive bijector type.\n    There are currently two options implemented: ""exp"" and ""softplus"".\n    """"""\n    type_map = positive_bijector_type_map()\n    if isinstance(value, str):\n        value = value.lower()\n    if value not in type_map:\n        raise ValueError(f""`{value}` not in set of valid bijectors: {sorted(type_map)}"")\n\n    set_config(replace(config(), positive_bijector=value))\n\n\ndef set_default_positive_minimum(value: float):\n    """"""Sets shift constant for positive transformation.""""""\n    if not (\n        isinstance(value, (tf.Tensor, np.ndarray)) and len(value.shape) == 0\n    ) and not isinstance(value, float):\n        raise TypeError(""Expected float32 or float64 scalar value"")\n\n    if value < 0:\n        raise ValueError(""Positive minimum must be non-negative"")\n\n    set_config(replace(config(), positive_minimum=value))\n\n\ndef set_default_summary_fmt(value: str):\n    formats = tabulate.tabulate_formats + [""notebook"", None]\n    if value not in formats:\n        raise ValueError(f""Summary does not support \'{value}\' format"")\n\n    set_config(replace(config(), summary_fmt=value))\n\n\ndef positive_bijector_type_map() -> Dict[str, type]:\n    return {\n        ""exp"": tfp.bijectors.Exp,\n        ""softplus"": tfp.bijectors.Softplus,\n    }\n\n\n@contextlib.contextmanager\ndef as_context(temporary_config: Optional[Config] = None):\n    """"""Ensure that global configs defaults, with a context manager. Useful for testing.""""""\n    current_config = config()\n    temporary_config = replace(current_config) if temporary_config is None else temporary_config\n    try:\n        set_config(temporary_config)\n        yield\n    finally:\n        set_config(current_config)\n\n\n# Set global config.\nset_config(Config())\n'"
gpflow/config/__init__.py,0,b'from .__config__ import *\n'
gpflow/covariances/__init__.py,0,"b'from .dispatch import Kuf\nfrom .dispatch import Kuu\n\nfrom . import kufs, kuus\nfrom . import multioutput\n'"
gpflow/covariances/dispatch.py,0,"b'from ..utilities import Dispatcher\n\nKuu = Dispatcher(""Kuu"")\nKuf = Dispatcher(""Kuf"")\n'"
gpflow/covariances/kufs.py,4,"b'import tensorflow as tf\n\nfrom ..base import TensorLike\nfrom ..inducing_variables import InducingPoints, Multiscale, InducingPatches\nfrom ..kernels import Kernel, SquaredExponential, Convolutional\nfrom .dispatch import Kuf\n\n\n@Kuf.register(InducingPoints, Kernel, TensorLike)\ndef Kuf_kernel_inducingpoints(inducing_variable: InducingPoints, kernel: Kernel, Xnew):\n    return kernel(inducing_variable.Z, Xnew)\n\n\n@Kuf.register(Multiscale, SquaredExponential, TensorLike)\ndef Kuf_sqexp_multiscale(inducing_variable: Multiscale, kernel: SquaredExponential, Xnew):\n    Xnew, _ = kernel.slice(Xnew, None)\n    Zmu, Zlen = kernel.slice(inducing_variable.Z, inducing_variable.scales)\n    idlengthscales = kernel.lengthscales + Zlen\n    d = inducing_variable._cust_square_dist(Xnew, Zmu, idlengthscales)\n    lengthscales = tf.reduce_prod(kernel.lengthscales / idlengthscales, 1)\n    lengthscales = tf.reshape(lengthscales, (1, -1))\n    return tf.transpose(kernel.variance * tf.exp(-0.5 * d) * lengthscales)\n\n\n@Kuf.register(InducingPatches, Convolutional, object)\ndef Kuf_conv_patch(feat, kern, Xnew):\n    Xp = kern.get_patches(Xnew)  # [N, num_patches, patch_len]\n    bigKzx = kern.base_kernel.K(feat.Z, Xp)  # [M, N, P] -- thanks to broadcasting of kernels\n    Kzx = tf.reduce_sum(bigKzx * kern.weights if hasattr(kern, ""weights"") else bigKzx, [2])\n    return Kzx / kern.num_patches\n'"
gpflow/covariances/kuus.py,6,"b'import tensorflow as tf\n\nfrom ..inducing_variables import InducingPoints, Multiscale, InducingPatches\nfrom ..kernels import Kernel, SquaredExponential, Convolutional\nfrom .dispatch import Kuu\nfrom ..config import default_float\n\n\n@Kuu.register(InducingPoints, Kernel)\ndef Kuu_kernel_inducingpoints(inducing_variable: InducingPoints, kernel: Kernel, *, jitter=0.0):\n    Kzz = kernel(inducing_variable.Z)\n    Kzz += jitter * tf.eye(len(inducing_variable), dtype=Kzz.dtype)\n    return Kzz\n\n\n@Kuu.register(Multiscale, SquaredExponential)\ndef Kuu_sqexp_multiscale(inducing_variable: Multiscale, kernel: SquaredExponential, *, jitter=0.0):\n    Zmu, Zlen = kernel.slice(inducing_variable.Z, inducing_variable.scales)\n    idlengthscales2 = tf.square(kernel.lengthscales + Zlen)\n    sc = tf.sqrt(\n        idlengthscales2[None, ...] + idlengthscales2[:, None, ...] - kernel.lengthscales ** 2\n    )\n    d = inducing_variable._cust_square_dist(Zmu, Zmu, sc)\n    Kzz = kernel.variance * tf.exp(-d / 2) * tf.reduce_prod(kernel.lengthscales / sc, 2)\n    Kzz += jitter * tf.eye(len(inducing_variable), dtype=Kzz.dtype)\n    return Kzz\n\n\n@Kuu.register(InducingPatches, Convolutional)\ndef Kuu_conv_patch(feat, kern, jitter=0.0):\n    return kern.base_kernel.K(feat.Z) + jitter * tf.eye(len(feat), dtype=default_float())\n'"
gpflow/expectations/__init__.py,0,"b'from .expectations import expectation, quadrature_expectation\n\nfrom . import (\n    cross_kernels,\n    linears,\n    mean_functions,\n    misc,\n    products,\n    quadratures,\n    squared_exponentials,\n    sums,\n)\n\n__all__ = [""expectation"", ""quadrature_expectation""]\n'"
gpflow/expectations/cross_kernels.py,19,"b'import tensorflow as tf\n\nfrom . import dispatch\nfrom .. import kernels\nfrom ..inducing_variables import InducingPoints\nfrom ..probability_distributions import DiagonalGaussian, Gaussian\nfrom .expectations import expectation\n\n\n@dispatch.expectation.register(\n    (Gaussian, DiagonalGaussian),\n    kernels.SquaredExponential,\n    InducingPoints,\n    kernels.Linear,\n    InducingPoints,\n)\ndef _E(p, sqexp_kern, feat1, lin_kern, feat2, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <Ka_{Z1, x_n} Kb_{x_n, Z2}>_p(x_n)\n        - K_lin_{.,.} :: SqExp kernel\n        - K_sqexp_{.,.} :: Linear kernel\n    Different Z1 and Z2 are handled if p is diagonal and K_lin and K_sqexp have disjoint\n    active_dims, in which case the joint expectations simplify into a product of expectations\n\n    :return: NxM1xM2\n    """"""\n    if sqexp_kern.on_separate_dims(lin_kern) and isinstance(\n        p, DiagonalGaussian\n    ):  # no joint expectations required\n        eKxz1 = expectation(p, (sqexp_kern, feat1))\n        eKxz2 = expectation(p, (lin_kern, feat2))\n        return eKxz1[:, :, None] * eKxz2[:, None, :]\n\n    if feat1 != feat2:\n        raise NotImplementedError(""inducing_variables have to be the same for both kernels."")\n\n    if sqexp_kern.active_dims != lin_kern.active_dims:\n        raise NotImplementedError(""active_dims have to be the same for both kernels."")\n\n    # use only active dimensions\n    Xcov = sqexp_kern.slice_cov(tf.linalg.diag(p.cov) if isinstance(p, DiagonalGaussian) else p.cov)\n    Z, Xmu = sqexp_kern.slice(feat1.Z, p.mu)\n\n    N = tf.shape(Xmu)[0]\n    D = tf.shape(Xmu)[1]\n\n    def take_with_ard(value):\n        if not sqexp_kern.ard:\n            return tf.zeros((D,), dtype=value.dtype) + value\n        return value\n\n    lin_kern_variances = take_with_ard(lin_kern.variance)\n    sqexp_kern_lengthscales = take_with_ard(sqexp_kern.lengthscales)\n\n    chol_L_plus_Xcov = tf.linalg.cholesky(\n        tf.linalg.diag(sqexp_kern_lengthscales ** 2) + Xcov\n    )  # NxDxD\n\n    Z_transpose = tf.transpose(Z)\n    all_diffs = Z_transpose - tf.expand_dims(Xmu, 2)  # NxDxM\n    exponent_mahalanobis = tf.linalg.triangular_solve(\n        chol_L_plus_Xcov, all_diffs, lower=True\n    )  # NxDxM\n    exponent_mahalanobis = tf.reduce_sum(tf.square(exponent_mahalanobis), 1)  # NxM\n    exponent_mahalanobis = tf.exp(-0.5 * exponent_mahalanobis)  # NxM\n\n    sqrt_det_L = tf.reduce_prod(sqexp_kern_lengthscales)\n    sqrt_det_L_plus_Xcov = tf.exp(\n        tf.reduce_sum(tf.math.log(tf.linalg.diag_part(chol_L_plus_Xcov)), axis=1)\n    )\n    determinants = sqrt_det_L / sqrt_det_L_plus_Xcov  # N\n    eKxz_sqexp = sqexp_kern.variance * (\n        determinants[:, None] * exponent_mahalanobis\n    )  ## NxM <- End RBF eKxz code\n\n    tiled_Z = tf.tile(tf.expand_dims(Z_transpose, 0), (N, 1, 1))  # NxDxM\n    z_L_inv_Xcov = tf.linalg.matmul(\n        tiled_Z, Xcov / sqexp_kern_lengthscales[:, None] ** 2.0, transpose_a=True\n    )  # NxMxD\n\n    cross_eKzxKxz = tf.linalg.cholesky_solve(\n        chol_L_plus_Xcov, (lin_kern_variances * sqexp_kern_lengthscales ** 2.0)[..., None] * tiled_Z\n    )  # NxDxM\n\n    cross_eKzxKxz = tf.linalg.matmul(\n        (z_L_inv_Xcov + Xmu[:, None, :]) * eKxz_sqexp[..., None], cross_eKzxKxz\n    )  # NxMxM\n    return cross_eKzxKxz\n\n\n@dispatch.expectation.register(\n    (Gaussian, DiagonalGaussian),\n    kernels.Linear,\n    InducingPoints,\n    kernels.SquaredExponential,\n    InducingPoints,\n)\ndef _E(p, lin_kern, feat1, sqexp_kern, feat2, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <Ka_{Z1, x_n} Kb_{x_n, Z2}>_p(x_n)\n        - K_lin_{.,.} :: Linear kernel\n        - K_sqexp_{.,.} :: sqexp kernel\n    Different Z1 and Z2 are handled if p is diagonal and K_lin and K_sqexp have disjoint\n    active_dims, in which case the joint expectations simplify into a product of expectations\n\n    :return: NxM1xM2\n    """"""\n    return tf.linalg.adjoint(expectation(p, (sqexp_kern, feat2), (lin_kern, feat1)))\n'"
gpflow/expectations/dispatch.py,0,"b'from ..utilities import Dispatcher\n\nexpectation = Dispatcher(""expectation"")\nquadrature_expectation = Dispatcher(""quadrature_expectation"")\nvariational_expectation = Dispatcher(""variational_expectation"")\n'"
gpflow/expectations/expectations.py,0,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom ..probability_distributions import DiagonalGaussian, Gaussian, MarkovGaussian\nfrom . import dispatch\n\n\ndef expectation(p, obj1, obj2=None, nghp=None):\n    """"""\n    Compute the expectation <obj1(x) obj2(x)>_p(x)\n    Uses multiple-dispatch to select an analytical implementation,\n    if one is available. If not, it falls back to quadrature.\n\n    :type p: (mu, cov) tuple or a `ProbabilityDistribution` object\n    :type obj1: kernel, mean function, (kernel, inducing_variable), or None\n    :type obj2: kernel, mean function, (kernel, inducing_variable), or None\n    :param int nghp: passed to `_quadrature_expectation` to set the number\n                     of Gauss-Hermite points used: `num_gauss_hermite_points`\n    :return: a 1-D, 2-D, or 3-D tensor containing the expectation\n\n    Allowed combinations\n\n    - Psi statistics:\n        >>> eKdiag = expectation(p, kernel)  (N)  # Psi0\n        >>> eKxz = expectation(p, (kernel, inducing_variable))  (NxM)  # Psi1\n        >>> exKxz = expectation(p, identity_mean, (kernel, inducing_variable))  (NxDxM)\n        >>> eKzxKxz = expectation(p, (kernel, inducing_variable), (kernel, inducing_variable))  (NxMxM)  # Psi2\n\n    - kernels and mean functions:\n        >>> eKzxMx = expectation(p, (kernel, inducing_variable), mean)  (NxMxQ)\n        >>> eMxKxz = expectation(p, mean, (kernel, inducing_variable))  (NxQxM)\n\n    - only mean functions:\n        >>> eMx = expectation(p, mean)  (NxQ)\n        >>> eM1x_M2x = expectation(p, mean1, mean2)  (NxQ1xQ2)\n        .. note:: mean(x) is 1xQ (row vector)\n\n    - different kernels. This occurs, for instance, when we are calculating Psi2 for Sum kernels:\n        >>> eK1zxK2xz = expectation(p, (kern1, inducing_variable), (kern2, inducing_variable))  (NxMxM)\n    """"""\n    p, obj1, feat1, obj2, feat2 = _init_expectation(p, obj1, obj2)\n    try:\n        return dispatch.expectation(p, obj1, feat1, obj2, feat2, nghp=nghp)\n    except NotImplementedError as error:\n        return dispatch.quadrature_expectation(p, obj1, feat1, obj2, feat2, nghp=nghp)\n\n\ndef quadrature_expectation(p, obj1, obj2=None, nghp=None):\n    """"""\n    Compute the expectation <obj1(x) obj2(x)>_p(x)\n    Uses Gauss-Hermite quadrature for approximate integration.\n\n    :type p: (mu, cov) tuple or a `ProbabilityDistribution` object\n    :type obj1: kernel, mean function, (kernel, inducing_variable), or None\n    :type obj2: kernel, mean function, (kernel, inducing_variable), or None\n    :param int num_gauss_hermite_points: passed to `_quadrature_expectation` to set\n                                         the number of Gauss-Hermite points used\n    :return: a 1-D, 2-D, or 3-D tensor containing the expectation\n    """"""\n    print(f""2. p={p}, obj1={obj1}, obj2={obj2}"")\n    p, obj1, feat1, obj2, feat2 = _init_expectation(p, obj1, obj2)\n    return dispatch.quadrature_expectation(p, obj1, feat1, obj2, feat2, nghp=nghp)\n\n\ndef _init_expectation(p, obj1, obj2):\n    if isinstance(p, tuple):\n        mu, cov = p\n        classes = [DiagonalGaussian, Gaussian, MarkovGaussian]\n        p = classes[cov.ndim - 2](*p)\n\n    obj1, feat1 = obj1 if isinstance(obj1, tuple) else (obj1, None)\n    obj2, feat2 = obj2 if isinstance(obj2, tuple) else (obj2, None)\n    return p, obj1, feat1, obj2, feat2\n'"
gpflow/expectations/linears.py,13,"b'import tensorflow as tf\n\nfrom .. import kernels\nfrom .. import mean_functions as mfn\nfrom ..inducing_variables import InducingPoints\nfrom ..probability_distributions import DiagonalGaussian, Gaussian, MarkovGaussian\nfrom . import dispatch\nfrom .expectations import expectation\n\n\nNoneType = type(None)\n\n\n@dispatch.expectation.register(Gaussian, kernels.Linear, NoneType, NoneType, NoneType)\ndef _E(p, kernel, _, __, ___, nghp=None):\n    """"""\n    Compute the expectation:\n    <diag(K_{X, X})>_p(X)\n        - K_{.,.} :: Linear kernel\n\n    :return: N\n    """"""\n    # use only active dimensions\n    Xmu, _ = kernel.slice(p.mu, None)\n    Xcov = kernel.slice_cov(p.cov)\n\n    return tf.reduce_sum(kernel.variance * (tf.linalg.diag_part(Xcov) + Xmu ** 2), 1)\n\n\n@dispatch.expectation.register(Gaussian, kernels.Linear, InducingPoints, NoneType, NoneType)\ndef _E(p, kernel, inducing_variable, _, __, nghp=None):\n    """"""\n    Compute the expectation:\n    <K_{X, Z}>_p(X)\n        - K_{.,.} :: Linear kernel\n\n    :return: NxM\n    """"""\n    # use only active dimensions\n    Z, Xmu = kernel.slice(inducing_variable.Z, p.mu)\n\n    return tf.linalg.matmul(Xmu, Z * kernel.variance, transpose_b=True)\n\n\n@dispatch.expectation.register(Gaussian, kernels.Linear, InducingPoints, mfn.Identity, NoneType)\ndef _E(p, kernel, inducing_variable, mean, _, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <K_{Z, x_n} x_n^T>_p(x_n)\n        - K_{.,.} :: Linear kernel\n\n    :return: NxMxD\n    """"""\n    Xmu, Xcov = p.mu, p.cov\n\n    N = tf.shape(Xmu)[0]\n    var_Z = kernel.variance * inducing_variable.Z  # MxD\n    tiled_Z = tf.tile(tf.expand_dims(var_Z, 0), (N, 1, 1))  # NxMxD\n    return tf.linalg.matmul(tiled_Z, Xcov + (Xmu[..., None] * Xmu[:, None, :]))\n\n\n@dispatch.expectation.register(\n    MarkovGaussian, kernels.Linear, InducingPoints, mfn.Identity, NoneType\n)\ndef _E(p, kernel, inducing_variable, mean, _, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <K_{Z, x_n} x_{n+1}^T>_p(x_{n:n+1})\n        - K_{.,.} :: Linear kernel\n        - p       :: MarkovGaussian distribution (p.cov 2x(N+1)xDxD)\n\n    :return: NxMxD\n    """"""\n    Xmu, Xcov = p.mu, p.cov\n\n    N = tf.shape(Xmu)[0] - 1\n    var_Z = kernel.variance * inducing_variable.Z  # MxD\n    tiled_Z = tf.tile(tf.expand_dims(var_Z, 0), (N, 1, 1))  # NxMxD\n    eXX = Xcov[1, :-1] + (Xmu[:-1][..., None] * Xmu[1:][:, None, :])  # NxDxD\n    return tf.linalg.matmul(tiled_Z, eXX)\n\n\n@dispatch.expectation.register(\n    (Gaussian, DiagonalGaussian), kernels.Linear, InducingPoints, kernels.Linear, InducingPoints\n)\ndef _E(p, kern1, feat1, kern2, feat2, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <Ka_{Z1, x_n} Kb_{x_n, Z2}>_p(x_n)\n        - Ka_{.,.}, Kb_{.,.} :: Linear kernels\n    Ka and Kb as well as Z1 and Z2 can differ from each other, but this is supported\n    only if the Gaussian p is Diagonal (p.cov NxD) and Ka, Kb have disjoint active_dims\n    in which case the joint expectations simplify into a product of expectations\n\n    :return: NxMxM\n    """"""\n    if kern1.on_separate_dims(kern2) and isinstance(\n        p, DiagonalGaussian\n    ):  # no joint expectations required\n        eKxz1 = expectation(p, (kern1, feat1))\n        eKxz2 = expectation(p, (kern2, feat2))\n        return eKxz1[:, :, None] * eKxz2[:, None, :]\n\n    if kern1 != kern2 or feat1 != feat2:\n        raise NotImplementedError(\n            ""The expectation over two kernels has only an ""\n            ""analytical implementation if both kernels are equal.""\n        )\n\n    kernel = kern1\n    inducing_variable = feat1\n\n    # use only active dimensions\n    Xcov = kernel.slice_cov(tf.linalg.diag(p.cov) if isinstance(p, DiagonalGaussian) else p.cov)\n    Z, Xmu = kernel.slice(inducing_variable.Z, p.mu)\n\n    N = tf.shape(Xmu)[0]\n    var_Z = kernel.variance * Z\n    tiled_Z = tf.tile(tf.expand_dims(var_Z, 0), (N, 1, 1))  # NxMxD\n    XX = Xcov + tf.expand_dims(Xmu, 1) * tf.expand_dims(Xmu, 2)  # NxDxD\n    return tf.linalg.matmul(tf.linalg.matmul(tiled_Z, XX), tiled_Z, transpose_b=True)\n'"
gpflow/expectations/mean_functions.py,8,"b'# noqa: F811\n\nimport tensorflow as tf\n\nfrom .. import mean_functions as mfn\nfrom ..probability_distributions import Gaussian\nfrom . import dispatch\nfrom .expectations import expectation\n\nNoneType = type(None)\n\n\n@dispatch.expectation.register(Gaussian, (mfn.Linear, mfn.Constant), NoneType, NoneType, NoneType)\ndef _E(p, mean, _, __, ___, nghp=None):\n    """"""\n    Compute the expectation:\n    <m(X)>_p(X)\n        - m(x) :: Linear, Identity or Constant mean function\n\n    :return: NxQ\n    """"""\n    return mean(p.mu)\n\n\n@dispatch.expectation.register(Gaussian, mfn.Constant, NoneType, mfn.Constant, NoneType)\ndef _E(p, mean1, _, mean2, __, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m1(x_n)^T m2(x_n)>_p(x_n)\n        - m1(.), m2(.) :: Constant mean functions\n\n    :return: NxQ1xQ2\n    """"""\n    return mean1(p.mu)[:, :, None] * mean2(p.mu)[:, None, :]\n\n\n@dispatch.expectation.register(Gaussian, mfn.Constant, NoneType, mfn.MeanFunction, NoneType)\ndef _E(p, mean1, _, mean2, __, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m1(x_n)^T m2(x_n)>_p(x_n)\n        - m1(.) :: Constant mean function\n        - m2(.) :: General mean function\n\n    :return: NxQ1xQ2\n    """"""\n    e_mean2 = expectation(p, mean2)\n    return mean1(p.mu)[:, :, None] * e_mean2[:, None, :]\n\n\n@dispatch.expectation.register(Gaussian, mfn.MeanFunction, NoneType, mfn.Constant, NoneType)\ndef _E(p, mean1, _, mean2, __, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m1(x_n)^T m2(x_n)>_p(x_n)\n        - m1(.) :: General mean function\n        - m2(.) :: Constant mean function\n\n    :return: NxQ1xQ2\n    """"""\n    e_mean1 = expectation(p, mean1)\n    return e_mean1[:, :, None] * mean2(p.mu)[:, None, :]\n\n\n@dispatch.expectation.register(Gaussian, mfn.Identity, NoneType, mfn.Identity, NoneType)\ndef _E(p, mean1, _, mean2, __, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m1(x_n)^T m2(x_n)>_p(x_n)\n        - m1(.), m2(.) :: Identity mean functions\n\n    :return: NxDxD\n    """"""\n    return p.cov + (p.mu[:, :, None] * p.mu[:, None, :])\n\n\n@dispatch.expectation.register(Gaussian, mfn.Identity, NoneType, mfn.Linear, NoneType)\ndef _E(p, mean1, _, mean2, __, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m1(x_n)^T m2(x_n)>_p(x_n)\n        - m1(.) :: Identity mean function\n        - m2(.) :: Linear mean function\n\n    :return: NxDxQ\n    """"""\n    N = tf.shape(p.mu)[0]\n    e_xxt = p.cov + (p.mu[:, :, None] * p.mu[:, None, :])  # NxDxD\n    e_xxt_A = tf.linalg.matmul(e_xxt, tf.tile(mean2.A[None, ...], (N, 1, 1)))  # NxDxQ\n    e_x_bt = p.mu[:, :, None] * mean2.b[None, None, :]  # NxDxQ\n\n    return e_xxt_A + e_x_bt\n\n\n@dispatch.expectation.register(Gaussian, mfn.Linear, NoneType, mfn.Identity, NoneType)\ndef _E(p, mean1, _, mean2, __, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m1(x_n)^T m2(x_n)>_p(x_n)\n        - m1(.) :: Linear mean function\n        - m2(.) :: Identity mean function\n\n    :return: NxQxD\n    """"""\n    N = tf.shape(p.mu)[0]\n    e_xxt = p.cov + (p.mu[:, :, None] * p.mu[:, None, :])  # NxDxD\n    e_A_xxt = tf.linalg.matmul(\n        tf.tile(mean1.A[None, ...], (N, 1, 1)), e_xxt, transpose_a=True\n    )  # NxQxD\n    e_b_xt = mean1.b[None, :, None] * p.mu[:, None, :]  # NxQxD\n\n    return e_A_xxt + e_b_xt\n\n\n@dispatch.expectation.register(Gaussian, mfn.Linear, NoneType, mfn.Linear, NoneType)\ndef _E(p, mean1, _, mean2, __, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m1(x_n)^T m2(x_n)>_p(x_n)\n        - m1(.), m2(.) :: Linear mean functions\n\n    :return: NxQ1xQ2\n    """"""\n    e_xxt = p.cov + (p.mu[:, :, None] * p.mu[:, None, :])  # NxDxD\n    e_A1t_xxt_A2 = tf.einsum(""iq,nij,jz->nqz"", mean1.A, e_xxt, mean2.A)  # NxQ1xQ2\n    e_A1t_x_b2t = tf.einsum(""iq,ni,z->nqz"", mean1.A, p.mu, mean2.b)  # NxQ1xQ2\n    e_b1_xt_A2 = tf.einsum(""q,ni,iz->nqz"", mean1.b, p.mu, mean2.A)  # NxQ1xQ2\n    e_b1_b2t = mean1.b[:, None] * mean2.b[None, :]  # Q1xQ2\n\n    return e_A1t_xxt_A2 + e_A1t_x_b2t + e_b1_xt_A2 + e_b1_b2t\n'"
gpflow/expectations/misc.py,7,"b'import tensorflow as tf\n\nfrom .. import kernels\nfrom .. import mean_functions as mfn\nfrom ..inducing_variables import InducingVariables, InducingPoints\nfrom ..probability_distributions import DiagonalGaussian, Gaussian, MarkovGaussian\nfrom . import dispatch\nfrom .expectations import expectation\n\n\nNoneType = type(None)\n\n# ================ exKxz transpose and mean function handling =================\n\n\n@dispatch.expectation.register(\n    (Gaussian, MarkovGaussian), mfn.Identity, NoneType, kernels.Linear, InducingPoints\n)\ndef _E(p, mean, _, kernel, inducing_variable, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <x_n K_{x_n, Z}>_p(x_n)\n        - K_{.,} :: Linear kernel\n    or the equivalent for MarkovGaussian\n\n    :return: NxDxM\n    """"""\n    return tf.linalg.adjoint(expectation(p, (kernel, inducing_variable), mean))\n\n\n@dispatch.expectation.register(\n    (Gaussian, MarkovGaussian), kernels.Kernel, InducingVariables, mfn.MeanFunction, NoneType\n)\ndef _E(p, kernel, inducing_variable, mean, _, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <K_{Z, x_n} m(x_n)>_p(x_n)\n    or the equivalent for MarkovGaussian\n\n    :return: NxMxQ\n    """"""\n    return tf.linalg.adjoint(expectation(p, mean, (kernel, inducing_variable), nghp=nghp))\n\n\n@dispatch.expectation.register(Gaussian, mfn.Constant, NoneType, kernels.Kernel, InducingPoints)\ndef _E(p, constant_mean, _, kernel, inducing_variable, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m(x_n)^T K_{x_n, Z}>_p(x_n)\n        - m(x_i) = c :: Constant function\n        - K_{.,.}    :: Kernel function\n\n    :return: NxQxM\n    """"""\n    c = constant_mean(p.mu)  # NxQ\n    eKxz = expectation(p, (kernel, inducing_variable), nghp=nghp)  # NxM\n\n    return c[..., None] * eKxz[:, None, :]\n\n\n@dispatch.expectation.register(Gaussian, mfn.Linear, NoneType, kernels.Kernel, InducingPoints)\ndef _E(p, linear_mean, _, kernel, inducing_variable, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <m(x_n)^T K_{x_n, Z}>_p(x_n)\n        - m(x_i) = A x_i + b :: Linear mean function\n        - K_{.,.}            :: Kernel function\n\n    :return: NxQxM\n    """"""\n    N = tf.shape(p.mu)[0]\n    D = tf.shape(p.mu)[1]\n    exKxz = expectation(p, mfn.Identity(D), (kernel, inducing_variable), nghp=nghp)\n    eKxz = expectation(p, (kernel, inducing_variable), nghp=nghp)\n    eAxKxz = tf.linalg.matmul(\n        tf.tile(linear_mean.A[None, :, :], (N, 1, 1)), exKxz, transpose_a=True\n    )\n    ebKxz = linear_mean.b[None, :, None] * eKxz[:, None, :]\n    return eAxKxz + ebKxz\n\n\n@dispatch.expectation.register(Gaussian, mfn.Identity, NoneType, kernels.Kernel, InducingPoints)\ndef _E(p, identity_mean, _, kernel, inducing_variable, nghp=None):\n    """"""\n    This prevents infinite recursion for kernels that don\'t have specific\n    implementations of _expectation(p, identity_mean, None, kernel, inducing_variable).\n    Recursion can arise because Identity is a subclass of Linear mean function\n    so _expectation(p, linear_mean, none, kernel, inducing_variable) would call itself.\n    More specific signatures (e.g. (p, identity_mean, None, RBF, inducing_variable)) will\n    be found and used whenever available\n    """"""\n    raise NotImplementedError\n\n\n# ============== Conversion to Gaussian from Diagonal or Markov ===============\n# Catching missing DiagonalGaussian implementations by converting to full Gaussian:\n\n\n@dispatch.expectation.register(\n    DiagonalGaussian, object, (InducingVariables, NoneType), object, (InducingVariables, NoneType)\n)\ndef _E(p, obj1, feat1, obj2, feat2, nghp=None):\n    gaussian = Gaussian(p.mu, tf.linalg.diag(p.cov))\n    return expectation(gaussian, (obj1, feat1), (obj2, feat2), nghp=nghp)\n\n\n# Catching missing MarkovGaussian implementations by converting to Gaussian (when indifferent):\n\n\n@dispatch.expectation.register(\n    MarkovGaussian, object, (InducingVariables, NoneType), object, (InducingVariables, NoneType)\n)\ndef _E(p, obj1, feat1, obj2, feat2, nghp=None):\n    """"""\n    Nota Bene: if only one object is passed, obj1 is\n    associated with x_n, whereas obj2 with x_{n+1}\n\n    """"""\n    if obj2 is None:\n        gaussian = Gaussian(p.mu[:-1], p.cov[0, :-1])\n        return expectation(gaussian, (obj1, feat1), nghp=nghp)\n    elif obj1 is None:\n        gaussian = Gaussian(p.mu[1:], p.cov[0, 1:])\n        return expectation(gaussian, (obj2, feat2), nghp=nghp)\n    else:\n        return expectation(p, (obj1, feat1), (obj2, feat2), nghp=nghp)\n'"
gpflow/expectations/products.py,3,"b'from functools import reduce\n\nimport tensorflow as tf\n\nfrom .. import kernels\nfrom ..inducing_variables import InducingPoints\nfrom ..probability_distributions import DiagonalGaussian\nfrom . import dispatch\nfrom .expectations import expectation\n\nNoneType = type(None)\n\n\n@dispatch.expectation.register(DiagonalGaussian, kernels.Product, NoneType, NoneType, NoneType)\ndef _E(p, kernel, _, __, ___, nghp=None):\n    r""""""\n    Compute the expectation:\n    <\\HadamardProd_i diag(Ki_{X[:, active_dims_i], X[:, active_dims_i]})>_p(X)\n        - \\HadamardProd_i Ki_{.,.} :: Product kernel\n        - p                        :: DiagonalGaussian distribution (p.cov NxD)\n\n    :return: N\n    """"""\n    if not kernel.on_separate_dimensions:\n        raise NotImplementedError(\n            ""Product currently needs to be defined on separate dimensions.""\n        )  # pragma: no cover\n\n    exps = [expectation(p, k, nghp=nghp) for k in kernel.kernels]\n    return reduce(tf.multiply, exps)\n\n\n@dispatch.expectation.register(\n    DiagonalGaussian, kernels.Product, InducingPoints, NoneType, NoneType\n)\ndef _E(p, kernel, inducing_variable, __, ___, nghp=None):\n    r""""""\n    Compute the expectation:\n    <\\HadamardProd_i Ki_{X[:, active_dims_i], Z[:, active_dims_i]}>_p(X)\n        - \\HadamardProd_i Ki_{.,.} :: Product kernel\n        - p                        :: DiagonalGaussian distribution (p.cov NxD)\n\n    :return: NxM\n    """"""\n    if not kernel.on_separate_dimensions:\n        raise NotImplementedError(\n            ""Product currently needs to be defined on separate dimensions.""\n        )  # pragma: no cover\n\n    exps = [expectation(p, (k, inducing_variable), nghp=nghp) for k in kernel.kernels]\n    return reduce(tf.multiply, exps)\n\n\n@dispatch.expectation.register(\n    DiagonalGaussian, kernels.Product, InducingPoints, kernels.Product, InducingPoints\n)\ndef _E(p, kern1, feat1, kern2, feat2, nghp=None):\n    r""""""\n    Compute the expectation:\n    expectation[n] = < prodK_{Z, x_n} prodK_{x_n, Z} >_p(x_n)\n                   = < (\\HadamardProd_i Ki_{Z[:, active_dims_i], x[n, active_dims_i]})  <-- Mx1\n               1xM -->  (\\HadamardProd_j Kj_{x[n, active_dims_j], Z[:, active_dims_j]}) >_p(x_n)  (MxM)\n\n        - \\HadamardProd_i Ki_{.,.}, \\HadamardProd_j Kj_{.,.} :: Product kernels\n        - p                        :: DiagonalGaussian distribution (p.cov NxD)\n\n    :return: NxMxM\n    """"""\n    if feat1 != feat2:\n        raise NotImplementedError(""Different inducing variables are not supported."")\n    if kern1 != kern2:\n        raise NotImplementedError(\n            ""Calculating the expectation over two "" ""different Product kernels is not supported.""\n        )\n\n    kernel = kern1\n    inducing_variable = feat1\n\n    if not kernel.on_separate_dimensions:\n        raise NotImplementedError(\n            ""Product currently needs to be defined on separate dimensions.""\n        )  # pragma: no cover\n\n    exps = [\n        expectation(p, (k, inducing_variable), (k, inducing_variable), nghp=nghp)\n        for k in kernel.kernels\n    ]\n    return reduce(tf.multiply, exps)\n'"
gpflow/expectations/quadratures.py,8,"b'import numpy as np\nimport tensorflow as tf\n\nfrom .. import kernels\nfrom .. import mean_functions as mfn\nfrom ..covariances import Kuf\nfrom ..inducing_variables import InducingVariables\nfrom ..probability_distributions import DiagonalGaussian, Gaussian, MarkovGaussian\nfrom ..quadrature import mvnquad\nfrom . import dispatch\nfrom .expectations import quadrature_expectation\n\nregister = dispatch.quadrature_expectation.register\n\n\nNoneType = type(None)\n\n\ndef get_eval_func(obj, inducing_variable, slice=None):\n    """"""\n    Return the function of interest (kernel or mean) for the expectation\n    depending on the type of :obj: and whether any inducing are given\n    """"""\n\n    slice = ... if slice is None else slice\n    if inducing_variable is not None:\n        # kernel + inducing_variable combination\n        if not isinstance(inducing_variable, InducingVariables) or not isinstance(\n            obj, kernels.Kernel\n        ):\n            raise TypeError(""If `inducing_variable` is supplied, `obj` must be a kernel."")\n        return lambda x: tf.transpose(Kuf(inducing_variable, obj, x))[slice]\n    elif isinstance(obj, mfn.MeanFunction):\n        return lambda x: obj(x)[slice]\n    elif isinstance(obj, kernels.Kernel):\n        return lambda x: obj(x, full_cov=False)\n\n    raise NotImplementedError()\n\n\n@dispatch.quadrature_expectation.register(\n    (Gaussian, DiagonalGaussian),\n    object,\n    (InducingVariables, NoneType),\n    object,\n    (InducingVariables, NoneType),\n)\ndef _quadrature_expectation(p, obj1, inducing_variable1, obj2, inducing_variable2, nghp=None):\n    """"""\n    General handling of quadrature expectations for Gaussians and DiagonalGaussians\n    Fallback method for missing analytic expectations\n    """"""\n    nghp = 100 if nghp is None else nghp\n\n    # logger.warning(\n    #     ""Quadrature is used to calculate the expectation. This means that ""\n    #     ""an analytical implementations is not available for the given combination.""\n    # )\n\n    if obj1 is None:\n        raise NotImplementedError(""First object cannot be None."")\n\n    if not isinstance(p, DiagonalGaussian):\n        cov = p.cov\n    else:\n        iskern1 = isinstance(obj1, kernels.Kernel)\n        iskern2 = isinstance(obj2, kernels.Kernel)\n        if iskern1 and iskern2 and obj1.on_separate_dims(obj2):  # no joint expectations required\n            eKxz1 = quadrature_expectation(p, (obj1, inducing_variable1), nghp=nghp)\n            eKxz2 = quadrature_expectation(p, (obj2, inducing_variable2), nghp=nghp)\n            return eKxz1[:, :, None] * eKxz2[:, None, :]\n        cov = tf.linalg.diag(p.cov)\n\n    if obj2 is None:\n\n        def eval_func(x):\n            fn = get_eval_func(obj1, inducing_variable1)\n            return fn(x)\n\n    else:\n\n        def eval_func(x):\n            fn1 = get_eval_func(obj1, inducing_variable1, np.s_[:, :, None])\n            fn2 = get_eval_func(obj2, inducing_variable2, np.s_[:, None, :])\n            return fn1(x) * fn2(x)\n\n    return mvnquad(eval_func, p.mu, cov, nghp)\n\n\n@dispatch.quadrature_expectation.register(\n    MarkovGaussian, object, (InducingVariables, NoneType), object, (InducingVariables, NoneType)\n)\ndef _quadrature_expectation(p, obj1, inducing_variable1, obj2, inducing_variable2, nghp=None):\n    """"""\n    Handling of quadrature expectations for Markov Gaussians (useful for time series)\n    Fallback method for missing analytic expectations wrt Markov Gaussians\n    Nota Bene: obj1 is always associated with x_n, whereas obj2 always with x_{n+1}\n               if one requires e.g. <x_{n+1} K_{x_n, Z}>_p(x_{n:n+1}), compute the\n               transpose and then transpose the result of the expectation\n    """"""\n    nghp = 40 if nghp is None else nghp\n\n    # logger.warning(\n    #     ""Quadrature is used to calculate the expectation. This means that ""\n    #     ""an analytical implementations is not available for the given combination.""\n    # )\n\n    if obj2 is None:\n\n        def eval_func(x):\n            return get_eval_func(obj1, inducing_variable1)(x)\n\n        mu, cov = p.mu[:-1], p.cov[0, :-1]  # cross covariances are not needed\n    elif obj1 is None:\n\n        def eval_func(x):\n            return get_eval_func(obj2, inducing_variable2)(x)\n\n        mu, cov = p.mu[1:], p.cov[0, 1:]  # cross covariances are not needed\n    else:\n\n        def eval_func(x):\n            x1 = tf.split(x, 2, 1)[0]\n            x2 = tf.split(x, 2, 1)[1]\n            res1 = get_eval_func(obj1, inducing_variable1, np.s_[:, :, None])(x1)\n            res2 = get_eval_func(obj2, inducing_variable2, np.s_[:, None, :])(x2)\n            return res1 * res2\n\n        mu = tf.concat((p.mu[:-1, :], p.mu[1:, :]), 1)  # Nx2D\n        cov_top = tf.concat((p.cov[0, :-1, :, :], p.cov[1, :-1, :, :]), 2)  # NxDx2D\n        cov_bottom = tf.concat((tf.linalg.adjoint(p.cov[1, :-1, :, :]), p.cov[0, 1:, :, :]), 2)\n        cov = tf.concat((cov_top, cov_bottom), 1)  # Nx2Dx2D\n\n    return mvnquad(eval_func, mu, cov, nghp)\n'"
gpflow/expectations/squared_exponentials.py,55,"b'import tensorflow as tf\n\nfrom .. import kernels\nfrom .. import mean_functions as mfn\nfrom ..inducing_variables import InducingPoints\nfrom ..probability_distributions import DiagonalGaussian, Gaussian, MarkovGaussian\nfrom ..utilities.ops import square_distance\nfrom . import dispatch\nfrom .expectations import expectation\n\nNoneType = type(None)\n\n\n@dispatch.expectation.register(Gaussian, kernels.SquaredExponential, NoneType, NoneType, NoneType)\ndef _E(p, kernel, _, __, ___, nghp=None):\n    """"""\n    Compute the expectation:\n    <diag(K_{X, X})>_p(X)\n        - K_{.,.} :: RBF kernel\n\n    :return: N\n    """"""\n    return kernel(p.mu, full_cov=False)\n\n\n@dispatch.expectation.register(\n    Gaussian, kernels.SquaredExponential, InducingPoints, NoneType, NoneType\n)\ndef _E(p, kernel, inducing_variable, _, __, nghp=None):\n    """"""\n    Compute the expectation:\n    <K_{X, Z}>_p(X)\n        - K_{.,.} :: RBF kernel\n\n    :return: NxM\n    """"""\n    # use only active dimensions\n    Xcov = kernel.slice_cov(p.cov)\n    Z, Xmu = kernel.slice(inducing_variable.Z, p.mu)\n    D = tf.shape(Xmu)[1]\n\n    lengthscales = kernel.lengthscales\n    if not kernel.ard:\n        lengthscales = tf.zeros((D,), dtype=lengthscales.dtype) + kernel.lengthscales\n\n    chol_L_plus_Xcov = tf.linalg.cholesky(tf.linalg.diag(lengthscales ** 2) + Xcov)  # NxDxD\n\n    all_diffs = tf.transpose(Z) - tf.expand_dims(Xmu, 2)  # NxDxM\n    exponent_mahalanobis = tf.linalg.triangular_solve(\n        chol_L_plus_Xcov, all_diffs, lower=True\n    )  # NxDxM\n    exponent_mahalanobis = tf.reduce_sum(tf.square(exponent_mahalanobis), 1)  # NxM\n    exponent_mahalanobis = tf.exp(-0.5 * exponent_mahalanobis)  # NxM\n\n    sqrt_det_L = tf.reduce_prod(lengthscales)\n    sqrt_det_L_plus_Xcov = tf.exp(\n        tf.reduce_sum(tf.math.log(tf.linalg.diag_part(chol_L_plus_Xcov)), axis=1)\n    )\n    determinants = sqrt_det_L / sqrt_det_L_plus_Xcov  # N\n\n    return kernel.variance * (determinants[:, None] * exponent_mahalanobis)\n\n\n@dispatch.expectation.register(\n    Gaussian, mfn.Identity, NoneType, kernels.SquaredExponential, InducingPoints\n)\ndef _E(p, mean, _, kernel, inducing_variable, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <x_n K_{x_n, Z}>_p(x_n)\n        - K_{.,.} :: RBF kernel\n\n    :return: NxDxM\n    """"""\n    Xmu, Xcov = p.mu, p.cov\n\n    D = tf.shape(Xmu)[1]\n\n    lengthscales = kernel.lengthscales\n    if not kernel.ard:\n        lengthscales = tf.zeros((D,), dtype=lengthscales.dtype) + lengthscales\n\n    chol_L_plus_Xcov = tf.linalg.cholesky(tf.linalg.diag(lengthscales ** 2) + Xcov)  # NxDxD\n    all_diffs = tf.transpose(inducing_variable.Z) - tf.expand_dims(Xmu, 2)  # NxDxM\n\n    sqrt_det_L = tf.reduce_prod(lengthscales)\n    sqrt_det_L_plus_Xcov = tf.exp(\n        tf.reduce_sum(tf.math.log(tf.linalg.diag_part(chol_L_plus_Xcov)), axis=1)\n    )\n    determinants = sqrt_det_L / sqrt_det_L_plus_Xcov  # N\n\n    exponent_mahalanobis = tf.linalg.cholesky_solve(chol_L_plus_Xcov, all_diffs)  # NxDxM\n    non_exponent_term = tf.linalg.matmul(Xcov, exponent_mahalanobis, transpose_a=True)\n    non_exponent_term = tf.expand_dims(Xmu, 2) + non_exponent_term  # NxDxM\n\n    exponent_mahalanobis = tf.reduce_sum(all_diffs * exponent_mahalanobis, 1)  # NxM\n    exponent_mahalanobis = tf.exp(-0.5 * exponent_mahalanobis)  # NxM\n\n    return (\n        kernel.variance\n        * (determinants[:, None] * exponent_mahalanobis)[:, None, :]\n        * non_exponent_term\n    )\n\n\n@dispatch.expectation.register(\n    MarkovGaussian, mfn.Identity, NoneType, kernels.SquaredExponential, InducingPoints\n)\ndef _E(p, mean, _, kernel, inducing_variable, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <x_{n+1} K_{x_n, Z}>_p(x_{n:n+1})\n        - K_{.,.} :: RBF kernel\n        - p       :: MarkovGaussian distribution (p.cov 2x(N+1)xDxD)\n\n    :return: NxDxM\n    """"""\n    Xmu, Xcov = p.mu, p.cov\n\n    D = tf.shape(Xmu)[1]\n    lengthscales = kernel.lengthscales\n    if not kernel.ard:\n        lengthscales = tf.zeros((D,), dtype=lengthscales.dtype) + lengthscales\n\n    chol_L_plus_Xcov = tf.linalg.cholesky(tf.linalg.diag(lengthscales ** 2) + Xcov[0, :-1])  # NxDxD\n    all_diffs = tf.transpose(inducing_variable.Z) - tf.expand_dims(Xmu[:-1], 2)  # NxDxM\n\n    sqrt_det_L = tf.reduce_prod(lengthscales)\n    sqrt_det_L_plus_Xcov = tf.exp(\n        tf.reduce_sum(tf.math.log(tf.linalg.diag_part(chol_L_plus_Xcov)), axis=1)\n    )\n    determinants = sqrt_det_L / sqrt_det_L_plus_Xcov  # N\n\n    exponent_mahalanobis = tf.linalg.cholesky_solve(chol_L_plus_Xcov, all_diffs)  # NxDxM\n    non_exponent_term = tf.linalg.matmul(Xcov[1, :-1], exponent_mahalanobis, transpose_a=True)\n    non_exponent_term = tf.expand_dims(Xmu[1:], 2) + non_exponent_term  # NxDxM\n\n    exponent_mahalanobis = tf.reduce_sum(all_diffs * exponent_mahalanobis, 1)  # NxM\n    exponent_mahalanobis = tf.exp(-0.5 * exponent_mahalanobis)  # NxM\n\n    return (\n        kernel.variance\n        * (determinants[:, None] * exponent_mahalanobis)[:, None, :]\n        * non_exponent_term\n    )\n\n\n@dispatch.expectation.register(\n    (Gaussian, DiagonalGaussian),\n    kernels.SquaredExponential,\n    InducingPoints,\n    kernels.SquaredExponential,\n    InducingPoints,\n)\ndef _E(p, kern1, feat1, kern2, feat2, nghp=None):\n    """"""\n    Compute the expectation:\n    expectation[n] = <Ka_{Z1, x_n} Kb_{x_n, Z2}>_p(x_n)\n        - Ka_{.,.}, Kb_{.,.} :: RBF kernels\n    Ka and Kb as well as Z1 and Z2 can differ from each other, but this is supported\n    only if the Gaussian p is Diagonal (p.cov NxD) and Ka, Kb have disjoint active_dims\n    in which case the joint expectations simplify into a product of expectations\n\n    :return: NxMxM\n    """"""\n    if kern1.on_separate_dims(kern2) and isinstance(\n        p, DiagonalGaussian\n    ):  # no joint expectations required\n        eKxz1 = expectation(p, (kern1, feat1))\n        eKxz2 = expectation(p, (kern2, feat2))\n        return eKxz1[:, :, None] * eKxz2[:, None, :]\n\n    if feat1 != feat2 or kern1 != kern2:\n        raise NotImplementedError(\n            ""The expectation over two kernels has only an ""\n            ""analytical implementation if both kernels are equal.""\n        )\n\n    kernel = kern1\n    inducing_variable = feat1\n\n    # use only active dimensions\n    Xcov = kernel.slice_cov(tf.linalg.diag(p.cov) if isinstance(p, DiagonalGaussian) else p.cov)\n    Z, Xmu = kernel.slice(inducing_variable.Z, p.mu)\n\n    N = tf.shape(Xmu)[0]\n    D = tf.shape(Xmu)[1]\n\n    squared_lengthscales = kernel.lengthscales ** 2\n    if not kernel.ard:\n        zero_lengthscales = tf.zeros((D,), dtype=squared_lengthscales.dtype)\n        squared_lengthscales = squared_lengthscales + zero_lengthscales\n\n    sqrt_det_L = tf.reduce_prod(0.5 * squared_lengthscales) ** 0.5\n    C = tf.linalg.cholesky(0.5 * tf.linalg.diag(squared_lengthscales) + Xcov)  # NxDxD\n    dets = sqrt_det_L / tf.exp(tf.reduce_sum(tf.math.log(tf.linalg.diag_part(C)), axis=1))  # N\n\n    C_inv_mu = tf.linalg.triangular_solve(C, tf.expand_dims(Xmu, 2), lower=True)  # NxDx1\n    C_inv_z = tf.linalg.triangular_solve(\n        C, tf.tile(tf.expand_dims(0.5 * tf.transpose(Z), 0), [N, 1, 1]), lower=True\n    )  # NxDxM\n    mu_CC_inv_mu = tf.expand_dims(tf.reduce_sum(tf.square(C_inv_mu), 1), 2)  # Nx1x1\n    z_CC_inv_z = tf.reduce_sum(tf.square(C_inv_z), 1)  # NxM\n    zm_CC_inv_zn = tf.linalg.matmul(C_inv_z, C_inv_z, transpose_a=True)  # NxMxM\n    two_z_CC_inv_mu = 2 * tf.linalg.matmul(C_inv_z, C_inv_mu, transpose_a=True)[:, :, 0]  # NxM\n    # NxMxM\n    exponent_mahalanobis = (\n        mu_CC_inv_mu\n        + tf.expand_dims(z_CC_inv_z, 1)\n        + tf.expand_dims(z_CC_inv_z, 2)\n        + 2 * zm_CC_inv_zn\n        - tf.expand_dims(two_z_CC_inv_mu, 2)\n        - tf.expand_dims(two_z_CC_inv_mu, 1)\n    )\n    exponent_mahalanobis = tf.exp(-0.5 * exponent_mahalanobis)  # NxMxM\n\n    # Compute sqrt(self(Z)) explicitly to prevent automatic gradient from\n    # being NaN sometimes, see pull request #615\n    kernel_sqrt = tf.exp(-0.25 * square_distance(Z / kernel.lengthscales, None))\n    return kernel.variance ** 2 * kernel_sqrt * tf.reshape(dets, [N, 1, 1]) * exponent_mahalanobis\n'"
gpflow/expectations/sums.py,6,"b'import itertools\nfrom functools import reduce\n\nimport tensorflow as tf\n\nfrom .. import kernels\nfrom .. import mean_functions as mfn\nfrom ..inducing_variables import InducingPoints\nfrom ..probability_distributions import DiagonalGaussian, Gaussian, MarkovGaussian\nfrom . import dispatch\nfrom .expectations import expectation\n\nNoneType = type(None)\n\n\n@dispatch.expectation.register(Gaussian, kernels.Sum, NoneType, NoneType, NoneType)\ndef _E(p, kernel, _, __, ___, nghp=None):\n    r""""""\n    Compute the expectation:\n    <\\Sum_i diag(Ki_{X, X})>_p(X)\n        - \\Sum_i Ki_{.,.} :: Sum kernel\n\n    :return: N\n    """"""\n    exps = [expectation(p, k, nghp=nghp) for k in kernel.kernels]\n    return reduce(tf.add, exps)\n\n\n@dispatch.expectation.register(Gaussian, kernels.Sum, InducingPoints, NoneType, NoneType)\ndef _E(p, kernel, inducing_variable, _, __, nghp=None):\n    r""""""\n    Compute the expectation:\n    <\\Sum_i Ki_{X, Z}>_p(X)\n        - \\Sum_i Ki_{.,.} :: Sum kernel\n\n    :return: NxM\n    """"""\n    exps = [expectation(p, (k, inducing_variable), nghp=nghp) for k in kernel.kernels]\n    return reduce(tf.add, exps)\n\n\n@dispatch.expectation.register(\n    Gaussian, (mfn.Linear, mfn.Identity, mfn.Constant), NoneType, kernels.Sum, InducingPoints\n)\ndef _E(p, mean, _, kernel, inducing_variable, nghp=None):\n    r""""""\n    Compute the expectation:\n    expectation[n] = <m(x_n)^T (\\Sum_i Ki_{x_n, Z})>_p(x_n)\n        - \\Sum_i Ki_{.,.} :: Sum kernel\n\n    :return: NxQxM\n    """"""\n    exps = [expectation(p, mean, (k, inducing_variable), nghp=nghp) for k in kernel.kernels]\n    return reduce(tf.add, exps)\n\n\n@dispatch.expectation.register(MarkovGaussian, mfn.Identity, NoneType, kernels.Sum, InducingPoints)\ndef _E(p, mean, _, kernel, inducing_variable, nghp=None):\n    r""""""\n    Compute the expectation:\n    expectation[n] = <x_{n+1} (\\Sum_i Ki_{x_n, Z})>_p(x_{n:n+1})\n        - \\Sum_i Ki_{.,.} :: Sum kernel\n\n    :return: NxDxM\n    """"""\n    exps = [expectation(p, mean, (k, inducing_variable), nghp=nghp) for k in kernel.kernels]\n    return reduce(tf.add, exps)\n\n\n@dispatch.expectation.register(\n    (Gaussian, DiagonalGaussian), kernels.Sum, InducingPoints, kernels.Sum, InducingPoints\n)\ndef _E(p, kern1, feat1, kern2, feat2, nghp=None):\n    r""""""\n    Compute the expectation:\n    expectation[n] = <(\\Sum_i K1_i_{Z1, x_n}) (\\Sum_j K2_j_{x_n, Z2})>_p(x_n)\n        - \\Sum_i K1_i_{.,.}, \\Sum_j K2_j_{.,.} :: Sum kernels\n\n    :return: NxM1xM2\n    """"""\n    crossexps = []\n\n    if kern1 == kern2 and feat1 == feat2:  # avoid duplicate computation by using transposes\n        for i, k1 in enumerate(kern1.kernels):\n            crossexps.append(expectation(p, (k1, feat1), (k1, feat1), nghp=nghp))\n\n            for k2 in kern1.kernels[:i]:\n                eKK = expectation(p, (k1, feat1), (k2, feat2), nghp=nghp)\n                eKK += tf.linalg.adjoint(eKK)\n                crossexps.append(eKK)\n    else:\n        for k1, k2 in itertools.product(kern1.kernels, kern2.kernels):\n            crossexps.append(expectation(p, (k1, feat1), (k2, feat2), nghp=nghp))\n\n    return reduce(tf.add, crossexps)\n'"
gpflow/expectations/variationals.py,0,b''
gpflow/inducing_variables/__init__.py,0,"b'from .inducing_variables import InducingVariables, InducingPoints, Multiscale\nfrom .inducing_patch import InducingPatches\nfrom . import multioutput\nfrom .multioutput import (\n    MultioutputInducingVariables,\n    FallbackSharedIndependentInducingVariables,\n    FallbackSeparateIndependentInducingVariables,\n    SharedIndependentInducingVariables,\n    SeparateIndependentInducingVariables,\n)\n'"
gpflow/inducing_variables/inducing_patch.py,0,b'from .inducing_variables import InducingPoints\n\n\nclass InducingPatches(InducingPoints):\n    pass\n'
gpflow/inducing_variables/inducing_variables.py,2,"b'# Copyright 2017 GPflow\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nfrom typing import Optional\n\nimport tensorflow as tf\n\nfrom ..base import Module, Parameter, TensorData, TensorType\nfrom ..config import default_float\nfrom ..utilities import positive\n\n\nclass InducingVariables(Module):\n    """"""\n    Abstract base class for inducing variables.\n    """"""\n\n    @abc.abstractmethod\n    def __len__(self) -> int:\n        """"""\n        Returns the number of inducing variables, relevant for example\n        to determine the size of the variational distribution.\n        """"""\n        raise NotImplementedError\n\n\nclass InducingPointsBase(InducingVariables):\n    def __init__(self, Z: TensorData, name: Optional[str] = None):\n        """"""\n        :param Z: the initial positions of the inducing points, size [M, D]\n        """"""\n        super().__init__(name=name)\n        self.Z = Parameter(Z, dtype=default_float())\n\n    def __len__(self) -> int:\n        return self.Z.shape[0]\n\n\nclass InducingPoints(InducingPointsBase):\n    """"""\n    Real-space inducing points\n    """"""\n\n\nclass Multiscale(InducingPointsBase):\n    r""""""\n    Multi-scale inducing variables\n\n    Originally proposed in\n\n    ::\n\n      @incollection{NIPS2009_3876,\n        title = {Inter-domain Gaussian Processes for Sparse Inference using Inducing Features},\n        author = {Miguel L\\\'{a}zaro-Gredilla and An\\\'{\\i}bal Figueiras-Vidal},\n        booktitle = {Advances in Neural Information Processing Systems 22},\n        year = {2009},\n      }\n    """"""\n\n    def __init__(self, Z: TensorData, scales: TensorData):\n        super().__init__(Z)\n        # Multi-scale inducing_variable widths (std. dev. of Gaussian)\n        self.scales = Parameter(scales, transform=positive())\n        if self.Z.shape != self.scales.shape:\n            raise ValueError(\n                ""Input locations `Z` and `scales` must have the same shape.""\n            )  # pragma: no cover\n\n    @staticmethod\n    def _cust_square_dist(A: TensorType, B: TensorType, sc: TensorType) -> tf.Tensor:\n        """"""\n        Custom version of _square_dist that allows sc to provide per-datapoint length\n        scales. sc: [N, M, D].\n        """"""\n        return tf.reduce_sum(tf.square((tf.expand_dims(A, 1) - tf.expand_dims(B, 0)) / sc), 2)\n'"
gpflow/kernels/__init__.py,0,"b'from .base import Combination, Kernel, Product, Sum\nfrom .convolutional import Convolutional\nfrom .changepoints import ChangePoints\nfrom .linears import Linear, Polynomial\nfrom .misc import ArcCosine, Coregion\nfrom . import multioutput\n\nfrom .multioutput import (\n    MultioutputKernel,\n    SeparateIndependent,\n    SharedIndependent,\n    IndependentLatent,\n    LinearCoregionalization,\n)\nfrom .periodic import Periodic\nfrom .statics import Constant, Static, White\nfrom .stationaries import (\n    SquaredExponential,\n    Cosine,\n    Exponential,\n    Matern12,\n    Matern32,\n    Matern52,\n    RationalQuadratic,\n    Stationary,\n    IsotropicStationary,\n    AnisotropicStationary,\n)\n\nBias = Constant\nRBF = SquaredExponential\n'"
gpflow/kernels/base.py,16,"b'# Copyright 2018 GPflow\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nr""""""\nKernels form a core component of GPflow models and allow prior information to\nbe encoded about a latent function of interest. The effect of choosing\ndifferent kernels, and how it is possible to combine multiple kernels is shown\nin the `""Using kernels in GPflow"" notebook <notebooks/kernels.html>`_.\n\nBroadcasting over leading dimensions:\n`kernel.K(X1, X2)` returns the kernel evaluated on every pair in X1 and X2.\nE.g. if X1 has shape [S1, N1, D] and X2 has shape [S2, N2, D], kernel.K(X1, X2)\nwill return a tensor of shape [S1, N1, S2, N2]. Similarly, kernel.K(X1, X1)\nreturns a tensor of shape [S1, N1, S1, N1]. In contrast, the return shape of\nkernel.K(X1) is [S1, N1, N1]. (Without leading dimensions, the behaviour of\nkernel.K(X, None) is identical to kernel.K(X, X).)\n""""""\n\nimport abc\nfrom functools import partial, reduce\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..base import Module\n\nActiveDims = Union[slice, list]\n\n\nclass Kernel(Module, metaclass=abc.ABCMeta):\n    """"""\n    The basic kernel class. Handles active dims.\n    """"""\n\n    def __init__(self, active_dims: Optional[ActiveDims] = None, name: Optional[str] = None):\n        """"""\n        :param active_dims: active dimensions, either a slice or list of\n            indices into the columns of X.\n        :param name: optional kernel name.\n        """"""\n        super().__init__(name=name)\n        self._active_dims = self._normalize_active_dims(active_dims)\n\n    @staticmethod\n    def _normalize_active_dims(value):\n        if value is None:\n            value = slice(None, None, None)\n        if not isinstance(value, slice):\n            value = np.array(value, dtype=int)\n        return value\n\n    @property\n    def active_dims(self):\n        return self._active_dims\n\n    @active_dims.setter\n    def active_dims(self, value):\n        self._active_dims = self._normalize_active_dims(value)\n\n    def on_separate_dims(self, other):\n        """"""\n        Checks if the dimensions, over which the kernels are specified, overlap.\n        Returns True if they are defined on different/separate dimensions and False otherwise.\n        """"""\n        if isinstance(self.active_dims, slice) or isinstance(other.active_dims, slice):\n            # Be very conservative for kernels defined over slices of dimensions\n            return False\n\n        if self.active_dims is None or other.active_dims is None:\n            return False\n\n        this_dims = self.active_dims.reshape(-1, 1)\n        other_dims = other.active_dims.reshape(1, -1)\n        return not np.any(this_dims == other_dims)\n\n    def slice(self, X: tf.Tensor, X2: Optional[tf.Tensor] = None):\n        """"""\n        Slice the correct dimensions for use in the kernel, as indicated by `self.active_dims`.\n\n        :param X: Input 1 [N, D].\n        :param X2: Input 2 [M, D], can be None.\n        :return: Sliced X, X2, [N, I], I - input dimension.\n        """"""\n        dims = self.active_dims\n        if isinstance(dims, slice):\n            X = X[..., dims]\n            if X2 is not None:\n                X2 = X2[..., dims]\n        elif dims is not None:\n            X = tf.gather(X, dims, axis=-1)\n            if X2 is not None:\n                X2 = tf.gather(X2, dims, axis=-1)\n        return X, X2\n\n    def slice_cov(self, cov: tf.Tensor) -> tf.Tensor:\n        """"""\n        Slice the correct dimensions for use in the kernel, as indicated by\n        `self.active_dims` for covariance matrices. This requires slicing the\n        rows *and* columns. This will also turn flattened diagonal\n        matrices into a tensor of full diagonal matrices.\n\n        :param cov: Tensor of covariance matrices, [N, D, D] or [N, D].\n        :return: [N, I, I].\n        """"""\n        if cov.shape.ndims == 2:\n            cov = tf.linalg.diag(cov)\n\n        dims = self.active_dims\n\n        if isinstance(dims, slice):\n            return cov[..., dims, dims]\n        elif dims is not None:\n            nlast = tf.shape(cov)[-1]\n            ndims = len(dims)\n\n            cov_shape = tf.shape(cov)\n            cov_reshaped = tf.reshape(cov, [-1, nlast, nlast])\n            gather1 = tf.gather(tf.transpose(cov_reshaped, [2, 1, 0]), dims)\n            gather2 = tf.gather(tf.transpose(gather1, [1, 0, 2]), dims)\n            cov = tf.reshape(\n                tf.transpose(gather2, [2, 0, 1]), tf.concat([cov_shape[:-2], [ndims, ndims]], 0)\n            )\n\n        return cov\n\n    def _validate_ard_active_dims(self, ard_parameter):\n        """"""\n        Validate that ARD parameter matches the number of active_dims (provided active_dims\n        has been specified as an array).\n        """"""\n        if self.active_dims is None or isinstance(self.active_dims, slice):\n            # Can only validate parameter if active_dims is an array\n            return\n\n        if ard_parameter.shape.rank > 0 and ard_parameter.shape[0] != len(self.active_dims):\n            raise ValueError(\n                f""Size of `active_dims` {self.active_dims} does not match ""\n                f""size of ard parameter ({ard_parameter.shape[0]})""\n            )\n\n    @abc.abstractmethod\n    def K(self, X, X2=None):\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def K_diag(self, X):\n        raise NotImplementedError\n\n    def __call__(self, X, X2=None, *, full_cov=True, presliced=False):\n        if (not full_cov) and (X2 is not None):\n            raise ValueError(""Ambiguous inputs: `not full_cov` and `X2` are not compatible."")\n\n        if not presliced:\n            X, X2 = self.slice(X, X2)\n\n        if not full_cov:\n            assert X2 is None\n            return self.K_diag(X)\n\n        else:\n            return self.K(X, X2)\n\n    def __add__(self, other):\n        return Sum([self, other])\n\n    def __mul__(self, other):\n        return Product([self, other])\n\n\nclass Combination(Kernel):\n    """"""\n    Combine a list of kernels, e.g. by adding or multiplying (see inheriting\n    classes).\n\n    The names of the kernels to be combined are generated from their class\n    names.\n    """"""\n\n    _reduction = None\n\n    def __init__(self, kernels: List[Kernel], name: Optional[str] = None):\n        super().__init__(name=name)\n\n        if not all(isinstance(k, Kernel) for k in kernels):\n            raise TypeError(""can only combine Kernel instances"")  # pragma: no cover\n\n        self._set_kernels(kernels)\n\n    def _set_kernels(self, kernels: List[Kernel]):\n        # add kernels to a list, flattening out instances of this class therein\n        kernels_list = []\n        for k in kernels:\n            if isinstance(k, self.__class__):\n                kernels_list.extend(k.kernels)\n            else:\n                kernels_list.append(k)\n        self.kernels = kernels_list\n\n    @property\n    def on_separate_dimensions(self):\n        """"""\n        Checks whether the kernels in the combination act on disjoint subsets\n        of dimensions. Currently, it is hard to asses whether two slice objects\n        will overlap, so this will always return False.\n\n        :return: Boolean indicator.\n        """"""\n        if np.any([isinstance(k.active_dims, slice) for k in self.kernels]):\n            # Be conservative in the case of a slice object\n            return False\n        else:\n            dimlist = [k.active_dims for k in self.kernels]\n            overlapping = False\n            for i, dims_i in enumerate(dimlist):\n                for dims_j in dimlist[i + 1 :]:\n                    print(f""dims_i = {type(dims_i)}"")\n                    if np.any(dims_i.reshape(-1, 1) == dims_j.reshape(1, -1)):\n                        overlapping = True\n            return not overlapping\n\n\nclass ReducingCombination(Combination):\n    def __call__(self, X, X2=None, *, full_cov=True, presliced=False):\n        return self._reduce(\n            [k(X, X2, full_cov=full_cov, presliced=presliced) for k in self.kernels]\n        )\n\n    def K(self, X: tf.Tensor, X2: Optional[tf.Tensor] = None) -> tf.Tensor:\n        return self._reduce([k.K(X, X2) for k in self.kernels])\n\n    def K_diag(self, X: tf.Tensor) -> tf.Tensor:\n        return self._reduce([k.K_diag(X) for k in self.kernels])\n\n    @property\n    @abc.abstractmethod\n    def _reduce(self):\n        pass\n\n\nclass Sum(ReducingCombination):\n    @property\n    def _reduce(self):\n        return tf.add_n\n\n\nclass Product(ReducingCombination):\n    @property\n    def _reduce(self):\n        return partial(reduce, tf.multiply)\n'"
gpflow/kernels/changepoints.py,23,"b'from collections.abc import Iterable\nfrom typing import List, Optional, Union\n\nimport tensorflow as tf\n\nfrom ..base import Parameter\nfrom ..utilities import positive\nfrom .base import Combination, Kernel\n\n\nclass ChangePoints(Combination):\n    r""""""\n    The ChangePoints kernel defines a fixed number of change-points along a 1d\n    input space where different kernels govern different parts of the space.\n\n    The kernel is by multiplication and addition of the base kernels with\n    sigmoid functions (\xcf\x83). A single change-point kernel is defined as:\n\n        K\xe2\x82\x81(x, x\') * (1 - \xcf\x83(x)) * (1 - \xcf\x83(x\')) + K\xe2\x82\x82(x, x\') * \xcf\x83(x) * \xcf\x83(x\')\n\n    where K\xe2\x82\x81 is deactivated around the change-point and K\xe2\x82\x82 is activated. The\n    single change-point version can be found in \\citet{lloyd2014}. Each sigmoid\n    is a logistic function defined as:\n\n        \xcf\x83(x) = 1 / (1 + exp{-s(x - x\xe2\x82\x80)})\n\n    parameterized by location ""x\xe2\x82\x80"" and steepness ""s"".\n\n    @incollection{lloyd2014,\n      author = {Lloyd, James Robert et al},\n      title = {Automatic Construction and Natural-language Description of Nonparametric Regression Models},\n      booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},\n      year = {2014},\n      url = {http://dl.acm.org/citation.cfm?id=2893873.2894066},\n    }\n    """"""\n\n    def __init__(\n        self,\n        kernels: List[Kernel],\n        locations: List[float],\n        steepness: Union[float, List[float]] = 1.0,\n        name: Optional[str] = None,\n    ):\n        """"""\n        :param kernels: list of kernels defining the different regimes\n        :param locations: list of change-point locations in the 1d input space\n        :param steepness: the steepness parameter(s) of the sigmoids, this can be\n            common between them or decoupled\n        """"""\n        if len(kernels) != len(locations) + 1:\n            raise ValueError(\n                ""Number of kernels ({nk}) must be one more than the number of ""\n                ""changepoint locations ({nl})"".format(nk=len(kernels), nl=len(locations))\n            )\n\n        if isinstance(steepness, Iterable) and len(steepness) != len(locations):\n            raise ValueError(\n                ""Dimension of steepness ({ns}) does not match number of changepoint ""\n                ""locations ({nl})"".format(ns=len(steepness), nl=len(locations))\n            )\n\n        super().__init__(kernels, name=name)\n\n        self.locations = Parameter(locations)\n        self.steepness = Parameter(steepness, transform=positive())\n\n    def _set_kernels(self, kernels: List[Kernel]):\n        # it is not clear how to flatten out nested change-points\n        self.kernels = kernels\n\n    def K(self, X: tf.Tensor, X2: Optional[tf.Tensor] = None) -> tf.Tensor:\n        sig_X = self._sigmoids(X)  # N1 x 1 x Ncp\n        sig_X2 = self._sigmoids(X2) if X2 is not None else sig_X  # N2 x 1 x Ncp\n\n        # `starters` are the sigmoids going from 0 -> 1, whilst `stoppers` go\n        # from 1 -> 0, dimensions are N1 x N2 x Ncp\n        starters = sig_X * tf.transpose(sig_X2, perm=(1, 0, 2))\n        stoppers = (1 - sig_X) * tf.transpose((1 - sig_X2), perm=(1, 0, 2))\n\n        # prepend `starters` with ones and append ones to `stoppers` since the\n        # first kernel has no start and the last kernel has no end\n        N1 = tf.shape(X)[0]\n        N2 = tf.shape(X2)[0] if X2 is not None else N1\n        ones = tf.ones((N1, N2, 1), dtype=X.dtype)\n        starters = tf.concat([ones, starters], axis=2)\n        stoppers = tf.concat([stoppers, ones], axis=2)\n\n        # now combine with the underlying kernels\n        kernel_stack = tf.stack([k(X, X2) for k in self.kernels], axis=2)\n        return tf.reduce_sum(kernel_stack * starters * stoppers, axis=2)\n\n    def K_diag(self, X: tf.Tensor) -> tf.Tensor:\n        N = tf.shape(X)[0]\n        sig_X = tf.reshape(self._sigmoids(X), (N, -1))  # N x Ncp\n\n        ones = tf.ones((N, 1), dtype=X.dtype)\n        starters = tf.concat([ones, sig_X * sig_X], axis=1)  # N x Ncp\n        stoppers = tf.concat([(1 - sig_X) * (1 - sig_X), ones], axis=1)\n\n        kernel_stack = tf.stack([k(X, full_cov=False) for k in self.kernels], axis=1)\n        return tf.reduce_sum(kernel_stack * starters * stoppers, axis=1)\n\n    def _sigmoids(self, X: tf.Tensor) -> tf.Tensor:\n        locations = tf.sort(self.locations)  # ensure locations are ordered\n        locations = tf.reshape(locations, (1, 1, -1))\n        steepness = tf.reshape(self.steepness, (1, 1, -1))\n        return tf.sigmoid(steepness * (X[:, :, None] - locations))\n'"
gpflow/kernels/convolutional.py,9,"b'import numpy as np\nimport tensorflow as tf\n\nfrom .base import Kernel\nfrom ..base import Parameter\nfrom ..config import default_float\nfrom ..utilities import to_default_float\n\n\nclass Convolutional(Kernel):\n    r""""""\n    Plain convolutional kernel as described in \\citet{vdw2017convgp}. Defines\n    a GP f( ) that is constructed from a sum of responses of individual patches\n    in an image:\n      f(x) = \\sum_p x^{[p]}\n    where x^{[p]} is the pth patch in the image.\n\n    @incollection{vdw2017convgp,\n      title = {Convolutional Gaussian Processes},\n      author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},\n      booktitle = {Advances in Neural Information Processing Systems 30},\n      year = {2017},\n      url = {http://papers.nips.cc/paper/6877-convolutional-gaussian-processes.pdf}\n    }\n    """"""\n\n    def __init__(self, base_kernel, image_shape, patch_shape, weights=None, colour_channels=1):\n        super().__init__()\n        self.image_shape = image_shape\n        self.patch_shape = patch_shape\n        self.base_kernel = base_kernel\n        self.colour_channels = colour_channels\n        self.weights = Parameter(\n            np.ones(self.num_patches, dtype=default_float()) if weights is None else weights\n        )\n\n    # @lru_cache() -- Can we do some kind of memoizing with TF2?\n    def get_patches(self, X):\n        """"""\n        Extracts patches from the images X. Patches are extracted separately for each of the colour channels.\n        :param X: (N x input_dim)\n        :return: Patches (N, num_patches, patch_shape)\n        """"""\n        # Roll the colour channel to the front, so it appears to\n        # `tf.extract_image_patches()` as separate images. Then extract patches\n        # and reshape to have the first axis the same as the number of images.\n        # The separate patches will then be in the second axis.\n        num_data = tf.shape(X)[0]\n        castX = tf.transpose(tf.reshape(X, [num_data, -1, self.colour_channels]), [0, 2, 1])\n        patches = tf.image.extract_patches(\n            tf.reshape(castX, [-1, self.image_shape[0], self.image_shape[1], 1], name=""rX""),\n            [1, self.patch_shape[0], self.patch_shape[1], 1],\n            [1, 1, 1, 1],\n            [1, 1, 1, 1],\n            ""VALID"",\n        )\n        shp = tf.shape(patches)  # img x out_rows x out_cols\n        reshaped_patches = tf.reshape(\n            patches, [num_data, self.colour_channels * shp[1] * shp[2], shp[3]]\n        )\n        return to_default_float(reshaped_patches)\n\n    def K(self, X, X2=None):\n        Xp = self.get_patches(X)  # [N, P, patch_len]\n        Xp2 = Xp if X2 is None else self.get_patches(X2)\n\n        bigK = self.base_kernel.K(Xp, Xp2)  # [N, num_patches, N, num_patches]\n\n        W2 = self.weights[:, None] * self.weights[None, :]  # [P, P]\n        W2bigK = bigK * W2[None, :, None, :]\n        return tf.reduce_sum(W2bigK, [1, 3]) / self.num_patches ** 2.0\n\n    def K_diag(self, X):\n        Xp = self.get_patches(X)  # N x num_patches x patch_dim\n        W2 = self.weights[:, None] * self.weights[None, :]  # [P, P]\n        bigK = self.base_kernel.K(Xp)  # [N, P, P]\n        return tf.reduce_sum(bigK * W2[None, :, :], [1, 2]) / self.num_patches ** 2.0\n\n    @property\n    def patch_len(self):\n        return np.prod(self.patch_shape)\n\n    @property\n    def num_patches(self):\n        return (\n            (self.image_shape[0] - self.patch_shape[0] + 1)\n            * (self.image_shape[1] - self.patch_shape[1] + 1)\n            * self.colour_channels\n        )\n'"
gpflow/kernels/linears.py,3,"b'import tensorflow as tf\n\nfrom ..base import Parameter\nfrom ..utilities import positive\nfrom .base import Kernel\n\n\nclass Linear(Kernel):\n    """"""\n    The linear kernel. Functions drawn from a GP with this kernel are linear, i.e. f(x) = cx.\n    The kernel equation is\n\n        k(x, y) = \xcf\x83\xc2\xb2xy\n\n    where \xcf\x83\xc2\xb2 is the variance parameter.\n    """"""\n\n    def __init__(self, variance=1.0, active_dims=None):\n        """"""\n        :param variance: the (initial) value for the variance parameter(s),\n            to induce ARD behaviour this must be initialised as an array the same\n            length as the the number of active dimensions e.g. [1., 1., 1.]\n        :param active_dims: a slice or list specifying which columns of X are used\n        """"""\n        super().__init__(active_dims)\n        self.variance = Parameter(variance, transform=positive())\n        self._validate_ard_active_dims(self.variance)\n\n    @property\n    def ard(self) -> bool:\n        """"""\n        Whether ARD behaviour is active.\n        """"""\n        return self.variance.shape.ndims > 0\n\n    def K(self, X, X2=None):\n        if X2 is None:\n            return tf.matmul(X * self.variance, X, transpose_b=True)\n        else:\n            return tf.tensordot(X * self.variance, X2, [[-1], [-1]])\n\n    def K_diag(self, X):\n        return tf.reduce_sum(tf.square(X) * self.variance, axis=-1)\n\n\nclass Polynomial(Linear):\n    """"""\n    The Polynomial kernel. Functions drawn from a GP with this kernel are\n    polynomials of degree `d`. The kernel equation is\n\n        k(x, y) = (\xcf\x83\xc2\xb2xy + \xce\xb3)\xe1\xb5\x88\n\n    where:\n    \xcf\x83\xc2\xb2 is the variance parameter,\n    \xce\xb3 is the offset parameter,\n    d is the degree parameter.\n    """"""\n\n    def __init__(self, degree=3.0, variance=1.0, offset=1.0, active_dims=None):\n        """"""\n        :param degree: the degree of the polynomial\n        :param variance: the (initial) value for the variance parameter(s),\n            to induce ARD behaviour this must be initialised as an array the same\n            length as the the number of active dimensions e.g. [1., 1., 1.]\n        :param offset: the offset of the polynomial\n        :param active_dims: a slice or list specifying which columns of X are used\n        """"""\n        super().__init__(variance, active_dims)\n        self.degree = degree\n        self.offset = Parameter(offset, transform=positive())\n\n    def K(self, X, X2=None):\n        return (super().K(X, X2) + self.offset) ** self.degree\n\n    def K_diag(self, X):\n        return (super().K_diag(X) + self.offset) ** self.degree\n'"
gpflow/kernels/misc.py,17,"b'# Copyright 2018-2020 GPflow\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..base import Parameter\nfrom ..utilities import positive, to_default_float\nfrom .base import Kernel, ActiveDims\n\n\nclass ArcCosine(Kernel):\n    """"""\n    The Arc-cosine family of kernels which mimics the computation in neural\n    networks. The order parameter specifies the assumed activation function.\n    The Multi Layer Perceptron (MLP) kernel is closely related to the ArcCosine\n    kernel of order 0. The key reference is\n\n    ::\n\n        @incollection{NIPS2009_3628,\n            title = {Kernel Methods for Deep Learning},\n            author = {Youngmin Cho and Lawrence K. Saul},\n            booktitle = {Advances in Neural Information Processing Systems 22},\n            year = {2009},\n            url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}\n        }\n    """"""\n\n    implemented_orders = {0, 1, 2}\n\n    def __init__(\n        self,\n        order: int = 0,\n        variance=1.0,\n        weight_variances=1.0,\n        bias_variance=1.0,\n        *,\n        active_dims: Optional[ActiveDims] = None,\n        name: Optional[str] = None,\n    ):\n        """"""\n        :param order: specifies the activation function of the neural network\n          the function is a rectified monomial of the chosen order\n        :param variance: the (initial) value for the variance parameter\n        :param weight_variances: the (initial) value for the weight_variances parameter,\n            to induce ARD behaviour this must be initialised as an array the same\n            length as the the number of active dimensions e.g. [1., 1., 1.]\n        :param bias_variance: the (initial) value for the bias_variance parameter\n            defaults to 1.0\n        :param active_dims: a slice or list specifying which columns of X are used\n        """"""\n        super().__init__(active_dims=active_dims, name=name)\n\n        if order not in self.implemented_orders:\n            raise ValueError(""Requested kernel order is not implemented."")\n        self.order = order\n\n        self.variance = Parameter(variance, transform=positive())\n        self.bias_variance = Parameter(bias_variance, transform=positive())\n        self.weight_variances = Parameter(weight_variances, transform=positive())\n        self._validate_ard_active_dims(self.weight_variances)\n\n    @property\n    def ard(self) -> bool:\n        """"""\n        Whether ARD behaviour is active.\n        """"""\n        return self.weight_variances.shape.ndims > 0\n\n    def _weighted_product(self, X, X2=None):\n        if X2 is None:\n            return tf.reduce_sum(self.weight_variances * tf.square(X), axis=1) + self.bias_variance\n        return (\n            tf.linalg.matmul((self.weight_variances * X), X2, transpose_b=True) + self.bias_variance\n        )\n\n    def _J(self, theta):\n        """"""\n        Implements the order dependent family of functions defined in equations\n        4 to 7 in the reference paper.\n        """"""\n        if self.order == 0:\n            return np.pi - theta\n        elif self.order == 1:\n            return tf.sin(theta) + (np.pi - theta) * tf.cos(theta)\n        elif self.order == 2:\n            return 3.0 * tf.sin(theta) * tf.cos(theta) + (np.pi - theta) * (\n                1.0 + 2.0 * tf.cos(theta) ** 2\n            )\n\n    def K(self, X, X2=None):\n        X_denominator = tf.sqrt(self._weighted_product(X))\n        if X2 is None:\n            X2 = X\n            X2_denominator = X_denominator\n        else:\n            X2_denominator = tf.sqrt(self._weighted_product(X2))\n\n        numerator = self._weighted_product(X, X2)\n        cos_theta = numerator / X_denominator[:, None] / X2_denominator[None, :]\n        jitter = 1e-15\n        theta = tf.acos(jitter + (1 - 2 * jitter) * cos_theta)\n\n        return (\n            self.variance\n            * (1.0 / np.pi)\n            * self._J(theta)\n            * X_denominator[:, None] ** self.order\n            * X2_denominator[None, :] ** self.order\n        )\n\n    def K_diag(self, X):\n        X_product = self._weighted_product(X)\n        const = (1.0 / np.pi) * self._J(to_default_float(0.0))\n        return self.variance * const * X_product ** self.order\n\n\nclass Coregion(Kernel):\n    """"""\n    A Coregionalization kernel. The inputs to this kernel are _integers_ (we\n    cast them from floats as needed) which usually specify the *outputs* of a\n    Coregionalization model.\n\n    The kernel function is an indexing of a positive-definite matrix:\n\n      K(x, y) = B[x, y] .\n\n    To ensure that B is positive-definite, it is specified by the two\n    parameters of this kernel, W and kappa:\n\n      B = W W\xe1\xb5\x80 + diag(kappa) .\n\n    We refer to the size of B as ""output_dim x output_dim"", since this is the\n    number of outputs in a coregionalization model. We refer to the number of\n    columns on W as \'rank\': it is the number of degrees of correlation between\n    the outputs.\n\n    NB. There is a symmetry between the elements of W, which creates a local\n    minimum at W=0. To avoid this, it is recommended to initialize the\n    optimization (or MCMC chain) using a random W.\n    """"""\n\n    def __init__(\n        self,\n        output_dim: int,\n        rank: int,\n        *,\n        active_dims: Optional[ActiveDims] = None,\n        name: Optional[str] = None,\n    ):\n        """"""\n        :param output_dim: number of outputs expected (0 <= X < output_dim)\n        :param rank: number of degrees of correlation between outputs\n        """"""\n\n        # assert input_dim == 1, ""Coregion kernel in 1D only""\n        super().__init__(active_dims=active_dims, name=name)\n\n        self.output_dim = output_dim\n        self.rank = rank\n        W = 0.1 * np.ones((self.output_dim, self.rank))\n        kappa = np.ones(self.output_dim)\n        self.W = Parameter(W)\n        self.kappa = Parameter(kappa, transform=positive())\n\n    def output_covariance(self):\n        B = tf.linalg.matmul(self.W, self.W, transpose_b=True) + tf.linalg.diag(self.kappa)\n        return B\n\n    def output_variance(self):\n        B_diag = tf.reduce_sum(tf.square(self.W), 1) + self.kappa\n        return B_diag\n\n    def K(self, X, X2=None):\n        shape_constraints = [\n            (X, [..., ""N"", 1]),\n        ]\n        if X2 is not None:\n            shape_constraints.append((X2, [..., ""M"", 1]))\n        tf.debugging.assert_shapes(shape_constraints)\n\n        X = tf.cast(X[..., 0], tf.int32)\n        if X2 is None:\n            X2 = X\n        else:\n            X2 = tf.cast(X2[..., 0], tf.int32)\n\n        B = self.output_covariance()\n        return tf.gather(tf.transpose(tf.gather(B, X2)), X)\n\n    def K_diag(self, X):\n        tf.debugging.assert_shapes([(X, [..., ""N"", 1])])\n        X = tf.cast(X[..., 0], tf.int32)\n        B_diag = self.output_variance()\n        return tf.gather(B_diag, X)\n'"
gpflow/kernels/periodic.py,5,"b'from typing import List, Optional, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..base import Parameter\nfrom ..utilities import positive\nfrom ..utilities.ops import difference_matrix\nfrom .base import Kernel\nfrom .stationaries import Stationary, IsotropicStationary\n\n\nclass Periodic(Kernel):\n    """"""\n    The periodic family of kernels. Can be used to wrap any Stationary kernel\n    to transform it into a periodic version. The canonical form (based on the\n    SquaredExponential kernel) can be found in Equation (47) of\n\n    D.J.C.MacKay. Introduction to Gaussian processes. In C.M.Bishop, editor,\n    Neural Networks and Machine Learning, pages 133--165. Springer, 1998.\n\n    The derivation can be achieved by mapping the original inputs through the\n    transformation u = (cos(x), sin(x)).\n\n    For the SquaredExponential base kernel, the result can be expressed as:\n\n        k(r) =  \xcf\x83\xc2\xb2 exp{ -0.5 sin\xc2\xb2(\xcf\x80 r / \xce\xb3) / \xe2\x84\x93\xc2\xb2}\n\n    where:\n    r is the Euclidean distance between the input points\n    \xe2\x84\x93 is the lengthscales parameter,\n    \xcf\x83\xc2\xb2 is the variance parameter,\n    \xce\xb3 is the period parameter.\n\n    NOTE: usually we have a factor of 4 instead of 0.5 in front but this\n        is absorbed into the lengthscales hyperparameter.\n    NOTE: periodic kernel uses `active_dims` of a base kernel, therefore\n        the constructor doesn\'t have it as an argument.\n    """"""\n\n    def __init__(self, base_kernel: IsotropicStationary, period: Union[float, List[float]] = 1.0):\n        """"""\n        :param base_kernel: the base kernel to make periodic; must inherit from Stationary\n            Note that `active_dims` should be specified in the base kernel.\n        :param period: the period; to induce a different period per active dimension\n            this must be initialized with an array the same length as the number\n            of active dimensions e.g. [1., 1., 1.]\n        """"""\n        if not isinstance(base_kernel, IsotropicStationary):\n            raise TypeError(""Periodic requires an IsotropicStationary kernel as the `base_kernel`"")\n\n        super().__init__()\n        self.base_kernel = base_kernel\n        self.period = Parameter(period, transform=positive())\n        self.base_kernel._validate_ard_active_dims(self.period)\n\n    @property\n    def active_dims(self):\n        return self.base_kernel.active_dims\n\n    @active_dims.setter\n    def active_dims(self, value):\n        self.base_kernel.active_dims = value\n\n    def K_diag(self, X: tf.Tensor) -> tf.Tensor:\n        return self.base_kernel.K_diag(X)\n\n    def K(self, X: tf.Tensor, X2: Optional[tf.Tensor] = None) -> tf.Tensor:\n        r = np.pi * (difference_matrix(X, X2)) / self.period\n        scaled_sine = tf.sin(r) / self.base_kernel.lengthscales\n        if hasattr(self.base_kernel, ""K_r""):\n            sine_r = tf.reduce_sum(tf.abs(scaled_sine), -1)\n            K = self.base_kernel.K_r(sine_r)\n        else:\n            sine_r2 = tf.reduce_sum(tf.square(scaled_sine), -1)\n            K = self.base_kernel.K_r2(sine_r2)\n        return K\n'"
gpflow/kernels/statics.py,11,"b'import tensorflow as tf\n\nfrom ..base import Parameter\nfrom ..utilities import positive\nfrom .base import Kernel\n\n\nclass Static(Kernel):\n    """"""\n    Kernels who don\'t depend on the value of the inputs are \'Static\'.  The only\n    parameter is a variance, \xcf\x83\xc2\xb2.\n    """"""\n\n    def __init__(self, variance=1.0, active_dims=None):\n        super().__init__(active_dims)\n        self.variance = Parameter(variance, transform=positive())\n\n    def K_diag(self, X):\n        return tf.fill(tf.shape(X)[:-1], tf.squeeze(self.variance))\n\n\nclass White(Static):\n    """"""\n    The White kernel: this kernel produces \'white noise\'. The kernel equation is\n\n        k(x_n, x_m) = \xce\xb4(n, m) \xcf\x83\xc2\xb2\n\n    where:\n    \xce\xb4(.,.) is the Kronecker delta,\n    \xcf\x83\xc2\xb2  is the variance parameter.\n    """"""\n\n    def K(self, X, X2=None):\n        if X2 is None:\n            d = tf.fill(tf.shape(X)[:-1], tf.squeeze(self.variance))\n            return tf.linalg.diag(d)\n        else:\n            shape = tf.concat([tf.shape(X)[:-1], tf.shape(X2)[:-1]], axis=0)\n            return tf.zeros(shape, dtype=X.dtype)\n\n\nclass Constant(Static):\n    """"""\n    The Constant (aka Bias) kernel. Functions drawn from a GP with this kernel\n    are constant, i.e. f(x) = c, with c ~ N(0, \xcf\x83^2). The kernel equation is\n\n        k(x, y) = \xcf\x83\xc2\xb2\n\n    where:\n    \xcf\x83\xc2\xb2  is the variance parameter.\n    """"""\n\n    def K(self, X, X2=None):\n        if X2 is None:\n            shape = tf.concat(\n                [\n                    tf.shape(X)[:-2],\n                    tf.reshape(tf.shape(X)[-2], [1]),\n                    tf.reshape(tf.shape(X)[-2], [1]),\n                ],\n                axis=0,\n            )\n        else:\n            shape = tf.concat([tf.shape(X)[:-1], tf.shape(X2)[:-1]], axis=0)\n\n        return tf.fill(shape, tf.squeeze(self.variance))\n'"
gpflow/kernels/stationaries.py,9,"b'import numpy as np\nimport tensorflow as tf\n\nfrom ..base import Parameter\nfrom ..utilities import positive\nfrom ..utilities.ops import square_distance, difference_matrix\nfrom .base import Kernel\n\n\nclass Stationary(Kernel):\n    """"""\n    Base class for kernels that are stationary, that is, they only depend on\n\n        d = x - x\'\n\n    This class handles \'ard\' behaviour, which stands for \'Automatic Relevance\n    Determination\'. This means that the kernel has one lengthscale per\n    dimension, otherwise the kernel is isotropic (has a single lengthscale).\n    """"""\n\n    def __init__(self, variance=1.0, lengthscales=1.0, **kwargs):\n        """"""\n        :param variance: the (initial) value for the variance parameter.\n        :param lengthscales: the (initial) value for the lengthscale\n            parameter(s), to induce ARD behaviour this must be initialised as\n            an array the same length as the the number of active dimensions\n            e.g. [1., 1., 1.]. If only a single value is passed, this value\n            is used as the lengthscale of each dimension.\n        :param kwargs: accepts `name` and `active_dims`, which is a list or\n            slice of indices which controls which columns of X are used (by\n            default, all columns are used).\n        """"""\n        for kwarg in kwargs:\n            if kwarg not in {""name"", ""active_dims""}:\n                raise TypeError(""Unknown keyword argument:"", kwarg)\n\n        super().__init__(**kwargs)\n        self.variance = Parameter(variance, transform=positive())\n        self.lengthscales = Parameter(lengthscales, transform=positive())\n        self._validate_ard_active_dims(self.lengthscales)\n\n    @property\n    def ard(self) -> bool:\n        """"""\n        Whether ARD behaviour is active.\n        """"""\n        return self.lengthscales.shape.ndims > 0\n\n    def scale(self, X):\n        X_scaled = X / self.lengthscales if X is not None else X\n        return X_scaled\n\n    def K_diag(self, X):\n        return tf.fill(tf.shape(X)[:-1], tf.squeeze(self.variance))\n\n\nclass IsotropicStationary(Stationary):\n    """"""\n    Base class for isotropic stationary kernels, i.e. kernels that only\n    depend on\n\n        r = \xe2\x80\x96x - x\'\xe2\x80\x96\n\n    Derived classes should implement one of:\n\n        K_r2(self, r2): Returns the kernel evaluated on r\xc2\xb2 (r2), which is the\n        squared scaled Euclidean distance Should operate element-wise on r2.\n\n        K_r(self, r): Returns the kernel evaluated on r, which is the scaled\n        Euclidean distance. Should operate element-wise on r.\n    """"""\n\n    def K(self, X, X2=None):\n        r2 = self.scaled_squared_euclid_dist(X, X2)\n        return self.K_r2(r2)\n\n    def K_r2(self, r2):\n        if hasattr(self, ""K_r""):\n            # Clipping around the (single) float precision which is ~1e-45.\n            r = tf.sqrt(tf.maximum(r2, 1e-36))\n            return self.K_r(r)  # pylint: disable=no-member\n        raise NotImplementedError\n\n    def scaled_squared_euclid_dist(self, X, X2=None):\n        """"""\n        Returns \xe2\x80\x96(X - X2\xe1\xb5\x80) / \xe2\x84\x93\xe2\x80\x96\xc2\xb2, i.e. the squared L\xe2\x82\x82-norm.\n        """"""\n        return square_distance(self.scale(X), self.scale(X2))\n\n\nclass AnisotropicStationary(Stationary):\n    """"""\n    Base class for anisotropic stationary kernels, i.e. kernels that only\n    depend on\n\n        d = x - x\'\n\n    Derived classes should implement K_d(self, d): Returns the kernel evaluated\n    on d, which is the pairwise difference matrix, scaled by the lengthscale\n    parameter \xe2\x84\x93 (i.e. [(X - X2\xe1\xb5\x80) / \xe2\x84\x93]). The last axis corresponds to the\n    input dimension.\n    """"""\n\n    def K(self, X, X2=None):\n        return self.K_d(self.scaled_difference_matrix(X, X2))\n\n    def scaled_difference_matrix(self, X, X2=None):\n        """"""\n        Returns [(X - X2\xe1\xb5\x80) / \xe2\x84\x93]. If X has shape [..., N, D] and\n        X2 has shape [..., M, D], the output will have shape [..., N, M, D].\n        """"""\n        return difference_matrix(self.scale(X), self.scale(X2))\n\n    def K_d(self, d):\n        raise NotImplementedError\n\n\nclass SquaredExponential(IsotropicStationary):\n    """"""\n    The radial basis function (RBF) or squared exponential kernel. The kernel equation is\n\n        k(r) = \xcf\x83\xc2\xb2 exp{-\xc2\xbd r\xc2\xb2}\n\n    where:\n    r   is the Euclidean distance between the input points, scaled by the lengthscales parameter \xe2\x84\x93.\n    \xcf\x83\xc2\xb2  is the variance parameter\n\n    Functions drawn from a GP with this kernel are infinitely differentiable!\n    """"""\n\n    def K_r2(self, r2):\n        return self.variance * tf.exp(-0.5 * r2)\n\n\nclass RationalQuadratic(IsotropicStationary):\n    """"""\n    Rational Quadratic kernel,\n\n    k(r) = \xcf\x83\xc2\xb2 (1 + r\xc2\xb2 / 2\xce\xb1\xe2\x84\x93\xc2\xb2)^(-\xce\xb1)\n\n    \xcf\x83\xc2\xb2 : variance\n    \xe2\x84\x93  : lengthscales\n    \xce\xb1  : alpha, determines relative weighting of small-scale and large-scale fluctuations\n\n    For \xce\xb1 \xe2\x86\x92 \xe2\x88\x9e, the RQ kernel becomes equivalent to the squared exponential.\n    """"""\n\n    def __init__(self, variance=1.0, lengthscales=1.0, alpha=1.0, active_dims=None):\n        super().__init__(variance=variance, lengthscales=lengthscales, active_dims=active_dims)\n        self.alpha = Parameter(alpha, transform=positive())\n\n    def K_r2(self, r2):\n        return self.variance * (1 + 0.5 * r2 / self.alpha) ** (-self.alpha)\n\n\nclass Exponential(IsotropicStationary):\n    """"""\n    The Exponential kernel. It is equivalent to a Matern12 kernel with doubled lengthscales.\n    """"""\n\n    def K_r(self, r):\n        return self.variance * tf.exp(-0.5 * r)\n\n\nclass Matern12(IsotropicStationary):\n    """"""\n    The Matern 1/2 kernel. Functions drawn from a GP with this kernel are not\n    differentiable anywhere. The kernel equation is\n\n    k(r) = \xcf\x83\xc2\xb2 exp{-r}\n\n    where:\n    r  is the Euclidean distance between the input points, scaled by the lengthscales parameter \xe2\x84\x93.\n    \xcf\x83\xc2\xb2 is the variance parameter\n    """"""\n\n    def K_r(self, r):\n        return self.variance * tf.exp(-r)\n\n\nclass Matern32(IsotropicStationary):\n    """"""\n    The Matern 3/2 kernel. Functions drawn from a GP with this kernel are once\n    differentiable. The kernel equation is\n\n    k(r) = \xcf\x83\xc2\xb2 (1 + \xe2\x88\x9a3r) exp{-\xe2\x88\x9a3 r}\n\n    where:\n    r  is the Euclidean distance between the input points, scaled by the lengthscales parameter \xe2\x84\x93,\n    \xcf\x83\xc2\xb2 is the variance parameter.\n    """"""\n\n    def K_r(self, r):\n        sqrt3 = np.sqrt(3.0)\n        return self.variance * (1.0 + sqrt3 * r) * tf.exp(-sqrt3 * r)\n\n\nclass Matern52(IsotropicStationary):\n    """"""\n    The Matern 5/2 kernel. Functions drawn from a GP with this kernel are twice\n    differentiable. The kernel equation is\n\n    k(r) = \xcf\x83\xc2\xb2 (1 + \xe2\x88\x9a5r + 5/3r\xc2\xb2) exp{-\xe2\x88\x9a5 r}\n\n    where:\n    r  is the Euclidean distance between the input points, scaled by the lengthscales parameter \xe2\x84\x93,\n    \xcf\x83\xc2\xb2 is the variance parameter.\n    """"""\n\n    def K_r(self, r):\n        sqrt5 = np.sqrt(5.0)\n        return self.variance * (1.0 + sqrt5 * r + 5.0 / 3.0 * tf.square(r)) * tf.exp(-sqrt5 * r)\n\n\nclass Cosine(AnisotropicStationary):\n    """"""\n    The Cosine kernel. Functions drawn from a GP with this kernel are sinusoids\n    (with a random phase).  The kernel equation is\n\n        k(r) = \xcf\x83\xc2\xb2 cos{2\xcf\x80d}\n\n    where:\n    d  is the sum of the per-dimension differences between the input points, scaled by the\n    lengthscale parameter \xe2\x84\x93 (i.e. \xce\xa3\xe1\xb5\xa2 [(X - X2\xe1\xb5\x80) / \xe2\x84\x93]\xe1\xb5\xa2),\n    \xcf\x83\xc2\xb2 is the variance parameter.\n    """"""\n\n    def K_d(self, d):\n        d = tf.reduce_sum(d, axis=-1)\n        return self.variance * tf.cos(2 * np.pi * d)\n'"
gpflow/likelihoods/__init__.py,0,"b'from .base import Likelihood, ScalarLikelihood, SwitchedLikelihood, MonteCarloLikelihood\nfrom .scalar_discrete import (\n    Bernoulli,\n    Ordinal,\n    Poisson,\n)\nfrom .scalar_continuous import (\n    Beta,\n    Exponential,\n    Gamma,\n    Gaussian,\n    StudentT,\n)\nfrom .misc import GaussianMC\nfrom .multiclass import (\n    MultiClass,\n    Softmax,\n    RobustMax,\n)\n'"
gpflow/likelihoods/base.py,22,"b'# Copyright 2016 Valentine Svensson, James Hensman, alexggmatthews, Alexis Boukouvalas\n# Copyright 2017 Artem Artemev @awav\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nLikelihoods are another core component of GPflow. This describes how likely the\ndata is under the assumptions made about the underlying latent functions\np(Y|F). Different likelihoods make different\nassumptions about the distribution of the data, as such different data-types\n(continuous, binary, ordinal, count) are better modelled with different\nlikelihood assumptions.\n\nUse of any likelihood other than Gaussian typically introduces the need to use\nan approximation to perform inference, if one isn\'t already needed. A\nvariational inference and MCMC models are included in GPflow and allow\napproximate inference with non-Gaussian likelihoods. An introduction to these\nmodels can be found :ref:`here <implemented_models>`. Specific notebooks\nillustrating non-Gaussian likelihood regressions are available for\n`classification <notebooks/classification.html>`_ (binary data), `ordinal\n<notebooks/ordinal.html>`_ and `multiclass <notebooks/multiclass.html>`_.\n\nCreating new likelihoods\n----------\nLikelihoods are defined by their\nlog-likelihood. When creating new likelihoods, the\n:func:`logp <gpflow.likelihoods.Likelihood.logp>` method (log p(Y|F)), the\n:func:`conditional_mean <gpflow.likelihoods.Likelihood.conditional_mean>`,\n:func:`conditional_variance\n<gpflow.likelihoods.Likelihood.conditional_variance>`.\n\nIn order to perform variational inference with non-Gaussian likelihoods a term\ncalled ``variational expectations``, \xe2\x88\xab q(F) log p(Y|F) dF, needs to\nbe computed under a Gaussian distribution q(F) ~ N(\xce\xbc, \xce\xa3).\n\nThe :func:`variational_expectations <gpflow.likelihoods.Likelihood.variational_expectations>`\nmethod can be overriden if this can be computed in closed form, otherwise; if\nthe new likelihood inherits\n:class:`Likelihood <gpflow.likelihoods.Likelihood>` the default will use\nGauss-Hermite numerical integration (works well when F is 1D\nor 2D), if the new likelihood inherits from\n:class:`MonteCarloLikelihood <gpflow.likelihoods.MonteCarloLikelihood>` the\nintegration is done by sampling (can be more suitable when F is higher dimensional).\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nimport abc\nimport warnings\n\nfrom ..base import Module\nfrom ..quadrature import hermgauss, ndiag_mc, ndiagquad\n\n\nclass Likelihood(Module, metaclass=abc.ABCMeta):\n    def __init__(self, latent_dim: int, observation_dim: int):\n        """"""\n        A base class for likelihoods, which specifies an observation model \n        connecting the latent functions (\'F\') to the data (\'Y\').\n\n        All of the members of this class are expected to obey some shape conventions, as specified\n        by latent_dim and observation_dim.\n\n        If we\'re operating on an array of function values \'F\', then the last dimension represents\n        multiple functions (preceding dimensions could represent different data points, or\n        different random samples, for example). Similarly, the last dimension of Y represents a\n        single data point. We check that the dimensions are as this object expects.\n\n        The return shapes of all functions in this class is the broadcasted shape of the arguments,\n        excluding the last dimension of each argument.\n\n        :param latent_dim: the dimension of the vector F of latent functions for a single data point\n        :param observation_dim: the dimension of the observation vector Y for a single data point\n        """"""\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.observation_dim = observation_dim\n\n    def _check_last_dims_valid(self, F, Y):\n        """"""\n        Assert that the dimensions of the latent functions F and the data Y are compatible.\n\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., observation_dim]\n        """"""\n        self._check_latent_dims(F)\n        self._check_data_dims(Y)\n\n    def _check_return_shape(self, result, F, Y):\n        """"""\n        Check that the shape of a computed statistic of the data\n        is the broadcasted shape from F and Y.\n\n        :param result: result Tensor, with shape [...]\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., observation_dim]\n        """"""\n        expected_shape = tf.broadcast_dynamic_shape(tf.shape(F)[:-1], tf.shape(Y)[:-1])\n        tf.debugging.assert_equal(tf.shape(result), expected_shape)\n\n    def _check_latent_dims(self, F):\n        """"""\n        Ensure that a tensor of latent functions F has latent_dim as right-most dimension.\n\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        """"""\n        tf.debugging.assert_shapes([(F, (..., self.latent_dim))])\n\n    def _check_data_dims(self, Y):\n        """"""\n        Ensure that a tensor of data Y has observation_dim as right-most dimension.\n\n        :param Y: observation Tensor, with shape [..., observation_dim]\n        """"""\n        tf.debugging.assert_shapes([(Y, (..., self.observation_dim))])\n\n    def log_prob(self, F, Y):\n        """"""\n        The log probability density log p(Y|F)\n\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., observation_dim]:\n        :returns: log pdf, with shape [...]\n        """"""\n        self._check_last_dims_valid(F, Y)\n        res = self._log_prob(F, Y)\n        self._check_return_shape(res, F, Y)\n        return res\n\n    @abc.abstractmethod\n    def _log_prob(self, F, Y):\n        raise NotImplementedError\n\n    def conditional_mean(self, F):\n        """"""\n        The conditional mean of Y|F: [E[Y\xe2\x82\x81|F], ..., E[Y\xe2\x82\x96|F]]\n        where K = observation_dim\n\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        :returns: mean [..., observation_dim]\n        """"""\n        self._check_latent_dims(F)\n        expected_Y = self._conditional_mean(F)\n        self._check_data_dims(expected_Y)\n        return expected_Y\n\n    def _conditional_mean(self, F):\n        raise NotImplementedError\n\n    def conditional_variance(self, F):\n        """"""\n        The conditional marginal variance of Y|F: [var(Y\xe2\x82\x81|F), ..., var(Y\xe2\x82\x96|F)]\n        where K = observation_dim\n\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        :returns: variance [..., observation_dim]\n        """"""\n        self._check_latent_dims(F)\n        var_Y = self._conditional_variance(F)\n        self._check_data_dims(var_Y)\n        return var_Y\n\n    def _conditional_variance(self, F):\n        raise NotImplementedError\n\n    def predict_mean_and_var(self, Fmu, Fvar):\n        """"""\n        Given a Normal distribution for the latent function,\n        return the mean and marginal variance of Y,\n\n        i.e. if\n            q(f) = N(Fmu, Fvar)\n\n        and this object represents\n\n            p(y|f)\n\n        then this method computes the predictive mean\n\n           \xe2\x88\xab\xe2\x88\xab y p(y|f)q(f) df dy\n\n        and the predictive variance\n\n           \xe2\x88\xab\xe2\x88\xab y\xc2\xb2 p(y|f)q(f) df dy  - [ \xe2\x88\xab\xe2\x88\xab y p(y|f)q(f) df dy ]\xc2\xb2\n\n\n        :param Fmu: mean function evaluation Tensor, with shape [..., latent_dim]\n        :param Fvar: variance of function evaluation Tensor, with shape [..., latent_dim]\n        :returns: mean and variance, both with shape [..., observation_dim]\n        """"""\n        self._check_latent_dims(Fmu)\n        self._check_latent_dims(Fvar)\n        mu, var = self._predict_mean_and_var(Fmu, Fvar)\n        self._check_data_dims(mu)\n        self._check_data_dims(var)\n        return mu, var\n\n    @abc.abstractmethod\n    def _predict_mean_and_var(self, Fmu, Fvar):\n        raise NotImplementedError\n\n    def predict_log_density(self, Fmu, Fvar, Y):\n        r""""""\n        Given a Normal distribution for the latent function, and a datum Y,\n        compute the log predictive density of Y,\n\n        i.e. if\n            q(F) = N(Fmu, Fvar)\n\n        and this object represents\n\n            p(y|F)\n\n        then this method computes the predictive density\n\n            log \xe2\x88\xab p(y=Y|F)q(F) df\n\n        :param Fmu: mean function evaluation Tensor, with shape [..., latent_dim]\n        :param Fvar: variance of function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., observation_dim]:\n        :returns: log predictive density, with shape [...]\n        """"""\n        tf.debugging.assert_equal(tf.shape(Fmu), tf.shape(Fvar))\n        self._check_last_dims_valid(Fmu, Y)\n        res = self._predict_log_density(Fmu, Fvar, Y)\n        self._check_return_shape(res, Fmu, Y)\n        return res\n\n    @abc.abstractmethod\n    def _predict_log_density(self, Fmu, Fvar, Y):\n        raise NotImplementedError\n\n    def predict_density(self, Fmu, Fvar, Y):\n        """"""\n        Deprecated: see `predict_log_density`\n        """"""\n        warnings.warn(\n            ""predict_density is deprecated and will be removed in GPflow 2.1, use predict_log_density instead"",\n            DeprecationWarning,\n        )\n        return self.predict_log_density(Fmu, Fvar, Y)\n\n    def variational_expectations(self, Fmu, Fvar, Y):\n        r""""""\n        Compute the expected log density of the data, given a Gaussian\n        distribution for the function values,\n\n        i.e. if\n            q(f) = N(Fmu, Fvar)\n\n        and this object represents\n\n            p(y|f)\n\n        then this method computes\n\n           \xe2\x88\xab log(p(y=Y|f)) q(f) df.\n\n        This only works if the broadcasting dimension of the statistics of q(f) (mean and variance)\n        are broadcastable with that of the data Y.\n\n        :param Fmu: mean function evaluation Tensor, with shape [..., latent_dim]\n        :param Fvar: variance of function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., observation_dim]:\n        :returns: expected log density of the data given q(F), with shape [...]\n        """"""\n        tf.debugging.assert_equal(tf.shape(Fmu), tf.shape(Fvar))\n        # returns an error if Y[:-1] and Fmu[:-1] do not broadcast together\n        _ = tf.broadcast_dynamic_shape(tf.shape(Fmu)[:-1], tf.shape(Y)[:-1])\n        self._check_last_dims_valid(Fmu, Y)\n        ret = self._variational_expectations(Fmu, Fvar, Y)\n        self._check_return_shape(ret, Fmu, Y)\n        return ret\n\n    @abc.abstractmethod\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        raise NotImplementedError\n\n\nclass ScalarLikelihood(Likelihood):\n    """"""\n    A likelihood class that helps with scalar likelihood functions: likelihoods where\n    each scalar latent function is associated with a single scalar observation variable.\n\n    If there are multiple latent functions, then there must be a corresponding number of data: we\n    check for this.\n\n    The `Likelihood` class contains methods to compute marginal statistics of functions\n    of the latents and the data \xcf\x95(y,f):\n     * variational_expectations:  \xcf\x95(y,f) = log p(y|f)\n     * predict_log_density: \xcf\x95(y,f) = p(y|f)\n    Those statistics are computed after having first marginalized the latent processes f\n    under a multivariate normal distribution q(f) that is fully factorized.\n\n    Some univariate integrals can be done by quadrature: we implement quadrature routines for 1D\n    integrals in this class, though they may be overwritten by inheriting classes where those\n    integrals are available in closed form.\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(latent_dim=None, observation_dim=None, **kwargs)\n        self.num_gauss_hermite_points = 20\n\n    def _check_last_dims_valid(self, F, Y):\n        """"""\n        Assert that the dimensions of the latent functions and the data are compatible\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., latent_dim]\n        """"""\n        tf.debugging.assert_shapes([(F, (..., ""num_latent"")), (Y, (..., ""num_latent""))])\n\n    def _log_prob(self, F, Y):\n        r""""""\n        Compute log p(Y|F), where by convention we sum out the last axis as it represented\n        independent latent functions and observations.\n        :param F: function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., latent_dim]\n        """"""\n        return tf.reduce_sum(self._scalar_log_prob(F, Y), axis=-1)\n\n    @abc.abstractmethod\n    def _scalar_log_prob(self, F, Y):\n        raise NotImplementedError\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        r""""""\n        Here, we implement a default Gauss-Hermite quadrature routine, but some\n        likelihoods (Gaussian, Poisson) will implement specific cases.\n        :param Fmu: mean function evaluation Tensor, with shape [..., latent_dim]\n        :param Fvar: variance of function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., latent_dim]:\n        :returns: variational expectations, with shape [...]\n        """"""\n        return tf.reduce_sum(\n            ndiagquad(self._scalar_log_prob, self.num_gauss_hermite_points, Fmu, Fvar, Y=Y),\n            axis=-1,\n        )\n\n    def _predict_log_density(self, Fmu, Fvar, Y):\n        r""""""\n        Here, we implement a default Gauss-Hermite quadrature routine, but some\n        likelihoods (Gaussian, Poisson) will implement specific cases.\n        :param Fmu: mean function evaluation Tensor, with shape [..., latent_dim]\n        :param Fvar: variance of function evaluation Tensor, with shape [..., latent_dim]\n        :param Y: observation Tensor, with shape [..., latent_dim]:\n        :returns: log predictive density, with shape [...]\n        """"""\n        return tf.reduce_sum(\n            ndiagquad(\n                self._scalar_log_prob, self.num_gauss_hermite_points, Fmu, Fvar, logspace=True, Y=Y,\n            ),\n            axis=-1,\n        )\n\n    def _predict_mean_and_var(self, Fmu, Fvar):\n        r""""""\n        Here, we implement a default Gauss-Hermite quadrature routine, but some\n        likelihoods (e.g. Gaussian) will implement specific cases.\n\n        :param Fmu: mean function evaluation Tensor, with shape [..., latent_dim]\n        :param Fvar: variance of function evaluation Tensor, with shape [..., latent_dim]\n        :returns: mean and variance, both with shape [..., observation_dim]\n        """"""\n\n        def integrand(*X):\n            return self.conditional_variance(*X) + self.conditional_mean(*X) ** 2\n\n        integrands = [self.conditional_mean, integrand]\n        E_y, E_y2 = ndiagquad(integrands, self.num_gauss_hermite_points, Fmu, Fvar)\n        V_y = E_y2 - E_y ** 2\n        return E_y, V_y\n\n\nclass SwitchedLikelihood(ScalarLikelihood):\n    def __init__(self, likelihood_list, **kwargs):\n        """"""\n        In this likelihood, we assume at extra column of Y, which contains\n        integers that specify a likelihood from the list of likelihoods.\n        """"""\n        super().__init__(**kwargs)\n        for l in likelihood_list:\n            assert isinstance(l, ScalarLikelihood)\n        self.likelihoods = likelihood_list\n\n    def _partition_and_stitch(self, args, func_name):\n        """"""\n        args is a list of tensors, to be passed to self.likelihoods.<func_name>\n\n        args[-1] is the \'Y\' argument, which contains the indexes to self.likelihoods.\n\n        This function splits up the args using dynamic_partition, calls the\n        relevant function on the likelihoods, and re-combines the result.\n        """"""\n        # get the index from Y\n        Y = args[-1]\n        ind = Y[..., -1]\n        ind = tf.cast(ind, tf.int32)\n        Y = Y[..., :-1]\n        args[-1] = Y\n\n        # split up the arguments into chunks corresponding to the relevant likelihoods\n        args = zip(*[tf.dynamic_partition(X, ind, len(self.likelihoods)) for X in args])\n\n        # apply the likelihood-function to each section of the data\n        funcs = [getattr(lik, func_name) for lik in self.likelihoods]\n        results = [f(*args_i) for f, args_i in zip(funcs, args)]\n\n        # stitch the results back together\n        partitions = tf.dynamic_partition(tf.range(0, tf.size(ind)), ind, len(self.likelihoods))\n        results = tf.dynamic_stitch(partitions, results)\n\n        return results\n\n    def _check_last_dims_valid(self, F, Y):\n        tf.assert_equal(tf.shape(F)[-1], tf.shape(Y)[-1] - 1)\n\n    def _scalar_log_prob(self, F, Y):\n        return self._partition_and_stitch([F, Y], ""_scalar_log_prob"")\n\n    def _predict_log_density(self, Fmu, Fvar, Y):\n        return self._partition_and_stitch([Fmu, Fvar, Y], ""predict_log_density"")\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        return self._partition_and_stitch([Fmu, Fvar, Y], ""variational_expectations"")\n\n    def _predict_mean_and_var(self, Fmu, Fvar):\n        mvs = [lik.predict_mean_and_var(Fmu, Fvar) for lik in self.likelihoods]\n        mu_list, var_list = zip(*mvs)\n        mu = tf.concat(mu_list, 1)\n        var = tf.concat(var_list, 1)\n        return mu, var\n\n    def _conditional_mean(self, F):\n        raise NotImplementedError\n\n    def _conditional_variance(self, F):\n        raise NotImplementedError\n\n\nclass MonteCarloLikelihood(Likelihood):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_monte_carlo_points = 100\n\n    def _mc_quadrature(self, funcs, Fmu, Fvar, logspace: bool = False, epsilon=None, **Ys):\n        return ndiag_mc(funcs, self.num_monte_carlo_points, Fmu, Fvar, logspace, epsilon, **Ys)\n\n    def _predict_mean_and_var(self, Fmu, Fvar, epsilon=None):\n        r""""""\n        Given a Normal distribution for the latent function,\n        return the mean of Y\n\n        if\n            q(f) = N(Fmu, Fvar)\n\n        and this object represents\n\n            p(y|f)\n\n        then this method computes the predictive mean\n\n           \xe2\x88\xab\xe2\x88\xab y p(y|f)q(f) df dy\n\n        and the predictive variance\n\n           \xe2\x88\xab\xe2\x88\xab y\xc2\xb2 p(y|f)q(f) df dy  - [ \xe2\x88\xab\xe2\x88\xab y p(y|f)q(f) df dy ]\xc2\xb2\n\n        Here, we implement a default Monte Carlo routine.\n        """"""\n        integrand2 = lambda *X: self.conditional_variance(*X) + tf.square(self.conditional_mean(*X))\n        E_y, E_y2 = self._mc_quadrature(\n            [self.conditional_mean, integrand2], Fmu, Fvar, epsilon=epsilon\n        )\n        V_y = E_y2 - tf.square(E_y)\n        return E_y, V_y  # [N, D]\n\n    def _predict_log_density(self, Fmu, Fvar, Y, epsilon=None):\n        r""""""\n        Given a Normal distribution for the latent function, and a datum Y,\n        compute the log predictive density of Y.\n\n        i.e. if\n            q(f) = N(Fmu, Fvar)\n\n        and this object represents\n\n            p(y|f)\n\n        then this method computes the predictive density\n\n            log \xe2\x88\xab p(y=Y|f)q(f) df\n\n        Here, we implement a default Monte Carlo routine.\n        """"""\n        return tf.reduce_sum(\n            self._mc_quadrature(self.log_prob, Fmu, Fvar, Y=Y, logspace=True, epsilon=epsilon),\n            axis=-1,\n        )\n\n    def _variational_expectations(self, Fmu, Fvar, Y, epsilon=None):\n        r""""""\n        Compute the expected log density of the data, given a Gaussian\n        distribution for the function values.\n\n        if\n            q(f) = N(Fmu, Fvar)  - Fmu: [N, D]  Fvar: [N, D]\n\n        and this object represents\n\n            p(y|f)  - Y: [N, 1]\n\n        then this method computes\n\n           \xe2\x88\xab (log p(y|f)) q(f) df.\n\n\n        Here, we implement a default Monte Carlo quadrature routine.\n        """"""\n        return tf.reduce_sum(\n            self._mc_quadrature(self.log_prob, Fmu, Fvar, Y=Y, epsilon=epsilon), axis=-1\n        )\n'"
gpflow/likelihoods/misc.py,0,"b'from .base import MonteCarloLikelihood\nfrom .scalar_continuous import Gaussian\n\n\nclass GaussianMC(MonteCarloLikelihood, Gaussian):\n    """"""\n    Stochastic version of Gaussian likelihood for demonstration purposes only.\n    """"""\n\n    pass\n'"
gpflow/likelihoods/multiclass.py,29,"b'import numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom ..base import Module, Parameter\nfrom ..config import default_float\nfrom ..utilities import to_default_float, to_default_int\nfrom ..quadrature import hermgauss\nfrom .base import Likelihood, MonteCarloLikelihood\n\n\nclass Softmax(MonteCarloLikelihood):\n    """"""\n    The soft-max multi-class likelihood.  It can only provide a stochastic\n    Monte-Carlo estimate of the variational expectations term, but this\n    added variance tends to be small compared to that due to mini-batching\n    (when using the SVGP model).\n    """"""\n\n    def __init__(self, num_classes, **kwargs):\n        super().__init__(latent_dim=num_classes, observation_dim=None, **kwargs)\n        self.num_classes = self.latent_dim\n\n    def _log_prob(self, F, Y):\n        return -tf.nn.sparse_softmax_cross_entropy_with_logits(logits=F, labels=Y[:, 0])\n\n    def _conditional_mean(self, F):\n        return tf.nn.softmax(F)\n\n    def _conditional_variance(self, F):\n        p = self.conditional_mean(F)\n        return p - p ** 2\n\n\nclass RobustMax(Module):\n    """"""\n    This class represent a multi-class inverse-link function. Given a vector\n    f=[f_1, f_2, ... f_k], the result of the mapping is\n\n    y = [y_1 ... y_k]\n\n    with\n\n    y_i = (1-epsilon)  i == argmax(f)\n          epsilon/(k-1)  otherwise\n\n    where k is the number of classes.\n    """"""\n\n    def __init__(self, num_classes, epsilon=1e-3, **kwargs):\n        """"""\n        `epsilon` represents the fraction of \'errors\' in the labels of the\n        dataset. This may be a hard parameter to optimize, so by default\n        it is set un-trainable, at a small value.\n        """"""\n        super().__init__(**kwargs)\n        transform = tfp.bijectors.Sigmoid()\n        prior = tfp.distributions.Beta(to_default_float(0.2), to_default_float(5.0))\n        self.epsilon = Parameter(epsilon, transform=transform, prior=prior, trainable=False)\n        self.num_classes = num_classes\n        self._squash = 1e-6\n\n    def __call__(self, F):\n        i = tf.argmax(F, 1)\n        return tf.one_hot(\n            i, self.num_classes, tf.squeeze(1.0 - self.epsilon), tf.squeeze(self.eps_k1)\n        )\n\n    @property\n    def eps_k1(self):\n        return self.epsilon / (self.num_classes - 1.0)\n\n    def safe_sqrt(self, val):\n        return tf.sqrt(tf.clip_by_value(val, 1e-10, np.inf))\n\n    def prob_is_largest(self, Y, mu, var, gh_x, gh_w):\n        Y = to_default_int(Y)\n        # work out what the mean and variance is of the indicated latent function.\n        oh_on = tf.cast(\n            tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 1.0, 0.0), dtype=mu.dtype\n        )\n        mu_selected = tf.reduce_sum(oh_on * mu, 1)\n        var_selected = tf.reduce_sum(oh_on * var, 1)\n\n        # generate Gauss Hermite grid\n        X = tf.reshape(mu_selected, (-1, 1)) + gh_x * tf.reshape(\n            self.safe_sqrt(2.0 * var_selected), (-1, 1)\n        )\n\n        # compute the CDF of the Gaussian between the latent functions and the grid (including the selected function)\n        dist = (tf.expand_dims(X, 1) - tf.expand_dims(mu, 2)) / tf.expand_dims(\n            self.safe_sqrt(var), 2\n        )\n        cdfs = 0.5 * (1.0 + tf.math.erf(dist / np.sqrt(2.0)))\n\n        cdfs = cdfs * (1 - 2 * self._squash) + self._squash\n\n        # blank out all the distances on the selected latent function\n        oh_off = tf.cast(\n            tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 0.0, 1.0), dtype=mu.dtype\n        )\n        cdfs = cdfs * tf.expand_dims(oh_off, 2) + tf.expand_dims(oh_on, 2)\n\n        # take the product over the latent functions, and the sum over the GH grid.\n        return tf.reduce_prod(cdfs, axis=[1]) @ tf.reshape(gh_w / np.sqrt(np.pi), (-1, 1))\n\n\nclass MultiClass(Likelihood):\n    def __init__(self, num_classes, invlink=None, **kwargs):\n        """"""\n        A likelihood for multi-way classification.  Currently the only valid\n        choice of inverse-link function (invlink) is an instance of RobustMax.\n\n        For most problems, the stochastic `Softmax` likelihood may be more\n        appropriate (note that you then cannot use Scipy optimizer).\n        """"""\n        super().__init__(latent_dim=num_classes, observation_dim=None, **kwargs)\n        self.num_classes = num_classes\n        self.num_gauss_hermite_points = 20\n\n        if invlink is None:\n            invlink = RobustMax(self.num_classes)\n\n        if not isinstance(invlink, RobustMax):\n            raise NotImplementedError\n\n        self.invlink = invlink\n\n    def _log_prob(self, F, Y):\n        hits = tf.equal(tf.expand_dims(tf.argmax(F, 1), 1), tf.cast(Y, tf.int64))\n        yes = tf.ones(tf.shape(Y), dtype=default_float()) - self.invlink.epsilon\n        no = tf.zeros(tf.shape(Y), dtype=default_float()) + self.invlink.eps_k1\n        p = tf.where(hits, yes, no)\n        return tf.reduce_sum(tf.math.log(p), axis=-1)\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n        p = self.invlink.prob_is_largest(Y, Fmu, Fvar, gh_x, gh_w)\n        ve = p * tf.math.log(1.0 - self.invlink.epsilon) + (1.0 - p) * tf.math.log(\n            self.invlink.eps_k1\n        )\n        return tf.reduce_sum(ve, axis=-1)\n\n    def _predict_mean_and_var(self, Fmu, Fvar):\n        possible_outputs = [\n            tf.fill(tf.stack([tf.shape(Fmu)[0], 1]), np.array(i, dtype=np.int64))\n            for i in range(self.num_classes)\n        ]\n        ps = [self._predict_non_logged_density(Fmu, Fvar, po) for po in possible_outputs]\n        ps = tf.transpose(tf.stack([tf.reshape(p, (-1,)) for p in ps]))\n        return ps, ps - tf.square(ps)\n\n    def _predict_log_density(self, Fmu, Fvar, Y):\n        return tf.reduce_sum(tf.math.log(self._predict_non_logged_density(Fmu, Fvar, Y)), axis=-1)\n\n    def _predict_non_logged_density(self, Fmu, Fvar, Y):\n        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n        p = self.invlink.prob_is_largest(Y, Fmu, Fvar, gh_x, gh_w)\n        den = p * (1.0 - self.invlink.epsilon) + (1.0 - p) * (self.invlink.eps_k1)\n        return den\n\n    def _conditional_mean(self, F):\n        return self.invlink(F)\n\n    def _conditional_variance(self, F):\n        p = self.conditional_mean(F)\n        return p - tf.square(p)\n'"
gpflow/likelihoods/scalar_continuous.py,18,"b'import numpy as np\nimport tensorflow as tf\n\nfrom .. import logdensities\nfrom ..base import Parameter\nfrom ..utilities import positive\nfrom .base import ScalarLikelihood\nfrom .utils import inv_probit\n\n\nclass Gaussian(ScalarLikelihood):\n    r""""""\n    The Gaussian likelihood is appropriate where uncertainties associated with\n    the data are believed to follow a normal distribution, with constant\n    variance.\n\n    Very small uncertainties can lead to numerical instability during the\n    optimization process. A lower bound of 1e-6 is therefore imposed on the\n    likelihood variance by default.\n    """"""\n\n    DEFAULT_VARIANCE_LOWER_BOUND = 1e-6\n\n    def __init__(self, variance=1.0, variance_lower_bound=DEFAULT_VARIANCE_LOWER_BOUND, **kwargs):\n        """"""\n        :param variance: The noise variance; must be greater than\n            ``variance_lower_bound``.\n        :param variance_lower_bound: The lower (exclusive) bound of ``variance``.\n        :param kwargs: Keyword arguments forwarded to :class:`ScalarLikelihood`.\n        """"""\n        super().__init__(**kwargs)\n\n        if variance <= variance_lower_bound:\n            raise ValueError(\n                f""The variance of the Gaussian likelihood must be strictly greater than {variance_lower_bound}""\n            )\n\n        self.variance = Parameter(variance, transform=positive(lower=variance_lower_bound))\n\n    def _scalar_log_prob(self, F, Y):\n        return logdensities.gaussian(Y, F, self.variance)\n\n    def _conditional_mean(self, F):  # pylint: disable=R0201\n        return tf.identity(F)\n\n    def _conditional_variance(self, F):\n        return tf.fill(tf.shape(F), tf.squeeze(self.variance))\n\n    def _predict_mean_and_var(self, Fmu, Fvar):\n        return tf.identity(Fmu), Fvar + self.variance\n\n    def _predict_log_density(self, Fmu, Fvar, Y):\n        return tf.reduce_sum(logdensities.gaussian(Y, Fmu, Fvar + self.variance), axis=-1)\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        return tf.reduce_sum(\n            -0.5 * np.log(2 * np.pi)\n            - 0.5 * tf.math.log(self.variance)\n            - 0.5 * ((Y - Fmu) ** 2 + Fvar) / self.variance,\n            axis=-1,\n        )\n\n\nclass Exponential(ScalarLikelihood):\n    def __init__(self, invlink=tf.exp, **kwargs):\n        super().__init__(**kwargs)\n        self.invlink = invlink\n\n    def _scalar_log_prob(self, F, Y):\n        return logdensities.exponential(Y, self.invlink(F))\n\n    def _conditional_mean(self, F):\n        return self.invlink(F)\n\n    def _conditional_variance(self, F):\n        return tf.square(self.invlink(F))\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        if self.invlink is tf.exp:\n            return tf.reduce_sum(-tf.exp(-Fmu + Fvar / 2) * Y - Fmu, axis=-1)\n        return super()._variational_expectations(Fmu, Fvar, Y)\n\n\nclass StudentT(ScalarLikelihood):\n    def __init__(self, scale=1.0, df=3.0, **kwargs):\n        """"""\n        :param scale float: scale parameter\n        :param df float: degrees of freedom\n        """"""\n        super().__init__(**kwargs)\n        self.df = df\n        self.scale = Parameter(scale, transform=positive())\n\n    def _scalar_log_prob(self, F, Y):\n        return logdensities.student_t(Y, F, self.scale, self.df)\n\n    def _conditional_mean(self, F):\n        return F\n\n    def _conditional_variance(self, F):\n        var = (self.scale ** 2) * (self.df / (self.df - 2.0))\n        return tf.fill(tf.shape(F), tf.squeeze(var))\n\n\nclass Gamma(ScalarLikelihood):\n    """"""\n    Use the transformed GP to give the *scale* (inverse rate) of the Gamma\n    """"""\n\n    def __init__(self, invlink=tf.exp, **kwargs):\n        super().__init__(**kwargs)\n        self.invlink = invlink\n        self.shape = Parameter(1.0, transform=positive())\n\n    def _scalar_log_prob(self, F, Y):\n        return logdensities.gamma(Y, self.shape, self.invlink(F))\n\n    def _conditional_mean(self, F):\n        return self.shape * self.invlink(F)\n\n    def _conditional_variance(self, F):\n        scale = self.invlink(F)\n        return self.shape * (scale ** 2)\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        if self.invlink is tf.exp:\n            return tf.reduce_sum(\n                -self.shape * Fmu\n                - tf.math.lgamma(self.shape)\n                + (self.shape - 1.0) * tf.math.log(Y)\n                - Y * tf.exp(-Fmu + Fvar / 2.0),\n                axis=-1,\n            )\n        else:\n            return super()._variational_expectations(Fmu, Fvar, Y)\n\n\nclass Beta(ScalarLikelihood):\n    """"""\n    This uses a reparameterisation of the Beta density. We have the mean of the\n    Beta distribution given by the transformed process:\n\n        m = invlink(f)\n\n    and a scale parameter. The familiar \xce\xb1, \xce\xb2 parameters are given by\n\n        m     = \xce\xb1 / (\xce\xb1 + \xce\xb2)\n        scale = \xce\xb1 + \xce\xb2\n\n    so:\n        \xce\xb1 = scale * m\n        \xce\xb2  = scale * (1-m)\n    """"""\n\n    def __init__(self, invlink=inv_probit, scale=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.scale = Parameter(scale, transform=positive())\n        self.invlink = invlink\n\n    def _scalar_log_prob(self, F, Y):\n        mean = self.invlink(F)\n        alpha = mean * self.scale\n        beta = self.scale - alpha\n        return logdensities.beta(Y, alpha, beta)\n\n    def _conditional_mean(self, F):\n        return self.invlink(F)\n\n    def _conditional_variance(self, F):\n        mean = self.invlink(F)\n        return (mean - tf.square(mean)) / (self.scale + 1.0)\n'"
gpflow/likelihoods/scalar_discrete.py,22,"b'import numpy as np\nimport tensorflow as tf\n\nfrom .. import logdensities\nfrom ..base import Parameter\nfrom ..config import default_float\nfrom ..utilities import positive, to_default_int\nfrom .base import ScalarLikelihood\nfrom .utils import inv_probit\n\n\nclass Poisson(ScalarLikelihood):\n    r""""""\n    Poisson likelihood for use with count data, where the rate is given by the (transformed) GP.\n\n    let g(.) be the inverse-link function, then this likelihood represents\n\n    p(y\xe1\xb5\xa2 | f\xe1\xb5\xa2) = Poisson(y\xe1\xb5\xa2 | g(f\xe1\xb5\xa2) * binsize)\n\n    Note:binsize\n    For use in a Log Gaussian Cox process (doubly stochastic model) where the\n    rate function of an inhomogeneous Poisson process is given by a GP.  The\n    intractable likelihood can be approximated via a Riemann sum (with bins\n    of size \'binsize\') and using this Poisson likelihood.\n    """"""\n\n    def __init__(self, invlink=tf.exp, binsize=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.invlink = invlink\n        self.binsize = np.array(binsize, dtype=default_float())\n\n    def _scalar_log_prob(self, F, Y):\n        return logdensities.poisson(Y, self.invlink(F) * self.binsize)\n\n    def _conditional_variance(self, F):\n        return self.invlink(F) * self.binsize\n\n    def _conditional_mean(self, F):\n        return self.invlink(F) * self.binsize\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        if self.invlink is tf.exp:\n            return tf.reduce_sum(\n                Y * Fmu\n                - tf.exp(Fmu + Fvar / 2) * self.binsize\n                - tf.math.lgamma(Y + 1)\n                + Y * tf.math.log(self.binsize),\n                axis=-1,\n            )\n        return super()._variational_expectations(Fmu, Fvar, Y)\n\n\nclass Bernoulli(ScalarLikelihood):\n    def __init__(self, invlink=inv_probit, **kwargs):\n        super().__init__(**kwargs)\n        self.invlink = invlink\n\n    def _scalar_log_prob(self, F, Y):\n        return logdensities.bernoulli(Y, self.invlink(F))\n\n    def _predict_mean_and_var(self, Fmu, Fvar):\n        if self.invlink is inv_probit:\n            p = inv_probit(Fmu / tf.sqrt(1 + Fvar))\n            return p, p - tf.square(p)\n        else:\n            # for other invlink, use quadrature\n            return super()._predict_mean_and_var(Fmu, Fvar)\n\n    def _predict_log_density(self, Fmu, Fvar, Y):\n        p = self.predict_mean_and_var(Fmu, Fvar)[0]\n        return tf.reduce_sum(logdensities.bernoulli(Y, p), axis=-1)\n\n    def _conditional_mean(self, F):\n        return self.invlink(F)\n\n    def _conditional_variance(self, F):\n        p = self.conditional_mean(F)\n        return p - (p ** 2)\n\n\nclass Ordinal(ScalarLikelihood):\n    """"""\n    A likelihood for doing ordinal regression.\n\n    The data are integer values from 0 to k, and the user must specify (k-1)\n    \'bin edges\' which define the points at which the labels switch. Let the bin\n    edges be [a\xe2\x82\x80, a\xe2\x82\x81, ... a\xe2\x82\x96\xe2\x82\x8b\xe2\x82\x81], then the likelihood is\n\n    p(Y=0|F) = \xc9\xb8((a\xe2\x82\x80 - F) / \xcf\x83)\n    p(Y=1|F) = \xc9\xb8((a\xe2\x82\x81 - F) / \xcf\x83) - \xc9\xb8((a\xe2\x82\x80 - F) / \xcf\x83)\n    p(Y=2|F) = \xc9\xb8((a\xe2\x82\x82 - F) / \xcf\x83) - \xc9\xb8((a\xe2\x82\x81 - F) / \xcf\x83)\n    ...\n    p(Y=K|F) = 1 - \xc9\xb8((a\xe2\x82\x96\xe2\x82\x8b\xe2\x82\x81 - F) / \xcf\x83)\n\n    where \xc9\xb8 is the cumulative density function of a Gaussian (the inverse probit\n    function) and \xcf\x83 is a parameter to be learned. A reference is:\n\n    @article{chu2005gaussian,\n      title={Gaussian processes for ordinal regression},\n      author={Chu, Wei and Ghahramani, Zoubin},\n      journal={Journal of Machine Learning Research},\n      volume={6},\n      number={Jul},\n      pages={1019--1041},\n      year={2005}\n    }\n    """"""\n\n    def __init__(self, bin_edges, **kwargs):\n        """"""\n        bin_edges is a numpy array specifying at which function value the\n        output label should switch. If the possible Y values are 0...K, then\n        the size of bin_edges should be (K-1).\n        """"""\n        super().__init__(**kwargs)\n        self.bin_edges = bin_edges\n        self.num_bins = bin_edges.size + 1\n        self.sigma = Parameter(1.0, transform=positive())\n\n    def _scalar_log_prob(self, F, Y):\n        Y = to_default_int(Y)\n        scaled_bins_left = tf.concat([self.bin_edges / self.sigma, np.array([np.inf])], 0)\n        scaled_bins_right = tf.concat([np.array([-np.inf]), self.bin_edges / self.sigma], 0)\n        selected_bins_left = tf.gather(scaled_bins_left, Y)\n        selected_bins_right = tf.gather(scaled_bins_right, Y)\n\n        return tf.math.log(\n            inv_probit(selected_bins_left - F / self.sigma)\n            - inv_probit(selected_bins_right - F / self.sigma)\n            + 1e-6\n        )\n\n    def _make_phi(self, F):\n        """"""\n        A helper function for making predictions. Constructs a probability\n        matrix where each row output the probability of the corresponding\n        label, and the rows match the entries of F.\n\n        Note that a matrix of F values is flattened.\n        """"""\n        scaled_bins_left = tf.concat([self.bin_edges / self.sigma, np.array([np.inf])], 0)\n        scaled_bins_right = tf.concat([np.array([-np.inf]), self.bin_edges / self.sigma], 0)\n        return inv_probit(scaled_bins_left - tf.reshape(F, (-1, 1)) / self.sigma) - inv_probit(\n            scaled_bins_right - tf.reshape(F, (-1, 1)) / self.sigma\n        )\n\n    def _conditional_mean(self, F):\n        phi = self._make_phi(F)\n        Ys = tf.reshape(np.arange(self.num_bins, dtype=default_float()), (-1, 1))\n        return tf.reshape(tf.linalg.matmul(phi, Ys), tf.shape(F))\n\n    def _conditional_variance(self, F):\n        phi = self._make_phi(F)\n        Ys = tf.reshape(np.arange(self.num_bins, dtype=default_float()), (-1, 1))\n        E_y = phi @ Ys\n        E_y2 = phi @ (Ys ** 2)\n        return tf.reshape(E_y2 - E_y ** 2, tf.shape(F))\n'"
gpflow/likelihoods/utils.py,1,b'import numpy as np\nimport tensorflow as tf\n\n\ndef inv_probit(x):\n    jitter = 1e-3  # ensures output is strictly between 0 and 1\n    return 0.5 * (1.0 + tf.math.erf(x / np.sqrt(2.0))) * (1 - 2 * jitter) + jitter\n'
gpflow/models/__init__.py,0,"b'# Copyright 2017 Artem Artemev @awav\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# flake8: noqa\n\nfrom .gplvm import GPLVM, BayesianGPLVM\nfrom .gpmc import GPMC\nfrom .gpr import GPR\nfrom .model import BayesianModel, GPModel\nfrom .training_mixins import (\n    ExternalDataTrainingLossMixin,\n    InternalDataTrainingLossMixin,\n)\n\n# from .gplvm import PCA_reduce\nfrom .sgpmc import SGPMC\nfrom .sgpr import GPRFITC, SGPR\nfrom .svgp import SVGP\nfrom .vgp import VGP, VGPOpperArchambeau\nfrom .util import (\n    training_loss,\n    training_loss_closure,\n    maximum_log_likelihood_objective,\n)\n'"
gpflow/models/gplvm.py,56,"b'# Copyright 2016 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .. import covariances, kernels, likelihoods\nfrom ..base import Parameter\nfrom ..config import default_float, default_jitter\nfrom ..expectations import expectation\nfrom ..inducing_variables import InducingPoints\nfrom ..kernels import Kernel\nfrom ..mean_functions import MeanFunction, Zero\nfrom ..probability_distributions import DiagonalGaussian\nfrom ..utilities import positive, to_default_float\nfrom ..utilities.ops import pca_reduce\nfrom .gpr import GPR\nfrom .model import GPModel, MeanAndVariance\nfrom .training_mixins import InputData, InternalDataTrainingLossMixin, OutputData\nfrom .util import data_input_to_tensor, inducingpoint_wrapper\n\n\nclass GPLVM(GPR):\n    """"""\n    Standard GPLVM where the likelihood can be optimised with respect to the latent X.\n    """"""\n\n    def __init__(\n        self,\n        data: OutputData,\n        latent_dim: int,\n        X_data_mean: Optional[tf.Tensor] = None,\n        kernel: Optional[Kernel] = None,\n        mean_function: Optional[MeanFunction] = None,\n    ):\n        """"""\n        Initialise GPLVM object. This method only works with a Gaussian likelihood.\n\n        :param data: y data matrix, size N (number of points) x D (dimensions)\n        :param latent_dim: the number of latent dimensions (Q)\n        :param X_data_mean: latent positions ([N, Q]), for the initialisation of the latent space.\n        :param kernel: kernel specification, by default Squared Exponential\n        :param mean_function: mean function, by default None.\n        """"""\n        if X_data_mean is None:\n            X_data_mean = pca_reduce(data, latent_dim)\n\n        num_latent_gps = X_data_mean.shape[1]\n        if num_latent_gps != latent_dim:\n            msg = ""Passed in number of latent {0} does not match initial X {1}.""\n            raise ValueError(msg.format(latent_dim, num_latent_gps))\n\n        if mean_function is None:\n            mean_function = Zero()\n\n        if kernel is None:\n            kernel = kernels.SquaredExponential(lengthscales=tf.ones((latent_dim,)))\n\n        if data.shape[1] < num_latent_gps:\n            raise ValueError(""More latent dimensions than observed."")\n\n        gpr_data = (Parameter(X_data_mean), data_input_to_tensor(data))\n        super().__init__(gpr_data, kernel, mean_function=mean_function)\n\n\nclass BayesianGPLVM(GPModel, InternalDataTrainingLossMixin):\n    def __init__(\n        self,\n        data: OutputData,\n        X_data_mean: tf.Tensor,\n        X_data_var: tf.Tensor,\n        kernel: Kernel,\n        num_inducing_variables: Optional[int] = None,\n        inducing_variable=None,\n        X_prior_mean=None,\n        X_prior_var=None,\n    ):\n        """"""\n        Initialise Bayesian GPLVM object. This method only works with a Gaussian likelihood.\n\n        :param data: data matrix, size N (number of points) x D (dimensions)\n        :param X_data_mean: initial latent positions, size N (number of points) x Q (latent dimensions).\n        :param X_data_var: variance of latent positions ([N, Q]), for the initialisation of the latent space.\n        :param kernel: kernel specification, by default Squared Exponential\n        :param num_inducing_variables: number of inducing points, M\n        :param inducing_variable: matrix of inducing points, size M (inducing points) x Q (latent dimensions). By default\n            random permutation of X_data_mean.\n        :param X_prior_mean: prior mean used in KL term of bound. By default 0. Same size as X_data_mean.\n        :param X_prior_var: prior variance used in KL term of bound. By default 1.\n        """"""\n        num_data, num_latent_gps = X_data_mean.shape\n        super().__init__(kernel, likelihoods.Gaussian(), num_latent_gps=num_latent_gps)\n        self.data = data_input_to_tensor(data)\n        assert X_data_var.ndim == 2\n\n        self.X_data_mean = Parameter(X_data_mean)\n        self.X_data_var = Parameter(X_data_var, transform=positive())\n\n        self.num_data = num_data\n        self.output_dim = self.data.shape[-1]\n\n        assert np.all(X_data_mean.shape == X_data_var.shape)\n        assert X_data_mean.shape[0] == self.data.shape[0], ""X mean and Y must be same size.""\n        assert X_data_var.shape[0] == self.data.shape[0], ""X var and Y must be same size.""\n\n        if (inducing_variable is None) == (num_inducing_variables is None):\n            raise ValueError(\n                ""BayesianGPLVM needs exactly one of `inducing_variable` and `num_inducing_variables`""\n            )\n\n        if inducing_variable is None:\n            # By default we initialize by subset of initial latent points\n            # Note that tf.random.shuffle returns a copy, it does not shuffle in-place\n            Z = tf.random.shuffle(X_data_mean)[:num_inducing_variables]\n            inducing_variable = InducingPoints(Z)\n\n        self.inducing_variable = inducingpoint_wrapper(inducing_variable)\n\n        assert X_data_mean.shape[1] == self.num_latent_gps\n\n        # deal with parameters for the prior mean variance of X\n        if X_prior_mean is None:\n            X_prior_mean = tf.zeros((self.num_data, self.num_latent_gps), dtype=default_float())\n        if X_prior_var is None:\n            X_prior_var = tf.ones((self.num_data, self.num_latent_gps))\n\n        self.X_prior_mean = tf.convert_to_tensor(np.atleast_1d(X_prior_mean), dtype=default_float())\n        self.X_prior_var = tf.convert_to_tensor(np.atleast_1d(X_prior_var), dtype=default_float())\n\n        assert self.X_prior_mean.shape[0] == self.num_data\n        assert self.X_prior_mean.shape[1] == self.num_latent_gps\n        assert self.X_prior_var.shape[0] == self.num_data\n        assert self.X_prior_var.shape[1] == self.num_latent_gps\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.elbo()\n\n    def elbo(self) -> tf.Tensor:\n        """"""\n        Construct a tensorflow function to compute the bound on the marginal\n        likelihood.\n        """"""\n        Y_data = self.data\n\n        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)\n\n        num_inducing = len(self.inducing_variable)\n        psi0 = tf.reduce_sum(expectation(pX, self.kernel))\n        psi1 = expectation(pX, (self.kernel, self.inducing_variable))\n        psi2 = tf.reduce_sum(\n            expectation(\n                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)\n            ),\n            axis=0,\n        )\n        cov_uu = covariances.Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n        L = tf.linalg.cholesky(cov_uu)\n        sigma2 = self.likelihood.variance\n        sigma = tf.sqrt(sigma2)\n\n        # Compute intermediate matrices\n        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma\n        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)\n        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2\n        B = AAT + tf.eye(num_inducing, dtype=default_float())\n        LB = tf.linalg.cholesky(B)\n        log_det_B = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))\n        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma\n\n        # KL[q(x) || p(x)]\n        dX_data_var = (\n            self.X_data_var\n            if self.X_data_var.shape.ndims == 2\n            else tf.linalg.diag_part(self.X_data_var)\n        )\n        NQ = to_default_float(tf.size(self.X_data_mean))\n        D = to_default_float(tf.shape(Y_data)[1])\n        KL = -0.5 * tf.reduce_sum(tf.math.log(dX_data_var))\n        KL += 0.5 * tf.reduce_sum(tf.math.log(self.X_prior_var))\n        KL -= 0.5 * NQ\n        KL += 0.5 * tf.reduce_sum(\n            (tf.square(self.X_data_mean - self.X_prior_mean) + dX_data_var) / self.X_prior_var\n        )\n\n        # compute log marginal bound\n        ND = to_default_float(tf.size(Y_data))\n        bound = -0.5 * ND * tf.math.log(2 * np.pi * sigma2)\n        bound += -0.5 * D * log_det_B\n        bound += -0.5 * tf.reduce_sum(tf.square(Y_data)) / sigma2\n        bound += 0.5 * tf.reduce_sum(tf.square(c))\n        bound += -0.5 * D * (tf.reduce_sum(psi0) / sigma2 - tf.reduce_sum(tf.linalg.diag_part(AAT)))\n        bound -= KL\n        return bound\n\n    def predict_f(\n        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False\n    ) -> MeanAndVariance:\n        """"""\n        Compute the mean and variance of the latent function at some new points.\n        Note that this is very similar to the SGPR prediction, for which\n        there are notes in the SGPR notebook.\n\n        Note: This model does not allow full output covariances.\n\n        :param Xnew: points at which to predict\n        """"""\n        if full_output_cov:\n            raise NotImplementedError\n\n        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)\n\n        Y_data = self.data\n        num_inducing = len(self.inducing_variable)\n        psi1 = expectation(pX, (self.kernel, self.inducing_variable))\n        psi2 = tf.reduce_sum(\n            expectation(\n                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)\n            ),\n            axis=0,\n        )\n        jitter = default_jitter()\n        Kus = covariances.Kuf(self.inducing_variable, self.kernel, Xnew)\n        sigma2 = self.likelihood.variance\n        sigma = tf.sqrt(sigma2)\n        L = tf.linalg.cholesky(covariances.Kuu(self.inducing_variable, self.kernel, jitter=jitter))\n\n        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma\n        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)\n        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2\n        B = AAT + tf.eye(num_inducing, dtype=default_float())\n        LB = tf.linalg.cholesky(B)\n        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma\n        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)\n        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)\n        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)\n        if full_cov:\n            var = (\n                self.kernel(Xnew)\n                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)\n                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)\n            )\n            shape = tf.stack([1, 1, tf.shape(Y_data)[1]])\n            var = tf.tile(tf.expand_dims(var, 2), shape)\n        else:\n            var = (\n                self.kernel(Xnew, full_cov=False)\n                + tf.reduce_sum(tf.square(tmp2), axis=0)\n                - tf.reduce_sum(tf.square(tmp1), axis=0)\n            )\n            shape = tf.stack([1, tf.shape(Y_data)[1]])\n            var = tf.tile(tf.expand_dims(var, 1), shape)\n        return mean + self.mean_function(Xnew), var\n\n    def predict_log_density(self, data: OutputData) -> tf.Tensor:\n        raise NotImplementedError\n'"
gpflow/models/gpmc.py,8,"b'# Copyright 2016 James Hensman, alexggmatthews\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Optional\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom ..base import Parameter\nfrom ..conditionals import conditional\nfrom ..config import default_float, default_jitter\nfrom ..kernels import Kernel\nfrom ..likelihoods import Likelihood\nfrom ..mean_functions import MeanFunction\nfrom ..utilities import to_default_float\nfrom .model import InputData, RegressionData, MeanAndVariance, GPModel\nfrom .training_mixins import InternalDataTrainingLossMixin\nfrom .util import data_input_to_tensor\n\n\nclass GPMC(GPModel, InternalDataTrainingLossMixin):\n    def __init__(\n        self,\n        data: RegressionData,\n        kernel: Kernel,\n        likelihood: Likelihood,\n        mean_function: Optional[MeanFunction] = None,\n        num_latent_gps: Optional[int] = None,\n    ):\n        """"""\n        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]\n        kernel, likelihood, mean_function are appropriate GPflow objects\n\n        This is a vanilla implementation of a GP with a non-Gaussian\n        likelihood. The latent function values are represented by centered\n        (whitened) variables, so\n\n            v ~ N(0, I)\n            f = Lv + m(x)\n\n        with\n\n            L L^T = K\n\n        """"""\n        if num_latent_gps is None:\n            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)\n        super().__init__(kernel, likelihood, mean_function, num_latent_gps)\n        self.data = data_input_to_tensor(data)\n        self.num_data = self.data[0].shape[0]\n        self.V = Parameter(np.zeros((self.num_data, self.num_latent_gps)))\n        self.V.prior = tfp.distributions.Normal(\n            loc=to_default_float(0.0), scale=to_default_float(1.0)\n        )\n\n    def log_posterior_density(self) -> tf.Tensor:\n        return self.log_likelihood() + self.log_prior_density()\n\n    def _training_loss(self) -> tf.Tensor:\n        return -self.log_posterior_density()\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.log_likelihood()\n\n    def log_likelihood(self) -> tf.Tensor:\n        r""""""\n        Construct a tf function to compute the likelihood of a general GP\n        model.\n\n            \\log p(Y | V, theta).\n\n        """"""\n        X_data, Y_data = self.data\n        K = self.kernel(X_data)\n        L = tf.linalg.cholesky(\n            K + tf.eye(tf.shape(X_data)[0], dtype=default_float()) * default_jitter()\n        )\n        F = tf.linalg.matmul(L, self.V) + self.mean_function(X_data)\n\n        return tf.reduce_sum(self.likelihood.log_prob(F, Y_data))\n\n    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:\n        """"""\n        Xnew is a data matrix, point at which we want to predict\n\n        This method computes\n\n            p(F* | (F=LV) )\n\n        where F* are points on the GP at Xnew, F=LV are points on the GP at X.\n\n        """"""\n        X_data, Y_data = self.data\n        mu, var = conditional(\n            Xnew, X_data, self.kernel, self.V, full_cov=full_cov, q_sqrt=None, white=True\n        )\n        return mu + self.mean_function(Xnew), var\n'"
gpflow/models/gpr.py,9,"b'# Copyright 2016 James Hensman, Valentine Svensson, alexggmatthews, fujiisoup\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Tuple\n\nimport tensorflow as tf\n\nimport gpflow\n\nfrom ..kernels import Kernel\nfrom ..logdensities import multivariate_normal\nfrom ..mean_functions import MeanFunction\nfrom .model import GPModel, InputData, MeanAndVariance, RegressionData\nfrom .training_mixins import InternalDataTrainingLossMixin\nfrom .util import data_input_to_tensor\n\n\nclass GPR(GPModel, InternalDataTrainingLossMixin):\n    r""""""\n    Gaussian Process Regression.\n\n    This is a vanilla implementation of GP regression with a Gaussian\n    likelihood.  Multiple columns of Y are treated independently.\n\n    The log likelihood of this model is sometimes referred to as the \'log\n    marginal likelihood\', and is given by\n\n    .. math::\n       \\log p(\\mathbf y \\,|\\, \\mathbf f) =\n            \\mathcal N(\\mathbf{y} \\,|\\, 0, \\mathbf{K} + \\sigma_n \\mathbf{I})\n    """"""\n\n    def __init__(\n        self,\n        data: RegressionData,\n        kernel: Kernel,\n        mean_function: Optional[MeanFunction] = None,\n        noise_variance: float = 1.0,\n    ):\n        likelihood = gpflow.likelihoods.Gaussian(noise_variance)\n        _, Y_data = data\n        super().__init__(kernel, likelihood, mean_function, num_latent_gps=Y_data.shape[-1])\n        self.data = data_input_to_tensor(data)\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.log_marginal_likelihood()\n\n    def log_marginal_likelihood(self) -> tf.Tensor:\n        r""""""\n        Computes the log marginal likelihood.\n\n        .. math::\n            \\log p(Y | \\theta).\n\n        """"""\n        X, Y = self.data\n        K = self.kernel(X)\n        num_data = tf.shape(X)[0]\n        k_diag = tf.linalg.diag_part(K)\n        s_diag = tf.fill([num_data], self.likelihood.variance)\n        ks = tf.linalg.set_diag(K, k_diag + s_diag)\n        L = tf.linalg.cholesky(ks)\n        m = self.mean_function(X)\n\n        # [R,] log-likelihoods for each independent dimension of Y\n        log_prob = multivariate_normal(Y, m, L)\n        return tf.reduce_sum(log_prob)\n\n    def predict_f(\n        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False\n    ) -> MeanAndVariance:\n        r""""""\n        This method computes predictions at X \\in R^{N \\x D} input points\n\n        .. math::\n            p(F* | Y)\n\n        where F* are points on the GP at new data points, Y are noisy observations at training data points.\n        """"""\n        X_data, Y_data = self.data\n        err = Y_data - self.mean_function(X_data)\n\n        kmm = self.kernel(X_data)\n        knn = self.kernel(Xnew, full_cov=full_cov)\n        kmn = self.kernel(X_data, Xnew)\n\n        num_data = X_data.shape[0]\n        s = tf.linalg.diag(tf.fill([num_data], self.likelihood.variance))\n\n        conditional = gpflow.conditionals.base_conditional\n        f_mean_zero, f_var = conditional(\n            kmn, kmm + s, knn, err, full_cov=full_cov, white=False\n        )  # [N, P], [N, P] or [P, N, N]\n        f_mean = f_mean_zero + self.mean_function(Xnew)\n        return f_mean, f_var\n'"
gpflow/models/model.py,10,"b'# Copyright 2016 James Hensman, Mark van der Wilk, Valentine Svensson, alexggmatthews, fujiisoup\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nfrom typing import Optional, Tuple\n\nimport tensorflow as tf\n\nfrom .training_mixins import InputData, RegressionData\nfrom ..base import Module\nfrom ..conditionals.util import sample_mvn\nfrom ..kernels import Kernel, MultioutputKernel\nfrom ..likelihoods import Likelihood, SwitchedLikelihood\nfrom ..mean_functions import MeanFunction, Zero\nfrom ..utilities import to_default_float\n\nMeanAndVariance = Tuple[tf.Tensor, tf.Tensor]\n\n\nclass BayesianModel(Module, metaclass=abc.ABCMeta):\n    """""" Bayesian model. """"""\n\n    def log_prior_density(self) -> tf.Tensor:\n        """"""\n        Sum of the log prior probability densities of all (constrained) variables in this model.\n        """"""\n        if self.trainable_parameters:\n            return tf.add_n([p.log_prior_density() for p in self.trainable_parameters])\n        else:\n            return to_default_float(0.0)\n\n    def log_posterior_density(self, *args, **kwargs) -> tf.Tensor:\n        """"""\n        This may be the posterior with respect to the hyperparameters (e.g. for\n        GPR) or the posterior with respect to the function (e.g. for GPMC and\n        SGPMC). It assumes that maximum_log_likelihood_objective() is defined\n        sensibly.\n        """"""\n        return self.maximum_log_likelihood_objective(*args, **kwargs) + self.log_prior_density()\n\n    def _training_loss(self, *args, **kwargs) -> tf.Tensor:\n        """"""\n        Training loss definition. To allow MAP (maximum a-posteriori) estimation,\n        adds the log density of all priors to maximum_log_likelihood_objective().\n        """"""\n        return -(self.maximum_log_likelihood_objective(*args, **kwargs) + self.log_prior_density())\n\n    @abc.abstractmethod\n    def maximum_log_likelihood_objective(self, *args, **kwargs) -> tf.Tensor:\n        """"""\n        Objective for maximum likelihood estimation. Should be maximized. E.g.\n        log-marginal likelihood (hyperparameter likelihood) for GPR, or lower\n        bound to the log-marginal likelihood (ELBO) for sparse and variational\n        GPs.\n        """"""\n        raise NotImplementedError\n\n\nclass GPModel(BayesianModel):\n    r""""""\n    A stateless base class for Gaussian process models, that is, those of the\n    form\n\n    .. math::\n       :nowrap:\n\n       \\begin{align}\n           \\theta        & \\sim p(\\theta) \\\\\n           f             & \\sim \\mathcal{GP}(m(x), k(x, x\'; \\theta)) \\\\\n           f_i           & = f(x_i) \\\\\n           y_i \\,|\\, f_i & \\sim p(y_i|f_i)\n       \\end{align}\n\n    This class mostly adds functionality for predictions. To use it, inheriting\n    classes must define a predict_f function, which computes the means and\n    variances of the latent function.\n\n    These predictions are then pushed through the likelihood to obtain means\n    and variances of held out data, self.predict_y.\n\n    The predictions can also be used to compute the (log) density of held-out\n    data via self.predict_log_density.\n\n    It is also possible to draw samples from the latent GPs using\n    self.predict_f_samples.\n    """"""\n\n    def __init__(\n        self,\n        kernel: Kernel,\n        likelihood: Likelihood,\n        mean_function: Optional[MeanFunction] = None,\n        num_latent_gps: int = None,\n    ):\n        super().__init__()\n        assert num_latent_gps is not None, ""GPModel requires specification of num_latent_gps""\n        self.num_latent_gps = num_latent_gps\n        if mean_function is None:\n            mean_function = Zero()\n        self.mean_function = mean_function\n        self.kernel = kernel\n        self.likelihood = likelihood\n\n    @staticmethod\n    def calc_num_latent_gps_from_data(data, kernel: Kernel, likelihood: Likelihood) -> int:\n        """"""\n        Calculates the number of latent GPs required based on the data as well\n        as the type of kernel and likelihood.\n        """"""\n        _, Y = data\n        output_dim = Y.shape[-1]\n        return GPModel.calc_num_latent_gps(kernel, likelihood, output_dim)\n\n    @staticmethod\n    def calc_num_latent_gps(kernel: Kernel, likelihood: Likelihood, output_dim: int) -> int:\n        """"""\n        Calculates the number of latent GPs required given the number of\n        outputs `output_dim` and the type of likelihood and kernel.\n\n        Note: It\'s not nice for `GPModel` to need to be aware of specific\n        likelihoods as here. However, `num_latent_gps` is a bit more broken in\n        general, we should fix this in the future. There are also some slightly\n        problematic assumptions re the output dimensions of mean_function.\n        See https://github.com/GPflow/GPflow/issues/1343\n        """"""\n        if isinstance(kernel, MultioutputKernel):\n            # MultioutputKernels already have num_latent_gps attributes\n            num_latent_gps = kernel.num_latent_gps\n        elif isinstance(likelihood, SwitchedLikelihood):\n            # the SwitchedLikelihood partitions/stitches based on the last\n            # column in Y, but we should not add a separate latent GP for this!\n            # hence decrement by 1\n            num_latent_gps = output_dim - 1\n            assert num_latent_gps > 0\n        else:\n            num_latent_gps = output_dim\n\n        return num_latent_gps\n\n    @abc.abstractmethod\n    def predict_f(\n        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False\n    ) -> MeanAndVariance:\n        raise NotImplementedError\n\n    def predict_f_samples(\n        self,\n        Xnew: InputData,\n        num_samples: Optional[int] = None,\n        full_cov: bool = True,\n        full_output_cov: bool = False,\n    ) -> tf.Tensor:\n        """"""\n        Produce samples from the posterior latent function(s) at the input points.\n\n        :param Xnew: InputData\n            Input locations at which to draw samples, shape [..., N, D]\n            where N is the number of rows and D is the input dimension of each point.\n        :param num_samples:\n            Number of samples to draw.\n            If `None`, a single sample is drawn and the return shape is [..., N, P],\n            for any positive integer the return shape contains an extra batch\n            dimension, [..., S, N, P], with S = num_samples and P is the number of outputs.\n        :param full_cov:\n            If True, draw correlated samples over the inputs. Computes the Cholesky over the\n            dense covariance matrix of size [num_data, num_data].\n            If False, draw samples that are uncorrelated over the inputs.\n        :param full_output_cov:\n            If True, draw correlated samples over the outputs.\n            If False, draw samples that are uncorrelated over the outputs.\n\n        Currently, the method does not support `full_output_cov=True` and `full_cov=True`.\n        """"""\n        if full_cov and full_output_cov:\n            raise NotImplementedError(\n                ""The combination of both `full_cov` and `full_output_cov` is not supported.""\n            )\n\n        # check below for shape info\n        mean, cov = self.predict_f(Xnew, full_cov=full_cov, full_output_cov=full_output_cov)\n        if full_cov:\n            # mean: [..., N, P]\n            # cov: [..., P, N, N]\n            mean_for_sample = tf.linalg.adjoint(mean)  # [..., P, N]\n            samples = sample_mvn(\n                mean_for_sample, cov, full_cov, num_samples=num_samples\n            )  # [..., (S), P, N]\n            samples = tf.linalg.adjoint(samples)  # [..., (S), N, P]\n        else:\n            # mean: [..., N, P]\n            # cov: [..., N, P] or [..., N, P, P]\n            samples = sample_mvn(\n                mean, cov, full_output_cov, num_samples=num_samples\n            )  # [..., (S), N, P]\n        return samples  # [..., (S), N, P]\n\n    def predict_y(\n        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False\n    ) -> MeanAndVariance:\n        """"""\n        Compute the mean and variance of the held-out data at the input points.\n        """"""\n        f_mean, f_var = self.predict_f(Xnew, full_cov=full_cov, full_output_cov=full_output_cov)\n        return self.likelihood.predict_mean_and_var(f_mean, f_var)\n\n    def predict_log_density(\n        self, data: RegressionData, full_cov: bool = False, full_output_cov: bool = False\n    ) -> tf.Tensor:\n        """"""\n        Compute the log density of the data at the new data points.\n        """"""\n        X, Y = data\n        f_mean, f_var = self.predict_f(X, full_cov=full_cov, full_output_cov=full_output_cov)\n        return self.likelihood.predict_log_density(f_mean, f_var, Y)\n'"
gpflow/models/sgpmc.py,5,"b'# Copyright 2016 James Hensman, alexggmatthews\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Optional\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom ..base import Parameter\nfrom ..conditionals import conditional\nfrom ..inducing_variables import InducingPoints\nfrom ..kernels import Kernel\nfrom ..likelihoods import Likelihood\nfrom ..mean_functions import MeanFunction\nfrom ..utilities import to_default_float\nfrom .model import GPModel, InputData, MeanAndVariance, RegressionData\nfrom .training_mixins import InternalDataTrainingLossMixin\nfrom .util import data_input_to_tensor, inducingpoint_wrapper\n\n\nclass SGPMC(GPModel, InternalDataTrainingLossMixin):\n    r""""""\n    This is the Sparse Variational GP using MCMC (SGPMC). The key reference is\n\n    ::\n\n      @inproceedings{hensman2015mcmc,\n        title={MCMC for Variatinoally Sparse Gaussian Processes},\n        author={Hensman, James and Matthews, Alexander G. de G.\n                and Filippone, Maurizio and Ghahramani, Zoubin},\n        booktitle={Proceedings of NIPS},\n        year={2015}\n      }\n\n    The latent function values are represented by centered\n    (whitened) variables, so\n\n    .. math::\n       :nowrap:\n\n       \\begin{align}\n       \\mathbf v & \\sim N(0, \\mathbf I) \\\\\n       \\mathbf u &= \\mathbf L\\mathbf v\n       \\end{align}\n\n    with\n\n    .. math::\n        \\mathbf L \\mathbf L^\\top = \\mathbf K\n\n\n    """"""\n\n    def __init__(\n        self,\n        data: RegressionData,\n        kernel: Kernel,\n        likelihood: Likelihood,\n        mean_function: Optional[MeanFunction] = None,\n        num_latent_gps: Optional[int] = None,\n        inducing_variable: Optional[InducingPoints] = None,\n    ):\n        """"""\n        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]\n        Z is a data matrix, of inducing inputs, size [M, D]\n        kernel, likelihood, mean_function are appropriate GPflow objects\n        """"""\n        if num_latent_gps is None:\n            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)\n        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)\n        self.data = data_input_to_tensor(data)\n        self.num_data = data[0].shape[0]\n        self.inducing_variable = inducingpoint_wrapper(inducing_variable)\n        self.V = Parameter(np.zeros((len(self.inducing_variable), self.num_latent_gps)))\n        self.V.prior = tfp.distributions.Normal(\n            loc=to_default_float(0.0), scale=to_default_float(1.0)\n        )\n\n    def log_posterior_density(self) -> tf.Tensor:\n        return self.log_likelihood_lower_bound() + self.log_prior_density()\n\n    def _training_loss(self) -> tf.Tensor:\n        return -self.log_posterior_density()\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.log_likelihood_lower_bound()\n\n    def log_likelihood_lower_bound(self) -> tf.Tensor:\n        """"""\n        This function computes the optimal density for v, q*(v), up to a constant\n        """"""\n        # get the (marginals of) q(f): exactly predicting!\n        X_data, Y_data = self.data\n        fmean, fvar = self.predict_f(X_data, full_cov=False)\n        return tf.reduce_sum(self.likelihood.variational_expectations(fmean, fvar, Y_data))\n\n    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:\n        """"""\n        Xnew is a data matrix of the points at which we want to predict\n\n        This method computes\n\n            p(F* | (U=LV) )\n\n        where F* are points on the GP at Xnew, F=LV are points on the GP at Z,\n\n        """"""\n        mu, var = conditional(\n            Xnew,\n            self.inducing_variable,\n            self.kernel,\n            self.V,\n            full_cov=full_cov,\n            q_sqrt=None,\n            white=True,\n            full_output_cov=full_output_cov,\n        )\n        return mu + self.mean_function(Xnew), var\n'"
gpflow/models/sgpr.py,82,"b'# Copyright 2016 James Hensman, alexggmatthews, Mark van der Wilk\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom gpflow.kernels import Kernel\n\nfrom .. import likelihoods\nfrom ..config import default_float, default_jitter\nfrom ..covariances.dispatch import Kuf, Kuu\nfrom ..inducing_variables import InducingPoints\nfrom ..mean_functions import MeanFunction\nfrom ..utilities import to_default_float\nfrom .model import GPModel, MeanAndVariance\nfrom .training_mixins import InputData, InternalDataTrainingLossMixin, RegressionData\nfrom .util import data_input_to_tensor, inducingpoint_wrapper\n\n\nclass SGPRBase(GPModel, InternalDataTrainingLossMixin):\n    """"""\n    Common base class for SGPR and GPRFITC that provides the common __init__\n    and upper_bound() methods.\n    """"""\n\n    def __init__(\n        self,\n        data: RegressionData,\n        kernel: Kernel,\n        inducing_variable: InducingPoints,\n        *,\n        mean_function: Optional[MeanFunction] = None,\n        num_latent_gps: Optional[int] = None,\n        noise_variance: float = 1.0,\n    ):\n        """"""\n        `data`:  a tuple of (X, Y), where the inputs X has shape [N, D]\n            and the outputs Y has shape [N, R].\n        `inducing_variable`:  an InducingPoints instance or a matrix of\n            the pseudo inputs Z, of shape [M, D].\n        `kernel`, `mean_function` are appropriate GPflow objects\n\n        This method only works with a Gaussian likelihood, its variance is\n        initialized to `noise_variance`.\n        """"""\n        likelihood = likelihoods.Gaussian(noise_variance)\n        X_data, Y_data = data_input_to_tensor(data)\n        num_latent_gps = Y_data.shape[-1] if num_latent_gps is None else num_latent_gps\n        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)\n\n        self.data = X_data, Y_data\n        self.num_data = X_data.shape[0]\n\n        self.inducing_variable = inducingpoint_wrapper(inducing_variable)\n\n    def upper_bound(self) -> tf.Tensor:\n        """"""\n        Upper bound for the sparse GP regression marginal likelihood.  Note that\n        the same inducing points are used for calculating the upper bound, as are\n        used for computing the likelihood approximation. This may not lead to the\n        best upper bound. The upper bound can be tightened by optimising Z, just\n        like the lower bound. This is especially important in FITC, as FITC is\n        known to produce poor inducing point locations. An optimisable upper bound\n        can be found in https://github.com/markvdw/gp_upper.\n\n        The key reference is\n\n        ::\n\n          @misc{titsias_2014,\n            title={Variational Inference for Gaussian and Determinantal Point Processes},\n            url={http://www2.aueb.gr/users/mtitsias/papers/titsiasNipsVar14.pdf},\n            publisher={Workshop on Advances in Variational Inference (NIPS 2014)},\n            author={Titsias, Michalis K.},\n            year={2014},\n            month={Dec}\n          }\n\n        The key quantity, the trace term, can be computed via\n\n        >>> _, v = conditionals.conditional(X, model.inducing_variable.Z, model.kernel,\n        ...                                 np.zeros((len(model.inducing_variable), 1)))\n\n        which computes each individual element of the trace term.\n        """"""\n        X_data, Y_data = self.data\n        num_data = to_default_float(tf.shape(Y_data)[0])\n\n        Kdiag = self.kernel(X_data, full_cov=False)\n        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n        kuf = Kuf(self.inducing_variable, self.kernel, X_data)\n\n        I = tf.eye(tf.shape(kuu)[0], dtype=default_float())\n\n        L = tf.linalg.cholesky(kuu)\n        A = tf.linalg.triangular_solve(L, kuf, lower=True)\n        AAT = tf.linalg.matmul(A, A, transpose_b=True)\n        B = I + AAT / self.likelihood.variance\n        LB = tf.linalg.cholesky(B)\n\n        # Using the Trace bound, from Titsias\' presentation\n        c = tf.reduce_sum(Kdiag) - tf.reduce_sum(tf.square(A))\n\n        # Alternative bound on max eigenval:\n        corrected_noise = self.likelihood.variance + c\n\n        const = -0.5 * num_data * tf.math.log(2 * np.pi * self.likelihood.variance)\n        logdet = -tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))\n\n        LC = tf.linalg.cholesky(I + AAT / corrected_noise)\n        v = tf.linalg.triangular_solve(\n            LC, tf.linalg.matmul(A, Y_data) / corrected_noise, lower=True\n        )\n        quad = -0.5 * tf.reduce_sum(tf.square(Y_data)) / corrected_noise + 0.5 * tf.reduce_sum(\n            tf.square(v)\n        )\n\n        return const + logdet + quad\n\n\nclass SGPR(SGPRBase):\n    """"""\n    Sparse Variational GP regression. The key reference is\n\n    ::\n\n      @inproceedings{titsias2009variational,\n        title={Variational learning of inducing variables in\n               sparse Gaussian processes},\n        author={Titsias, Michalis K},\n        booktitle={International Conference on\n                   Artificial Intelligence and Statistics},\n        pages={567--574},\n        year={2009}\n      }\n\n\n\n    """"""\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.elbo()\n\n    def elbo(self) -> tf.Tensor:\n        """"""\n        Construct a tensorflow function to compute the bound on the marginal\n        likelihood. For a derivation of the terms in here, see the associated\n        SGPR notebook.\n        """"""\n        X_data, Y_data = self.data\n\n        num_inducing = len(self.inducing_variable)\n        num_data = to_default_float(tf.shape(Y_data)[0])\n        output_dim = to_default_float(tf.shape(Y_data)[1])\n\n        err = Y_data - self.mean_function(X_data)\n        Kdiag = self.kernel(X_data, full_cov=False)\n        kuf = Kuf(self.inducing_variable, self.kernel, X_data)\n        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n        L = tf.linalg.cholesky(kuu)\n        sigma = tf.sqrt(self.likelihood.variance)\n\n        # Compute intermediate matrices\n        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma\n        AAT = tf.linalg.matmul(A, A, transpose_b=True)\n        B = AAT + tf.eye(num_inducing, dtype=default_float())\n        LB = tf.linalg.cholesky(B)\n        Aerr = tf.linalg.matmul(A, err)\n        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma\n\n        # compute log marginal bound\n        bound = -0.5 * num_data * output_dim * np.log(2 * np.pi)\n        bound += tf.negative(output_dim) * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))\n        bound -= 0.5 * num_data * output_dim * tf.math.log(self.likelihood.variance)\n        bound += -0.5 * tf.reduce_sum(tf.square(err)) / self.likelihood.variance\n        bound += 0.5 * tf.reduce_sum(tf.square(c))\n        bound += -0.5 * output_dim * tf.reduce_sum(Kdiag) / self.likelihood.variance\n        bound += 0.5 * output_dim * tf.reduce_sum(tf.linalg.diag_part(AAT))\n\n        return bound\n\n    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:\n        """"""\n        Compute the mean and variance of the latent function at some new points\n        Xnew. For a derivation of the terms in here, see the associated SGPR\n        notebook.\n        """"""\n        X_data, Y_data = self.data\n        num_inducing = len(self.inducing_variable)\n        err = Y_data - self.mean_function(X_data)\n        kuf = Kuf(self.inducing_variable, self.kernel, X_data)\n        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n        Kus = Kuf(self.inducing_variable, self.kernel, Xnew)\n        sigma = tf.sqrt(self.likelihood.variance)\n        L = tf.linalg.cholesky(kuu)\n        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma\n        B = tf.linalg.matmul(A, A, transpose_b=True) + tf.eye(num_inducing, dtype=default_float())\n        LB = tf.linalg.cholesky(B)\n        Aerr = tf.linalg.matmul(A, err)\n        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma\n        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)\n        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)\n        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)\n        if full_cov:\n            var = (\n                self.kernel(Xnew)\n                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)\n                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)\n            )\n            var = tf.tile(var[None, ...], [self.num_latent_gps, 1, 1])  # [P, N, N]\n        else:\n            var = (\n                self.kernel(Xnew, full_cov=False)\n                + tf.reduce_sum(tf.square(tmp2), 0)\n                - tf.reduce_sum(tf.square(tmp1), 0)\n            )\n            var = tf.tile(var[:, None], [1, self.num_latent_gps])\n        return mean + self.mean_function(Xnew), var\n\n    def compute_qu(self) -> Tuple[tf.Tensor, tf.Tensor]:\n        """"""\n        Computes the mean and variance of q(u) = N(mu, cov), the variational distribution on\n        inducing outputs. SVGP with this q(u) should predict identically to\n        SGPR.\n        :return: mu, cov\n        """"""\n        X_data, Y_data = self.data\n\n        kuf = Kuf(self.inducing_variable, self.kernel, X_data)\n        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n\n        sig = kuu + (self.likelihood.variance ** -1) * tf.matmul(kuf, kuf, transpose_b=True)\n        sig_sqrt = tf.linalg.cholesky(sig)\n\n        sig_sqrt_kuu = tf.linalg.triangular_solve(sig_sqrt, kuu)\n\n        cov = tf.linalg.matmul(sig_sqrt_kuu, sig_sqrt_kuu, transpose_a=True)\n        err = Y_data - self.mean_function(X_data)\n        mu = (\n            tf.linalg.matmul(\n                sig_sqrt_kuu,\n                tf.linalg.triangular_solve(sig_sqrt, tf.linalg.matmul(kuf, err)),\n                transpose_a=True,\n            )\n            / self.likelihood.variance\n        )\n\n        return mu, cov\n\n\nclass GPRFITC(SGPRBase):\n    """"""\n    This implements GP regression with the FITC approximation.\n    The key reference is\n\n    ::\n\n      @inproceedings{Snelson06sparsegaussian,\n        author = {Edward Snelson and Zoubin Ghahramani},\n        title = {Sparse Gaussian Processes using Pseudo-inputs},\n        booktitle = {Advances In Neural Information Processing Systems},\n        year = {2006},\n        pages = {1257--1264},\n        publisher = {MIT press}\n      }\n\n    Implementation loosely based on code from GPML matlab library although\n    obviously gradients are automatic in GPflow.\n    """"""\n\n    def common_terms(self):\n        X_data, Y_data = self.data\n        num_inducing = len(self.inducing_variable)\n        err = Y_data - self.mean_function(X_data)  # size [N, R]\n        Kdiag = self.kernel(X_data, full_cov=False)\n        kuf = Kuf(self.inducing_variable, self.kernel, X_data)\n        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n\n        Luu = tf.linalg.cholesky(kuu)  # => Luu Luu^T = kuu\n        V = tf.linalg.triangular_solve(Luu, kuf)  # => V^T V = Qff = kuf^T kuu^-1 kuf\n\n        diagQff = tf.reduce_sum(tf.square(V), 0)\n        nu = Kdiag - diagQff + self.likelihood.variance\n\n        B = tf.eye(num_inducing, dtype=default_float()) + tf.linalg.matmul(\n            V / nu, V, transpose_b=True\n        )\n        L = tf.linalg.cholesky(B)\n        beta = err / tf.expand_dims(nu, 1)  # size [N, R]\n        alpha = tf.linalg.matmul(V, beta)  # size [N, R]\n\n        gamma = tf.linalg.triangular_solve(L, alpha, lower=True)  # size [N, R]\n\n        return err, nu, Luu, L, alpha, beta, gamma\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.fitc_log_marginal_likelihood()\n\n    def fitc_log_marginal_likelihood(self) -> tf.Tensor:\n        """"""\n        Construct a tensorflow function to compute the bound on the marginal\n        likelihood.\n        """"""\n\n        # FITC approximation to the log marginal likelihood is\n        # log ( normal( y | mean, K_fitc ) )\n        # where K_fitc = Qff + diag( \\nu )\n        # where Qff = Kfu kuu^{-1} kuf\n        # with \\nu_i = Kff_{i,i} - Qff_{i,i} + \\sigma^2\n\n        # We need to compute the Mahalanobis term -0.5* err^T K_fitc^{-1} err\n        # (summed over functions).\n\n        # We need to deal with the matrix inverse term.\n        # K_fitc^{-1} = ( Qff + \\diag( \\nu ) )^{-1}\n        #            = ( V^T V + \\diag( \\nu ) )^{-1}\n        # Applying the Woodbury identity we obtain\n        #            = \\diag( \\nu^{-1} ) - \\diag( \\nu^{-1} ) V^T ( I + V \\diag( \\nu^{-1} ) V^T )^{-1) V \\diag(\\nu^{-1} )\n        # Let \\beta =  \\diag( \\nu^{-1} ) err\n        # and let \\alpha = V \\beta\n        # then Mahalanobis term = -0.5* ( \\beta^T err - \\alpha^T Solve( I + V \\diag( \\nu^{-1} ) V^T, alpha ) )\n\n        err, nu, Luu, L, alpha, beta, gamma = self.common_terms()\n\n        mahalanobisTerm = -0.5 * tf.reduce_sum(\n            tf.square(err) / tf.expand_dims(nu, 1)\n        ) + 0.5 * tf.reduce_sum(tf.square(gamma))\n\n        # We need to compute the log normalizing term -N/2 \\log 2 pi - 0.5 \\log \\det( K_fitc )\n\n        # We need to deal with the log determinant term.\n        # \\log \\det( K_fitc ) = \\log \\det( Qff + \\diag( \\nu ) )\n        #                    = \\log \\det( V^T V + \\diag( \\nu ) )\n        # Applying the determinant lemma we obtain\n        #                    = \\log [ \\det \\diag( \\nu ) \\det( I + V \\diag( \\nu^{-1} ) V^T ) ]\n        #                    = \\log [ \\det \\diag( \\nu ) ] + \\log [ \\det( I + V \\diag( \\nu^{-1} ) V^T ) ]\n\n        constantTerm = -0.5 * self.num_data * tf.math.log(tf.constant(2.0 * np.pi, default_float()))\n        logDeterminantTerm = -0.5 * tf.reduce_sum(tf.math.log(nu)) - tf.reduce_sum(\n            tf.math.log(tf.linalg.diag_part(L))\n        )\n        logNormalizingTerm = constantTerm + logDeterminantTerm\n\n        return mahalanobisTerm + logNormalizingTerm * self.num_latent_gps\n\n    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:\n        """"""\n        Compute the mean and variance of the latent function at some new points\n        Xnew.\n        """"""\n        _, _, Luu, L, _, _, gamma = self.common_terms()\n        Kus = Kuf(self.inducing_variable, self.kernel, Xnew)  # [M, N]\n\n        w = tf.linalg.triangular_solve(Luu, Kus, lower=True)  # [M, N]\n\n        tmp = tf.linalg.triangular_solve(tf.transpose(L), gamma, lower=False)\n        mean = tf.linalg.matmul(w, tmp, transpose_a=True) + self.mean_function(Xnew)\n        intermediateA = tf.linalg.triangular_solve(L, w, lower=True)\n\n        if full_cov:\n            var = (\n                self.kernel(Xnew)\n                - tf.linalg.matmul(w, w, transpose_a=True)\n                + tf.linalg.matmul(intermediateA, intermediateA, transpose_a=True)\n            )\n            var = tf.tile(var[None, ...], [self.num_latent_gps, 1, 1])  # [P, N, N]\n        else:\n            var = (\n                self.kernel(Xnew, full_cov=False)\n                - tf.reduce_sum(tf.square(w), 0)\n                + tf.reduce_sum(tf.square(intermediateA), 0)\n            )  # [N, P]\n            var = tf.tile(var[:, None], [1, self.num_latent_gps])\n\n        return mean, var\n'"
gpflow/models/svgp.py,8,"b'# Copyright 2016 James Hensman, Valentine Svensson, alexggmatthews, Mark van der Wilk\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .. import kullback_leiblers\nfrom ..base import Parameter\nfrom ..conditionals import conditional\nfrom ..config import default_float\nfrom ..utilities import positive, triangular\nfrom .model import GPModel, InputData, RegressionData, MeanAndVariance\nfrom .training_mixins import ExternalDataTrainingLossMixin\nfrom .util import inducingpoint_wrapper\n\n\nclass SVGP(GPModel, ExternalDataTrainingLossMixin):\n    """"""\n    This is the Sparse Variational GP (SVGP). The key reference is\n\n    ::\n\n      @inproceedings{hensman2014scalable,\n        title={Scalable Variational Gaussian Process Classification},\n        author={Hensman, James and Matthews, Alexander G. de G. and Ghahramani, Zoubin},\n        booktitle={Proceedings of AISTATS},\n        year={2015}\n      }\n\n    """"""\n\n    def __init__(\n        self,\n        kernel,\n        likelihood,\n        inducing_variable,\n        *,\n        mean_function=None,\n        num_latent_gps: int = 1,\n        q_diag: bool = False,\n        q_mu=None,\n        q_sqrt=None,\n        whiten: bool = True,\n        num_data=None,\n    ):\n        """"""\n        - kernel, likelihood, inducing_variables, mean_function are appropriate\n          GPflow objects\n        - num_latent_gps is the number of latent processes to use, defaults to 1\n        - q_diag is a boolean. If True, the covariance is approximated by a\n          diagonal matrix.\n        - whiten is a boolean. If True, we use the whitened representation of\n          the inducing points.\n        - num_data is the total number of observations, defaults to X.shape[0]\n          (relevant when feeding in external minibatches)\n        """"""\n        # init the super class, accept args\n        super().__init__(kernel, likelihood, mean_function, num_latent_gps)\n        self.num_data = num_data\n        self.q_diag = q_diag\n        self.whiten = whiten\n        self.inducing_variable = inducingpoint_wrapper(inducing_variable)\n\n        # init variational parameters\n        num_inducing = len(self.inducing_variable)\n        self._init_variational_parameters(num_inducing, q_mu, q_sqrt, q_diag)\n\n    def _init_variational_parameters(self, num_inducing, q_mu, q_sqrt, q_diag):\n        """"""\n        Constructs the mean and cholesky of the covariance of the variational Gaussian posterior.\n        If a user passes values for `q_mu` and `q_sqrt` the routine checks if they have consistent\n        and correct shapes. If a user does not specify any values for `q_mu` and `q_sqrt`, the routine\n        initializes them, their shape depends on `num_inducing` and `q_diag`.\n\n        Note: most often the comments refer to the number of observations (=output dimensions) with P,\n        number of latent GPs with L, and number of inducing points M. Typically P equals L,\n        but when certain multioutput kernels are used, this can change.\n\n        Parameters\n        ----------\n        :param num_inducing: int\n            Number of inducing variables, typically refered to as M.\n        :param q_mu: np.array or None\n            Mean of the variational Gaussian posterior. If None the function will initialise\n            the mean with zeros. If not None, the shape of `q_mu` is checked.\n        :param q_sqrt: np.array or None\n            Cholesky of the covariance of the variational Gaussian posterior.\n            If None the function will initialise `q_sqrt` with identity matrix.\n            If not None, the shape of `q_sqrt` is checked, depending on `q_diag`.\n        :param q_diag: bool\n            Used to check if `q_mu` and `q_sqrt` have the correct shape or to\n            construct them with the correct shape. If `q_diag` is true,\n            `q_sqrt` is two dimensional and only holds the square root of the\n            covariance diagonal elements. If False, `q_sqrt` is three dimensional.\n        """"""\n        q_mu = np.zeros((num_inducing, self.num_latent_gps)) if q_mu is None else q_mu\n        self.q_mu = Parameter(q_mu, dtype=default_float())  # [M, P]\n\n        if q_sqrt is None:\n            if self.q_diag:\n                ones = np.ones((num_inducing, self.num_latent_gps), dtype=default_float())\n                self.q_sqrt = Parameter(ones, transform=positive())  # [M, P]\n            else:\n                q_sqrt = [\n                    np.eye(num_inducing, dtype=default_float()) for _ in range(self.num_latent_gps)\n                ]\n                q_sqrt = np.array(q_sqrt)\n                self.q_sqrt = Parameter(q_sqrt, transform=triangular())  # [P, M, M]\n        else:\n            if q_diag:\n                assert q_sqrt.ndim == 2\n                self.num_latent_gps = q_sqrt.shape[1]\n                self.q_sqrt = Parameter(q_sqrt, transform=positive())  # [M, L|P]\n            else:\n                assert q_sqrt.ndim == 3\n                self.num_latent_gps = q_sqrt.shape[0]\n                num_inducing = q_sqrt.shape[1]\n                self.q_sqrt = Parameter(q_sqrt, transform=triangular())  # [L|P, M, M]\n\n    def prior_kl(self) -> tf.Tensor:\n        return kullback_leiblers.prior_kl(\n            self.inducing_variable, self.kernel, self.q_mu, self.q_sqrt, whiten=self.whiten\n        )\n\n    def maximum_log_likelihood_objective(self, data: RegressionData) -> tf.Tensor:\n        return self.elbo(data)\n\n    def elbo(self, data: RegressionData) -> tf.Tensor:\n        """"""\n        This gives a variational bound (the evidence lower bound or ELBO) on\n        the log marginal likelihood of the model.\n        """"""\n        X, Y = data\n        kl = self.prior_kl()\n        f_mean, f_var = self.predict_f(X, full_cov=False, full_output_cov=False)\n        var_exp = self.likelihood.variational_expectations(f_mean, f_var, Y)\n        if self.num_data is not None:\n            num_data = tf.cast(self.num_data, kl.dtype)\n            minibatch_size = tf.cast(tf.shape(X)[0], kl.dtype)\n            scale = num_data / minibatch_size\n        else:\n            scale = tf.cast(1.0, kl.dtype)\n        return tf.reduce_sum(var_exp) * scale - kl\n\n    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:\n        q_mu = self.q_mu\n        q_sqrt = self.q_sqrt\n        mu, var = conditional(\n            Xnew,\n            self.inducing_variable,\n            self.kernel,\n            q_mu,\n            q_sqrt=q_sqrt,\n            full_cov=full_cov,\n            white=self.whiten,\n            full_output_cov=full_output_cov,\n        )\n        # tf.debugging.assert_positive(var)  # We really should make the tests pass with this here\n        return mu + self.mean_function(Xnew), var\n'"
gpflow/models/training_mixins.py,16,"b'""""""\nThis module provides mixin classes to be used in conjunction with inheriting\nfrom gpflow.models.BayesianModel (or its subclass gpflow.models.GPModel).\n\nThey provide a unified interface to obtain closures that return the training\nloss, to be passed as the first argument to the minimize() method of the\noptimizers defined in TensorFlow and GPflow.\n\nAll TrainingLossMixin classes assume that self._training_loss()\n(which is provided by the BayesianModel base class), will be available. Note\nthat new models only need to implement the maximum_log_likelihood_objective\nmethod that is defined as abstract in BayesianModel.\n\nThere are different mixins depending on whether the model already contains the\ntraining data (InternalDataTrainingLossMixin), or requires it to be passed in\nto the objective function (ExternalDataTrainingLossMixin).\n""""""\n\nimport abc\nfrom typing import Callable, Iterator, Optional, Tuple, TypeVar, Union\n\nimport tensorflow as tf\nfrom tensorflow.python.data.ops.iterator_ops import OwnedIterator as DatasetOwnedIterator\nimport numpy as np\n\n\nInputData = Union[tf.Tensor]\nOutputData = Union[tf.Tensor]\nRegressionData = Tuple[InputData, OutputData]\nData = TypeVar(""Data"", RegressionData, InputData, OutputData)\n\n\nclass InternalDataTrainingLossMixin:\n    """"""\n    Mixin utility for training loss methods for models that own their own data. It provides\n\n      - a uniform API for the training loss :meth:`training_loss`\n      - a convenience method :meth:`training_loss_closure` for constructing the closure expected by\n        various optimizers, namely :class:`gpflow.optimizers.Scipy` and subclasses of\n        `tf.optimizers.Optimizer`.\n\n    See :class:`ExternalDataTrainingLossMixin` for an equivalent mixin for models that do **not**\n    own their own data.\n    """"""\n\n    def training_loss(self) -> tf.Tensor:\n        """"""\n        Returns the training loss for this model.\n        """"""\n        return self._training_loss()\n\n    def training_loss_closure(self, *, compile=True) -> Callable[[], tf.Tensor]:\n        """"""\n        Convenience method. Returns a closure which itself returns the training loss. This closure\n        can be passed to the minimize methods on :class:`gpflow.optimizers.Scipy` and subclasses of\n        `tf.optimizers.Optimizer`.\n\n        :param compile: If `True` (default), compile the training loss function in a TensorFlow graph\n            by wrapping it in tf.function()\n        """"""\n        if compile:\n            return tf.function(self.training_loss)\n        return self.training_loss\n\n\nclass ExternalDataTrainingLossMixin:\n    """"""\n    Mixin utility for training loss methods for models that do **not** own their own data.\n    It provides\n\n      - a uniform API for the training loss :meth:`training_loss`\n      - a convenience method :meth:`training_loss_closure` for constructing the closure expected by\n        various optimizers, namely :class:`gpflow.optimizers.Scipy` and subclasses of\n        `tf.optimizers.Optimizer`.\n\n    See :class:`InternalDataTrainingLossMixin` for an equivalent mixin for models that **do** own\n    their own data.\n    """"""\n\n    def training_loss(self, data: Data) -> tf.Tensor:\n        """"""\n        Returns the training loss for this model.\n        \n        :param data: the data to be used for computing the model objective.\n        """"""\n        return self._training_loss(data)\n\n    def training_loss_closure(\n        self, data: Union[Data, DatasetOwnedIterator], *, compile=True,\n    ) -> Callable[[], tf.Tensor]:\n        """"""\n        Returns a closure that computes the training loss, which by default is\n        wrapped in tf.function(). This can be disabled by passing `compile=False`.\n        \n        :param data: the data to be used by the closure for computing the model\n            objective. Can be the full dataset or an iterator, e.g.\n            `iter(dataset.batch(batch_size))`, where dataset is an instance of\n            tf.data.Dataset.\n        :param compile: if True, wrap training loss in tf.function()\n        """"""\n        training_loss = self.training_loss\n\n        if isinstance(data, DatasetOwnedIterator):\n            if compile:\n                input_signature = [data.element_spec]\n                training_loss = tf.function(training_loss, input_signature=input_signature)\n\n            def closure():\n                batch = next(data)\n                return training_loss(batch)\n\n        else:\n\n            def closure():\n                return training_loss(data)\n\n            if compile:\n                closure = tf.function(closure)\n\n        return closure\n'"
gpflow/models/util.py,11,"b'from typing import Callable, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..config import default_float\nfrom ..inducing_variables import InducingPoints, InducingVariables\nfrom .model import BayesianModel\nfrom .training_mixins import Data, ExternalDataTrainingLossMixin\n\n\ndef inducingpoint_wrapper(\n    inducing_variable: Union[InducingVariables, tf.Tensor, np.ndarray]\n) -> InducingVariables:\n    """"""\n    This wrapper allows transparently passing either an InducingVariables\n    object or an array specifying InducingPoints positions.\n    """"""\n    if not isinstance(inducing_variable, InducingVariables):\n        inducing_variable = InducingPoints(inducing_variable)\n    return inducing_variable\n\n\ndef _assert_equal_data(data1, data2):\n    if isinstance(data1, tf.Tensor) and isinstance(data2, tf.Tensor):\n        tf.debugging.assert_equal(data1, data2)\n    else:\n        for v1, v2 in zip(data1, data2):\n            tf.debugging.assert_equal(v1, v2)\n\n\ndef training_loss_closure(\n    model: BayesianModel, data: Data, **closure_kwargs\n) -> Callable[[], tf.Tensor]:\n    if isinstance(model, ExternalDataTrainingLossMixin):\n        return model.training_loss_closure(data, **closure_kwargs)\n    else:\n        _assert_equal_data(model.data, data)\n        return model.training_loss_closure(**closure_kwargs)\n\n\ndef training_loss(model: BayesianModel, data: Data) -> tf.Tensor:\n    if isinstance(model, ExternalDataTrainingLossMixin):\n        return model.training_loss(data)\n    else:\n        _assert_equal_data(model.data, data)\n        return model.training_loss()\n\n\ndef maximum_log_likelihood_objective(model: BayesianModel, data: Data) -> tf.Tensor:\n    if isinstance(model, ExternalDataTrainingLossMixin):\n        return model.maximum_log_likelihood_objective(data)\n    else:\n        _assert_equal_data(model.data, data)\n        return model.maximum_log_likelihood_objective()\n\n\ndef data_input_to_tensor(structure):\n    """"""\n    Converts non-tensor elements of a structure to TensorFlow tensors retaining the structure itself.\n    The function doesn\'t keep original element\'s dtype and forcefully converts\n    them to GPflow\'s default float type.\n    """"""\n\n    def convert_to_tensor(elem):\n        if tf.is_tensor(elem):\n            return elem\n        elif isinstance(elem, np.ndarray):\n            return tf.convert_to_tensor(elem)\n        return tf.convert_to_tensor(elem, dtype=default_float())\n\n    return tf.nest.map_structure(convert_to_tensor, structure)\n'"
gpflow/models/vgp.py,34,"b'# Copyright 2016 James Hensman, Valentine Svensson, alexggmatthews, fujiisoup\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport numpy as np\nimport tensorflow as tf\n\nimport gpflow\n\nfrom ..base import Parameter\nfrom ..conditionals import conditional\nfrom ..config import default_float, default_jitter\nfrom ..kernels import Kernel\nfrom ..kullback_leiblers import gauss_kl\nfrom ..likelihoods import Likelihood\nfrom ..mean_functions import MeanFunction, Zero\nfrom ..utilities import triangular\nfrom .model import GPModel, InputData, MeanAndVariance, RegressionData\nfrom .training_mixins import InternalDataTrainingLossMixin\nfrom .util import data_input_to_tensor, inducingpoint_wrapper\n\n\nclass VGP(GPModel, InternalDataTrainingLossMixin):\n    r""""""\n    This method approximates the Gaussian process posterior using a multivariate Gaussian.\n\n    The idea is that the posterior over the function-value vector F is\n    approximated by a Gaussian, and the KL divergence is minimised between\n    the approximation and the posterior.\n\n    This implementation is equivalent to SVGP with X=Z, but is more efficient.\n    The whitened representation is used to aid optimization.\n\n    The posterior approximation is\n\n    .. math::\n\n       q(\\mathbf f) = N(\\mathbf f \\,|\\, \\boldsymbol \\mu, \\boldsymbol \\Sigma)\n\n    """"""\n\n    def __init__(\n        self,\n        data: RegressionData,\n        kernel: Kernel,\n        likelihood: Likelihood,\n        mean_function: Optional[MeanFunction] = None,\n        num_latent_gps: Optional[int] = None,\n    ):\n        """"""\n        data = (X, Y) contains the input points [N, D] and the observations [N, P]\n        kernel, likelihood, mean_function are appropriate GPflow objects\n        """"""\n        if num_latent_gps is None:\n            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)\n        super().__init__(kernel, likelihood, mean_function, num_latent_gps)\n\n        self.data = data_input_to_tensor(data)\n        X_data, Y_data = self.data\n        num_data = X_data.shape[0]\n        self.num_data = num_data\n\n        self.q_mu = Parameter(np.zeros((num_data, self.num_latent_gps)))\n        q_sqrt = np.array([np.eye(num_data) for _ in range(self.num_latent_gps)])\n        self.q_sqrt = Parameter(q_sqrt, transform=triangular())\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.elbo()\n\n    def elbo(self) -> tf.Tensor:\n        r""""""\n        This method computes the variational lower bound on the likelihood,\n        which is:\n\n            E_{q(F)} [ \\log p(Y|F) ] - KL[ q(F) || p(F)]\n\n        with\n\n            q(\\mathbf f) = N(\\mathbf f \\,|\\, \\boldsymbol \\mu, \\boldsymbol \\Sigma)\n\n        """"""\n        X_data, Y_data = self.data\n        # Get prior KL.\n        KL = gauss_kl(self.q_mu, self.q_sqrt)\n\n        # Get conditionals\n        K = self.kernel(X_data) + tf.eye(self.num_data, dtype=default_float()) * default_jitter()\n        L = tf.linalg.cholesky(K)\n        fmean = tf.linalg.matmul(L, self.q_mu) + self.mean_function(X_data)  # [NN, ND] -> ND\n        q_sqrt_dnn = tf.linalg.band_part(self.q_sqrt, -1, 0)  # [D, N, N]\n        L_tiled = tf.tile(tf.expand_dims(L, 0), tf.stack([self.num_latent_gps, 1, 1]))\n        LTA = tf.linalg.matmul(L_tiled, q_sqrt_dnn)  # [D, N, N]\n        fvar = tf.reduce_sum(tf.square(LTA), 2)\n\n        fvar = tf.transpose(fvar)\n\n        # Get variational expectations.\n        var_exp = self.likelihood.variational_expectations(fmean, fvar, Y_data)\n\n        return tf.reduce_sum(var_exp) - KL\n\n    def predict_f(\n        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False\n    ) -> MeanAndVariance:\n        X_data, _ = self.data\n        mu, var = conditional(\n            Xnew, X_data, self.kernel, self.q_mu, q_sqrt=self.q_sqrt, full_cov=full_cov, white=True,\n        )\n        return mu + self.mean_function(Xnew), var\n\n\nclass VGPOpperArchambeau(GPModel, InternalDataTrainingLossMixin):\n    r""""""\n    This method approximates the Gaussian process posterior using a multivariate Gaussian.\n    The key reference is:\n    ::\n      @article{Opper:2009,\n          title = {The Variational Gaussian Approximation Revisited},\n          author = {Opper, Manfred and Archambeau, Cedric},\n          journal = {Neural Comput.},\n          year = {2009},\n          pages = {786--792},\n      }\n    The idea is that the posterior over the function-value vector F is\n    approximated by a Gaussian, and the KL divergence is minimised between\n    the approximation and the posterior. It turns out that the optimal\n    posterior precision shares off-diagonal elements with the prior, so\n    only the diagonal elements of the precision need be adjusted.\n    The posterior approximation is\n    .. math::\n       q(\\mathbf f) = N(\\mathbf f \\,|\\, \\mathbf K \\boldsymbol \\alpha,\n                         [\\mathbf K^{-1} + \\textrm{diag}(\\boldsymbol \\lambda))^2]^{-1})\n\n    This approach has only 2ND parameters, rather than the N + N^2 of vgp,\n    but the optimization is non-convex and in practice may cause difficulty.\n\n    """"""\n\n    def __init__(\n        self,\n        data: RegressionData,\n        kernel: Kernel,\n        likelihood: Likelihood,\n        mean_function: Optional[MeanFunction] = None,\n        num_latent_gps: Optional[int] = None,\n    ):\n        """"""\n        data = (X, Y) contains the input points [N, D] and the observations [N, P]\n        kernel, likelihood, mean_function are appropriate GPflow objects\n        """"""\n        if num_latent_gps is None:\n            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)\n        super().__init__(kernel, likelihood, mean_function, num_latent_gps)\n\n        self.data = data_input_to_tensor(data)\n        X_data, Y_data = self.data\n        self.num_data = X_data.shape[0]\n        self.q_alpha = Parameter(np.zeros((self.num_data, self.num_latent_gps)))\n        self.q_lambda = Parameter(\n            np.ones((self.num_data, self.num_latent_gps)), transform=gpflow.utilities.positive()\n        )\n\n    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n        return self.elbo()\n\n    def elbo(self) -> tf.Tensor:\n        r""""""\n        q_alpha, q_lambda are variational parameters, size [N, R]\n        This method computes the variational lower bound on the likelihood,\n        which is:\n            E_{q(F)} [ \\log p(Y|F) ] - KL[ q(F) || p(F)]\n        with\n            q(f) = N(f | K alpha + mean, [K^-1 + diag(square(lambda))]^-1) .\n        """"""\n        X_data, Y_data = self.data\n\n        K = self.kernel(X_data)\n        K_alpha = tf.linalg.matmul(K, self.q_alpha)\n        f_mean = K_alpha + self.mean_function(X_data)\n\n        # compute the variance for each of the outputs\n        I = tf.tile(\n            tf.eye(self.num_data, dtype=default_float())[None, ...], [self.num_latent_gps, 1, 1]\n        )\n        A = (\n            I\n            + tf.transpose(self.q_lambda)[:, None, ...]\n            * tf.transpose(self.q_lambda)[:, :, None, ...]\n            * K\n        )\n        L = tf.linalg.cholesky(A)\n        Li = tf.linalg.triangular_solve(L, I)\n        tmp = Li / tf.transpose(self.q_lambda)[:, None, ...]\n        f_var = 1.0 / tf.square(self.q_lambda) - tf.transpose(tf.reduce_sum(tf.square(tmp), 1))\n\n        # some statistics about A are used in the KL\n        A_logdet = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)))\n        trAi = tf.reduce_sum(tf.square(Li))\n\n        KL = 0.5 * (\n            A_logdet\n            + trAi\n            - self.num_data * self.num_latent_gps\n            + tf.reduce_sum(K_alpha * self.q_alpha)\n        )\n\n        v_exp = self.likelihood.variational_expectations(f_mean, f_var, Y_data)\n        return tf.reduce_sum(v_exp) - KL\n\n    def predict_f(\n        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False\n    ) -> MeanAndVariance:\n        r""""""\n        The posterior variance of F is given by\n            q(f) = N(f | K alpha + mean, [K^-1 + diag(lambda**2)]^-1)\n        Here we project this to F*, the values of the GP at Xnew which is given\n        by\n           q(F*) = N ( F* | K_{*F} alpha + mean, K_{**} - K_{*f}[K_{ff} +\n                                           diag(lambda**-2)]^-1 K_{f*} )\n\n        Note: This model currently does not allow full output covariances\n        """"""\n        if full_output_cov:\n            raise NotImplementedError\n\n        X_data, _ = self.data\n        # compute kernel things\n        Kx = self.kernel(X_data, Xnew)\n        K = self.kernel(X_data)\n\n        # predictive mean\n        f_mean = tf.linalg.matmul(Kx, self.q_alpha, transpose_a=True) + self.mean_function(Xnew)\n\n        # predictive var\n        A = K + tf.linalg.diag(tf.transpose(1.0 / tf.square(self.q_lambda)))\n        L = tf.linalg.cholesky(A)\n        Kx_tiled = tf.tile(Kx[None, ...], [self.num_latent_gps, 1, 1])\n        LiKx = tf.linalg.triangular_solve(L, Kx_tiled)\n        if full_cov:\n            f_var = self.kernel(Xnew) - tf.linalg.matmul(LiKx, LiKx, transpose_a=True)\n        else:\n            f_var = self.kernel(Xnew, full_cov=False) - tf.reduce_sum(tf.square(LiKx), axis=1)\n        return f_mean, tf.transpose(f_var)\n'"
gpflow/monitor/__init__.py,0,"b'# Copyright 2020 GPflow authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Provides basic functionality to monitor optimisation runs """"""\n\nfrom .base import *\nfrom .tensorboard import *\n'"
gpflow/monitor/base.py,1,"b'# Copyright 2020 GPflow authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" MonitorTask base classes """"""\n\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, List, Union\n\nimport tensorflow as tf\n\n\n__all__ = [""MonitorTask"", ""ExecuteCallback"", ""MonitorTaskGroup"", ""Monitor""]\n\n\nclass MonitorTask(ABC):\n    """"""\n    A base class for a monitoring task.\n\n    All monitoring tasks are callable objects.\n    A descendant class must implement the `run` method, which is the body of the monitoring task.\n    """"""\n\n    def __call__(self, step: int, **kwargs):\n        """"""\n        It calls the \'run\' function and sets the current step.\n\n        :param step: current step in the optimisation.\n        :param kwargs: additional keyword arguments that can be passed\n            to the `run` method of the task. This is in particular handy for\n            passing keyword argument to the callback of `ScalarToTensorBoard`.\n        """"""\n        self.current_step = tf.cast(step, tf.int64)\n        self.run(**kwargs)\n\n    @abstractmethod\n    def run(self, **kwargs):\n        """"""\n        Implements the task to be executed on __call__.\n        The current step is available through `self.current_step`.\n\n        :param kwargs: keyword arguments available to the run method.\n        """"""\n        raise NotImplementedError\n\n\nclass ExecuteCallback(MonitorTask):\n    """""" Executes a callback as task """"""\n\n    def __init__(self, callback: Callable[..., None]):\n        """"""\n        :param callback: callable to be executed during the task.\n            Arguments can be passed using keyword arguments.\n        """"""\n        super().__init__()\n        self.callback = callback\n\n    def run(self, **kwargs):\n        self.callback(**kwargs)\n\n\nclass MonitorTaskGroup:\n    """"""\n    Class for grouping `MonitorTask` instances. A group defines\n    all the tasks that are run at the same frequency, given by `period`.\n\n    A `MonitorTaskGroup` can exist of a single instance or a list of\n    `MonitorTask` instances.\n    """"""\n\n    def __init__(self, task_or_tasks: Union[List[MonitorTask], MonitorTask], period: int = 1):\n        """"""\n        :param task_or_tasks: a single instance or a list of `MonitorTask` instances.\n            Each `MonitorTask` in the list will be run with the given `period`.\n        :param period: defines how often to run the tasks; they will execute every `period`th step.\n            For large values of `period` the tasks will be less frequently run. Defaults to\n            running at every step (`period = 1`).\n        """"""\n        self.tasks = task_or_tasks\n        self._period = period\n\n    @property\n    def tasks(self) -> List[MonitorTask]:\n        return self._tasks\n\n    @tasks.setter\n    def tasks(self, task_or_tasks: Union[List[MonitorTask], MonitorTask]) -> None:\n        """"""Ensures the tasks are stored as a list. Even if there is only a single task.""""""\n        if not isinstance(task_or_tasks, List):\n            self._tasks = [task_or_tasks]\n        else:\n            self._tasks = task_or_tasks\n\n    def __call__(self, step, **kwargs):\n        """"""Call each task in the group.""""""\n        if step % self._period == 0:\n            for task in self.tasks:\n                task(step, **kwargs)\n\n\nclass Monitor:\n    r""""""\n    Accepts any number of of `MonitorTaskGroup` instances, and runs them\n    according to their specified periodicity.\n\n    Example use-case:\n        ```\n        # Create some monitor tasks\n        log_dir = ""logs""\n        model_task = ModelToTensorBoard(log_dir, model)\n        image_task = ImageToTensorBoard(log_dir, plot_prediction, ""image_samples"")\n        lml_task = ScalarToTensorBoard(log_dir, lambda: model.log_marginal_likelihood(), ""lml"")\n\n        # Plotting tasks can be quite slow, so we want to run them less frequently.\n        # We group them in a `MonitorTaskGroup` and set the period to 5.\n        slow_tasks = MonitorTaskGroup(image_task, period=5)\n\n        # The other tasks are fast. We run them at each iteration of the optimisation.\n        fast_tasks = MonitorTaskGroup([model_task, lml_task], period=1)\n\n        # We pass both groups to the `Monitor`\n        monitor = Monitor(fast_tasks, slow_tasks)\n        ```\n    """"""\n\n    def __init__(self, *task_groups: MonitorTaskGroup):\n        """"""\n        :param task_groups: a list of `MonitorTaskGroup`s to be executed.\n        """"""\n        self.task_groups = task_groups\n\n    def __call__(self, step, **kwargs):\n        for group in self.task_groups:\n            group(step, **kwargs)\n'"
gpflow/monitor/tensorboard.py,9,"b'# Copyright 2020 GPflow authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Tasks that write to TensorBoard """"""\n\nfrom io import BytesIO\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..base import Parameter\nfrom ..models import BayesianModel\nfrom ..utilities import parameter_dict\nfrom .base import MonitorTask\n\n\n__all__ = [""ToTensorBoard"", ""ModelToTensorBoard"", ""ScalarToTensorBoard"", ""ImageToTensorBoard""]\n\n\nclass ToTensorBoard(MonitorTask):\n    writers = {}\n\n    def __init__(self, log_dir: str):\n        """"""\n        :param log_dir: directory in which to store the tensorboard files.\n            Can be nested, e.g. ./logs/my_run/\n        """"""\n        super().__init__()\n        if log_dir not in self.writers:\n            self.writers[log_dir] = tf.summary.create_file_writer(log_dir)\n        self.file_writer = self.writers[log_dir]\n\n    def __call__(self, step, **kwargs):\n        with self.file_writer.as_default():\n            super().__call__(step, **kwargs)\n        self.file_writer.flush()\n\n\nclass ModelToTensorBoard(ToTensorBoard):\n    """"""\n    Monitoring task that creates a sensible TensorBoard for a model.\n\n    Monitors all the model\'s parameters for which their name matches with `keywords_to_monitor`.\n    By default, ""kernel"" and ""likelihood"" are elements of `keywords_to_monitor`.\n    Example:\n        keyword = ""kernel"", parameter = ""kernel.lengthscale"" => match\n        keyword = ""variational"", parameter = ""kernel.lengthscale"" => no match\n    """"""\n\n    def __init__(\n        self,\n        log_dir: str,\n        model: BayesianModel,\n        *,\n        max_size: int = 3,\n        keywords_to_monitor: List[str] = [""kernel"", ""likelihood""],\n        left_strip_character: str = ""."",\n    ):\n        """"""\n        :param log_dir: directory in which to store the tensorboard files.\n            Can be a nested: for example, \'./logs/my_run/\'.\n        :param model: model to be monitord.\n        :param max_size: maximum size of arrays (incl.) to store each\n            element of the array independently as a scalar in the TensorBoard.\n            Setting max_size to -1 will write all values. Use with care.\n        :param keywords_to_monitor: specifies keywords to be monitored.\n            If the parameter\'s name includes any of the keywords specified it\n            will be monitored. By default, parameters that match the `kernel` or\n            `likelihood` keyword are monitored.\n            Adding a ""*"" to the list will match with all parameters,\n            i.e. no parameters or variables will be filtered out.\n        :param left_strip_character: certain frameworks prepend their variables with\n            a character. GPflow adds a \'.\' and Keras add a \'_\', for example.\n            When a `left_strip_character` is specified it will be stripped from the\n            parameter\'s name. By default the \'.\' is left stripped, for example:\n            "".likelihood.variance"" becomes ""likelihood.variance"".\n        """"""\n        super().__init__(log_dir)\n        self.model = model\n        self.max_size = max_size\n        self.keywords_to_monitor = keywords_to_monitor\n        self.summarize_all = ""*"" in self.keywords_to_monitor\n        self.left_strip_character = left_strip_character\n\n    def run(self, **unused_kwargs):\n        for name, parameter in parameter_dict(self.model).items():\n            # check if the parameter name matches any of the specified keywords\n            if self.summarize_all or any(keyword in name for keyword in self.keywords_to_monitor):\n                # keys are sometimes prepended with a character, which we strip\n                name = name.lstrip(self.left_strip_character)\n                self._summarize_parameter(name, parameter)\n\n    def _summarize_parameter(self, name: str, param: Union[Parameter, tf.Variable]):\n        """"""\n        :param name: identifier used in tensorboard\n        :param param: parameter to be stored in tensorboard\n        """"""\n        param = tf.reshape(param, (-1,))\n        size = param.shape[0]\n\n        if not isinstance(size, int):\n            raise ValueError(\n                f""The monitoring can not be autographed as the size of a parameter {param} ""\n                ""is unknown at compile time. If compiling the monitor task is important, ""\n                ""make sure the shape of all parameters is known beforehand. Otherwise, ""\n                ""run the monitor outside the `tf.function`.""\n            )\n\n        if size == 1:\n            tf.summary.scalar(name, param[0], step=self.current_step)\n        else:\n            for i in range(min(size, self.max_size)):\n                tf.summary.scalar(f""{name}[{i}]"", param[i], step=self.current_step)\n\n\nclass ScalarToTensorBoard(ToTensorBoard):\n    """"""Stores the return value of a callback in a TensorBoard.""""""\n\n    def __init__(self, log_dir: str, callback: Callable[[], float], name: str):\n        """"""\n        :param log_dir: directory in which to store the tensorboard files.\n            For example, \'./logs/my_run/\'.\n        :param callback: callback to be executed and result written to TensorBoard.\n            A callback can have arguments (e.g. data) passed to the function using\n            keyword arguments.\n            For example:\n            ```\n            lambda cb(x=None): 2 * x\n            task = ScalarToTensorBoard(logdir, cb, ""callback"")\n\n            # specify the argument of the function using kwargs, the names need to match.\n            task(step, x=1)\n            ```\n        :param name: name used in TensorBoard.\n        """"""\n        super().__init__(log_dir)\n        self.name = name\n        self.callback = callback\n\n    def run(self, **kwargs):\n        tf.summary.scalar(self.name, self.callback(**kwargs), step=self.current_step)\n\n\nclass ImageToTensorBoard(ToTensorBoard):\n    def __init__(\n        self,\n        log_dir: str,\n        plotting_function: Callable[\n            [""matplotlib.figure.Figure"", ""matplotlib.figure.Axes""], ""matplotlib.figure.Figure""\n        ],\n        name: Optional[str] = None,\n        *,\n        fig_kw: Optional[Dict[str, Any]] = None,\n        subplots_kw: Optional[Dict[str, Any]] = None,\n    ):\n        """"""\n        :param log_dir: directory in which to store the tensorboard files.\n            Can be nested: for example, \'./logs/my_run/\'.\n        :param plotting_function: function performing the plotting.\n        :param name: name used in TensorBoard.\n        :params fig_kw: keyword arguments to be passed to Figure constructor, e.g. `figsize`.\n        :params subplots_kw: keyword arguments to be passed to figure.subplots constructor, e.g.\n            `nrows`, `ncols`, `sharex`, `sharey`. By default the default values\n            from matplotlib.pyplot are used.\n        """"""\n        super().__init__(log_dir)\n        self.plotting_function = plotting_function\n        self.name = name\n        self.fig_kw = fig_kw or {}\n        self.subplots_kw = subplots_kw or {}\n\n        try:\n            from matplotlib.figure import Figure\n        except ImportError:\n            raise RuntimeError(""ImageToTensorBoard requires the matplotlib package to be installed"")\n\n        self.fig = Figure(**self.fig_kw)\n        if self.subplots_kw != {}:\n            self.axes = self.fig.subplots(**self.subplots_kw)\n        else:\n            self.axes = self.fig.add_subplot(111)\n\n    def _clear_axes(self):\n        if isinstance(self.axes, np.ndarray):\n            for ax in self.axes.flatten():\n                ax.clear()\n        else:\n            self.axes.clear()\n\n    def run(self, **unused_kwargs):\n        from matplotlib.backends.backend_agg import FigureCanvasAgg\n\n        self._clear_axes()\n        self.plotting_function(self.fig, self.axes)\n        canvas = FigureCanvasAgg(self.fig)\n        canvas.draw()\n\n        # get PNG data from the figure\n        png_buffer = BytesIO()\n        canvas.print_png(png_buffer)\n        png_encoded = png_buffer.getvalue()\n        png_buffer.close()\n\n        image_tensor = tf.io.decode_png(png_encoded)[None]\n\n        # Write to TensorBoard\n        tf.summary.image(self.name, image_tensor, step=self.current_step)\n'"
gpflow/optimizers/__init__.py,0,b'# pylint: disable=wildcard-import\n\nfrom . import natgrad\nfrom .natgrad import *\nfrom .scipy import Scipy\nfrom .mcmc import SamplingHelper\n'
gpflow/optimizers/mcmc.py,7,"b'# Copyright 2019 Artem Artemev @awav, Eric Hambro @condnsdmatters\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Callable, Sequence, Optional\n\nimport tensorflow as tf\n\nfrom gpflow.base import Parameter\n\n__all__ = [""SamplingHelper""]\n\n\nclass SamplingHelper:\n    """"""\n    This helper makes it easy to read from variables being set with a prior and\n    writes values back to the same variables.\n\n    Example:\n        model = ...  # Create a GPflow model\n        hmc_helper = SamplingHelper(model.log_posterior_density, model.trainable_parameters)\n\n        target_log_prob_fn = hmc_helper.target_log_prob_fn\n        current_state = hmc_helper.current_state\n\n        hmc = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn=target_log_prob_fn, ...)\n        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(hmc, ...)\n\n        @tf.function\n        def run_chain_fn():\n            return mcmc.sample_chain(\n                num_samples, num_burnin_steps, current_state, kernel=adaptive_hmc)\n\n        hmc_samples = run_chain_fn()\n        parameter_samples = hmc_helper.convert_to_constrained_values(hmc_samples)\n    """"""\n\n    def __init__(\n        self, target_log_prob_fn: Callable[[], tf.Tensor], parameters: Sequence[Parameter]\n    ):\n        """"""\n        :param target_log_prob_fn: a callable which returns the log-density of the model\n            under the target distribution; needs to implicitly depend on the `parameters`.\n            E.g. `model.log_posterior_density`.\n        :param parameters: List of :class:`gpflow.Parameter` used as a state of the Markov chain.\n            E.g. `model.trainable_parameters`\n            Note that each parameter must have been given a prior.\n        """"""\n        if not all(isinstance(p, Parameter) and p.prior is not None for p in parameters):\n            raise ValueError(\n                ""`parameters` should only contain gpflow.Parameter objects with priors""\n            )\n\n        self._parameters = parameters\n        self._target_log_prob_fn = target_log_prob_fn\n        self._variables = [p.unconstrained_variable for p in parameters]\n\n    @property\n    def current_state(self):\n        """"""Return the current state of the unconstrained variables, used in HMC.""""""\n\n        return self._variables\n\n    @property\n    def target_log_prob_fn(self):\n        """"""\n        The target log probability, adjusted to allow for optimisation to occur on the tracked\n        unconstrained underlying variables.\n        """"""\n        variables_list = self.current_state\n\n        @tf.custom_gradient\n        def _target_log_prob_fn_closure(*variables):\n            for v_old, v_new in zip(variables_list, variables):\n                v_old.assign(v_new)\n\n            with tf.GradientTape(watch_accessed_variables=False) as tape:\n                tape.watch(variables_list)\n                log_prob = self._target_log_prob_fn()\n                # Now need to correct for the fact that the prob fn is evaluated on the\n                # constrained space while we wish to evaluate it in the unconstrained space\n                for param in self._parameters:\n                    if param.transform is not None:\n                        x = param.unconstrained_variable\n                        log_det_jacobian = param.transform.forward_log_det_jacobian(\n                            x, x.shape.ndims\n                        )\n                        log_prob += tf.reduce_sum(log_det_jacobian)\n\n            @tf.function\n            def grad_fn(dy, variables: Optional[tf.Tensor] = None):\n                grad = tape.gradient(log_prob, variables_list)\n                return grad, [None] * len(variables)\n\n            return log_prob, grad_fn\n\n        return _target_log_prob_fn_closure\n\n    def convert_to_constrained_values(self, hmc_samples):\n        """"""\n        Converts list of unconstrained values in `hmc_samples` to constrained\n        versions. Each value in the list corresponds to an entry in parameters\n        passed to the constructor; for parameters that have a transform, the\n        constrained representation is returned.\n        """"""\n        values = []\n        for hmc_value, param in zip(hmc_samples, self._parameters):\n            if param.transform is not None:\n                value = param.transform.forward(hmc_value)\n            else:\n                value = hmc_value\n            values.append(value)\n        return values\n'"
gpflow/optimizers/natgrad.py,35,"b'# Copyright 2018-2020 Hugh Salimbeni, Artem Artemev @awav, ST John @st--\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CO[N, D]ITIONS OF ANY KI[N, D], either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nimport functools\nfrom typing import Callable, Optional, Sequence, Tuple, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..base import Parameter, _to_constrained\n\nScalar = Union[float, tf.Tensor, np.ndarray]\nLossClosure = Callable[[], tf.Tensor]\nNatGradParameters = Union[Tuple[Parameter, Parameter], Tuple[Parameter, Parameter, ""XiTransform""]]\n\n__all__ = [\n    ""NaturalGradient"",\n    ""XiTransform"",\n    ""XiNat"",\n    ""XiSqrtMeanVar"",\n]\n\n\n#\n# Xi transformations necessary for natural gradient optimizer.\n# Abstract class and two implementations: XiNat and XiSqrtMeanVar.\n#\n\n\nclass XiTransform(metaclass=abc.ABCMeta):\n    """"""\n    XiTransform is the base class that implements three transformations necessary\n    for the natural gradient calculation wrt any parameterization.\n    This class does not handle any shape information, but it is assumed that\n    the parameters pairs are always of shape (N, D) and (D, N, N).\n    """"""\n\n    @staticmethod\n    @abc.abstractmethod\n    def meanvarsqrt_to_xi(mean, varsqrt):\n        """"""\n        Transforms the parameter `mean` and `varsqrt` to `xi1`, `xi2`\n\n        :param mean: the mean parameter (N, D)\n        :param varsqrt: the varsqrt parameter (D, N, N)\n        :return: tuple (xi1, xi2), the xi parameters (N, D), (D, N, N)\n        """"""\n\n    @staticmethod\n    @abc.abstractmethod\n    def xi_to_meanvarsqrt(xi1, xi2):\n        """"""\n        Transforms the parameter `xi1`, `xi2` to `mean`, `varsqrt`\n\n        :param xi1: the \xce\xbe\xe2\x82\x81 parameter\n        :param xi2: the \xce\xbe\xe2\x82\x82 parameter\n        :return: tuple (mean, varsqrt), the meanvarsqrt parameters\n        """"""\n\n    @staticmethod\n    @abc.abstractmethod\n    def naturals_to_xi(nat1, nat2):\n        """"""\n        Applies the transform so that `nat1`, `nat2` is mapped to `xi1`, `xi2`\n\n        :param nat1: the \xce\xb8\xe2\x82\x81 parameter\n        :param nat2: the \xce\xb8\xe2\x82\x82 parameter\n        :return: tuple `xi1`, `xi2`\n        """"""\n\n\nclass XiNat(XiTransform):\n    """"""\n    This is the default transform. Using the natural directly saves the forward mode\n    gradient, and also gives the analytic optimal solution for gamma=1 in the case\n    of Gaussian likelihood.\n    """"""\n\n    @staticmethod\n    def meanvarsqrt_to_xi(mean, varsqrt):\n        return meanvarsqrt_to_natural(mean, varsqrt)\n\n    @staticmethod\n    def xi_to_meanvarsqrt(xi1, xi2):\n        return natural_to_meanvarsqrt(xi1, xi2)\n\n    @staticmethod\n    def naturals_to_xi(nat1, nat2):\n        return nat1, nat2\n\n\nclass XiSqrtMeanVar(XiTransform):\n    """"""\n    This transformation will perform natural gradient descent on the model parameters,\n    so saves the conversion to and from Xi.\n    """"""\n\n    @staticmethod\n    def meanvarsqrt_to_xi(mean, varsqrt):\n        return mean, varsqrt\n\n    @staticmethod\n    def xi_to_meanvarsqrt(xi1, xi2):\n        return xi1, xi2\n\n    @staticmethod\n    def naturals_to_xi(nat1, nat2):\n        return natural_to_meanvarsqrt(nat1, nat2)\n\n\nclass NaturalGradient(tf.optimizers.Optimizer):\n    """"""\n    Implements a natural gradient descent optimizer for variational models\n    that are based on a distribution q(u) = N(q_mu, q_sqrt q_sqrt\xe1\xb5\x80) that is\n    parameterized by mean q_mu and lower-triangular Cholesky factor q_sqrt\n    of the covariance.\n\n    Note that this optimizer does not implement the standard API of\n    tf.optimizers.Optimizer. Its only public method is minimize(), which has\n    a custom signature (var_list needs to be a list of (q_mu, q_sqrt) tuples,\n    where q_mu and q_sqrt are gpflow.Parameter instances, not tf.Variable).\n\n    Note furthermore that the natural gradients are implemented only for the\n    full covariance case (i.e., q_diag=True is NOT supported).\n\n    When using in your work, please cite\n\n        @inproceedings{salimbeni18,\n            title={Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models},\n            author={Salimbeni, Hugh and Eleftheriadis, Stefanos and Hensman, James},\n            booktitle={AISTATS},\n            year={2018}\n    """"""\n\n    def __init__(self, gamma: Scalar, xi_transform: XiTransform = XiNat(), name=None):\n        """"""\n        :param gamma: natgrad step length\n        :param xi_transform: default \xce\xbe transform (can be overridden in the call to minimize())\n            The XiNat default choice works well in general.\n        """"""\n        name = self.__class__.__name__ if name is None else name\n        super().__init__(name)\n        self.gamma = gamma\n        self.xi_transform = xi_transform\n\n    def minimize(\n        self, loss_fn: LossClosure, var_list: Sequence[NatGradParameters],\n    ):\n        """"""\n        Minimizes objective function of the model.\n        Natural Gradient optimizer works with variational parameters only.\n\n        :param loss_fn: Loss function.\n        :param var_list: List of pair tuples of variational parameters or\n            triplet tuple with variational parameters and \xce\xbe transformation.\n            If \xce\xbe is not specified, will use self.xi_transform.\n            For example, `var_list` could be\n            ```\n            var_list = [\n                (q_mu1, q_sqrt1),\n                (q_mu2, q_sqrt2, XiSqrtMeanVar())\n            ]\n            ```\n\n        GPflow implements the `XiNat` (default) and `XiSqrtMeanVar` transformations\n        for parameters. Custom transformations that implement the `XiTransform`\n        interface are also possible.\n        """"""\n        parameters = [(v[0], v[1], (v[2] if len(v) > 2 else None)) for v in var_list]\n        self._natgrad_steps(loss_fn, parameters)\n\n    def _natgrad_steps(\n        self,\n        loss_fn: LossClosure,\n        parameters: Sequence[Tuple[Parameter, Parameter, Optional[XiTransform]]],\n    ):\n        """"""\n        Computes gradients of loss_fn() w.r.t. q_mu and q_sqrt, and updates\n        these parameters using the natgrad backwards step, for all sets of\n        variational parameters passed in.\n\n        :param loss_fn: Loss function.\n        :param parameters: List of tuples (q_mu, q_sqrt, xi_transform)\n        """"""\n        q_mus, q_sqrts, xis = zip(*parameters)\n        unconstrained_variables = [\n            p.unconstrained_variable for params in (q_mus, q_sqrts) for p in params\n        ]\n\n        with tf.GradientTape(watch_accessed_variables=False) as tape:\n            tape.watch(unconstrained_variables)\n            loss = loss_fn()\n\n        q_mu_grads, q_sqrt_grads = tape.gradient(loss, [q_mus, q_sqrts])\n        # NOTE that these are the gradients in *unconstrained* space\n\n        with tf.name_scope(f""{self._name}/natural_gradient_steps""):\n            for q_mu_grad, q_sqrt_grad, q_mu, q_sqrt, xi_transform in zip(\n                q_mu_grads, q_sqrt_grads, q_mus, q_sqrts, xis\n            ):\n                self._natgrad_apply_gradients(q_mu_grad, q_sqrt_grad, q_mu, q_sqrt, xi_transform)\n\n    def _assert_shapes(self, q_mu, q_sqrt):\n        tf.debugging.assert_shapes(\n            [(q_mu, [""M"", ""L""]), (q_sqrt, [""L"", ""M"", ""M""]),]\n        )\n\n    def _natgrad_apply_gradients(\n        self,\n        q_mu_grad: tf.Tensor,\n        q_sqrt_grad: tf.Tensor,\n        q_mu: Parameter,\n        q_sqrt: Parameter,\n        xi_transform: Optional[XiTransform] = None,\n    ):\n        """"""\n        This function does the backward step on the q_mu and q_sqrt parameters,\n        given the gradients of the loss function with respect to their unconstrained\n        variables. I.e., it expects the arguments to come from\n\n            with tf.GradientTape() as tape:\n                loss = loss_function()\n            q_mu_grad, q_mu_sqrt = tape.gradient(loss, [q_mu, q_sqrt])\n\n        (Note that tape.gradient() returns the gradients in *unconstrained* space!)\n\n        Implements equation [10] from\n\n        @inproceedings{salimbeni18,\n            title={Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models},\n            author={Salimbeni, Hugh and Eleftheriadis, Stefanos and Hensman, James},\n            booktitle={AISTATS},\n            year={2018}\n\n        In addition, for convenience with the rest of GPflow, this code computes \xe2\x88\x82L/\xe2\x88\x82\xce\xb7 using\n        the chain rule (the following assumes a numerator layout where the gradient is a row\n        vector; note that TensorFlow actually returns a column vector), where L is the loss:\n\n        \xe2\x88\x82L/\xe2\x88\x82\xce\xb7 = (\xe2\x88\x82L / \xe2\x88\x82[q_mu, q_sqrt])(\xe2\x88\x82[q_mu, q_sqrt] / \xe2\x88\x82\xce\xb7)\n\n        In total there are three derivative calculations:\n        natgrad of L w.r.t \xce\xbe  = (\xe2\x88\x82\xce\xbe / \xe2\x88\x82\xce\xb8) [(\xe2\x88\x82L / \xe2\x88\x82[q_mu, q_sqrt]) (\xe2\x88\x82[q_mu, q_sqrt] / \xe2\x88\x82\xce\xb7)]\xe1\xb5\x80\n\n        Note that if \xce\xbe = \xce\xb8 (i.e. [q_mu, q_sqrt]) some of these calculations are the identity.\n        In the code \xce\xb7 = eta, \xce\xbe = xi, \xce\xb8 = nat.\n\n        :param q_mu_grad: gradient of loss w.r.t. q_mu (in unconstrained space)\n        :param q_sqrt_grad: gradient of loss w.r.t. q_sqrt (in unconstrained space)\n        :param q_mu: parameter for the mean of q(u) with shape [M, L]\n        :param q_sqrt: parameter for the square root of the covariance of q(u)\n            with shape [L, M, M] (the diagonal parametrization, q_diag=True, is NOT supported)\n        :param xi_transform: the \xce\xbe transform to use (self.xi_transform if not specified)\n        """"""\n        self._assert_shapes(q_mu, q_sqrt)\n\n        if xi_transform is None:\n            xi_transform = self.xi_transform\n\n        # 1) the ordinary gpflow gradient\n        dL_dmean = _to_constrained(q_mu_grad, q_mu.transform)\n        dL_dvarsqrt = _to_constrained(q_sqrt_grad, q_sqrt.transform)\n\n        with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n            tape.watch([q_mu.unconstrained_variable, q_sqrt.unconstrained_variable])\n\n            # the three parameterizations as functions of [q_mu, q_sqrt]\n            eta1, eta2 = meanvarsqrt_to_expectation(q_mu, q_sqrt)\n            # we need these to calculate the relevant gradients\n            meanvarsqrt = expectation_to_meanvarsqrt(eta1, eta2)\n\n            if not isinstance(xi_transform, XiNat):\n                nat1, nat2 = meanvarsqrt_to_natural(q_mu, q_sqrt)\n                xi1_nat, xi2_nat = xi_transform.naturals_to_xi(nat1, nat2)\n                dummy_tensors = tf.ones_like(xi1_nat), tf.ones_like(xi2_nat)\n                with tf.GradientTape(watch_accessed_variables=False) as forward_tape:\n                    forward_tape.watch(dummy_tensors)\n                    dummy_gradients = tape.gradient(\n                        [xi1_nat, xi2_nat], [nat1, nat2], output_gradients=dummy_tensors\n                    )\n\n        # 2) the chain rule to get \xe2\x88\x82L/\xe2\x88\x82\xce\xb7, where \xce\xb7 (eta) are the expectation parameters\n        dL_deta1, dL_deta2 = tape.gradient(\n            meanvarsqrt, [eta1, eta2], output_gradients=[dL_dmean, dL_dvarsqrt]\n        )\n\n        if not isinstance(xi_transform, XiNat):\n            nat_dL_xi1, nat_dL_xi2 = forward_tape.gradient(\n                dummy_gradients, dummy_tensors, output_gradients=[dL_deta1, dL_deta2]\n            )\n        else:\n            nat_dL_xi1, nat_dL_xi2 = dL_deta1, dL_deta2\n\n        del tape  # Remove ""persistent"" tape\n\n        xi1, xi2 = xi_transform.meanvarsqrt_to_xi(q_mu, q_sqrt)\n        xi1_new = xi1 - self.gamma * nat_dL_xi1\n        xi2_new = xi2 - self.gamma * nat_dL_xi2\n\n        # Transform back to the model parameters [q_mu, q_sqrt]\n        mean_new, varsqrt_new = xi_transform.xi_to_meanvarsqrt(xi1_new, xi2_new)\n\n        q_mu.assign(mean_new)\n        q_sqrt.assign(varsqrt_new)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({""gamma"": self._serialize_hyperparameter(""gamma"")})\n        return config\n\n\n#\n# Auxiliary gaussian parameter conversion functions.\n#\n# The following functions expect their first and second inputs to have shape\n# [D, N, 1] and [D, N, N], respectively. Return values are also of shapes [D, N, 1] and [D, N, N].\n\n\ndef swap_dimensions(method):\n    """"""\n    Converts between GPflow indexing and tensorflow indexing\n    `method` is a function that broadcasts over the first dimension (i.e. like all tensorflow matrix ops):\n        `method` inputs [D, N, 1], [D, N, N]\n        `method` outputs [D, N, 1], [D, N, N]\n    :return: Function that broadcasts over the final dimension (i.e. compatible with GPflow):\n        inputs: [N, D], [D, N, N]\n        outputs: [N, D], [D, N, N]\n    """"""\n\n    @functools.wraps(method)\n    def wrapper(a_nd, b_dnn, swap=True):\n        if swap:\n            if a_nd.shape.ndims != 2:  # pragma: no cover\n                raise ValueError(""The mean parametrization must have 2 dimensions."")\n            if b_dnn.shape.ndims != 3:  # pragma: no cover\n                raise ValueError(""The covariance parametrization must have 3 dimensions."")\n            a_dn1 = tf.linalg.adjoint(a_nd)[:, :, None]\n            A_dn1, B_dnn = method(a_dn1, b_dnn)\n            A_nd = tf.linalg.adjoint(A_dn1[:, :, 0])\n            return A_nd, B_dnn\n        else:\n            return method(a_nd, b_dnn)\n\n    return wrapper\n\n\n@swap_dimensions\ndef natural_to_meanvarsqrt(nat1: tf.Tensor, nat2: tf.Tensor):\n    var_sqrt_inv = tf.linalg.cholesky(-2 * nat2)\n    var_sqrt = _inverse_lower_triangular(var_sqrt_inv)\n    S = tf.linalg.matmul(var_sqrt, var_sqrt, transpose_a=True)\n    mu = tf.linalg.matmul(S, nat1)\n    # We need the decomposition of S as L L^T, not as L^T L,\n    # hence we need another cholesky.\n    return mu, tf.linalg.cholesky(S)\n\n\n@swap_dimensions\ndef meanvarsqrt_to_natural(mu: tf.Tensor, s_sqrt: tf.Tensor):\n    s_sqrt_inv = _inverse_lower_triangular(s_sqrt)\n    s_inv = tf.linalg.matmul(s_sqrt_inv, s_sqrt_inv, transpose_a=True)\n    return tf.linalg.matmul(s_inv, mu), -0.5 * s_inv\n\n\n@swap_dimensions\ndef natural_to_expectation(nat1: tf.Tensor, nat2: tf.Tensor):\n    args = natural_to_meanvarsqrt(nat1, nat2, swap=False)\n    return meanvarsqrt_to_expectation(*args, swap=False)\n\n\n@swap_dimensions\ndef expectation_to_natural(eta1: tf.Tensor, eta2: tf.Tensor):\n    args = expectation_to_meanvarsqrt(eta1, eta2, swap=False)\n    return meanvarsqrt_to_natural(*args, swap=False)\n\n\n@swap_dimensions\ndef expectation_to_meanvarsqrt(eta1: tf.Tensor, eta2: tf.Tensor):\n    var = eta2 - tf.linalg.matmul(eta1, eta1, transpose_b=True)\n    return eta1, tf.linalg.cholesky(var)\n\n\n@swap_dimensions\ndef meanvarsqrt_to_expectation(m: tf.Tensor, v_sqrt: tf.Tensor):\n    v = tf.linalg.matmul(v_sqrt, v_sqrt, transpose_b=True)\n    return m, v + tf.linalg.matmul(m, m, transpose_b=True)\n\n\ndef _inverse_lower_triangular(M):\n    """"""\n    Take inverse of lower triangular (e.g. Cholesky) matrix. This function\n    broadcasts over the first index.\n\n    :param M: Tensor with lower triangular structure of shape [D, N, N]\n    :return: The inverse of the Cholesky decomposition. Same shape as input.\n    """"""\n    if M.shape.ndims != 3:  # pragma: no cover\n        raise ValueError(""Number of dimensions for input is required to be 3."")\n    D, N = tf.shape(M)[0], tf.shape(M)[1]\n    I_dnn = tf.eye(N, dtype=M.dtype)[None, :, :] * tf.ones((D, 1, 1), dtype=M.dtype)\n    return tf.linalg.triangular_solve(M, I_dnn)\n'"
gpflow/optimizers/scipy.py,25,"b'from typing import Callable, Iterable, List, Optional, Sequence, Tuple, TypeVar, Union\n\nimport numpy as np\nimport scipy.optimize\nimport tensorflow as tf\nfrom scipy.optimize import OptimizeResult\n\n__all__ = [""Scipy""]\n\nVariables = Iterable[tf.Variable]  # deprecated\nStepCallback = Callable[[int, Sequence[tf.Variable], Sequence[tf.Tensor]], None]\nLossClosure = Callable[[], tf.Tensor]\n\n\nclass Scipy:\n    def minimize(\n        self,\n        closure: LossClosure,\n        variables: Sequence[tf.Variable],\n        method: Optional[str] = ""L-BFGS-B"",\n        step_callback: Optional[StepCallback] = None,\n        compile: bool = True,\n        **scipy_kwargs,\n    ) -> OptimizeResult:\n        """"""\n        Minimize is a wrapper around the `scipy.optimize.minimize` function\n        handling the packing and unpacking of a list of shaped variables on the\n        TensorFlow side vs. the flat numpy array required on the Scipy side.\n\n        Args:\n            closure: A closure that re-evaluates the model, returning the loss\n                to be minimized.\n            variables: The list (tuple) of variables to be optimized\n                (typically `model.trainable_variables`)\n            method: The type of solver to use in SciPy. Defaults to ""L-BFGS-B"".\n            step_callback: If not None, a callable that gets called once after\n                each optimisation step. The callable is passed the arguments\n                `step`, `variables`, and `values`. `step` is the optimisation\n                step counter, `variables` is the list of trainable variables as\n                above, and `values` is the corresponding list of tensors of\n                matching shape that contains their value at this optimisation\n                step.\n            compile: If True, wraps the evaluation function (the passed `closure`\n                as well as its gradient computation) inside a `tf.function()`,\n                which will improve optimization speed in most cases.\n\n            scipy_kwargs: Arguments passed through to `scipy.optimize.minimize`\n                Note that Scipy\'s minimize() takes a `callback` argument, but\n                you probably want to use our wrapper and pass in `step_callback`.\n\n        Returns:\n            The optimization result represented as a Scipy ``OptimizeResult``\n            object. See the Scipy documentation for description of attributes.\n        """"""\n        if not callable(closure):\n            raise TypeError(\n                ""The \'closure\' argument is expected to be a callable object.""\n            )  # pragma: no cover\n        variables = tuple(variables)\n        if not all(isinstance(v, tf.Variable) for v in variables):\n            raise TypeError(\n                ""The \'variables\' argument is expected to only contain tf.Variable instances (use model.trainable_variables, not model.trainable_parameters)""\n            )  # pragma: no cover\n        initial_params = self.initial_parameters(variables)\n\n        func = self.eval_func(closure, variables, compile=compile)\n        if step_callback is not None:\n            if ""callback"" in scipy_kwargs:\n                raise ValueError(""Callback passed both via `step_callback` and `callback`"")\n\n            callback = self.callback_func(variables, step_callback)\n            scipy_kwargs.update(dict(callback=callback))\n\n        return scipy.optimize.minimize(\n            func, initial_params, jac=True, method=method, **scipy_kwargs\n        )\n\n    @classmethod\n    def initial_parameters(cls, variables: Sequence[tf.Variable]) -> tf.Tensor:\n        return cls.pack_tensors(variables)\n\n    @classmethod\n    def eval_func(\n        cls, closure: LossClosure, variables: Sequence[tf.Variable], compile: bool = True\n    ) -> Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n        def _tf_eval(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n            values = cls.unpack_tensors(variables, x)\n            cls.assign_tensors(variables, values)\n\n            loss, grads = _compute_loss_and_gradients(closure, variables)\n            return loss, cls.pack_tensors(grads)\n\n        if compile:\n            _tf_eval = tf.function(_tf_eval)\n\n        def _eval(x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n            loss, grad = _tf_eval(tf.convert_to_tensor(x))\n            return loss.numpy().astype(np.float64), grad.numpy().astype(np.float64)\n\n        return _eval\n\n    @classmethod\n    def callback_func(\n        cls, variables: Sequence[tf.Variable], step_callback: StepCallback\n    ) -> Callable[[np.ndarray], None]:\n        step = 0  # type: int\n\n        def _callback(x: np.ndarray) -> None:\n            nonlocal step\n            values = cls.unpack_tensors(variables, x)\n            step_callback(step, variables, values)\n            step += 1\n\n        return _callback\n\n    @staticmethod\n    def pack_tensors(tensors: Sequence[Union[tf.Tensor, tf.Variable]]) -> tf.Tensor:\n        flats = [tf.reshape(tensor, (-1,)) for tensor in tensors]\n        tensors_vector = tf.concat(flats, axis=0)\n        return tensors_vector\n\n    @staticmethod\n    def unpack_tensors(\n        to_tensors: Sequence[Union[tf.Tensor, tf.Variable]], from_vector: tf.Tensor\n    ) -> List[tf.Tensor]:\n        s = 0\n        values = []\n        for target_tensor in to_tensors:\n            shape = tf.shape(target_tensor)\n            dtype = target_tensor.dtype\n            tensor_size = tf.reduce_prod(shape)\n            tensor_vector = from_vector[s : s + tensor_size]\n            tensor = tf.reshape(tf.cast(tensor_vector, dtype), shape)\n            values.append(tensor)\n            s += tensor_size\n        return values\n\n    @staticmethod\n    def assign_tensors(to_tensors: Sequence[tf.Variable], values: Sequence[tf.Tensor]) -> None:\n        if len(to_tensors) != len(values):\n            raise ValueError(""to_tensors and values should have same length"")\n        for target, value in zip(to_tensors, values):\n            target.assign(value)\n\n\ndef _compute_loss_and_gradients(\n    loss_closure: LossClosure, variables: Sequence[tf.Variable]\n) -> Tuple[tf.Tensor, Sequence[tf.Tensor]]:\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(variables)\n        loss = loss_closure()\n    grads = tape.gradient(loss, variables)\n    return loss, grads\n'"
gpflow/utilities/__init__.py,0,b'from .bijectors import *\nfrom .multipledispatch import Dispatcher\nfrom .utilities import *\n'
gpflow/utilities/bijectors.py,0,"b'from typing import Optional\n\nimport tensorflow_probability as tfp\n\nfrom .. import config\nfrom .utilities import to_default_float\n\n\n__all__ = [""positive"", ""triangular""]\n\n\ndef positive(lower: Optional[float] = None, base: Optional[str] = None) -> tfp.bijectors.Bijector:\n    """"""\n    Returns a positive bijector (a reversible transformation from real to positive numbers).\n\n    :param lower: overrides default lower bound\n        (if None, defaults to gpflow.config.default_positive_minimum())\n    :param base: overrides base positive bijector\n        (if None, defaults to gpflow.config.default_positive_bijector())\n    :returns: a bijector instance\n    """"""\n    bijector = base if base is not None else config.default_positive_bijector()\n    bijector = config.positive_bijector_type_map()[bijector.lower()]()\n\n    lower_bound = lower if lower is not None else config.default_positive_minimum()\n\n    if lower_bound != 0.0:\n        shift = tfp.bijectors.Shift(to_default_float(lower_bound))\n        bijector = tfp.bijectors.Chain([shift, bijector])  # from unconstrained to constrained\n    return bijector\n\n\ndef triangular() -> tfp.bijectors.Bijector:\n    """"""\n    Returns instance of a triangular bijector.\n    """"""\n    return tfp.bijectors.FillTriangular()\n'"
gpflow/utilities/multipledispatch.py,0,"b'from multipledispatch import Dispatcher as GeneratorDispatcher\nfrom multipledispatch.dispatcher import variadic_signature_matches\nfrom multipledispatch.variadic import isvariadic\n\n__all__ = [""Dispatcher""]\n\n\nclass Dispatcher(GeneratorDispatcher):\n    """"""\n    multipledispatch.Dispatcher uses a generator to yield the \n    desired function implementation, which is problematic as TensorFlow\'s\n    autograph is not able to compile code that passes through generators.\n\n    This class overwrites the problematic method in the original\n    Dispatcher and solely makes use of simple for-loops, which are\n    compilable by AutoGraph.\n    """"""\n\n    def dispatch(self, *types):\n        """"""\n        Returns matching function for `types`; if not existing returns None.\n        """"""\n        if types in self.funcs:\n            return self.funcs[types]\n\n        return self.get_first_occurrence(*types)\n\n    def get_first_occurrence(self, *types):\n        """""" \n        Returns the first occurrence of a matching function \n        \n        Based on `multipledispatch.Dispatcher.dispatch_iter`, which\n        returns an iterator of matching functions. This method uses\n        the same logic to select functions, but simply returns the first\n        element of the iterator. If no matching functions are found, \n        `None` is returned.\n        """"""\n        n = len(types)\n        for signature in self.ordering:\n            if len(signature) == n and all(map(issubclass, types, signature)):\n                result = self.funcs[signature]\n                return result\n            elif len(signature) and isvariadic(signature[-1]):\n                if variadic_signature_matches(types, signature):\n                    result = self.funcs[signature]\n                    return result\n        return None\n'"
gpflow/utilities/ops.py,37,"b'import copy\nfrom typing import List, Optional, Union\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\n\n\nEllipsisType = type(...)\n\n\ndef cast(\n    value: Union[tf.Tensor, np.ndarray], dtype: tf.DType, name: Optional[str] = None\n) -> tf.Tensor:\n    if not tf.is_tensor(value):\n        # TODO(awav): Release TF2.2 resolves this issue\n        # workaround for https://github.com/tensorflow/tensorflow/issues/35938\n        return tf.convert_to_tensor(value, dtype, name=name)\n    return tf.cast(value, dtype, name=name)\n\n\ndef eye(num: int, value: tf.Tensor, dtype: Optional[tf.DType] = None) -> tf.Tensor:\n    if dtype is not None:\n        value = cast(value, dtype)\n    return tf.linalg.diag(tf.fill([num], value))\n\n\ndef leading_transpose(\n    tensor: tf.Tensor, perm: List[Union[int, EllipsisType]], leading_dim: int = 0\n) -> tf.Tensor:\n    """"""\n    Transposes tensors with leading dimensions. Leading dimensions in\n    permutation list represented via ellipsis `...`.\n    When leading dimensions are found, `transpose` method\n    considers them as a single grouped element indexed by 0 in `perm` list. So, passing\n    `perm=[-2, ..., -1]`, you assume that your input tensor has [..., A, B] shape,\n    and you want to move leading dims between A and B dimensions.\n    Dimension indices in permutation list can be negative or positive. Valid positive\n    indices start from 1 up to the tensor rank, viewing leading dimensions `...` as zero\n    index.\n    Example:\n        a = tf.random.normal((1, 2, 3, 4, 5, 6))\n            # [..., A, B, C],\n            # where A is 1st element,\n            # B is 2nd element and\n            # C is 3rd element in\n            # permutation list,\n            # leading dimensions are [1, 2, 3]\n            # which are 0th element in permutation\n            # list\n        b = leading_transpose(a, [3, -3, ..., -2])  # [C, A, ..., B]\n        sess.run(b).shape\n        output> (6, 4, 1, 2, 3, 5)\n    :param tensor: TensorFlow tensor.\n    :param perm: List of permutation indices.\n    :returns: TensorFlow tensor.\n    :raises: ValueError when `...` cannot be found.\n    """"""\n    perm = copy.copy(perm)\n    idx = perm.index(...)\n    perm[idx] = leading_dim\n\n    rank = tf.rank(tensor)\n    perm_tf = perm % rank\n\n    leading_dims = tf.range(rank - len(perm) + 1)\n    perm = tf.concat([perm_tf[:idx], leading_dims, perm_tf[idx + 1 :]], 0)\n    return tf.transpose(tensor, perm)\n\n\ndef broadcasting_elementwise(op, a, b):\n    """"""\n    Apply binary operation `op` to every pair in tensors `a` and `b`.\n\n    :param op: binary operator on tensors, e.g. tf.add, tf.substract\n    :param a: tf.Tensor, shape [n_1, ..., n_a]\n    :param b: tf.Tensor, shape [m_1, ..., m_b]\n    :return: tf.Tensor, shape [n_1, ..., n_a, m_1, ..., m_b]\n    """"""\n    flatres = op(tf.reshape(a, [-1, 1]), tf.reshape(b, [1, -1]))\n    return tf.reshape(flatres, tf.concat([tf.shape(a), tf.shape(b)], 0))\n\n\ndef square_distance(X, X2):\n    """"""\n    Returns ||X - X2\xe1\xb5\x80||\xc2\xb2\n    Due to the implementation and floating-point imprecision, the\n    result may actually be very slightly negative for entries very\n    close to each other.\n\n    This function can deal with leading dimensions in X and X2.\n    In the sample case, where X and X2 are both 2 dimensional,\n    for example, X is [N, D] and X2 is [M, D], then a tensor of shape\n    [N, M] is returned. If X is [N1, S1, D] and X2 is [N2, S2, D]\n    then the output will be [N1, S1, N2, S2].\n    """"""\n    if X2 is None:\n        Xs = tf.reduce_sum(tf.square(X), axis=-1, keepdims=True)\n        dist = -2 * tf.matmul(X, X, transpose_b=True)\n        dist += Xs + tf.linalg.adjoint(Xs)\n        return dist\n    Xs = tf.reduce_sum(tf.square(X), axis=-1)\n    X2s = tf.reduce_sum(tf.square(X2), axis=-1)\n    dist = -2 * tf.tensordot(X, X2, [[-1], [-1]])\n    dist += broadcasting_elementwise(tf.add, Xs, X2s)\n    return dist\n\n\ndef difference_matrix(X, X2):\n    """"""\n    Returns (X - X2\xe1\xb5\x80)\n\n    This function can deal with leading dimensions in X and X2.\n    For example, If X has shape [M, D] and X2 has shape [N, D],\n    the output will have shape [M, N, D]. If X has shape [I, J, M, D]\n    and X2 has shape [K, L, N, D], the output will have shape\n    [I, J, M, K, L, N, D].\n    """"""\n    if X2 is None:\n        X2 = X\n        diff = X[..., :, tf.newaxis, :] - X2[..., tf.newaxis, :, :]\n        return diff\n    Xshape = tf.shape(X)\n    X2shape = tf.shape(X2)\n    X = tf.reshape(X, (-1, Xshape[-1]))\n    X2 = tf.reshape(X2, (-1, X2shape[-1]))\n    diff = X[:, tf.newaxis, :] - X2[tf.newaxis, :, :]\n    diff = tf.reshape(diff, tf.concat((Xshape[:-1], X2shape[:-1], [Xshape[-1]]), 0))\n    return diff\n\n\ndef pca_reduce(X: tf.Tensor, latent_dim: tf.Tensor) -> tf.Tensor:\n    """"""\n    A helpful function for linearly reducing the dimensionality of the input\n    points X to `latent_dim` dimensions.\n\n    :param X: data array of size N (number of points) x D (dimensions)\n    :param latent_dim: Number of latent dimensions Q < D\n    :return: PCA projection array of size [N, Q].\n    """"""\n    if latent_dim > X.shape[1]:  # pragma: no cover\n        raise ValueError(""Cannot have more latent dimensions than observed"")\n    X_cov = tfp.stats.covariance(X)\n    evals, evecs = tf.linalg.eigh(X_cov)\n    W = evecs[:, -latent_dim:]\n    return (X - tf.reduce_mean(X, axis=0, keepdims=True)) @ W\n'"
gpflow/utilities/utilities.py,46,"b'import copy\nimport re\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union, Iterable\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tabulate import tabulate\n\nfrom .ops import cast\nfrom ..base import Parameter\nfrom ..config import default_float, default_int, default_summary_fmt\n\n__all__ = [\n    ""set_trainable"",\n    ""multiple_assign"",\n    ""training_loop"",\n    ""print_summary"",\n    ""tabulate_module_summary"",\n    ""deepcopy"",\n    ""freeze"",\n    ""leaf_components"",\n    ""parameter_dict"",\n    ""read_values"",\n    ""to_default_float"",\n    ""to_default_int"",\n    ""reset_cache_bijectors"",\n    ""select_dict_parameters_with_prior"",\n]\n\nTraverseInput = TypeVar(""TraverseInput"", tf.Variable, tf.Module, Parameter)\nState = Any\nPath = str\nAccumulator = Tuple[Path, State]\nTraverseUpdateCallable = Callable[[TraverseInput, Path, State], State]\n\n\ndef to_default_int(x):\n    return cast(x, dtype=default_int())\n\n\ndef to_default_float(x):\n    return cast(x, dtype=default_float())\n\n\ndef set_trainable(model: Union[tf.Module, Iterable[tf.Module]], flag: bool) -> None:\n    """"""\n    Set trainable flag for all `tf.Variable`s and `gpflow.Parameter`s in a `tf.Module` or collection\n    of `tf.Module`s.\n    """"""\n    modules = [model] if isinstance(model, tf.Module) else model\n\n    for mod in modules:\n        for variable in mod.variables:\n            variable._trainable = flag\n\n\ndef multiple_assign(module: tf.Module, parameters: Dict[str, tf.Tensor]):\n    """"""\n    Multiple assign takes a dictionary with new values. Dictionary keys are paths to the\n    `tf.Variable`s or `gpflow.Parameter` of the input module.\n\n    :param module: `tf.Module`.\n    :param parameters: a dictionary with keys of the form "".module.path.to.variable"" and new value tensors.\n    """"""\n    reference_var_dict = parameter_dict(module)\n    for path, value in parameters.items():\n        reference_var_dict[path].assign(value)\n\n\ndef read_values(module: tf.Module) -> Dict[str, np.ndarray]:\n    """"""Returns a dictionary of numpy values of the module parameters (variables).""""""\n    return {k: v.numpy() for k, v in parameter_dict(module).items()}\n\n\ndef parameter_dict(module: tf.Module) -> Dict[str, Union[Parameter, tf.Variable]]:\n    """"""\n    Returns a dictionary of parameters (variables) for the `tf.Module` component.\n    Dictionary keys are relative paths to the attributes to which parameters (variables) assigned to.\n\n        class SubModule(tf.Module):\n            def __init__(self):\n                self.parameter = gpflow.Parameter(1.0)\n                self.variable = tf.Variable(1.0)\n\n        class Module(tf.Module):\n            def __init__(self):\n                self.submodule = SubModule()\n\n        m = Module()\n        params = parameter_dict(m)\n        # {\n        #   "".submodule.parameter"": <parameter object>,\n        #   "".submodule.variable"": <variable object>\n        # }\n    """"""\n    param_dict = leaf_components(module)\n    return {f"".{key.split(\'.\', 1)[-1]}"": value for key, value in param_dict.items()}\n\n\ndef training_loop(\n    closure: Callable[[], tf.Tensor],\n    optimizer: Optional[tf.optimizers.Optimizer] = None,\n    var_list: List[tf.Variable] = None,\n    maxiter=1e3,\n    compile=False,\n):\n    """"""\n    Simple generic training loop. At each iteration uses a GradientTape to compute\n    the gradients of a loss function with respect to a set of variables.\n\n    :param closure: Callable that constructs a loss function based on data and model being trained\n    :param optimizer: tf.optimizers or tf.keras.optimizers that updates variables by applying the\n        corresponding loss gradients. Adam is a default optimizer with default settings.\n    :param var_list: List of model variables to be learnt during training\n    :param maxiter: Maximum number of\n    :return:\n    """"""\n\n    optimizer = tf.optimizers.Adam() if optimizer is None else optimizer\n\n    def optimization_step():\n        with tf.GradientTape(watch_accessed_variables=False) as tape:\n            tape.watch(var_list)\n            loss = closure()\n        grads = tape.gradient(loss, var_list)\n        optimizer.apply_gradients(zip(grads, var_list))\n\n    if compile:\n        optimization_step = tf.function(optimization_step)\n\n    for _ in range(int(maxiter)):\n        optimization_step()\n\n\ndef print_summary(module: tf.Module, fmt: str = None):\n    """"""\n    Prints a summary of the parameters and variables contained in a tf.Module.\n    """"""\n    fmt = fmt if fmt is not None else default_summary_fmt()\n    if fmt == ""notebook"":\n        from IPython.core.display import display, HTML\n\n        tab = tabulate_module_summary(module, ""html"")\n        display(HTML(tab))\n    else:\n        print(tabulate_module_summary(module, fmt))\n\n\ndef tabulate_module_summary(module: tf.Module, tablefmt: Optional[str] = None) -> str:\n    def get_transform(path, var):\n        if hasattr(var, ""transform"") and var.transform is not None:\n            if isinstance(var.transform, tfp.bijectors.Chain):\n                return "" + "".join(b.__class__.__name__ for b in var.transform.bijectors[::-1])\n            return var.transform.__class__.__name__\n        return None\n\n    def get_prior(path, var):\n        if hasattr(var, ""prior"") and var.prior is not None:\n            return var.prior.name\n        return None\n\n    # list of (column_name: str, column_getter: Callable[[tf.Variable], str]) tuples:\n    column_definition = [\n        (""name"", lambda path, var: path),\n        (""class"", lambda path, var: var.__class__.__name__),\n        (""transform"", get_transform),\n        (""prior"", get_prior),\n        (""trainable"", lambda path, var: var.trainable),\n        (""shape"", lambda path, var: var.shape),\n        (""dtype"", lambda path, var: var.dtype.name),\n        (""value"", lambda path, var: _str_tensor_value(var.numpy())),\n    ]\n    column_names, column_getters = zip(*column_definition)\n\n    merged_leaf_components = _merge_leaf_components(leaf_components(module))\n\n    column_values = [\n        [getter(path, variable) for getter in column_getters]\n        for path, variable in merged_leaf_components.items()\n    ]\n    return tabulate(column_values, headers=column_names, tablefmt=tablefmt)\n\n\ndef leaf_components(input: tf.Module):\n    return _get_leaf_components(input)\n\n\ndef _merge_leaf_components(\n    input: Dict[str, Union[tf.Variable, tf.Tensor, Parameter]]\n) -> Dict[str, Union[tf.Variable, tf.Tensor, Parameter]]:\n    input_values = set([value.experimental_ref() for value in input.values()])\n    if len(input_values) == len(input):\n        return input\n    tmp_dict = dict()  # Type: Dict[ref, str]\n    for key, variable in input.items():\n        ref = variable.experimental_ref()\n        if ref in tmp_dict:\n            tmp_dict[ref] = f""{tmp_dict[ref]}\\n{key}""\n        else:\n            tmp_dict[ref] = key\n    return {key: ref.deref() for ref, key in tmp_dict.items()}\n\n\ndef _get_leaf_components(input_module: tf.Module):\n    """"""\n    Returns a list of tuples each corresponding to a gpflow.Parameter or tf.Variable in the each\n    submodules of a given tf.Module. Each tuple consists of an specific Parameter (or Variable) and\n    its relative path inside the module, which is constructed recursively by adding a prefix with\n    the path to the current module. Designed to be used as a helper for the method \'print_summary\'.\n\n    :param input_module: tf.Module including keras.Model, keras.layers.Layer and gpflow.Module.\n    :return:\n    """"""\n    target_types = (Parameter, tf.Variable)\n    input_name, state = input_module.__class__.__name__, dict()\n    accumulator = (input_name, state)\n\n    def update_state(parameter_or_variable, path, state):\n        state[path] = parameter_or_variable\n        return state\n\n    state = traverse_module(input_module, accumulator, update_state, target_types)\n    return state\n\n\ndef reset_cache_bijectors(input_module: tf.Module) -> tf.Module:\n    """"""\n    Recursively finds tfp.bijectors.Bijector-s inside the components of the tf.Module using `traverse_component`.\n    Resets the caches stored inside each tfp.bijectors.Bijector.\n\n    :param input_module: tf.Module including keras.Model, keras.layers.Layer and gpflow.Module.\n    :return:\n    """"""\n    target_types = (tfp.bijectors.Bijector,)\n    accumulator = ("""", None)\n\n    def clear_cache(b):\n        if isinstance(b, tfp.bijectors.Bijector):\n            # `_from_x` and `_from_y` are cache dictionaries for forward and inverse transformations\n            # in bijector class.\n            b._from_x.clear()\n            b._from_y.clear()\n\n    def clear_bijector(bijector, _, state):\n        clear_cache(bijector)\n        if isinstance(bijector, tfp.bijectors.Chain):\n            for m in bijector.submodules:\n                clear_cache(m)\n        return state\n\n    _ = traverse_module(input_module, accumulator, clear_bijector, target_types)\n    return input_module\n\n\nM = TypeVar(""M"", bound=tf.Module)\n\n\ndef deepcopy(input_module: M, memo: Optional[Dict[int, Any]] = None) -> M:\n    """"""\n    Returns a deepcopy of the input tf.Module. To do that first resets the caches stored inside each\n    tfp.bijectors.Bijector to allow the deepcopy of the tf.Module.\n\n    :param input_module: tf.Module including keras.Model, keras.layers.Layer and gpflow.Module.\n    :param memo: passed through to func:`copy.deepcopy` (see https://docs.python.org/3/library/copy.html).\n    :return: Returns a deepcopy of an input object.\n    """"""\n    return copy.deepcopy(reset_cache_bijectors(input_module), memo)\n\n\ndef freeze(input_module: M) -> M:\n    """"""\n    Returns a deepcopy of the input tf.Module with constants instead of variables and parameters.\n\n    :param input_module: tf.Module or gpflow.Module.\n    :return: Returns a frozen deepcopy of an input object.\n    """"""\n    objects_to_freeze = _get_leaf_components(input_module)\n    memo_tensors = {id(v): tf.convert_to_tensor(v) for v in objects_to_freeze.values()}\n    module_copy = deepcopy(input_module, memo_tensors)\n    return module_copy\n\n\ndef traverse_module(\n    m: TraverseInput, acc: Accumulator, update_cb: TraverseUpdateCallable, target_types: tuple\n) -> Accumulator:\n    """"""\n    Recursively traverses `m`, accumulating in `acc` a path and a state until it finds an object of type\n    in `target_types` to apply `update_cb` to update the accumulator `acc` and/or the object.\n\n    :param m: tf.Module, tf.Variable or gpflow.Parameter\n    :param acc: Tuple of path and state\n    :param update_cb: Callable\n    :param target_types: target class types\n    :return:\n    """"""\n    path, state = acc\n\n    new_state = state\n\n    if isinstance(m, target_types):\n        return update_cb(m, path, state)\n\n    if isinstance(m, (list, tuple)):\n        for term_idx, subterm in enumerate(m):\n            new_acc = (f""{path}[{term_idx}]"", new_state)\n            new_state = traverse_module(subterm, new_acc, update_cb, target_types)\n    elif isinstance(m, dict):\n        for term_idx, subterm in m.items():\n            new_acc = (f""{path}[\'{term_idx}\']"", new_state)\n            new_state = traverse_module(subterm, new_acc, update_cb, target_types)\n    elif isinstance(m, tf.Module):\n        for name, submodule in vars(m).items():\n            ignored_attributes = m._TF_MODULE_IGNORED_PROPERTIES\n            # NOTE(awav): since tfp version 0.10.0, tfp.bijectors.Bijector instances have\n            # `_parameters` dictionary with ""self"" references that cause\n            # infinite recursive loop.\n            if isinstance(m, tfp.bijectors.Bijector):\n                ignored_attributes = ignored_attributes.union({""_parameters""})\n            if name in ignored_attributes:\n                continue\n            new_acc = (f""{path}.{name}"", new_state)\n            new_state = traverse_module(submodule, new_acc, update_cb, target_types)\n    return new_state\n\n\n@lru_cache()\ndef _first_three_elements_regexp():\n    num_re = r""[+\\-]?(?:0|[1-9]\\d*)(?:\\.\\d*)?(?:[eE][+\\-]?\\d+)?""\n    pat_re = rf""^(?:(\\[+)\\s*)?({num_re})(?:\\s+({num_re})(?:\\s+({num_re}))?)?.*?""\n    return re.compile(pat_re)\n\n\ndef _str_tensor_value(value: np.ndarray):\n    value_str = str(value)\n    if value.size <= 3:\n        return value_str\n\n    max_chars = 500\n    value_str = value_str[:max_chars]\n    regexp = _first_three_elements_regexp()\n    match = regexp.match(value_str)\n    assert match is not None\n    brackets, elem1, elem2, elem3 = match.groups()\n\n    out = f""{elem1}""\n    if elem2 is not None:\n        out = f""{out}{f\', {elem2}\'}""\n        if elem3 is not None:\n            out = f""{out}{f\', {elem3}\'}""\n    if brackets is not None:\n        out = f""{brackets}{out}...""\n\n    return out\n\n\ndef select_dict_parameters_with_prior(model: tf.Module) -> Dict[str, Parameter]:\n    """"""Collects parameters with prior into a dictionary.""""""\n    return {\n        k: p\n        for k, p in parameter_dict(model).items()\n        if hasattr(p, ""prior"") and p.prior is not None\n    }\n'"
tests/gpflow/__init__.py,0,b''
tests/gpflow/test_base.py,7,"b'# Copyright 2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport gpflow\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom gpflow.utilities import positive\n\n\ndef test_parameter_assign_validation():\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        param = gpflow.Parameter(0.0, transform=positive())\n\n    param = gpflow.Parameter(0.1, transform=positive())\n    param.assign(0.2)\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        param.assign(0.0)\n\n\ndef test_cast_to_dtype_precision_issue():\n    """"""\n    TensorFlow\'s tf.cast(value, dtype) implicitly does a tf.convert_to_tensor(value)\n    *before* the cast when the value is not a tensor already. When value is a python float,\n    this results in the following behaviour:\n\n    >>> tf.cast(0.2, tf.float64)\n    <tf.Tensor: id=37, shape=(), dtype=float64, numpy=0.20000000298023224>\n    \n    instead of the expected expansion of 0.2 to float64 precision that you get when\n    passing in an object that already carries dtype information, such as a numpy array\n    (which has float64 precision by default):\n\n    >>> tf.cast(np.array(0.2), tf.float64)\n    <tf.Tensor: id=40, shape=(), dtype=float64, numpy=0.2>\n\n    This affected *all* gpflow.Parameter objects, resulting in numerical discrepancies\n    between GPflow 1 and 2, due to the pass through _cast_to_dtype, which is now fixed.\n    This is the corresponding regression test.\n    """"""\n    p = gpflow.Parameter(0.2, dtype=np.float64)\n    actual_value = p.numpy()\n    assert actual_value.dtype == np.float64\n    expected_value = np.float64(0.2)\n    assert actual_value == expected_value\n'"
tests/gpflow/test_base_prior.py,1,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom gpflow.base import PriorOn\nfrom tensorflow_probability.python.distributions import Uniform\nfrom tensorflow_probability.python.bijectors import Exp\n\nimport gpflow\nfrom gpflow.config import set_default_float\nfrom gpflow.utilities import to_default_float\n\nnp.random.seed(1)\n\n\nclass Datum:\n    X = 10 * np.random.randn(5, 1)\n    Y = 10 * np.random.randn(5, 1)\n    lengthscale = 3.3\n\n\ndef test_gpr_objective_equivalence():\n    """"""\n    In Maximum Likelihood Estimation (MLE), i.e. when there are no priors on\n    the parameters, the objective should not depend on any transforms on the\n    parameters.\n    We use GPR as a simple model that has an objective.\n    """"""\n    data = (Datum.X, Datum.Y)\n    l_value = Datum.lengthscale\n\n    l_variable = tf.Variable(l_value, dtype=gpflow.default_float(), trainable=True)\n    m1 = gpflow.models.GPR(data, kernel=gpflow.kernels.SquaredExponential(lengthscales=l_value))\n    m2 = gpflow.models.GPR(data, kernel=gpflow.kernels.SquaredExponential())\n    m2.kernel.lengthscales = gpflow.Parameter(l_variable, transform=None)\n    assert np.allclose(\n        m1.kernel.lengthscales.numpy(), m2.kernel.lengthscales.numpy()\n    )  # consistency check\n\n    assert np.allclose(\n        m1.log_marginal_likelihood().numpy(), m2.log_marginal_likelihood().numpy()\n    ), ""MLE objective should not depend on Parameter transform""\n\n\ndef test_log_prior_with_no_prior():\n    """"""\n    A parameter without any prior should have zero log-prior,\n    even if it has a transform to constrain it.\n    """"""\n    param = gpflow.Parameter(5.3, transform=gpflow.utilities.positive())\n    assert param.log_prior_density().numpy() == 0.0\n\n\ndef test_log_prior_for_uniform_prior():\n    """"""\n    If we assign a Uniform prior to a parameter, we should not expect the value of the prior density\n    to change with the parameter value, even if it has a transform associated with it.\n    """"""\n\n    uniform_prior = Uniform(low=np.float64(0), high=np.float64(100))\n    param = gpflow.Parameter(1.0, transform=gpflow.utilities.positive(), prior=uniform_prior)\n    low_value = param.log_prior_density().numpy()\n    param.assign(10.0)\n    high_value = param.log_prior_density().numpy()\n\n    assert np.isclose(low_value, high_value)\n\n\ndef test_log_prior_on_unconstrained():\n    """"""\n    A parameter with an Exp transform, and a uniform prior on its unconstrained, should have a\n    prior in the constrained space that scales as 1/value.\n    """"""\n\n    initial_value = 1.0\n    scale_factor = 10.0\n    uniform_prior = Uniform(low=np.float64(0), high=np.float64(100))\n    param = gpflow.Parameter(\n        initial_value, transform=Exp(), prior=uniform_prior, prior_on=PriorOn.UNCONSTRAINED,\n    )\n    low_value = param.log_prior_density().numpy()\n    param.assign(scale_factor * initial_value)\n    high_value = param.log_prior_density().numpy()\n\n    assert np.isclose(low_value, high_value + np.log(scale_factor))\n\n\nclass DummyModel(gpflow.models.BayesianModel):\n    value = 3.3\n    log_scale = 0.4\n\n    def __init__(self, with_transform):\n        super().__init__()\n\n        prior = tfp.distributions.Normal(to_default_float(1.0), to_default_float(1.0))\n\n        scale = np.exp(self.log_scale)\n        if with_transform:\n            transform = tfp.bijectors.AffineScalar(scale=to_default_float(scale))\n        else:\n            transform = None\n\n        self.theta = gpflow.Parameter(self.value, prior=prior, transform=transform)\n\n    def maximum_log_likelihood_objective(self):\n        return (self.theta + 5) ** 2\n\n\ndef test_map_invariance_to_transform():\n    m1 = DummyModel(with_transform=True)\n    m2 = DummyModel(with_transform=False)\n    assert np.allclose(\n        m1.log_posterior_density().numpy(), m2.log_posterior_density().numpy()\n    ), ""log posterior density should not be affected by a transform""\n\n\ndef get_gpmc_model_params():\n    kernel = gpflow.kernels.Matern32()\n    likelihood = gpflow.likelihoods.Gaussian()\n    data = [np.random.randn(5, 1), np.random.randn(5, 1)]\n    return data, kernel, likelihood\n\n\n@pytest.mark.parametrize(\n    ""model_class, args"",\n    [\n        (gpflow.models.GPMC, get_gpmc_model_params()),\n        # (gpflow.models.SGPMC, get_SGPMC_model_params()) # Fails due to inducing_variable=None bug\n    ],\n)\ndef test_v_prior_dtypes(model_class, args):\n    with gpflow.config.as_context():\n        set_default_float(np.float32)\n        m = model_class(*args)\n        assert m.V.prior.dtype == np.float32\n        set_default_float(np.float64)\n        m = model_class(*args)\n        assert m.V.prior.dtype == np.float64\n'"
tests/gpflow/test_base_training.py,3,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport gpflow\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom gpflow.utilities import (\n    multiple_assign,\n    set_trainable,\n    leaf_components,\n    read_values,\n)\n\nrng = np.random.RandomState(0)\n\n\nclass Data:\n    H0 = 5\n    H1 = 2\n    M = 10\n    D = 1\n    Z = rng.rand(M, 1)\n    ls = 2.0\n    ls_new = 1.5\n    var = 1.0\n    var_new = 10.0\n\n\n# ------------------------------------------\n# Fixtures\n# ------------------------------------------\n\n\n@pytest.fixture\ndef model():\n    return gpflow.models.SVGP(\n        kernel=gpflow.kernels.SquaredExponential(lengthscales=Data.ls, variance=Data.var),\n        likelihood=gpflow.likelihoods.Gaussian(),\n        inducing_variable=Data.Z,\n        q_diag=True,\n    )\n\n\n# ------------------------------------------\n# Reference\n# ------------------------------------------\n\nmodel_param_updates = {\n    "".kernel.lengthscales"": Data.ls_new,\n    "".likelihood.variance"": Data.var_new,\n    "".inducing_variable.Z"": np.zeros_like(Data.Z),\n    "".q_sqrt"": 0.5 * np.ones((Data.M, 1)),\n}\nmodel_wrong_path = [\n    {""kernel.lengthscales"": Data.ls_new},\n    {"".Gaussian.variance"": Data.var_new},\n    {""inducing_variable.Z"": np.zeros_like(Data.Z)},\n    {"".q_std"": 0.5 * np.ones((Data.M, 1))},\n]\n\nmodel_wrong_value = [\n    {"".likelihood.variance"": np.ones((2, 1), dtype=np.int32)},\n    {"".inducing_variable.Z"": [1, 2, 3]},\n]\n\n\n@pytest.mark.parametrize(""var_update_dict"", [model_param_updates])\ndef test_multiple_assign_updates_correct_values(model, var_update_dict):\n    old_value_dict = leaf_components(model).copy()\n    multiple_assign(model, var_update_dict)\n    for path, variable in leaf_components(model).items():\n        if path in var_update_dict.keys():\n            np.testing.assert_almost_equal(\n                variable.value().numpy(), var_update_dict[path], decimal=7\n            )\n        else:\n            np.testing.assert_equal(variable.value().numpy(), old_value_dict[path].value().numpy())\n\n\n@pytest.mark.parametrize(""wrong_var_update_dict"", model_wrong_path)\ndef test_multiple_assign_fails_with_invalid_path(model, wrong_var_update_dict):\n    with pytest.raises(KeyError):\n        multiple_assign(model, wrong_var_update_dict)\n\n\n@pytest.mark.parametrize(""wrong_var_update_dict"", model_wrong_value)\ndef test_multiple_assign_fails_with_invalid_values(model, wrong_var_update_dict):\n    with pytest.raises(ValueError):\n        multiple_assign(model, wrong_var_update_dict)\n\n\ndef test_dict_utilities(model):\n    """"""\n    Test both `parameter_dict()` and `read_values()`\n    """"""\n\n    class SubModule(tf.Module):\n        def __init__(self):\n            self.parameter = gpflow.Parameter(1.0)\n            self.variable = tf.Variable(1.0)\n\n    class Module(tf.Module):\n        def __init__(self):\n            self.submodule = SubModule()\n            self.top_parameter = gpflow.Parameter(3.0)\n\n    m = Module()\n    params = gpflow.utilities.parameter_dict(m)\n    # {\n    #   "".submodule.parameter"": <parameter object>,\n    #   "".submodule.variable"": <variable object>\n    # }\n    assert list(params.keys()) == [\n        "".submodule.parameter"",\n        "".submodule.variable"",\n        "".top_parameter"",\n    ]\n    assert list(params.values()) == [\n        m.submodule.parameter,\n        m.submodule.variable,\n        m.top_parameter,\n    ]\n\n    for k, v in read_values(m).items():\n        assert params[k].numpy() == v\n'"
tests/gpflow/test_kullback_leiblers.py,15,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose, assert_almost_equal\n\nimport gpflow\nfrom gpflow import default_float, default_jitter, Parameter\nfrom gpflow.inducing_variables import InducingPoints\nfrom gpflow.kullback_leiblers import gauss_kl, prior_kl\nfrom gpflow.utilities.bijectors import triangular\n\nrng = np.random.RandomState(0)\n\n# ------------------------------------------\n# Fixtures\n# ------------------------------------------\n\nLn = 2\nNn = 10\nMn = 50\n\n\n@pytest.fixture(scope=""module"")\ndef kernel():\n    k = gpflow.kernels.Matern32() + gpflow.kernels.White()\n    k.kernels[1].variance.assign(0.01)\n    return k\n\n\n@pytest.fixture(scope=""module"")\ndef inducing_points():\n    return InducingPoints(rng.randn(Nn, 1))\n\n\n@pytest.fixture(scope=""module"")\ndef mu():\n    return Parameter(rng.randn(Nn, Ln))\n\n\n# ------------------------------------------\n# Helpers\n# ------------------------------------------\n\n\ndef make_sqrt(N, M):\n    return np.array([np.tril(rng.randn(M, M)) for _ in range(N)])  # [N, M, M]\n\n\ndef make_K_batch(N, M):\n    K_np = rng.randn(N, M, M)\n    beye = np.array([np.eye(M) for _ in range(N)])\n    return 0.1 * (K_np + np.transpose(K_np, (0, 2, 1))) + beye\n\n\ndef compute_kl_1d(q_mu, q_sigma, p_var=1.0):\n    p_var = tf.ones_like(q_sigma) if p_var is None else p_var\n    q_var = tf.square(q_sigma)\n    kl = 0.5 * (q_var / p_var + tf.square(q_mu) / p_var - 1 + tf.math.log(p_var / q_var))\n    return tf.reduce_sum(kl)\n\n\n# ------------------------------------------\n# Data classes: storing constants\n# ------------------------------------------\n\n\nclass Datum:\n    M, N = 5, 4\n\n    mu = rng.randn(M, N)  # [M, N]\n    A = rng.randn(M, M)\n    I = np.eye(M)  # [M, M]\n    K = A @ A.T + default_jitter() * I  # [M, M]\n    sqrt = make_sqrt(N, M)  # [N, M, M]\n    sqrt_diag = rng.randn(M, N)  # [M, N]\n    K_batch = make_K_batch(N, M)\n    K_cholesky = np.linalg.cholesky(K)\n\n\n@pytest.mark.parametrize(""diag"", [True, False])\ndef test_kl_k_cholesky(diag):\n    """"""\n    Test that passing K or K_cholesky yield the same answer\n    """"""\n    q_mu = Datum.mu\n    q_sqrt = Datum.sqrt_diag if diag else Datum.sqrt\n    kl_K = gauss_kl(q_mu, q_sqrt, K=Datum.K)\n    kl_K_chol = gauss_kl(q_mu, q_sqrt, K_cholesky=Datum.K_cholesky)\n\n    np.testing.assert_allclose(kl_K.numpy(), kl_K_chol.numpy())\n\n\n@pytest.mark.parametrize(""white"", [True, False])\ndef test_diags(white):\n    """"""\n    The covariance of q(x) can be Cholesky matrices or diagonal matrices.\n    Here we make sure the behaviours overlap.\n    """"""\n    # the chols are diagonal matrices, with the same entries as the diag representation.\n    chol_from_diag = tf.stack(\n        [tf.linalg.diag(Datum.sqrt_diag[:, i]) for i in range(Datum.N)]  # [N, M, M]\n    )\n    kl_diag = gauss_kl(Datum.mu, Datum.sqrt_diag, Datum.K if white else None)\n    kl_dense = gauss_kl(Datum.mu, chol_from_diag, Datum.K if white else None)\n\n    np.testing.assert_allclose(kl_diag, kl_dense)\n\n\n@pytest.mark.parametrize(""diag"", [True, False])\ndef test_whitened(diag):\n    """"""\n    Check that K=Identity and K=None give same answer\n    """"""\n    chol_from_diag = tf.stack(\n        [tf.linalg.diag(Datum.sqrt_diag[:, i]) for i in range(Datum.N)]  # [N, M, M]\n    )\n    s = Datum.sqrt_diag if diag else chol_from_diag\n\n    kl_white = gauss_kl(Datum.mu, s)\n    kl_nonwhite = gauss_kl(Datum.mu, s, Datum.I)\n\n    np.testing.assert_allclose(kl_white, kl_nonwhite)\n\n\n@pytest.mark.parametrize(""shared_k"", [True, False])\n@pytest.mark.parametrize(""diag"", [True, False])\ndef test_sumkl_equals_batchkl(shared_k, diag):\n    """"""\n    gauss_kl implicitely performs a sum of KL divergences\n    This test checks that doing the sum outside of the function is equivalent\n    For q(X)=prod q(x_l) and p(X)=prod p(x_l), check that sum KL(q(x_l)||p(x_l)) = KL(q(X)||p(X))\n    Here, q(X) has covariance [L, M, M]\n    p(X) has covariance [L, M, M] ( or [M, M] )\n    Here, q(x_i) has covariance [1, M, M]\n    p(x_i) has covariance [M, M]\n    """"""\n    s = Datum.sqrt_diag if diag else Datum.sqrt\n    kl_batch = gauss_kl(Datum.mu, s, Datum.K if shared_k else Datum.K_batch)\n    kl_sum = []\n    for n in range(Datum.N):\n        q_mu_n = Datum.mu[:, n][:, None]  # [M, 1]\n        q_sqrt_n = (\n            Datum.sqrt_diag[:, n][:, None] if diag else Datum.sqrt[n, :, :][None, :, :]\n        )  # [1, M, M] or [M, 1]\n        K_n = Datum.K if shared_k else Datum.K_batch[n, :, :][None, :, :]  # [1, M, M] or [M, M]\n        kl_n = gauss_kl(q_mu_n, q_sqrt_n, K=K_n)\n        kl_sum.append(kl_n)\n\n    kl_sum = tf.reduce_sum(kl_sum)\n    assert_almost_equal(kl_sum, kl_batch)\n\n\n@pytest.mark.parametrize(""dim"", [0, 1])\n@pytest.mark.parametrize(""white"", [True, False])\ndef test_oned(white, dim):\n    """"""\n    Check that the KL divergence matches a 1D by-hand calculation.\n    """"""\n    mu1d = Datum.mu[dim, :][None, :]  # [1, N]\n    s1d = Datum.sqrt[:, dim, dim][:, None, None]  # [N, 1, 1]\n    K1d = Datum.K_batch[:, dim, dim][:, None, None]  # [N, 1, 1]\n\n    kl = gauss_kl(mu1d, s1d, K1d if not white else None)\n    kl_1d = compute_kl_1d(\n        tf.reshape(mu1d, (-1,)),  # N\n        tf.reshape(s1d, (-1,)),  # N\n        None if white else tf.reshape(K1d, (-1,)),\n    )  # N\n    np.testing.assert_allclose(kl, kl_1d)\n\n\ndef test_unknown_size_inputs():\n    """"""\n    Test for #725 and #734. When the shape of the Gaussian\'s mean had at least\n    one unknown parameter, `gauss_kl` would blow up. This happened because\n    `tf.size` can only output types `tf.int32` or `tf.int64`.\n    """"""\n    mu = np.ones([1, 4], dtype=default_float())\n    sqrt = np.ones([4, 1, 1], dtype=default_float())\n\n    known_shape = gauss_kl(*map(tf.constant, [mu, sqrt]))\n    unknown_shape = gauss_kl(mu, sqrt)\n\n    np.testing.assert_allclose(known_shape, unknown_shape)\n\n\n@pytest.mark.parametrize(""white"", [True, False])\ndef test_q_sqrt_constraints(inducing_points, kernel, mu, white):\n    """""" Test that sending in an unconstrained q_sqrt returns the same conditional\n    evaluation and gradients. This is important to match the behaviour of the KL, which\n    enforces q_sqrt is triangular.\n    """"""\n\n    tril = np.tril(rng.randn(Ln, Nn, Nn))\n\n    q_sqrt_constrained = Parameter(tril, transform=triangular())\n    q_sqrt_unconstrained = Parameter(tril)\n\n    diff_before_gradient_step = (q_sqrt_constrained - q_sqrt_unconstrained).numpy()\n    assert_allclose(diff_before_gradient_step, 0)\n\n    kls = []\n    for q_sqrt in [q_sqrt_constrained, q_sqrt_unconstrained]:\n\n        with tf.GradientTape() as tape:\n            kl = prior_kl(inducing_points, kernel, mu, q_sqrt, whiten=white)\n\n        grad = tape.gradient(kl, q_sqrt.unconstrained_variable)\n        q_sqrt.unconstrained_variable.assign_sub(grad)\n        kls.append(kl)\n\n    diff_kls_before_gradient_step = kls[0] - kls[1]\n\n    assert_allclose(diff_kls_before_gradient_step, 0)\n\n    diff_after_gradient_step = (q_sqrt_constrained - q_sqrt_unconstrained).numpy()\n    assert_allclose(diff_after_gradient_step, 0)\n'"
tests/gpflow/test_logdensities.py,0,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom numpy.random import randn\nimport pytest\nimport tensorflow as tf\nfrom gpflow import logdensities\nfrom gpflow import default_float\nfrom gpflow.utilities import to_default_float\nimport scipy.stats\nfrom scipy.stats import multivariate_normal as mvn\nfrom numpy.testing import assert_allclose\n\nrng = np.random.RandomState(1)\n\n\n@pytest.mark.parametrize(""x, mu, var"", [(0.9, 0.5, 1.3)])\ndef test_gaussian(x, mu, var):\n    gpf = logdensities.gaussian(x, mu, var).numpy()\n    sps = scipy.stats.norm(loc=mu, scale=np.sqrt(var)).logpdf(x)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x, mu, var"", [(0.9, 0.5, 1.3)])\ndef test_lognormal(x, mu, var):\n    gpf = logdensities.lognormal(x, mu, var).numpy()\n    sps = scipy.stats.lognorm(s=np.sqrt(var), scale=np.exp(mu)).logpdf(x)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x, p"", [[1, 0.6], [0, 0.6],])\ndef test_bernoulli(x, p):\n    gpf = logdensities.bernoulli(x, p).numpy()\n    sps = scipy.stats.bernoulli.logpmf(k=x, p=p)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x, lam"", [[0, 1.3], [1, 1.3], [2, 1.3],])\ndef test_poisson(x, lam):\n    gpf = logdensities.poisson(x, lam).numpy()\n    sps = scipy.stats.poisson.logpmf(k=x, mu=lam)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x, scale"", [(0.9, 1.3)])\ndef test_exponential(x, scale):\n    gpf = logdensities.exponential(x, scale).numpy()\n    sps = scipy.stats.expon(loc=0.0, scale=scale).logpdf(x)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x, shape, scale"", [(0.9, 0.5, 1.3)])\ndef test_gamma(x, shape, scale):\n    gpf = logdensities.gamma(x, shape, scale).numpy()\n    sps = scipy.stats.gamma(a=shape, loc=0.0, scale=scale).logpdf(x)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(\n    ""x, mean, scale, df"", [(0.9, 0.5, 1.3, 1), (0.9, 0.5, 1.3, 2), (0.9, 0.5, 1.3, 3),]\n)\ndef test_student_t(x, mean, scale, df):\n    cast = to_default_float\n    gpf = logdensities.student_t(cast(x), cast(mean), cast(scale), df).numpy()\n    sps = scipy.stats.t(df=df, loc=mean, scale=scale).logpdf(x)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x, alpha, beta"", [(0.9, 0.5, 1.3)])\ndef test_beta(x, alpha, beta):\n    gpf = logdensities.beta(x, alpha, beta).numpy()\n    sps = scipy.stats.beta(a=alpha, b=beta).logpdf(x)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x, mu, sigma"", [(0.9, 0.5, 1.3)])\ndef test_laplace(x, mu, sigma):\n    gpf = logdensities.laplace(x, mu, sigma).numpy()\n    sps = scipy.stats.laplace(loc=mu, scale=sigma).logpdf(x)\n    np.testing.assert_allclose(gpf, sps)\n\n\n@pytest.mark.parametrize(""x"", [randn(4, 10), randn(4, 1)])\n@pytest.mark.parametrize(""mu"", [randn(4, 10), randn(4, 1)])\n@pytest.mark.parametrize(""cov_sqrt"", [randn(4, 4), np.eye(4)])\ndef test_multivariate_normal(x, mu, cov_sqrt):\n    cov = np.dot(cov_sqrt, cov_sqrt.T)\n    L = np.linalg.cholesky(cov)\n\n    gp_result = logdensities.multivariate_normal(x, mu, L)\n\n    if mu.shape[1] > 1:\n        if x.shape[1] > 1:\n            sp_result = [mvn.logpdf(x[:, i], mu[:, i], cov) for i in range(mu.shape[1])]\n        else:\n            sp_result = [mvn.logpdf(x.ravel(), mu[:, i], cov) for i in range(mu.shape[1])]\n    else:\n        sp_result = mvn.logpdf(x.T, mu.ravel(), cov)\n    assert_allclose(gp_result, sp_result)\n'"
tests/gpflow/test_mean_functions.py,0,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.config import default_int\nfrom gpflow.inducing_variables import InducingPoints\nfrom gpflow.mean_functions import (\n    Additive,\n    Constant,\n    Linear,\n    Product,\n    SwitchedMeanFunction,\n    Zero,\n)\n\nrng = np.random.RandomState(99021)\n\n\nclass Datum:\n    input_dim, output_dim = 3, 2\n    N, Ntest, M = 20, 30, 10\n\n\n_mean_functions = [\n    Zero(),\n    Linear(\n        A=rng.randn(Datum.input_dim, Datum.output_dim),\n        b=rng.randn(Datum.output_dim, 1).reshape(-1),\n    ),\n    Constant(c=rng.randn(Datum.output_dim, 1).reshape(-1)),\n]\n\n\n@pytest.mark.parametrize(""mean_function_1"", _mean_functions)\n@pytest.mark.parametrize(""mean_function_2"", _mean_functions)\n@pytest.mark.parametrize(""operation"", [""+"", ""*""])\ndef test_mean_functions_output_shape(mean_function_1, mean_function_2, operation):\n    """"""\n    Test the output shape for basic and compositional mean functions, also\n    check that the combination of mean functions returns the correct class\n    """"""\n    X = np.random.randn(Datum.N, Datum.input_dim)\n    Y = mean_function_1(X)\n    # basic output shape check\n    assert Y.shape in [(Datum.N, Datum.output_dim), (Datum.N, 1)]\n\n    # composed mean function output shape check\n    if operation == ""+"":\n        mean_composed = mean_function_1 + mean_function_2\n    elif operation == ""*"":\n        mean_composed = mean_function_1 * mean_function_2\n    else:\n        raise (NotImplementedError)\n\n    Y_composed = mean_composed(X)\n    assert Y_composed.shape in [(Datum.N, Datum.output_dim), (Datum.N, 1)]\n\n\n@pytest.mark.parametrize(""mean_function_1"", _mean_functions)\n@pytest.mark.parametrize(""mean_function_2"", _mean_functions)\n@pytest.mark.parametrize(""operation"", [""+"", ""*""])\ndef test_mean_functions_composite_type(mean_function_1, mean_function_2, operation):\n    if operation == ""+"":\n        mean_composed = mean_function_1 + mean_function_2\n        assert isinstance(mean_composed, Additive)\n    elif operation == ""*"":\n        mean_composed = mean_function_1 * mean_function_2\n        assert isinstance(mean_composed, Product)\n    else:\n        raise (NotImplementedError)\n\n\n_linear_functions = [\n    Linear(\n        A=rng.randn(Datum.input_dim, Datum.output_dim),\n        b=rng.randn(Datum.output_dim, 1).reshape(-1),\n    )\n    for _ in range(3)\n]\n\n# Append inverse of first Linear mean function in _linear_functions\n_linear_functions.append(Linear(A=-1.0 * _linear_functions[0].A, b=-1.0 * _linear_functions[0].b))\n\n_constant_functions = [Constant(c=rng.randn(Datum.output_dim, 1).reshape(-1)) for _ in range(3)]\n# Append inverse of first Constant mean function in _constant_functions\n_constant_functions.append(Constant(c=-1.0 * _constant_functions[0].c))\n\n\ndef _create_GPR_model_with_bias(X, Y, mean_function):\n    return gpflow.models.GPR(\n        (X, Y), mean_function=mean_function, kernel=gpflow.kernels.Bias(Datum.input_dim)\n    )\n\n\n@pytest.mark.parametrize(""mean_functions"", [_linear_functions, _constant_functions])\ndef test_mean_functions_distributive_property(mean_functions):\n    """"""\n    Tests that distributive property of addition and multiplication holds for mean functions\n    (both Constant and Linear): A * (B + C) = A * B + A * C\n    """"""\n    X, Y = rng.randn(Datum.N, Datum.input_dim), rng.randn(Datum.N, Datum.output_dim)\n    Xtest = rng.randn(30, Datum.input_dim)\n    A, B, C = mean_functions[0], mean_functions[1], mean_functions[2]\n    lhs = Product(A, Additive(B, C))  # A * (B + C)\n    rhs = Additive(Product(A, B), Product(A, C))  # A * B + A * C\n\n    model_lhs = _create_GPR_model_with_bias(X, Y, mean_function=lhs)\n    model_rhs = _create_GPR_model_with_bias(X, Y, mean_function=rhs)\n\n    mu_lhs, var_lhs = model_lhs.predict_f(Xtest)\n    mu_rhs, var_rhs = model_rhs.predict_f(Xtest)\n\n    assert_allclose(mu_lhs, mu_rhs)\n    assert_allclose(var_lhs, var_rhs)\n\n\n@pytest.mark.parametrize(""mean_functions"", [_linear_functions, _constant_functions])\ndef test_mean_functions_A_minus_A_equals_zero(mean_functions):\n    """"""\n    Tests that the addition the inverse of a mean function to itself is equivalent to having a\n    Zero mean function: A + (-A) = 0\n    """"""\n    X, Y = rng.randn(Datum.N, Datum.input_dim), rng.randn(Datum.N, Datum.output_dim)\n    Xtest = rng.randn(30, Datum.input_dim)\n    A, A_inverse = mean_functions[0], mean_functions[-1]\n    lhs = Additive(A, A_inverse)  # A + (-A)\n    rhs = Zero()  # 0\n\n    model_lhs = _create_GPR_model_with_bias(X, Y, mean_function=lhs)\n    model_rhs = _create_GPR_model_with_bias(X, Y, mean_function=rhs)\n\n    mu_lhs, var_lhs = model_lhs.predict_f(Xtest)\n    mu_rhs, var_rhs = model_rhs.predict_f(Xtest)\n\n    assert_allclose(mu_lhs, mu_rhs)\n    assert_allclose(var_lhs, var_rhs)\n\n\n@pytest.mark.parametrize(""mean_functions"", [_linear_functions])\ndef test_linear_mean_functions_associative_property(mean_functions):\n    """"""\n    Tests that associative property of addition holds for linear mean functions:\n    A + (B + (-A)) = B = (A + B) + (-A)\n    """"""\n    X, Y = rng.randn(Datum.N, Datum.input_dim), rng.randn(Datum.N, Datum.output_dim)\n    Xtest = rng.randn(30, Datum.input_dim)\n    A, B, A_inverse = mean_functions[0], mean_functions[1], mean_functions[-1]\n\n    lhs = Additive(A, Additive(B, A_inverse))  # A + (B + (-A))\n    rhs = Additive(Additive(A, B), A_inverse)  # (A + B) + (-A)\n\n    model_lhs = _create_GPR_model_with_bias(X, Y, mean_function=lhs)\n    model_b = _create_GPR_model_with_bias(X, Y, mean_function=B)\n    model_rhs = _create_GPR_model_with_bias(X, Y, mean_function=rhs)\n\n    mu_lhs, var_lhs = model_lhs.predict_f(Xtest)\n    mu_b, var_b = model_b.predict_f(Xtest)\n    mu_rhs, var_rhs = model_rhs.predict_f(Xtest)\n\n    assert_allclose(mu_lhs, mu_b)\n    assert_allclose(var_lhs, var_b)\n    assert_allclose(mu_b, mu_rhs)\n    assert_allclose(var_b, var_rhs)\n\n\n@pytest.mark.parametrize(""N, D"", [[10, 3]])\ndef test_switched_mean_function(N, D):\n    """"""\n    Test for the SwitchedMeanFunction.\n    """"""\n    X = np.hstack([rng.randn(N, D), 1.0 * rng.randint(0, 2, N).reshape(-1, 1)])\n    zeros, ones = Constant(np.zeros(1)), Constant(np.ones(1))\n    switched_mean = SwitchedMeanFunction([zeros, ones])\n\n    np_list = np.array([0.0, 1.0])\n    result_ref = (np_list[X[:, D].astype(default_int())]).reshape(-1, 1)\n    result = switched_mean(X)\n\n    assert_allclose(result, result_ref)\n\n\ndef test_bug_277_regression():\n    """"""\n    See github issue #277. This is a regression test.\n    """"""\n    model1, model2 = Linear(), Linear()\n    assert model1.b.numpy() == model2.b.numpy()\n    model2.b.assign([1.0])\n    assert not model1.b.numpy() == model2.b.numpy()\n\n\n_model_classes = [\n    gpflow.models.GPR,\n    gpflow.models.SGPR,\n    gpflow.models.GPRFITC,\n    gpflow.models.SVGP,\n    gpflow.models.VGP,\n    gpflow.models.GPMC,\n    gpflow.models.SGPMC,\n]\n\n\n@pytest.mark.parametrize(""model_class"", _model_classes)\ndef test_models_with_mean_functions_changes(model_class):\n    """"""\n    Simply check that all models have a higher prediction with a constant mean\n    function than with a zero mean function.\n\n    For compositions of mean functions check that multiplication/ addition of\n    a constant results in a higher prediction, whereas addition of zero/\n    mutliplication with one does not.\n    """"""\n    data = rng.randn(Datum.N, Datum.input_dim), rng.randn(Datum.N, 1)\n    Xnew = rng.randn(Datum.Ntest, Datum.input_dim)\n    inducing_variable = InducingPoints(Z=rng.randn(Datum.M, Datum.input_dim))\n    kernel = gpflow.kernels.Matern32()\n    likelihood = gpflow.likelihoods.Gaussian()\n    zero_mean = Zero()\n    non_zero_mean = Constant(c=np.ones(1) * 10)\n\n    if model_class in [gpflow.models.GPR]:\n        model_zero_mean = model_class(data, kernel=kernel, mean_function=zero_mean)\n        model_non_zero_mean = model_class(data, kernel=kernel, mean_function=non_zero_mean)\n    elif model_class in [gpflow.models.VGP]:\n        model_zero_mean = model_class(\n            data, likelihood=likelihood, kernel=kernel, mean_function=zero_mean\n        )\n        model_non_zero_mean = model_class(\n            data, likelihood=likelihood, kernel=kernel, mean_function=non_zero_mean\n        )\n    elif model_class in [gpflow.models.SVGP]:\n        model_zero_mean = model_class(\n            kernel=kernel,\n            likelihood=likelihood,\n            inducing_variable=inducing_variable,\n            mean_function=zero_mean,\n        )\n        model_non_zero_mean = model_class(\n            kernel=kernel,\n            likelihood=likelihood,\n            inducing_variable=inducing_variable,\n            mean_function=non_zero_mean,\n        )\n    elif model_class in [gpflow.models.SGPR, gpflow.models.GPRFITC]:\n        model_zero_mean = model_class(\n            data, kernel=kernel, inducing_variable=inducing_variable, mean_function=zero_mean,\n        )\n        model_non_zero_mean = model_class(\n            data, kernel=kernel, inducing_variable=inducing_variable, mean_function=non_zero_mean,\n        )\n    elif model_class in [gpflow.models.SGPMC]:\n        model_zero_mean = model_class(\n            data,\n            kernel=kernel,\n            likelihood=likelihood,\n            inducing_variable=inducing_variable,\n            mean_function=zero_mean,\n        )\n        model_non_zero_mean = model_class(\n            data,\n            kernel=kernel,\n            likelihood=likelihood,\n            inducing_variable=inducing_variable,\n            mean_function=non_zero_mean,\n        )\n    elif model_class in [gpflow.models.GPMC]:\n        model_zero_mean = model_class(\n            data, kernel=kernel, likelihood=likelihood, mean_function=zero_mean\n        )\n        model_non_zero_mean = model_class(\n            data, kernel=kernel, likelihood=likelihood, mean_function=non_zero_mean\n        )\n    else:\n        raise (NotImplementedError)\n\n    mu_zero, var_zero = model_zero_mean.predict_f(Xnew)\n    mu_non_zero, var_non_zero = model_non_zero_mean.predict_f(Xnew)\n    # predictive variance remains unchanged after modifying mean function\n    assert np.all(var_zero.numpy() == var_non_zero.numpy())\n    # predictive mean changes after modifying mean function\n    assert not np.all(mu_zero.numpy() == mu_non_zero.numpy())\n'"
tests/gpflow/test_monitor.py,17,"b'import os\nfrom typing import List\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow.monitor import (\n    ExecuteCallback,\n    ImageToTensorBoard,\n    ModelToTensorBoard,\n    Monitor,\n    MonitorTaskGroup,\n    ScalarToTensorBoard,\n)\n\n\nclass Data:\n    num_data = 20\n    num_steps = 2\n\n\n@pytest.fixture\ndef model():\n    data = (\n        np.random.randn(Data.num_data, 2),  # [N, 2]\n        np.random.randn(Data.num_data, 2),  # [N, 1]\n    )\n    kernel = gpflow.kernels.SquaredExponential(lengthscales=[1.0, 2.0])\n    return gpflow.models.GPR(data, kernel, noise_variance=0.01)\n\n\n@pytest.fixture\ndef monitor(model, tmp_path):\n    tmp_path = str(tmp_path)\n\n    def lml_callback():\n        return model.log_marginal_likelihood()\n\n    def print_callback():\n        print(""foo"")\n\n    return Monitor(\n        MonitorTaskGroup(\n            [\n                ModelToTensorBoard(tmp_path, model),\n                ScalarToTensorBoard(tmp_path, lml_callback, ""lml""),\n            ],\n            period=2,\n        ),\n        MonitorTaskGroup(ExecuteCallback(print_callback), period=1),\n    )\n\n\ndef _get_size_directory(dir):\n    """"""Calculating the size of a directory (in Bytes).""""""\n    return sum(d.stat().st_size for d in os.scandir(dir) if d.is_file())\n\n\n# Smoke tests for the individual tasks\n# #####################################\n\n\ndef test_ExecuteCallback():\n    def callback():\n        print(""ExecuteCallback test"")\n\n    task = ExecuteCallback(callback)\n    task(0)\n    compiled_task = tf.function(task)\n    compiled_task(0)\n\n\ndef test_ImageToTensorBoard(tmp_path):\n    """"""Smoke test `ImageToTensorBoard` in Eager and Compiled mode.""""""\n    tmp_path = str(tmp_path)\n\n    def plotting_cb(fig, axes):\n        axes[0, 0].plot(np.random.randn(2), np.random.randn(2))\n        axes[1, 0].plot(np.random.randn(2), np.random.randn(2))\n        axes[0, 1].plot(np.random.randn(2), np.random.randn(2))\n        axes[1, 1].plot(np.random.randn(2), np.random.randn(2))\n\n    fig_kwargs = dict(figsize=(10, 10))\n    subplots_kwargs = dict(sharex=True, nrows=2, ncols=2)\n    task = ImageToTensorBoard(\n        tmp_path, plotting_cb, ""image"", fig_kw=fig_kwargs, subplots_kw=subplots_kwargs\n    )\n\n    task(0)\n    compiled_task = tf.function(task)\n    compiled_task(0)\n\n\ndef test_ScalarToTensorBoard(tmp_path):\n    """"""Smoke test `ScalarToTensorBoard` in Eager and Compiled mode.""""""\n    tmp_path = str(tmp_path)\n\n    def scalar_cb():\n        return 0.0\n\n    task = ScalarToTensorBoard(tmp_path, scalar_cb, ""scalar"")\n    task(0)\n    compiled_task = tf.function(task)\n    compiled_task(0)\n\n\ndef test_ScalarToTensorBoard_with_argument(tmp_path):\n    """"""Smoke test `ScalarToTensorBoard` in Eager and Compiled mode.""""""\n    tmp_path = str(tmp_path)\n\n    def scalar_cb(x=None):\n        return 2 * x\n\n    task = ScalarToTensorBoard(tmp_path, scalar_cb, ""scalar"")\n    compiled_task = tf.function(task)\n    task(0, x=1.0)\n    compiled_task(0, x=1.0)\n\n\ndef test_ScalarToTensorBoard_with_wrong_keyword_argument(tmp_path):\n    tmp_path = str(tmp_path)\n\n    def scalar_cb(x=None):\n        return 2 * x\n\n    task = ScalarToTensorBoard(tmp_path, scalar_cb, ""scalar"")\n    compiled_task = tf.function(task)\n\n    with pytest.raises(TypeError, match=r""got an unexpected keyword argument \'y\'""):\n        task(0, y=1.0)\n\n    with pytest.raises(TypeError, match=r""got an unexpected keyword argument \'y\'""):\n        compiled_task(0, y=1.0)\n\n\ndef test_ModelToTensorboard(model, tmp_path):\n    """"""Smoke test `ModelToTensorBoard` in Eager and Compiled mode.""""""\n    tmp_path = str(tmp_path)\n    task = ModelToTensorBoard(tmp_path, model)\n    task(0)\n    compiled_task = tf.function(task)\n    compiled_task(0)\n\n\ndef test_ExecuteCallback_arguments(capsys):\n    def cb1(x=None, **_):\n        assert x is not None\n        print(x)\n\n    def cb2(**_):\n        print(2)\n\n    def cb3(y=None, **_):\n        assert y is not None\n        print(y)\n\n    group1 = MonitorTaskGroup([ExecuteCallback(cb1), ExecuteCallback(cb2)])\n    group2 = MonitorTaskGroup(ExecuteCallback(cb3))\n    monitor = Monitor(group1, group2)\n    monitor(0, x=1, y=3)\n    out, _ = capsys.readouterr()\n    assert out == ""1\\n2\\n3\\n""\n\n\n# Smoke test Monitor and MonitorTaskGroup\n# ########################################\n\n\n@pytest.mark.parametrize(\n    ""task_or_tasks"",\n    [\n        ExecuteCallback(lambda: 0.0),\n        [ExecuteCallback(lambda: 0.0)],\n        [ExecuteCallback(lambda: 0.0), ExecuteCallback(lambda: 0.0)],\n    ],\n)\ndef test_MonitorTaskGroup_and_Monitor(task_or_tasks):\n    group = MonitorTaskGroup(task_or_tasks, period=2)\n\n    # check that the tasks is actually a list (custom setter)\n    isinstance(group.tasks, list)\n\n    # Smoke test the __call__\n    group(0)\n    compiled_group = tf.function(group)\n    compiled_group(0)\n\n    # Smoke test the Monitor wrapper\n    monitor = Monitor(group)\n    monitor(0)\n    compiled_monitor = tf.function(monitor)\n    compiled_monitor(0)\n\n\ndef test_Monitor(monitor):\n    monitor(0)\n    compiled_monitor = tf.function(monitor)\n    compiled_monitor(0)\n\n\n# Functionality tests\n# ###################\n\n\ndef test_compiled_execute_callable(capsys):\n    """"""\n    Test that the `ExecuteCallback` when compiled behaves as expected.\n    We test that python prints are not executed anymore.\n    """"""\n    string_to_print = ""Eager mode""\n\n    def callback():\n        print(string_to_print)\n\n    task = ExecuteCallback(callback)\n\n    # Eager mode\n    for i in range(Data.num_steps):\n        task(i)\n    out, _ = capsys.readouterr()\n\n    # We expect a print for each step\n    assert out == (f""{string_to_print}\\n"" * Data.num_steps)\n\n    # Autograph mode\n    compiled_task = tf.function(task)\n    for i in tf.range(Data.num_steps):\n        compiled_task(i)\n    out, _ = capsys.readouterr()\n    assert out == f""{string_to_print}\\n""\n\n\ndef test_periodicity_group(capsys):\n    """"""Test that groups are called at different periods.""""""\n\n    task_a = ExecuteCallback(lambda: print(""a"", end="" ""))\n    task_b = ExecuteCallback(lambda: print(""b"", end="" ""))\n    task_X = ExecuteCallback(lambda: print(""X"", end="" ""))\n\n    group_often = MonitorTaskGroup([task_a, task_b], period=1)\n    group_seldom = MonitorTaskGroup([task_X], period=3)\n    monitor = Monitor(group_often, group_seldom)\n    for i in range(7):\n        monitor(i)\n\n    out, _ = capsys.readouterr()\n    expected = ""a b X a b a b a b X a b a b a b X ""\n    assert out == expected\n\n    # AutoGraph mode\n    compiled_monitor = tf.function(monitor)\n    for i in tf.range(7):\n        compiled_monitor(i)\n\n    # When using TF\'s range and compiling the monitoring we only expected the python prints once.\n    out, _ = capsys.readouterr()\n    assert ""a b X""\n\n\ndef test_logdir_created(monitor, model, tmp_path):\n    """"""\n    Check that TensorFlow summaries are written.\n    """"""\n    tmp_path = str(tmp_path)\n\n    # check existence\n    assert os.path.exists(tmp_path) and os.path.isdir(tmp_path)\n    size_before = _get_size_directory(tmp_path)\n    assert size_before > 0\n\n    opt = tf.optimizers.Adam()\n    for step in range(Data.num_steps):\n        opt.minimize(model.training_loss, model.trainable_variables)\n        monitor(step)\n\n    size_after = _get_size_directory(tmp_path)\n    assert size_after > size_before\n\n\ndef test_compile_monitor(monitor, model):\n    opt = tf.optimizers.Adam()\n\n    @tf.function\n    def tf_func(step):\n        opt.minimize(model.training_loss, model.trainable_variables)\n        monitor(step)\n\n    for step in tf.range(100):\n        tf_func(step)\n'"
tests/gpflow/test_quadrature.py,3,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow.quadrature as quadrature\n\n\n@pytest.mark.parametrize(""mu"", [np.array([1.0, 1.3])])\n@pytest.mark.parametrize(""var"", [np.array([3.0, 3.5])])\ndef test_diagquad_1d(mu, var):\n    num_gauss_hermite_points = 25\n    quad = quadrature.ndiagquad([lambda *X: tf.exp(X[0])], num_gauss_hermite_points, [mu], [var])\n    expected = np.exp(mu + var / 2)\n    assert_allclose(quad[0], expected)\n\n\n@pytest.mark.parametrize(""mu1"", [np.array([1.0, 1.3])])\n@pytest.mark.parametrize(""var1"", [np.array([3.0, 3.5])])\n@pytest.mark.parametrize(""mu2"", [np.array([-2.0, 0.3])])\n@pytest.mark.parametrize(""var2"", [np.array([4.0, 4.2])])\ndef test_diagquad_2d(mu1, var1, mu2, var2):\n    alpha = 2.5\n    # using logspace=True we can reduce this, see test_diagquad_logspace\n    num_gauss_hermite_points = 35\n    quad = quadrature.ndiagquad(\n        lambda *X: tf.exp(X[0] + alpha * X[1]), num_gauss_hermite_points, [mu1, mu2], [var1, var2],\n    )\n    expected = np.exp(mu1 + var1 / 2 + alpha * mu2 + alpha ** 2 * var2 / 2)\n    assert_allclose(quad, expected)\n\n\n@pytest.mark.parametrize(""mu1"", [np.array([1.0, 1.3])])\n@pytest.mark.parametrize(""var1"", [np.array([3.0, 3.5])])\n@pytest.mark.parametrize(""mu2"", [np.array([-2.0, 0.3])])\n@pytest.mark.parametrize(""var2"", [np.array([4.0, 4.2])])\ndef test_diagquad_logspace(mu1, var1, mu2, var2):\n    alpha = 2.5\n    num_gauss_hermite_points = 25\n    quad = quadrature.ndiagquad(\n        lambda *X: (X[0] + alpha * X[1]),\n        num_gauss_hermite_points,\n        [mu1, mu2],\n        [var1, var2],\n        logspace=True,\n    )\n    expected = mu1 + var1 / 2 + alpha * mu2 + alpha ** 2 * var2 / 2\n    assert_allclose(quad, expected)\n\n\n@pytest.mark.parametrize(""mu1"", [np.array([1.0, 1.3])])\n@pytest.mark.parametrize(""var1"", [np.array([3.0, 3.5])])\ndef test_diagquad_with_kwarg(mu1, var1):\n    alpha = np.array([2.5, -1.3])\n    num_gauss_hermite_points = 25\n    quad = quadrature.ndiagquad(\n        lambda X, Y: tf.exp(X * Y), num_gauss_hermite_points, mu1, var1, Y=alpha\n    )\n    expected = np.exp(alpha * mu1 + alpha ** 2 * var1 / 2)\n    assert_allclose(quad, expected)\n'"
tests/integration/__init__.py,0,b''
tests/integration/test_dynamic_shapes.py,6,"b'# Copyright 2019 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.config import default_jitter, default_float\nfrom gpflow.mean_functions import Constant\n\nrng = np.random.RandomState(0)\n\n\nclass Datum:\n    X = rng.rand(20, 1) * 10\n    Y = np.sin(X) + 0.9 * np.cos(X * 1.6) + rng.randn(*X.shape) * 0.8\n    Y = np.tile(Y, 2)  # two identical columns\n    Xtest = rng.rand(10, 1) * 10\n    data = (X, Y)\n\n    # for classification:\n    Yc = Y[:, :1]\n    cdata = (X, Yc)\n\n\n@pytest.mark.parametrize(""whiten"", [True, False])\n@pytest.mark.parametrize(""q_diag"", [True, False])\ndef test_svgp(whiten, q_diag):\n    model = gpflow.models.SVGP(\n        gpflow.kernels.SquaredExponential(),\n        gpflow.likelihoods.Gaussian(),\n        inducing_variable=Datum.X.copy(),\n        q_diag=q_diag,\n        whiten=whiten,\n        mean_function=gpflow.mean_functions.Constant(),\n        num_latent_gps=Datum.Y.shape[1],\n    )\n    gpflow.set_trainable(model.inducing_variable, False)\n\n    # test with explicitly unknown shapes:\n    tensor_spec = tf.TensorSpec(shape=None, dtype=default_float())\n    elbo = tf.function(model.elbo, input_signature=[(tensor_spec, tensor_spec)],)\n\n    @tf.function\n    def model_closure():\n        return -elbo(Datum.data)\n\n    opt = gpflow.optimizers.Scipy()\n\n    # simply test whether it runs without erroring...:\n    opt.minimize(\n        model_closure, variables=model.trainable_variables, options=dict(maxiter=3), compile=True,\n    )\n\n\ndef test_multiclass():\n    num_classes = 3\n    model = gpflow.models.SVGP(\n        gpflow.kernels.SquaredExponential(),\n        gpflow.likelihoods.MultiClass(num_classes=num_classes),\n        inducing_variable=Datum.X.copy(),\n        num_latent_gps=num_classes,\n    )\n    gpflow.set_trainable(model.inducing_variable, False)\n\n    # test with explicitly unknown shapes:\n    tensor_spec = tf.TensorSpec(shape=None, dtype=default_float())\n    elbo = tf.function(model.elbo, input_signature=[(tensor_spec, tensor_spec)],)\n\n    @tf.function\n    def model_closure():\n        return -elbo(Datum.cdata)\n\n    opt = gpflow.optimizers.Scipy()\n\n    # simply test whether it runs without erroring...:\n    opt.minimize(\n        model_closure, variables=model.trainable_variables, options=dict(maxiter=3), compile=True,\n    )\n'"
tests/integration/test_method_equivalence.py,0,"b'# Copyright 2019 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.config import default_jitter\nfrom gpflow.mean_functions import Constant\nfrom gpflow.models import maximum_log_likelihood_objective\n\nrng = np.random.RandomState(0)\n\n\nclass Datum:\n    X = rng.rand(20, 1) * 10\n    Y = np.sin(X) + 0.9 * np.cos(X * 1.6) + rng.randn(*X.shape) * 0.8\n    Y = np.tile(Y, 2)  # two identical columns\n    Xtest = rng.rand(10, 1) * 10\n    data = (X, Y)\n\n\nclass DatumVGP:\n    N, Ns, DX, DY = 100, 10, 2, 2\n    np.random.seed(1)\n    X = np.random.randn(N, DX)\n    Xs = np.random.randn(Ns, DX)\n    Y = np.random.randn(N, DY)\n    q_mu = np.random.randn(N, DY)\n    q_sqrt = np.random.randn(DY, N, N)\n    q_alpha = np.random.randn(N, DX)\n    q_lambda = np.random.randn(N, DX) ** 2\n    data = (X, Y)\n\n\nclass DatumUpper:\n    X = np.random.rand(100, 1)\n    Y = np.sin(1.5 * 2 * np.pi * X) + np.random.randn(*X.shape) * 0.1\n    data = (X, Y)\n\n\ndef _create_full_gp_model():\n    """"""\n    GP Regression\n    """"""\n    full_gp_model = gpflow.models.GPR(\n        (Datum.X, Datum.Y),\n        kernel=gpflow.kernels.SquaredExponential(),\n        mean_function=gpflow.mean_functions.Constant(),\n    )\n\n    opt = gpflow.optimizers.Scipy()\n    opt.minimize(\n        full_gp_model.training_loss,\n        variables=full_gp_model.trainable_variables,\n        options=dict(maxiter=300),\n    )\n    return full_gp_model\n\n\ndef _create_approximate_models():\n    """"""\n    1) Variational GP (with the likelihood set to Gaussian)\n    2) Sparse variational GP (likelihood is Gaussian, inducing points\n       at the data)\n    3) Sparse variational GP (as above, but with the whitening rotation\n       of the inducing variables)\n    4) Sparse variational GP Regression (as above, but there the inducing\n       variables are \'collapsed\' out, as in Titsias 2009)\n    5) FITC Sparse GP Regression\n    """"""\n    model_1 = gpflow.models.VGP(\n        (Datum.X, Datum.Y),\n        gpflow.kernels.SquaredExponential(),\n        likelihood=gpflow.likelihoods.Gaussian(),\n        mean_function=gpflow.mean_functions.Constant(),\n    )\n    model_2 = gpflow.models.SVGP(\n        gpflow.kernels.SquaredExponential(),\n        gpflow.likelihoods.Gaussian(),\n        inducing_variable=Datum.X.copy(),\n        q_diag=False,\n        mean_function=gpflow.mean_functions.Constant(),\n        num_latent_gps=Datum.Y.shape[1],\n    )\n    gpflow.set_trainable(model_2.inducing_variable, False)\n    model_3 = gpflow.models.SVGP(\n        kernel=gpflow.kernels.SquaredExponential(),\n        likelihood=gpflow.likelihoods.Gaussian(),\n        inducing_variable=Datum.X.copy(),\n        q_diag=False,\n        whiten=True,\n        mean_function=gpflow.mean_functions.Constant(),\n        num_latent_gps=Datum.Y.shape[1],\n    )\n    gpflow.set_trainable(model_3.inducing_variable, False)\n    model_4 = gpflow.models.GPRFITC(\n        (Datum.X, Datum.Y),\n        kernel=gpflow.kernels.SquaredExponential(),\n        inducing_variable=Datum.X.copy(),\n        mean_function=Constant(),\n    )\n    gpflow.set_trainable(model_4.inducing_variable, False)\n    model_5 = gpflow.models.SGPR(\n        (Datum.X, Datum.Y),\n        gpflow.kernels.SquaredExponential(),\n        inducing_variable=Datum.X.copy(),\n        mean_function=Constant(),\n    )\n    gpflow.set_trainable(model_5.inducing_variable, False)\n\n    # Train models\n\n    opt = gpflow.optimizers.Scipy()\n\n    opt.minimize(\n        model_1.training_loss, variables=model_1.trainable_variables, options=dict(maxiter=300),\n    )\n    opt.minimize(\n        model_2.training_loss_closure(Datum.data),\n        variables=model_2.trainable_variables,\n        options=dict(maxiter=300),\n    )\n    opt.minimize(\n        model_3.training_loss_closure(Datum.data),\n        variables=model_3.trainable_variables,\n        options=dict(maxiter=300),\n    )\n    opt.minimize(\n        model_4.training_loss, variables=model_4.trainable_variables, options=dict(maxiter=300),\n    )\n    opt.minimize(\n        model_5.training_loss, variables=model_5.trainable_variables, options=dict(maxiter=300),\n    )\n\n    return model_1, model_2, model_3, model_4, model_5\n\n\ndef _create_vgp_model(kernel, likelihood, q_mu=None, q_sqrt=None):\n    model_vgp = gpflow.models.VGP((DatumVGP.X, DatumVGP.Y), kernel, likelihood)\n    if q_mu is not None and q_sqrt is not None:\n        model_vgp.q_mu.assign(q_mu)\n        model_vgp.q_sqrt.assign(q_sqrt)\n    return model_vgp\n\n\ndef _create_vgpao_model(kernel, likelihood, q_alpha, q_lambda):\n    model_vgpoa = gpflow.models.VGPOpperArchambeau(\n        (DatumVGP.X, DatumVGP.Y), kernel, likelihood, num_latent_gps=DatumVGP.DY\n    )\n    model_vgpoa.q_alpha.assign(q_alpha)\n    model_vgpoa.q_lambda.assign(q_lambda)\n\n    return model_vgpoa\n\n\ndef _create_svgp_model(kernel, likelihood, q_mu, q_sqrt, whiten):\n    model_svgp = gpflow.models.SVGP(\n        kernel,\n        likelihood,\n        DatumVGP.X.copy(),\n        whiten=whiten,\n        q_diag=False,\n        num_latent_gps=DatumVGP.DY,\n    )\n    model_svgp.q_mu.assign(q_mu)\n    model_svgp.q_sqrt.assign(q_sqrt)\n    return model_svgp\n\n\n@pytest.mark.parametrize(""approximate_model"", _create_approximate_models())\ndef test_equivalence(approximate_model):\n    """"""\n    With a Gaussian likelihood, and inducing points (where appropriate)\n    positioned at the data, many of the gpflow methods are equivalent (perhaps\n    subject to some optimization).\n    """"""\n    gpr_model = _create_full_gp_model()\n    gpr_likelihood = gpr_model.log_marginal_likelihood()\n    approximate_likelihood = maximum_log_likelihood_objective(approximate_model, Datum.data)\n    assert_allclose(approximate_likelihood, gpr_likelihood, rtol=1e-6)\n\n    gpr_kernel_ls = gpr_model.kernel.lengthscales.read_value()\n    gpr_kernel_var = gpr_model.kernel.variance.read_value()\n\n    approximate_kernel_ls = approximate_model.kernel.lengthscales.read_value()\n    approximate_kernel_var = approximate_model.kernel.variance.read_value()\n\n    assert_allclose(gpr_kernel_ls, approximate_kernel_ls, 1e-4)\n    assert_allclose(gpr_kernel_var, approximate_kernel_var, 1e-3)\n\n    gpr_mu, gpr_var = gpr_model.predict_y(Datum.Xtest)\n    approximate_mu, approximate_var = approximate_model.predict_y(Datum.Xtest)\n\n    assert_allclose(gpr_mu, approximate_mu, 1e-3)\n    assert_allclose(gpr_var, approximate_var, 1e-4)\n\n\ndef test_equivalence_vgp_and_svgp():\n    kernel = gpflow.kernels.Matern52()\n    likelihood = gpflow.likelihoods.StudentT()\n\n    svgp_model = _create_svgp_model(kernel, likelihood, DatumVGP.q_mu, DatumVGP.q_sqrt, whiten=True)\n    vgp_model = _create_vgp_model(kernel, likelihood, DatumVGP.q_mu, DatumVGP.q_sqrt)\n\n    likelihood_svgp = svgp_model.elbo(DatumVGP.data)\n    likelihood_vgp = vgp_model.elbo()\n    assert_allclose(likelihood_svgp, likelihood_vgp, rtol=1e-2)\n\n    svgp_mu, svgp_var = svgp_model.predict_f(DatumVGP.Xs)\n    vgp_mu, vgp_var = vgp_model.predict_f(DatumVGP.Xs)\n\n    assert_allclose(svgp_mu, vgp_mu)\n    assert_allclose(svgp_var, vgp_var)\n\n\ndef test_equivalence_vgp_and_opper_archambeau():\n    kernel = gpflow.kernels.Matern52()\n    likelihood = gpflow.likelihoods.StudentT()\n\n    vgp_oa_model = _create_vgpao_model(kernel, likelihood, DatumVGP.q_alpha, DatumVGP.q_lambda)\n\n    K = kernel(DatumVGP.X) + np.eye(DatumVGP.N) * default_jitter()\n    L = np.linalg.cholesky(K)\n    L_inv = np.linalg.inv(L)\n    K_inv = np.linalg.inv(K)\n\n    mean = K @ DatumVGP.q_alpha\n\n    prec_dnn = K_inv[None, :, :] + np.array([np.diag(l ** 2) for l in DatumVGP.q_lambda.T])\n    var_dnn = np.linalg.inv(prec_dnn)\n\n    svgp_model_unwhitened = _create_svgp_model(\n        kernel, likelihood, mean, np.linalg.cholesky(var_dnn), whiten=False\n    )\n\n    mean_white_nd = L_inv.dot(mean)\n    var_white_dnn = np.einsum(""nN,dNM,mM->dnm"", L_inv, var_dnn, L_inv)\n    q_sqrt_nnd = np.linalg.cholesky(var_white_dnn)\n\n    vgp_model = _create_vgp_model(kernel, likelihood, mean_white_nd, q_sqrt_nnd)\n\n    likelihood_vgp = vgp_model.elbo()\n    likelihood_vgp_oa = vgp_oa_model.elbo()\n    likelihood_svgp_unwhitened = svgp_model_unwhitened.elbo(DatumVGP.data)\n\n    assert_allclose(likelihood_vgp, likelihood_vgp_oa, rtol=1e-2)\n    assert_allclose(likelihood_vgp, likelihood_svgp_unwhitened, rtol=1e-2)\n\n    vgp_oa_mu, vgp_oa_var = vgp_oa_model.predict_f(DatumVGP.Xs)\n    svgp_unwhitened_mu, svgp_unwhitened_var = svgp_model_unwhitened.predict_f(DatumVGP.Xs)\n    vgp_mu, vgp_var = vgp_model.predict_f(DatumVGP.Xs)\n\n    assert_allclose(vgp_oa_mu, vgp_mu)\n    assert_allclose(vgp_oa_var, vgp_var, rtol=1e-4)  # jitter?\n    assert_allclose(svgp_unwhitened_mu, vgp_mu)\n    assert_allclose(svgp_unwhitened_var, vgp_var, rtol=1e-4)\n\n\ndef test_upper_bound_few_inducing_points():\n    """"""\n    Test for upper bound for regression marginal likelihood\n    """"""\n    model_vfe = gpflow.models.SGPR(\n        (DatumUpper.X, DatumUpper.Y),\n        gpflow.kernels.SquaredExponential(),\n        inducing_variable=DatumUpper.X[:10, :].copy(),\n        mean_function=Constant(),\n    )\n    opt = gpflow.optimizers.Scipy()\n\n    opt.minimize(\n        model_vfe.training_loss, variables=model_vfe.trainable_variables, options=dict(maxiter=500),\n    )\n\n    full_gp = gpflow.models.GPR(\n        (DatumUpper.X, DatumUpper.Y),\n        kernel=gpflow.kernels.SquaredExponential(),\n        mean_function=Constant(),\n    )\n    full_gp.kernel.lengthscales.assign(model_vfe.kernel.lengthscales)\n    full_gp.kernel.variance.assign(model_vfe.kernel.variance)\n    full_gp.likelihood.variance.assign(model_vfe.likelihood.variance)\n    full_gp.mean_function.c.assign(model_vfe.mean_function.c)\n\n    lml_upper = model_vfe.upper_bound()\n    lml_vfe = model_vfe.elbo()\n    lml_full_gp = full_gp.log_marginal_likelihood()\n\n    assert lml_vfe < lml_full_gp\n    assert lml_full_gp < lml_upper\n'"
tests/integration/test_notebooks.py,0,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport glob\nimport itertools\nimport os\nimport sys\nimport traceback\n\nimport jupytext\nimport nbformat\nimport pytest\nfrom nbconvert.preprocessors import ExecutePreprocessor\nfrom nbconvert.preprocessors.execute import CellExecutionError\n\n\nNOTEBOOK_DIR = ""../../doc/source/notebooks""\n\n\ndef _nbpath():\n    this_dir = os.path.dirname(__file__)\n    return os.path.join(this_dir, NOTEBOOK_DIR)\n\n\ndef test_notebook_dir_exists():\n    assert os.path.isdir(_nbpath())\n\n\n# To blacklist a notebook, add its full base name (including .pct.py or .md\n# extension, but without any directory component). NOTE that if there are\n# several notebooks in different directories with the same base name, they will\n# all get blacklisted (change the blacklisting check to something else in that\n# case, if need be!)\nBLACKLISTED_NOTEBOOKS = []\n\n\ndef get_notebooks():\n    """"""\n    Returns all notebooks in `_nbpath` that are not blacklisted.\n    """"""\n\n    def notebook_blacklisted(nb):\n        blacklisted_notebooks_basename = map(os.path.basename, BLACKLISTED_NOTEBOOKS)\n        return os.path.basename(nb) in blacklisted_notebooks_basename\n\n    # recursively traverse the notebook directory in search for ipython notebooks\n    all_py_notebooks = glob.iglob(os.path.join(_nbpath(), ""**"", ""*.pct.py""), recursive=True)\n    all_md_notebooks = glob.iglob(os.path.join(_nbpath(), ""**"", ""*.md""), recursive=True)\n    all_notebooks = itertools.chain(all_md_notebooks, all_py_notebooks)\n    notebooks_to_test = [nb for nb in all_notebooks if not notebook_blacklisted(nb)]\n    return notebooks_to_test\n\n\ndef _preproc():\n    pythonkernel = ""python"" + str(sys.version_info[0])\n    return ExecutePreprocessor(timeout=300, kernel_name=pythonkernel, interrupt_on_timeout=True)\n\n\ndef _exec_notebook(notebook_filename):\n    with open(notebook_filename) as notebook_file:\n        nb = jupytext.read(notebook_file, as_version=nbformat.current_nbformat)\n        try:\n            meta_data = {""path"": os.path.dirname(notebook_filename)}\n            _preproc().preprocess(nb, {""metadata"": meta_data})\n        except CellExecutionError as cell_error:\n            traceback.print_exc(file=sys.stdout)\n            msg = ""Error executing the notebook {0}. See above for error.\\nCell error: {1}""\n            pytest.fail(msg.format(notebook_filename, str(cell_error)))\n\n\n@pytest.mark.notebooks\n@pytest.mark.parametrize(""notebook_file"", get_notebooks())\ndef test_notebook(notebook_file):\n    _exec_notebook(notebook_file)\n\n\ndef test_has_notebooks():\n    assert len(get_notebooks()) >= 35, ""there are probably some notebooks that were not discovered""\n'"
doc/source/notebooks/intro_to_gpflow2.pct.py,49,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.4.2\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# GPflow with TensorFlow 2\n# ===\n#\n# ##### Small steps big changes\n#\n# <br>\n#\n#\n\n# %%\nfrom typing import Tuple, Optional\nimport tempfile\nimport pathlib\n\nimport datetime\nimport io\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport tensorflow as tf\nimport gpflow\n\nfrom gpflow.config import default_float\nfrom gpflow.ci_utils import ci_niter\nfrom gpflow.utilities import to_default_float\n\nimport warnings\n\nwarnings.filterwarnings(""ignore"")\n\n# %% [markdown]\n# Make `tensorboard` work inside notebook:\n\n# %%\noutput_logdir = ""/tmp/tensorboard""\n\n# !rm -rf ""{output_logdir}""\n# !mkdir ""{output_logdir}""\n\n# %load_ext tensorboard\n# %matplotlib inline\n\n\ndef enumerated_logdir(_logdir_id: int = [0]):\n    logdir = pathlib.Path(output_logdir, str(_logdir_id[0]))\n    _logdir_id[0] += 1\n    return str(logdir)\n\n\n# %% [markdown]\n# Set up random seeds and default float for `gpflow` tensors:\n\n# %%\ngpflow.config.set_default_float(np.float64)\nnp.random.seed(0)\ntf.random.set_seed(0)\n\n\n# %% [markdown]\n# ## Loading data using TensorFlow Datasets\n#\n# For this example, we create a synthetic dataset (noisy sine function):\n\n# %%\ndef noisy_sin(x):\n    return tf.math.sin(x) + 0.1 * tf.random.normal(x.shape, dtype=default_float())\n\n\nnum_train_data, num_test_data = 100, 500\n\nX = tf.random.uniform((num_train_data, 1), dtype=default_float()) * 10\nXtest = tf.random.uniform((num_test_data, 1), dtype=default_float()) * 10\n\nY = noisy_sin(X)\nYtest = noisy_sin(Xtest)\n\ndata = (X, Y)\n\nplt.plot(X, Y, ""xk"")\nplt.show()\n\n# %% [markdown]\n# Working with TensorFlow Datasets is an efficient way to rapidly shuffle, iterate, and batch from data. For `prefetch` size we use `tf.data.experimental.AUTOTUNE` as recommended by TensorFlow [guidelines](https://www.tensorflow.org/guide/data_performance).\n\n# %%\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X, Y))\ntest_dataset = tf.data.Dataset.from_tensor_slices((Xtest, Ytest))\n\nbatch_size = 32\nnum_features = 10\nprefetch_size = tf.data.experimental.AUTOTUNE\nshuffle_buffer_size = num_train_data // 2\nnum_batches_per_epoch = num_train_data // batch_size\n\noriginal_train_dataset = train_dataset\ntrain_dataset = (\n    train_dataset.repeat()\n    .prefetch(prefetch_size)\n    .shuffle(buffer_size=shuffle_buffer_size)\n    .batch(batch_size)\n)\n\nprint(f""prefetch_size={prefetch_size}"")\nprint(f""shuffle_buffer_size={shuffle_buffer_size}"")\nprint(f""num_batches_per_epoch={num_batches_per_epoch}"")\n\n# %% [markdown]\n# ## Define a GP model\n#\n# In GPflow 2.0, we use `tf.Module` (or the very thin `gpflow.base.Module` wrapper) to build all our models, as well as their components (kernels, likelihoods, parameters, and so on).\n\n# %%\nkernel = gpflow.kernels.SquaredExponential(variance=2.0)\nlikelihood = gpflow.likelihoods.Gaussian()\ninducing_variable = np.linspace(0, 10, num_features).reshape(-1, 1)\n\nmodel = gpflow.models.SVGP(\n    kernel=kernel, likelihood=likelihood, inducing_variable=inducing_variable\n)\n\n# %% [markdown]\n# You can set a module (or a particular parameter) to be non-trainable using the auxiliary method ```set_trainable(module, False)```:\n\n# %%\nfrom gpflow import set_trainable\n\nset_trainable(likelihood, False)\nset_trainable(kernel.variance, False)\n\nset_trainable(likelihood, True)\nset_trainable(kernel.variance, True)\n\n# %% [markdown]\n# We can use ```param.assign(value)``` to assign a value to a parameter:\n\n# %%\nkernel.lengthscales.assign(0.5)\n\n# %% [markdown]\n# All these changes are reflected when we use ```print_summary(model)``` to print a detailed summary of the model. By default the output is displayed in a minimalistic and simple table.\n\n# %%\nfrom gpflow.utilities import print_summary\n\nprint_summary(model)  # same as print_summary(model, fmt=""fancy_table"")\n\n# %% [markdown]\n# We can change default printing so that it will look nicer in our notebook:\n\n# %%\ngpflow.config.set_default_summary_fmt(""notebook"")\n\nprint_summary(model)  # same as print_summary(model, fmt=""notebook"")\n\n# %% [markdown]\n# Jupyter notebooks also format GPflow classes (that are subclasses of `gpflow.base.Module`) in the same nice way when at the end of a cell (this is independent of the `default_summary_fmt`):\n\n# %%\nmodel\n\n# %% [markdown]\n# ## Training using training_loss and training_loss_closure\n#\n# GPflow models come with training_loss and training_loss_closure methods to make it easy to train your models.\n# There is a slight difference between models that own their own data (most of them, e.g. GPR, VGP, ...) and models that do not own the data (SVGP).\n#\n# ### Model-internal data\n# For models that own their own data (inheriting from InternalDataTrainingLossMixin), data is provided at model construction time.\n# In this case, model.training_loss does not take any arguments, and can be directly passed to an optimizer\'s `minimize()` method:\n\n# %%\nvgp_model = gpflow.models.VGP(data, kernel, likelihood)\noptimizer = tf.optimizers.Adam()\noptimizer.minimize(\n    vgp_model.training_loss, vgp_model.trainable_variables\n)  # Note: this does a single step\n# In practice, you will need to call minimize() many times, this will be further discussed below.\n\n# %% [markdown]\n# This also works for the Scipy optimizer, though it will do the full optimization on a single call to minimize():\n\n# %%\noptimizer = gpflow.optimizers.Scipy()\noptimizer.minimize(\n    vgp_model.training_loss, vgp_model.trainable_variables, options=dict(maxiter=ci_niter(1000))\n)\n\n# %% [markdown]\n# You can obtain a compiled version using training_loss_closure, whose `compile` argument is True by default:\n\n# %%\nvgp_model.training_loss_closure()  # compiled\nvgp_model.training_loss_closure(compile=True)  # compiled\nvgp_model.training_loss_closure(compile=False)  # uncompiled, same as vgp_model.training_loss\n\n# %% [markdown]\n# The SVGP model inherits from ExternalDataTrainingLossMixin and expects the data to be passed to training_loss().\n# For SVGP as for the other regression models, `data` is a two-tuple of `(X, Y)`, where `X` is an array/tensor with shape `(num_data, input_dim)` and `Y` is an array/tensor with shape `(num_data, output_dim)`:\n\n# %%\nassert isinstance(model, gpflow.models.SVGP)\nmodel.training_loss(data)\n\n# %% [markdown]\n# To make optimizing it easy, it has a `training_loss_closure()` method, that takes the data and returns a closure that computes the training loss on this data:\n\n# %%\noptimizer = tf.optimizers.Adam()\ntraining_loss = model.training_loss_closure(\n    data\n)  # We save the compiled closure in a variable so as not to re-compile it each step\noptimizer.minimize(training_loss, model.trainable_variables)  # Note that this does a single step\n\n# %% [markdown]\n# SVGP can handle mini-batching, and an iterator from a batched tf.data.Dataset can be passed to the model\'s training_loss_closure():\n\n# %%\nbatch_size = 5\nbatched_dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\ntraining_loss = model.training_loss_closure(iter(batched_dataset))\n\noptimizer.minimize(training_loss, model.trainable_variables)  # Note that this does a single step\n\n# %% [markdown]\n# As previously, training_loss_closure takes an optional `compile` argument for tf.function compilation (True by default).\n\n# %% [markdown]\n# ## Training using Gradient Tapes\n#\n# For a more elaborate example of a gradient update we can define an `optimization_step` that explicitly computes and applies gradients to the model.\n# In TensorFlow 2, we can optimize (trainable) model parameters with TensorFlow optimizers using `tf.GradientTape`. In this simple example, we perform one gradient update of the Adam optimizer to minimize the training_loss (in this case the negative ELBO) of our model.\n# The `optimization_step` can (and should) be wrapped in `tf.function` to be compiled to a graph if executing it many times.\n\n# %%\ndef optimization_step(model: gpflow.models.SVGP, batch: Tuple[tf.Tensor, tf.Tensor]):\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(model.trainable_variables)\n        loss = model.training_loss(batch)\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    return loss\n\n\n# %% [markdown]\n# We can use the functionality of TensorFlow Datasets to define a simple training loop that iterates over batches of the training dataset:\n\n# %%\ndef simple_training_loop(model: gpflow.models.SVGP, epochs: int = 1, logging_epoch_freq: int = 10):\n    tf_optimization_step = tf.function(optimization_step)\n\n    batches = iter(train_dataset)\n    for epoch in range(epochs):\n        for _ in range(ci_niter(num_batches_per_epoch)):\n            tf_optimization_step(model, next(batches))\n\n        epoch_id = epoch + 1\n        if epoch_id % logging_epoch_freq == 0:\n            tf.print(f""Epoch {epoch_id}: ELBO (train) {model.elbo(data)}"")\n\n\n# %%\nsimple_training_loop(model, epochs=10, logging_epoch_freq=2)\n\n# %% [markdown]\n# ## Monitoring\n#\n# `gpflow.monitor` provides a thin wrapper on top of tf.summary that makes it easy to monitor the training procedure.\n# For a more detailed tutorial see the [monitoring notebook](./basics/monitoring.pct.py).\n\n# %%\nfrom gpflow.monitor import (\n    ImageToTensorBoard,\n    ModelToTensorBoard,\n    ExecuteCallback,\n    Monitor,\n    MonitorTaskGroup,\n    ScalarToTensorBoard,\n)\n\n\nsamples_input = np.linspace(0, 10, 100).reshape(-1, 1)\n\n\ndef plot_model(fig, ax):\n    tf.print(""Plotting..."")\n    mean, var = model.predict_f(samples_input)\n    num_samples = 10\n    samples = model.predict_f_samples(samples_input, num_samples)\n    ax.plot(samples_input, mean, ""C0"", lw=2)\n    ax.fill_between(\n        samples_input[:, 0],\n        mean[:, 0] - 1.96 * np.sqrt(var[:, 0]),\n        mean[:, 0] + 1.96 * np.sqrt(var[:, 0]),\n        color=""C0"",\n        alpha=0.2,\n    )\n    ax.plot(X, Y, ""kx"")\n    ax.plot(samples_input, samples[:, :, 0].numpy().T, ""C0"", linewidth=0.5)\n    ax.set_ylim(-2.0, +2.0)\n    ax.set_xlim(0, 10)\n\n\ndef print_cb(epoch_id=None, data=None):\n    tf.print(f""Epoch {epoch_id}: ELBO (train)"", model.elbo(data))\n\n\ndef elbo_cb(data=None, **_):\n    return model.elbo(data)\n\n\noutput_logdir = enumerated_logdir()\n\nmodel_task = ModelToTensorBoard(output_logdir, model)\nelbo_task = ScalarToTensorBoard(output_logdir, elbo_cb, ""elbo"")\nprint_task = ExecuteCallback(callback=print_cb)\n\n# We group these tasks and specify a period of `100` steps for them\nfast_tasks = MonitorTaskGroup([model_task, elbo_task, print_task], period=100)\n\n# We also want to see the model\'s fit during the optimisation\nimage_task = ImageToTensorBoard(output_logdir, plot_model, ""samples_image"")\n\n# We typically don\'t want to plot too frequently during optimisation,\n# which is why we specify a larger period for this task.\nslow_taks = MonitorTaskGroup(image_task, period=500)\nmonitor = Monitor(fast_tasks, slow_taks)\n\n\ndef monitored_training_loop(epochs: int):\n    tf_optimization_step = tf.function(optimization_step)\n\n    batches = iter(train_dataset)\n\n    for epoch in range(epochs):\n        for _ in range(ci_niter(num_batches_per_epoch)):\n            batch = next(batches)\n            tf_optimization_step(model, batch)\n\n        epoch_id = epoch + 1\n        monitor(epoch, epoch_id=epoch_id, data=data)\n\n\n# %% [markdown]\n# NOTE: for optimal performance it is recommended to wrap the monitoring inside `tf.function`.\n# This is detailed in the [monitoring notebook](./basics/monitoring.ipynb).\n\n\n# %%\nmodel = gpflow.models.SVGP(\n    kernel=kernel, likelihood=likelihood, inducing_variable=inducing_variable\n)\n\nmonitored_training_loop(epochs=1000)\n\n# %% [markdown]\n# Then, we can use TensorBoard to examine the training procedure in more detail\n\n# %%\n# # %tensorboard --logdir ""{output_logdir}""\n\n# %% [markdown]\n# ## Saving and loading models\n#\n# ### Checkpointing\n#\n# With the help of `tf.train.CheckpointManager` and `tf.train.Checkpoint`, we can checkpoint the model throughout the training procedure. Let\'s start with a simple example using checkpointing to save and load a `tf.Variable`:\n\n# %%\ninitial_value = 1.2\na = tf.Variable(initial_value)\n\n# %% [markdown]\n# Create `Checkpoint` object:\n\n# %%\nckpt = tf.train.Checkpoint(a=a)\nmanager = tf.train.CheckpointManager(ckpt, output_logdir, max_to_keep=3)\n\n# %% [markdown]\n# Save the variable `a` and change its value right after:\n\n# %%\nmanager.save()\n_ = a.assign(0.33)\n\n# %% [markdown]\n# Now we can restore the old variable value:\n\n# %%\nprint(f""Current value of variable a: {a.numpy():0.3f}"")\n\nckpt.restore(manager.latest_checkpoint)\n\nprint(f""Value of variable a after restore: {a.numpy():0.3f}"")\n\n# %% [markdown]\n# In the example below, we modify a simple training loop to save the model every 100 epochs using the `CheckpointManager`.\n\n# %%\nmodel = gpflow.models.SVGP(\n    kernel=kernel, likelihood=likelihood, inducing_variable=inducing_variable\n)\n\n\ndef checkpointing_training_loop(\n    model: gpflow.models.SVGP,\n    batch_size: int,\n    epochs: int,\n    manager: tf.train.CheckpointManager,\n    logging_epoch_freq: int = 100,\n    epoch_var: Optional[tf.Variable] = None,\n    step_var: Optional[tf.Variable] = None,\n):\n    tf_optimization_step = tf.function(optimization_step)\n\n    batches = iter(train_dataset)\n\n    for epoch in range(epochs):\n        for step in range(ci_niter(num_batches_per_epoch)):\n            tf_optimization_step(model, next(batches))\n            if step_var is not None:\n                step_var.assign(epoch * num_batches_per_epoch + step + 1)\n        if epoch_var is not None:\n            epoch_var.assign(epoch + 1)\n\n        epoch_id = epoch + 1\n        if epoch_id % logging_epoch_freq == 0:\n            ckpt_path = manager.save()\n            tf.print(f""Epoch {epoch_id}: ELBO (train) {model.elbo(data)}, saved at {ckpt_path}"")\n\n\n# %%\nstep_var = tf.Variable(1, dtype=tf.int32, trainable=False)\nepoch_var = tf.Variable(1, dtype=tf.int32, trainable=False)\nckpt = tf.train.Checkpoint(model=model, step=step_var, epoch=epoch_var)\nmanager = tf.train.CheckpointManager(ckpt, output_logdir, max_to_keep=5)\n\nprint(f""Checkpoint folder path at: {output_logdir}"")\n\ncheckpointing_training_loop(\n    model,\n    batch_size=batch_size,\n    epochs=1000,\n    manager=manager,\n    epoch_var=epoch_var,\n    step_var=step_var,\n)\n\n# %% [markdown]\n# After the models have been saved, we can restore them using ```tf.train.Checkpoint.restore``` and assert that their performance corresponds to that logged during training.\n\n# %%\nfor i, recorded_checkpoint in enumerate(manager.checkpoints):\n    ckpt.restore(recorded_checkpoint)\n    print(\n        f""{i} restored model from epoch {int(epoch_var)} [step:{int(step_var)}] : ELBO training set {model.elbo(data)}""\n    )\n\n# %% [markdown]\n# ## Copying (hyper)parameter values between models\n#\n# It is easy to interact with the set of all parameters of a model or a subcomponent programmatically.\n#\n# The following returns a dictionary of all parameters within\n\n# %%\nmodel = gpflow.models.SGPR(data, kernel=kernel, inducing_variable=inducing_variable)\n\n# %%\ngpflow.utilities.parameter_dict(model)\n\n# %% [markdown]\n# Such a dictionary can be assigned back to this model (or another model with the same tree of parameters) as follows:\n\n# %%\nparams = gpflow.utilities.parameter_dict(model)\ngpflow.utilities.multiple_assign(model, params)\n\n# %% [markdown]\n# ### TensorFlow `saved_model`\n#\n# At present, TensorFlow does not support saving custom variables like instances of the `gpflow.base.Parameter` class, see [this TensorFlow github issue](https://github.com/tensorflow/tensorflow/issues/34908).\n#\n# However, once training is complete, it is possible to clone the model and replace all `gpflow.base.Parameter`s with `tf.constant`s holding the same value:\n\n# %%\nmodel\n\n# %%\nfrozen_model = gpflow.utilities.freeze(model)\n\n# %% [markdown]\n# In order to save the model we need to define a `tf.Module` holding the `tf.function`\'s that we wish to export, as well as a reference to the underlying model:\n\n# %%\nmodule_to_save = tf.Module()\npredict_fn = tf.function(\n    frozen_model.predict_f, input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.float64)]\n)\nmodule_to_save.predict = predict_fn\n\n# %% [markdown]\n# Save original result for futher comparison. We also convert `samples_input` to a tensor. For a tensor input a `tf.function` will compile a single graph.\n\n# %%\nsamples_input = tf.convert_to_tensor(samples_input, dtype=default_float())\noriginal_result = module_to_save.predict(samples_input)\n\n# %% [markdown]\n# Let\'s save the module\n# %%\nsave_dir = str(pathlib.Path(tempfile.gettempdir()))\ntf.saved_model.save(module_to_save, save_dir)\n\n# %% [markdown]\n# Load module back as new instance and compare predict results\n\n# %%\nloaded_model = tf.saved_model.load(save_dir)\nloaded_result = loaded_model.predict(samples_input)\n\nnp.testing.assert_array_equal(loaded_result, original_result)\n\n# %% [markdown]\n# ## User config update\n#\n# In this notebook, we used a lot `gpflow.config` methods for setting and getting default attributes from global configuration. However, GPflow provides a way for local config modification without updating values in global. As you can see below, using `gpflow.config.as_context` replaces temporarily global config with your instance. At creation time, custom config instance uses standard values from the global config:\n\n# %%\nuser_config = gpflow.config.Config(float=tf.float32, positive_bijector=""exp"")\n\nuser_str = ""User config\\t""\nglobal_str = ""Global config\\t""\n\nwith gpflow.config.as_context(user_config):\n    print(f""{user_str} gpflow.config.default_float = {gpflow.config.default_float()}"")\n    print(\n        f""{user_str} gpflow.config.positive_bijector = {gpflow.config.default_positive_bijector()}""\n    )\n\nprint(f""{global_str} gpflow.config.default_float = {gpflow.config.default_float()}"")\nprint(f""{global_str} gpflow.config.positive_bijector = {gpflow.config.default_positive_bijector()}"")\n\n# %%\nwith gpflow.config.as_context(user_config):\n    p = gpflow.Parameter(1.1, transform=gpflow.utilities.positive())\n    print(f""{user_str}{p}"")\n\np = gpflow.Parameter(1.1, transform=gpflow.utilities.positive())\nprint(f""{global_str}{p}"")\n'"
gpflow/conditionals/multioutput/__init__.py,0,b'from . import sample_conditionals\nfrom . import conditionals\n'
gpflow/conditionals/multioutput/conditionals.py,14,"b'# flake8: ignore=F811\n# noqa: ignore=F811\n# flake8: F811\n# noqa: F811\n\nimport tensorflow as tf\n\nfrom ... import covariances\nfrom ...inducing_variables import (\n    InducingPoints,\n    FallbackSharedIndependentInducingVariables,\n    FallbackSeparateIndependentInducingVariables,\n    SharedIndependentInducingVariables,\n    SeparateIndependentInducingVariables,\n)\nfrom ...kernels import (\n    Combination,\n    MultioutputKernel,\n    SeparateIndependent,\n    SharedIndependent,\n    IndependentLatent,\n    LinearCoregionalization,\n)\nfrom ...config import default_float, default_jitter\nfrom ..dispatch import conditional\nfrom ..util import (\n    base_conditional,\n    expand_independent_outputs,\n    fully_correlated_conditional,\n    independent_interdomain_conditional,\n    mix_latent_gp,\n    rollaxis_left,\n)\n\n\n@conditional.register(object, SharedIndependentInducingVariables, SharedIndependent, object)\ndef shared_independent_conditional(\n    Xnew,\n    inducing_variable,\n    kernel,\n    f,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""Multioutput conditional for an independent kernel and shared inducing inducing.\n    Same behaviour as conditional with non-multioutput kernels.\n    The covariance matrices used to calculate the conditional have the following shape:\n    - Kuu: [M, M]\n    - Kuf: [M, N]\n    - Kff: N or [N, N]\n\n    Further reference\n    -----------------\n    - See `gpflow.conditionals._conditional` for a detailed explanation of\n      conditional in the single-output case.\n    - See the multioutput notebook for more information about the multioutput framework.\n    Parameters\n    ----------\n    :param Xnew: data matrix, size [N, D].\n    :param f: data matrix, [M, P]\n    :param full_cov: return the covariance between the datapoints\n    :param full_output_cov: return the covariance between the outputs.\n        Note: as we are using a independent kernel these covariances will be zero.\n    :param q_sqrt: matrix of standard-deviations or Cholesky matrices,\n        size [M, P] or [P, M, M].\n    :param white: boolean of whether to use the whitened representation\n    :return:\n        - mean:     [N, P]\n        - variance: [N, P], [P, N, N], [N, P, P] or [N, P, N, P]\n        Please see `gpflow.conditional._expand_independent_outputs` for more information\n        about the shape of the variance, depending on `full_cov` and `full_output_cov`.\n    """"""\n    Kmm = covariances.Kuu(inducing_variable, kernel, jitter=default_jitter())  # [M, M]\n    Kmn = covariances.Kuf(inducing_variable, kernel, Xnew)  # [M, N]\n    Knn = kernel.kernel(Xnew, full_cov=full_cov)\n\n    fmean, fvar = base_conditional(\n        Kmn, Kmm, Knn, f, full_cov=full_cov, q_sqrt=q_sqrt, white=white\n    )  # [N, P],  [P, N, N] or [N, P]\n    return fmean, expand_independent_outputs(fvar, full_cov, full_output_cov)\n\n\n@conditional.register(object, SeparateIndependentInducingVariables, SeparateIndependent, object)\n@conditional.register(object, SharedIndependentInducingVariables, SeparateIndependent, object)\n@conditional.register(object, SeparateIndependentInducingVariables, SharedIndependent, object)\ndef separate_independent_conditional(\n    Xnew,\n    inducing_variable,\n    kernel,\n    f,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""Multi-output GP with independent GP priors.\n    Number of latent processes equals the number of outputs (L = P).\n    The covariance matrices used to calculate the conditional have the following shape:\n    - Kuu: [P, M, M]\n    - Kuf: [P, M, N]\n    - Kff: [P, N] or [P, N, N]\n\n    Further reference\n    -----------------\n    - See `gpflow.conditionals._conditional` for a detailed explanation of\n      conditional in the single-output case.\n    - See the multioutput notebook for more information about the multioutput framework.\n    - See above for the parameters and the return value.\n    """"""\n    # Following are: [P, M, M]  -  [P, M, N]  -  [P, N](x N)\n    Kmms = covariances.Kuu(inducing_variable, kernel, jitter=default_jitter())  # [P, M, M]\n    Kmns = covariances.Kuf(inducing_variable, kernel, Xnew)  # [P, M, N]\n    if isinstance(kernel, Combination):\n        kernels = kernel.kernels\n    else:\n        kernels = [kernel.kernel] * len(inducing_variable.inducing_variable_list)\n    Knns = tf.stack([k.K(Xnew) if full_cov else k.K_diag(Xnew) for k in kernels], axis=0)\n    fs = tf.transpose(f)[:, :, None]  # [P, M, 1]\n    # [P, 1, M, M]  or  [P, M, 1]\n    q_sqrts = tf.transpose(q_sqrt)[:, :, None] if q_sqrt.shape.ndims == 2 else q_sqrt[:, None, :, :]\n\n    def single_gp_conditional(t):\n        Kmm, Kmn, Knn, f, q_sqrt = t\n        return base_conditional(Kmn, Kmm, Knn, f, full_cov=full_cov, q_sqrt=q_sqrt, white=white)\n\n    rmu, rvar = tf.map_fn(\n        single_gp_conditional, (Kmms, Kmns, Knns, fs, q_sqrts), (default_float(), default_float())\n    )  # [P, N, 1], [P, 1, N, N] or [P, N, 1]\n\n    fmu = rollaxis_left(tf.squeeze(rmu, axis=-1), 1)  # [N, P]\n\n    if full_cov:\n        fvar = tf.squeeze(rvar, axis=-3)  # [..., 0, :, :]  # [P, N, N]\n    else:\n        fvar = rollaxis_left(tf.squeeze(rvar, axis=-1), 1)  # [N, P]\n\n    return fmu, expand_independent_outputs(fvar, full_cov, full_output_cov)\n\n\n@conditional.register(\n    object,\n    (FallbackSharedIndependentInducingVariables, FallbackSeparateIndependentInducingVariables),\n    IndependentLatent,\n    object,\n)\ndef fallback_independent_latent_conditional(\n    Xnew,\n    inducing_variable,\n    kernel,\n    f,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""Interdomain conditional with independent latents.\n    In this case the number of latent GPs (L) will be different than the number of outputs (P)\n    The covariance matrices used to calculate the conditional have the following shape:\n    - Kuu: [L, M, M]\n    - Kuf: [M, L, N, P]\n    - Kff: [N, P, N, P], [N, P, P], [N, P]\n\n    Further reference\n    -----------------\n    - See `gpflow.conditionals._conditional` for a detailed explanation of\n      conditional in the single-output case.\n    - See the multioutput notebook for more information about the multioutput framework.\n    - See above for the parameters and the return value.\n    """"""\n    Kmm = covariances.Kuu(inducing_variable, kernel, jitter=default_jitter())  # [L, M, M]\n    Kmn = covariances.Kuf(inducing_variable, kernel, Xnew)  # [M, L, N, P]\n    Knn = kernel(\n        Xnew, full_cov=full_cov, full_output_cov=full_output_cov\n    )  # [N, P](x N)x P  or  [N, P](x P)\n\n    return independent_interdomain_conditional(\n        Kmn,\n        Kmm,\n        Knn,\n        f,\n        full_cov=full_cov,\n        full_output_cov=full_output_cov,\n        q_sqrt=q_sqrt,\n        white=white,\n    )\n\n\n@conditional.register(object, InducingPoints, MultioutputKernel, object)\ndef inducing_point_conditional(\n    Xnew,\n    inducing_variable,\n    kernel,\n    f,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""Multi-output GP with fully correlated inducing variables.\n    The inducing variables are shaped in the same way as evaluations of K, to allow a default\n    inducing point scheme for multi-output kernels.\n    The covariance matrices used to calculate the conditional have the following shape:\n    - Kuu: [M, L, M, L]\n    - Kuf: [M, L, N, P]\n    - Kff: [N, P, N, P], [N, P, P], [N, P]\n\n    Further reference\n    -----------------\n    - See `gpflow.conditionals._conditional` for a detailed explanation of\n      conditional in the single-output case.\n    - See the multioutput notebook for more information about the multioutput framework.\n\n    Parameters\n    ----------\n    :param f: variational mean, [L, 1]\n    :param q_sqrt: standard-deviations or cholesky, [L, 1]  or  [1, L, L]\n    """"""\n    Kmm = covariances.Kuu(inducing_variable, kernel, jitter=default_jitter())  # [M, L, M, L]\n    Kmn = covariances.Kuf(inducing_variable, kernel, Xnew)  # [M, L, N, P]\n    Knn = kernel(\n        Xnew, full_cov=full_cov, full_output_cov=full_output_cov\n    )  # [N, P](x N)x P  or  [N, P](x P)\n\n    M, L, N, K = tf.unstack(tf.shape(Kmn), num=Kmn.shape.ndims, axis=0)\n    Kmm = tf.reshape(Kmm, (M * L, M * L))\n\n    if full_cov == full_output_cov:\n        Kmn = tf.reshape(Kmn, (M * L, N * K))\n        Knn = tf.reshape(Knn, (N * K, N * K)) if full_cov else tf.reshape(Knn, (N * K,))\n        fmean, fvar = base_conditional(\n            Kmn, Kmm, Knn, f, full_cov=full_cov, q_sqrt=q_sqrt, white=white\n        )  # [K, 1], [1, K](x NK)\n        fmean = tf.reshape(fmean, (N, K))\n        fvar = tf.reshape(fvar, (N, K, N, K) if full_cov else (N, K))\n    else:\n        Kmn = tf.reshape(Kmn, (M * L, N, K))\n        fmean, fvar = fully_correlated_conditional(\n            Kmn,\n            Kmm,\n            Knn,\n            f,\n            full_cov=full_cov,\n            full_output_cov=full_output_cov,\n            q_sqrt=q_sqrt,\n            white=white,\n        )\n    return fmean, fvar\n\n\n@conditional.register(\n    object,\n    (SharedIndependentInducingVariables, SeparateIndependentInducingVariables),\n    LinearCoregionalization,\n    object,\n)\ndef coregionalization_conditional(\n    Xnew,\n    inducing_variable,\n    kernel,\n    f,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""Most efficient routine to project L independent latent gps through a mixing matrix W.\n    The mixing matrix is a member of the `LinearCoregionalization` and has shape [P, L].\n    The covariance matrices used to calculate the conditional have the following shape:\n    - Kuu: [L, M, M]\n    - Kuf: [L, M, N]\n    - Kff: [L, N] or [L, N, N]\n\n    Further reference\n    -----------------\n    - See `gpflow.conditionals._conditional` for a detailed explanation of\n      conditional in the single-output case.\n    - See the multioutput notebook for more information about the multioutput framework.\n    """"""\n    ind_conditional = conditional.dispatch(\n        object, SeparateIndependentInducingVariables, SeparateIndependent, object\n    )\n    gmu, gvar = ind_conditional(\n        Xnew,\n        inducing_variable,\n        kernel,\n        f,\n        full_cov=full_cov,\n        q_sqrt=q_sqrt,\n        full_output_cov=False,\n        white=white,\n    )  # [N, L], [L, N, N] or [N, L]\n    return mix_latent_gp(kernel.W, gmu, gvar, full_cov, full_output_cov)\n'"
gpflow/conditionals/multioutput/sample_conditionals.py,1,"b'import tensorflow as tf\n\nfrom ...inducing_variables import (\n    SeparateIndependentInducingVariables,\n    SharedIndependentInducingVariables,\n)\nfrom ...kernels import SeparateIndependent, LinearCoregionalization\nfrom ..dispatch import conditional, sample_conditional\nfrom ..util import sample_mvn, mix_latent_gp\n\n\n@sample_conditional.register(\n    object, SharedIndependentInducingVariables, LinearCoregionalization, object\n)\ndef _sample_conditional(\n    Xnew,\n    inducing_variable,\n    kernel,\n    f,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n    num_samples=None,\n):\n    """"""\n    `sample_conditional` will return a sample from the conditinoal distribution.\n    In most cases this means calculating the conditional mean m and variance v and then\n    returning m + sqrt(v) * eps, with eps ~ N(0, 1).\n    However, for some combinations of Mok and Mof more efficient sampling routines exists.\n    The dispatcher will make sure that we use the most efficent one.\n    :return: [N, P] (full_output_cov = False) or [N, P, P] (full_output_cov = True)\n    """"""\n    if full_cov:\n        raise NotImplementedError(""full_cov not yet implemented"")\n    if full_output_cov:\n        raise NotImplementedError(""full_output_cov not yet implemented"")\n\n    ind_conditional = conditional.dispatch(\n        object, SeparateIndependentInducingVariables, SeparateIndependent, object\n    )\n    g_mu, g_var = ind_conditional(\n        Xnew, inducing_variable, kernel, f, white=white, q_sqrt=q_sqrt\n    )  # [..., N, L], [..., N, L]\n    g_sample = sample_mvn(g_mu, g_var, full_cov, num_samples=num_samples)  # [..., (S), N, L]\n    f_mu, f_var = mix_latent_gp(kernel.W, g_mu, g_var, full_cov, full_output_cov)\n    f_sample = tf.tensordot(g_sample, kernel.W, [[-1], [-1]])  # [..., N, P]\n    return f_sample, f_mu, f_var\n'"
gpflow/covariances/multioutput/__init__.py,0,"b'from . import kufs, kuus\n'"
gpflow/covariances/multioutput/kufs.py,14,"b'from typing import Union\n\nimport tensorflow as tf\n\nfrom ...inducing_variables import (\n    InducingPoints,\n    FallbackSharedIndependentInducingVariables,\n    FallbackSeparateIndependentInducingVariables,\n    SharedIndependentInducingVariables,\n    SeparateIndependentInducingVariables,\n)\nfrom ...kernels import (\n    MultioutputKernel,\n    SeparateIndependent,\n    LinearCoregionalization,\n    SharedIndependent,\n)\nfrom ..dispatch import Kuf\n\n\n@Kuf.register(InducingPoints, MultioutputKernel, object)\ndef _Kuf(inducing_variable: InducingPoints, kernel: MultioutputKernel, Xnew: tf.Tensor):\n    return kernel(inducing_variable.Z, Xnew, full_cov=True, full_output_cov=True)  # [M, P, N, P]\n\n\n@Kuf.register(SharedIndependentInducingVariables, SharedIndependent, object)\ndef _Kuf(\n    inducing_variable: SharedIndependentInducingVariables,\n    kernel: SharedIndependent,\n    Xnew: tf.Tensor,\n):\n    return Kuf(inducing_variable.inducing_variable, kernel.kernel, Xnew)  # [M, N]\n\n\n@Kuf.register(SeparateIndependentInducingVariables, SharedIndependent, object)\ndef _Kuf(\n    inducing_variable: SeparateIndependentInducingVariables,\n    kernel: SharedIndependent,\n    Xnew: tf.Tensor,\n):\n    return tf.stack(\n        [Kuf(f, kernel.kernel, Xnew) for f in inducing_variable.inducing_variable_list], axis=0\n    )  # [L, M, N]\n\n\n@Kuf.register(SharedIndependentInducingVariables, SeparateIndependent, object)\ndef _Kuf(\n    inducing_variable: SharedIndependentInducingVariables,\n    kernel: SeparateIndependent,\n    Xnew: tf.Tensor,\n):\n    return tf.stack(\n        [Kuf(inducing_variable.inducing_variable, k, Xnew) for k in kernel.kernels], axis=0\n    )  # [L, M, N]\n\n\n@Kuf.register(SeparateIndependentInducingVariables, SeparateIndependent, object)\ndef _Kuf(\n    inducing_variable: SeparateIndependentInducingVariables,\n    kernel: SeparateIndependent,\n    Xnew: tf.Tensor,\n):\n    Kufs = [\n        Kuf(f, k, Xnew) for f, k in zip(inducing_variable.inducing_variable_list, kernel.kernels)\n    ]\n    return tf.stack(Kufs, axis=0)  # [L, M, N]\n\n\n@Kuf.register(\n    (FallbackSeparateIndependentInducingVariables, FallbackSharedIndependentInducingVariables),\n    LinearCoregionalization,\n    object,\n)\ndef _Kuf(\n    inducing_variable: Union[\n        SeparateIndependentInducingVariables, SharedIndependentInducingVariables\n    ],\n    kernel: LinearCoregionalization,\n    Xnew: tf.Tensor,\n):\n    kuf_impl = Kuf.dispatch(type(inducing_variable), SeparateIndependent, object)\n    K = tf.transpose(kuf_impl(inducing_variable, kernel, Xnew), [1, 0, 2])  # [M, L, N]\n    return K[:, :, :, None] * tf.transpose(kernel.W)[None, :, None, :]  # [M, L, N, P]\n\n\n@Kuf.register(SharedIndependentInducingVariables, LinearCoregionalization, object)\ndef _Kuf(\n    inducing_variable: SharedIndependentInducingVariables,\n    kernel: SeparateIndependent,\n    Xnew: tf.Tensor,\n):\n    return tf.stack(\n        [Kuf(inducing_variable.inducing_variable, k, Xnew) for k in kernel.kernels], axis=0\n    )  # [L, M, N]\n\n\n@Kuf.register(SeparateIndependentInducingVariables, LinearCoregionalization, object)\ndef _Kuf(inducing_variable, kernel, Xnew):\n    return tf.stack(\n        [Kuf(f, k, Xnew) for f, k in zip(inducing_variable.inducing_variable_list, kernel.kernels)],\n        axis=0,\n    )  # [L, M, N]\n'"
gpflow/covariances/multioutput/kuus.py,9,"b'from typing import Union\n\nimport tensorflow as tf\n\nfrom ...inducing_variables import (\n    InducingPoints,\n    FallbackSharedIndependentInducingVariables,\n    FallbackSeparateIndependentInducingVariables,\n    SharedIndependentInducingVariables,\n)\nfrom ...kernels import (\n    MultioutputKernel,\n    SeparateIndependent,\n    LinearCoregionalization,\n    SharedIndependent,\n    IndependentLatent,\n)\nfrom ..dispatch import Kuu\n\n\n@Kuu.register(InducingPoints, MultioutputKernel)\ndef _Kuu(inducing_variable: InducingPoints, kernel: MultioutputKernel, *, jitter=0.0):\n    Kmm = kernel(inducing_variable.Z, full_cov=True, full_output_cov=True)  # [M, P, M, P]\n    M = tf.shape(Kmm)[0] * tf.shape(Kmm)[1]\n    jittermat = jitter * tf.reshape(tf.eye(M, dtype=Kmm.dtype), tf.shape(Kmm))\n    return Kmm + jittermat\n\n\n@Kuu.register(FallbackSharedIndependentInducingVariables, SharedIndependent)\ndef _Kuu(\n    inducing_variable: FallbackSharedIndependentInducingVariables,\n    kernel: SharedIndependent,\n    *,\n    jitter=0.0,\n):\n    Kmm = Kuu(inducing_variable.inducing_variable, kernel.kernel)  # [M, M]\n    jittermat = tf.eye(len(inducing_variable), dtype=Kmm.dtype) * jitter\n    return Kmm + jittermat\n\n\n@Kuu.register(FallbackSharedIndependentInducingVariables, (SeparateIndependent, IndependentLatent))\ndef _Kuu(\n    inducing_variable: FallbackSharedIndependentInducingVariables,\n    kernel: Union[SeparateIndependent, IndependentLatent],\n    *,\n    jitter=0.0,\n):\n    Kmm = tf.stack(\n        [Kuu(inducing_variable.inducing_variable, k) for k in kernel.kernels], axis=0\n    )  # [L, M, M]\n    jittermat = tf.eye(len(inducing_variable), dtype=Kmm.dtype)[None, :, :] * jitter\n    return Kmm + jittermat\n\n\n@Kuu.register(FallbackSeparateIndependentInducingVariables, SharedIndependent)\ndef _Kuu(\n    inducing_variable: FallbackSeparateIndependentInducingVariables,\n    kernel: SharedIndependent,\n    *,\n    jitter=0.0,\n):\n    Kmm = tf.stack(\n        [Kuu(f, kernel.kernel) for f in inducing_variable.inducing_variable_list], axis=0\n    )  # [L, M, M]\n    jittermat = tf.eye(len(inducing_variable), dtype=Kmm.dtype)[None, :, :] * jitter\n    return Kmm + jittermat\n\n\n@Kuu.register(\n    FallbackSeparateIndependentInducingVariables, (SeparateIndependent, LinearCoregionalization)\n)\ndef _Kuu(\n    inducing_variable: FallbackSeparateIndependentInducingVariables,\n    kernel: Union[SeparateIndependent, LinearCoregionalization],\n    *,\n    jitter=0.0,\n):\n    Kmms = [Kuu(f, k) for f, k in zip(inducing_variable.inducing_variable_list, kernel.kernels)]\n    Kmm = tf.stack(Kmms, axis=0)  # [L, M, M]\n    jittermat = tf.eye(len(inducing_variable), dtype=Kmm.dtype)[None, :, :] * jitter\n    return Kmm + jittermat\n'"
gpflow/inducing_variables/multioutput/__init__.py,0,"b'from .inducing_variables import (\n    MultioutputInducingVariables,\n    FallbackSharedIndependentInducingVariables,\n    FallbackSeparateIndependentInducingVariables,\n    SharedIndependentInducingVariables,\n    SeparateIndependentInducingVariables,\n)\n'"
gpflow/inducing_variables/multioutput/inducing_variables.py,0,"b'# Copyright 2018 GPflow authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Sequence, Tuple\n\nfrom ...base import TensorType\nfrom ..inducing_variables import InducingVariables\n\n\nclass MultioutputInducingVariables(InducingVariables):\n    """"""\n    Multioutput Inducing Variables\n    Base class for methods which define a collection of inducing variables which\n    in some way can be grouped. The main example is where the inducing variables\n    consist of outputs of various independent GPs. This can be because our model\n    uses multiple independent GPs (SharedIndependent, SeparateIndependent) or\n    because it is constructed from independent GPs (eg IndependentLatent,\n    LinearCoregionalization).\n    """"""\n\n    @property\n    def inducing_variables(self) -> Tuple[TensorType, ...]:\n        raise NotImplementedError\n\n\nclass FallbackSharedIndependentInducingVariables(MultioutputInducingVariables):\n    """"""\n    Shared definition of inducing variables for each independent latent process.\n\n    This class is designated to be used to\n     - provide a general interface for multioutput kernels\n       constructed from independent latent processes,\n     - only require the specification of Kuu and Kuf.\n    All multioutput kernels constructed from independent latent processes allow\n    the inducing variables to be specified in the latent processes, and a\n    reasonably efficient method (i.e. one that takes advantage of the\n    independence in the latent processes) can be specified quite generally by\n    only requiring the following covariances:\n     - Kuu: [L, M, M],\n     - Kuf: [L, M, N, P].\n    In `gpflow/conditionals/multioutput/conditionals.py` we define a conditional() implementation for this\n    combination. We specify this code path for all kernels which inherit from\n    `IndependentLatentBase`. This set-up allows inference with any such kernel\n    to be implemented by specifying only `Kuu()` and `Kuf()`.\n\n    We call this the base class, since many multioutput GPs that are constructed\n    from independent latent processes acutally allow even more efficient\n    approximations. However, we include this code path, as it does not require\n    specifying a new `conditional()` implementation.\n\n    Here, we share the definition of inducing variables between all latent\n    processes.\n    """"""\n\n    def __init__(self, inducing_variable: TensorType):\n        super().__init__()\n        self.inducing_variable = inducing_variable\n\n    def __len__(self) -> int:\n        return len(self.inducing_variable)\n\n    @property\n    def inducing_variables(self) -> Tuple[TensorType]:\n        return (self.inducing_variable,)\n\n\nclass FallbackSeparateIndependentInducingVariables(MultioutputInducingVariables):\n    """"""\n    Separate set of inducing variables for each independent latent process.\n\n    This class is designated to be used to\n     - provide a general interface for multioutput kernels\n       constructed from independent latent processes,\n     - only require the specification of Kuu and Kuf.\n    All multioutput kernels constructed from independent latent processes allow\n    the inducing variables to be specified in the latent processes, and a\n    reasonably efficient method (i.e. one that takes advantage of the\n    independence in the latent processes) can be specified quite generally by\n    only requiring the following covariances:\n     - Kuu: [L, M, M],\n     - Kuf: [L, M, N, P].\n    In `gpflow/multioutput/conditionals.py` we define a conditional() implementation for this\n    combination. We specify this code path for all kernels which inherit from\n    `IndependentLatentBase`. This set-up allows inference with any such kernel\n    to be implemented by specifying only `Kuu()` and `Kuf()`.\n\n    We call this the base class, since many multioutput GPs that are constructed\n    from independent latent processes acutally allow even more efficient\n    approximations. However, we include this code path, as it does not require\n    specifying a new `conditional()` implementation.\n\n    We use a different definition of inducing variables for each latent process.\n    Note: each object should have the same number of inducing variables, M.\n    """"""\n\n    def __init__(self, inducing_variable_list: Sequence[TensorType]):\n        super().__init__()\n        self.inducing_variable_list = inducing_variable_list\n\n    def __len__(self) -> int:\n        return len(self.inducing_variable_list[0])\n\n    @property\n    def inducing_variables(self) -> Tuple[TensorType, ...]:\n        return tuple(self.inducing_variable_list)\n\n\nclass SharedIndependentInducingVariables(FallbackSharedIndependentInducingVariables):\n    """"""\n    Here, we define the same inducing variables as in the base class. However,\n    this class is intended to be used without the constraints on the shapes that\n    `Kuu()` and `Kuf()` return. This allows a custom `conditional()` to provide\n    the most efficient implementation.\n    """"""\n\n    pass\n\n\nclass SeparateIndependentInducingVariables(FallbackSeparateIndependentInducingVariables):\n    """"""\n    Here, we define the same inducing variables as in the base class. However,\n    this class is intended to be used without the constraints on the shapes that\n    `Kuu()` and `Kuf()` return. This allows a custom `conditional()` to provide\n    the most efficient implementation.\n    """"""\n\n    pass\n'"
gpflow/kernels/multioutput/__init__.py,0,"b'from .kernels import (\n    MultioutputKernel,\n    SeparateIndependent,\n    SharedIndependent,\n    IndependentLatent,\n    LinearCoregionalization,\n)\n'"
gpflow/kernels/multioutput/kernels.py,23,"b'# Copyright 2018 GPflow authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\n\nimport tensorflow as tf\n\nfrom ...base import Parameter\nfrom ..base import Combination, Kernel\n\n\nclass MultioutputKernel(Kernel):\n    """"""\n    Multi Output Kernel class.\n    This kernel can represent correlation between outputs of different datapoints.\n    Therefore, subclasses of Mok should implement `K` which returns:\n    - [N, P, N, P] if full_output_cov = True\n    - [P, N, N] if full_output_cov = False\n    and `K_diag` returns:\n    - [N, P, P] if full_output_cov = True\n    - [N, P] if full_output_cov = False\n    The `full_output_cov` argument holds whether the kernel should calculate\n    the covariance between the outputs. In case there is no correlation but\n    `full_output_cov` is set to True the covariance matrix will be filled with zeros\n    until the appropriate size is reached.\n    """"""\n\n    @property\n    @abc.abstractmethod\n    def num_latent_gps(self):\n        """"""The number of latent GPs in the multioutput kernel""""""\n        raise NotImplementedError\n\n    @property\n    @abc.abstractmethod\n    def latent_kernels(self):\n        """"""The underlying kernels in the multioutput kernel""""""\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def K(self, X, X2=None, full_output_cov=True):\n        """"""\n        Returns the correlation of f(X) and f(X2), where f(.) can be multi-dimensional.\n        :param X: data matrix, [N1, D]\n        :param X2: data matrix, [N2, D]\n        :param full_output_cov: calculate correlation between outputs.\n        :return: cov[f(X), f(X2)] with shape\n        - [N1, P, N2, P] if `full_output_cov` = True\n        - [P, N1, N2] if `full_output_cov` = False\n        """"""\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def K_diag(self, X, full_output_cov=True):\n        """"""\n        Returns the correlation of f(X) and f(X), where f(.) can be multi-dimensional.\n        :param X: data matrix, [N, D]\n        :param full_output_cov: calculate correlation between outputs.\n        :return: var[f(X)] with shape\n        - [N, P, N, P] if `full_output_cov` = True\n        - [N, P] if `full_output_cov` = False\n        """"""\n        raise NotImplementedError\n\n    def __call__(self, X, X2=None, *, full_cov=False, full_output_cov=True, presliced=False):\n        if not presliced:\n            X, X2 = self.slice(X, X2)\n        if not full_cov and X2 is not None:\n            raise ValueError(\n                ""Ambiguous inputs: passing in `X2` is not compatible with `full_cov=False`.""\n            )\n        if not full_cov:\n            return self.K_diag(X, full_output_cov=full_output_cov)\n        return self.K(X, X2, full_output_cov=full_output_cov)\n\n\nclass SharedIndependent(MultioutputKernel):\n    """"""\n    - Shared: we use the same kernel for each latent GP\n    - Independent: Latents are uncorrelated a priori.\n    Note: this class is created only for testing and comparison purposes.\n    Use `gpflow.kernels` instead for more efficient code.\n    """"""\n\n    def __init__(self, kernel: Kernel, output_dim: int):\n        super().__init__()\n        self.kernel = kernel\n        self.output_dim = output_dim\n\n    @property\n    def num_latent_gps(self):\n        # In this case number of latent GPs (L) == output_dim (P)\n        return self.output_dim\n\n    @property\n    def latent_kernels(self):\n        """"""The underlying kernels in the multioutput kernel""""""\n        return (self.kernel,)\n\n    def K(self, X, X2=None, full_output_cov=True):\n        K = self.kernel.K(X, X2)  # [N, N2]\n        if full_output_cov:\n            Ks = tf.tile(K[..., None], [1, 1, self.output_dim])  # [N, N2, P]\n            return tf.transpose(tf.linalg.diag(Ks), [0, 2, 1, 3])  # [N, P, N2, P]\n        else:\n            return tf.tile(K[None, ...], [self.output_dim, 1, 1])  # [P, N, N2]\n\n    def K_diag(self, X, full_output_cov=True):\n        K = self.kernel.K_diag(X)  # N\n        Ks = tf.tile(K[:, None], [1, self.output_dim])  # [N, P]\n        return tf.linalg.diag(Ks) if full_output_cov else Ks  # [N, P, P] or [N, P]\n\n\nclass SeparateIndependent(MultioutputKernel, Combination):\n    """"""\n    - Separate: we use different kernel for each output latent\n    - Independent: Latents are uncorrelated a priori.\n    """"""\n\n    def __init__(self, kernels, name=None):\n        super().__init__(kernels=kernels, name=name)\n\n    @property\n    def num_latent_gps(self):\n        return len(self.kernels)\n\n    @property\n    def latent_kernels(self):\n        """"""The underlying kernels in the multioutput kernel""""""\n        return tuple(self.kernels)\n\n    def K(self, X, X2=None, full_output_cov=True):\n        if full_output_cov:\n            Kxxs = tf.stack([k.K(X, X2) for k in self.kernels], axis=2)  # [N, N2, P]\n            return tf.transpose(tf.linalg.diag(Kxxs), [0, 2, 1, 3])  # [N, P, N2, P]\n        else:\n            return tf.stack([k.K(X, X2) for k in self.kernels], axis=0)  # [P, N, N2]\n\n    def K_diag(self, X, full_output_cov=False):\n        stacked = tf.stack([k.K_diag(X) for k in self.kernels], axis=1)  # [N, P]\n        return tf.linalg.diag(stacked) if full_output_cov else stacked  # [N, P, P]  or  [N, P]\n\n\nclass IndependentLatent(MultioutputKernel):\n    """"""\n    Base class for multioutput kernels that are constructed from independent\n    latent Gaussian processes.\n\n    It should always be possible to specify inducing variables for such kernels\n    that give a block-diagonal Kuu, which can be represented as a [L, M, M]\n    tensor. A reasonable (but not optimal) inference procedure can be specified\n    by placing the inducing points in the latent processes and simply computing\n    Kuu [L, M, M] and Kuf [N, P, M, L] and using `fallback_independent_latent_\n    conditional()`. This can be specified by using `Fallback{Separate|Shared}\n    IndependentInducingVariables`.\n    """"""\n\n    @abc.abstractmethod\n    def Kgg(self, X, X2):\n        raise NotImplementedError\n\n\nclass LinearCoregionalization(IndependentLatent, Combination):\n    """"""\n    Linear mixing of the latent GPs to form the output.\n    """"""\n\n    def __init__(self, kernels, W, name=None):\n        Combination.__init__(self, kernels=kernels, name=name)\n        self.W = Parameter(W)  # [P, L]\n\n    @property\n    def num_latent_gps(self):\n        return self.W.shape[-1]  # L\n\n    @property\n    def latent_kernels(self):\n        """"""The underlying kernels in the multioutput kernel""""""\n        return tuple(self.kernels)\n\n    def Kgg(self, X, X2):\n        return tf.stack([k.K(X, X2) for k in self.kernels], axis=0)  # [L, N, N2]\n\n    def K(self, X, X2=None, full_output_cov=True):\n        Kxx = self.Kgg(X, X2)  # [L, N, N2]\n        KxxW = Kxx[None, :, :, :] * self.W[:, :, None, None]  # [P, L, N, N2]\n        if full_output_cov:\n            # return tf.einsum(\'lnm,kl,ql->nkmq\', Kxx, self.W, self.W)\n            WKxxW = tf.tensordot(self.W, KxxW, [[1], [1]])  # [P, P, N, N2]\n            return tf.transpose(WKxxW, [2, 0, 3, 1])  # [N, P, N2, P]\n        else:\n            # return tf.einsum(\'lnm,kl,kl->knm\', Kxx, self.W, self.W)\n            return tf.reduce_sum(self.W[:, :, None, None] * KxxW, [1])  # [P, N, N2]\n\n    def K_diag(self, X, full_output_cov=True):\n        K = tf.stack([k.K_diag(X) for k in self.kernels], axis=1)  # [N, L]\n        if full_output_cov:\n            # Can currently not use einsum due to unknown shape from `tf.stack()`\n            # return tf.einsum(\'nl,lk,lq->nkq\', K, self.W, self.W)  # [N, P, P]\n            Wt = tf.transpose(self.W)  # [L, P]\n            return tf.reduce_sum(\n                K[:, :, None, None] * Wt[None, :, :, None] * Wt[None, :, None, :], axis=1\n            )  # [N, P, P]\n        else:\n            # return tf.einsum(\'nl,lk,lk->nkq\', K, self.W, self.W)  # [N, P]\n            return tf.linalg.matmul(\n                K, self.W ** 2.0, transpose_b=True\n            )  # [N, L]  *  [L, P]  ->  [N, P]\n'"
tests/gpflow/conditionals/__init__.py,0,b''
tests/gpflow/conditionals/test_broadcasted_conditionals.py,9,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThis test suite will check if the conditionals broadcast correctly\nwhen the input tensors have leading dimensions.\n""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nimport gpflow.inducing_variables.multioutput as mf\nimport gpflow.kernels.multioutput as mk\nfrom gpflow.conditionals import sample_conditional\nfrom gpflow.conditionals.util import mix_latent_gp\n\n\n# ------------------------------------------\n# Data classes: storing constants\n# ------------------------------------------\n\n\nclass Data:\n    S1, S2, N, M = (\n        7,  # num samples 1\n        6,  # num samples 2\n        4,  # num datapoints\n        3,  # num inducing\n    )\n    Dx, Dy, L = (\n        2,  # input dim\n        5,  # output dim\n        4,  # num latent GPs\n    )\n    W = np.random.randn(Dy, L)  # mixing matrix\n\n    SX = np.random.randn(S1 * S2, N, Dx)\n    S1_S2_X = np.reshape(SX, [S1, S2, N, Dx])\n\n    Z = np.random.randn(M, Dx)\n\n\n@pytest.mark.parametrize(""full_cov"", [False, True])\n@pytest.mark.parametrize(""white"", [True, False])\n@pytest.mark.parametrize(""conditional_type"", [""mixing"", ""Z"", ""inducing_points""])\ndef test_conditional_broadcasting(full_cov, white, conditional_type):\n    """"""\n    Test that the `conditional` and `sample_conditional` broadcasts correctly\n    over leading dimensions of Xnew. Xnew can be shape [..., N, D],\n    and conditional should broadcast over the [...].\n    """"""\n    q_mu = np.random.randn(Data.M, Data.Dy)\n    q_sqrt = np.tril(np.random.randn(Data.Dy, Data.M, Data.M), -1)\n\n    if conditional_type == ""Z"":\n        inducing_variable = Data.Z\n        kernel = gpflow.kernels.Matern52(lengthscales=0.5)\n    elif conditional_type == ""inducing_points"":\n        inducing_variable = gpflow.inducing_variables.InducingPoints(Data.Z)\n        kernel = gpflow.kernels.Matern52(lengthscales=0.5)\n    elif conditional_type == ""mixing"":\n        # variational params have different output dim in this case\n        q_mu = np.random.randn(Data.M, Data.L)\n        q_sqrt = np.tril(np.random.randn(Data.L, Data.M, Data.M), -1)\n        inducing_variable = mf.SharedIndependentInducingVariables(\n            gpflow.inducing_variables.InducingPoints(Data.Z)\n        )\n        kernel = mk.LinearCoregionalization(\n            kernels=[gpflow.kernels.Matern52(lengthscales=0.5) for _ in range(Data.L)], W=Data.W,\n        )\n    else:\n        raise NotImplementedError\n\n    if conditional_type == ""mixing"" and full_cov:\n        pytest.skip(""combination is not implemented"")\n\n    num_samples = 5\n\n    def sample_conditional_fn(X):\n        return sample_conditional(\n            X,\n            inducing_variable,\n            kernel,\n            tf.convert_to_tensor(q_mu),\n            q_sqrt=tf.convert_to_tensor(q_sqrt),\n            white=white,\n            full_cov=full_cov,\n            num_samples=num_samples,\n        )\n\n    samples = np.array([sample_conditional_fn(X)[0] for X in Data.SX])\n    means = np.array([sample_conditional_fn(X)[1] for X in Data.SX])\n    variables = np.array([sample_conditional_fn(X)[2] for X in Data.SX])\n\n    samples_S12, means_S12, vars_S12 = sample_conditional(\n        Data.SX,\n        inducing_variable,\n        kernel,\n        tf.convert_to_tensor(q_mu),\n        q_sqrt=tf.convert_to_tensor(q_sqrt),\n        white=white,\n        full_cov=full_cov,\n        num_samples=num_samples,\n    )\n\n    samples_S1_S2, means_S1_S2, vars_S1_S2 = sample_conditional(\n        Data.S1_S2_X,\n        inducing_variable,\n        kernel,\n        tf.convert_to_tensor(q_mu),\n        q_sqrt=tf.convert_to_tensor(q_sqrt),\n        white=white,\n        full_cov=full_cov,\n        num_samples=num_samples,\n    )\n\n    assert_allclose(samples_S12.shape, samples.shape)\n    assert_allclose(samples_S1_S2.shape, [Data.S1, Data.S2, num_samples, Data.N, Data.Dy])\n    assert_allclose(means_S12, means)\n    assert_allclose(vars_S12, variables)\n    assert_allclose(means_S1_S2.numpy().reshape(Data.S1 * Data.S2, Data.N, Data.Dy), means)\n    if full_cov:\n        vars_s1_s2 = vars_S1_S2.numpy().reshape(Data.S1 * Data.S2, Data.Dy, Data.N, Data.N)\n        assert_allclose(vars_s1_s2, variables)\n    else:\n        vars_s1_s2 = vars_S1_S2.numpy().reshape(Data.S1 * Data.S2, Data.N, Data.Dy)\n        assert_allclose(vars_s1_s2, variables)\n\n\n# -------------------------------------------\n# Test utility functions used in conditionals\n# -------------------------------------------\n\n# _mix_latent_gps\n@pytest.mark.parametrize(""full_cov"", [True, False])\n@pytest.mark.parametrize(""full_output_cov"", [True, False])\ndef test_broadcasting_mix_latent_gps(full_cov, full_output_cov):\n    S, N = 7, 20  # batch size, num data points\n    P, L = 10, 5  # observation dimensionality, num latent GPs\n    W = np.random.randn(P, L)  # mixing matrix\n    g_mu = np.random.randn(S, N, L)  # mean of the L latent GPs\n\n    g_sqrt_diag = np.tril(np.random.randn(S * L, N, N), -1)  # [L*S, N, N]\n    g_sqrt_diag = np.reshape(g_sqrt_diag, [L, S, N, N])\n    g_var_diag = g_sqrt_diag @ np.transpose(g_sqrt_diag, [0, 1, 3, 2])  # [L, S, N, N]\n    g_var = np.zeros([S, N, L, N, L])\n    for l in range(L):\n        g_var[:, :, l, :, l] = g_var_diag[l, :, :, :]  # replace diagonal elements by g_var_diag\n\n    # reference numpy implementation for mean\n    f_mu_ref = g_mu @ W.T  # [S, N, P]\n\n    # reference numpy implementation for variance\n    g_var_tmp = np.transpose(g_var, [0, 1, 3, 2, 4])  # [S, N, N, L, L]\n    f_var_ref = W @ g_var_tmp @ W.T  # [S, N, N, P, P]\n    f_var_ref = np.transpose(f_var_ref, [0, 1, 3, 2, 4])  # [S, N, P, N, P]\n\n    if not full_cov:\n        g_var_diag = np.array([g_var_diag[:, :, n, n] for n in range(N)])  # [N, L, S]\n        g_var_diag = np.transpose(g_var_diag, [2, 0, 1])  # [S, N, L]\n\n    # run gpflow\'s implementation\n    f_mu, f_var = mix_latent_gp(\n        tf.convert_to_tensor(W),\n        tf.convert_to_tensor(g_mu),\n        tf.convert_to_tensor(g_var_diag),\n        full_cov,\n        full_output_cov,\n    )\n\n    # we strip down f_var_ref to the elements we need\n    if not full_output_cov and not full_cov:\n        f_var_ref = np.array([f_var_ref[:, :, p, :, p] for p in range(P)])  # [P, S, N, N]\n        f_var_ref = np.array([f_var_ref[:, :, n, n] for n in range(N)])  # [N, P, S]\n        f_var_ref = np.transpose(f_var_ref, [2, 0, 1])  # [S, N, P]\n\n    elif not full_output_cov and full_cov:\n        f_var_ref = np.array([f_var_ref[:, :, p, :, p] for p in range(P)])  # [P, S, N, N]\n        f_var_ref = np.transpose(f_var_ref, [1, 0, 2, 3])  # [S, P, N, N]\n\n    elif full_output_cov and not full_cov:\n        f_var_ref = np.array([f_var_ref[:, n, :, n, :] for n in range(N)])  # [N, S, P, P]\n        f_var_ref = np.transpose(f_var_ref, [1, 0, 2, 3])  # [S, N, P, P]\n\n    else:\n        pass  # f_var_ref has shape [..., N, P, N, P] as expected\n\n    # check equality for mean and variance of f\n    assert_allclose(f_mu_ref, f_mu)\n    assert_allclose(f_var_ref, f_var)\n'"
tests/gpflow/conditionals/test_conditionals.py,17,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow import Parameter\nfrom gpflow.utilities.bijectors import triangular\nfrom gpflow.conditionals import conditional\nfrom gpflow.config import default_float\n\nrng = np.random.RandomState(123)\n\nLn = 2\nNn = 10\nMn = 20\n\n\n@pytest.fixture(scope=""module"")\ndef kernel():\n    k = gpflow.kernels.Matern32() + gpflow.kernels.White()\n    k.kernels[1].variance.assign(0.01)\n    return k\n\n\n@pytest.fixture(scope=""module"")\ndef Xdata():\n    return tf.convert_to_tensor(rng.randn(Nn, 1))\n\n\n@pytest.fixture(scope=""module"")\ndef Xnew():\n    return tf.convert_to_tensor(rng.randn(Mn, 1))\n\n\n@pytest.fixture(scope=""module"")\ndef mu():\n    return tf.convert_to_tensor(rng.randn(Nn, Ln))\n\n\n@pytest.fixture(scope=""module"")\ndef sqrt():\n    return tf.convert_to_tensor(rng.randn(Nn, Ln))\n\n\n@pytest.fixture(scope=""module"")\ndef chol(sqrt):\n    return tf.stack([tf.linalg.diag(sqrt[:, i]) for i in range(Ln)])\n\n\n@pytest.mark.parametrize(""white"", [True, False])\ndef test_diag(Xdata, Xnew, kernel, mu, sqrt, chol, white):\n    Fstar_mean_1, Fstar_var_1 = conditional(Xnew, Xdata, kernel, mu, q_sqrt=sqrt, white=white)\n    Fstar_mean_2, Fstar_var_2 = conditional(Xnew, Xdata, kernel, mu, q_sqrt=chol, white=white)\n\n    mean_diff = Fstar_mean_1 - Fstar_mean_2\n    var_diff = Fstar_var_1 - Fstar_var_2\n\n    assert_allclose(mean_diff, 0)\n    assert_allclose(var_diff, 0)\n\n\ndef test_whiten(Xdata, Xnew, kernel, mu, sqrt):\n    """"""\n    Make sure that predicting using the whitened representation is the\n    sameas the non-whitened one.\n    """"""\n\n    K = kernel(Xdata) + tf.eye(Nn, dtype=default_float()) * 1e-6\n    L = tf.linalg.cholesky(K)\n    V = tf.linalg.triangular_solve(L, mu, lower=True)\n    mean1, var1 = conditional(Xnew, Xdata, kernel, mu)\n    mean2, var2 = conditional(Xnew, Xdata, kernel, V, white=True)\n\n    assert_allclose(mean1, mean2)\n    assert_allclose(var1, var2)\n\n\ndef test_gaussian_whiten(Xdata, Xnew, kernel, mu, sqrt):\n    """"""\n    Make sure that predicting using the whitened representation is the\n    same as the non-whitened one.\n    """"""\n    F_sqrt = tf.convert_to_tensor(rng.rand(Nn, Ln))\n\n    K = kernel(Xdata)\n    L = tf.linalg.cholesky(K)\n    V = tf.linalg.triangular_solve(L, mu, lower=True)\n    V_prime = tf.linalg.diag(tf.transpose(F_sqrt))\n    common_shape = tf.broadcast_static_shape(V_prime.shape, L.shape)\n    L = tf.broadcast_to(L, common_shape)\n    V_sqrt = tf.linalg.triangular_solve(L, tf.linalg.diag(tf.transpose(F_sqrt)), lower=True)\n\n    Fstar_mean, Fstar_var = conditional(Xnew, Xdata, kernel, mu, q_sqrt=F_sqrt)\n    Fstar_w_mean, Fstar_w_var = conditional(Xnew, Xdata, kernel, V, q_sqrt=V_sqrt, white=True)\n\n    mean_diff = Fstar_w_mean - Fstar_mean\n    var_diff = Fstar_w_var - Fstar_var\n\n    assert_allclose(mean_diff, 0, atol=4)\n    assert_allclose(var_diff, 0, atol=4)\n\n\n@pytest.mark.parametrize(""white"", [True, False])\ndef test_q_sqrt_constraints(Xdata, Xnew, kernel, mu, white):\n    """""" Test that sending in an unconstrained q_sqrt returns the same conditional\n    evaluation and gradients. This is important to match the behaviour of the KL, which\n    enforces q_sqrt is triangular.\n    """"""\n\n    tril = np.tril(rng.randn(Ln, Nn, Nn))\n\n    q_sqrt_constrained = Parameter(tril, transform=triangular())\n    q_sqrt_unconstrained = Parameter(tril)\n\n    diff_before_gradient_step = (q_sqrt_constrained - q_sqrt_unconstrained).numpy()\n    assert_allclose(diff_before_gradient_step, 0)\n\n    Fstars = []\n    for q_sqrt in [q_sqrt_constrained, q_sqrt_unconstrained]:\n\n        with tf.GradientTape() as tape:\n            _, Fstar_var = conditional(Xnew, Xdata, kernel, mu, q_sqrt=q_sqrt, white=white)\n\n        grad = tape.gradient(Fstar_var, q_sqrt.unconstrained_variable)\n        q_sqrt.unconstrained_variable.assign_sub(grad)\n        Fstars.append(Fstar_var)\n\n    diff_Fstar_before_gradient_step = Fstars[0] - Fstars[1]\n    assert_allclose(diff_Fstar_before_gradient_step, 0)\n\n    diff_after_gradient_step = (q_sqrt_constrained - q_sqrt_unconstrained).numpy()\n    assert_allclose(diff_after_gradient_step, 0)\n\n\n@pytest.mark.parametrize(""full_cov"", [True, False])\n@pytest.mark.parametrize(""features_inducing_points"", [False, True])\ndef test_base_conditional_vs_ref(full_cov, features_inducing_points):\n    """"""\n    Test that conditionals agree with a slow-but-clear numpy implementation\n    """"""\n    Dy, N, M, Dx = 5, 4, 3, 2\n    X = np.random.randn(N, Dx)\n    Z = np.random.randn(M, Dx)\n    kern = gpflow.kernels.Matern52(lengthscales=0.5)\n    q_mu = np.random.randn(M, Dy)\n    q_sqrt = np.tril(np.random.randn(Dy, M, M), -1)\n\n    def numpy_conditional(X, Z, kern, q_mu, q_sqrt):\n        Kmm = kern(Z, Z) + np.eye(M) * gpflow.config.default_jitter()\n        Kmn = kern(Z, X)\n        Knn = kern(X, X)\n\n        Kmm, Kmn, Knn = [k.numpy() for k in [Kmm, Kmn, Knn]]\n\n        Kmm, Kmn, Knm, Knn = [np.tile(k[None, :, :], [Dy, 1, 1]) for k in [Kmm, Kmn, Kmn.T, Knn]]\n\n        S = q_sqrt @ np.transpose(q_sqrt, [0, 2, 1])\n\n        Kmm_inv = np.linalg.inv(Kmm)\n        mean = np.einsum(""dmn,dmM,Md->nd"", Kmn, Kmm_inv, q_mu)\n        cov = Knn + Knm @ Kmm_inv @ (S - Kmm) @ Kmm_inv @ Kmn\n        return mean, cov\n\n    mean_np, cov_np = numpy_conditional(X, Z, kern, q_mu, q_sqrt)\n\n    if features_inducing_points:\n        Z = gpflow.inducing_variables.InducingPoints(Z)\n\n    mean_gpflow, cov_gpflow = [\n        v.numpy()\n        for v in gpflow.conditionals.conditional(\n            X, Z, kern, q_mu, q_sqrt=tf.identity(q_sqrt), white=False, full_cov=full_cov\n        )\n    ]\n\n    if not full_cov:\n        cov_np = np.diagonal(cov_np, axis1=-1, axis2=-2).T\n\n    assert_allclose(mean_np, mean_gpflow)\n    assert_allclose(cov_np, cov_gpflow)\n'"
tests/gpflow/conditionals/test_multioutput.py,33,"b'import numpy as np\nimport pytest\nimport scipy\nimport tensorflow as tf\n\nimport gpflow\nimport gpflow.inducing_variables.multioutput as mf\nimport gpflow.kernels.multioutput as mk\nfrom gpflow.conditionals import sample_conditional\nfrom gpflow.conditionals.util import (\n    fully_correlated_conditional,\n    fully_correlated_conditional_repeat,\n    sample_mvn,\n)\nfrom gpflow.inducing_variables import InducingPoints\nfrom gpflow.kernels import SquaredExponential\nfrom gpflow.likelihoods import Gaussian\nfrom gpflow.models import SVGP\nfrom gpflow.config import default_jitter, default_float\nfrom gpflow import set_trainable\n\nfloat_type = default_float()\nrng = np.random.RandomState(99201)\n\n\n# ------------------------------------------\n# Helpers\n# ------------------------------------------\n\n\ndef predict(model, Xnew, full_cov, full_output_cov):\n    m, v = model.predict_f(Xnew, full_cov=full_cov, full_output_cov=full_output_cov)\n    return [m, v]\n\n\ndef predict_all(models, Xnew, full_cov, full_output_cov):\n    """"""\n    Returns the mean and variance of f(Xnew) for each model in `models`.\n    """"""\n    ms, vs = [], []\n    for model in models:\n        m, v = predict(model, Xnew, full_cov, full_output_cov)\n        ms.append(m)\n        vs.append(v)\n    return ms, vs\n\n\ndef assert_all_array_elements_almost_equal(arr, decimal):\n    """"""\n    Check if consecutive elements of `arr` are almost equal.\n    """"""\n    for i in range(len(arr) - 1):\n        np.testing.assert_allclose(arr[i], arr[i + 1], atol=1e-5)\n\n\ndef check_equality_predictions(data, models, decimal=3):\n    """"""\n    Executes a couple of checks to compare the equality of predictions\n    of different models. The models should be configured with the same\n    training data (X, Y). The following checks are done:\n    - check if elbo is (almost) equal for all models\n    - check if predicted mean is (almost) equal\n    - check if predicted variance is (almost) equal.\n      All possible variances over the inputs and outputs are calculated\n      and equality is checked.\n    - check if variances within model are consistent. Parts of the covariance\n      matrices should overlap, and this is tested.\n    """"""\n\n    elbos = [m.elbo(data) for m in models]\n\n    # Check equality of log likelihood\n    assert_all_array_elements_almost_equal(elbos, decimal=5)\n\n    # Predict: full_cov = True and full_output_cov = True\n    means_tt, vars_tt = predict_all(models, Data.Xs, full_cov=True, full_output_cov=True)\n    # Predict: full_cov = True and full_output_cov = False\n    means_tf, vars_tf = predict_all(models, Data.Xs, full_cov=True, full_output_cov=False)\n    # Predict: full_cov = False and full_output_cov = True\n    means_ft, vars_ft = predict_all(models, Data.Xs, full_cov=False, full_output_cov=True)\n    # Predict: full_cov = False and full_output_cov = False\n    means_ff, vars_ff = predict_all(models, Data.Xs, full_cov=False, full_output_cov=False)\n\n    # check equality of all the means\n    all_means = means_tt + means_tf + means_ft + means_ff\n    assert_all_array_elements_almost_equal(all_means, decimal=decimal)\n\n    # check equality of all the variances within a category\n    # (e.g. full_cov=True and full_output_cov=False)\n    all_vars = [vars_tt, vars_tf, vars_ft, vars_ff]\n    _ = [assert_all_array_elements_almost_equal(var, decimal=decimal) for var in all_vars]\n\n    # Here we check that the variance in different categories are equal\n    # after transforming to the right shape.\n    var_tt = vars_tt[0]  # N x P x N x P\n    var_tf = vars_tf[0]  # P x N x c\n    var_ft = vars_ft[0]  # N x P x P\n    var_ff = vars_ff[0]  # N x P\n\n    np.testing.assert_almost_equal(\n        np.diagonal(var_tt, axis1=1, axis2=3), np.transpose(var_tf, [1, 2, 0]), decimal=decimal,\n    )\n    np.testing.assert_almost_equal(\n        np.diagonal(var_tt, axis1=0, axis2=2), np.transpose(var_ft, [1, 2, 0]), decimal=decimal,\n    )\n    np.testing.assert_almost_equal(\n        np.diagonal(np.diagonal(var_tt, axis1=0, axis2=2)), var_ff, decimal=decimal\n    )\n\n\ndef expand_cov(q_sqrt, W):\n    """"""\n    :param G: cholesky of covariance matrices, L x M x M\n    :param W: mixing matrix (square),  L x L\n    :return: cholesky of 1 x LM x LM covariance matrix\n    """"""\n    q_cov = np.matmul(q_sqrt, q_sqrt.transpose([0, 2, 1]))  # [L, M, M]\n    q_cov_expanded = scipy.linalg.block_diag(*q_cov)  # [LM, LM]\n    q_sqrt_expanded = np.linalg.cholesky(q_cov_expanded)  # [LM, LM]\n    return q_sqrt_expanded[None, ...]\n\n\ndef create_q_sqrt(M, L):\n    """""" returns an array of L lower triangular matrices of size M x M """"""\n    return np.array([np.tril(rng.randn(M, M)) for _ in range(L)])  # [L, M, M]\n\n\n# ------------------------------------------\n# Data classes: storing constants\n# ------------------------------------------\n\n\nclass Data:\n    N, Ntest = 20, 5\n    D = 1  # input dimension\n    M = 3  # inducing points\n    L = 2  # latent gps\n    P = 3  # output dimension\n    MAXITER = int(15e2)\n    X = tf.random.normal((N,), dtype=tf.float64)[:, None] * 10 - 5\n    G = np.hstack((0.5 * np.sin(3 * X) + X, 3.0 * np.cos(X) - X))\n    Ptrue = np.array([[0.5, -0.3, 1.5], [-0.4, 0.43, 0.0]])  # [L, P]\n\n    Y = tf.convert_to_tensor(G @ Ptrue)\n    G = tf.convert_to_tensor(np.hstack((0.5 * np.sin(3 * X) + X, 3.0 * np.cos(X) - X)))\n    Ptrue = tf.convert_to_tensor(np.array([[0.5, -0.3, 1.5], [-0.4, 0.43, 0.0]]))  # [L, P]\n    Y += tf.random.normal(Y.shape, dtype=tf.float64) * [0.2, 0.2, 0.2]\n    Xs = tf.convert_to_tensor(np.linspace(-6, 6, Ntest)[:, None])\n    data = (X, Y)\n\n\nclass DataMixedKernelWithEye(Data):\n    """""" Note in this class L == P """"""\n\n    M, L = 4, 3\n    W = np.eye(L)\n\n    G = np.hstack(\n        [0.5 * np.sin(3 * Data.X) + Data.X, 3.0 * np.cos(Data.X) - Data.X, 1.0 + Data.X]\n    )  # [N, P]\n\n    mu_data = tf.random.uniform((M, L), dtype=tf.float64)  # [M, L]\n    sqrt_data = create_q_sqrt(M, L)  # [L, M, M]\n\n    mu_data_full = tf.reshape(mu_data @ W, [-1, 1])  # [L, 1]\n    sqrt_data_full = expand_cov(sqrt_data, W)  # [1, LM, LM]\n\n    Y = tf.convert_to_tensor(G @ W)\n    G = tf.convert_to_tensor(G)\n    W = tf.convert_to_tensor(W)\n    sqrt_data = tf.convert_to_tensor(sqrt_data)\n    sqrt_data_full = tf.convert_to_tensor(sqrt_data_full)\n    Y += tf.random.normal(Y.shape, dtype=tf.float64) * tf.ones((L,), dtype=tf.float64) * 0.2\n    data = (Data.X, Y)\n\n\nclass DataMixedKernel(Data):\n    M = 5\n    L = 2\n    P = 3\n    W = rng.randn(P, L)\n    G = np.hstack([0.5 * np.sin(3 * Data.X) + Data.X, 3.0 * np.cos(Data.X) - Data.X])  # [N, L]\n\n    mu_data = tf.random.normal((M, L), dtype=tf.float64)  # [M, L]\n    sqrt_data = create_q_sqrt(M, L)  # [L, M, M]\n\n    Y = tf.convert_to_tensor(G @ W.T)\n    G = tf.convert_to_tensor(G)\n    W = tf.convert_to_tensor(W)\n    sqrt_data = tf.convert_to_tensor(sqrt_data)\n    Y += tf.random.normal(Y.shape, dtype=tf.float64) * tf.ones((P,), dtype=tf.float64) * 0.1\n    data = (Data.X, Y)\n\n\n# ------------------------------------------\n# Test sample conditional\n# ------------------------------------------\n\n\n@pytest.mark.parametrize(""full_cov"", [True, False])\ndef test_sample_mvn(full_cov):\n    """"""\n    Draws 10,000 samples from a distribution\n    with known mean and covariance. The test checks\n    if the mean and covariance of the samples is\n    close to the true mean and covariance.\n    """"""\n    N, D = 10000, 2\n    means = tf.ones((N, D), dtype=float_type)\n    if full_cov:\n        covs = tf.eye(D, batch_shape=[N], dtype=float_type)\n    else:\n        covs = tf.ones((N, D), dtype=float_type)\n\n    samples = sample_mvn(means, covs, full_cov)\n    samples_mean = np.mean(samples, axis=0)\n    samples_cov = np.cov(samples, rowvar=False)\n\n    np.testing.assert_array_almost_equal(samples_mean, [1.0, 1.0], decimal=1)\n    np.testing.assert_array_almost_equal(samples_cov, [[1.0, 0.0], [0.0, 1.0]], decimal=1)\n\n\n@pytest.mark.parametrize(""whiten"", [True, False])\n@pytest.mark.parametrize(""full_cov"", [True, False])\n@pytest.mark.parametrize(""full_output_cov"", [True, False])\ndef test_sample_conditional(whiten, full_cov, full_output_cov):\n    if full_cov and full_output_cov:\n        return\n\n    q_mu = tf.random.uniform((Data.M, Data.P), dtype=tf.float64)  # [M, P]\n    q_sqrt = tf.convert_to_tensor(\n        [np.tril(tf.random.uniform((Data.M, Data.M), dtype=tf.float64)) for _ in range(Data.P)]\n    )  # [P, M, M]\n\n    Z = Data.X[: Data.M, ...]  # [M, D]\n    Xs = np.ones((Data.N, Data.D), dtype=float_type)\n\n    inducing_variable = InducingPoints(Z)\n    kernel = SquaredExponential()\n\n    # Path 1\n    value_f, mean_f, var_f = sample_conditional(\n        Xs,\n        inducing_variable,\n        kernel,\n        q_mu,\n        q_sqrt=q_sqrt,\n        white=whiten,\n        full_cov=full_cov,\n        full_output_cov=full_output_cov,\n        num_samples=int(1e5),\n    )\n    value_f = value_f.numpy().reshape((-1,) + value_f.numpy().shape[2:])\n\n    # Path 2\n    if full_output_cov:\n        pytest.skip(\n            ""sample_conditional with X instead of inducing_variable does not support full_output_cov""\n        )\n\n    value_x, mean_x, var_x = sample_conditional(\n        Xs,\n        Z,\n        kernel,\n        q_mu,\n        q_sqrt=q_sqrt,\n        white=whiten,\n        full_cov=full_cov,\n        full_output_cov=full_output_cov,\n        num_samples=int(1e5),\n    )\n    value_x = value_x.numpy().reshape((-1,) + value_x.numpy().shape[2:])\n\n    # check if mean and covariance of samples are similar\n    np.testing.assert_array_almost_equal(\n        np.mean(value_x, axis=0), np.mean(value_f, axis=0), decimal=1\n    )\n    np.testing.assert_array_almost_equal(\n        np.cov(value_x, rowvar=False), np.cov(value_f, rowvar=False), decimal=1\n    )\n    np.testing.assert_allclose(mean_x, mean_f)\n    np.testing.assert_allclose(var_x, var_f)\n\n\ndef test_sample_conditional_mixedkernel():\n    q_mu = tf.random.uniform((Data.M, Data.L), dtype=tf.float64)  # M x L\n    q_sqrt = tf.convert_to_tensor(\n        [np.tril(tf.random.uniform((Data.M, Data.M), dtype=tf.float64)) for _ in range(Data.L)]\n    )  # L x M x M\n\n    Z = Data.X[: Data.M, ...]  # M x D\n    N = int(10e5)\n    Xs = np.ones((N, Data.D), dtype=float_type)\n\n    # Path 1: mixed kernel: most efficient route\n    W = np.random.randn(Data.P, Data.L)\n    mixed_kernel = mk.LinearCoregionalization([SquaredExponential() for _ in range(Data.L)], W)\n    optimal_inducing_variable = mf.SharedIndependentInducingVariables(InducingPoints(Z))\n\n    value, mean, var = sample_conditional(\n        Xs, optimal_inducing_variable, mixed_kernel, q_mu, q_sqrt=q_sqrt, white=True\n    )\n\n    # Path 2: independent kernels, mixed later\n    separate_kernel = mk.SeparateIndependent([SquaredExponential() for _ in range(Data.L)])\n    fallback_inducing_variable = mf.SharedIndependentInducingVariables(InducingPoints(Z))\n\n    value2, mean2, var2 = sample_conditional(\n        Xs, fallback_inducing_variable, separate_kernel, q_mu, q_sqrt=q_sqrt, white=True\n    )\n    value2 = np.matmul(value2, W.T)\n    # check if mean and covariance of samples are similar\n    np.testing.assert_array_almost_equal(np.mean(value, axis=0), np.mean(value2, axis=0), decimal=1)\n    np.testing.assert_array_almost_equal(\n        np.cov(value, rowvar=False), np.cov(value2, rowvar=False), decimal=1\n    )\n\n\n@pytest.mark.parametrize(\n    ""func, R"",\n    [\n        (fully_correlated_conditional_repeat, 5),\n        (fully_correlated_conditional_repeat, 1),\n        (fully_correlated_conditional, 1),\n    ],\n)\ndef test_fully_correlated_conditional_repeat_shapes(func, R):\n    L, M, N, P = Data.L, Data.M, Data.N, Data.P\n\n    Kmm = tf.ones((L * M, L * M)) + default_jitter() * tf.eye(L * M)\n    Kmn = tf.ones((L * M, N, P))\n    Knn = tf.ones((N, P))\n    f = tf.ones((L * M, R))\n    q_sqrt = None\n    white = True\n\n    m, v = func(\n        Kmn, Kmm, Knn, f, full_cov=False, full_output_cov=False, q_sqrt=q_sqrt, white=white,\n    )\n\n    assert v.shape.as_list() == m.shape.as_list()\n\n\n# ------------------------------------------\n# Test Mok Output Dims\n# ------------------------------------------\n\n\ndef test_shapes_of_mok():\n    data = DataMixedKernel\n\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n\n    k1 = mk.LinearCoregionalization(kern_list, W=data.W)\n    assert k1.num_latent_gps == data.L\n\n    k2 = mk.SeparateIndependent(kern_list)\n    assert k2.num_latent_gps == data.L\n\n    dims = 5\n    k3 = mk.SharedIndependent(SquaredExponential(), dims)\n    assert k3.num_latent_gps == dims\n\n\n# ------------------------------------------\n# Test Mixed Mok Kgg\n# ------------------------------------------\n\n\ndef test_MixedMok_Kgg():\n    data = DataMixedKernel\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n    kernel = mk.LinearCoregionalization(kern_list, W=data.W)\n\n    Kgg = kernel.Kgg(Data.X, Data.X)  # L x N x N\n    Kff = kernel.K(Data.X, Data.X)  # N x P x N x P\n\n    # Kff = W @ Kgg @ W^T\n    Kff_infered = np.einsum(""lnm,pl,ql->npmq"", Kgg, data.W, data.W)\n\n    np.testing.assert_array_almost_equal(Kff, Kff_infered, decimal=5)\n\n\n# ------------------------------------------\n# Integration tests\n# ------------------------------------------\n\n\ndef test_shared_independent_mok():\n    """"""\n    In this test we use the same kernel and the same inducing inducing\n    for each of the outputs. The outputs are considered to be uncorrelated.\n    This is how GPflow handled multiple outputs before the multioutput framework was added.\n    We compare three models here:\n        1) an ineffient one, where we use a SharedIndepedentMok with InducingPoints.\n           This combination will uses a Kff of size N x P x N x P, Kfu if size N x P x M x P\n           which is extremely inefficient as most of the elements are zero.\n        2) efficient: SharedIndependentMok and SharedIndependentMof\n           This combinations uses the most efficient form of matrices\n        3) the old way, efficient way: using Kernel and InducingPoints\n        Model 2) and 3) follow more or less the same code path.\n    """"""\n    np.random.seed(0)\n    # Model 1\n    q_mu_1 = np.random.randn(Data.M * Data.P, 1)  # MP x 1\n    q_sqrt_1 = np.tril(np.random.randn(Data.M * Data.P, Data.M * Data.P))[None, ...]  # 1 x MP x MP\n    kernel_1 = mk.SharedIndependent(SquaredExponential(variance=0.5, lengthscales=1.2), Data.P)\n    inducing_variable = InducingPoints(Data.X[: Data.M, ...])\n    model_1 = SVGP(\n        kernel_1,\n        Gaussian(),\n        inducing_variable,\n        q_mu=q_mu_1,\n        q_sqrt=q_sqrt_1,\n        num_latent_gps=Data.Y.shape[-1],\n    )\n    set_trainable(model_1, False)\n    set_trainable(model_1.q_sqrt, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_1.training_loss_closure(Data.data),\n        variables=model_1.trainable_variables,\n        options=dict(maxiter=500),\n        method=""BFGS"",\n        compile=True,\n    )\n\n    # Model 2\n    q_mu_2 = np.reshape(q_mu_1, [Data.M, Data.P])  # M x P\n    q_sqrt_2 = np.array(\n        [np.tril(np.random.randn(Data.M, Data.M)) for _ in range(Data.P)]\n    )  # P x M x M\n    kernel_2 = SquaredExponential(variance=0.5, lengthscales=1.2)\n    inducing_variable_2 = InducingPoints(Data.X[: Data.M, ...])\n    model_2 = SVGP(\n        kernel_2,\n        Gaussian(),\n        inducing_variable_2,\n        num_latent_gps=Data.P,\n        q_mu=q_mu_2,\n        q_sqrt=q_sqrt_2,\n    )\n    set_trainable(model_2, False)\n    set_trainable(model_2.q_sqrt, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_2.training_loss_closure(Data.data),\n        variables=model_2.trainable_variables,\n        options=dict(maxiter=500),\n        method=""BFGS"",\n        compile=True,\n    )\n\n    # Model 3\n    q_mu_3 = np.reshape(q_mu_1, [Data.M, Data.P])  # M x P\n    q_sqrt_3 = np.array(\n        [np.tril(np.random.randn(Data.M, Data.M)) for _ in range(Data.P)]\n    )  # P x M x M\n    kernel_3 = mk.SharedIndependent(SquaredExponential(variance=0.5, lengthscales=1.2), Data.P)\n    inducing_variable_3 = mf.SharedIndependentInducingVariables(\n        InducingPoints(Data.X[: Data.M, ...])\n    )\n    model_3 = SVGP(\n        kernel_3,\n        Gaussian(),\n        inducing_variable_3,\n        num_latent_gps=Data.P,\n        q_mu=q_mu_3,\n        q_sqrt=q_sqrt_3,\n    )\n    set_trainable(model_3, False)\n    set_trainable(model_3.q_sqrt, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_3.training_loss_closure(Data.data),\n        variables=model_3.trainable_variables,\n        options=dict(maxiter=500),\n        method=""BFGS"",\n        compile=True,\n    )\n\n    check_equality_predictions(Data.data, [model_1, model_2, model_3])\n\n\ndef test_separate_independent_mok():\n    """"""\n    We use different independent kernels for each of the output dimensions.\n    We can achieve this in two ways:\n        1) efficient: SeparateIndependentMok with Shared/SeparateIndependentMof\n        2) inefficient: SeparateIndependentMok with InducingPoints\n    However, both methods should return the same conditional,\n    and after optimization return the same log likelihood.\n    """"""\n    # Model 1 (Inefficient)\n    q_mu_1 = np.random.randn(Data.M * Data.P, 1)\n    q_sqrt_1 = np.tril(np.random.randn(Data.M * Data.P, Data.M * Data.P))[None, ...]  # 1 x MP x MP\n\n    kern_list_1 = [SquaredExponential(variance=0.5, lengthscales=1.2) for _ in range(Data.P)]\n    kernel_1 = mk.SeparateIndependent(kern_list_1)\n    inducing_variable_1 = InducingPoints(Data.X[: Data.M, ...])\n    model_1 = SVGP(\n        kernel_1, Gaussian(), inducing_variable_1, num_latent_gps=1, q_mu=q_mu_1, q_sqrt=q_sqrt_1,\n    )\n    set_trainable(model_1, False)\n    set_trainable(model_1.q_sqrt, True)\n    set_trainable(model_1.q_mu, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_1.training_loss_closure(Data.data),\n        variables=model_1.trainable_variables,\n        method=""BFGS"",\n        compile=True,\n    )\n\n    # Model 2 (efficient)\n    q_mu_2 = np.random.randn(Data.M, Data.P)\n    q_sqrt_2 = np.array(\n        [np.tril(np.random.randn(Data.M, Data.M)) for _ in range(Data.P)]\n    )  # P x M x M\n    kern_list_2 = [SquaredExponential(variance=0.5, lengthscales=1.2) for _ in range(Data.P)]\n    kernel_2 = mk.SeparateIndependent(kern_list_2)\n    inducing_variable_2 = mf.SharedIndependentInducingVariables(\n        InducingPoints(Data.X[: Data.M, ...])\n    )\n    model_2 = SVGP(\n        kernel_2,\n        Gaussian(),\n        inducing_variable_2,\n        num_latent_gps=Data.P,\n        q_mu=q_mu_2,\n        q_sqrt=q_sqrt_2,\n    )\n    set_trainable(model_2, False)\n    set_trainable(model_2.q_sqrt, True)\n    set_trainable(model_2.q_mu, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_2.training_loss_closure(Data.data),\n        variables=model_2.trainable_variables,\n        method=""BFGS"",\n        compile=True,\n    )\n\n    check_equality_predictions(Data.data, [model_1, model_2])\n\n\ndef test_separate_independent_mof():\n    """"""\n    Same test as above but we use different (i.e. separate) inducing inducing\n    for each of the output dimensions.\n    """"""\n    np.random.seed(0)\n\n    # Model 1 (INefficient)\n    q_mu_1 = np.random.randn(Data.M * Data.P, 1)\n    q_sqrt_1 = np.tril(np.random.randn(Data.M * Data.P, Data.M * Data.P))[None, ...]  # 1 x MP x MP\n\n    kernel_1 = mk.SharedIndependent(SquaredExponential(variance=0.5, lengthscales=1.2), Data.P)\n    inducing_variable_1 = InducingPoints(Data.X[: Data.M, ...])\n    model_1 = SVGP(kernel_1, Gaussian(), inducing_variable_1, q_mu=q_mu_1, q_sqrt=q_sqrt_1)\n    set_trainable(model_1, False)\n    set_trainable(model_1.q_sqrt, True)\n    set_trainable(model_1.q_mu, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_1.training_loss_closure(Data.data),\n        variables=model_1.trainable_variables,\n        method=""BFGS"",\n        compile=True,\n    )\n\n    # Model 2 (efficient)\n    q_mu_2 = np.random.randn(Data.M, Data.P)\n    q_sqrt_2 = np.array(\n        [np.tril(np.random.randn(Data.M, Data.M)) for _ in range(Data.P)]\n    )  # P x M x M\n    kernel_2 = mk.SharedIndependent(SquaredExponential(variance=0.5, lengthscales=1.2), Data.P)\n    inducing_variable_list_2 = [InducingPoints(Data.X[: Data.M, ...]) for _ in range(Data.P)]\n    inducing_variable_2 = mf.SeparateIndependentInducingVariables(inducing_variable_list_2)\n    model_2 = SVGP(kernel_2, Gaussian(), inducing_variable_2, q_mu=q_mu_2, q_sqrt=q_sqrt_2)\n    set_trainable(model_2, False)\n    set_trainable(model_2.q_sqrt, True)\n    set_trainable(model_2.q_mu, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_2.training_loss_closure(Data.data),\n        variables=model_2.trainable_variables,\n        method=""BFGS"",\n        compile=True,\n    )\n\n    # Model 3 (Inefficient): an idenitical inducing variable is used P times,\n    # and treated as a separate one.\n    q_mu_3 = np.random.randn(Data.M, Data.P)\n    q_sqrt_3 = np.array(\n        [np.tril(np.random.randn(Data.M, Data.M)) for _ in range(Data.P)]\n    )  # P x M x M\n    kern_list = [SquaredExponential(variance=0.5, lengthscales=1.2) for _ in range(Data.P)]\n    kernel_3 = mk.SeparateIndependent(kern_list)\n    inducing_variable_list_3 = [InducingPoints(Data.X[: Data.M, ...]) for _ in range(Data.P)]\n    inducing_variable_3 = mf.SeparateIndependentInducingVariables(inducing_variable_list_3)\n    model_3 = SVGP(kernel_3, Gaussian(), inducing_variable_3, q_mu=q_mu_3, q_sqrt=q_sqrt_3)\n    set_trainable(model_3, False)\n    set_trainable(model_3.q_sqrt, True)\n    set_trainable(model_3.q_mu, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_3.training_loss_closure(Data.data),\n        variables=model_3.trainable_variables,\n        method=""BFGS"",\n        compile=True,\n    )\n\n    check_equality_predictions(Data.data, [model_1, model_2, model_3])\n\n\ndef test_mixed_mok_with_Id_vs_independent_mok():\n    data = DataMixedKernelWithEye\n    # Independent model\n    k1 = mk.SharedIndependent(SquaredExponential(variance=0.5, lengthscales=1.2), data.L)\n    f1 = InducingPoints(data.X[: data.M, ...])\n    model_1 = SVGP(k1, Gaussian(), f1, q_mu=data.mu_data_full, q_sqrt=data.sqrt_data_full)\n    set_trainable(model_1, False)\n    set_trainable(model_1.q_sqrt, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_1.training_loss_closure(Data.data),\n        variables=model_1.trainable_variables,\n        method=""BFGS"",\n        compile=True,\n    )\n\n    # Mixed Model\n    kern_list = [SquaredExponential(variance=0.5, lengthscales=1.2) for _ in range(data.L)]\n    k2 = mk.LinearCoregionalization(kern_list, data.W)\n    f2 = InducingPoints(data.X[: data.M, ...])\n    model_2 = SVGP(k2, Gaussian(), f2, q_mu=data.mu_data_full, q_sqrt=data.sqrt_data_full)\n    set_trainable(model_2, False)\n    set_trainable(model_2.q_sqrt, True)\n\n    gpflow.optimizers.Scipy().minimize(\n        model_2.training_loss_closure(Data.data),\n        variables=model_2.trainable_variables,\n        method=""BFGS"",\n        compile=True,\n    )\n\n    check_equality_predictions(Data.data, [model_1, model_2])\n\n\ndef test_compare_mixed_kernel():\n    data = DataMixedKernel\n\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n    k1 = mk.LinearCoregionalization(kern_list, W=data.W)\n    f1 = mf.SharedIndependentInducingVariables(InducingPoints(data.X[: data.M, ...]))\n    model_1 = SVGP(k1, Gaussian(), inducing_variable=f1, q_mu=data.mu_data, q_sqrt=data.sqrt_data)\n\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n    k2 = mk.LinearCoregionalization(kern_list, W=data.W)\n    f2 = mf.SharedIndependentInducingVariables(InducingPoints(data.X[: data.M, ...]))\n    model_2 = SVGP(k2, Gaussian(), inducing_variable=f2, q_mu=data.mu_data, q_sqrt=data.sqrt_data)\n\n    check_equality_predictions(Data.data, [model_1, model_2])\n\n\ndef test_multioutput_with_diag_q_sqrt():\n    data = DataMixedKernel\n\n    q_sqrt_diag = np.ones((data.M, data.L)) * 2\n    q_sqrt = np.repeat(np.eye(data.M)[None, ...], data.L, axis=0) * 2  # L x M x M\n\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n    k1 = mk.LinearCoregionalization(kern_list, W=data.W)\n    f1 = mf.SharedIndependentInducingVariables(InducingPoints(data.X[: data.M, ...]))\n    model_1 = SVGP(\n        k1, Gaussian(), inducing_variable=f1, q_mu=data.mu_data, q_sqrt=q_sqrt_diag, q_diag=True,\n    )\n\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n    k2 = mk.LinearCoregionalization(kern_list, W=data.W)\n    f2 = mf.SharedIndependentInducingVariables(InducingPoints(data.X[: data.M, ...]))\n    model_2 = SVGP(\n        k2, Gaussian(), inducing_variable=f2, q_mu=data.mu_data, q_sqrt=q_sqrt, q_diag=False,\n    )\n\n    check_equality_predictions(Data.data, [model_1, model_2])\n\n\ndef test_MixedKernelSeparateMof():\n    data = DataMixedKernel\n\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n    inducing_variable_list = [InducingPoints(data.X[: data.M, ...]) for _ in range(data.L)]\n    k1 = mk.LinearCoregionalization(kern_list, W=data.W)\n    f1 = mf.SeparateIndependentInducingVariables(inducing_variable_list)\n    model_1 = SVGP(k1, Gaussian(), inducing_variable=f1, q_mu=data.mu_data, q_sqrt=data.sqrt_data)\n\n    kern_list = [SquaredExponential() for _ in range(data.L)]\n    inducing_variable_list = [InducingPoints(data.X[: data.M, ...]) for _ in range(data.L)]\n    k2 = mk.LinearCoregionalization(kern_list, W=data.W)\n    f2 = mf.SeparateIndependentInducingVariables(inducing_variable_list)\n    model_2 = SVGP(k2, Gaussian(), inducing_variable=f2, q_mu=data.mu_data, q_sqrt=data.sqrt_data)\n\n    check_equality_predictions(Data.data, [model_1, model_2])\n'"
tests/gpflow/conditionals/test_uncertain_conditional.py,10,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import namedtuple\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.mean_functions import Zero, Constant, Linear\nfrom gpflow.conditionals import conditional\nfrom gpflow.conditionals import uncertain_conditional\nfrom gpflow.optimizers import Scipy\nfrom gpflow.quadrature import mvnquad\nfrom gpflow.config import default_float\nfrom gpflow.utilities import training_loop\n\nrng = np.random.RandomState(1)\n\n# ------------------------------------------\n# Helpers\n# ------------------------------------------\n\n\nclass MomentMatchingSVGP(gpflow.models.SVGP):\n    def uncertain_predict_f_moment_matching(self, Xmu, Xcov):\n        return uncertain_conditional(\n            Xmu,\n            Xcov,\n            self.inducing_variable,\n            self.kernel,\n            self.q_mu,\n            self.q_sqrt,\n            mean_function=self.mean_function,\n            white=self.whiten,\n            full_output_cov=self.full_output_cov,\n        )\n\n    def uncertain_predict_f_monte_carlo(self, Xmu, Xchol, mc_iter=int(1e6)):\n        D_in = Xchol.shape[0]\n        X_samples = Xmu + np.reshape(\n            Xchol[None, :, :] @ rng.randn(mc_iter, D_in)[:, :, None], [mc_iter, D_in]\n        )\n        F_mu, F_var = self.predict_f(X_samples)\n        F_samples = (F_mu + rng.randn(*F_var.shape) * (F_var ** 0.5)).numpy()\n        mean = np.mean(F_samples, axis=0)\n        covar = np.cov(F_samples.T)\n        return mean, covar\n\n\ndef gen_L(n, *shape):\n    return np.array([np.tril(rng.randn(*shape)) for _ in range(n)])\n\n\ndef gen_q_sqrt(D_out, *shape):\n    return tf.convert_to_tensor(\n        np.array([np.tril(rng.randn(*shape)) for _ in range(D_out)]), dtype=default_float(),\n    )\n\n\ndef mean_function_factory(mean_function_name, D_in, D_out):\n    if mean_function_name == ""Zero"":\n        return Zero(output_dim=D_out)\n    elif mean_function_name == ""Constant"":\n        return Constant(c=rng.rand(D_out))\n    elif mean_function_name == ""Linear"":\n        return Linear(A=rng.rand(D_in, D_out), b=rng.rand(D_out))\n    else:\n        return None\n\n\n# ------------------------------------------\n# Data classes: storing constants\n# ------------------------------------------\n\n\nclass Data:\n    N = 7\n    N_new = 2\n    D_out = 3\n    D_in = 1\n    X = np.linspace(-5, 5, N)[:, None] + rng.randn(N, 1)\n    Y = np.hstack([np.sin(X), np.cos(X), X ** 2])\n    Xnew_mu = rng.randn(N_new, 1)\n    Xnew_covar = np.zeros((N_new, 1, 1))\n    data = (X, Y)\n\n\nclass DataMC1(Data):\n    Y = np.hstack([np.sin(Data.X), np.sin(Data.X) * 2, Data.X ** 2])\n    data = (Data.X, Y)\n\n\nclass DataMC2(Data):\n    N = 7\n    N_new = 5\n    D_out = 4\n    D_in = 2\n    X = rng.randn(N, D_in)\n    Y = np.hstack([np.sin(X), np.sin(X)])\n    Xnew_mu = rng.randn(N_new, D_in)\n    L = gen_L(N_new, D_in, D_in)\n    Xnew_covar = np.array([l @ l.T for l in L])\n    data = (X, Y)\n\n\nclass DataQuad:\n    num_data = 10\n    num_ind = 10\n    D_in = 2\n    D_out = 3\n    H = 150\n    Xmu = tf.convert_to_tensor(rng.randn(num_data, D_in), dtype=default_float())\n    L = gen_L(num_data, D_in, D_in)\n    Xvar = tf.convert_to_tensor(np.array([l @ l.T for l in L]), dtype=default_float())\n    Z = rng.randn(num_ind, D_in)\n    q_mu = tf.convert_to_tensor(rng.randn(num_ind, D_out), dtype=default_float())\n    q_sqrt = gen_q_sqrt(D_out, num_ind, num_ind)\n\n\nMEANS = [""Constant"", ""Linear"", ""Zero"", None]\n\n\n@pytest.mark.parametrize(""white"", [True, False])\n@pytest.mark.parametrize(""mean"", MEANS)\ndef test_no_uncertainty(white, mean):\n    mean_function = mean_function_factory(mean, Data.D_in, Data.D_out)\n    kernel = gpflow.kernels.SquaredExponential(variance=rng.rand())\n    model = MomentMatchingSVGP(\n        kernel,\n        gpflow.likelihoods.Gaussian(),\n        num_latent_gps=Data.D_out,\n        mean_function=mean_function,\n        inducing_variable=Data.X.copy(),\n        whiten=white,\n    )\n    model.full_output_cov = False\n\n    training_loop(\n        model.training_loss_closure(Data.data),\n        optimizer=tf.optimizers.Adam(),\n        var_list=model.trainable_variables,\n        maxiter=100,\n        compile=True,\n    )\n\n    mean1, var1 = model.predict_f(Data.Xnew_mu)\n    mean2, var2 = model.uncertain_predict_f_moment_matching(\n        *map(tf.convert_to_tensor, [Data.Xnew_mu, Data.Xnew_covar])\n    )\n\n    assert_allclose(mean1, mean2)\n    for n in range(Data.N_new):\n        assert_allclose(var1[n, :], var2[n, ...])\n\n\n@pytest.mark.parametrize(""white"", [True, False])\n@pytest.mark.parametrize(""mean"", MEANS)\ndef test_monte_carlo_1_din(white, mean):\n    kernel = gpflow.kernels.SquaredExponential(variance=rng.rand())\n    mean_function = mean_function_factory(mean, DataMC1.D_in, DataMC1.D_out)\n    model = MomentMatchingSVGP(\n        kernel,\n        gpflow.likelihoods.Gaussian(),\n        num_latent_gps=DataMC1.D_out,\n        mean_function=mean_function,\n        inducing_variable=DataMC1.X.copy(),\n        whiten=white,\n    )\n    model.full_output_cov = True\n\n    training_loop(\n        model.training_loss_closure(DataMC1.data),\n        optimizer=tf.optimizers.Adam(),\n        var_list=model.trainable_variables,\n        maxiter=200,\n        compile=True,\n    )\n\n    mean1, var1 = model.uncertain_predict_f_moment_matching(\n        *map(tf.convert_to_tensor, [DataMC1.Xnew_mu, DataMC1.Xnew_covar])\n    )\n\n    for n in range(DataMC1.N_new):\n        mean2, var2 = model.uncertain_predict_f_monte_carlo(\n            DataMC1.Xnew_mu[n, ...], DataMC1.Xnew_covar[n, ...] ** 0.5\n        )\n        assert_allclose(mean1[n, ...], mean2, atol=1e-3, rtol=1e-1)\n        assert_allclose(var1[n, ...], var2, atol=1e-2, rtol=1e-1)\n\n\n@pytest.mark.parametrize(""white"", [True, False])\n@pytest.mark.parametrize(""mean"", MEANS)\ndef test_monte_carlo_2_din(white, mean):\n    kernel = gpflow.kernels.SquaredExponential(variance=rng.rand())\n    mean_function = mean_function_factory(mean, DataMC2.D_in, DataMC2.D_out)\n    model = MomentMatchingSVGP(\n        kernel,\n        gpflow.likelihoods.Gaussian(),\n        num_latent_gps=DataMC2.D_out,\n        mean_function=mean_function,\n        inducing_variable=DataMC2.X.copy(),\n        whiten=white,\n    )\n    model.full_output_cov = True\n\n    training_loop(\n        model.training_loss_closure(DataMC2.data),\n        optimizer=tf.optimizers.Adam(),\n        var_list=model.trainable_variables,\n        maxiter=100,\n        compile=True,\n    )\n\n    mean1, var1 = model.uncertain_predict_f_moment_matching(\n        *map(tf.convert_to_tensor, [DataMC2.Xnew_mu, DataMC2.Xnew_covar])\n    )\n\n    for n in range(DataMC2.N_new):\n        mean2, var2 = model.uncertain_predict_f_monte_carlo(\n            DataMC2.Xnew_mu[n, ...], DataMC2.L[n, ...]\n        )\n        assert_allclose(mean1[n, ...], mean2, atol=1e-2)\n        assert_allclose(var1[n, ...], var2, atol=1e-2)\n\n\n@pytest.mark.parametrize(""mean"", MEANS)\n@pytest.mark.parametrize(""white"", [True, False])\ndef test_quadrature(white, mean):\n    kernel = gpflow.kernels.SquaredExponential()\n    inducing_variable = gpflow.inducing_variables.InducingPoints(DataQuad.Z)\n    mean_function = mean_function_factory(mean, DataQuad.D_in, DataQuad.D_out)\n\n    effective_mean = mean_function or (lambda X: 0.0)\n\n    def conditional_fn(X):\n        return conditional(\n            X, inducing_variable, kernel, DataQuad.q_mu, q_sqrt=DataQuad.q_sqrt, white=white,\n        )\n\n    def mean_fn(X):\n        return conditional_fn(X)[0] + effective_mean(X)\n\n    def var_fn(X):\n        return conditional_fn(X)[1]\n\n    quad_args = (\n        DataQuad.Xmu,\n        DataQuad.Xvar,\n        DataQuad.H,\n        DataQuad.D_in,\n        (DataQuad.D_out,),\n    )\n    mean_quad = mvnquad(mean_fn, *quad_args)\n    var_quad = mvnquad(var_fn, *quad_args)\n\n    def mean_sq_fn(X):\n        return mean_fn(X) ** 2\n\n    mean_sq_quad = mvnquad(mean_sq_fn, *quad_args)\n    var_quad = var_quad + (mean_sq_quad - mean_quad ** 2)\n\n    mean_analytic, var_analytic = uncertain_conditional(\n        DataQuad.Xmu,\n        DataQuad.Xvar,\n        inducing_variable,\n        kernel,\n        DataQuad.q_mu,\n        DataQuad.q_sqrt,\n        mean_function=mean_function,\n        full_output_cov=False,\n        white=white,\n    )\n\n    assert_allclose(mean_quad, mean_analytic, rtol=1e-6)\n    assert_allclose(var_quad, var_analytic, rtol=1e-6)\n'"
tests/gpflow/conditionals/test_util.py,10,"b'# Copyright 2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose, assert_equal\n\nfrom gpflow import default_float\nfrom gpflow.conditionals.util import leading_transpose, rollaxis_left, rollaxis_right, sample_mvn\n\n\ndef test_leading_transpose():\n    dims = [1, 2, 3, 4]\n    a = tf.zeros(dims)\n    b = leading_transpose(a, [..., -1, -2])\n    c = leading_transpose(a, [-1, ..., -2])\n    d = leading_transpose(a, [-1, -2, ...])\n    e = leading_transpose(a, [3, 2, ...])\n    f = leading_transpose(a, [3, -2, ...])\n\n    assert len(a.shape) == len(b.shape) == len(c.shape) == len(d.shape)\n    assert len(a.shape) == len(e.shape) == len(f.shape)\n    assert b.shape[-2:] == [4, 3]\n    assert c.shape[0] == 4 and c.shape[-1] == 3\n    assert d.shape[:2] == [4, 3]\n    assert d.shape == e.shape == f.shape\n\n\ndef test_leading_transpose_fails():\n    """""" Check that error is thrown if `perm` is not compatible with `a` """"""\n    dims = [1, 2, 3, 4]\n    a = tf.zeros(dims)\n\n    with pytest.raises(ValueError):\n        leading_transpose(a, [-1, -2])\n\n\ndef test_leading_transpose_with_tf_function_wrapper(caplog):\n    """""" Check that no warnings are thrown when compiling `leading_transpose` """"""\n    dims = [1, 2, 3, 4]\n    a = tf.zeros(dims)\n\n    @tf.function\n    def compiled_wrapper():\n        return leading_transpose(a, [..., -1, -2])\n\n    compiled_wrapper()\n\n    # When Autograph cannot compile a function it sends a WARNING message to the\n    # logs.\n    log_message_levels = [record.levelname for record in caplog.records]\n    assert ""WARNING"" not in log_message_levels\n\n\n# rollaxis\n@pytest.mark.parametrize(""rolls"", [1, 2])\n@pytest.mark.parametrize(""direction"", [""left"", ""right""])\ndef test_rollaxis(rolls, direction):\n    A = np.random.randn(10, 5, 3)\n    A_tf = tf.convert_to_tensor(A)\n\n    if direction == ""left"":\n        perm = [1, 2, 0] if rolls == 1 else [2, 0, 1]\n    elif direction == ""right"":\n        perm = [2, 0, 1] if rolls == 1 else [1, 2, 0]\n    else:\n        raise NotImplementedError\n\n    A_rolled_ref = np.transpose(A, perm)\n\n    if direction == ""left"":\n        A_rolled_tf = rollaxis_left(A_tf, rolls)\n    elif direction == ""right"":\n        A_rolled_tf = rollaxis_right(A_tf, rolls)\n    else:\n        raise NotImplementedError\n\n    assert_allclose(A_rolled_ref, A_rolled_tf)\n\n\n@pytest.mark.parametrize(""rolls"", [1, 2])\ndef test_rollaxis_idempotent(rolls):\n    A = np.random.randn(10, 5, 3, 20, 1)\n    A_tf = tf.convert_to_tensor(A)\n    A_left_right = rollaxis_left(rollaxis_right(A_tf, 2), 2)\n    A_right_left = rollaxis_right(rollaxis_left(A_tf, 2), 2)\n\n    assert_allclose(A, A_left_right)\n    assert_allclose(A, A_right_left)\n\n\n@pytest.mark.parametrize(""leading_dims"", [tuple(), (1,), (5,)])\n@pytest.mark.parametrize(""n"", [1, 5])\n@pytest.mark.parametrize(""d"", [1, 5])\n@pytest.mark.parametrize(""num_samples"", [None, 1, 5])\n@pytest.mark.parametrize(""full_cov"", [True, False])\ndef test_sample_mvn_shapes(leading_dims, n, d, num_samples, full_cov):\n    mean_shape = leading_dims + (n, d)\n    means = tf.zeros(mean_shape, dtype=default_float())\n\n    if full_cov:\n        covariance_shape = leading_dims + (n, d, d)\n        sqrt_cov = tf.random.normal(covariance_shape, dtype=default_float())\n        covariances = tf.matmul(sqrt_cov, sqrt_cov, transpose_b=True)\n    else:\n        covariance_shape = leading_dims + (n, d)\n        covariances = tf.random.normal(covariance_shape, dtype=default_float())\n\n    samples = sample_mvn(means, covariances, full_cov, num_samples)\n\n    if num_samples:\n        expected_shape = leading_dims + (num_samples, n, d)\n    else:\n        expected_shape = leading_dims + (n, d)\n\n    assert_equal(samples.shape, expected_shape)\n'"
tests/gpflow/config/__init__.py,0,b''
tests/gpflow/config/test_config.py,7,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom unittest import mock\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nimport gpflow\nfrom gpflow.config import (\n    default_float,\n    default_int,\n    default_jitter,\n    default_positive_bijector,\n    default_summary_fmt,\n    set_default_float,\n    set_default_int,\n    set_default_jitter,\n    set_default_positive_bijector,\n    set_default_summary_fmt,\n)\nfrom gpflow.utilities import to_default_float, to_default_int\n\n\n_env_values = [\n    (""int"", ""int16"", np.int16),\n    (""int"", ""int64"", np.int64),\n    (""float"", ""float16"", np.float16),\n    (""float"", ""float32"", np.float32),\n    (""positive_bijector"", ""exp"", ""exp""),\n    (""positive_bijector"", ""softplus"", ""softplus""),\n    (""summary_fmt"", ""simple"", ""simple""),\n    (""positive_minimum"", ""1e-3"", 1e-3),\n    (""jitter"", ""1e-2"", 1e-2),\n]\n\n\n@pytest.mark.parametrize(""attr_name, value, expected_value"", _env_values)\ndef test_env_variables(attr_name, value, expected_value):\n    env_name = f""GPFLOW_{attr_name.upper()}""\n    with mock.patch.dict(""os.environ"", {env_name: value}):\n        assert os.environ[env_name] == value\n        config = gpflow.config.Config()\n        assert getattr(config, attr_name) == expected_value\n\n\n@pytest.mark.parametrize(""attr_name"", set(list(zip(*_env_values))[0]))\ndef test_env_variables_failures(attr_name):\n    if attr_name == ""summary_fmt"":\n        pytest.skip(""The `summary_fmt` validation cannot be performed."")\n    env_name = f""GPFLOW_{attr_name.upper()}""\n    with mock.patch.dict(""os.environ"", {env_name: ""garbage""}):\n        with pytest.raises(TypeError):\n            gpflow.config.Config()\n\n\n@pytest.mark.parametrize(\n    ""getter, setter, valid_type_1, valid_type_2"",\n    [\n        (default_int, set_default_int, tf.int64, np.int32),\n        (default_float, set_default_float, tf.float32, np.float64),\n    ],\n)\ndef test_dtype_setting(getter, setter, valid_type_1, valid_type_2):\n    if valid_type_1 == valid_type_2:\n        raise ValueError(""cannot test config setting/getting when both types are equal"")\n    setter(valid_type_1)\n    assert getter() == valid_type_1\n    setter(valid_type_2)\n    assert getter() == valid_type_2\n\n\n@pytest.mark.parametrize(\n    ""setter, invalid_type"",\n    [\n        (set_default_int, str),\n        (set_default_int, np.float64),\n        (set_default_float, list),\n        (set_default_float, tf.int32),\n    ],\n)\ndef test_dtype_errorcheck(setter, invalid_type):\n    with pytest.raises(TypeError):\n        setter(invalid_type)\n\n\ndef test_jitter_setting():\n    set_default_jitter(1e-3)\n    assert default_jitter() == 1e-3\n    set_default_jitter(1e-6)\n    assert default_jitter() == 1e-6\n\n\ndef test_jitter_errorcheck():\n    with pytest.raises(TypeError):\n        set_default_jitter(""not a float"")\n    with pytest.raises(ValueError):\n        set_default_jitter(-1e-10)\n\n\n@pytest.mark.parametrize(\n    ""value, error_msg"",\n    [\n        (""Unknown"", r""`unknown` not in set of valid bijectors: \\[\'exp\', \'softplus\'\\]""),\n        (1.0, r""`1.0` not in set of valid bijectors: \\[\'exp\', \'softplus\'\\]""),\n    ],\n)\ndef test_positive_bijector_error(value, error_msg):\n    with pytest.raises(ValueError, match=error_msg):\n        set_default_positive_bijector(value)\n\n\n@pytest.mark.parametrize(""value"", [""exp"", ""SoftPlus""])\ndef test_positive_bijector_setting(value):\n    set_default_positive_bijector(value)\n    assert default_positive_bijector() == value.lower()\n\n\ndef test_default_summary_fmt_setting():\n    set_default_summary_fmt(""html"")\n    assert default_summary_fmt() == ""html""\n    set_default_summary_fmt(None)\n    assert default_summary_fmt() is None\n\n\ndef test_default_summary_fmt_errorcheck():\n    with pytest.raises(ValueError):\n        set_default_summary_fmt(""this_format_definitely_does_not_exist"")\n\n\n@pytest.mark.parametrize(\n    ""setter, getter, converter, dtype, value"",\n    [\n        (set_default_int, default_int, to_default_int, np.int32, 3),\n        (set_default_int, default_int, to_default_int, tf.int32, 3),\n        (set_default_int, default_int, to_default_int, tf.int64, [3, 1, 4, 1, 5, 9]),\n        (set_default_int, default_int, to_default_int, np.int64, [3, 1, 4, 1, 5, 9]),\n        (set_default_float, default_float, to_default_float, np.float32, 3.14159),\n        (\n            set_default_float,\n            default_float,\n            to_default_float,\n            tf.float32,\n            [3.14159, 3.14159, 3.14159],\n        ),\n        (\n            set_default_float,\n            default_float,\n            to_default_float,\n            np.float64,\n            [3.14159, 3.14159, 3.14159],\n        ),\n        (\n            set_default_float,\n            default_float,\n            to_default_float,\n            tf.float64,\n            [3.14159, 3.14159, 3.14159],\n        ),\n    ],\n)\ndef test_native_to_default_dtype(setter, getter, converter, dtype, value):\n    with gpflow.config.as_context():\n        setter(dtype)\n        assert converter(value).dtype == dtype\n        assert converter(value).dtype == getter()\n'"
tests/gpflow/covariances/__init__.py,0,b''
tests/gpflow/covariances/test_base_covariances.py,0,"b'# Copyright 2017 Mark van der Wilk\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\nimport pytest\nimport gpflow\nfrom gpflow.inducing_variables import InducingPoints, Multiscale, InducingPatches\nfrom gpflow.covariances import Kuu, Kuf\nfrom gpflow.config import default_jitter\n\n\n@pytest.mark.parametrize(""N, D"", [[17, 3], [10, 7]])\ndef test_inducing_points_inducing_variable_len(N, D):\n    Z = np.random.randn(N, D)\n    inducing_variable = InducingPoints(Z)\n    assert_equal(len(inducing_variable), N)\n\n\n_kernel_setups = [\n    gpflow.kernels.SquaredExponential(variance=0.46, lengthscales=np.random.uniform(0.5, 3.0, 5)),\n    gpflow.kernels.Periodic(\n        base_kernel=gpflow.kernels.SquaredExponential(variance=1.8), period=0.4\n    ),\n]\n\n\n@pytest.mark.parametrize(""N"", [10, 101])\n@pytest.mark.parametrize(""kernel"", _kernel_setups)\ndef test_inducing_equivalence(N, kernel):\n    # Inducing inducing must be the same as the kernel evaluations\n    Z = np.random.randn(N, 5)\n    inducing_variable = InducingPoints(Z)\n    assert_allclose(Kuu(inducing_variable, kernel), kernel(Z))\n\n\n@pytest.mark.parametrize(""N, M, D"", [[23, 13, 3], [10, 5, 7]])\ndef test_multi_scale_inducing_equivalence_inducing_points(N, M, D):\n    # Multiscale must be equivalent to inducing points when variance is zero\n    Xnew, Z = np.random.randn(N, D), np.random.randn(M, D)\n    rbf = gpflow.kernels.SquaredExponential(1.3441, lengthscales=np.random.uniform(0.5, 3.0, D))\n    inducing_variable_zero_lengthscales = Multiscale(Z, scales=np.zeros(Z.shape) + 1e-10)\n    inducing_variable_inducing_point = InducingPoints(Z)\n\n    multi_scale_Kuf = Kuf(inducing_variable_zero_lengthscales, rbf, Xnew)\n    inducing_point_Kuf = Kuf(inducing_variable_inducing_point, rbf, Xnew)\n\n    relative_error_Kuf = np.abs(multi_scale_Kuf - inducing_point_Kuf) / inducing_point_Kuf\n    assert np.max(relative_error_Kuf) < 0.1e-2  # 0.1 %\n\n    multi_scale_Kuu = Kuu(inducing_variable_zero_lengthscales, rbf)\n    inducing_point_Kuu = Kuu(inducing_variable_inducing_point, rbf)\n\n    relative_error_Kuu = np.abs(multi_scale_Kuu - inducing_point_Kuu) / inducing_point_Kuu\n    assert np.max(relative_error_Kuu) < 0.1e-2  # 0.1 %\n\n\n_inducing_variables_and_kernels = [\n    [\n        2,\n        InducingPoints(np.random.randn(71, 2)),\n        gpflow.kernels.SquaredExponential(\n            variance=1.84, lengthscales=np.random.uniform(0.5, 3.0, 2)\n        ),\n    ],\n    [\n        2,\n        InducingPoints(np.random.randn(71, 2)),\n        gpflow.kernels.Matern12(variance=1.84, lengthscales=np.random.uniform(0.5, 3.0, 2)),\n    ],\n    [\n        2,\n        Multiscale(np.random.randn(71, 2), np.random.uniform(0.5, 3, size=(71, 2))),\n        gpflow.kernels.SquaredExponential(\n            variance=1.84, lengthscales=np.random.uniform(0.5, 3.0, 2)\n        ),\n    ],\n    [\n        9,\n        InducingPatches(np.random.randn(71, 4)),\n        gpflow.kernels.Convolutional(gpflow.kernels.SquaredExponential(), [3, 3], [2, 2]),\n    ],\n]\n\n\n@pytest.mark.parametrize(""input_dim, inducing_variable, kernel"", _inducing_variables_and_kernels)\ndef test_inducing_variables_psd_schur(input_dim, inducing_variable, kernel):\n    # Conditional variance must be PSD.\n    X = np.random.randn(5, input_dim)\n    Kuf_values = Kuf(inducing_variable, kernel, X)\n    Kuu_values = Kuu(inducing_variable, kernel, jitter=default_jitter())\n    Kff_values = kernel(X)\n    Qff_values = Kuf_values.numpy().T @ np.linalg.solve(Kuu_values, Kuf_values)\n    assert np.all(np.linalg.eig(Kff_values - Qff_values)[0] > 0.0)\n'"
tests/gpflow/covariances/test_multioutput.py,2,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport gpflow\nimport gpflow.inducing_variables.multioutput as mf\nimport gpflow.kernels.multioutput as mk\nfrom gpflow.covariances.multioutput import kufs as mo_kufs, kuus as mo_kuus\n\nrng = np.random.RandomState(9911)\n\n# ------------------------------------------\n# Helpers\n# ------------------------------------------\n\n\ndef make_kernel():\n    return gpflow.kernels.SquaredExponential()\n\n\ndef make_kernels(num):\n    return [make_kernel() for _ in range(num)]\n\n\ndef make_ip():\n    X = rng.permutation(Datum.X)\n    return gpflow.inducing_variables.InducingPoints(X[: Datum.M, ...])\n\n\ndef make_ips(num):\n    return [make_ip() for _ in range(num)]\n\n\n# ------------------------------------------\n# Data classes: storing constants\n# ------------------------------------------\n\n\nclass Datum:\n    D = 1\n    L = 2\n    P = 3\n    M = 10\n    N = 100\n    W = rng.randn(P, L)\n    X = rng.randn(N)[:, None]\n    Xnew = rng.randn(N)[:, None]\n\n\nmultioutput_inducing_variable_list = [\n    mf.SharedIndependentInducingVariables(make_ip()),\n    mf.SeparateIndependentInducingVariables(make_ips(Datum.P)),\n]\n\nmultioutput_kernel_list = [\n    mk.SharedIndependent(make_kernel(), Datum.P),\n    mk.SeparateIndependent(make_kernels(Datum.L)),\n    mk.LinearCoregionalization(make_kernels(Datum.L), Datum.W),\n]\n\n\n@pytest.mark.parametrize(""inducing_variable"", multioutput_inducing_variable_list)\n@pytest.mark.parametrize(""kernel"", multioutput_kernel_list)\ndef test_kuu(inducing_variable, kernel):\n    Kuu = mo_kuus.Kuu(inducing_variable, kernel, jitter=1e-9)\n    tf.linalg.cholesky(Kuu)\n\n\n@pytest.mark.parametrize(""inducing_variable"", multioutput_inducing_variable_list)\n@pytest.mark.parametrize(""kernel"", multioutput_kernel_list)\ndef test_kuf(inducing_variable, kernel):\n    Kuf = mo_kufs.Kuf(inducing_variable, kernel, Datum.Xnew)\n\n\n@pytest.mark.parametrize(""fun"", [mo_kuus.Kuu, mo_kufs.Kuf])\ndef test_mixed_shared(fun):\n    inducing_variable = mf.SharedIndependentInducingVariables(make_ip())\n    kernel = mk.LinearCoregionalization(make_kernels(Datum.L), Datum.W)\n    if fun is mo_kuus.Kuu:\n        t = tf.linalg.cholesky(fun(inducing_variable, kernel, jitter=1e-9))\n    else:\n        t = fun(inducing_variable, kernel, Datum.Xnew)\n        print(t.shape)\n'"
tests/gpflow/expectations/__init__.py,0,b''
tests/gpflow/expectations/test_expectations.py,2,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\nfrom tensorflow import convert_to_tensor as ctt\n\nimport gpflow\nfrom gpflow import inducing_variables, kernels\nfrom gpflow import mean_functions as mf\nfrom gpflow.config import default_float\nfrom gpflow.expectations import expectation, quadrature_expectation\nfrom gpflow.probability_distributions import DiagonalGaussian, Gaussian, MarkovGaussian\n\nrng = np.random.RandomState(1)\nRTOL = 1e-6\n\nnum_data = 5\nnum_ind = 4\nD_in = 2\nD_out = 2\n\nXmu = ctt(rng.randn(num_data, D_in))\nXmu_markov = ctt(rng.randn(num_data + 1, D_in))  # (N+1)xD\nXcov = rng.randn(num_data, D_in, D_in)\nXcov = ctt(Xcov @ np.transpose(Xcov, (0, 2, 1)))\nZ = rng.randn(num_ind, D_in)\n\n\ndef markov_gauss():\n    cov_params = rng.randn(num_data + 1, D_in, 2 * D_in) / 2.0  # (N+1)xDx2D\n    Xcov = cov_params @ np.transpose(cov_params, (0, 2, 1))  # (N+1)xDxD\n    Xcross = cov_params[:-1] @ np.transpose(cov_params[1:], (0, 2, 1))  # NxDxD\n    Xcross = np.concatenate((Xcross, np.zeros((1, D_in, D_in))), 0)  # (N+1)xDxD\n    Xcov = np.stack([Xcov, Xcross])  # 2x(N+1)xDxD\n    return MarkovGaussian(Xmu_markov, ctt(Xcov))\n\n\n_means = {\n    ""lin"": mf.Linear(A=rng.randn(D_in, D_out), b=rng.randn(D_out)),\n    ""identity"": mf.Identity(input_dim=D_in),\n    ""const"": mf.Constant(c=rng.randn(D_out)),\n    ""zero"": mf.Zero(output_dim=D_out),\n}\n\n_distrs = {\n    ""gauss"": Gaussian(Xmu, Xcov),\n    ""dirac_gauss"": Gaussian(Xmu, np.zeros((num_data, D_in, D_in))),\n    ""gauss_diag"": DiagonalGaussian(Xmu, rng.rand(num_data, D_in)),\n    ""dirac_diag"": DiagonalGaussian(Xmu, np.zeros((num_data, D_in))),\n    ""dirac_markov_gauss"": MarkovGaussian(Xmu_markov, np.zeros((2, num_data + 1, D_in, D_in))),\n    ""markov_gauss"": markov_gauss(),\n}\n\n_kerns = {\n    ""rbf"": kernels.SquaredExponential(variance=rng.rand(), lengthscales=rng.rand() + 1.0),\n    ""lin"": kernels.Linear(variance=rng.rand()),\n    ""matern"": kernels.Matern32(variance=rng.rand()),\n    ""rbf_act_dim_0"": kernels.SquaredExponential(\n        variance=rng.rand(), lengthscales=rng.rand() + 1.0, active_dims=[0]\n    ),\n    ""rbf_act_dim_1"": kernels.SquaredExponential(\n        variance=rng.rand(), lengthscales=rng.rand() + 1.0, active_dims=[1]\n    ),\n    ""lin_act_dim_0"": kernels.Linear(variance=rng.rand(), active_dims=[0]),\n    ""lin_act_dim_1"": kernels.Linear(variance=rng.rand(), active_dims=[1]),\n    ""rbf_lin_sum"": kernels.Sum(\n        [\n            kernels.SquaredExponential(variance=rng.rand(), lengthscales=rng.rand() + 1.0),\n            kernels.Linear(variance=rng.rand()),\n        ]\n    ),\n    ""rbf_lin_sum2"": kernels.Sum(\n        [\n            kernels.Linear(variance=rng.rand()),\n            kernels.SquaredExponential(variance=rng.rand(), lengthscales=rng.rand() + 1.0),\n            kernels.Linear(variance=rng.rand()),\n            kernels.SquaredExponential(variance=rng.rand(), lengthscales=rng.rand() + 1.0),\n        ]\n    ),\n    ""rbf_lin_prod"": kernels.Product(\n        [\n            kernels.SquaredExponential(\n                variance=rng.rand(), lengthscales=rng.rand() + 1.0, active_dims=[0]\n            ),\n            kernels.Linear(variance=rng.rand(), active_dims=[1]),\n        ]\n    ),\n}\n\n\ndef kerns(*args):\n    return [_kerns[k] for k in args]\n\n\ndef distrs(*args):\n    return [_distrs[k] for k in args]\n\n\ndef means(*args):\n    return [_means[k] for k in args]\n\n\n@pytest.fixture\ndef inducing_variable():\n    return inducing_variables.InducingPoints(Z)\n\n\ndef _check(params):\n    analytic = expectation(*params)\n    quad = quadrature_expectation(*params)\n    assert_allclose(analytic, quad, rtol=RTOL)\n\n\n# =================================== TESTS ===================================\n\ndistr_args1 = distrs(""gauss"")\nmean_args = means(""lin"", ""identity"", ""const"", ""zero"")\nkern_args1 = kerns(""lin"", ""rbf"", ""rbf_lin_sum"", ""rbf_lin_prod"")\nkern_args2 = kerns(""lin"", ""rbf"", ""rbf_lin_sum"")\n\n\n@pytest.mark.parametrize(""distribution"", distr_args1)\n@pytest.mark.parametrize(""mean1"", mean_args)\n@pytest.mark.parametrize(""mean2"", mean_args)\n@pytest.mark.parametrize(""arg_filter"", [lambda p, m1, m2: (p, m1), lambda p, m1, m2: (p, m1, m2)])\ndef test_mean_function_only_expectations(distribution, mean1, mean2, arg_filter):\n    params = arg_filter(distribution, mean1, mean2)\n    _check(params)\n\n\n@pytest.mark.parametrize(""distribution"", distrs(""gauss"", ""gauss_diag""))\n@pytest.mark.parametrize(""kernel"", kern_args1)\n@pytest.mark.parametrize(\n    ""arg_filter"",\n    [lambda p, k, f: (p, k), lambda p, k, f: (p, (k, f)), lambda p, k, f: (p, (k, f), (k, f)),],\n)\ndef test_kernel_only_expectations(distribution, kernel, inducing_variable, arg_filter):\n    params = arg_filter(distribution, kernel, inducing_variable)\n    _check(params)\n\n\n@pytest.mark.parametrize(""distribution"", distr_args1)\n@pytest.mark.parametrize(""kernel"", kerns(""rbf"", ""lin"", ""matern"", ""rbf_lin_sum""))\n@pytest.mark.parametrize(""mean"", mean_args)\n@pytest.mark.parametrize(\n    ""arg_filter"", [lambda p, k, f, m: (p, (k, f), m), lambda p, k, f, m: (p, m, (k, f))]\n)\ndef test_kernel_mean_function_expectations(\n    distribution, kernel, inducing_variable, mean, arg_filter\n):\n    params = arg_filter(distribution, kernel, inducing_variable, mean)\n    _check(params)\n\n\n@pytest.mark.parametrize(""kernel"", kern_args1)\ndef test_eKdiag_no_uncertainty(kernel):\n    eKdiag = expectation(_distrs[""dirac_diag""], kernel)\n    Kdiag = kernel(Xmu, full_cov=False)\n    assert_allclose(eKdiag, Kdiag, rtol=RTOL)\n\n\n@pytest.mark.parametrize(""kernel"", kern_args1)\ndef test_eKxz_no_uncertainty(kernel, inducing_variable):\n    eKxz = expectation(_distrs[""dirac_diag""], (kernel, inducing_variable))\n    Kxz = kernel(Xmu, Z)\n    assert_allclose(eKxz, Kxz, rtol=RTOL)\n\n\n@pytest.mark.parametrize(""kernel"", kern_args2)\n@pytest.mark.parametrize(""mean"", mean_args)\ndef test_eMxKxz_no_uncertainty(kernel, inducing_variable, mean):\n    exKxz = expectation(_distrs[""dirac_diag""], mean, (kernel, inducing_variable))\n    Kxz = kernel(Xmu, Z)\n    xKxz = expectation(_distrs[""dirac_gauss""], mean)[:, :, None] * Kxz[:, None, :]\n    assert_allclose(exKxz, xKxz, rtol=RTOL)\n\n\n@pytest.mark.parametrize(""kernel"", kern_args1)\ndef test_eKzxKxz_no_uncertainty(kernel, inducing_variable):\n    eKzxKxz = expectation(\n        _distrs[""dirac_diag""], (kernel, inducing_variable), (kernel, inducing_variable)\n    )\n    Kxz = kernel(Xmu, Z)\n    KzxKxz = Kxz[:, :, None] * Kxz[:, None, :]\n    assert_allclose(eKzxKxz, KzxKxz, rtol=RTOL)\n\n\ndef test_RBF_eKzxKxz_gradient_notNaN():\n    """"""\n    Ensure that <K_{Z, x} K_{x, Z}>_p(x) is not NaN and correct, when\n    K_{Z, Z} is zero with finite precision. See pull request #595.\n    """"""\n    kernel = gpflow.kernels.SquaredExponential(1, lengthscales=0.1)\n    kernel.variance.assign(2.0)\n\n    p = gpflow.probability_distributions.Gaussian(\n        tf.constant([[10]], dtype=default_float()), tf.constant([[[0.1]]], dtype=default_float()),\n    )\n    z = gpflow.inducing_variables.InducingPoints([[-10.0], [10.0]])\n\n    with tf.GradientTape() as tape:\n        ekz = expectation(p, (kernel, z), (kernel, z))\n    grad = tape.gradient(ekz, kernel.lengthscales)\n    assert grad is not None and not np.isnan(grad)\n\n\n@pytest.mark.parametrize(""distribution"", distrs(""gauss_diag""))\n@pytest.mark.parametrize(""kern1"", kerns(""rbf_act_dim_0"", ""lin_act_dim_0""))\n@pytest.mark.parametrize(""kern2"", kerns(""rbf_act_dim_1"", ""lin_act_dim_1""))\ndef test_eKzxKxz_separate_dims_simplification(distribution, kern1, kern2, inducing_variable):\n    _check((distribution, (kern1, inducing_variable), (kern2, inducing_variable)))\n\n\n@pytest.mark.parametrize(""distribution"", distr_args1)\n@pytest.mark.parametrize(""kern1"", kerns(""rbf_lin_sum""))\n@pytest.mark.parametrize(""kern2"", kerns(""rbf_lin_sum2""))\ndef test_eKzxKxz_different_sum_kernels(distribution, kern1, kern2, inducing_variable):\n    _check((distribution, (kern1, inducing_variable), (kern2, inducing_variable)))\n\n\n@pytest.mark.parametrize(""distribution"", distr_args1)\n@pytest.mark.parametrize(""kern1"", kerns(""rbf_lin_sum2""))\n@pytest.mark.parametrize(""kern2"", kerns(""rbf_lin_sum2""))\ndef test_eKzxKxz_same_vs_different_sum_kernels(distribution, kern1, kern2, inducing_variable):\n    # check the result is the same if we pass different objects with the same value\n    same = expectation(*(distribution, (kern1, inducing_variable), (kern1, inducing_variable)))\n    different = expectation(*(distribution, (kern1, inducing_variable), (kern2, inducing_variable)))\n    assert_allclose(same, different, rtol=RTOL)\n\n\n@pytest.mark.parametrize(""distribution"", distrs(""markov_gauss""))\n@pytest.mark.parametrize(""kernel"", kern_args2)\n@pytest.mark.parametrize(""mean"", means(""identity""))\ndef test_exKxz_markov(distribution, kernel, mean, inducing_variable):\n    _check((distribution, (kernel, inducing_variable), mean))\n\n\n@pytest.mark.parametrize(""distribution"", distrs(""dirac_markov_gauss""))\n@pytest.mark.parametrize(""kernel"", kern_args2)\n@pytest.mark.parametrize(""mean"", means(""identity""))\ndef test_exKxz_markov_no_uncertainty(distribution, kernel, mean, inducing_variable):\n    exKxz = expectation(distribution, (kernel, inducing_variable), mean)\n    Kzx = kernel(Xmu_markov[:-1, :], Z)  # NxM\n    xKxz = Kzx[..., None] * Xmu_markov[1:, None, :]  # NxMxD\n    assert_allclose(exKxz, xKxz, rtol=RTOL)\n\n\n@pytest.mark.parametrize(""kernel"", kerns(""rbf""))\n@pytest.mark.parametrize(""distribution"", distrs(""gauss"", ""gauss_diag"", ""markov_gauss""))\ndef test_cov_shape_inference(distribution, kernel, inducing_variable):\n    gauss_tuple = (distribution.mu, distribution.cov)\n    _check((gauss_tuple, (kernel, inducing_variable)))\n    if isinstance(distribution, MarkovGaussian):\n        _check((gauss_tuple, None, (kernel, inducing_variable)))\n'"
tests/gpflow/kernels/__init__.py,0,b''
tests/gpflow/kernels/reference.py,0,"b'import numpy as np\n\n\ndef ref_rbf_kernel(X, lengthscales, signal_variance):\n    N, _ = X.shape\n    kernel = np.zeros((N, N))\n    for row_index in range(N):\n        for column_index in range(N):\n            vecA = X[row_index, :]\n            vecB = X[column_index, :]\n            delta = vecA - vecB\n            distance_squared = np.dot(delta.T, delta)\n            kernel[row_index, column_index] = signal_variance * np.exp(\n                -0.5 * distance_squared / lengthscales ** 2\n            )\n    return kernel\n\n\ndef ref_arccosine_kernel(X, order, weight_variances, bias_variance, signal_variance):\n    num_points = X.shape[0]\n    kernel = np.empty((num_points, num_points))\n    for row in range(num_points):\n        for col in range(num_points):\n            x = X[row]\n            y = X[col]\n\n            numerator = (weight_variances * x).dot(y) + bias_variance\n\n            x_denominator = np.sqrt((weight_variances * x).dot(x) + bias_variance)\n            y_denominator = np.sqrt((weight_variances * y).dot(y) + bias_variance)\n            denominator = x_denominator * y_denominator\n\n            theta = np.arccos(np.clip(numerator / denominator, -1.0, 1.0))\n            if order == 0:\n                J = np.pi - theta\n            elif order == 1:\n                J = np.sin(theta) + (np.pi - theta) * np.cos(theta)\n            elif order == 2:\n                J = 3.0 * np.sin(theta) * np.cos(theta)\n                J += (np.pi - theta) * (1.0 + 2.0 * np.cos(theta) ** 2)\n\n            kernel[row, col] = (\n                signal_variance\n                * (1.0 / np.pi)\n                * J\n                * x_denominator ** order\n                * y_denominator ** order\n            )\n    return kernel\n\n\ndef ref_periodic_kernel(X, base_name, lengthscales, signal_variance, period):\n    """"""\n    Calculates K(X) for the periodic kernel based on various base kernels.\n    """"""\n    sine_arg = np.pi * (X[:, None, :] - X[None, :, :]) / period\n    sine_base = np.sin(sine_arg) / lengthscales\n    if base_name in {""RBF"", ""SquaredExponential""}:\n        dist = 0.5 * np.sum(np.square(sine_base), axis=-1)\n        exp_dist = np.exp(-dist)\n    elif base_name == ""Matern12"":\n        dist = np.sum(np.abs(sine_base), axis=-1)\n        exp_dist = np.exp(-dist)\n    elif base_name == ""Matern32"":\n        dist = np.sqrt(3) * np.sum(np.abs(sine_base), axis=-1)\n        exp_dist = (1 + dist) * np.exp(-dist)\n    elif base_name == ""Matern52"":\n        dist = np.sqrt(5) * np.sum(np.abs(sine_base), axis=-1)\n        exp_dist = (1 + dist + dist ** 2 / 3) * np.exp(-dist)\n    return signal_variance * exp_dist\n'"
tests/gpflow/kernels/test_broadcasting.py,3,"b'# Copyright 2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tensorflow as tf\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport pytest\n\nimport gpflow\nimport gpflow.ci_utils\nfrom gpflow import kernels\n\n\nKERNEL_CLASSES = [\n    # Static kernels:\n    kernels.White,\n    kernels.Constant,\n    # Stationary kernels:\n    kernels.SquaredExponential,\n    kernels.RationalQuadratic,\n    kernels.Exponential,\n    kernels.Matern12,\n    kernels.Matern32,\n    kernels.Matern52,\n    # other kernels:\n    kernels.Cosine,\n    kernels.Linear,\n    kernels.Polynomial,\n    # sum and product kernels:\n    lambda: kernels.White() + kernels.Matern12(),\n    lambda: kernels.White() * kernels.Matern12(),\n    # Following kernels do not broadcast: see https://github.com/GPflow/GPflow/issues/1339\n    pytest.param(kernels.ArcCosine, marks=pytest.mark.xfail),  # broadcasting not implemented\n    pytest.param(kernels.Coregion, marks=pytest.mark.xfail),  # broadcasting not implemented\n    pytest.param(kernels.ChangePoints, marks=pytest.mark.xfail),  # broadcasting not implemented\n    pytest.param(kernels.Convolutional, marks=pytest.mark.xfail),  # broadcasting not implemented\n]\n\n\ndef check_broadcasting(kernel):\n    S, N, M, D = 5, 4, 3, 2\n    X1 = np.random.randn(S, N, D)\n    X2 = np.random.randn(M, D)\n\n    compare_vs_map(X1, X2, kernel)\n\n\n@pytest.mark.parametrize(""kernel_class"", gpflow.ci_utils.subclasses(kernels.Kernel))\ndef test_no_kernels_missed(kernel_class):\n    tested_kernel_classes = KERNEL_CLASSES + [kernels.Sum, kernels.Product]\n    skipped_kernel_classes = [\n        p.values[0] for p in KERNEL_CLASSES if isinstance(p, type(pytest.param()))\n    ]\n    abstract_base_classes = [\n        kernels.Kernel,\n        kernels.Combination,\n        gpflow.kernels.base.ReducingCombination,\n        kernels.Static,\n        kernels.Stationary,\n        kernels.IsotropicStationary,\n        kernels.AnisotropicStationary,\n    ]\n    needs_constructor_parameters = [kernels.Periodic]\n    if kernel_class in tested_kernel_classes:\n        return  # tested by test_broadcast_no_active_dims\n    if kernel_class in skipped_kernel_classes:\n        return  # not tested but currently expected to fail\n    if kernel_class in abstract_base_classes:\n        return  # cannot test abstract base classes\n    if kernel_class in needs_constructor_parameters:\n        return  # this has a separate test, see test_broadcast_no_active_dims_periodic\n    if issubclass(kernel_class, kernels.MultioutputKernel):\n        return  # TODO: cannot currently test MultioutputKernels - see https://github.com/GPflow/GPflow/issues/1339\n    assert False, f""no broadcasting test for kernel class {kernel_class}""\n\n\n@pytest.mark.parametrize(""kernel_class"", KERNEL_CLASSES)\ndef test_broadcast_no_active_dims(kernel_class):\n    check_broadcasting(kernel_class())\n\n\n@pytest.mark.parametrize(\n    ""base_class"", [kernel for kernel in gpflow.ci_utils.subclasses(kernels.IsotropicStationary)],\n)\ndef test_broadcast_no_active_dims_periodic(base_class):\n    kernel = gpflow.kernels.Periodic(base_class())\n    check_broadcasting(kernel)\n\n\n@pytest.mark.parametrize(""kernel_class"", [gpflow.kernels.SquaredExponential])\ndef test_broadcast_slice_active_dims(kernel_class):\n    S, N, M, D = 5, 4, 3, 4\n    d = 2\n    X1 = np.random.randn(S, N, D)\n    X2 = np.random.randn(M, D)\n    kernel = kernel_class(active_dims=slice(1, 1 + d))\n\n    compare_vs_map(X1, X2, kernel)\n\n\n@pytest.mark.parametrize(""kernel_class"", [gpflow.kernels.SquaredExponential])\ndef test_broadcast_indices_active_dims(kernel_class):\n    S, N, M, D = 5, 4, 3, 4\n\n    X1 = np.random.randn(S, N, D)\n    X2 = np.random.randn(M, D)\n    kernel = kernel_class(active_dims=[1, 3])\n\n    compare_vs_map(X1, X2, kernel)\n\n\ndef compare_vs_map(X1, X2, kernel):\n    K12_loop = tf.stack([kernel(x, X2) for x in X1])  # [S, N, M]\n    K12_native = kernel(X1, X2)  # [S, N, M]\n    assert_allclose(K12_loop.numpy(), K12_native.numpy())\n\n    K11_loop = tf.stack([kernel(x) for x in X1])\n    K11_native = kernel(X1)\n    assert_allclose(K11_loop.numpy(), K11_native.numpy())\n\n    K1_loop = tf.stack([kernel(x, full_cov=False) for x in X1])\n    K1_native = kernel(X1, full_cov=False)\n    assert_allclose(K1_loop.numpy(), K1_native.numpy())\n'"
tests/gpflow/kernels/test_changepoints.py,0,"b'import numpy as np\nimport gpflow\n\n\ndef test_changepoint_with_X1_X2():\n    N = 100\n    X = np.linspace(0, 100, N).reshape(N, 1)\n    base_k1 = gpflow.kernels.Matern32(lengthscales=0.2)\n    base_k2 = gpflow.kernels.Matern32(lengthscales=2.0)\n    k = gpflow.kernels.ChangePoints([base_k1, base_k2], [0.0], steepness=5.0)\n    K = k(X)\n    assert K.shape == [N, N]\n\n    N2 = 25\n    X2 = np.linspace(0, 50, N2).reshape(N2, 1)\n    K = k(X, X2)\n    assert K.shape == [N, N2]\n'"
tests/gpflow/kernels/test_coregion.py,0,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nimport tensorflow as tf\nfrom gpflow.mean_functions import Constant\nfrom gpflow import set_trainable\n\nrng = np.random.RandomState(0)\n\n\nclass Datum:\n    N1, N2 = 6, 16\n    X = [rng.rand(N1, 2) * 1, rng.rand(N2, 2) * 1]\n    Y = [\n        np.sin(x[:, :1]) + 0.9 * np.cos(x[:, 1:2] * 1.6) + rng.randn(x.shape[0], 1) * 0.8 for x in X\n    ]\n    label = [np.zeros((N1, 1)), np.ones((N2, 1))]\n    X_augmented0 = np.hstack([X[0], label[0]])\n    X_augmented1 = np.hstack([X[1], label[1]])\n    X_augmented = np.vstack([X_augmented0, X_augmented1])\n\n    Y_augmented0 = np.hstack([Y[0], label[0]])\n    Y_augmented1 = np.hstack([Y[1], label[1]])\n    Y_augmented = np.vstack([Y_augmented0, Y_augmented1])\n\n    # For predict tests\n    N = 10\n    Xtest = rng.rand(N, 2) * N\n    Xtest_augmented0 = np.hstack([Xtest, np.zeros((N, 1))])\n    Xtest_augmented1 = np.hstack([Xtest, np.ones((N, 1))])\n    Ytest = np.sin(Xtest[:, :1]) + 0.9 * np.cos(Xtest[:, 1:2] * 1.6)\n    Ytest_augmented0 = np.hstack([Ytest, np.zeros((N, 1))])\n    Ytest_augmented1 = np.hstack([Ytest, np.ones((N, 1))])\n\n\ndef _prepare_models():\n    """"""\n    Prepare models to make sure the coregionalized model with diagonal coregion kernel and\n    with fixed lengthscales is equivalent with normal GP regression.\n    """"""\n    # 1. Two independent VGPs for two sets of data\n    k0 = gpflow.kernels.SquaredExponential()\n    set_trainable(k0.lengthscales, False)\n    k1 = gpflow.kernels.SquaredExponential()\n    set_trainable(k1.lengthscales, False)\n    vgp0 = gpflow.models.VGP(\n        (Datum.X[0], Datum.Y[0]),\n        kernel=k0,\n        mean_function=Constant(),\n        likelihood=gpflow.likelihoods.Gaussian(),\n        num_latent_gps=1,\n    )\n    vgp1 = gpflow.models.VGP(\n        (Datum.X[1], Datum.Y[1]),\n        kernel=k1,\n        mean_function=Constant(),\n        likelihood=gpflow.likelihoods.Gaussian(),\n        num_latent_gps=1,\n    )\n    # 2. Coregionalized VGP\n    kc = gpflow.kernels.SquaredExponential(active_dims=[0, 1])\n    set_trainable(kc.lengthscales, False)\n    set_trainable(kc.variance, False)  # variance is handled by the Coregion kernel\n    coreg = gpflow.kernels.Coregion(output_dim=2, rank=1, active_dims=[2])\n    coreg.W.assign(np.zeros((2, 1)))  # zero correlation between outputs\n    set_trainable(coreg.W, False)\n    lik = gpflow.likelihoods.SwitchedLikelihood(\n        [gpflow.likelihoods.Gaussian(), gpflow.likelihoods.Gaussian()]\n    )\n    mean_c = gpflow.mean_functions.SwitchedMeanFunction(\n        [gpflow.mean_functions.Constant(), gpflow.mean_functions.Constant()]\n    )\n    cvgp = gpflow.models.VGP(\n        (Datum.X_augmented, Datum.Y_augmented),\n        kernel=kc * coreg,\n        mean_function=mean_c,\n        likelihood=lik,\n        num_latent_gps=1,\n    )\n\n    # Train them for a small number of iterations\n\n    opt = gpflow.optimizers.Scipy()\n    opt.minimize(\n        vgp0.training_loss,\n        variables=vgp0.trainable_variables,\n        options=dict(maxiter=1000),\n        method=""BFGS"",\n    )\n    opt.minimize(\n        vgp1.training_loss,\n        variables=vgp1.trainable_variables,\n        options=dict(maxiter=1000),\n        method=""BFGS"",\n    )\n    opt.minimize(\n        cvgp.training_loss,\n        variables=cvgp.trainable_variables,\n        options=dict(maxiter=1000),\n        method=""BFGS"",\n    )\n\n    return vgp0, vgp1, cvgp\n\n\n# ------------------------------------------\n# Tests\n# ------------------------------------------\n\n\ndef test_likelihood_variance():\n    vgp0, vgp1, cvgp = _prepare_models()\n    assert_allclose(\n        vgp0.likelihood.variance.read_value(),\n        cvgp.likelihood.likelihoods[0].variance.read_value(),\n        atol=1e-2,\n    )\n    assert_allclose(\n        vgp1.likelihood.variance.read_value(),\n        cvgp.likelihood.likelihoods[1].variance.read_value(),\n        atol=1e-2,\n    )\n\n\ndef test_kernel_variance():\n    vgp0, vgp1, cvgp = _prepare_models()\n    assert_allclose(\n        vgp0.kernel.variance.read_value(),\n        cvgp.kernel.kernels[1].kappa.read_value()[0],\n        atol=1.0e-4,\n    )\n    assert_allclose(\n        vgp1.kernel.variance.read_value(),\n        cvgp.kernel.kernels[1].kappa.read_value()[1],\n        atol=1.0e-4,\n    )\n\n\ndef test_mean_values():\n    vgp0, vgp1, cvgp = _prepare_models()\n    assert_allclose(\n        vgp0.mean_function.c.read_value(),\n        cvgp.mean_function.meanfunctions[0].c.read_value(),\n        atol=1.0e-4,\n    )\n    assert_allclose(\n        vgp1.mean_function.c.read_value(),\n        cvgp.mean_function.meanfunctions[1].c.read_value(),\n        atol=1.0e-4,\n    )\n\n\ndef test_predict_f():\n    vgp0, vgp1, cvgp = _prepare_models()\n\n    pred_f0 = vgp0.predict_f(Datum.Xtest)\n    pred_fc0 = cvgp.predict_f(Datum.Xtest_augmented0)\n    assert_allclose(pred_f0, pred_fc0, atol=1.0e-4)\n    pred_f1 = vgp1.predict_f(Datum.Xtest)\n    pred_fc1 = cvgp.predict_f(Datum.Xtest_augmented1)\n    assert_allclose(pred_f1, pred_fc1, atol=1.0e-4)\n\n    # check predict_f_full_cov\n    vgp0.predict_f(Datum.Xtest, full_cov=True)\n    cvgp.predict_f(Datum.Xtest_augmented0, full_cov=True)\n    vgp1.predict_f(Datum.Xtest, full_cov=True)\n    cvgp.predict_f(Datum.Xtest_augmented1, full_cov=True)\n\n\ndef test_predict_y():\n    vgp0, vgp1, cvgp = _prepare_models()\n    mu1, var1 = vgp0.predict_y(Datum.Xtest)\n    c_mu1, c_var1 = cvgp.predict_y(np.hstack([Datum.Xtest, np.zeros((Datum.Xtest.shape[0], 1))]))\n\n    # predict_y returns results for all the likelihoods in multi_likelihood\n    assert_allclose(mu1, c_mu1[:, :1], atol=1.0e-4)\n    assert_allclose(var1, c_var1[:, :1], atol=1.0e-4)\n\n    mu2, var2 = vgp1.predict_y(Datum.Xtest)\n    c_mu2, c_var2 = cvgp.predict_y(np.hstack([Datum.Xtest, np.ones((Datum.Xtest.shape[0], 1))]))\n\n    # predict_y returns results for all the likelihoods in multi_likelihood\n    assert_allclose(mu2, c_mu2[:, 1:2], atol=1.0e-4)\n    assert_allclose(var2, c_var2[:, 1:2], atol=1.0e-4)\n\n\ndef test_predict_log_density():\n    vgp0, vgp1, cvgp = _prepare_models()\n\n    pred_ydensity0 = vgp0.predict_log_density((Datum.Xtest, Datum.Ytest))\n    pred_ydensity_c0 = cvgp.predict_log_density((Datum.Xtest_augmented0, Datum.Ytest_augmented0))\n    assert_allclose(pred_ydensity0, pred_ydensity_c0, atol=1e-2)\n    pred_ydensity1 = vgp1.predict_log_density((Datum.Xtest, Datum.Ytest))\n    pred_ydensity_c1 = cvgp.predict_log_density((Datum.Xtest_augmented1, Datum.Ytest_augmented1))\n    assert_allclose(pred_ydensity1, pred_ydensity_c1, atol=1e-2)\n\n\ndef test_predict_f_samples():\n    vgp0, vgp1, cvgp = _prepare_models()\n    # just check predict_f_samples(self) works\n    cvgp.predict_f_samples(Datum.X_augmented0, 1)\n    cvgp.predict_f_samples(Datum.X_augmented1, 1)\n'"
tests/gpflow/kernels/test_kernels.py,2,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.config import default_float\nfrom gpflow.kernels import (\n    SquaredExponential,\n    ArcCosine,\n    Linear,\n    White,\n    SeparateIndependent,\n    SharedIndependent,\n    LinearCoregionalization,\n)\nimport gpflow.ci_utils\nfrom tests.gpflow.kernels.reference import (\n    ref_rbf_kernel,\n    ref_arccosine_kernel,\n    ref_periodic_kernel,\n)\n\n\nrng = np.random.RandomState(1)\n\n\ndef _ref_changepoints(X, kernels, locations, steepness):\n    """"""\n    Calculates K(X) for each kernel in `kernels`, then multiply by sigmoid functions\n    in order to smoothly transition betwen them. The sigmoid transitions are defined\n    by a location and a steepness parameter.\n    """"""\n    locations = sorted(locations)\n    steepness = steepness if isinstance(steepness, list) else [steepness] * len(locations)\n    locations = np.array(locations).reshape((1, 1, -1))\n    steepness = np.array(steepness).reshape((1, 1, -1))\n\n    sig_X = 1.0 / (1.0 + np.exp(-steepness * (X[:, :, None] - locations)))\n\n    starters = sig_X * np.transpose(sig_X, axes=(1, 0, 2))\n    stoppers = (1 - sig_X) * np.transpose((1 - sig_X), axes=(1, 0, 2))\n\n    ones = np.ones((X.shape[0], X.shape[0], 1))\n    starters = np.concatenate([ones, starters], axis=2)\n    stoppers = np.concatenate([stoppers, ones], axis=2)\n\n    kernel_stack = np.stack([k(X) for k in kernels], axis=2)\n    return (kernel_stack * starters * stoppers).sum(axis=2)\n\n\n@pytest.mark.parametrize(""variance, lengthscales"", [[2.3, 1.4]])\ndef test_rbf_1d(variance, lengthscales):\n    X = rng.randn(3, 1)\n    kernel = gpflow.kernels.SquaredExponential(lengthscales=lengthscales, variance=variance)\n\n    gram_matrix = kernel(X)\n    reference_gram_matrix = ref_rbf_kernel(X, lengthscales, variance)\n\n    assert_allclose(gram_matrix, reference_gram_matrix)\n\n\n@pytest.mark.parametrize(""variance, lengthscales"", [[2.3, 1.4]])\ndef test_rq_1d(variance, lengthscales):\n    kSE = gpflow.kernels.SquaredExponential(lengthscales=lengthscales, variance=variance)\n    kRQ = gpflow.kernels.RationalQuadratic(lengthscales=lengthscales, variance=variance, alpha=1e8)\n    rng = np.random.RandomState(1)\n    X = rng.randn(6, 1).astype(default_float())\n\n    gram_matrix_SE = kSE(X)\n    gram_matrix_RQ = kRQ(X)\n    assert_allclose(gram_matrix_SE, gram_matrix_RQ)\n\n\ndef _assert_arccosine_kern_err(variance, weight_variances, bias_variance, order, X):\n    kernel = gpflow.kernels.ArcCosine(\n        order=order,\n        variance=variance,\n        weight_variances=weight_variances,\n        bias_variance=bias_variance,\n    )\n    gram_matrix = kernel(X)\n    reference_gram_matrix = ref_arccosine_kernel(\n        X, order, weight_variances, bias_variance, variance\n    )\n    assert_allclose(gram_matrix, reference_gram_matrix)\n\n\n@pytest.mark.parametrize(""order"", gpflow.kernels.ArcCosine.implemented_orders)\n@pytest.mark.parametrize(""D, weight_variances"", [[1, 1.7], [3, 1.7], [3, (1.1, 1.7, 1.9)]])\n@pytest.mark.parametrize(""N, bias_variance, variance"", [[3, 0.6, 2.3]])\ndef test_arccosine_1d_and_3d(order, D, N, weight_variances, bias_variance, variance):\n    X_data = rng.randn(N, D)\n    _assert_arccosine_kern_err(variance, weight_variances, bias_variance, order, X_data)\n\n\n@pytest.mark.parametrize(""order"", [42])\ndef test_arccosine_non_implemented_order(order):\n    with pytest.raises(ValueError):\n        gpflow.kernels.ArcCosine(order=order)\n\n\n@pytest.mark.parametrize(""D, N"", [[1, 4]])\ndef test_arccosine_nan_gradient(D, N):\n    X = rng.rand(N, D)\n    kernel = gpflow.kernels.ArcCosine()\n    with tf.GradientTape() as tape:\n        Kff = kernel(X)\n    grads = tape.gradient(Kff, kernel.trainable_variables)\n    assert not np.any(np.isnan(grads))\n\n\n@pytest.mark.parametrize(\n    ""base_class"",\n    [\n        gpflow.kernels.SquaredExponential,\n        gpflow.kernels.Matern12,\n        gpflow.kernels.Matern32,\n        gpflow.kernels.Matern52,\n    ],\n)\n@pytest.mark.parametrize(\n    ""D, lengthscales, period"",\n    [\n        [1, 2.0, 3.0],  # 1d, single lengthscale, single period\n        [2, 11.5, 3.0],  # 2d, single lengthscale, single period\n        [2, 11.5, (3.0, 6.0)],  # 2d, single lengthscale, ard period\n        [2, (11.5, 12.5), 3.0],  # 2d, ard lengthscales, single period\n        [2, (11.5, 12.5), (3.0, 6.0)],  # 2d, ard lengthscales, ard period\n    ],\n)\n@pytest.mark.parametrize(""N, variance"", [[3, 2.3], [5, 1.3],])\ndef test_periodic(base_class, D, N, lengthscales, variance, period):\n    X = rng.randn(N, D) if D == 1 else rng.multivariate_normal(np.zeros(D), np.eye(D), N)\n\n    base_kernel = base_class(lengthscales=lengthscales, variance=variance)\n    kernel = gpflow.kernels.Periodic(base_kernel, period=period)\n    gram_matrix = kernel(X)\n    reference_gram_matrix = ref_periodic_kernel(\n        X, base_class.__name__, lengthscales, variance, period\n    )\n\n    assert_allclose(gram_matrix, reference_gram_matrix)\n\n\n@pytest.mark.parametrize(\n    ""base_class"", [gpflow.kernels.SquaredExponential, gpflow.kernels.Matern12,]\n)\ndef test_periodic_diag(base_class):\n    N, D = 5, 3\n    X = rng.multivariate_normal(np.zeros(D), np.eye(D), N)\n    base_kernel = base_class(lengthscales=2.0, variance=1.0)\n    kernel = gpflow.kernels.Periodic(base_kernel, period=6.0)\n    assert_allclose(base_kernel(X, full_cov=False), kernel(X, full_cov=False))\n\n\ndef test_periodic_non_stationary_base_kernel():\n    error_msg = r""Periodic requires an IsotropicStationary kernel as the `base_kernel`""\n    with pytest.raises(TypeError, match=error_msg):\n        gpflow.kernels.Periodic(gpflow.kernels.Linear())\n\n\ndef test_periodic_bad_ard_period():\n    error_msg = r""Size of `active_dims` \\[1 2\\] does not match size of ard parameter \\(3\\)""\n    base_kernel = gpflow.kernels.RBF(active_dims=[1, 2])\n    with pytest.raises(ValueError, match=error_msg):\n        gpflow.kernels.Periodic(base_kernel, period=[1.0, 1.0, 1.0])\n\n\nkernel_setups = [\n    kernel()\n    for kernel in gpflow.ci_utils.subclasses(gpflow.kernels.Stationary)\n    if kernel not in (gpflow.kernels.IsotropicStationary, gpflow.kernels.AnisotropicStationary)\n] + [\n    gpflow.kernels.Constant(),\n    gpflow.kernels.Linear(),\n    gpflow.kernels.Polynomial(),\n    gpflow.kernels.ArcCosine(),\n]\n\n\n@pytest.mark.parametrize(""D"", [1, 5])\n@pytest.mark.parametrize(""kernel"", kernel_setups)\n@pytest.mark.parametrize(""N"", [10])\ndef test_kernel_symmetry_1d_and_5d(D, kernel, N):\n    X = rng.randn(N, D)\n    errors = kernel(X) - kernel(X, X)\n    assert np.allclose(errors, 0)\n\n\n@pytest.mark.parametrize(""N, N2, input_dim, output_dim, rank"", [[10, 12, 1, 3, 2]])\ndef test_coregion_shape(N, N2, input_dim, output_dim, rank):\n    X = np.random.randint(0, output_dim, (N, input_dim))\n    X2 = np.random.randint(0, output_dim, (N2, input_dim))\n    kernel = gpflow.kernels.Coregion(output_dim=output_dim, rank=rank)\n    kernel.W = rng.randn(output_dim, rank)\n    kernel.kappa = rng.randn(output_dim, 1).reshape(-1) + 1.0\n\n    Kff2 = kernel(X, X2)\n    assert Kff2.shape == (10, 12)\n    Kff = kernel(X)\n    assert Kff.shape == (10, 10)\n\n\n@pytest.mark.parametrize(""N, input_dim, output_dim, rank"", [[10, 1, 3, 2]])\ndef test_coregion_diag(N, input_dim, output_dim, rank):\n    X = np.random.randint(0, output_dim, (N, input_dim))\n    kernel = gpflow.kernels.Coregion(output_dim=output_dim, rank=rank)\n    kernel.W = rng.randn(output_dim, rank)\n    kernel.kappa = rng.randn(output_dim, 1).reshape(-1) + 1.0\n\n    K = kernel(X)\n    Kdiag = kernel.K_diag(X)\n    assert np.allclose(np.diag(K), Kdiag)\n\n\n@pytest.mark.parametrize(""N, input_dim, output_dim, rank"", [[10, 1, 3, 2]])\ndef test_coregion_slice(N, input_dim, output_dim, rank):\n    X = np.random.randint(0, output_dim, (N, input_dim))\n    X = np.hstack((X, rng.randn(10, 1)))\n    kernel1 = gpflow.kernels.Coregion(output_dim=output_dim, rank=rank, active_dims=[0])\n    # compute another kernel with additinoal inputs,\n    # make sure out kernel is still okay.\n    kernel2 = gpflow.kernels.SquaredExponential(active_dims=[1])\n    kernel_prod = kernel1 * kernel2\n    K1 = kernel_prod(X)\n    K2 = kernel1(X) * kernel2(X)  # slicing happens inside kernel\n    assert np.allclose(K1, K2)\n\n\n_dim = 3\nkernel_setups_extended = (\n    kernel_setups\n    + [\n        SquaredExponential() + Linear(),\n        SquaredExponential() * Linear(),\n        SquaredExponential() + Linear(variance=rng.rand(_dim)),\n    ]\n    + [ArcCosine(order=order) for order in ArcCosine.implemented_orders]\n)\n\n\n@pytest.mark.parametrize(""kernel"", kernel_setups_extended)\n@pytest.mark.parametrize(""N, dim"", [[30, _dim]])\ndef test_diags(kernel, N, dim):\n    X = np.random.randn(N, dim)\n    kernel1 = tf.linalg.diag_part(kernel(X, full_cov=True))\n    kernel2 = kernel(X, full_cov=False)\n    assert np.allclose(kernel1, kernel2)\n\n\ndef test_conv_diag():\n    kernel = gpflow.kernels.Convolutional(gpflow.kernels.SquaredExponential(), [3, 3], [2, 2])\n    X = np.random.randn(3, 9)\n    kernel_full = np.diagonal(kernel(X, full_cov=True))\n    kernel_diag = kernel(X, full_cov=False)\n    assert np.allclose(kernel_full, kernel_diag)\n\n\n# Add a rbf and linear kernel, make sure the result is the same as adding the result of\n# the kernels separately.\n_kernel_setups_add = [\n    gpflow.kernels.SquaredExponential(),\n    gpflow.kernels.Linear(),\n    gpflow.kernels.SquaredExponential() + gpflow.kernels.Linear(),\n]\n\n\n@pytest.mark.parametrize(""N, D"", [[10, 1]])\ndef test_add_symmetric(N, D):\n    X = rng.randn(N, D)\n    Kffs = [kernel(X) for kernel in _kernel_setups_add]\n\n    assert np.allclose(Kffs[0] + Kffs[1], Kffs[2])\n\n\n@pytest.mark.parametrize(""N, M, D"", [[10, 12, 1]])\ndef test_add_asymmetric(N, M, D):\n    X, Z = rng.randn(N, D), rng.randn(M, D)\n    Kfus = [kernel(X, Z) for kernel in _kernel_setups_add]\n\n    assert np.allclose(Kfus[0] + Kfus[1], Kfus[2])\n\n\n@pytest.mark.parametrize(""N, D"", [[10, 1]])\ndef test_white(N, D):\n    """"""\n    The white kernel should not give the same result when called with k(X) and\n    k(X, X)\n    """"""\n    X = rng.randn(N, D)\n    kernel = gpflow.kernels.White()\n    Kff_sym = kernel(X)\n    Kff_asym = kernel(X, X)\n\n    assert not np.allclose(Kff_sym, Kff_asym)\n\n\n_kernel_classes_slice = [\n    kernel\n    for kernel in gpflow.ci_utils.subclasses(gpflow.kernels.Stationary)\n    if kernel not in (gpflow.kernels.IsotropicStationary, gpflow.kernels.AnisotropicStationary)\n] + [gpflow.kernels.Constant, gpflow.kernels.Linear, gpflow.kernels.Polynomial,]\n\n_kernel_triples_slice = [\n    (k1(active_dims=[0]), k2(active_dims=[1]), k3(active_dims=slice(0, 1)))\n    for k1, k2, k3 in zip(_kernel_classes_slice, _kernel_classes_slice, _kernel_classes_slice)\n]\n\n\n@pytest.mark.parametrize(""kernel_triple"", _kernel_triples_slice)\n@pytest.mark.parametrize(""N, D"", [[20, 2]])\ndef test_slice_symmetric(kernel_triple, N, D):\n    X = rng.randn(N, D)\n    K1, K3 = kernel_triple[0](X), kernel_triple[2](X[:, :1])\n    assert np.allclose(K1, K3)\n    K2, K4 = kernel_triple[1](X), kernel_triple[2](X[:, 1:])\n    assert np.allclose(K2, K4)\n\n\n@pytest.mark.parametrize(""kernel_triple"", _kernel_triples_slice)\n@pytest.mark.parametrize(""N, M, D"", [[10, 12, 2]])\ndef test_slice_asymmetric(kernel_triple, N, M, D):\n    X = rng.randn(N, D)\n    Z = rng.randn(M, D)\n    K1, K3 = kernel_triple[0](X, Z), kernel_triple[2](X[:, :1], Z[:, :1])\n    assert np.allclose(K1, K3)\n    K2, K4 = kernel_triple[1](X, Z), kernel_triple[2](X[:, 1:], Z[:, 1:])\n    assert np.allclose(K2, K4)\n\n\n_kernel_setups_prod = [\n    gpflow.kernels.Matern32(),\n    gpflow.kernels.Matern52(lengthscales=0.3),\n    gpflow.kernels.Matern32() * gpflow.kernels.Matern52(lengthscales=0.3),\n]\n\n\n@pytest.mark.parametrize(""N, D"", [[30, 2]])\ndef test_product(N, D):\n    X = rng.randn(N, D)\n    Kffs = [kernel(X) for kernel in _kernel_setups_prod]\n\n    assert np.allclose(Kffs[0] * Kffs[1], Kffs[2])\n\n\n@pytest.mark.parametrize(""N, D"", [[30, 4], [10, 7]])\ndef test_active_product(N, D):\n    X = rng.randn(N, D)\n    dims, rand_idx, ls = (\n        list(range(D)),\n        int(rng.randint(0, D)),\n        rng.uniform(1.0, 7.0, D),\n    )\n    active_dims_list = [dims[:rand_idx] + dims[rand_idx + 1 :], [rand_idx], dims]\n    lengthscales_list = [\n        np.hstack([ls[:rand_idx], ls[rand_idx + 1 :]]),\n        ls[rand_idx],\n        ls,\n    ]\n    kernels = [\n        gpflow.kernels.SquaredExponential(lengthscales=lengthscales, active_dims=dims)\n        for dims, lengthscales in zip(active_dims_list, lengthscales_list)\n    ]\n    kernel_prod = kernels[0] * kernels[1]\n\n    Kff = kernels[2](X)\n    Kff_prod = kernel_prod(X)\n\n    assert np.allclose(Kff, Kff_prod)\n\n\n@pytest.mark.parametrize(""D"", [4, 7])\ndef test_ard_init_scalar(D):\n    """"""\n    For ard kernels, make sure that kernels can be instantiated with a single\n    scalar lengthscale or a suitable array of lengthscales\n    """"""\n    kernel_1 = gpflow.kernels.SquaredExponential(lengthscales=2.3)\n    kernel_2 = gpflow.kernels.SquaredExponential(lengthscales=np.ones(D) * 2.3)\n    lengthscales_1 = kernel_1.lengthscales.read_value()\n    lengthscales_2 = kernel_2.lengthscales.read_value()\n    assert np.allclose(lengthscales_1, lengthscales_2, atol=1e-10)\n\n\ndef test_ard_invalid_active_dims():\n    msg = r""Size of `active_dims` \\[1\\] does not match size of ard parameter \\(2\\)""\n    with pytest.raises(ValueError, match=msg):\n        gpflow.kernels.SquaredExponential(lengthscales=np.ones(2), active_dims=[1])\n\n\n@pytest.mark.parametrize(\n    ""kernel_class, param_name"",\n    [\n        [gpflow.kernels.SquaredExponential, ""lengthscales""],\n        [gpflow.kernels.Linear, ""variance""],\n        [gpflow.kernels.ArcCosine, ""weight_variances""],\n    ],\n)\n@pytest.mark.parametrize(""param_value, ard"", [[1.0, False], [[1.0], True], [[1.0, 1.0], True],])\ndef test_ard_property(kernel_class, param_name, param_value, ard):\n    kernel = kernel_class(**{param_name: param_value})\n    assert kernel.ard is ard\n\n\n@pytest.mark.parametrize(\n    ""locations, steepness, error_msg"",\n    [\n        # 1. Kernels locations dimension mismatch\n        [\n            [1.0],\n            1.0,\n            r""Number of kernels \\(3\\) must be one more than the number of changepoint locations \\(1\\)"",\n        ],\n        # 2. Locations steepness dimension mismatch\n        [\n            [1.0, 2.0],\n            [1.0],\n            r""Dimension of steepness \\(1\\) does not match number of changepoint locations \\(2\\)"",\n        ],\n    ],\n)\ndef test_changepoints_init_fail(locations, steepness, error_msg):\n    kernels = [\n        gpflow.kernels.Matern12(),\n        gpflow.kernels.Linear(),\n        gpflow.kernels.Matern32(),\n    ]\n    with pytest.raises(ValueError, match=error_msg):\n        gpflow.kernels.ChangePoints(kernels, locations, steepness)\n\n\ndef _assert_changepoints_kern_err(X, kernels, locations, steepness):\n    kernel = gpflow.kernels.ChangePoints(kernels, locations, steepness=steepness)\n    reference_gram_matrix = _ref_changepoints(X, kernels, locations, steepness)\n\n    assert_allclose(kernel(X), reference_gram_matrix)\n    assert_allclose(kernel.K_diag(X), np.diag(reference_gram_matrix))\n\n\n@pytest.mark.parametrize(""N"", [2, 10])\n@pytest.mark.parametrize(\n    ""kernels, locations, steepness"",\n    [\n        # 1. Single changepoint\n        [[gpflow.kernels.Constant(), gpflow.kernels.Constant()], [2.0], 5.0],\n        # 2. Two changepoints\n        [\n            [gpflow.kernels.Constant(), gpflow.kernels.Constant(), gpflow.kernels.Constant(),],\n            [1.0, 2.0],\n            5.0,\n        ],\n        # 3. Multiple steepness\n        [\n            [gpflow.kernels.Constant(), gpflow.kernels.Constant(), gpflow.kernels.Constant(),],\n            [1.0, 2.0],\n            [5.0, 10.0],\n        ],\n        # 4. Variety of kernels\n        [\n            [\n                gpflow.kernels.Matern12(),\n                gpflow.kernels.Linear(),\n                gpflow.kernels.SquaredExponential(),\n                gpflow.kernels.Constant(),\n            ],\n            [1.0, 2.0, 3.0],\n            5.0,\n        ],\n    ],\n)\ndef test_changepoints(N, kernels, locations, steepness):\n    X_data = rng.randn(N, 1)\n    _assert_changepoints_kern_err(X_data, kernels, locations, steepness)\n\n\n@pytest.mark.parametrize(\n    ""active_dims_1, active_dims_2, is_separate"",\n    [\n        [[1, 2, 3], None, False],\n        [None, [1, 2, 3], False],\n        [None, None, False],\n        [[1, 2, 3], [3, 4, 5], False],\n        [[1, 2, 3], [4, 5, 6], True],\n    ],\n)\ndef test_on_separate_dims(active_dims_1, active_dims_2, is_separate):\n    kernel_1 = gpflow.kernels.Linear(active_dims=active_dims_1)\n    kernel_2 = gpflow.kernels.SquaredExponential(active_dims=active_dims_2)\n    assert kernel_1.on_separate_dims(kernel_2) == is_separate\n    assert kernel_2.on_separate_dims(kernel_1) == is_separate\n    assert kernel_1.on_separate_dims(kernel_1) is False\n    assert kernel_2.on_separate_dims(kernel_2) is False\n\n\n@pytest.mark.parametrize(""kernel"", kernel_setups_extended)\ndef test_kernel_call_diag_and_X2_errors(kernel):\n    X = rng.randn(4, 1)\n    X2 = rng.randn(5, 1)\n\n    with pytest.raises(ValueError):\n        kernel(X, X2, full_cov=False)\n\n\ndef test_periodic_active_dims_matches():\n    active_dims = [1]\n    base_kernel = gpflow.kernels.SquaredExponential(active_dims=active_dims)\n    kernel = gpflow.kernels.Periodic(base_kernel=base_kernel)\n\n    assert kernel.active_dims == base_kernel.active_dims\n\n    kernel.active_dims = [2]\n    assert kernel.active_dims == base_kernel.active_dims\n\n    base_kernel.active_dims = [3]\n    assert kernel.active_dims == base_kernel.active_dims\n\n\ndef test_latent_kernels():\n    kernel_list = [SquaredExponential(), White(), White() + Linear()]\n\n    multioutput_kernel_list = [\n        SharedIndependent(SquaredExponential(), 3),\n        SeparateIndependent(kernel_list),\n        LinearCoregionalization(kernel_list, np.random.random((5, 3))),\n    ]\n    assert len(multioutput_kernel_list[0].latent_kernels) == 1\n    assert multioutput_kernel_list[1].latent_kernels == tuple(kernel_list)\n    assert multioutput_kernel_list[2].latent_kernels == tuple(kernel_list)\n\n\ndef test_combination_LMC_kernels():\n    N, D, P = 100, 3, 2\n    kernel_list1 = [Linear(active_dims=[1]), SquaredExponential()]\n    L1 = len(kernel_list1)\n    kernel_list2 = [SquaredExponential(), Linear(), Linear()]\n    L2 = len(kernel_list2)\n    k1 = LinearCoregionalization(kernel_list1, np.random.randn(P, L1))\n    k2 = LinearCoregionalization(kernel_list2, np.random.randn(P, L2))\n    kernel = k1 + k2\n    X = np.random.randn(N, D)\n    K1 = k1(X, full_cov=True)\n    K2 = k2(X, full_cov=True)\n    K = kernel(X, full_cov=True)\n    assert K.shape == [N, P, N, P]\n    np.testing.assert_allclose(K, K1 + K2)\n'"
tests/gpflow/kernels/test_positive_semidefinite.py,1,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_array_less\n\nfrom gpflow import kernels\nimport gpflow.ci_utils\n\nKERNEL_CLASSES = [\n    kernel\n    for cls in (kernels.Static, kernels.Stationary, kernels.Linear)\n    for kernel in gpflow.ci_utils.subclasses(cls)\n    if kernel not in (kernels.IsotropicStationary, kernels.AnisotropicStationary)\n] + [kernels.ArcCosine]\n\nrng = np.random.RandomState(42)\n\n\ndef pos_semidefinite(kernel):\n    N, D = 100, 5\n    X = rng.randn(N, D)\n\n    cov = kernel(X)\n    eig = tf.linalg.eigvalsh(cov).numpy()\n    assert_array_less(-1e-12, eig)\n\n\n@pytest.mark.parametrize(""kernel_class"", KERNEL_CLASSES)\ndef test_positive_semidefinite(kernel_class):\n    """"""\n    A valid kernel is positive semidefinite. Some kernels are only valid for\n    particular input shapes, see https://github.com/GPflow/GPflow/issues/1328\n    """"""\n    kernel = kernel_class()\n    pos_semidefinite(kernel)\n\n\n@pytest.mark.parametrize(\n    ""base_class"", [kernel for kernel in gpflow.ci_utils.subclasses(kernels.IsotropicStationary)]\n)\ndef test_positive_semidefinite_periodic(base_class):\n    """"""\n    A valid kernel is positive semidefinite. Some kernels are only valid for\n    particular input shapes, see https://github.com/GPflow/GPflow/issues/1328\n    """"""\n    kernel = kernels.Periodic(base_class())\n    pos_semidefinite(kernel)\n'"
tests/gpflow/kernels/test_scaled_euclid_dist.py,2,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport gpflow.kernels as kernels\n\nrng = np.random.RandomState(0)\n\n\nclass Datum:\n    num_data = 100\n    D = 100\n    X = rng.rand(num_data, D) * 100\n\n\nkernel_list = [\n    kernels.Matern12(),\n    kernels.Matern32(),\n    kernels.Matern52(),\n    kernels.Exponential(),\n    kernels.Cosine(),\n]\n\n\n@pytest.mark.parametrize(""kernel"", kernel_list)\ndef test_kernel_euclidean_distance(kernel):\n    """"""\n    Tests output & gradients of kernels that are a function of the (scaled) euclidean distance\n    of the points. We test on a high dimensional space, which can generate very small distances\n    causing the scaled_square_dist to generate some negative values.\n    """"""\n    K = kernel(Datum.X)\n    assert not np.isnan(K).any(), ""NaNs in the output of the "" + kernel.__name__ + ""kernel.""\n    assert np.isfinite(K).all(), ""Infs in the output of the "" + kernel.__name__ + "" kernel.""\n\n    X_as_param = tf.Variable(Datum.X)\n    with tf.GradientTape() as tape:\n        K_value = kernel(X_as_param, X_as_param)\n    dK = tape.gradient(K_value, X_as_param)[0]\n\n    assert not np.isnan(dK).any(), ""NaNs in the gradient of the "" + kernel.__name__ + "" kernel.""\n    assert np.isfinite(dK).all(), ""Infs in the output of the "" + kernel.__name__ + "" kernel.""\n'"
tests/gpflow/likelihoods/__init__.py,0,b''
tests/gpflow/likelihoods/test_gaussian.py,0,"b'import pytest\nimport gpflow\n\n\n@pytest.mark.parametrize(\n    ""init_lower"", [0.0, 1e-6, gpflow.likelihoods.Gaussian.DEFAULT_VARIANCE_LOWER_BOUND]\n)\ndef test_gaussian_lower_bound_constructor_check(init_lower):\n    with pytest.raises(\n        ValueError, match=""variance of the Gaussian likelihood must be strictly greater than""\n    ):\n        _ = gpflow.likelihoods.Gaussian(variance=0.0, variance_lower_bound=init_lower)\n'"
tests/gpflow/likelihoods/test_likelihoods.py,16,"b'# Copyright 2017 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.likelihoods import (\n    ScalarLikelihood,\n    Bernoulli,\n    Beta,\n    Exponential,\n    Gamma,\n    Gaussian,\n    GaussianMC,\n    Likelihood,\n    MonteCarloLikelihood,\n    MultiClass,\n    Ordinal,\n    Poisson,\n    StudentT,\n)\nfrom gpflow.quadrature import ndiagquad\nfrom gpflow.config import default_float, default_int\nimport gpflow.ci_utils\n\ntf.random.set_seed(99012)\n\n\nclass Datum:\n    tolerance = 1e-06\n    Yshape = (10, 3)\n    Y = tf.random.normal(Yshape, dtype=tf.float64)\n    F = tf.random.normal(Yshape, dtype=tf.float64)\n    Fmu = tf.random.normal(Yshape, dtype=tf.float64)\n    Fvar = 0.01 * tf.random.normal(Yshape, dtype=tf.float64) ** 2\n    Fvar_zero = tf.zeros(Yshape, dtype=tf.float64)\n\n\nclass LikelihoodSetup(object):\n    def __init__(self, likelihood, Y=Datum.Y, rtol=1e-06, atol=0.0):\n        self.likelihood = likelihood\n        self.Y = Y\n        self.rtol = rtol\n        self.atol = atol\n\n    def __repr__(self):\n        name = self.likelihood.__class__.__name__\n        return f""{name}-rtol={self.rtol}-atol={self.atol}""\n\n\nscalar_likelihood_setups = [\n    LikelihoodSetup(Gaussian()),\n    LikelihoodSetup(StudentT()),\n    LikelihoodSetup(Beta(), Y=tf.random.uniform(Datum.Yshape, dtype=default_float())),\n    LikelihoodSetup(\n        Ordinal(np.array([-1, 1])), Y=tf.random.uniform(Datum.Yshape, 0, 3, dtype=default_int()),\n    ),\n    LikelihoodSetup(\n        Poisson(invlink=tf.square), Y=tf.random.poisson(Datum.Yshape, 1.0, dtype=default_float()),\n    ),\n    LikelihoodSetup(\n        Exponential(invlink=tf.square), Y=tf.random.uniform(Datum.Yshape, dtype=default_float()),\n    ),\n    LikelihoodSetup(\n        Gamma(invlink=tf.square), Y=tf.random.uniform(Datum.Yshape, dtype=default_float()),\n    ),\n    LikelihoodSetup(\n        Bernoulli(invlink=tf.sigmoid), Y=tf.random.uniform(Datum.Yshape, dtype=default_float()),\n    ),\n]\n\nlikelihood_setups = scalar_likelihood_setups + [\n    LikelihoodSetup(\n        MultiClass(3), Y=tf.argmax(Datum.Y, 1).numpy().reshape(-1, 1), rtol=1e-3, atol=1e-3,\n    ),\n]\n\n\ndef filter_analytic_scalar_likelihood(method_name):\n    assert method_name in (\n        ""_variational_expectations"",\n        ""_predict_log_density"",\n        ""_predict_mean_and_var"",\n    )\n\n    def is_analytic(likelihood):\n        assert not isinstance(likelihood, MonteCarloLikelihood)\n        assert isinstance(likelihood, ScalarLikelihood)\n        quadrature_fallback = getattr(ScalarLikelihood, method_name)\n        actual_method = getattr(likelihood.__class__, method_name)\n        return actual_method is not quadrature_fallback\n\n    return [l for l in scalar_likelihood_setups if is_analytic(get_likelihood(l))]\n\n\ndef get_likelihood(likelihood_setup):\n    if isinstance(likelihood_setup, type(pytest.param())):\n        (likelihood_setup,) = likelihood_setup.values\n    return likelihood_setup.likelihood\n\n\ndef test_no_missing_likelihoods():\n    tested_likelihood_types = [get_likelihood(l).__class__ for l in likelihood_setups]\n    for likelihood_class in gpflow.ci_utils.subclasses(Likelihood):\n        if likelihood_class in (ScalarLikelihood, MonteCarloLikelihood):\n            # abstract base classes that cannot be tested\n            continue\n\n        if likelihood_class in tested_likelihood_types:\n            # tested by parametrized tests (see test_multiclass.py for MultiClass quadrature)\n            continue\n\n        if likelihood_class is gpflow.likelihoods.SwitchedLikelihood:\n            # tested separately, see test_switched_likelihood.py\n            continue\n\n        if issubclass(likelihood_class, MonteCarloLikelihood):\n            if likelihood_class is GaussianMC:\n                continue  # tested explicitly by test_montecarlo_*\n            if likelihood_class is gpflow.likelihoods.Softmax:\n                continue  # tested explicitly by test_softmax_{y_shape_assert,bernoulli_equivalence}, see test_multiclass.py\n\n        assert False, f""no test for likelihood class {likelihood_class}""\n\n\n@pytest.mark.parametrize(""likelihood_setup"", likelihood_setups)\n@pytest.mark.parametrize(""mu, var"", [[Datum.Fmu, tf.zeros_like(Datum.Fmu)]])\ndef test_conditional_mean_and_variance(likelihood_setup, mu, var):\n    """"""\n    Here we make sure that the conditional_mean and conditional_var functions\n    give the same result as the predict_mean_and_var function if the prediction\n    has no uncertainty.\n    """"""\n    mu1 = likelihood_setup.likelihood.conditional_mean(mu)\n    var1 = likelihood_setup.likelihood.conditional_variance(mu)\n    mu2, var2 = likelihood_setup.likelihood.predict_mean_and_var(mu, var)\n    assert_allclose(mu1, mu2, rtol=likelihood_setup.rtol, atol=likelihood_setup.atol)\n    assert_allclose(var1, var2, rtol=likelihood_setup.rtol, atol=likelihood_setup.atol)\n\n\n@pytest.mark.parametrize(""likelihood_setup"", likelihood_setups)\ndef test_variational_expectations(likelihood_setup):\n    """"""\n    Here we make sure that the variational_expectations gives the same result\n    as log_prob if the latent function has no uncertainty.\n    """"""\n    likelihood = likelihood_setup.likelihood\n    F = Datum.F\n    Y = likelihood_setup.Y\n    r1 = likelihood.log_prob(F, Y)\n    r2 = likelihood.variational_expectations(F, tf.zeros_like(F), Y)\n    assert_allclose(r1, r2, atol=likelihood_setup.atol, rtol=likelihood_setup.rtol)\n\n\n@pytest.mark.parametrize(\n    ""likelihood_setup"", filter_analytic_scalar_likelihood(""_variational_expectations"")\n)\n@pytest.mark.parametrize(""mu, var"", [[Datum.Fmu, Datum.Fvar]])\ndef test_scalar_likelihood_quadrature_variational_expectation(likelihood_setup, mu, var):\n    """"""\n    Where quadrature methods have been overwritten, make sure the new code\n    does something close to the quadrature.\n    """"""\n    likelihood, y = likelihood_setup.likelihood, likelihood_setup.Y\n    F1 = likelihood.variational_expectations(mu, var, y)\n    F2 = ScalarLikelihood.variational_expectations(likelihood, mu, var, y)\n    assert_allclose(F1, F2, rtol=likelihood_setup.rtol, atol=likelihood_setup.atol)\n\n\n@pytest.mark.parametrize(\n    ""likelihood_setup"", filter_analytic_scalar_likelihood(""_predict_log_density"")\n)\n@pytest.mark.parametrize(""mu, var"", [[Datum.Fmu, Datum.Fvar]])\ndef test_scalar_likelihood_quadrature_predict_log_density(likelihood_setup, mu, var):\n    likelihood, y = likelihood_setup.likelihood, likelihood_setup.Y\n    F1 = likelihood.predict_log_density(mu, var, y)\n    F2 = ScalarLikelihood.predict_log_density(likelihood, mu, var, y)\n    assert_allclose(F1, F2, rtol=likelihood_setup.rtol, atol=likelihood_setup.atol)\n\n\n@pytest.mark.parametrize(\n    ""likelihood_setup"", filter_analytic_scalar_likelihood(""_predict_mean_and_var"")\n)\n@pytest.mark.parametrize(""mu, var"", [[Datum.Fmu, Datum.Fvar]])\ndef test_scalar_likelihood_quadrature_predict_mean_and_var(likelihood_setup, mu, var):\n    likelihood = likelihood_setup.likelihood\n    F1m, F1v = likelihood.predict_mean_and_var(mu, var)\n    F2m, F2v = ScalarLikelihood.predict_mean_and_var(likelihood, mu, var)\n    assert_allclose(F1m, F2m, rtol=likelihood_setup.rtol, atol=likelihood_setup.atol)\n    assert_allclose(F1v, F2v, rtol=likelihood_setup.rtol, atol=likelihood_setup.atol)\n\n\ndef _make_montecarlo_mu_var_y():\n    mu_var_y = [tf.random.normal((3, 10), dtype=tf.float64)] * 3\n    mu_var_y[1] = 0.01 * (mu_var_y[1] ** 2)\n    return mu_var_y\n\n\ndef _make_montecarlo_likelihoods(var):\n    gaussian_mc_likelihood = GaussianMC(var)\n    gaussian_mc_likelihood.num_monte_carlo_points = 1000000\n    return gaussian_mc_likelihood, Gaussian(var)\n\n\n@pytest.mark.parametrize(""likelihood_var"", [0.3, 0.5, 1])\n@pytest.mark.parametrize(""mu, var, y"", [_make_montecarlo_mu_var_y()])\ndef test_montecarlo_variational_expectation(likelihood_var, mu, var, y):\n    likelihood_gaussian_mc, likelihood_gaussian = _make_montecarlo_likelihoods(likelihood_var)\n    assert_allclose(\n        likelihood_gaussian_mc.variational_expectations(mu, var, y),\n        likelihood_gaussian.variational_expectations(mu, var, y),\n        rtol=5e-4,\n        atol=1e-4,\n    )\n\n\n@pytest.mark.parametrize(""likelihood_var"", [0.3, 0.5, 1.0])\n@pytest.mark.parametrize(""mu, var, y"", [_make_montecarlo_mu_var_y()])\ndef test_montecarlo_predict_log_density(likelihood_var, mu, var, y):\n    likelihood_gaussian_mc, likelihood_gaussian = _make_montecarlo_likelihoods(likelihood_var)\n    assert_allclose(\n        likelihood_gaussian_mc.predict_log_density(mu, var, y),\n        likelihood_gaussian.predict_log_density(mu, var, y),\n        rtol=5e-4,\n        atol=1e-4,\n    )\n\n\n@pytest.mark.parametrize(""likelihood_var"", [0.3, 0.5, 1.0])\n@pytest.mark.parametrize(""mu, var, y"", [_make_montecarlo_mu_var_y()])\ndef test_montecarlo_predict_mean_and_var(likelihood_var, mu, var, y):\n    likelihood_gaussian_mc, likelihood_gaussian = _make_montecarlo_likelihoods(likelihood_var)\n    mean1, var1 = likelihood_gaussian_mc.predict_mean_and_var(mu, var)\n    mean2, var2 = likelihood_gaussian.predict_mean_and_var(mu, var)\n    assert_allclose(mean1, mean2, rtol=5e-4, atol=1e-4)\n    assert_allclose(var1, var2, rtol=5e-4, atol=1e-4)\n'"
tests/gpflow/likelihoods/test_multiclass.py,13,"b'# Copyright 2017-2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.likelihoods import (\n    Bernoulli,\n    MultiClass,\n    RobustMax,\n    Softmax,\n)\nfrom gpflow.config import default_float, default_int\nfrom gpflow.utilities import to_default_float, to_default_int\n\ntf.random.set_seed(99012)\n\n\n@pytest.mark.parametrize(""num, dimF"", [[10, 5], [3, 2]])\n@pytest.mark.parametrize(""dimY"", [10, 2, 1])\ndef test_softmax_y_shape_assert(num, dimF, dimY):\n    """"""\n    SoftMax assumes the class is given as a label (not, e.g., one-hot\n    encoded), and hence just uses the first column of Y. To prevent\n    silent errors, there is a tf assertion that ensures Y only has one\n    dimension. This test checks that this assert works as intended.\n    """"""\n    F = tf.random.normal((num, dimF))\n    dY = np.vstack((np.random.randn(num - 3, dimY), np.ones((3, dimY)))) > 0\n    Y = tf.convert_to_tensor(dY, dtype=default_int())\n    likelihood = Softmax(dimF)\n    try:\n        likelihood.log_prob(F, Y)\n    except tf.errors.InvalidArgumentError as e:\n        assert ""Condition x == y did not hold."" in e.message\n\n\n@pytest.mark.parametrize(""num"", [10, 3])\n@pytest.mark.parametrize(""dimF, dimY"", [[2, 1]])\ndef test_softmax_bernoulli_equivalence(num, dimF, dimY):\n    dF = np.vstack((np.random.randn(num - 3, dimF), np.array([[-3.0, 0.0], [3, 0.0], [0.0, 0.0]])))\n    dY = np.vstack((np.random.randn(num - 3, dimY), np.ones((3, dimY)))) > 0\n    F = to_default_float(dF)\n    Fvar = tf.exp(tf.stack([F[:, 1], -10.0 + tf.zeros(F.shape[0], dtype=F.dtype)], axis=1))\n    F = tf.stack([F[:, 0], tf.zeros(F.shape[0], dtype=F.dtype)], axis=1)\n    Y = to_default_int(dY)\n    Ylabel = 1 - Y\n\n    softmax_likelihood = Softmax(dimF)\n    bernoulli_likelihood = Bernoulli(invlink=tf.sigmoid)\n    softmax_likelihood.num_monte_carlo_points = int(\n        0.3e7\n    )  # Minimum number of points to pass the test on CircleCI\n    bernoulli_likelihood.num_gauss_hermite_points = 40\n\n    assert_allclose(\n        softmax_likelihood.conditional_mean(F)[:, :1],\n        bernoulli_likelihood.conditional_mean(F[:, :1]),\n    )\n\n    assert_allclose(\n        softmax_likelihood.conditional_variance(F)[:, :1],\n        bernoulli_likelihood.conditional_variance(F[:, :1]),\n    )\n\n    assert_allclose(\n        softmax_likelihood.log_prob(F, Ylabel), bernoulli_likelihood.log_prob(F[:, :1], Y.numpy()),\n    )\n\n    mean1, var1 = softmax_likelihood.predict_mean_and_var(F, Fvar)\n    mean2, var2 = bernoulli_likelihood.predict_mean_and_var(F[:, :1], Fvar[:, :1])\n\n    assert_allclose(mean1[:, 0, None], mean2, rtol=2e-3)\n    assert_allclose(var1[:, 0, None], var2, rtol=2e-3)\n\n    ls_ve = softmax_likelihood.variational_expectations(F, Fvar, Ylabel)\n    lb_ve = bernoulli_likelihood.variational_expectations(F[:, :1], Fvar[:, :1], Y.numpy())\n    assert_allclose(ls_ve, lb_ve, rtol=5e-3)\n\n\n@pytest.mark.parametrize(""num_classes, num_points"", [[10, 3]])\n@pytest.mark.parametrize(""tol, epsilon"", [[1e-4, 1e-3]])\ndef test_robust_max_multiclass_symmetric(num_classes, num_points, tol, epsilon):\n    """"""\n    This test is based on the observation that for\n    symmetric inputs the class predictions must have equal probability.\n    """"""\n    rng = np.random.RandomState(1)\n    p = 1.0 / num_classes\n    F = tf.ones((num_points, num_classes), dtype=default_float())\n    Y = tf.convert_to_tensor(rng.randint(num_classes, size=(num_points, 1)), dtype=default_float())\n\n    likelihood = MultiClass(num_classes)\n    likelihood.invlink.epsilon = tf.convert_to_tensor(epsilon, dtype=default_float())\n\n    mu, _ = likelihood.predict_mean_and_var(F, F)\n    pred = likelihood.predict_log_density(F, F, Y)\n    variational_expectations = likelihood.variational_expectations(F, F, Y)\n\n    expected_mu = (p * (1.0 - epsilon) + (1.0 - p) * epsilon / (num_classes - 1)) * np.ones(\n        (num_points, 1)\n    )\n    expected_log_density = np.log(expected_mu)\n\n    # assert_allclose() would complain about shape mismatch\n    assert np.allclose(mu, expected_mu, tol, tol)\n    assert np.allclose(pred, expected_log_density, 1e-3, 1e-3)\n\n    validation_variational_expectation = p * np.log(1.0 - epsilon) + (1.0 - p) * np.log(\n        epsilon / (num_classes - 1)\n    )\n    assert_allclose(\n        variational_expectations,\n        np.ones((num_points,)) * validation_variational_expectation,\n        tol,\n        tol,\n    )\n\n\n@pytest.mark.parametrize(""num_classes, num_points"", [[5, 100]])\n@pytest.mark.parametrize(\n    ""mock_prob, expected_prediction, tol, epsilon"",\n    [\n        [0.73, -0.5499780059, 1e-4, 0.231]\n        # Expected prediction evaluated on calculator:\n        # log((1 - \xce\xb5) * 0.73 + (1-0.73) * \xce\xb5 / (num_classes -1))\n    ],\n)\ndef test_robust_max_multiclass_predict_log_density(\n    num_classes, num_points, mock_prob, expected_prediction, tol, epsilon\n):\n    class MockRobustMax(RobustMax):\n        def prob_is_largest(self, Y, Fmu, Fvar, gh_x, gh_w):\n            return tf.ones((num_points, 1), dtype=default_float()) * mock_prob\n\n    likelihood = MultiClass(num_classes, invlink=MockRobustMax(num_classes, epsilon))\n    F = tf.ones((num_points, num_classes))\n    rng = np.random.RandomState(1)\n    Y = to_default_int(rng.randint(num_classes, size=(num_points, 1)))\n    prediction = likelihood.predict_log_density(F, F, Y)\n\n    assert_allclose(prediction, expected_prediction, tol, tol)\n\n\n@pytest.mark.parametrize(""num_classes"", [5, 100])\n@pytest.mark.parametrize(""initial_epsilon, new_epsilon"", [[1e-3, 0.412]])\ndef test_robust_max_multiclass_eps_k1_changes(num_classes, initial_epsilon, new_epsilon):\n    """"""\n    Checks that eps K1 changes when epsilon changes. This used to not happen and had to be\n    manually changed.\n    """"""\n    likelihood = RobustMax(num_classes, initial_epsilon)\n    expected_eps_k1 = initial_epsilon / (num_classes - 1.0)\n    actual_eps_k1 = likelihood.eps_k1\n    assert_allclose(expected_eps_k1, actual_eps_k1)\n\n    likelihood.epsilon = tf.convert_to_tensor(new_epsilon, dtype=default_float())\n    expected_eps_k2 = new_epsilon / (num_classes - 1.0)\n    actual_eps_k2 = likelihood.eps_k1\n    assert_allclose(expected_eps_k2, actual_eps_k2)\n\n\n@pytest.mark.skip(\n    ""ndiagquad cannot handle MultiClass (see https://github.com/GPflow/GPflow/issues/1091""\n)\ndef test_multiclass_quadrature_variational_expectations():\n    pass\n\n\n@pytest.mark.skip(\n    ""ndiagquad cannot handle MultiClass (see https://github.com/GPflow/GPflow/issues/1091""\n)\ndef test_multiclass_quadrature_predict_log_density():\n    pass\n\n\n@pytest.mark.skip(\n    ""ndiagquad cannot handle MultiClass (see https://github.com/GPflow/GPflow/issues/1091""\n)\ndef test_multiclass_quadrature_predict_mean_and_var():\n    pass\n'"
tests/gpflow/likelihoods/test_switched_likelihood.py,13,"b'# Copyright 2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.inducing_variables import InducingPoints\nfrom gpflow.likelihoods import Gaussian, StudentT, SwitchedLikelihood\nfrom gpflow.inducing_variables import InducingPoints\n\n\n@pytest.mark.parametrize(""Y_list"", [[tf.random.normal((i, 2)) for i in range(3, 6)]])\n@pytest.mark.parametrize(""F_list"", [[tf.random.normal((i, 2)) for i in range(3, 6)]])\n@pytest.mark.parametrize(""Fvar_list"", [[tf.exp(tf.random.normal((i, 2))) for i in range(3, 6)]])\n@pytest.mark.parametrize(""Y_label"", [[tf.ones((i, 2)) * (i - 3.0) for i in range(3, 6)]])\ndef test_switched_likelihood_log_prob(Y_list, F_list, Fvar_list, Y_label):\n    """"""\n    SwitchedLikelihood is separately tested here.\n    Here, we make sure the partition-stitch works fine.\n    """"""\n    Y_perm = list(range(3 + 4 + 5))\n    np.random.shuffle(Y_perm)\n    # shuffle the original data\n    Y_sw = np.hstack([np.concatenate(Y_list), np.concatenate(Y_label)])[Y_perm, :3]\n    F_sw = np.concatenate(F_list)[Y_perm, :]\n    likelihoods = [Gaussian()] * 3\n    for lik in likelihoods:\n        lik.variance = np.exp(np.random.randn(1)).squeeze().astype(np.float32)\n    switched_likelihood = SwitchedLikelihood(likelihoods)\n\n    switched_results = switched_likelihood.log_prob(F_sw, Y_sw)\n    results = [lik.log_prob(f, y) for lik, y, f in zip(likelihoods, Y_list, F_list)]\n\n    assert_allclose(switched_results, np.concatenate(results)[Y_perm])\n\n\n@pytest.mark.parametrize(""Y_list"", [[tf.random.normal((i, 2)) for i in range(3, 6)]])\n@pytest.mark.parametrize(""F_list"", [[tf.random.normal((i, 2)) for i in range(3, 6)]])\n@pytest.mark.parametrize(""Fvar_list"", [[tf.exp(tf.random.normal((i, 2))) for i in range(3, 6)]])\n@pytest.mark.parametrize(""Y_label"", [[tf.ones((i, 2)) * (i - 3.0) for i in range(3, 6)]])\ndef test_switched_likelihood_predict_log_density(Y_list, F_list, Fvar_list, Y_label):\n    Y_perm = list(range(3 + 4 + 5))\n    np.random.shuffle(Y_perm)\n    # shuffle the original data\n    Y_sw = np.hstack([np.concatenate(Y_list), np.concatenate(Y_label)])[Y_perm, :3]\n    F_sw = np.concatenate(F_list)[Y_perm, :]\n    Fvar_sw = np.concatenate(Fvar_list)[Y_perm, :]\n\n    likelihoods = [Gaussian()] * 3\n    for lik in likelihoods:\n        lik.variance = np.exp(np.random.randn(1)).squeeze().astype(np.float32)\n    switched_likelihood = SwitchedLikelihood(likelihoods)\n\n    switched_results = switched_likelihood.predict_log_density(F_sw, Fvar_sw, Y_sw)\n    # likelihood\n    results = [\n        lik.predict_log_density(f, fvar, y)\n        for lik, y, f, fvar in zip(likelihoods, Y_list, F_list, Fvar_list)\n    ]\n    assert_allclose(switched_results, np.concatenate(results)[Y_perm])\n\n\n@pytest.mark.parametrize(""Y_list"", [[tf.random.normal((i, 2)) for i in range(3, 6)]])\n@pytest.mark.parametrize(""F_list"", [[tf.random.normal((i, 2)) for i in range(3, 6)]])\n@pytest.mark.parametrize(""Fvar_list"", [[tf.exp(tf.random.normal((i, 2))) for i in range(3, 6)]])\n@pytest.mark.parametrize(""Y_label"", [[tf.ones((i, 2)) * (i - 3.0) for i in range(3, 6)]])\ndef test_switched_likelihood_variational_expectations(Y_list, F_list, Fvar_list, Y_label):\n    Y_perm = list(range(3 + 4 + 5))\n    np.random.shuffle(Y_perm)\n    # shuffle the original data\n    Y_sw = np.hstack([np.concatenate(Y_list), np.concatenate(Y_label)])[Y_perm, :3]\n    F_sw = np.concatenate(F_list)[Y_perm, :]\n    Fvar_sw = np.concatenate(Fvar_list)[Y_perm, :]\n\n    likelihoods = [Gaussian()] * 3\n    for lik in likelihoods:\n        lik.variance = np.exp(np.random.randn(1)).squeeze().astype(np.float32)\n    switched_likelihood = SwitchedLikelihood(likelihoods)\n\n    switched_results = switched_likelihood.variational_expectations(F_sw, Fvar_sw, Y_sw)\n    results = [\n        lik.variational_expectations(f, fvar, y)\n        for lik, y, f, fvar in zip(likelihoods, Y_list, F_list, Fvar_list)\n    ]\n    assert_allclose(switched_results, np.concatenate(results)[Y_perm])\n\n\ndef test_switched_likelihood_with_vgp():\n    """"""\n    Reproduces the bug in https://github.com/GPflow/GPflow/issues/951\n    """"""\n    X = np.random.randn(12 + 15, 1)\n    Y = np.random.randn(12 + 15, 1)\n    idx = np.array([0] * 12 + [1] * 15)\n    Y_aug = np.c_[Y, idx]\n    assert Y_aug.shape == (12 + 15, 2)\n\n    kernel = gpflow.kernels.Matern32()\n    likelihood = gpflow.likelihoods.SwitchedLikelihood([StudentT(), StudentT()])\n    model = gpflow.models.VGP((X, Y_aug), kernel=kernel, likelihood=likelihood)\n    # without bugfix, optimization errors out\n    opt = gpflow.optimizers.Scipy()\n    opt.minimize(model.training_loss, model.trainable_variables, options=dict(maxiter=1))\n\n\n@pytest.mark.parametrize(""num_latent_gps"", [1, 2])\ndef test_switched_likelihood_regression_valid_num_latent_gps(num_latent_gps):\n    """"""\n    A Regression test when using Switched likelihood: the number of latent\n    functions in a GP model must be equal to the number of columns in Y minus\n    one. The final column of Y is used to index the switch. If the number of\n    latent functions does not match, an exception will be raised.\n    """"""\n    x = np.random.randn(100, 1)\n    y = np.hstack((np.random.randn(100, 1), np.random.randint(0, 3, (100, 1))))\n    data = x, y\n\n    Z = InducingPoints(np.random.randn(num_latent_gps, 1))\n    likelihoods = [StudentT()] * 3\n    switched_likelihood = SwitchedLikelihood(likelihoods)\n    m = gpflow.models.SVGP(\n        kernel=gpflow.kernels.Matern12(),\n        inducing_variable=Z,\n        likelihood=switched_likelihood,\n        num_latent_gps=num_latent_gps,\n    )\n    if num_latent_gps == 1:\n        _ = m.training_loss(data)\n    else:\n        with pytest.raises(tf.errors.InvalidArgumentError):\n            _ = m.training_loss(data)\n'"
tests/gpflow/models/__init__.py,0,b''
tests/gpflow/models/test_gplvm.py,0,"b'# Copyright 2019 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow.utilities.ops import pca_reduce\n\n\nclass Data:\n    rng = np.random.RandomState(999)\n    N = 20\n    D = 5\n    Y = rng.randn(N, D)\n    Q = 2\n    M = 10\n    X = rng.randn(N, Q)\n\n\n@pytest.mark.parametrize(\n    ""kernel"",\n    [\n        None,  # default kernel: SquaredExponential\n        gpflow.kernels.Periodic(base_kernel=gpflow.kernels.SquaredExponential()),\n    ],\n)\ndef test_gplvm_with_kernels(kernel):\n    m = gpflow.models.GPLVM(Data.Y, Data.Q, kernel=kernel)\n    lml_initial = m.log_marginal_likelihood()\n    opt = gpflow.optimizers.Scipy()\n    opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=2))\n    assert m.log_marginal_likelihood() > lml_initial\n\n\ndef test_bayesian_gplvm_1d():\n    Q = 1\n    kernel = gpflow.kernels.SquaredExponential()\n    inducing_variable = np.linspace(0, 1, Data.M)[:, None]\n    m = gpflow.models.BayesianGPLVM(\n        Data.Y,\n        np.zeros((Data.N, Q)),\n        np.ones((Data.N, Q)),\n        kernel,\n        inducing_variable=inducing_variable,\n    )\n    assert len(m.inducing_variable) == Data.M\n\n    elbo_initial = m.elbo()\n    opt = gpflow.optimizers.Scipy()\n    opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=2))\n    assert m.elbo() > elbo_initial\n\n\ndef test_bayesian_gplvm_2d():\n    Q = 2  # latent dimensions\n    X_data_mean = pca_reduce(Data.Y, Q)\n    kernel = gpflow.kernels.SquaredExponential()\n\n    m = gpflow.models.BayesianGPLVM(\n        Data.Y, X_data_mean, np.ones((Data.N, Q)), kernel, num_inducing_variables=Data.M\n    )\n\n    elbo_initial = m.elbo()\n    opt = gpflow.optimizers.Scipy()\n    opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=2))\n    assert m.elbo() > elbo_initial\n\n    # test prediction\n    Xtest = Data.rng.randn(10, Q)\n    mu_f, var_f = m.predict_f(Xtest)\n    mu_fFull, var_fFull = m.predict_f(Xtest, full_cov=True)\n    np.testing.assert_allclose(mu_fFull, mu_f)\n\n    for i in range(Data.D):\n        np.testing.assert_allclose(var_f[:, i], np.diag(var_fFull[:, :, i]))\n\n\ndef test_gplvm_constructor_checks():\n    with pytest.raises(ValueError):\n        assert Data.X.shape[1] == Data.Q\n        latents_wrong_shape = Data.X[:, : Data.Q - 1]\n        gpflow.models.GPLVM(Data.Y, Data.Q, X_data_mean=latents_wrong_shape)\n    with pytest.raises(ValueError):\n        observations_wrong_shape = Data.Y[:, : Data.Q - 1]\n        gpflow.models.GPLVM(observations_wrong_shape, Data.Q)\n    with pytest.raises(ValueError):\n        observations_wrong_shape = Data.Y[:, : Data.Q - 1]\n        gpflow.models.GPLVM(observations_wrong_shape, Data.Q, X_data_mean=Data.X)\n\n\ndef test_bayesian_gplvm_constructor_check():\n    Q = 1\n    kernel = gpflow.kernels.SquaredExponential()\n    inducing_variable = np.linspace(0, 1, Data.M)[:, None]\n    with pytest.raises(ValueError):\n        gpflow.models.BayesianGPLVM(\n            Data.Y,\n            np.zeros((Data.N, Q)),\n            np.ones((Data.N, Q)),\n            kernel,\n            inducing_variable=inducing_variable,\n            num_inducing_variables=len(inducing_variable),\n        )\n'"
tests/gpflow/models/test_gpr.py,2,"b'# Copyright 2019 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gpflow\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom gpflow import set_trainable\n\nrng = np.random.RandomState(0)\n\n\nclass Data:\n    N = 10\n    D = 1\n    X = rng.rand(N, D)\n    Y = rng.rand(N, 1)\n    ls = 2.0\n    var = 1.0\n\n\ndef test_non_trainable_model_objective():\n    """"""\n    Checks that we can still compute the objective of a model that has no\n    trainable parameters whatsoever (regression test for bug in log_prior()).\n    In this case we have no priors, so log_prior should be zero to add no\n    contribution to the objective.\n    """"""\n    model = gpflow.models.GPR(\n        (Data.X, Data.Y),\n        kernel=gpflow.kernels.SquaredExponential(lengthscales=Data.ls, variance=Data.var),\n    )\n\n    set_trainable(model, False)\n\n    _ = model.log_marginal_likelihood()\n    assert model.log_prior_density() == 0.0\n\n\ndef test_varying_data():\n    input_dim = 2\n    output_dim = 1\n\n    N = 5\n    X, Y = rng.randn(N, input_dim), rng.randn(N, output_dim)\n\n    var_data = (tf.Variable(X, shape=[None, input_dim]), tf.Variable(Y, shape=[None, output_dim]))\n    m = gpflow.models.GPR(var_data, gpflow.kernels.SquaredExponential())\n\n    lml_func = tf.function(m.log_marginal_likelihood)\n    old_lml = lml_func()\n\n    new_N = 7\n    new_X, new_Y = rng.randn(new_N, input_dim), rng.randn(new_N, output_dim)\n\n    # assign new data:\n    for var, new_value in zip(var_data, (new_X, new_Y)):\n        var.assign(new_value)\n\n    new_lml = lml_func()  # re-use compiled function\n\n    assert (\n        np.abs((old_lml - new_lml) / (new_lml + old_lml)) > 0.1\n    ), ""we expect the LML for different data to be significantly different""\n'"
tests/gpflow/models/test_mcmc.py,2,"b'# Copyright 2016-2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.config import default_float\n\n\ndef test_sparse_mcmc_likelihoods_and_gradients():\n    """"""\n    This test makes sure that when the inducing points are the same as the data\n    points, the sparse mcmc is the same as full mcmc\n    """"""\n    rng = np.random.RandomState(0)\n    X, Y = rng.randn(10, 1), rng.randn(10, 1)\n    v_vals = rng.randn(10, 1)\n\n    likelihood = gpflow.likelihoods.StudentT()\n    model_1 = gpflow.models.GPMC(\n        data=(X, Y), kernel=gpflow.kernels.Exponential(), likelihood=likelihood\n    )\n    model_2 = gpflow.models.SGPMC(\n        data=(X, Y),\n        kernel=gpflow.kernels.Exponential(),\n        inducing_variable=X.copy(),\n        likelihood=likelihood,\n    )\n    model_1.V = tf.convert_to_tensor(v_vals, dtype=default_float())\n    model_2.V = tf.convert_to_tensor(v_vals, dtype=default_float())\n    model_1.kernel.lengthscales.assign(0.8)\n    model_2.kernel.lengthscales.assign(0.8)\n    model_1.kernel.variance.assign(4.2)\n    model_2.kernel.variance.assign(4.2)\n\n    assert_allclose(\n        model_1.log_posterior_density(), model_2.log_posterior_density(), rtol=1e-5, atol=1e-5\n    )\n'"
tests/gpflow/models/test_methods.py,0,"b'# Copyright 2016 the GPflow authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose, assert_array_equal, assert_array_less\n\nimport gpflow\nfrom gpflow.config import default_float\n\n\n# ------------------------------------------\n# Data classes: storing constants\n# ------------------------------------------\n\n\n@dataclass(frozen=True)\nclass Datum:\n    rng: np.random.RandomState = np.random.RandomState(0)\n    X: np.ndarray = rng.randn(100, 2)\n    Y: np.ndarray = rng.randn(100, 1)\n    Z: np.ndarray = rng.randn(10, 2)\n    Xs: np.ndarray = rng.randn(10, 2)\n    lik = gpflow.likelihoods.Gaussian()\n    kernel = gpflow.kernels.Matern32()\n\n\ndefault_datum = Datum()\n\n\n_gp_models = [\n    gpflow.models.VGP((default_datum.X, default_datum.Y), default_datum.kernel, default_datum.lik),\n    gpflow.models.GPMC((default_datum.X, default_datum.Y), default_datum.kernel, default_datum.lik),\n    gpflow.models.SGPMC(\n        (default_datum.X, default_datum.Y),\n        default_datum.kernel,\n        default_datum.lik,\n        inducing_variable=default_datum.Z,\n    ),\n    gpflow.models.SGPR(\n        (default_datum.X, default_datum.Y), default_datum.kernel, inducing_variable=default_datum.Z,\n    ),\n    gpflow.models.GPR((default_datum.X, default_datum.Y), default_datum.kernel),\n    gpflow.models.GPRFITC(\n        (default_datum.X, default_datum.Y), default_datum.kernel, inducing_variable=default_datum.Z,\n    ),\n]\n\n_state_less_gp_models = [\n    gpflow.models.SVGP(default_datum.kernel, default_datum.lik, inducing_variable=default_datum.Z)\n]\n\n\n@pytest.mark.parametrize(""model"", _state_less_gp_models + _gp_models)\ndef test_methods_predict_f(model):\n    mf, vf = model.predict_f(default_datum.Xs)\n    assert_array_equal(mf.shape, vf.shape)\n    assert_array_equal(mf.shape, (10, 1))\n    assert_array_less(np.full_like(vf, -1e-6), vf)\n\n\n@pytest.mark.parametrize(""model"", _state_less_gp_models + _gp_models)\ndef test_methods_predict_y(model):\n    mf, vf = model.predict_y(default_datum.Xs)\n    assert_array_equal(mf.shape, vf.shape)\n    assert_array_equal(mf.shape, (10, 1))\n    assert_array_less(np.full_like(vf, -1e-6), vf)\n\n\n@pytest.mark.parametrize(""model"", _state_less_gp_models + _gp_models)\ndef test_methods_predict_log_density(model):\n    rng = Datum().rng\n    Ys = rng.randn(10, 1)\n    d = model.predict_log_density((default_datum.Xs, Ys))\n    assert_array_equal(d.shape, (10,))\n'"
tests/gpflow/models/test_model_predict.py,0,"b'# Copyright 2016 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\n\nimport gpflow\nfrom gpflow.inducing_variables import InducingPoints\nfrom gpflow.kernels import Matern32\n\nrng = np.random.RandomState(0)\n\n\nclass ModelSetup:\n    def __init__(\n        self,\n        model_class,\n        kernel=Matern32(),\n        likelihood=gpflow.likelihoods.Gaussian(),\n        whiten=None,\n        q_diag=None,\n        requires_inducing_variables=True,\n        requires_data=False,\n        requires_likelihood=True,\n    ):\n\n        self.model_class = model_class\n        self.kernel = kernel\n        self.likelihood = likelihood\n        self.whiten = whiten\n        self.q_diag = q_diag\n        self.requires_inducing_variables = requires_inducing_variables\n        self.requires_data = requires_data\n        self.requires_likelihood = requires_likelihood\n\n    def get_model(self, Z, num_latent_gps, data=None):\n        params = dict(kernel=self.kernel, num_latent_gps=num_latent_gps)\n\n        if self.whiten is not None and self.q_diag is not None:\n            params.update(inducing_variable=Z, whiten=self.whiten, q_diag=self.q_diag)\n\n        if self.requires_inducing_variables:\n            params.update(dict(inducing_variable=Z))\n\n        if self.requires_data:\n            params.update(dict(data=data))\n\n        if self.requires_likelihood:\n            params.update(dict(likelihood=self.likelihood))\n\n        return self.model_class(**params)\n\n    def __repr__(self):\n        return f""ModelSetup({self.model_class.__name__}, {self.whiten}, {self.q_diag})""\n\n\nmodel_setups = [\n    ModelSetup(model_class=gpflow.models.SVGP, whiten=False, q_diag=True),\n    ModelSetup(model_class=gpflow.models.SVGP, whiten=True, q_diag=False),\n    ModelSetup(model_class=gpflow.models.SVGP, whiten=True, q_diag=True),\n    ModelSetup(model_class=gpflow.models.SVGP, whiten=False, q_diag=False),\n    ModelSetup(model_class=gpflow.models.SGPR, requires_data=True, requires_likelihood=False),\n    ModelSetup(\n        model_class=gpflow.models.VGP, requires_inducing_variables=False, requires_data=True,\n    ),\n    #     ModelSetup(model_class=gpflow.models.GPRF),\n    ModelSetup(\n        model_class=gpflow.models.GPMC, requires_data=True, requires_inducing_variables=False,\n    ),\n    ModelSetup(\n        model_class=gpflow.models.SGPMC, requires_data=True, requires_inducing_variables=True,\n    ),\n]\n\n\n@pytest.mark.parametrize(""Ntrain, Ntest, D"", [[100, 10, 2]])\ndef test_gaussian_mean_and_variance(Ntrain, Ntest, D):\n    data = rng.randn(Ntrain, D), rng.randn(Ntrain, 1)\n    Xtest, _ = rng.randn(Ntest, D), rng.randn(Ntest, 1)\n    kernel = Matern32() + gpflow.kernels.White()\n    model_gp = gpflow.models.GPR(data, kernel=kernel)\n\n    mu_f, var_f = model_gp.predict_f(Xtest)\n    mu_y, var_y = model_gp.predict_y(Xtest)\n\n    assert np.allclose(mu_f, mu_y)\n    assert np.allclose(var_f, var_y - 1.0)\n\n\n@pytest.mark.parametrize(""Ntrain, Ntest, D"", [[100, 10, 2]])\ndef test_gaussian_log_density(Ntrain, Ntest, D):\n    data = rng.randn(Ntrain, D), rng.randn(Ntrain, 1)\n    Xtest, Ytest = rng.randn(Ntest, D), rng.randn(Ntest, 1)\n    kernel = Matern32() + gpflow.kernels.White()\n    model_gp = gpflow.models.GPR(data, kernel=kernel)\n\n    mu_y, var_y = model_gp.predict_y(Xtest)\n    data = Xtest, Ytest\n    log_density = model_gp.predict_log_density(data)\n    log_density_hand = np.squeeze(\n        -0.5 * np.log(2 * np.pi) - 0.5 * np.log(var_y) - 0.5 * np.square(mu_y - Ytest) / var_y,\n        axis=-1,\n    )\n\n    assert np.allclose(log_density_hand, log_density)\n\n\n@pytest.mark.parametrize(""input_dim, output_dim, N, Ntest, M"", [[3, 2, 20, 30, 5]])\ndef test_gaussian_full_cov(input_dim, output_dim, N, Ntest, M):\n    covar_shape = (output_dim, Ntest, Ntest)\n    X, Y, Z = rng.randn(N, input_dim), rng.randn(N, output_dim), rng.randn(M, input_dim)\n    Xtest = rng.randn(Ntest, input_dim)\n    kernel = Matern32()\n    model_gp = gpflow.models.GPR([X, Y], kernel=kernel)\n\n    mu1, var = model_gp.predict_f(Xtest, full_cov=False)\n    mu2, covar = model_gp.predict_f(Xtest, full_cov=True)\n\n    assert np.allclose(mu1, mu2, atol=1.0e-10)\n    assert covar.shape == covar_shape\n    assert var.shape == (Ntest, output_dim)\n    for i in range(output_dim):\n        assert np.allclose(var[:, i], np.diag(covar[i, :, :]))\n\n\n@pytest.mark.parametrize(""input_dim, output_dim, N, Ntest, M, num_samples"", [[3, 2, 20, 30, 5, 5]])\ndef test_gaussian_full_cov_samples(input_dim, output_dim, N, Ntest, M, num_samples):\n    samples_shape = (num_samples, Ntest, output_dim)\n    X, Y, _ = rng.randn(N, input_dim), rng.randn(N, output_dim), rng.randn(M, input_dim)\n    Xtest = rng.randn(Ntest, input_dim)\n    kernel = Matern32()\n    model_gp = gpflow.models.GPR([X, Y], kernel=kernel)\n\n    samples = model_gp.predict_f_samples(Xtest, num_samples)\n    assert samples.shape == samples_shape\n\n    samples = model_gp.predict_f_samples(Xtest, num_samples, full_cov=False)\n    assert samples.shape == samples_shape\n\n\n@pytest.mark.parametrize(""model_setup"", model_setups)\n@pytest.mark.parametrize(""input_dim"", [3])\n@pytest.mark.parametrize(""output_dim"", [2])\n@pytest.mark.parametrize(""N"", [20])\n@pytest.mark.parametrize(""Ntest"", [30])\n@pytest.mark.parametrize(""M"", [5])\ndef test_other_models_full_cov(model_setup, input_dim, output_dim, N, Ntest, M):\n    covar_shape = (output_dim, Ntest, Ntest)\n    X, Y = rng.randn(N, input_dim), rng.randn(N, output_dim)\n    Z = InducingPoints(rng.randn(M, input_dim))\n    Xtest = rng.randn(Ntest, input_dim)\n    model_gp = model_setup.get_model(Z, num_latent_gps=output_dim, data=(X, Y))\n\n    mu1, var = model_gp.predict_f(Xtest, full_cov=False)\n    mu2, covar = model_gp.predict_f(Xtest, full_cov=True)\n\n    assert np.allclose(mu1, mu2, atol=1.0e-10)\n    assert covar.shape == covar_shape\n    assert var.shape == (Ntest, output_dim)\n    for i in range(output_dim):\n        assert np.allclose(var[:, i], np.diag(covar[i, :, :]))\n\n\n@pytest.mark.parametrize(""model_setup"", model_setups)\n@pytest.mark.parametrize(""input_dim"", [3])\n@pytest.mark.parametrize(""output_dim"", [2])\n@pytest.mark.parametrize(""N"", [20])\n@pytest.mark.parametrize(""Ntest"", [30])\n@pytest.mark.parametrize(""M"", [5])\n@pytest.mark.parametrize(""num_samples"", [5])\ndef test_other_models_full_cov_samples(\n    model_setup, input_dim, output_dim, N, Ntest, M, num_samples\n):\n    samples_shape = (num_samples, Ntest, output_dim)\n    X, Y, Z = rng.randn(N, input_dim), rng.randn(N, output_dim), rng.randn(M, input_dim)\n    Xtest = rng.randn(Ntest, input_dim)\n    model_gp = model_setup.get_model(Z, num_latent_gps=output_dim, data=(X, Y))\n\n    samples = model_gp.predict_f_samples(Xtest, num_samples)\n    assert samples.shape == samples_shape\n'"
tests/gpflow/models/test_sgpr.py,1,"b'# Copyright 2016-2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow.config import default_float\nfrom gpflow.utilities import to_default_float\n\n\n@dataclass(frozen=True)\nclass Datum:\n    rng: np.random.RandomState = np.random.RandomState(0)\n    X: np.ndarray = rng.randn(100, 2)\n    Y: np.ndarray = rng.randn(100, 1)\n    Z: np.ndarray = rng.randn(10, 2)\n    Xs: np.ndarray = rng.randn(10, 2)\n    lik = gpflow.likelihoods.Gaussian()\n    kernel = gpflow.kernels.Matern32()\n\n\ndef test_sgpr_qu():\n    rng = Datum().rng\n    X = to_default_float(rng.randn(100, 2))\n    Z = to_default_float(rng.randn(20, 2))\n    Y = to_default_float(np.sin(X @ np.array([[-1.4], [0.5]])) + 0.5 * rng.randn(len(X), 1))\n    model = gpflow.models.SGPR(\n        (X, Y), kernel=gpflow.kernels.SquaredExponential(), inducing_variable=Z\n    )\n\n    gpflow.optimizers.Scipy().minimize(model.training_loss, variables=model.trainable_variables)\n\n    qu_mean, qu_cov = model.compute_qu()\n    f_at_Z_mean, f_at_Z_cov = model.predict_f(model.inducing_variable.Z, full_cov=True)\n\n    np.testing.assert_allclose(qu_mean, f_at_Z_mean, rtol=1e-5, atol=1e-5)\n    np.testing.assert_allclose(tf.reshape(qu_cov, (1, 20, 20)), f_at_Z_cov, rtol=1e-5, atol=1e-5)\n'"
tests/gpflow/models/test_svgp.py,2,"b'# Copyright 2016-2020 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose, assert_array_equal, assert_array_less\n\nimport gpflow\nfrom gpflow.config import default_float\nfrom gpflow import set_trainable\n\n\n@dataclass(frozen=True)\nclass DatumSVGP:\n    rng: np.random.RandomState = np.random.RandomState(0)\n    X = rng.randn(20, 1)\n    Y = rng.randn(20, 2) ** 2\n    Z = rng.randn(3, 1)\n    qsqrt = (rng.randn(3, 2) ** 2) * 0.01\n    qmean = rng.randn(3, 2)\n    lik = gpflow.likelihoods.Exponential()\n    data = (X, Y)\n\n\ndefault_datum_svgp = DatumSVGP()\n\n\ndef test_svgp_fixing_q_sqrt():\n    """"""\n    In response to bug #46, we need to make sure that the q_sqrt matrix can be fixed\n    """"""\n    num_latent_gps = default_datum_svgp.Y.shape[1]\n    model = gpflow.models.SVGP(\n        kernel=gpflow.kernels.SquaredExponential(),\n        likelihood=default_datum_svgp.lik,\n        q_diag=True,\n        num_latent_gps=num_latent_gps,\n        inducing_variable=default_datum_svgp.Z,\n        whiten=False,\n    )\n    default_num_trainable_variables = len(model.trainable_variables)\n    set_trainable(model.q_sqrt, False)\n    assert len(model.trainable_variables) == default_num_trainable_variables - 1\n\n\ndef test_svgp_white():\n    """"""\n    Tests that the SVGP bound on the likelihood is the same when using\n    with and without diagonals when whitening.\n    """"""\n    num_latent_gps = default_datum_svgp.Y.shape[1]\n    model_1 = gpflow.models.SVGP(\n        kernel=gpflow.kernels.SquaredExponential(),\n        likelihood=default_datum_svgp.lik,\n        q_diag=True,\n        num_latent_gps=num_latent_gps,\n        inducing_variable=default_datum_svgp.Z,\n        whiten=True,\n    )\n    model_2 = gpflow.models.SVGP(\n        kernel=gpflow.kernels.SquaredExponential(),\n        likelihood=default_datum_svgp.lik,\n        q_diag=False,\n        num_latent_gps=num_latent_gps,\n        inducing_variable=default_datum_svgp.Z,\n        whiten=True,\n    )\n    model_1.q_sqrt.assign(default_datum_svgp.qsqrt)\n    model_1.q_mu.assign(default_datum_svgp.qmean)\n    model_2.q_sqrt.assign(\n        np.array(\n            [np.diag(default_datum_svgp.qsqrt[:, 0]), np.diag(default_datum_svgp.qsqrt[:, 1]),]\n        )\n    )\n    model_2.q_mu.assign(default_datum_svgp.qmean)\n    assert_allclose(model_1.elbo(default_datum_svgp.data), model_2.elbo(default_datum_svgp.data))\n\n\ndef test_svgp_non_white():\n    """"""\n    Tests that the SVGP bound on the likelihood is the same when using\n    with and without diagonals when whitening is not used.\n    """"""\n    num_latent_gps = default_datum_svgp.Y.shape[1]\n    model_1 = gpflow.models.SVGP(\n        kernel=gpflow.kernels.SquaredExponential(),\n        likelihood=default_datum_svgp.lik,\n        q_diag=True,\n        num_latent_gps=num_latent_gps,\n        inducing_variable=default_datum_svgp.Z,\n        whiten=False,\n    )\n    model_2 = gpflow.models.SVGP(\n        kernel=gpflow.kernels.SquaredExponential(),\n        likelihood=default_datum_svgp.lik,\n        q_diag=False,\n        num_latent_gps=num_latent_gps,\n        inducing_variable=default_datum_svgp.Z,\n        whiten=False,\n    )\n    model_1.q_sqrt.assign(default_datum_svgp.qsqrt)\n    model_1.q_mu.assign(default_datum_svgp.qmean)\n    model_2.q_sqrt.assign(\n        np.array(\n            [np.diag(default_datum_svgp.qsqrt[:, 0]), np.diag(default_datum_svgp.qsqrt[:, 1]),]\n        )\n    )\n    model_2.q_mu.assign(default_datum_svgp.qmean)\n    assert_allclose(model_1.elbo(default_datum_svgp.data), model_2.elbo(default_datum_svgp.data))\n\n\ndef _check_models_close(m1, m2, tolerance=1e-2):\n    m1_params = {p.name: p for p in list(m1.trainable_parameters)}\n    m2_params = {p.name: p for p in list(m2.trainable_parameters)}\n    if set(m2_params.keys()) != set(m2_params.keys()):\n        return False\n    for key in m1_params:\n        p1 = m1_params[key]\n        p2 = m2_params[key]\n        if not np.allclose(p1.read_value(), p2.read_value(), rtol=tolerance, atol=tolerance):\n            return False\n    return True\n\n\n@pytest.mark.parametrize(\n    ""indices_1, indices_2, num_data1, num_data2, max_iter"",\n    [[[0, 1], [1, 0], 2, 2, 3], [[0, 1], [0, 0], 1, 2, 1], [[0, 0], [0, 1], 1, 1, 2],],\n)\ndef test_stochastic_gradients(indices_1, indices_2, num_data1, num_data2, max_iter):\n    """"""\n    In response to bug #281, we need to make sure stochastic update\n    happens correctly in tf optimizer mode.\n    To do this compare stochastic updates with deterministic updates\n    that should be equivalent.\n\n    Data term in svgp likelihood is\n    \\sum_{i=1^N}E_{q(i)}[\\log p(y_i | f_i )\n\n    This sum is then approximated with an unbiased minibatch estimate.\n    In this test we substitute a deterministic analogue of the batchs\n    sampler for which we can predict the effects of different updates.\n    """"""\n    X, Y = np.atleast_2d(np.array([0.0, 1.0])).T, np.atleast_2d(np.array([-1.0, 3.0])).T\n    Z = np.atleast_2d(np.array([0.5]))\n\n    def get_model(num_data):\n        return gpflow.models.SVGP(\n            kernel=gpflow.kernels.SquaredExponential(),\n            num_data=num_data,\n            likelihood=gpflow.likelihoods.Gaussian(),\n            inducing_variable=Z,\n        )\n\n    def training_loop(indices, num_data, max_iter):\n        model = get_model(num_data)\n        opt = tf.optimizers.SGD(learning_rate=0.001)\n        data = X[indices], Y[indices]\n        for _ in range(max_iter):\n            with tf.GradientTape() as tape:\n                loss = model.training_loss(data)\n            grads = tape.gradient(loss, model.trainable_variables)\n            opt.apply_gradients(zip(grads, model.trainable_variables))\n        return model\n\n    model_1 = training_loop(indices_1, num_data=num_data1, max_iter=max_iter)\n    model_2 = training_loop(indices_2, num_data=num_data2, max_iter=max_iter)\n    assert _check_models_close(model_1, model_2)\n'"
tests/gpflow/models/test_training_mixins.py,2,"b'import numpy as np\nimport tensorflow as tf\nimport gpflow\n\n\nclass DummyModel(gpflow.models.BayesianModel, gpflow.models.ExternalDataTrainingLossMixin):\n    def maximum_log_likelihood_objective(self, data):\n        X, Y = data\n        return tf.reduce_sum(X * Y)\n\n\ndef test_training_loss_closure_with_minibatch():\n    N = 13\n    B = 5\n    num_batches = int(np.ceil(N / B))\n\n    data = (np.random.randn(N, 2), np.random.randn(N, 1))\n    dataset = tf.data.Dataset.from_tensor_slices(data)\n\n    model = DummyModel()\n\n    training_loss_full_data = model.training_loss_closure(data, compile=True)\n    loss_full = training_loss_full_data()\n\n    it = iter(dataset.batch(B))\n    training_loss_minibatch = model.training_loss_closure(it, compile=True)\n    batch_losses = [training_loss_minibatch() for _ in range(num_batches)]\n\n    np.testing.assert_allclose(loss_full, np.sum(batch_losses))\n'"
tests/gpflow/models/test_variational.py,0,"b'# Copyright 2016 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.kernels import SquaredExponential\nfrom gpflow.likelihoods import Gaussian\nfrom tests.gpflow.kernels.reference import ref_rbf_kernel\n\nrng = np.random.RandomState(1)\n\n# ------------------------------------------\n# Helpers\n# ------------------------------------------\n\n\ndef univariate_log_marginal_likelihood(y, K, noise_var):\n    return -0.5 * y * y / (K + noise_var) - 0.5 * np.log(K + noise_var) - 0.5 * np.log(np.pi * 2.0)\n\n\ndef univariate_posterior(y, K, noise_var):\n    mean = K * y / (K + noise_var)\n    variance = K - K / (K + noise_var)\n    return mean, variance\n\n\ndef univariate_prior_KL(meanA, meanB, varA, varB):\n    # KL[ qA | qB ] = E_{qA} \\log [qA / qB] where qA and qB are univariate normal distributions.\n    return 0.5 * (\n        np.log(varB) - np.log(varA) - 1.0 + varA / varB + (meanB - meanA) * (meanB - meanA) / varB\n    )\n\n\ndef multivariate_prior_KL(meanA, covA, meanB, covB):\n    # KL[ qA | qB ] = E_{qA} \\log [qA / qB] where qA and aB are\n    # K dimensional multivariate normal distributions.\n    # Analytically tractable and equal to...\n    # 0.5 * (Tr(covB^{-1} covA) + (meanB - meanA)^T covB^{-1} (meanB - meanA)\n    #        - K + log(det(covB)) - log (det(covA)))\n    K = covA.shape[0]\n    traceTerm = 0.5 * np.trace(np.linalg.solve(covB, covA))\n    delta = meanB - meanA\n    mahalanobisTerm = 0.5 * np.dot(delta.T, np.linalg.solve(covB, delta))\n    constantTerm = -0.5 * K\n    priorLogDeterminantTerm = 0.5 * np.linalg.slogdet(covB)[1]\n    variationalLogDeterminantTerm = -0.5 * np.linalg.slogdet(covA)[1]\n    return (\n        traceTerm\n        + mahalanobisTerm\n        + constantTerm\n        + priorLogDeterminantTerm\n        + variationalLogDeterminantTerm\n    )\n\n\n# ------------------------------------------\n# Data classes: storing constants\n# ------------------------------------------\n\n\nclass Datum:\n    num_latent_gps = 1\n    y_data = 2.0\n    X = np.atleast_2d(np.array([0.0]))\n    Y = np.atleast_2d(np.array([y_data]))\n    Z = X.copy()\n    zero_mean = 0.0\n    K = 1.0\n    noise_var = 0.5\n    posterior_mean, posterior_var = univariate_posterior(y=y_data, K=K, noise_var=noise_var)\n    posterior_std = np.sqrt(posterior_var)\n    data = (X, Y)\n\n\nclass MultiDatum:\n    dim = 3\n    num_latent_gps = 1\n    Y = rng.randn(dim, 1)\n    X = rng.randn(dim, 1)\n    Z = X.copy()\n    noise_var = 0.5\n    signal_var = 1.5\n    ls = 1.7\n    q_mean = rng.randn(dim, num_latent_gps)\n    q_sqrt_diag = rng.rand(dim, num_latent_gps)\n    q_sqrt_full = np.tril(rng.rand(dim, dim))\n\n\ndef test_reference_implementation_consistency():\n    q_mean = rng.rand(1, 1)\n    q_cov = rng.rand(1, 1)\n    p_mean = rng.rand(1, 1)\n    p_cov = rng.rand(1, 1)\n\n    multivariate_KL = multivariate_prior_KL(q_mean, p_mean, q_cov, p_cov)\n    univariate_KL = univariate_prior_KL(\n        q_mean.reshape(-1), p_mean.reshape(-1), q_cov.reshape(-1), p_cov.reshape(-1)\n    )\n\n    assert_allclose(univariate_KL.squeeze(), multivariate_KL.squeeze(), atol=4)\n\n\n@pytest.mark.parametrize(""diag"", [True, False])\n@pytest.mark.parametrize(""whiten"", [True, False])\ndef test_variational_univariate_prior_KL(diag, whiten):\n    reference_kl = univariate_prior_KL(\n        Datum.posterior_mean, Datum.zero_mean, Datum.posterior_var, Datum.K\n    )\n    q_mu = np.ones((1, Datum.num_latent_gps)) * Datum.posterior_mean\n    ones = np.ones((1, Datum.num_latent_gps)) if diag else np.ones((1, 1, Datum.num_latent_gps))\n    q_sqrt = ones * Datum.posterior_std\n    model = gpflow.models.SVGP(\n        kernel=SquaredExponential(variance=Datum.K),\n        likelihood=Gaussian(),\n        inducing_variable=Datum.Z,\n        num_latent_gps=Datum.num_latent_gps,\n        q_diag=diag,\n        whiten=whiten,\n        q_mu=q_mu,\n        q_sqrt=q_sqrt,\n    )\n    assert_allclose(model.prior_kl(), reference_kl, atol=4)\n\n\n@pytest.mark.parametrize(""diag"", [True, False])\n@pytest.mark.parametrize(""whiten"", [True, False])\ndef test_variational_univariate_log_likelihood(diag, whiten):\n    # reference marginal likelihood estimate\n    reference_log_marginal_likelihood = univariate_log_marginal_likelihood(\n        y=Datum.y_data, K=Datum.K, noise_var=Datum.noise_var\n    )\n    q_mu = np.ones((1, Datum.num_latent_gps)) * Datum.posterior_mean\n    ones = np.ones((1, Datum.num_latent_gps)) if diag else np.ones((1, 1, Datum.num_latent_gps))\n    q_sqrt = ones * Datum.posterior_std\n    model = gpflow.models.SVGP(\n        kernel=SquaredExponential(variance=Datum.K),\n        likelihood=Gaussian(),\n        inducing_variable=Datum.Z,\n        num_latent_gps=Datum.num_latent_gps,\n        q_diag=diag,\n        whiten=whiten,\n        q_mu=q_mu,\n        q_sqrt=q_sqrt,\n    )\n    model_likelihood = model.elbo(Datum.data).numpy()\n    assert_allclose(model_likelihood, reference_log_marginal_likelihood, atol=4)\n\n\n@pytest.mark.parametrize(""diag"", [True, False])\n@pytest.mark.parametrize(""whiten"", [True, False])\ndef test_variational_univariate_conditionals(diag, whiten):\n    q_mu = np.ones((1, Datum.num_latent_gps)) * Datum.posterior_mean\n    ones = np.ones((1, Datum.num_latent_gps)) if diag else np.ones((1, 1, Datum.num_latent_gps))\n    q_sqrt = ones * Datum.posterior_std\n    model = gpflow.models.SVGP(\n        kernel=SquaredExponential(variance=Datum.K),\n        likelihood=Gaussian(),\n        inducing_variable=Datum.Z,\n        num_latent_gps=Datum.num_latent_gps,\n        q_diag=diag,\n        whiten=whiten,\n        q_mu=q_mu,\n        q_sqrt=q_sqrt,\n    )\n\n    fmean_func, fvar_func = gpflow.conditionals.conditional(\n        Datum.X, Datum.Z, model.kernel, model.q_mu, q_sqrt=model.q_sqrt, white=whiten\n    )\n    mean_value, var_value = fmean_func[0, 0], fvar_func[0, 0]\n\n    assert_allclose(mean_value, Datum.posterior_mean, atol=4)\n    assert_allclose(var_value, Datum.posterior_var, atol=4)\n\n\n@pytest.mark.parametrize(""whiten"", [True, False])\ndef test_variational_multivariate_prior_KL_full_q(whiten):\n    cov_q = MultiDatum.q_sqrt_full @ MultiDatum.q_sqrt_full.T\n    mean_prior = np.zeros((MultiDatum.dim, 1))\n    cov_prior = (\n        np.eye(MultiDatum.dim)\n        if whiten\n        else ref_rbf_kernel(MultiDatum.X, MultiDatum.ls, MultiDatum.signal_var)\n    )\n    reference_kl = multivariate_prior_KL(MultiDatum.q_mean, cov_q, mean_prior, cov_prior)\n\n    q_sqrt = MultiDatum.q_sqrt_full[None, :, :]\n    model = gpflow.models.SVGP(\n        kernel=SquaredExponential(variance=MultiDatum.signal_var, lengthscales=MultiDatum.ls),\n        likelihood=Gaussian(MultiDatum.noise_var),\n        inducing_variable=MultiDatum.Z,\n        num_latent_gps=MultiDatum.num_latent_gps,\n        q_diag=False,\n        whiten=whiten,\n        q_mu=MultiDatum.q_mean,\n        q_sqrt=q_sqrt,\n    )\n\n    assert_allclose(model.prior_kl(), reference_kl, atol=4)\n'"
tests/gpflow/optimizers/__init__.py,0,b''
tests/gpflow/optimizers/test_mcmc.py,2,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nimport gpflow\nfrom gpflow import default_float\nfrom gpflow.base import PriorOn\nfrom gpflow.config import set_default_float\nfrom gpflow.utilities import to_default_float\nfrom tensorflow_probability.python.bijectors import Exp\nfrom tensorflow_probability.python.distributions import Uniform, Gamma\n\nnp.random.seed(1)\n\n\ndef build_data():\n    N = 30\n    X = np.random.rand(N, 1)\n    Y = np.sin(12 * X) + 0.66 * np.cos(25 * X) + np.random.randn(N, 1) * 0.1 + 3\n    return (X, Y)\n\n\ndef build_model(data):\n\n    kernel = gpflow.kernels.Matern52(lengthscales=0.3)\n\n    meanf = gpflow.mean_functions.Linear(1.0, 0.0)\n    model = gpflow.models.GPR(data, kernel, meanf, noise_variance=0.01)\n\n    for p in model.parameters:\n        p.prior = Gamma(to_default_float(1.0), to_default_float(1.0))\n\n    return model\n\n\ndef test_mcmc_helper_parameters():\n    data = build_data()\n    model = build_model(data)\n\n    hmc_helper = gpflow.optimizers.SamplingHelper(\n        model.log_posterior_density, model.trainable_parameters\n    )\n\n    for i in range(len(model.trainable_parameters)):\n        assert model.trainable_parameters[i].shape == hmc_helper.current_state[i].shape\n        assert model.trainable_parameters[i] == hmc_helper._parameters[i]\n        assert model.trainable_parameters[i].unconstrained_variable == hmc_helper.current_state[i]\n\n\ndef test_mcmc_helper_target_function_constrained():\n    """""" Set up priors on the model parameters such that we can\n    readily compute their expected values. """"""\n    data = build_data()\n    model = build_model(data)\n\n    prior_width = 200.0\n\n    hmc_helper = gpflow.optimizers.SamplingHelper(\n        model.log_posterior_density, model.trainable_parameters\n    )\n    target_log_prob_fn = hmc_helper.target_log_prob_fn\n\n    # Priors which are set on the constrained space\n    expected_log_prior = 0.0\n    for param in model.trainable_parameters:\n        if param.value() < 1e-3:\n            # Avoid values which would be pathological for the Exp transform\n            param.assign(1.0)\n\n        param.transform = Exp()\n\n        low_value = -100\n        high_value = low_value + prior_width\n\n        param.prior = Uniform(low=np.float64(low_value), high=np.float64(high_value))\n        param.prior_on = PriorOn.CONSTRAINED\n\n        prior_density_on_constrained = 1 / prior_width\n        prior_density_on_unconstrained = prior_density_on_constrained * param.value()\n\n        expected_log_prior += np.log(prior_density_on_unconstrained)\n\n    log_marginal_likelihood = model.log_marginal_likelihood().numpy()\n    expected_log_prob = log_marginal_likelihood + expected_log_prior\n\n    np.testing.assert_allclose(target_log_prob_fn(), expected_log_prob)\n\n\ndef test_mcmc_helper_target_function_unconstrained():\n    """""" Verifies the objective for a set of priors which are defined on the unconstrained space.\n    """"""\n    data = build_data()\n    model = build_model(data)\n\n    # Set up priors on the model parameters such that we can readily compute their expected values.\n    expected_log_prior = 0.0\n    prior_width = 200.0\n\n    hmc_helper = gpflow.optimizers.SamplingHelper(\n        model.log_posterior_density, model.trainable_parameters\n    )\n\n    for param in model.trainable_parameters:\n        low_value = -100\n        high_value = low_value + prior_width\n        param.prior = Uniform(low=np.float64(low_value), high=np.float64(high_value))\n        param.prior_on = ""unconstrained""\n\n        prior_density = 1 / prior_width\n        expected_log_prior += np.log(prior_density)\n\n    target_log_prob_fn = hmc_helper.target_log_prob_fn\n    expected_log_prob = model.log_marginal_likelihood().numpy() + expected_log_prior\n\n    np.testing.assert_allclose(target_log_prob_fn(), expected_log_prob)\n\n\n@pytest.mark.parametrize(""prior_on"", [""constrained"", ""unconstrained""])\ndef test_mcmc_helper_target_function_no_transforms(prior_on):\n    """""" Verifies the objective for a set of priors where no transforms are set.\n    """"""\n    data = build_data()\n    model = build_model(data)\n    expected_log_prior = 0.0\n    prior_width = 200.0\n\n    hmc_helper = gpflow.optimizers.SamplingHelper(\n        model.log_posterior_density, model.trainable_parameters\n    )\n\n    for param in model.trainable_parameters:\n        param.transform = None\n        low_value = -100\n        high_value = low_value + prior_width\n        param.prior = Uniform(low=np.float64(low_value), high=np.float64(high_value))\n        param.prior_on = prior_on\n\n        prior_density = 1 / prior_width\n        expected_log_prior += np.log(prior_density)\n\n    log_marginal_likelihood = model.log_marginal_likelihood().numpy()\n    expected_log_prob = log_marginal_likelihood + expected_log_prior\n    target_log_prob_fn = hmc_helper.target_log_prob_fn\n\n    np.testing.assert_allclose(target_log_prob_fn(), expected_log_prob)\n\n    # Test the wrapped closure\n    log_prob, grad_fn = target_log_prob_fn.__original_wrapped__()\n    grad, nones = grad_fn(1, [None] * len(model.trainable_parameters))\n    assert len(grad) == len(model.trainable_parameters)\n    assert nones == [None] * len(model.trainable_parameters)\n\n\ndef test_mcmc_sampler_integration():\n    data = build_data()\n    model = build_model(data)\n\n    hmc_helper = gpflow.optimizers.SamplingHelper(\n        model.log_posterior_density, model.trainable_parameters\n    )\n\n    hmc = tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=hmc_helper.target_log_prob_fn, num_leapfrog_steps=2, step_size=0.01,\n    )\n\n    adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n        hmc,\n        num_adaptation_steps=2,\n        target_accept_prob=gpflow.utilities.to_default_float(0.75),\n        adaptation_rate=0.1,\n    )\n\n    num_samples = 5\n\n    @tf.function\n    def run_chain_fn():\n        return tfp.mcmc.sample_chain(\n            num_results=num_samples,\n            num_burnin_steps=2,\n            current_state=hmc_helper.current_state,\n            kernel=adaptive_hmc,\n            trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n        )\n\n    samples, _ = run_chain_fn()\n\n    assert len(samples) == len(model.trainable_parameters)\n    parameter_samples = hmc_helper.convert_to_constrained_values(samples)\n    assert len(parameter_samples) == len(samples)\n\n    for i in range(len(model.trainable_parameters)):\n        assert len(samples[i]) == num_samples\n        assert hmc_helper.current_state[i].numpy() == samples[i][-1]\n        assert hmc_helper._parameters[i].numpy() == parameter_samples[i][-1]\n\n\ndef test_helper_with_variables_fails():\n    variable = tf.Variable(0.1)\n    with pytest.raises(\n        ValueError, match=r""`parameters` should only contain gpflow.Parameter objects with priors""\n    ):\n        gpflow.optimizers.SamplingHelper(lambda: variable ** 2, (variable,))\n'"
tests/gpflow/optimizers/test_natural_gradient.py,7,"b'from typing import Optional\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow.config import default_float\nfrom gpflow.optimizers import NaturalGradient\nfrom gpflow import set_trainable\n\n\nclass Setup:\n    N, M, D = 4, 3, 2\n    likelihood_variance = 0.1\n    rng = np.random.RandomState(42)\n    X = rng.randn(N, D)\n    Y = rng.randn(N, 1)\n    Z = rng.randn(M, D)\n\n\n@pytest.fixture\ndef data():\n    X = tf.convert_to_tensor(Setup.X, dtype=default_float())\n    Y = tf.convert_to_tensor(Setup.Y, dtype=default_float())\n    return (X, Y)\n\n\n@pytest.fixture\ndef inducing_variable():\n    Z = tf.convert_to_tensor(Setup.Z, dtype=default_float())\n    return Z\n\n\n@pytest.fixture\ndef kernel():\n    return gpflow.kernels.SquaredExponential()\n\n\n@pytest.fixture\ndef likelihood():\n    return gpflow.likelihoods.Gaussian(variance=Setup.likelihood_variance)\n\n\n@pytest.fixture\ndef gpr_and_vgp(data, kernel, likelihood):\n    vgp = gpflow.models.VGP(data, kernel, likelihood)\n    gpr = gpflow.models.GPR(data, kernel)\n    gpr.likelihood.variance.assign(likelihood.variance)\n    set_trainable(vgp, False)\n    set_trainable(vgp.q_mu, True)\n    set_trainable(vgp.q_sqrt, True)\n    return gpr, vgp\n\n\n@pytest.fixture\ndef sgpr_and_svgp(data, inducing_variable, kernel, likelihood):\n    svgp = gpflow.models.SVGP(kernel, likelihood, inducing_variable)\n    sgpr = gpflow.models.SGPR(data, kernel, inducing_variable=inducing_variable)\n    sgpr.likelihood.variance.assign(Setup.likelihood_variance)\n    set_trainable(svgp, False)\n    set_trainable(svgp.q_mu, True)\n    set_trainable(svgp.q_sqrt, True)\n    return sgpr, svgp\n\n\ndef assert_different(value1, value2, rtol=0.07):\n    """""" assert relative difference > rtol """"""\n    relative_difference = (value1 - value2) / (value1 + value2)\n    assert np.abs(relative_difference) > rtol\n\n\ndef assert_gpr_vs_vgp(\n    m1: gpflow.models.BayesianModel,\n    m2: gpflow.models.BayesianModel,\n    gamma: float = 1.0,\n    maxiter: int = 1,\n    xi_transform: Optional[gpflow.optimizers.natgrad.XiTransform] = None,\n):\n    assert maxiter >= 1\n\n    m1_ll_before = m1.training_loss()\n    m2_ll_before = m2.training_loss()\n\n    assert_different(m2_ll_before, m1_ll_before)\n\n    params = (m2.q_mu, m2.q_sqrt)\n    if xi_transform is not None:\n        params += (xi_transform,)\n\n    opt = NaturalGradient(gamma)\n\n    @tf.function\n    def minimize_step():\n        opt.minimize(m2.training_loss, var_list=[params])\n\n    for _ in range(maxiter):\n        minimize_step()\n\n    m1_ll_after = m1.training_loss()\n    m2_ll_after = m2.training_loss()\n\n    np.testing.assert_allclose(m1_ll_after, m2_ll_after, atol=1e-4)\n\n\ndef assert_sgpr_vs_svgp(\n    m1: gpflow.models.BayesianModel, m2: gpflow.models.BayesianModel,\n):\n    data = m1.data\n\n    m1_ll_before = m1.training_loss()\n    m2_ll_before = m2.training_loss(data)\n\n    assert_different(m2_ll_before, m1_ll_before)\n\n    params = [(m2.q_mu, m2.q_sqrt)]\n    opt = NaturalGradient(1.0)\n    opt.minimize(m2.training_loss_closure(data), var_list=params)\n\n    m1_ll_after = m1.training_loss()\n    m2_ll_after = m2.training_loss(data)\n\n    np.testing.assert_allclose(m1_ll_after, m2_ll_after, atol=1e-4)\n\n\ndef test_vgp_vs_gpr(gpr_and_vgp):\n    """"""\n    With a Gaussian likelihood the Gaussian variational (VGP) model should be equivalent to the exact\n    regression model (GPR) after a single nat grad step of size 1\n    """"""\n    gpr, vgp = gpr_and_vgp\n    assert_gpr_vs_vgp(gpr, vgp)\n\n\ndef test_small_q_sqrt_handeled_correctly(gpr_and_vgp, data):\n    """"""\n    This is an extra test to make sure things still work when q_sqrt is small. This was breaking (#767)\n    """"""\n    gpr, vgp = gpr_and_vgp\n    vgp.q_mu.assign(np.random.randn(data[0].shape[0], 1))\n    vgp.q_sqrt.assign(np.eye(data[0].shape[0])[None, :, :] * 1e-3)\n    assert_gpr_vs_vgp(gpr, vgp)\n\n\ndef test_svgp_vs_sgpr(sgpr_and_svgp):\n    """"""\n    With a Gaussian likelihood the sparse Gaussian variational (SVGP) model\n    should be equivalent to the analytically optimial sparse regression model (SGPR)\n    after a single nat grad step of size 1.0\n    """"""\n    sgpr, svgp = sgpr_and_svgp\n    assert_sgpr_vs_svgp(sgpr, svgp)\n\n\nclass XiEta(gpflow.optimizers.XiTransform):\n    @staticmethod\n    def meanvarsqrt_to_xi(mean: tf.Tensor, varsqrt: tf.Tensor) -> tf.Tensor:\n        return gpflow.optimizers.natgrad.meanvarsqrt_to_expectation(mean, varsqrt)\n\n    @staticmethod\n    def xi_to_meanvarsqrt(xi1: tf.Tensor, xi2: tf.Tensor) -> tf.Tensor:\n        return gpflow.optimizers.natgrad.expectation_to_meanvarsqrt(xi1, xi2)\n\n    @staticmethod\n    def naturals_to_xi(nat1: tf.Tensor, nat2: tf.Tensor) -> tf.Tensor:\n        return gpflow.optimizers.natgrad.natural_to_expectation(nat1, nat2)\n\n\n@pytest.mark.parametrize(""xi_transform"", [gpflow.optimizers.XiSqrtMeanVar(), XiEta()])\ndef test_xi_transform_vgp_vs_gpr(gpr_and_vgp, xi_transform):\n    """"""\n    With other transforms the solution is not given in a single step, but it should still give the same answer\n    after a number of smaller steps.\n    """"""\n    gpr, vgp = gpr_and_vgp\n    assert_gpr_vs_vgp(gpr, vgp, gamma=0.01, xi_transform=xi_transform, maxiter=500)\n'"
tests/gpflow/optimizers/test_scipy.py,0,"b'# Copyright 2019 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom numpy.testing import assert_allclose\n\nimport gpflow\nfrom gpflow.config import default_jitter\nfrom gpflow.mean_functions import Constant\n\nrng = np.random.RandomState(0)\n\n\nclass Datum:\n    X = rng.rand(20, 1) * 10\n    Y = np.sin(X) + 0.9 * np.cos(X * 1.6) + rng.randn(*X.shape) * 0.8\n    Y = np.tile(Y, 2)  # two identical columns\n    Xtest = rng.rand(10, 1) * 10\n    data = (X, Y)\n\n\ndef _create_full_gp_model():\n    """"""\n    GP Regression\n    """"""\n    return gpflow.models.GPR(\n        (Datum.X, Datum.Y),\n        kernel=gpflow.kernels.SquaredExponential(),\n        mean_function=gpflow.mean_functions.Constant(),\n    )\n\n\ndef test_scipy_jit():\n    m1 = _create_full_gp_model()\n    m2 = _create_full_gp_model()\n\n    opt1 = gpflow.optimizers.Scipy()\n    opt2 = gpflow.optimizers.Scipy()\n\n    opt1.minimize(\n        m1.training_loss, variables=m1.trainable_variables, options=dict(maxiter=50), compile=False,\n    )\n    opt2.minimize(\n        m2.training_loss, variables=m2.trainable_variables, options=dict(maxiter=50), compile=True,\n    )\n\n    def get_values(model):\n        return np.array([var.value().numpy().squeeze() for var in model.trainable_variables])\n\n    # The tolerance of the following test had to be loosened slightly from atol=1e-15\n    # due to the changes introduced by PR #1213, which removed some implicit casts\n    # to float32.\n    np.testing.assert_allclose(get_values(m1), get_values(m2), rtol=1e-14, atol=1e-14)\n'"
tests/gpflow/utilities/__init__.py,0,b''
tests/gpflow/utilities/test_deepcopy.py,11,"b'import copy\nimport pickle\n\nimport gpflow\nimport pytest\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\nclass A(tf.Module):\n    def __init__(self):\n        self.var = tf.Variable([1.0])\n        shift = tf.Variable(1e-6)\n        self.bijector = tfp.bijectors.Chain([tfp.bijectors.Softplus(), tfp.bijectors.Shift(shift)])\n\n    def __call__(self, x):\n        return self.bijector(x)\n\n\nclass B(tf.Module):\n    def __init__(self):\n        self.var = tf.Variable([2.0])\n        self.a = A()\n\n    def __call__(self, x):\n        return self.a(x)\n\n\nclass C(tf.Module):\n    def __init__(self):\n        self.var = gpflow.Parameter([2.0], transform=gpflow.utilities.positive(lower=1e-6))\n\n\nclass D(tf.Module):\n    def __init__(self):\n        self.var = gpflow.Parameter([10.0], transform=gpflow.utilities.positive())\n        self.var2 = gpflow.Parameter([5.0])\n        self.c = C()\n\n\nclass NestedModule(tf.Module):\n    def __init__(self, module: tf.Module):\n        self.module = module\n\n\n@pytest.mark.parametrize(""module"", [A(), B()])\ndef test_clears_bijector_cache_and_deepcopy(module):\n    """"""\n    With each forward pass through a bijector, a cache is stored inside which prohibits the deepcopy of the bijector.\n    This is due to the fact that HashableWeakRef objects are not pickle-able, which raises a TypeError. Alternatively,\n    one can make use of `deepcopy_component` to deepcopy a module containing used bijectors.\n    """"""\n    input = 1.0\n    _ = module(input)\n    with pytest.raises(TypeError):\n        copy.deepcopy(module)\n    module_copy = gpflow.utilities.deepcopy(module)\n    assert module.var == module_copy.var\n    assert module.var is not module_copy.var\n    module_copy.var.assign([5.0])\n    assert module.var != module_copy.var\n\n\ndef test_freeze():\n    module = NestedModule(NestedModule(A()))\n    module_frozen = gpflow.utilities.freeze(module)\n    assert len(module.variables) == 2\n    assert module_frozen.variables == ()\n    assert isinstance(module.module.module.var, tf.Variable)\n    assert isinstance(module_frozen.module.module.var, tf.Tensor)\n\n\ndef test_pickle_frozen():\n    """"""\n    Regression test for the bug described in GPflow/GPflow#1338\n    """"""\n    module = D()\n    module_frozen = gpflow.utilities.freeze(module)\n\n    pickled = pickle.dumps(module_frozen)\n    loaded = pickle.loads(pickled)\n\n    assert loaded.var == module_frozen.var\n    assert loaded.var2 == module_frozen.var2\n    assert loaded.c.var == module_frozen.c.var\n'"
tests/gpflow/utilities/test_multipledispatch.py,3,"b'import re\nimport warnings\n\nimport multipledispatch\nimport pytest\nimport tensorflow as tf\n\nimport gpflow\n\n\nclass A1:\n    pass\n\n\nclass A2(A1):\n    pass\n\n\nclass B1:\n    pass\n\n\nclass B2(B1):\n    pass\n\n\ndef test_our_multipledispatch():\n    test_fn = gpflow.utilities.Dispatcher(""test_fn"")\n\n    @test_fn.register(A1, B1)\n    def test_a1_b1(x, y):\n        return ""a1-b1""\n\n    @test_fn.register(A2, B1)\n    def test_a2_b1(x, y):\n        return ""a2-b1""\n\n    @test_fn.register(A1, B2)\n    def test_a1_b2(x, y):\n        return ""a1-b2""\n\n    assert test_fn(A1(), B1()) == ""a1-b1""\n    assert test_fn(A2(), B1()) == ""a2-b1""\n    assert test_fn(A1(), B2()) == ""a1-b2""\n\n    # test the ambiguous case:\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n\n        assert test_fn(A2(), B2()) == ""a1-b2""  # the last definition wins\n\n        assert len(w) == 1\n        assert issubclass(w[0].category, multipledispatch.conflict.AmbiguityWarning)\n\n    # test that adding the child-child definition removes ambiguity warning:\n\n    @test_fn.register(A2, B2)\n    def test_a2_b2(x, y):\n        return ""a2-b2""\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n\n        assert test_fn(A2(), B2()) == ""a2-b2""\n\n        assert len(w) == 0\n\n\n@pytest.mark.parametrize(\n    ""Dispatcher, expect_autograph_warning"",\n    [(multipledispatch.Dispatcher, True), (gpflow.utilities.Dispatcher, False),],\n)\ndef test_dispatcher_autograph_warnings(capsys, Dispatcher, expect_autograph_warning):\n    tf.autograph.set_verbosity(0, alsologtostdout=True)  # to be able to capture it using capsys\n\n    test_fn = Dispatcher(""test_fn"")\n\n    # generator would only be invoked when defining for base class...\n    @test_fn.register(gpflow.inducing_variables.InducingVariables)\n    def test_iv(x):\n        return tf.reduce_sum(x.Z)\n\n    test_fn_compiled = tf.function(test_fn)  # with autograph=True by default\n\n    # ...but calling using subclass\n    result = test_fn_compiled(gpflow.inducing_variables.InducingPoints([1.0, 2.0]))\n    assert result.numpy() == 3.0  # expect computation to work either way\n\n    captured = capsys.readouterr()\n\n    tf_warning = ""WARNING:.*Entity .* appears to be a generator function. It will not be converted by AutoGraph.""\n    assert bool(re.match(tf_warning, captured.out)) == expect_autograph_warning\n'"
tests/gpflow/utilities/test_ops.py,4,"b'import pytest\nimport numpy as np\nimport tensorflow as tf\n\nimport gpflow\n\n\ndef pca_reduce(X: np.ndarray, Q: np.ndarray) -> np.ndarray:\n    """"""\n    A helpful function for linearly reducing the dimensionality of the data X\n    to Q.\n    :param X: data array of size N (number of points) x D (dimensions)\n    :param Q: Number of latent dimensions, Q < D\n    :return: PCA projection array of size N x Q.\n    """"""\n    if Q > X.shape[1]:  # pragma: no cover\n        raise ValueError(""Cannot have more latent dimensions than observed"")\n    if isinstance(X, tf.Tensor):\n        X = X.numpy()\n        # TODO why not use tf.linalg.eigh?\n    evals, evecs = np.linalg.eigh(np.cov(X.T))\n    W = evecs[:, -Q:]\n    return (X - X.mean(0)).dot(W)\n\n\n@pytest.mark.parametrize(""N"", [3, 7])\n@pytest.mark.parametrize(""D"", [2, 5, 9])\n@pytest.mark.parametrize(""Q"", [2, 5, 9])\ndef test_numpy_equivalence(N, D, Q):\n    X = np.random.randn(N, D)\n\n    if Q > D:\n        with pytest.raises(ValueError):\n            gpflow.utilities.ops.pca_reduce(tf.convert_to_tensor(X), Q)\n\n    else:\n        np_result = pca_reduce(X, Q)\n        tf_result = gpflow.utilities.ops.pca_reduce(tf.convert_to_tensor(X), Q).numpy()\n        assert np_result.shape == tf_result.shape == (N, Q)\n\n        for i in range(Q):\n            # PCA does not necessarily preserve the overall sign, so also accept it to flip\n            tf_column = tf_result[:, i]\n            np_column = np_result[:, i]\n            assert np.allclose(tf_column, np_column) or np.allclose(tf_column, -np_column)\n'"
tests/gpflow/utilities/test_printing.py,12,"b'# Copyright 2018 the GPflow authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow.config import Config, as_context\nfrom gpflow.utilities.utilities import (\n    leaf_components,\n    _merge_leaf_components,\n    tabulate_module_summary,\n    set_trainable,\n)\n\nrng = np.random.RandomState(0)\n\n\nclass Data:\n    H0 = 5\n    H1 = 2\n    M = 10\n    D = 1\n    Z = 0.5 * np.ones((M, 1))\n    ls = 2.0\n    var = 1.0\n\n\n# ------------------------------------------\n# Helpers\n# ------------------------------------------\n\n\nclass A(tf.Module):\n    def __init__(self, name=None):\n        super().__init__(name)\n        self.var_trainable = tf.Variable(tf.zeros((2, 2, 1)), trainable=True)\n        self.var_fixed = tf.Variable(tf.ones((2, 2, 1)), trainable=False)\n\n\nclass B(tf.Module):\n    def __init__(self, name=None):\n        super().__init__(name)\n        self.submodule_list = [A(), A()]\n        self.submodule_dict = dict(a=A(), b=A())\n        self.var_trainable = tf.Variable(tf.zeros((2, 2, 1)), trainable=True)\n        self.var_fixed = tf.Variable(tf.ones((2, 2, 1)), trainable=False)\n\n\nclass C(tf.keras.Model):\n    def __init__(self, name=None):\n        super().__init__(name)\n        self.variable = tf.Variable(tf.zeros((2, 2, 1)), trainable=True)\n        self.param = gpflow.Parameter(0.0)\n        self.dense = tf.keras.layers.Dense(5)\n\n\ndef create_kernel():\n    kern = gpflow.kernels.SquaredExponential(lengthscales=Data.ls, variance=Data.var)\n    set_trainable(kern.lengthscales, False)\n    return kern\n\n\ndef create_compose_kernel():\n    kernel = gpflow.kernels.Product(\n        [\n            gpflow.kernels.Sum([create_kernel(), create_kernel()]),\n            gpflow.kernels.Sum([create_kernel(), create_kernel()]),\n        ]\n    )\n    return kernel\n\n\ndef create_model():\n    kernel = create_kernel()\n    model = gpflow.models.SVGP(\n        kernel=kernel,\n        likelihood=gpflow.likelihoods.Gaussian(variance_lower_bound=0.0),\n        inducing_variable=Data.Z,\n        q_diag=True,\n    )\n    set_trainable(model.q_mu, False)\n    return model\n\n\n# ------------------------------------------\n# Reference\n# ------------------------------------------\n\nexample_tf_module_variable_dict = {\n    ""A.var_trainable"": {""value"": np.zeros((2, 2, 1)), ""trainable"": True, ""shape"": (2, 2, 1),},\n    ""A.var_fixed"": {""value"": np.ones((2, 2, 1)), ""trainable"": False, ""shape"": (2, 2, 1),},\n}\n\nexample_module_list_variable_dict = {\n    ""submodule_list[0].var_trainable"": example_tf_module_variable_dict[""A.var_trainable""],\n    ""submodule_list[0].var_fixed"": example_tf_module_variable_dict[""A.var_fixed""],\n    ""submodule_list[1].var_trainable"": example_tf_module_variable_dict[""A.var_trainable""],\n    ""submodule_list[1].var_fixed"": example_tf_module_variable_dict[""A.var_fixed""],\n    ""submodule_dict[\'a\'].var_trainable"": example_tf_module_variable_dict[""A.var_trainable""],\n    ""submodule_dict[\'a\'].var_fixed"": example_tf_module_variable_dict[""A.var_fixed""],\n    ""submodule_dict[\'b\'].var_trainable"": example_tf_module_variable_dict[""A.var_trainable""],\n    ""submodule_dict[\'b\'].var_fixed"": example_tf_module_variable_dict[""A.var_fixed""],\n    ""B.var_trainable"": example_tf_module_variable_dict[""A.var_trainable""],\n    ""B.var_fixed"": example_tf_module_variable_dict[""A.var_fixed""],\n}\n\nkernel_param_dict = {\n    ""SquaredExponential.lengthscales"": {""value"": Data.ls, ""trainable"": False, ""shape"": (),},\n    ""SquaredExponential.variance"": {""value"": Data.var, ""trainable"": True, ""shape"": ()},\n}\n\ncompose_kernel_param_dict = {\n    ""kernels[0].kernels[0].variance"": kernel_param_dict[""SquaredExponential.variance""],\n    ""kernels[0].kernels[0].lengthscales"": kernel_param_dict[""SquaredExponential.lengthscales""],\n    ""kernels[0].kernels[1].variance"": kernel_param_dict[""SquaredExponential.variance""],\n    ""kernels[0].kernels[1].lengthscales"": kernel_param_dict[""SquaredExponential.lengthscales""],\n    ""kernels[1].kernels[0].variance"": kernel_param_dict[""SquaredExponential.variance""],\n    ""kernels[1].kernels[0].lengthscales"": kernel_param_dict[""SquaredExponential.lengthscales""],\n    ""kernels[1].kernels[1].variance"": kernel_param_dict[""SquaredExponential.variance""],\n    ""kernels[1].kernels[1].lengthscales"": kernel_param_dict[""SquaredExponential.lengthscales""],\n}\n\nmodel_gp_param_dict = {\n    ""kernel.lengthscales"": kernel_param_dict[""SquaredExponential.lengthscales""],\n    ""kernel.variance"": kernel_param_dict[""SquaredExponential.variance""],\n    ""likelihood.variance"": {""value"": 1.0, ""trainable"": True, ""shape"": ()},\n    ""inducing_variable.Z"": {""value"": Data.Z, ""trainable"": True, ""shape"": (Data.M, Data.D),},\n    ""SVGP.q_mu"": {""value"": np.zeros((Data.M, 1)), ""trainable"": False, ""shape"": (Data.M, 1),},\n    ""SVGP.q_sqrt"": {""value"": np.ones((Data.M, 1)), ""trainable"": True, ""shape"": (Data.M, 1),},\n}\n\nexample_dag_module_param_dict = {\n    ""SVGP.kernel.variance\\nSVGP.kernel.lengthscales"": kernel_param_dict[\n        ""SquaredExponential.lengthscales""\n    ],\n    ""SVGP.likelihood.variance"": {""value"": 1.0, ""trainable"": True, ""shape"": ()},\n    ""SVGP.inducing_variable.Z"": {""value"": Data.Z, ""trainable"": True, ""shape"": (Data.M, Data.D),},\n    ""SVGP.q_mu"": {""value"": np.zeros((Data.M, 1)), ""trainable"": False, ""shape"": (Data.M, 1),},\n    ""SVGP.q_sqrt"": {""value"": np.ones((Data.M, 1)), ""trainable"": True, ""shape"": (Data.M, 1),},\n}\n\ncompose_kernel_param_print_string = """"""\\\nname                                        class      transform    prior    trainable    shape    dtype      value\\n\\\n------------------------------------------  ---------  -----------  -------  -----------  -------  -------  -------\\n\\\nProduct.kernels[0].kernels[0].variance      Parameter  Softplus              True         ()       float64        1\\n\\\nProduct.kernels[0].kernels[0].lengthscales  Parameter  Softplus              False        ()       float64        2\\n\\\nProduct.kernels[0].kernels[1].variance      Parameter  Softplus              True         ()       float64        1\\n\\\nProduct.kernels[0].kernels[1].lengthscales  Parameter  Softplus              False        ()       float64        2\\n\\\nProduct.kernels[1].kernels[0].variance      Parameter  Softplus              True         ()       float64        1\\n\\\nProduct.kernels[1].kernels[0].lengthscales  Parameter  Softplus              False        ()       float64        2\\n\\\nProduct.kernels[1].kernels[1].variance      Parameter  Softplus              True         ()       float64        1\\n\\\nProduct.kernels[1].kernels[1].lengthscales  Parameter  Softplus              False        ()       float64        2""""""\n\nkernel_param_print_string = """"""\\\nname                             class      transform    prior    trainable    shape    dtype      value\\n\\\n-------------------------------  ---------  -----------  -------  -----------  -------  -------  -------\\n\\\nSquaredExponential.variance      Parameter  Softplus              True         ()       float64        1\\n\\\nSquaredExponential.lengthscales  Parameter  Softplus              False        ()       float64        2""""""\n\nkernel_param_print_string_with_shift = """"""\\\nname                             class      transform         prior    trainable    shape    dtype      value\\n\\\n-------------------------------  ---------  ----------------  -------  -----------  -------  -------  -------\\n\\\nSquaredExponential.variance      Parameter  Softplus + Shift           True         ()       float64        1\\n\\\nSquaredExponential.lengthscales  Parameter  Softplus + Shift           False        ()       float64        2""""""\n\nmodel_gp_param_print_string = """"""\\\nname                      class      transform    prior    trainable    shape    dtype    value\\n\\\n------------------------  ---------  -----------  -------  -----------  -------  -------  --------\\n\\\nSVGP.kernel.variance      Parameter  Softplus              True         ()       float64  1.0\\n\\\nSVGP.kernel.lengthscales  Parameter  Softplus              False        ()       float64  2.0\\n\\\nSVGP.likelihood.variance  Parameter  Softplus              True         ()       float64  1.0\\n\\\nSVGP.inducing_variable.Z  Parameter                        True         (10, 1)  float64  [[0.5...\\n\\\nSVGP.q_mu                 Parameter                        False        (10, 1)  float64  [[0....\\n\\\nSVGP.q_sqrt               Parameter  Softplus              True         (10, 1)  float64  [[1....""""""\n\nexample_tf_module_variable_print_string = """"""\\\nname             class             transform    prior    trainable    shape      dtype    value\\n\\\n---------------  ----------------  -----------  -------  -----------  ---------  -------  --------\\n\\\nA.var_trainable  ResourceVariable                        True         (2, 2, 1)  float32  [[[0....\\n\\\nA.var_fixed      ResourceVariable                        False        (2, 2, 1)  float32  [[[1....""""""\n\nexample_module_list_variable_print_string = """"""\\\nname                                 class             transform    prior    trainable    shape      dtype    value\\n\\\n-----------------------------------  ----------------  -----------  -------  -----------  ---------  -------  --------\\n\\\nB.submodule_list[0].var_trainable    ResourceVariable                        True         (2, 2, 1)  float32  [[[0....\\n\\\nB.submodule_list[0].var_fixed        ResourceVariable                        False        (2, 2, 1)  float32  [[[1....\\n\\\nB.submodule_list[1].var_trainable    ResourceVariable                        True         (2, 2, 1)  float32  [[[0....\\n\\\nB.submodule_list[1].var_fixed        ResourceVariable                        False        (2, 2, 1)  float32  [[[1....\\n\\\nB.submodule_dict[\'a\'].var_trainable  ResourceVariable                        True         (2, 2, 1)  float32  [[[0....\\n\\\nB.submodule_dict[\'a\'].var_fixed      ResourceVariable                        False        (2, 2, 1)  float32  [[[1....\\n\\\nB.submodule_dict[\'b\'].var_trainable  ResourceVariable                        True         (2, 2, 1)  float32  [[[0....\\n\\\nB.submodule_dict[\'b\'].var_fixed      ResourceVariable                        False        (2, 2, 1)  float32  [[[1....\\n\\\nB.var_trainable                      ResourceVariable                        True         (2, 2, 1)  float32  [[[0....\\n\\\nB.var_fixed                          ResourceVariable                        False        (2, 2, 1)  float32  [[[1....""""""\n\n# Note: we use grid format here because we have a double reference to the same variable\n# which does not render nicely in the table formatting.\nexample_tf_keras_model = """"""\\\n+-------------------------+------------------+-------------+---------+-------------+-----------+---------+----------+\\n\\\n| name                    | class            | transform   | prior   | trainable   | shape     | dtype   | value    |\\n\\\n+=========================+==================+=============+=========+=============+===========+=========+==========+\\n\\\n| C._trainable_weights[0] | ResourceVariable |             |         | True        | (2, 2, 1) | float32 | [[[0.... |\\n\\\n| C.variable              |                  |             |         |             |           |         |          |\\n\\\n+-------------------------+------------------+-------------+---------+-------------+-----------+---------+----------+\\n\\\n| C.param                 | Parameter        |             |         | True        | ()        | float64 | 0.0      |\\n\\\n+-------------------------+------------------+-------------+---------+-------------+-----------+---------+----------+""""""\n\n# ------------------------------------------\n# Fixtures\n# ------------------------------------------\n\n\n@pytest.fixture(params=[A, B, create_kernel, create_model])\ndef module(request):\n    return request.param()\n\n\n@pytest.fixture\ndef dag_module():\n    dag = create_model()\n    dag.kernel.variance = dag.kernel.lengthscales\n    return dag\n\n\n# ------------------------------------------\n# Tests\n# ------------------------------------------\n\n\ndef test_leaf_components_only_returns_parameters_and_variables(module):\n    for path, variable in leaf_components(module).items():\n        assert isinstance(variable, tf.Variable) or isinstance(variable, gpflow.Parameter)\n\n\n@pytest.mark.parametrize(\n    ""module_callable, expected_param_dicts"",\n    [(create_kernel, kernel_param_dict), (create_model, model_gp_param_dict)],\n)\ndef test_leaf_components_registers_variable_properties(module_callable, expected_param_dicts):\n    module = module_callable()\n    for path, variable in leaf_components(module).items():\n        param_name = path.split(""."")[-2] + ""."" + path.split(""."")[-1]\n        assert isinstance(variable, gpflow.Parameter)\n        np.testing.assert_equal(variable.value().numpy(), expected_param_dicts[param_name][""value""])\n        assert variable.trainable == expected_param_dicts[param_name][""trainable""]\n        assert variable.shape == expected_param_dicts[param_name][""shape""]\n\n\n@pytest.mark.parametrize(\n    ""module_callable, expected_param_dicts"", [(create_compose_kernel, compose_kernel_param_dict),],\n)\ndef test_leaf_components_registers_compose_kernel_variable_properties(\n    module_callable, expected_param_dicts\n):\n    module = module_callable()\n    leaf_components_dict = leaf_components(module)\n    assert len(leaf_components_dict) > 0\n    for path, variable in leaf_components_dict.items():\n        path_as_list = path.split(""."")\n        param_name = path_as_list[-3] + ""."" + path_as_list[-2] + ""."" + path_as_list[-1]\n        assert isinstance(variable, gpflow.Parameter)\n        np.testing.assert_equal(variable.value().numpy(), expected_param_dicts[param_name][""value""])\n        assert variable.trainable == expected_param_dicts[param_name][""trainable""]\n        assert variable.shape == expected_param_dicts[param_name][""shape""]\n\n\n@pytest.mark.parametrize(\n    ""module_class, expected_var_dicts"",\n    [(A, example_tf_module_variable_dict), (B, example_module_list_variable_dict),],\n)\ndef test_leaf_components_registers_param_properties(module_class, expected_var_dicts):\n    module = module_class()\n    for path, variable in leaf_components(module).items():\n        var_name = path.split(""."")[-2] + ""."" + path.split(""."")[-1]\n        assert isinstance(variable, tf.Variable)\n        np.testing.assert_equal(variable.numpy(), expected_var_dicts[var_name][""value""])\n        assert variable.trainable == expected_var_dicts[var_name][""trainable""]\n        assert variable.shape == expected_var_dicts[var_name][""shape""]\n\n\n@pytest.mark.parametrize(""expected_var_dicts"", [example_dag_module_param_dict])\ndef test_merge_leaf_components_merges_keys_with_same_values(dag_module, expected_var_dicts):\n    leaf_components_dict = leaf_components(dag_module)\n    for path, variable in _merge_leaf_components(leaf_components_dict).items():\n        assert path in expected_var_dicts\n        for sub_path in path.split(""\\n""):\n            assert sub_path in leaf_components_dict\n            assert leaf_components_dict[sub_path] is variable\n\n\n@pytest.mark.parametrize(\n    ""module_callable, expected_param_print_string"",\n    [\n        (create_compose_kernel, compose_kernel_param_print_string),\n        (create_kernel, kernel_param_print_string),\n        (create_model, model_gp_param_print_string),\n        (A, example_tf_module_variable_print_string),\n        (B, example_module_list_variable_print_string),\n    ],\n)\ndef test_print_summary_output_string(module_callable, expected_param_print_string):\n    with as_context(Config(positive_minimum=0.0)):\n        assert tabulate_module_summary(module_callable()) == expected_param_print_string\n\n\ndef test_print_summary_output_string_with_positive_minimum():\n    with as_context(Config(positive_minimum=1e-6)):\n        assert tabulate_module_summary(create_kernel()) == kernel_param_print_string_with_shift\n\n\ndef test_print_summary_for_keras_model():\n    # Note: best to use `grid` formatting for `tf.keras.Model` printing\n    # because of the duplicates in the references to the variables.\n    assert tabulate_module_summary(C(), tablefmt=""grid"") == example_tf_keras_model\n\n\ndef test_leaf_components_combination_kernel():\n    """"""\n    Regression test for kernel compositions - output for printing should not be empty (issue #1066).\n    """"""\n    k = gpflow.kernels.SquaredExponential() + gpflow.kernels.SquaredExponential()\n    assert leaf_components(k), ""Combination kernel should have non-empty leaf components""\n\n\ndef test_module_parameters_return_iterators_not_generators():\n    """"""\n    Regression test: Ensure that gpflow.Module parameters return iterators like in TF2, not\n    generators.\n\n    Reason:\n    param = m.params  # <generator object>\n    x = [p for p in param] # List[Parameters]\n    y = [p for p in param] # [] empty!\n    """"""\n    m = create_model()\n    assert isinstance(m, gpflow.base.Module)\n    assert isinstance(m.parameters, tuple)\n    assert isinstance(m.trainable_parameters, tuple)\n'"
tests/gpflow/utilities/test_set_trainable.py,3,"b'import tensorflow as tf\n\nfrom gpflow import Parameter, set_trainable\n\n\ndef _module() -> tf.Module:\n    class _Mod(tf.Module):\n        def __init__(self):\n            super().__init__()\n            self.var = tf.Variable(0.0)\n            self.param = Parameter(0.0)\n\n    module = _Mod()\n\n    assert len(module.trainable_variables) == 2\n    assert len(module.variables) == 2\n\n    return module\n\n\ndef test_can_set_not_trainable():\n    module = _module()\n    set_trainable(module, False)\n    assert len(module.trainable_variables) == 0\n\n\ndef test_can_set_not_trainable_then_trainable_again():\n    module = _module()\n    set_trainable(module, False)\n    set_trainable(module, True)\n    assert len(module.trainable_variables) == len(module.variables)\n\n\ndef test_can_set_not_trainable_iterable():\n    modules = [_module(), _module(), _module()]\n    set_trainable(modules, False)\n    assert all(len(m.trainable_variables) == 0 for m in modules)\n\n\ndef test_can_set_not_trainable_then_trainable_iterable():\n    modules = [_module(), _module(), _module()]\n    set_trainable(modules, False)\n    set_trainable(modules, True)\n    assert all(len(m.trainable_variables) == len(m.variables) for m in modules)\n'"
tests/gpflow/utilities/test_utilities.py,7,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nimport gpflow\nfrom gpflow.config import Config, as_context\nfrom gpflow.models.util import data_input_to_tensor\nfrom gpflow.utilities import positive, triangular\nfrom gpflow.utilities.ops import difference_matrix\n\n\n@pytest.mark.parametrize(\n    ""env_lower, override_lower"",\n    [\n        (0.1, None),  # ensure default from config is applied\n        (0.0, 0.2),  # ensure override is applied\n        (0.3, 0.4),  # ensure local overrides config\n    ],\n)\ndef test_positive_lower(env_lower, override_lower):\n    expected_lower = override_lower or env_lower\n    with as_context(Config(positive_bijector=""softplus"", positive_minimum=env_lower)):\n        bijector = positive(lower=override_lower)\n        assert isinstance(bijector, tfp.bijectors.Chain)\n        assert np.isclose(bijector.bijectors[0].shift, expected_lower)\n\n\n@pytest.mark.parametrize(\n    ""env_bijector, override_bijector, expected_class"",\n    [\n        (""softplus"", None, tfp.bijectors.Softplus),\n        (""softplus"", ""Exp"", tfp.bijectors.Exp),\n        (""exp"", None, tfp.bijectors.Exp),\n        (""exp"", ""Softplus"", tfp.bijectors.Softplus),\n    ],\n)\ndef test_positive_bijector(env_bijector, override_bijector, expected_class):\n    with as_context(Config(positive_bijector=env_bijector, positive_minimum=0.0)):\n        bijector = positive(base=override_bijector)\n        assert isinstance(bijector, expected_class)\n\n\ndef test_positive_calculation_order():\n    value, lower = -10.0, 10.0\n    expected = np.exp(value) + lower\n    with as_context(Config(positive_bijector=""exp"", positive_minimum=lower)):\n        result = positive()(value).numpy()\n    assert np.isclose(result, expected)\n    assert result >= lower\n\n\ndef test_triangular():\n    assert isinstance(triangular(), tfp.bijectors.FillTriangular)\n\n\ndef test_select_parameters_with_prior():\n    kernel = gpflow.kernels.SquaredExponential()\n    params = gpflow.utilities.select_dict_parameters_with_prior(kernel)\n    assert params == {}\n\n    kernel.variance.prior = tfp.distributions.Gamma(1.0, 1.0)\n    params = gpflow.utilities.select_dict_parameters_with_prior(kernel)\n    assert len(params) == 1\n\n\ndef test_difference_matrix_broadcasting_symmetric():\n    X = np.random.randn(5, 4, 3, 2)\n    d = difference_matrix(X, None)\n    assert d.shape == (5, 4, 3, 3, 2)\n\n\ndef test_difference_matrix_broadcasting_cross():\n    X = np.random.randn(2, 3, 4, 5)\n    X2 = np.random.randn(8, 7, 6, 5)\n    d = difference_matrix(X, X2)\n    assert d.shape == (2, 3, 4, 8, 7, 6, 5)\n\n\ndef test_data_input_to_tensor():\n    input1 = (1.0, (2.0,))\n    output1 = data_input_to_tensor(input1)\n    assert output1[0].dtype == tf.float64\n    assert output1[1][0].dtype == tf.float64\n\n    input2 = (1.0, [2.0])\n    output2 = data_input_to_tensor(input2)\n    assert output2[0].dtype == tf.float64\n    assert output2[1][0].dtype == tf.float64\n\n    input3 = (1.0, (np.arange(3, dtype=np.float16),) * 2)\n    output3 = data_input_to_tensor(input3)\n    assert output3[0].dtype == tf.float64\n    assert output3[1][0].dtype == tf.float16\n    assert output3[1][1].dtype == tf.float16\n'"
doc/source/notebooks/advanced/advanced_many_points.pct.py,0,"b""# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# More details on models with many observation points\n# --\n#\n# see SGPR.ipynb. I guess it would also be interesting to cover mini-batching here...\n"""
doc/source/notebooks/advanced/changepoints.pct.py,0,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Change points\n#\n# *Joseph Hall (October 2019)*\n#\n# This notebook demonstrates the use of the `ChangePoints` kernel, which can be used to describe one-dimensional functions that contain a number of change-points, or regime changes. The kernel makes use of sigmoids ($\\sigma$) to blend smoothly between different kernels. For example, a single change-point kernel is defined by:\n#\n# \\begin{equation}\n# \\textrm{cov}(f(x), f(y)) = k_1(x, y)\\cdot\\bar{\\sigma}(x, y) + k_2(x, y)\\cdot\\sigma(x, y)\n# \\end{equation}\n#\n# where $\\sigma(x, y) = \\sigma(x)\\cdot\\sigma(y)$ and $\\bar{\\sigma}(x, y) = (1 - \\sigma(x))\\cdot(1 - \\sigma(y))$. The sigmoid ($\\sigma$) is parameterized by a location ($l$) and a width ($w$).\n\n# %%\nimport gpflow\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)  # for reproducibility of this notebook\n\nplt.style.use(""ggplot"")\n# %matplotlib inline\n\n\ndef plotkernelsample(k, ax, xmin=-3, xmax=3, title=None):\n    xx = np.linspace(xmin, xmax, 100)[:, None]\n    ax.plot(xx, np.random.multivariate_normal(np.zeros(100), k(xx), 3).T)\n    ax.set_title(title)\n\n\n# %% [markdown]\n# We demonstrate the use of the kernel by drawing a number of samples from different parameterizations. Firstly, a simple single change-point between two kernels of differing lengthscales.\n\n# %%\nnp.random.seed(1)\n\nbase_k1 = gpflow.kernels.Matern32(lengthscales=0.2)\nbase_k2 = gpflow.kernels.Matern32(lengthscales=2.0)\nk = gpflow.kernels.ChangePoints([base_k1, base_k2], [0.0], steepness=5.0)\n\nf, ax = plt.subplots(1, 1, figsize=(10, 3))\nplotkernelsample(k, ax)\n\n# %% [markdown]\n# Secondly, an implementation of a ""change window"" in which we change from one kernel to another, then back to the original.\n\n# %%\nnp.random.seed(3)\n\nbase_k1 = gpflow.kernels.Matern32(lengthscales=0.3)\nbase_k2 = gpflow.kernels.Constant()\nk = gpflow.kernels.ChangePoints([base_k1, base_k2, base_k1], locations=[-1, 1], steepness=10.0)\n\nf, ax = plt.subplots(1, 1, figsize=(10, 3))\nplotkernelsample(k, ax)\n\n# %% [markdown]\n# And finally, allowing different change-points to occur more or less abruptly by defining different steepness parameters.\n\n# %%\nnp.random.seed(2)\n\nbase_k1 = gpflow.kernels.Matern32(lengthscales=0.3)\nbase_k2 = gpflow.kernels.Constant()\nk = gpflow.kernels.ChangePoints(\n    [base_k1, base_k2, base_k1], locations=[-1, 1], steepness=[5.0, 50.0]\n)\n\nf, ax = plt.subplots(1, 1, figsize=(10, 3))\nplotkernelsample(k, ax)\n'"
doc/source/notebooks/advanced/convolutional.pct.py,1,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Convolutional Gaussian Processes\n# Mark van der Wilk (July 2019)\n#\n# Here we show a simple example of the rectangles experiment, where we compare a normal squared exponential GP, and a convolutional GP. This is similar to the experiment in [1].\n#\n# [1] Van der Wilk, Rasmussen, Hensman (2017). Convolutional Gaussian Processes. *Advances in Neural Information Processing Systems 30*.\n\n# %% [markdown]\n# ## Generate dataset\n# Generate a simple dataset of rectangles. We want to classify whether they are tall or wide. **NOTE:** Here we take care to make sure that the rectangles don\'t touch the edge, which is different to the original paper. We do this to avoid needing to use patch weights, which are needed to correctly account for edge effects.\n\n# %%\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gpflow\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom gpflow import set_trainable\nfrom gpflow.ci_utils import is_continuous_integration\n\ngpflow.config.set_default_float(np.float64)\ngpflow.config.set_default_jitter(1e-4)\ngpflow.config.set_default_summary_fmt(""notebook"")\n\n# for reproducibility of this notebook:\nnp.random.seed(123)\ntf.random.set_seed(42)\n\nMAXITER = 2 if is_continuous_integration() else 100\nNUM_TRAIN_DATA = (\n    5 if is_continuous_integration() else 100\n)  # This is less than in the original rectangles dataset\nNUM_TEST_DATA = 7 if is_continuous_integration() else 300\nH = W = 14  # width and height. In the original paper this is 28\nIMAGE_SHAPE = [H, W]\n\n\n# %%\ndef make_rectangle(arr, x0, y0, x1, y1):\n    arr[y0:y1, x0] = 1\n    arr[y0:y1, x1] = 1\n    arr[y0, x0:x1] = 1\n    arr[y1, x0 : x1 + 1] = 1\n\n\ndef make_random_rectangle(arr):\n    x0 = np.random.randint(1, arr.shape[1] - 3)\n    y0 = np.random.randint(1, arr.shape[0] - 3)\n    x1 = np.random.randint(x0 + 2, arr.shape[1] - 1)\n    y1 = np.random.randint(y0 + 2, arr.shape[0] - 1)\n    make_rectangle(arr, x0, y0, x1, y1)\n    return x0, y0, x1, y1\n\n\ndef make_rectangles_dataset(num, w, h):\n    d, Y = np.zeros((num, h, w)), np.zeros((num, 1))\n    for i, img in enumerate(d):\n        for j in range(1000):  # Finite number of tries\n            x0, y0, x1, y1 = make_random_rectangle(img)\n            rw, rh = y1 - y0, x1 - x0\n            if rw == rh:\n                img[:, :] = 0\n                continue\n            Y[i, 0] = rw > rh\n            break\n    return (\n        d.reshape(num, w * h).astype(gpflow.config.default_float()),\n        Y.astype(gpflow.config.default_float()),\n    )\n\n\n# %%\nX, Y = data = make_rectangles_dataset(NUM_TRAIN_DATA, *IMAGE_SHAPE)\nXt, Yt = test_data = make_rectangles_dataset(NUM_TEST_DATA, *IMAGE_SHAPE)\n\n# %%\nplt.figure(figsize=(8, 3))\nfor i in range(4):\n    plt.subplot(1, 4, i + 1)\n    plt.imshow(X[i, :].reshape(*IMAGE_SHAPE))\n    plt.title(Y[i, 0])\n\n# %% [markdown]\n# ## Squared Exponential kernel\n\n# %%\nrbf_m = gpflow.models.SVGP(\n    gpflow.kernels.SquaredExponential(),\n    gpflow.likelihoods.Bernoulli(),\n    gpflow.inducing_variables.InducingPoints(X.copy()),\n)\n\n# %%\nrbf_training_loss_closure = rbf_m.training_loss_closure(data, compile=True)\nrbf_elbo = lambda: -rbf_training_loss_closure().numpy()\nprint(""RBF elbo before training: %.4e"" % rbf_elbo())\n\n# %%\nset_trainable(rbf_m.inducing_variable, False)\nstart_time = time.time()\nres = gpflow.optimizers.Scipy().minimize(\n    rbf_training_loss_closure,\n    variables=rbf_m.trainable_variables,\n    method=""l-bfgs-b"",\n    options={""disp"": True, ""maxiter"": MAXITER},\n)\nprint(f""{res.nfev / (time.time() - start_time):.3f} iter/s"")\n\n# %%\ntrain_acc = np.mean((rbf_m.predict_y(X)[0] > 0.5).numpy().astype(""float"") == Y)\ntest_acc = np.mean((rbf_m.predict_y(Xt)[0] > 0.5).numpy().astype(""float"") == Yt)\nprint(f""Train acc: {train_acc * 100}%\\nTest acc : {test_acc*100}%"")\nprint(""RBF elbo after training: %.4e"" % rbf_elbo())\n\n# %% [markdown]\n# ## Convolutional kernel\n\n# %%\nf64 = lambda x: np.array(x, dtype=np.float64)\npositive_with_min = lambda: tfp.bijectors.AffineScalar(shift=f64(1e-4))(tfp.bijectors.Softplus())\nconstrained = lambda: tfp.bijectors.AffineScalar(shift=f64(1e-4), scale=f64(100.0))(\n    tfp.bijectors.Sigmoid()\n)\nmax_abs_1 = lambda: tfp.bijectors.AffineScalar(shift=f64(-2.0), scale=f64(4.0))(\n    tfp.bijectors.Sigmoid()\n)\n\npatch_shape = [3, 3]\nconv_k = gpflow.kernels.Convolutional(gpflow.kernels.SquaredExponential(), IMAGE_SHAPE, patch_shape)\nconv_k.base_kernel.lengthscales = gpflow.Parameter(1.0, transform=positive_with_min())\n# Weight scale and variance are non-identifiable. We also need to prevent variance from shooting off crazily.\nconv_k.base_kernel.variance = gpflow.Parameter(1.0, transform=constrained())\nconv_k.weights = gpflow.Parameter(conv_k.weights.numpy(), transform=max_abs_1())\nconv_f = gpflow.inducing_variables.InducingPatches(\n    np.unique(conv_k.get_patches(X).numpy().reshape(-1, 9), axis=0)\n)\n\n# %%\nconv_m = gpflow.models.SVGP(conv_k, gpflow.likelihoods.Bernoulli(), conv_f)\n\n# %%\nset_trainable(conv_m.inducing_variable, False)\nset_trainable(conv_m.kernel.base_kernel.variance, False)\nset_trainable(conv_m.kernel.weights, False)\n\n# %%\nconv_training_loss_closure = conv_m.training_loss_closure(data, compile=True)\nconv_elbo = lambda: -conv_training_loss_closure().numpy()\nprint(""conv elbo before training: %.4e"" % conv_elbo())\n\n# %%\nstart_time = time.time()\nres = gpflow.optimizers.Scipy().minimize(\n    conv_training_loss_closure,\n    variables=conv_m.trainable_variables,\n    method=""l-bfgs-b"",\n    options={""disp"": True, ""maxiter"": MAXITER / 10},\n)\nprint(f""{res.nfev / (time.time() - start_time):.3f} iter/s"")\n\n# %%\nset_trainable(conv_m.kernel.base_kernel.variance, True)\nres = gpflow.optimizers.Scipy().minimize(\n    conv_training_loss_closure,\n    variables=conv_m.trainable_variables,\n    method=""l-bfgs-b"",\n    options={""disp"": True, ""maxiter"": MAXITER},\n)\ntrain_acc = np.mean((conv_m.predict_y(X)[0] > 0.5).numpy().astype(""float"") == Y)\ntest_acc = np.mean((conv_m.predict_y(Xt)[0] > 0.5).numpy().astype(""float"") == Yt)\nprint(f""Train acc: {train_acc * 100}%\\nTest acc : {test_acc*100}%"")\nprint(""conv elbo after training: %.4e"" % conv_elbo())\n\n# %%\nres = gpflow.optimizers.Scipy().minimize(\n    conv_training_loss_closure,\n    variables=conv_m.trainable_variables,\n    method=""l-bfgs-b"",\n    options={""disp"": True, ""maxiter"": MAXITER},\n)\ntrain_acc = np.mean((conv_m.predict_y(X)[0] > 0.5).numpy().astype(""float"") == Y)\ntest_acc = np.mean((conv_m.predict_y(Xt)[0] > 0.5).numpy().astype(""float"") == Yt)\nprint(f""Train acc: {train_acc * 100}%\\nTest acc : {test_acc*100}%"")\nprint(""conv elbo after training: %.4e"" % conv_elbo())\n\n# %%\nset_trainable(conv_m.kernel.weights, True)\nres = gpflow.optimizers.Scipy().minimize(\n    conv_training_loss_closure,\n    variables=conv_m.trainable_variables,\n    method=""l-bfgs-b"",\n    options={""disp"": True, ""maxiter"": MAXITER},\n)\ntrain_acc = np.mean((conv_m.predict_y(X)[0] > 0.5).numpy().astype(""float"") == Y)\ntest_acc = np.mean((conv_m.predict_y(Xt)[0] > 0.5).numpy().astype(""float"") == Yt)\nprint(f""Train acc: {train_acc * 100}%\\nTest acc : {test_acc*100}%"")\nprint(""conv elbo after training: %.4e"" % conv_elbo())\n\n# %%\ngpflow.utilities.print_summary(rbf_m)\n\n# %%\ngpflow.utilities.print_summary(conv_m)\n\n# %% [markdown]\n# ## Conclusion\n# The convolutional kernel performs much better in this simple task. It demonstrates non-local generalization of the strong assumptions in the kernel.\n'"
doc/source/notebooks/advanced/coregionalisation.pct.py,0,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # A simple demonstration of coregionalization\n#\n# This notebook shows how to construct a multi-output GP model using GPflow. We will consider a regression problem for functions $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^P$. We assume that the dataset is of the form $(X_1, f_1), \\dots, (X_P, f_P)$, that is, we do not necessarily observe all the outputs for a particular input location (for cases where there are fully observed outputs for each input, see [Multi-output Gaussian processes in GPflow](./multioutput.ipynb) for a more efficient implementation). We allow each $f_i$ to have a different noise distribution by assigning a different likelihood to each.\n#\n# For this problem, we model $f$ as a *coregionalized* Gaussian process, which assumes a kernel of the form:\n#\n# \\begin{equation}\n# \\textrm{cov}(f_i(X), f_j(X^\\prime)) = k(X, X^\\prime) \\cdot B[i, j].\n# \\end{equation}\n#\n# The covariance of the $i$th function at $X$ and the $j$th function at $X^\\prime$ is a kernel applied at $X$ and $X^\\prime$, times the $(i, j)$th entry of a positive definite $P \\times P$ matrix $B$. This is known as the **intrinsic model of coregionalization (ICM)** (Bonilla and Williams, 2008).\n#\n# To make sure that B is positive-definite, we parameterize it as:\n#\n# \\begin{equation}\n# B = W W^\\top + \\textrm{diag}(\\kappa).\n# \\end{equation}\n#\n# To build such a model in GPflow, we need to perform the following two steps:\n#\n#  * Create the kernel function defined previously, using the `Coregion` kernel class.\n#  * Augment the training data X with an extra column that contains an integer index to indicate which output an observation is associated with. This is essential to make the data work with the `Coregion` kernel.\n#  * Create a likelihood for each output using the `SwitchedLikelihood` class, which is a container for other likelihoods.\n#  * Augment the training data Y with an extra column that contains an integer index to indicate which likelihood an observation is associated with.\n\n# %%\nimport gpflow\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\n\nfrom gpflow.ci_utils import ci_niter\n\nplt.rcParams[""figure.figsize""] = (12, 6)\nnp.random.seed(123)\n\n# %% [markdown]\n# ## Data preparation\n# We start by generating some training data to fit the model with. For this example, we choose the following two correlated functions for our outputs:\n#\n# \\begin{align}\n# y_1 &= \\sin(6x) + \\epsilon_1, \\qquad \\epsilon_1 \\sim \\mathcal{N}(0, 0.009) \\\\\n# y_2 &= \\sin(6x + 0.7) + \\epsilon_2, \\qquad \\epsilon_2 \\sim \\mathcal{N}(0, 0.01)\n# \\end{align}\n#\n\n# %%\n# make a dataset with two outputs, correlated, heavy-tail noise. One has more noise than the other.\nX1 = np.random.rand(100, 1)  # Observed locations for first output\nX2 = np.random.rand(50, 1) * 0.5  # Observed locations for second output\n\nY1 = np.sin(6 * X1) + np.random.randn(*X1.shape) * 0.03\nY2 = np.sin(6 * X2 + 0.7) + np.random.randn(*X2.shape) * 0.1\n\nplt.figure(figsize=(8, 4))\nplt.plot(X1, Y1, ""x"", mew=2)\n_ = plt.plot(X2, Y2, ""x"", mew=2)\n\n# %% [markdown]\n# ## Data formatting for the coregionalized model\n# We add an extra column to our training dataset that contains an index that specifies which output is observed.\n\n# %%\n# Augment the input with ones or zeros to indicate the required output dimension\nX_augmented = np.vstack((np.hstack((X1, np.zeros_like(X1))), np.hstack((X2, np.ones_like(X2)))))\n\n# Augment the Y data with ones or zeros that specify a likelihood from the list of likelihoods\nY_augmented = np.vstack((np.hstack((Y1, np.zeros_like(Y1))), np.hstack((Y2, np.ones_like(Y2)))))\n\n# %% [markdown]\n# ## Building the coregionalization kernel\n# We build a coregionalization kernel with the Matern 3/2 kernel as the base kernel. This acts on the leading ([0]) data dimension of the augmented X values. The `Coregion` kernel indexes the outputs, and acts on the last ([1]) data dimension (indices) of the augmented X values. To specify these dimensions, we use the built-in `active_dims` argument in the kernel constructor. To construct the full multi-output kernel, we take the product of the two kernels (for a more in-depth tutorial on kernel combination, see [Manipulating kernels](./kernels.ipynb)).\n\n# %%\noutput_dim = 2  # Number of outputs\nrank = 1  # Rank of W\n\n# Base kernel\nk = gpflow.kernels.Matern32(active_dims=[0])\n\n# Coregion kernel\ncoreg = gpflow.kernels.Coregion(output_dim=output_dim, rank=rank, active_dims=[1])\n\nkern = k * coreg\n\n# %% [markdown]\n# **Note:** W = 0 is a saddle point in the objective, which would result in the value of `W` not being optimized to fit the data.\n# Hence, by default, the `W` matrix is initialized with 0.1. Alternatively, you could re-initialize the matrix to random entries.\n\n# %% [markdown]\n# ## Constructing the model\n# The final element in building the model is to specify the likelihood for each output dimension. To do this, use the `SwitchedLikelihood` object in GPflow.\n\n# %%\n# This likelihood switches between Gaussian noise with different variances for each f_i:\nlik = gpflow.likelihoods.SwitchedLikelihood(\n    [gpflow.likelihoods.Gaussian(), gpflow.likelihoods.Gaussian()]\n)\n\n# now build the GP model as normal\nm = gpflow.models.VGP((X_augmented, Y_augmented), kernel=kern, likelihood=lik)\n\n# fit the covariance function parameters\nmaxiter = ci_niter(10000)\ngpflow.optimizers.Scipy().minimize(\n    m.training_loss, m.trainable_variables, options=dict(maxiter=maxiter), method=""L-BFGS-B"",\n)\n\n\n# %% [markdown]\n# That\'s it: the model is trained. Let\'s plot the model fit to see what\'s happened.\n\n# %%\ndef plot_gp(x, mu, var, color, label):\n    plt.plot(x, mu, color=color, lw=2, label=label)\n    plt.fill_between(\n        x[:, 0],\n        (mu - 2 * np.sqrt(var))[:, 0],\n        (mu + 2 * np.sqrt(var))[:, 0],\n        color=color,\n        alpha=0.4,\n    )\n\n\ndef plot(m):\n    plt.figure(figsize=(8, 4))\n    Xtest = np.linspace(0, 1, 100)[:, None]\n    (line,) = plt.plot(X1, Y1, ""x"", mew=2)\n    mu, var = m.predict_f(np.hstack((Xtest, np.zeros_like(Xtest))))\n    plot_gp(Xtest, mu, var, line.get_color(), ""Y1"")\n\n    (line,) = plt.plot(X2, Y2, ""x"", mew=2)\n    mu, var = m.predict_f(np.hstack((Xtest, np.ones_like(Xtest))))\n    plot_gp(Xtest, mu, var, line.get_color(), ""Y2"")\n\n    plt.legend()\n\n\nplot(m)\n\n# %% [markdown]\n# From the plots, we can see:\n#\n#  - The first function (blue) has low posterior variance everywhere because there are so many observations, and the noise variance is small.\n#  - The second function (orange) has higher posterior variance near the data, because the data are more noisy, and very high posterior variance where there are no observations (x > 0.5).\n#  - The model has done a reasonable job of estimating the noise variance and lengthscale.\n#  - The model recognises the correlation between the two functions and is able to suggest (with uncertainty) that because x > 0.5 the orange curve should follow the blue curve (which we know to be the truth from the data-generating procedure).\n#\n# The covariance matrix between outputs is as follows:\n\n# %%\nB = coreg.output_covariance().numpy()\nprint(""B ="", B)\n_ = plt.imshow(B)\n\n# %% [markdown]\n# ## References\n#\n# Bonilla, Edwin V., Kian M. Chai, and Christopher Williams. ""Multi-task Gaussian process prediction."" _Advances in neural information processing systems_. 2008.\n'"
doc/source/notebooks/advanced/gps_for_big_data.pct.py,7,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Stochastic Variational Inference for scalability with SVGP\n\n# %% [markdown]\n# One of the main criticisms of Gaussian processes is their scalability to large datasets. In this notebook, we illustrate how to use the state-of-the-art Stochastic Variational Gaussian Process (SVGP) (*Hensman, et. al. 2013*) to overcome this problem.\n\n# %%\n# %matplotlib inline\nimport itertools\nimport numpy as np\nimport time\nimport gpflow\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom gpflow.ci_utils import ci_niter\n\nplt.style.use(""ggplot"")\n\n# for reproducibility of this notebook:\nrng = np.random.RandomState(123)\ntf.random.set_seed(42)\n\n# %% [markdown]\n# ## Generating data\n# For this notebook example, we generate 10,000 noisy observations from a test function:\n# \\begin{equation}\n# f(x) = \\sin(3\\pi x) + 0.3\\cos(9\\pi x) + \\frac{\\sin(7 \\pi x)}{2}\n# \\end{equation}\n\n# %%\ndef func(x):\n    return np.sin(x * 3 * 3.14) + 0.3 * np.cos(x * 9 * 3.14) + 0.5 * np.sin(x * 7 * 3.14)\n\n\nN = 10000  # Number of training observations\n\nX = rng.rand(N, 1) * 2 - 1  # X values\nY = func(X) + 0.2 * rng.randn(N, 1)  # Noisy Y values\ndata = (X, Y)\n\n# %% [markdown]\n# We plot the data along with the noiseless generating function:\n\n# %%\nplt.plot(X, Y, ""x"", alpha=0.2)\nXt = np.linspace(-1.1, 1.1, 1000)[:, None]\nYt = func(Xt)\n_ = plt.plot(Xt, Yt, c=""k"")\n\n# %% [markdown]\n# ## Building the model\n# The main idea behind SVGP is to approximate the true GP posterior with a GP conditioned on a small set of ""inducing"" values. This smaller set can be thought of as summarizing the larger dataset. For this example, we will select a set of 50 inducing locations that are initialized from the training dataset:\n\n# %%\nM = 50  # Number of inducing locations\n\nkernel = gpflow.kernels.SquaredExponential()\nZ = X[:M, :].copy()  # Initialize inducing locations to the first M inputs in the dataset\n\nm = gpflow.models.SVGP(kernel, gpflow.likelihoods.Gaussian(), Z, num_data=N)\n\n# %% [markdown]\n# ## Likelihood computation: batch vs. minibatch\n# First we showcase the model\'s performance using the whole dataset to compute the ELBO.\n\n# %%\nelbo = tf.function(m.elbo)\n\n# %%\n# TensorFlow re-traces & compiles a `tf.function`-wrapped method at *every* call if the arguments are numpy arrays instead of tf.Tensors. Hence:\ntensor_data = tuple(map(tf.convert_to_tensor, data))\nelbo(tensor_data)  # run it once to trace & compile\n\n# %%\n# %%timeit\nelbo(tensor_data)\n\n# %% [markdown]\n# We can speed up this calculation by using minibatches of the data. For this example, we use minibatches of size 100.\n\n# %%\nminibatch_size = 100\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).repeat().shuffle(N)\n\ntrain_iter = iter(train_dataset.batch(minibatch_size))\n\nground_truth = elbo(tensor_data).numpy()\n\n# %%\n# %%timeit\nelbo(next(train_iter))\n\n# %% [markdown]\n# ### Stochastical estimation of ELBO\n# The minibatch estimate should be an unbiased estimator of the `ground_truth`. Here we show a histogram of the value from different evaluations, together with its mean and the ground truth. The small difference between the mean of the minibatch estimations and the ground truth shows that the minibatch estimator is working as expected.\n\n# %%\nevals = [elbo(minibatch).numpy() for minibatch in itertools.islice(train_iter, 100)]\n\n# %%\nplt.hist(evals, label=""Minibatch estimations"")\nplt.axvline(ground_truth, c=""k"", label=""Ground truth"")\nplt.axvline(np.mean(evals), c=""g"", ls=""--"", label=""Minibatch mean"")\nplt.legend()\nplt.title(""Histogram of ELBO evaluations using minibatches"")\nprint(""Discrepancy between ground truth and minibatch estimate:"", ground_truth - np.mean(evals))\n\n# %% [markdown]\n# ### Minibatches speed up computation\n# The reason for using minibatches is that it decreases the time needed to make an optimization step, because estimating the objective is computationally cheaper with fewer data points. Here we plot the change in time required with the size of the minibatch. We see that smaller minibatches result in a cheaper estimate of the objective.\n\n# %%\n# Evaluate objective for different minibatch sizes\nminibatch_proportions = np.logspace(-2, 0, 10)\ntimes = []\nobjs = []\nfor mbp in minibatch_proportions:\n    batchsize = int(N * mbp)\n    train_iter = iter(train_dataset.batch(batchsize))\n    start_time = time.time()\n    objs.append([elbo(minibatch) for minibatch in itertools.islice(train_iter, 20)])\n    times.append(time.time() - start_time)\n\n# %%\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\nax1.plot(minibatch_proportions, times, ""x-"")\nax1.set_xlabel(""Minibatch proportion"")\nax1.set_ylabel(""Time taken"")\n\nax2.plot(minibatch_proportions, np.array(objs), ""kx"")\nax2.set_xlabel(""Minibatch proportion"")\nax2.set_ylabel(""ELBO estimates"")\n\n\n# %% [markdown]\n# ### Running stochastic optimization\n\n# %% [markdown]\n# First we create a utility function that plots the model\'s predictions:\n\n# %%\ndef plot(title=""""):\n    plt.figure(figsize=(12, 4))\n    plt.title(title)\n    pX = np.linspace(-1, 1, 100)[:, None]  # Test locations\n    pY, pYv = m.predict_y(pX)  # Predict Y values at test locations\n    plt.plot(X, Y, ""x"", label=""Training points"", alpha=0.2)\n    (line,) = plt.plot(pX, pY, lw=1.5, label=""Mean of predictive posterior"")\n    col = line.get_color()\n    plt.fill_between(\n        pX[:, 0],\n        (pY - 2 * pYv ** 0.5)[:, 0],\n        (pY + 2 * pYv ** 0.5)[:, 0],\n        color=col,\n        alpha=0.6,\n        lw=1.5,\n    )\n    Z = m.inducing_variable.Z.numpy()\n    plt.plot(Z, np.zeros_like(Z), ""k|"", mew=2, label=""Inducing locations"")\n    plt.legend(loc=""lower right"")\n\n\nplot(title=""Predictions before training"")\n\n# %% [markdown]\n# Now we can train our model. For optimizing the ELBO, we use the Adam Optimizer *(Kingma and Ba 2015)* which is designed for stochastic objective functions. We create a `run_adam` utility function  to perform the optimization.\n\n# %%\nminibatch_size = 100\n\n# We turn off training for inducing point locations\ngpflow.set_trainable(m.inducing_variable, False)\n\n\ndef run_adam(model, iterations):\n    """"""\n    Utility function running the Adam optimizer\n    \n    :param model: GPflow model\n    :param interations: number of iterations\n    """"""\n    # Create an Adam Optimizer action\n    logf = []\n    train_iter = iter(train_dataset.batch(minibatch_size))\n    training_loss = model.training_loss_closure(train_iter, compile=True)\n    optimizer = tf.optimizers.Adam()\n\n    @tf.function\n    def optimization_step():\n        optimizer.minimize(training_loss, model.trainable_variables)\n\n    for step in range(iterations):\n        optimization_step()\n        if step % 10 == 0:\n            elbo = -training_loss().numpy()\n            logf.append(elbo)\n    return logf\n\n\n# %% [markdown]\n# Now we run the optimization loop for 20,000 iterations.\n\n# %%\nmaxiter = ci_niter(20000)\n\nlogf = run_adam(m, maxiter)\nplt.plot(np.arange(maxiter)[::10], logf)\nplt.xlabel(""iteration"")\n_ = plt.ylabel(""ELBO"")\n\n# %% [markdown]\n# Finally, we plot the model\'s predictions.\n\n# %%\nplot(""Predictions after training"")\n\n# %% [markdown]\n# ## Further reading\n#\n# Several notebooks expand on this one:\n#\n# - [Advanced Sparse GP regression](../advanced/advanced_many_points.ipynb), which goes into deeper detail on sparse Gaussian process methods.\n# - [Optimization](../advanced/optimisation.ipynb) discussing optimizing GP models.\n# - [Natural gradients](../advanced/natural_gradients.ipynb) for optimizing SVGP models efficiently.\n\n# %% [markdown]\n# ## References:\n# Hensman, James, Nicolo Fusi, and Neil D. Lawrence. ""Gaussian processes for big data."" Uncertainty in Artificial Intelligence (2013).\n#\n# Kingma, Diederik P., and Jimmy Ba. ""Adam: A method for stochastic optimization."" arXiv preprint arXiv:1412.6980 (2014).\n'"
doc/source/notebooks/advanced/kernels.pct.py,0,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Manipulating kernels\n#\n#\n# GPflow comes with a range of kernels. In this notebook, we examine some of them, show how you can combine them to make new kernels, and discuss the `active_dims` feature.\n\n# %%\nimport gpflow\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.style.use(""ggplot"")\nimport tensorflow as tf\n\n# %matplotlib inline\n\n# %% [markdown]\n# ## Standard kernels in GPflow\n#\n# GPflow comes with lots of standard kernels. Some very simple kernels produce constant functions, linear functions, and white noise functions:\n#\n#  * `gpflow.kernels.Constant`\n#\n#  * `gpflow.kernels.Linear`\n#\n#  * `gpflow.kernels.White`\n#\n# Some stationary functions produce samples with varying degrees of smoothness:\n#\n#  * `gpflow.kernels.Exponential`\n#\n#  * `gpflow.kernels.Matern12`\n#\n#  * `gpflow.kernels.Matern32`\n#\n#  * `gpflow.kernels.Matern52`\n#\n#  * `gpflow.kernels.SquaredExponential` (also known as `gpflow.kernels.RBF`)\n#\n#  * `gpflow.kernels.RationalQuadratic`\n#\n# Two kernels produce periodic samples:\n#\n#  * `gpflow.kernels.Cosine`\n#\n#  * `gpflow.kernels.Periodic`\n#\n# Other kernels that are implemented in core GPflow include:\n#\n#  * `gpflow.kernels.Polynomial`\n#\n#  * `gpflow.kernels.ArcCosine` (""neural network kernel"")\n#\n#  * `gpflow.kernels.Coregion`\n#\n# Let\'s define some plotting utils functions and have a look at samples from the prior for some of them:\n\n# %%\ndef plotkernelsample(k, ax, xmin=-3, xmax=3):\n    xx = np.linspace(xmin, xmax, 100)[:, None]\n    K = k(xx)\n    ax.plot(xx, np.random.multivariate_normal(np.zeros(100), K, 3).T)\n    ax.set_title(k.__class__.__name__)\n\n\nnp.random.seed(27)\nf, axes = plt.subplots(2, 4, figsize=(12, 6), sharex=True, sharey=True)\nplotkernelsample(gpflow.kernels.Matern12(), axes[0, 0])\nplotkernelsample(gpflow.kernels.Matern32(), axes[0, 1])\nplotkernelsample(gpflow.kernels.Matern52(), axes[0, 2])\nplotkernelsample(gpflow.kernels.RBF(), axes[0, 3])\nplotkernelsample(gpflow.kernels.Constant(), axes[1, 0])\nplotkernelsample(gpflow.kernels.Linear(), axes[1, 1])\nplotkernelsample(gpflow.kernels.Cosine(), axes[1, 2])\nplotkernelsample(gpflow.kernels.Periodic(gpflow.kernels.SquaredExponential()), axes[1, 3])\n_ = axes[0, 0].set_ylim(-3, 3)\n\n# %% [markdown]\n# ## First example: create a Matern 3/2 covariance kernel\n#\n# Many kernels have hyperparameters, for example `variance` and `lengthscales`. You can change the value of these parameters from their default value of `1.0`.\n\n# %%\nk = gpflow.kernels.Matern32(variance=10.0, lengthscales=2)\n\n# %% [markdown]\n# **NOTE:** The values specified for the `variance` and `lengthscales` parameters are **floats**.\n#\n# To get information about the kernel, use `print_summary(k)` (plain text) or, in a notebook, pass the option `fmt=""notebook""` to obtain a nicer rendering:\n\n# %%\nfrom gpflow.utilities import print_summary\n\nprint_summary(k)\nprint_summary(k, fmt=""notebook"")\n# You can change the default format as follows:\ngpflow.config.set_default_summary_fmt(""notebook"")\nprint_summary(k)\n\n# %% [markdown]\n# You can access the parameter values and assign new values with the same syntax as for models:\n\n# %%\nprint(k.lengthscales)\nk.lengthscales.assign(0.5)\nprint(k.lengthscales)\n\n# %% [markdown]\n# Finally, you can *call* the kernel object to compute covariance matrices:\n\n# %%\nX1 = np.array([[0.0]])\nX2 = np.linspace(-2, 2, 101).reshape(-1, 1)\n\nK21 = k(X2, X1)  # cov(f(X2), f(X1)): matrix with shape [101, 1]\nK22 = k(X2)  # equivalent to k(X2, X2) (but more efficient): matrix with shape [101, 101]\n\n# plotting\nplt.figure()\n_ = plt.plot(X2, K21)\n\n# %% [markdown]\n# ## Combine kernels\n# Sums and products of kernels are also valid kernels.\n# You can add or multiply instances of kernels to create a new composite kernel with the parameters of the old ones:\n\n# %%\nk1 = gpflow.kernels.Matern12()\nk2 = gpflow.kernels.Linear()\n\nk3 = k1 + k2\nk4 = k1 * k2\n\nprint_summary(k3)\nprint_summary(k4)\n\n\ndef plotkernelfunction(k, ax, xmin=-3, xmax=3, other=0):\n    xx = np.linspace(xmin, xmax, 200)[:, None]\n    ax.plot(xx, k(xx, np.zeros((1, 1)) + other))\n    ax.set_title(k.__class__.__name__ + "" k(x, %f)"" % other)\n\n\nf, axes = plt.subplots(2, 2, figsize=(12, 6), sharex=True)\nplotkernelfunction(k3, axes[0, 0], other=1.0)\nplotkernelfunction(k4, axes[0, 1], other=1.0)\nplotkernelsample(k3, axes[1, 0])\nplotkernelsample(k4, axes[1, 1])\n\n# %% [markdown]\n# ## Kernels for higher-dimensional input spaces\n#\n# Kernels generalize to multiple dimensions straightforwardly. Stationary kernels support ""Automatic Relevance Determination"" (ARD), that is, having a different lengthscale parameter for each input dimension. Simply pass in an array of the same length as the number of input dimensions. **NOTE:** This means that the kernel object is then able to process only inputs of that dimension!\n\n# %% [markdown]\n# You can also initialize the lengthscales when the object is created:\n\n# %%\nk = gpflow.kernels.Matern52(lengthscales=[0.1, 0.2, 5.0])\nprint_summary(k)\n\n# %% [markdown]\n# ## Specify active dimensions\n#\n# When combining kernels, it\'s often helpful to have bits of the kernel working on different dimensions. For example, to model a function that is linear in the first dimension and smooth in the second, we could use a combination of Linear and Matern52 kernels, one for each dimension.\n#\n# To tell GPflow which dimension a kernel applies to, specify a list of integers as the value of the `active_dims` parameter.\n\n# %%\nk1 = gpflow.kernels.Linear(active_dims=[0])\nk2 = gpflow.kernels.Matern52(active_dims=[1])\nk = k1 + k2\n\n# %% [markdown]\n# `active_dims` makes it easy to create additive models. Here we build an additive Matern 5/2 kernel:\n\n# %%\nk = gpflow.kernels.Matern52(active_dims=[0], lengthscales=2) + gpflow.kernels.Matern52(\n    active_dims=[1], lengthscales=2\n)\n\n# %% [markdown]\n# Let\'s plot this kernel and sample from it:\n\n# %%\nn_grid = 30\nx = np.linspace(-10, 10, n_grid)\nX, Y = np.meshgrid(x, x)\nX = np.vstack((X.flatten(), Y.flatten())).T\n\nx0 = np.array([[2.0, 2.0]])\n# plot the kernel\nKxX = k(X, x0).numpy().reshape(n_grid, n_grid)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(KxX, extent=[-10, 10, -10, 10])\naxes[0].set_title(f""$k((7, 5), (x1, x2))$"")\n\n# plot a GP sample\nK = k(X).numpy()\nZ = np.random.multivariate_normal(np.zeros(n_grid ** 2), K, 2)\naxes[1].imshow(Z[0, :].reshape(n_grid, n_grid), extent=[-10, 10, -10, 10])\naxes[1].set_title(""GP sample 1"")\naxes[2].imshow(Z[1, :].reshape(n_grid, n_grid), extent=[-10, 10, -10, 10])\n_ = axes[2].set_title(""GP sample 2"")\n\n# %% [markdown]\n# ## Define new covariance functions\n#\n# GPflow makes it easy to define new covariance functions. See [Kernel design](../tailor/kernel_design.ipynb) for more information.\n'"
doc/source/notebooks/advanced/mcmc.pct.py,6,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.4.0\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # MCMC (Markov Chain Monte Carlo)\n\n# %% [markdown]\n# GPflow allows you to approximate the posterior over the latent functions of its models (and over the hyperparemeters after setting a prior for those) using Hamiltonian Monte Carlo (HMC)\n\n# %%\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\n\nimport gpflow\nfrom gpflow.ci_utils import ci_niter\nfrom gpflow import set_trainable\nfrom multiclass_classification import plot_from_samples, colors\n\ngpflow.config.set_default_float(np.float64)\ngpflow.config.set_default_jitter(1e-4)\ngpflow.config.set_default_summary_fmt(""notebook"")\n# convert to float64 for tfp to play nicely with gpflow in 64\nf64 = gpflow.utilities.to_default_float\n\ntf.random.set_seed(123)\n\n\n# %matplotlib inline\n\n# %% [markdown]\n#\n# In this notebook, we provide three examples:\n#\n# * [Example 1](#Example-1:-GP-regression): Sampling hyperparameters in Gaussian process regression\n# * [Example 2](#Example-2:-Sparse-MC-for-multiclass-classification): Sparse Variational MC applied to the multiclass classification problem\n# * [Example 3](#Example-3:-Fully-Bayesian-inference-for-generalized-GP-models-with-HMC): Full Bayesian inference for Gaussian process models\n\n# %% [markdown]\n# ## Example 1: GP regression\n\n# %% [markdown]\n# We first consider the GP regression (with Gaussian noise) for which the marginal likelihood $p(\\mathbf y\\,|\\,\\theta)$ can be computed exactly.\n#\n# The GPR model parameterized by $\\theta = [\\tau]$ is given by\n# \\begin{equation}\n# Y_i = f(X_i) + \\varepsilon_i\n# \\end{equation}\n# where $f \\sim \\mathcal{GP}(\\mu(.), k(., .))$, and $\\varepsilon \\sim \\mathcal{N}(0, \\tau^2 I)$.\n#\n# See the [Basic (Gaussian likelihood) GP regression model](../basics/regression.ipynb) for more details on GPR and for a treatment of the direct likelihood maximization.\n#\n#\n\n# %% [markdown]\n# ### Data for a one-dimensional regression problem\n\n# %%\nrng = np.random.RandomState(42)\n\nN = 30\n\n\ndef synthetic_data(num: int, rng: np.random.RandomState):\n    X = rng.rand(num, 1)\n    Y = np.sin(12 * X) + 0.66 * np.cos(25 * X) + rng.randn(num, 1) * 0.1 + 3\n    return X, Y\n\n\ndata = (X, Y) = synthetic_data(N, rng)\n\nplt.figure(figsize=(12, 6))\nplt.plot(X, Y, ""kx"", mew=2)\nplt.xlabel(""$X$"")\nplt.ylabel(""$Y$"")\nplt.title(""toy data"")\nplt.show()\n\n# %% [markdown]\n# ### MCMC for hyperparameters $\\theta$\n#\n# We now want to sample from the posterior over $\\theta$:\n# \\begin{equation}\n# p(\\theta|\\mathbf{y}) \\propto p(\\mathbf{y}|\\theta)p(\\theta)\n# \\end{equation}\n#\n# Firstly, we build the GPR model.\n\n# %%\nkernel = gpflow.kernels.Matern52(lengthscales=0.3)\nmean_function = gpflow.mean_functions.Linear(1.0, 0.0)\nmodel = gpflow.models.GPR(data, kernel, mean_function, noise_variance=0.01)\n\n# %% [markdown]\n# Secondly, we initialize the model to the maximum likelihood solution.\n\n# %%\noptimizer = gpflow.optimizers.Scipy()\noptimizer.minimize(model.training_loss, model.trainable_variables)\n\nprint(f""log posterior density at optimum: {model.log_posterior_density()}"")\n\n# %% [markdown]\n# Thirdly, we add priors to the hyperparameters.\n\n# %%\n# tfp.distributions dtype is inferred from parameters - so convert to 64-bit\nmodel.kernel.lengthscales.prior = tfd.Gamma(f64(1.0), f64(1.0))\nmodel.kernel.variance.prior = tfd.Gamma(f64(1.0), f64(1.0))\nmodel.likelihood.variance.prior = tfd.Gamma(f64(1.0), f64(1.0))\nmodel.mean_function.A.prior = tfd.Normal(f64(0.0), f64(10.0))\nmodel.mean_function.b.prior = tfd.Normal(f64(0.0), f64(10.0))\n\ngpflow.utilities.print_summary(model)\n\n# %% [markdown]\n# We now sample from the posterior using HMC.\n\n# %%\nnum_burnin_steps = ci_niter(300)\nnum_samples = ci_niter(500)\n\n# Note that here we need model.trainable_parameters, not trainable_variables - only parameters can have priors!\nhmc_helper = gpflow.optimizers.SamplingHelper(\n    model.log_posterior_density, model.trainable_parameters\n)\n\nhmc = tfp.mcmc.HamiltonianMonteCarlo(\n    target_log_prob_fn=hmc_helper.target_log_prob_fn, num_leapfrog_steps=10, step_size=0.01\n)\nadaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n    hmc, num_adaptation_steps=10, target_accept_prob=f64(0.75), adaptation_rate=0.1\n)\n\n\n@tf.function\ndef run_chain_fn():\n    return tfp.mcmc.sample_chain(\n        num_results=num_samples,\n        num_burnin_steps=num_burnin_steps,\n        current_state=hmc_helper.current_state,\n        kernel=adaptive_hmc,\n        trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n    )\n\n\nsamples, traces = run_chain_fn()\nparameter_samples = hmc_helper.convert_to_constrained_values(samples)\n\nparam_to_name = {param: name for name, param in gpflow.utilities.parameter_dict(model).items()}\n\n\n# %% [markdown]\n# **NOTE:** All the Hamiltonian MCMC sampling takes place in an unconstrained space (where constrained parameters have been mapped via a bijector to an unconstrained space). This makes the optimization, as required in the gradient step, much easier.\n#\n# However, we often wish to sample the constrained parameter values, not the unconstrained one. The `SamplingHelper` helps us convert our unconstrained values to constrained parameter ones.\n#\n\n# %%\ndef plot_samples(samples, parameters, y_axis_label):\n    plt.figure(figsize=(8, 4))\n    for val, param in zip(samples, parameters):\n        plt.plot(tf.squeeze(val), label=param_to_name[param])\n    plt.legend(bbox_to_anchor=(1.0, 1.0))\n    plt.xlabel(""HMC iteration"")\n    plt.ylabel(y_axis_label)\n\n\nplot_samples(samples, model.trainable_parameters, ""unconstrained values"")\nplot_samples(parameter_samples, model.trainable_parameters, ""constrained parameter values"")\n\n\n# %% [markdown]\n# You can also inspect the marginal distribution of samples.\n\n# %%\ndef marginal_samples(samples, parameters, y_axis_label):\n    fig, axes = plt.subplots(1, len(param_to_name), figsize=(15, 3), constrained_layout=True)\n    for ax, val, param in zip(axes, samples, parameters):\n        ax.hist(np.stack(val).flatten(), bins=20)\n        ax.set_title(param_to_name[param])\n    fig.suptitle(y_axis_label)\n    plt.show()\n\n\nmarginal_samples(samples, model.trainable_parameters, ""unconstrained variable samples"")\nmarginal_samples(parameter_samples, model.trainable_parameters, ""constrained parameter samples"")\n\n\n# %% [markdown]\n#\n#\n# **NOTE:** The sampler runs in unconstrained space (so that positive parameters remain positive, and parameters that are not trainable are ignored).\n#\n# For serious analysis you most certainly want to run the sampler longer, with multiple chains and convergence checks. This will do for illustration though!\n#\n\n# %%\ndef plot_joint_marginals(samples, parameters, y_axis_label):\n    name_to_index = {param_to_name[param]: i for i, param in enumerate(parameters)}\n    f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n\n    axs[0].plot(\n        samples[name_to_index["".likelihood.variance""]],\n        samples[name_to_index["".kernel.variance""]],\n        ""k."",\n        alpha=0.15,\n    )\n    axs[0].set_xlabel(""noise_variance"")\n    axs[0].set_ylabel(""signal_variance"")\n\n    axs[1].plot(\n        samples[name_to_index["".likelihood.variance""]],\n        samples[name_to_index["".kernel.lengthscales""]],\n        ""k."",\n        alpha=0.15,\n    )\n    axs[1].set_xlabel(""noise_variance"")\n    axs[1].set_ylabel(""lengthscale"")\n\n    axs[2].plot(\n        samples[name_to_index["".kernel.lengthscales""]],\n        samples[name_to_index["".kernel.variance""]],\n        ""k."",\n        alpha=0.1,\n    )\n    axs[2].set_xlabel(""lengthscale"")\n    axs[2].set_ylabel(""signal_variance"")\n    f.suptitle(y_axis_label)\n    plt.show()\n\n\nplot_joint_marginals(samples, model.trainable_parameters, ""unconstrained variable samples"")\nplot_joint_marginals(parameter_samples, model.trainable_parameters, ""parameter samples"")\n\n\n# %% [markdown]\n# To plot the posterior of predictions, we\'ll iterate through the samples and set the model state with each sample. Then, for that state (set of hyperparameters) we\'ll draw some samples from the prediction function.\n\n# %%\n# plot the function posterior\nxx = np.linspace(-0.1, 1.1, 100)[:, None]\nplt.figure(figsize=(12, 6))\n\nfor i in range(0, num_samples, 20):\n    for var, var_samples in zip(hmc_helper.current_state, samples):\n        var.assign(var_samples[i])\n    f = model.predict_f_samples(xx, 1)\n    plt.plot(xx, f[0, :, :], ""C0"", lw=2, alpha=0.3)\n\nplt.plot(X, Y, ""kx"", mew=2)\n_ = plt.xlim(xx.min(), xx.max())\n_ = plt.ylim(0, 6)\nplt.xlabel(""$x$"")\nplt.ylabel(""$f|X,Y$"")\nplt.title(""Posterior GP samples"")\n\nplt.show()\n\n# %% [markdown]\n# ## Example 2: Sparse MC for multiclass classification\n\n# %% [markdown]\n# We now consider the multiclass classification problem (see the [Multiclass classification](../advanced/multiclass_classification.ipynb) notebook). Here the marginal likelihood is not available in closed form. Instead we use a sparse variational approximation where we approximate the posterior for each GP as $q(f_c) \\propto p(f_c|\\mathbf{u}_c)q(\\mathbf{u}_c)$\n#\n# In the standard Sparse Variational GP (SVGP) formulation, $q(\\mathbf{u_c})$ is parameterized as a multivariate Gaussian.\n#\n# An alternative is to directly sample from the optimal $q(\\mathbf{u}_c)$; this is what Sparse Variational GP using MCMC (SGPMC) does.\n\n# %% [markdown]\n# We first build a multiclass classification dataset.\n\n# %%\n# Generate data by sampling from SquaredExponential kernel, and classifying with the argmax\nrng = np.random.RandomState(42)\n\nC, N = 3, 100\nX = rng.rand(N, 1)\nkernel = gpflow.kernels.SquaredExponential(lengthscales=0.1)\nK = kernel.K(X) + np.eye(N) * 1e-6\n\nf = rng.multivariate_normal(mean=np.zeros(N), cov=K, size=(C)).T\nY = np.argmax(f, 1).reshape(-1,).astype(int)\n# One-hot encoding\nY_hot = np.zeros((N, C), dtype=bool)\nY_hot[np.arange(N), Y] = 1\n\ndata = (X, Y)\n\n# %%\nplt.figure(figsize=(12, 6))\norder = np.argsort(X.reshape(-1,))\n\nfor c in range(C):\n    plt.plot(X[order], f[order, c], ""."", color=colors[c], label=str(c))\n    plt.plot(X[order], Y_hot[order, c], ""-"", color=colors[c])\n\nplt.legend()\nplt.xlabel(""$X$"")\nplt.ylabel(""Latent (dots) and one-hot labels (lines)"")\nplt.title(""Sample from the joint $p(Y, \\mathbf{f})$"")\nplt.grid()\nplt.show()\n\n# %% [markdown]\n# We then build the SGPMC model.\n\n# %%\nkernel = gpflow.kernels.Matern32(lengthscales=0.1) + gpflow.kernels.White(variance=0.01)\n\nmodel = gpflow.models.SGPMC(\n    data,\n    kernel=kernel,\n    likelihood=gpflow.likelihoods.MultiClass(3),\n    inducing_variable=X[::5].copy(),\n    num_latent_gps=3,\n)\nmodel.kernel.kernels[0].variance.prior = tfd.Gamma(f64(1.0), f64(1.0))\nmodel.kernel.kernels[0].lengthscales.prior = tfd.Gamma(f64(2.0), f64(2.0))\nset_trainable(model.kernel.kernels[1].variance, False)\n\ngpflow.utilities.print_summary(model)\n\n# %%\n# The inducing point locations Z should not be included in the MCMC (see [Hensman et al. (2015)](https://papers.nips.cc/paper/5875-mcmc-for-variationally-sparse-gaussian-processes), hence we set them to non-trainable.\nset_trainable(model.inducing_variable, False)\n\n# %% [markdown]\n# The chain of samples for $\\mathbf{u}_c, \\theta$ is initialized at the value maximizing  $p(Y|\\mathbf{u}_c, \\theta)$.\n\n# %%\noptimizer = gpflow.optimizers.Scipy()\noptimizer.minimize(model.training_loss, model.trainable_variables, options={""maxiter"": 20})\nprint(f""log posterior density at optimum: {model.log_posterior_density()}"")\n\n# %% [markdown]\n# Sampling starts with a \'burn in\' period.\n\n# %%\nnum_burnin_steps = ci_niter(100)\nnum_samples = ci_niter(500)\n\n# Note that here we need model.trainable_parameters, not trainable_variables - only parameters can have priors!\nhmc_helper = gpflow.optimizers.SamplingHelper(\n    model.log_posterior_density, model.trainable_parameters\n)\n\nhmc = tfp.mcmc.HamiltonianMonteCarlo(\n    target_log_prob_fn=hmc_helper.target_log_prob_fn, num_leapfrog_steps=10, step_size=0.01\n)\n\nadaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n    hmc, num_adaptation_steps=10, target_accept_prob=f64(0.75), adaptation_rate=0.1\n)\n\n\n@tf.function\ndef run_chain_fn():\n    return tfp.mcmc.sample_chain(\n        num_results=num_samples,\n        num_burnin_steps=num_burnin_steps,\n        current_state=hmc_helper.current_state,\n        kernel=adaptive_hmc,\n        trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n    )\n\n\nsamples, _ = run_chain_fn()\nconstrained_samples = hmc_helper.convert_to_constrained_values(samples)\n\n# %% [markdown]\n# Statistics of the posterior samples can now be reported.\n\n# %%\nplot_from_samples(model, X, Y, model.trainable_parameters, constrained_samples, thin=10)\n\n# %% [markdown]\n# You can also display the sequence of sampled hyperparameters.\n\n# %%\nparam_to_name = {param: name for name, param in gpflow.utilities.parameter_dict(model).items()}\nname_to_index = {param_to_name[param]: i for i, param in enumerate(model.trainable_parameters)}\nhyperparameters = ["".kernel.kernels[0].lengthscales"", "".kernel.kernels[0].variance""]\n\nplt.figure(figsize=(8, 4))\nfor param_name in hyperparameters:\n    plt.plot(constrained_samples[name_to_index[param_name]], label=param_name)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.xlabel(""HMC iteration"")\n_ = plt.ylabel(""hyperparameter value"")\n\n\n# %% [markdown]\n# ## Example 3: Fully Bayesian inference for generalized GP models with HMC\n\n# %% [markdown]\n# You can construct very flexible models with Gaussian processes by combining them with different likelihoods (sometimes called \'families\' in the GLM literature). This makes inference of the GP intractable because the likelihoods are not generally conjugate to the Gaussian process. The general form of the model is\n# \\begin{align}\n# \\theta &\\sim p(\\theta) \\\\\n# f &\\sim \\mathcal {GP}(m(x; \\theta),\\, k(x, x\'; \\theta)) \\\\\n# y_i &\\sim p(y | g(f(x_i))\\,.\n# \\end{align}\n#\n#\n# To perform inference in this model, we\'ll run MCMC using Hamiltonian Monte Carlo (HMC) over the function values and the parameters $\\theta$ jointly. The key to an effective scheme is rotation of the field using the Cholesky decomposition. We write:\n#\n# \\begin{align}\n# \\theta &\\sim p(\\theta) \\\\\n# v &\\sim \\mathcal {N}(0,\\, I) \\\\\n# LL^\\top &= K \\\\\n# f &= m + Lv \\\\\n# y_i &\\sim p(y | g(f(x_i))\\,.\n# \\end{align}\n#\n# Joint HMC over $v$ and the function values is not widely adopted in the literature because of the difficulty in differentiating $LL^\\top=K$. We\'ve made this derivative available in TensorFlow, and so application of HMC is relatively straightforward.\n\n# %% [markdown]\n# ### Exponential Regression\n# We consider an exponential regression model:\n# \\begin{align}\n# \\theta &\\sim p(\\theta) \\\\\n# f &\\sim \\mathcal {GP}(0, k(x, x\'; \\theta)) \\\\\n# f_i &= f(x_i) \\\\\n# y_i &\\sim \\mathcal {Exp} (e^{f_i})\n# \\end{align}\n#\n# We\'ll use MCMC to deal with both the kernel parameters $\\theta$ and the latent function values $f$. Firstly, generate a data set.\n\n# %%\nrng = np.random.RandomState(14)\n\nX = np.linspace(-3, 3, 20)\nY = rng.exponential(np.sin(X) ** 2)\n\nplt.figure()\nplt.plot(X, Y, ""x"")\nplt.xlabel(""input $X$"")\nplt.ylabel(""output $Y$"")\nplt.title(""toy dataset"")\nplt.show()\n\ndata = (X[:, None], Y[:, None])\n\n# %% [markdown]\n# GPflow\'s model for fully-Bayesian MCMC is called GPMC. It\'s constructed like any other model, but contains a parameter `V` which represents the centered values of the function.\n\n# %%\nkernel = gpflow.kernels.Matern32() + gpflow.kernels.Constant()\nlikelihood = gpflow.likelihoods.Exponential()\nmodel = gpflow.models.GPMC(data, kernel, likelihood)\n\n# %% [markdown]\n# The `V` parameter already has a prior applied. We\'ll add priors to the parameters also (these are rather arbitrary, for illustration).\n\n# %%\nmodel.kernel.kernels[0].lengthscales.prior = tfd.Gamma(f64(1.0), f64(1.0))\nmodel.kernel.kernels[0].variance.prior = tfd.Gamma(f64(1.0), f64(1.0))\nmodel.kernel.kernels[1].variance.prior = tfd.Gamma(f64(1.0), f64(1.0))\n\ngpflow.utilities.print_summary(model)\n\n\n# %% [markdown]\n# Running HMC is pretty similar to optimizing a model. GPflow builds on top of [tensorflow_probability\'s mcmc module](https://www.tensorflow.org/probability/api_docs/python/tfp/mcmc) and provides a SamplingHelper class to make interfacing easier.\n\n# %% [markdown]\n# We initialize HMC at the maximum a posteriori parameter values of the model.\n\n# %%\noptimizer = gpflow.optimizers.Scipy()\nmaxiter = ci_niter(3000)\n_ = optimizer.minimize(\n    model.training_loss, model.trainable_variables, options=dict(maxiter=maxiter)\n)\n# We can now start HMC near maximum a posteriori (MAP)\n\n# %% [markdown]\n# We then run the sampler,\n\n# %%\nnum_burnin_steps = ci_niter(600)\nnum_samples = ci_niter(1000)\n\n# Note that here we need model.trainable_parameters, not trainable_variables - only parameters can have priors!\nhmc_helper = gpflow.optimizers.SamplingHelper(\n    model.log_posterior_density, model.trainable_parameters\n)\n\nhmc = tfp.mcmc.HamiltonianMonteCarlo(\n    target_log_prob_fn=hmc_helper.target_log_prob_fn, num_leapfrog_steps=10, step_size=0.01\n)\n\nadaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n    hmc, num_adaptation_steps=10, target_accept_prob=f64(0.75), adaptation_rate=0.1\n)\n\n\n@tf.function\ndef run_chain_fn():\n    return tfp.mcmc.sample_chain(\n        num_results=num_samples,\n        num_burnin_steps=num_burnin_steps,\n        current_state=hmc_helper.current_state,\n        kernel=adaptive_hmc,\n        trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n    )\n\n\nsamples, _ = run_chain_fn()\n\n# %% [markdown]\n# And compute the posterior prediction on a grid for plotting purposes.\n\n# %%\nXtest = np.linspace(-4, 4, 100)[:, None]\nf_samples = []\n\nfor i in range(num_samples):\n    # Note that hmc_helper.current_state contains the unconstrained variables\n    for var, var_samples in zip(hmc_helper.current_state, samples):\n        var.assign(var_samples[i])\n    f = model.predict_f_samples(Xtest, 5)\n    f_samples.append(f)\nf_samples = np.vstack(f_samples)\n\n# %%\nrate_samples = np.exp(f_samples[:, :, 0])\n\n(line,) = plt.plot(Xtest, np.mean(rate_samples, 0), lw=2)\nplt.fill_between(\n    Xtest[:, 0],\n    np.percentile(rate_samples, 5, axis=0),\n    np.percentile(rate_samples, 95, axis=0),\n    color=line.get_color(),\n    alpha=0.2,\n)\n\nplt.plot(X, Y, ""kx"", mew=2)\n_ = plt.ylim(-0.1, np.max(np.percentile(rate_samples, 95, axis=0)))\n\n# %% [markdown]\n# You can also display the sequence of sampled hyperparameters.\n\n# %%\nparameter_samples = hmc_helper.convert_to_constrained_values(samples)\nparam_to_name = {param: name for name, param in gpflow.utilities.parameter_dict(model).items()}\nname_to_index = {param_to_name[param]: i for i, param in enumerate(model.trainable_parameters)}\nhyperparameters = [\n    "".kernel.kernels[0].lengthscales"",\n    "".kernel.kernels[0].variance"",\n    "".kernel.kernels[1].variance"",\n]\n\n\nplt.figure(figsize=(8, 4))\nfor param_name in hyperparameters:\n    plt.plot(parameter_samples[name_to_index[param_name]], label=param_name)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.xlabel(""HMC iteration"")\n_ = plt.ylabel(""hyperparameter value"")\n\n\n# %% [markdown]\n# You can also inspect the marginal of the posterior samples.\n\n# %%\nfig, axes = plt.subplots(1, len(hyperparameters), sharex=True, figsize=(12, 4))\nfor ax, param_name in zip(axes, hyperparameters):\n    ax.hist(parameter_samples[name_to_index[param_name]], bins=20)\n    ax.set_title(param_name)\nplt.tight_layout()\n\n# %% [markdown]\n# ## Prior on constrained and unconstrained parameters\n\n# %% [markdown]\n# GPflow\'s `Parameter` class provides options for setting a prior. `Parameter` wraps a constrained tensor and\n# provides computation of the gradient with respect to unconstrained transformation of that tensor.\n# The user can set a prior either in **constrained** space or **unconstrained** space.\n\n# %% [markdown]\n# By default, the prior for the `Parameter` is set on the _constrained_ space.\n# To explicitly set the space on which the prior is defined, use the `prior_on` keyword argument:\n\n# %%\nprior_distribution = tfd.Normal(f64(0.0), f64(1.0))\n_ = gpflow.Parameter(1.0, prior_on=""unconstrained"", prior=prior_distribution)\n_ = gpflow.Parameter(1.0, prior_on=""constrained"", prior=prior_distribution)\n\n# %% [markdown]\n# `gpflow.optimizers.SamplingHelper` makes sure that the prior density correctly reflects the space in which the prior is defined.\n\n# %% [markdown]\n# Below we repeat the same experiment as before, but with some priors defined in the `unconstrained` space.\n# We are using the exponential transform to ensure positivity of the kernel parameters (`set_default_positive_bijector(""exp"")`),\n# so a log-normal prior on a constrained parameter corresponds to a normal prior on the unconstrained space:\n\n# %%\ngpflow.config.set_default_positive_bijector(""exp"")\ngpflow.config.set_default_positive_minimum(1e-6)\n\nrng = np.random.RandomState(42)\ndata = synthetic_data(30, rng)\n\nkernel = gpflow.kernels.Matern52(lengthscales=0.3)\nmeanf = gpflow.mean_functions.Linear(1.0, 0.0)\nmodel = gpflow.models.GPR(data, kernel, meanf)\nmodel.likelihood.variance.assign(0.01)\n\nmu = f64(0.0)\nstd = f64(4.0)\none = f64(1.0)\n\nmodel.kernel.lengthscales.prior_on = ""unconstrained""\nmodel.kernel.lengthscales.prior = tfd.Normal(mu, std)\nmodel.kernel.variance.prior_on = ""unconstrained""\nmodel.kernel.variance.prior = tfd.Normal(mu, std)\nmodel.likelihood.variance.prior_on = ""unconstrained""\nmodel.likelihood.variance.prior = tfd.Normal(mu, std)\n\nmodel.mean_function.A.prior_on = ""constrained""\nmodel.mean_function.A.prior = tfd.Normal(mu, std)\nmodel.mean_function.b.prior_on = ""constrained""\nmodel.mean_function.b.prior = tfd.Normal(mu, std)\n\nmodel.kernel.lengthscales.prior_on\n\n# %% [markdown]\n# Let\'s run HMC and plot chain traces:\n\n# %%\nnum_burnin_steps = ci_niter(300)\nnum_samples = ci_niter(500)\n\nhmc_helper = gpflow.optimizers.SamplingHelper(\n    model.log_posterior_density, model.trainable_parameters\n)\n\nhmc = tfp.mcmc.HamiltonianMonteCarlo(\n    target_log_prob_fn=hmc_helper.target_log_prob_fn, num_leapfrog_steps=10, step_size=0.01\n)\nadaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n    hmc, num_adaptation_steps=10, target_accept_prob=f64(0.75), adaptation_rate=0.1\n)\n\n\n@tf.function\ndef run_chain_fn_unconstrained():\n    return tfp.mcmc.sample_chain(\n        num_results=num_samples,\n        num_burnin_steps=num_burnin_steps,\n        current_state=hmc_helper.current_state,\n        kernel=adaptive_hmc,\n        trace_fn=lambda _, pkr: pkr.inner_results.is_accepted,\n    )\n\n\nsamples, traces = run_chain_fn_unconstrained()\nparameter_samples = hmc_helper.convert_to_constrained_values(samples)\n\nparam_to_name = {param: name for name, param in gpflow.utilities.parameter_dict(model).items()}\nmarginal_samples(samples, model.trainable_parameters, ""unconstrained variable samples"")\nmarginal_samples(parameter_samples, model.trainable_parameters, ""constrained parameter samples"")\n'"
doc/source/notebooks/advanced/multiclass_classification.pct.py,1,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Multiclass classification\n\n# %% [markdown]\n# The multiclass classification problem is a regression problem from an input $x \\in {\\cal X}$ to discrete labels $y\\in {\\cal Y}$, where ${\\cal Y}$ is a discrete set of size $C$ bigger than two (for $C=2$ it is the more usual binary classification).\n#\n# Labels are encoded in a one-hot fashion, that is if $C=4$ and $y=2$, we note $\\bar{y} = [0,1,0,0]$.\n#\n# The generative model for this problem consists of:\n#\n# * $C$ latent functions $\\mathbf{f} = [f_1,...,f_C]$ with an independent Gaussian Process prior\n# * a deterministic function that builds a discrete distribution $\\pi(\\mathbf{f}) = [\\pi_1(f_1),...,\\pi_C(f_C)]$ from the latents such that $\\sum_c \\pi_c(f_c) = 1$\n# * a discrete likelihood $p(y|\\mathbf{f}) = Discrete(y;\\pi(\\mathbf{f})) = \\prod_c \\pi_c(f_c)^{\\bar{y}_c}$\n#\n# A typical example of $\\pi$ is the softmax function:\n#\n# \\begin{equation}\n# \\pi_c (f_c) \\propto \\exp( f_c)\n# \\end{equation}\n#\n# Another convenient one is the robust max:\n# \\begin{equation}\n# \\pi_c(\\mathbf{f}) = \\begin{cases} 1 - \\epsilon, & \\mbox{if } c = \\arg \\max_c f_c \\\\\n#  \\epsilon /(C-1), & \\mbox{ otherwise} \\end{cases}\n# \\end{equation}\n#\n#\n#\n#\n\n# %%\nimport numpy as np\nimport tensorflow as tf\n\nimport warnings\n\nwarnings.filterwarnings(""ignore"")  # ignore DeprecationWarnings from tensorflow\n\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\n\nimport gpflow\n\nfrom gpflow.utilities import print_summary, set_trainable\nfrom gpflow.ci_utils import ci_niter\n\nfrom multiclass_classification import plot_posterior_predictions, colors\n\n# reproducibility:\nnp.random.seed(0)\ntf.random.set_seed(123)\n\n# %% [markdown]\n# ## Sampling from the GP multiclass generative model\n\n# %% [markdown]\n# ### Declaring model parameters and input\n\n# %%\n# Number of functions and number of data points\nC = 3\nN = 100\n\n# Lengthscale of the SquaredExponential kernel (isotropic -- change to `[0.1] * C` for ARD)\nlengthscales = 0.1\n\n# Jitter\njitter_eye = np.eye(N) * 1e-6\n\n# Input\nX = np.random.rand(N, 1)\n\n# %% [markdown]\n# ### Sampling\n\n# %%\n# SquaredExponential kernel matrix\nkernel_se = gpflow.kernels.SquaredExponential(lengthscales=lengthscales)\nK = kernel_se(X) + jitter_eye\n\n# Latents prior sample\nf = np.random.multivariate_normal(mean=np.zeros(N), cov=K, size=(C)).T\n\n# Hard max observation\nY = np.argmax(f, 1).reshape(-1,).astype(int)\n\n# One-hot encoding\nY_hot = np.zeros((N, C), dtype=bool)\nY_hot[np.arange(N), Y] = 1\n\ndata = (X, Y)\n\n# %% [markdown]\n# ### Plotting\n\n# %%\nplt.figure(figsize=(12, 6))\norder = np.argsort(X.reshape(-1,))\n\nfor c in range(C):\n    plt.plot(X[order], f[order, c], ""."", color=colors[c], label=str(c))\n    plt.plot(X[order], Y_hot[order, c], ""-"", color=colors[c])\n\n\nplt.legend()\nplt.xlabel(""$X$"")\nplt.ylabel(""Latent (dots) and one-hot labels (lines)"")\nplt.title(""Sample from the joint $p(Y, \\mathbf{f})$"")\nplt.grid()\nplt.show()\n\n# %% [markdown]\n# ## Inference\n#\n\n# %% [markdown]\n# Inference here consists of computing the posterior distribution over the latent functions given the data $p(\\mathbf{f}|Y, X)$.\n#\n# You can use different inference methods. Here we perform variational inference.\n# For a treatment of the multiclass classification problem using MCMC sampling, see [Markov Chain Monte Carlo (MCMC)](../advanced/mcmc.ipynb).\n#\n#\n\n# %% [markdown]\n# ### Approximate inference: Sparse Variational Gaussian Process\n\n# %% [markdown]\n# #### Declaring the SVGP model (see [GPs for big data](../advanced/gps_for_big_data.ipynb))\n\n# %%\n# sum kernel: Matern32 + White\nkernel = gpflow.kernels.Matern32() + gpflow.kernels.White(variance=0.01)\n\n# Robustmax Multiclass Likelihood\ninvlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\nlikelihood = gpflow.likelihoods.MultiClass(3, invlink=invlink)  # Multiclass likelihood\nZ = X[::5].copy()  # inducing inputs\n\nm = gpflow.models.SVGP(\n    kernel=kernel,\n    likelihood=likelihood,\n    inducing_variable=Z,\n    num_latent_gps=C,\n    whiten=True,\n    q_diag=True,\n)\n\n# Only train the variational parameters\nset_trainable(m.kernel.kernels[1].variance, False)\nset_trainable(m.inducing_variable, False)\nprint_summary(m, fmt=""notebook"")\n\n# %% [markdown]\n# #### Running inference\n\n# %%\nopt = gpflow.optimizers.Scipy()\n\nopt_logs = opt.minimize(\n    m.training_loss_closure(data), m.trainable_variables, options=dict(maxiter=ci_niter(1000))\n)\nprint_summary(m, fmt=""notebook"")\n\n# %%\nplot_posterior_predictions(m, X, Y)\n'"
doc/source/notebooks/advanced/multiclass_classification.py,0,"b'# Function to plot the predictions of the trained\n# multi-class classification model in multiclass_classification.ipynb\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ncolors = [""#1f77b4"", ""#ff7f0e"", ""#2ca02c""]\n\n\ndef plot_posterior_predictions(m, X, Y):\n\n    f = plt.figure(figsize=(12, 6))\n    a1 = f.add_axes([0.05, 0.05, 0.9, 0.6])\n    a2 = f.add_axes([0.05, 0.7, 0.9, 0.1])\n    a3 = f.add_axes([0.05, 0.85, 0.9, 0.1])\n\n    xx = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n    mu, var = m.predict_f(xx)\n    p, _ = m.predict_y(xx)\n\n    a3.set_xticks([])\n    a3.set_yticks([])\n\n    for c in range(m.likelihood.num_classes):\n        x = X[Y.flatten() == c]\n\n        color = colors[c]\n        a3.plot(x, x * 0, ""."", color=color)\n        a1.plot(xx, mu[:, c], color=color, lw=2, label=""%d"" % c)\n        a1.plot(xx, mu[:, c] + 2 * np.sqrt(var[:, c]), ""--"", color=color)\n        a1.plot(xx, mu[:, c] - 2 * np.sqrt(var[:, c]), ""--"", color=color)\n        a2.plot(xx, p[:, c], ""-"", color=color, lw=2)\n\n    a2.set_ylim(-0.1, 1.1)\n    a2.set_yticks([0, 1])\n    a2.set_xticks([])\n\n    a3.set_title(""inputs X"")\n    a2.set_title(\n        ""predicted mean label value \\\n                 $\\mathbb{E}_{q(\\mathbf{u})}[y^*|x^*, Z, \\mathbf{u}]$""\n    )\n    a1.set_title(\n        ""posterior process \\\n                $\\int d\\mathbf{u} q(\\mathbf{u})p(f^*|\\mathbf{u}, Z, x^*)$""\n    )\n\n    handles, labels = a1.get_legend_handles_labels()\n    a1.legend(handles, labels)\n    f.tight_layout()\n    plt.show()\n\n\ndef plot_from_samples(m, X, Y, parameters, parameter_samples, thin):\n\n    f = plt.figure(figsize=(12, 6))\n    a1 = f.add_axes([0.05, 0.05, 0.9, 0.6])\n    a2 = f.add_axes([0.05, 0.7, 0.9, 0.1])\n    a3 = f.add_axes([0.05, 0.85, 0.9, 0.1])\n\n    xx = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n\n    Fpred, Ypred = [], []\n    num_samples = len(parameter_samples[0])\n    for i in range(0, num_samples, thin):\n        for parameter, var_samples in zip(parameters, parameter_samples):\n            parameter.assign(var_samples[i])\n        Ypred.append(m.predict_y(xx)[0])\n        Fpred.append(np.squeeze(m.predict_f_samples(xx, 1)))\n\n    for i in range(m.likelihood.num_classes):\n        x = X[Y.reshape(-1) == i]\n        (points,) = a3.plot(x, x * 0, ""."")\n        color = points.get_color()\n        for F in Fpred:\n            a1.plot(xx, F[:, i], color=color, lw=0.2, alpha=1.0)\n        for Yp in Ypred:\n            a2.plot(xx, Yp[:, i], color=color, lw=0.5, alpha=1.0)\n\n    a2.set_ylim(-0.1, 1.1)\n    a2.set_yticks([0, 1])\n    a2.set_xticks([])\n\n    a3.set_xticks([])\n    a3.set_yticks([])\n\n    a3.set_title(""inputs X"")\n    a2.set_title(\n        ""predicted mean label value \\\n                 $\\mathbb{E}_{q(\\mathbf{u})}[y^*|x^*, Z, \\mathbf{u}]$""\n    )\n    a1.set_title(\n        ""posterior process samples \\\n                $\\int d\\mathbf{u} q(\\mathbf{u})p(f^*|\\mathbf{u}, Z, x^*)$""\n    )\n'"
doc/source/notebooks/advanced/multioutput.pct.py,0,"b'# -*- coding: utf-8 -*-\n# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.4.1\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Multi-output Gaussian processes in GPflow\n\n# %% [markdown]\n# This notebook shows how to construct a multi-output GP model using GPflow, together with different interdomain inducing variables which lead to different approximation properties. GPflow provides a framework for specifying multioutput GP priors, and interdomain approximations which is\n# - modular, by providing a consistent interface for the user of the resulting `SVGP` model,\n# - extensible, by allowing new interdomain variables and kernels to be specified while reusing exising code where possible,\n# - efficient, by allowing the most efficient custom code path to be specified where desired.\n#\n# Getting to grips with the maths and code can be a bit daunting, so to accompany the documentation there is an [in-depth review on arXiv](https://arxiv.org/abs/2003.01115), which provides a unified mathematical framework, together with a high-level description of software design choices in GPflow.\n#\n# This notebook shows the various design choices that can be made, to show the reader the flexibility of the framework. This is done in the hope that an example is provided that can be easily adapted to the special case that the reader wants to implement.\n#\n# A reader who just wants to use a multioutput kernel should simply choose the most efficient set of inducing variables.\n#\n# To cite this framework, please reference our [arXiv paper](https://arxiv.org/abs/2003.01115).\n# ```\n# @article{GPflow2020multioutput,\n#   author = {{van der Wilk}, Mark and Dutordoir, Vincent and John, ST and\n#             Artemev, Artem and Adam, Vincent and Hensman, James},\n#   title = {A Framework for Interdomain and Multioutput {G}aussian Processes},\n#   year = {2020},\n#   journal = {arXiv:2003.01115},\n#   url = {https://arxiv.org/abs/2003.01115}\n# }\n# ```\n#\n# \\begin{equation}\n# \\newcommand{\\GP}{\\mathcal{GP}}\n# \\newcommand{\\NN}{\\mathcal{N}}\n# \\newcommand{\\LL}{\\mathcal{L}}\n# \\newcommand{\\RR}{\\mathbb{R}}\n# \\newcommand{\\EE}{\\mathbb{E}}\n# \\newcommand{\\valpha}{\\boldsymbol\\alpha}\n# \\newcommand{\\vf}{\\mathbf{f}}\n# \\newcommand{\\vF}{\\mathbf{F}}\n# \\newcommand{\\vg}{\\mathbf{g}}\n# \\newcommand{\\vW}{\\mathbf{W}}\n# \\newcommand{\\vI}{\\mathbf{I}}\n# \\newcommand{\\vZ}{\\mathbf{Z}}\n# \\newcommand{\\vu}{\\mathbf{u}}\n# \\newcommand{\\vU}{\\mathbf{U}}\n# \\newcommand{\\vX}{\\mathbf{X}}\n# \\newcommand{\\vY}{\\mathbf{Y}}\n# \\newcommand{\\identity}{\\mathbb{I}}\n# \\end{equation}\n#\n#\n#\n# ## Task\n# We will consider a regression problem for functions $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^P$. We assume that the dataset is of the form $(X, f_1), \\dots, (X, f_P)$, that is, we observe all the outputs for a particular input location (for cases where there are **not** fully observed outputs for each input, see [A simple demonstration of coregionalization](./coregionalisation.ipynb)).\n#\n# Here we assume a model of the form:\n# \\begin{equation}\n# f(x) = W g(x),\n# \\end{equation}\n# where $g(x) \\in \\mathbb{R}^L$, $f(x) \\in \\mathbb{R}^P$ and $W \\in \\mathbb{R}^{P \\times L}$. We assume that the outputs of $g$ are uncorrelated, and that by *mixing* them with $W$ they become correlated. In this notebook, we show how to build this model using Sparse Variational Gaussian Process (SVGP) for $g$, which scales well with the numbers of data points and outputs.\n#\n# Here we have two options for $g$:\n# 1. The output dimensions of $g$ share the same kernel.\n# 1. Each output of $g$ has a separate kernel.\n#\n#\n# In addition, we have two further suboptions for the inducing inputs of $g$:\n# 1. The instances of $g$ share the same inducing inputs.\n# 1. Each output of $g$ has its own set of inducing inputs.\n#\n# The notation is as follows:\n# - $X \\in \\mathbb{R}^{N \\times D}$ denotes the input\n# - $Y \\in \\RR^{N \\times P}$ denotes the output\n# - $k_{1..L}$, $L$ are kernels on $\\RR^{N \\times D}$\n# - $g_{1..L}$, $L$ are independent $\\GP$s  with $g_l \\sim \\GP(0,k_l)$\n# - $f_{1..P}$, $P$ are correlated  $\\GP$s  with $\\vf = \\vW \\vg$\n\n# %%\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gpflow as gpf\nimport tensorflow as tf\n\nfrom gpflow.utilities import print_summary\nfrom gpflow.ci_utils import ci_niter\n\ngpf.config.set_default_float(np.float64)\ngpf.config.set_default_summary_fmt(""notebook"")\nnp.random.seed(0)\n# %matplotlib inline\n\nMAXITER = ci_niter(2000)\n\n# %% [markdown]\n# ## Generate synthetic data\n# We create a utility function to generate synthetic data. We assume that:\n\n# %%\nN = 100  # number of points\nD = 1  # number of input dimensions\nM = 15  # number of inducing points\nL = 2  # number of latent GPs\nP = 3  # number of observations = output dimensions\n\n\n# %%\ndef generate_data(N=100):\n    X = np.random.rand(N)[:, None] * 10 - 5  # Inputs = N x D\n    G = np.hstack((0.5 * np.sin(3 * X) + X, 3.0 * np.cos(X) - X))  # G = N x L\n    W = np.array([[0.5, -0.3, 1.5], [-0.4, 0.43, 0.0]])  # L x P\n    F = np.matmul(G, W)  # N x P\n    Y = F + np.random.randn(*F.shape) * [0.2, 0.2, 0.2]\n\n    return X, Y\n\n\n# %%\nX, Y = data = generate_data(N)\nZinit = np.linspace(-5, 5, M)[:, None]\n\n\n# %% [markdown]\n# We create a utility function for plotting:\n\n# %%\ndef plot_model(m, lower=-8.0, upper=8.0):\n    pX = np.linspace(lower, upper, 100)[:, None]\n    pY, pYv = m.predict_y(pX)\n    if pY.ndim == 3:\n        pY = pY[:, 0, :]\n    plt.plot(X, Y, ""x"")\n    plt.gca().set_prop_cycle(None)\n    plt.plot(pX, pY)\n    for i in range(pY.shape[1]):\n        top = pY[:, i] + 2.0 * pYv[:, i] ** 0.5\n        bot = pY[:, i] - 2.0 * pYv[:, i] ** 0.5\n        plt.fill_between(pX[:, 0], top, bot, alpha=0.3)\n    plt.xlabel(""X"")\n    plt.ylabel(""f"")\n    plt.title(f""ELBO: {m.elbo(data):.3}"")\n    plt.plot(Z, Z * 0.0, ""o"")\n\n\n# %% [markdown]\n# ## Model the outputs of $f(x)$ directly\n# The three following examples show how to model the outputs of the model $f(x)$ directly. Mathematically, this case is equivalent to having:\n# \\begin{equation}\n# f(x) = I g(x),\n# \\end{equation}\n# i.e. $W = I$ and $P = L$.\n\n# %% [markdown]\n# ### 1. Shared independent multi-output kernel (MOK) and shared independent inducing variables\n# Here the priors on all outputs are constrained to have the same kernel hyperparameters. We also share the inducing inputs between all outputs. The different GPs are independent both in the prior and the approximate posterior.\n\n# %%\n# create multi-output kernel\nkernel = gpf.kernels.SharedIndependent(\n    gpf.kernels.SquaredExponential() + gpf.kernels.Linear(), output_dim=P\n)\n# initialization of inducing input locations (M random points from the training inputs)\nZ = Zinit.copy()\n# create multi-output inducing variables from Z\niv = gpf.inducing_variables.SharedIndependentInducingVariables(\n    gpf.inducing_variables.InducingPoints(Z)\n)\n\n# %%\n# create SVGP model as usual and optimize\nm = gpf.models.SVGP(kernel, gpf.likelihoods.Gaussian(), inducing_variable=iv, num_latent_gps=P)\nprint_summary(m)\n\n\n# %%\ndef optimize_model_with_scipy(model):\n    optimizer = gpf.optimizers.Scipy()\n    optimizer.minimize(\n        model.training_loss_closure(data),\n        variables=model.trainable_variables,\n        method=""l-bfgs-b"",\n        options={""disp"": True, ""maxiter"": MAXITER},\n    )\n\n\noptimize_model_with_scipy(m)\n\n# %%\nprint_summary(m)\n\n# %%\n# Plot predictions and observations\nplot_model(m)\n\n# %%\nprint_summary(m.kernel)\nm.kernel.kernel.kernels[0].lengthscales\n\n# %% [markdown]\n# ### 2. Separate independent MOK and shared independent inducing variables\n# Here we allow different hyperparameters for the priors of each output. We still share the inducing inputs between all outputs.\n\n# %%\n# Create list of kernels for each output\nkern_list = [gpf.kernels.SquaredExponential() + gpf.kernels.Linear() for _ in range(P)]\n# Create multi-output kernel from kernel list\nkernel = gpf.kernels.SeparateIndependent(kern_list)\n# initialization of inducing input locations (M random points from the training inputs)\nZ = Zinit.copy()\n# create multi-output inducing variables from Z\niv = gpf.inducing_variables.SharedIndependentInducingVariables(\n    gpf.inducing_variables.InducingPoints(Z)\n)\n\n# %%\n# create SVGP model as usual and optimize\nm = gpf.models.SVGP(kernel, gpf.likelihoods.Gaussian(), inducing_variable=iv, num_latent_gps=P)\n\n# %%\noptimize_model_with_scipy(m)\n\n# %%\nprint_summary(m.kernel)\n\n# %%\nplot_model(m)\n\n# %%\n[k.kernels[0].lengthscales for k in m.kernel.kernels]\n\n# %% [markdown]\n# ### 3. Separate independent kernel and separate independent inducing variables\n# Here we allow different hyperparameters for the priors of each output. We now allow different inducing inputs for each output.\n\n# %%\n# Create list of kernels for each output\nkern_list = [gpf.kernels.SquaredExponential() + gpf.kernels.Linear() for _ in range(P)]\n# Create multi-output kernel from kernel list\nkernel = gpf.kernels.SeparateIndependent(kern_list)\n# initialization of inducing input locations, one set of locations per output\nZs = [Zinit.copy() for _ in range(P)]\n# initialize as list inducing inducing variables\niv_list = [gpf.inducing_variables.InducingPoints(Z) for Z in Zs]\n# create multi-output inducing variables from iv_list\niv = gpf.inducing_variables.SeparateIndependentInducingVariables(iv_list)\n\n# %% [markdown]\n# **NOTE:** While the inducing points are independent, there needs to be the same number of inducing points per dimension.\n\n# %%\n# create SVGP model as usual and optimize\nm = gpf.models.SVGP(kernel, gpf.likelihoods.Gaussian(), inducing_variable=iv, num_latent_gps=P)\n\n# %%\noptimize_model_with_scipy(m)\n\n# %%\nplot_model(m)\n\n# %% [markdown]\n# The following plot shows that we use different inducing *inputs* in each output dimension.\n\n# %%\nfor i in range(len(m.inducing_variable.inducing_variable_list)):\n    q_mu_unwhitened, q_var_unwhitened = m.predict_f(m.inducing_variable.inducing_variable_list[i].Z)\n    plt.plot(\n        m.inducing_variable.inducing_variable_list[i].Z.numpy(),\n        q_mu_unwhitened[:, i, None].numpy(),\n        ""o"",\n    )\nplt.gca().set_xticks(np.linspace(-6, 6, 20), minor=True)\nplt.gca().set_yticks(np.linspace(-9, 9, 20), minor=True)\nplt.grid(which=""minor"")\n\n# %%\nm.inducing_variable.inducing_variable_list\n\n# %% [markdown]\n# ## Model $f(x)$ by doing inference in the $g$ space\n# ### Mixed kernel and uncorrelated inducing variables\n#\n# Remember the general case: $f(x) = W g(x)$, where $g(x) \\in \\mathbb{R}^L$, $f(x) \\in \\mathbb{R}^P$ and $W \\in \\mathbb{R}^{P \\times L}$, where $L \\leq P$.\n# We assume that the outputs of $g$ are uncorrelated, and by *mixing* them with $W$ they become correlated.\n# With this setup we perform the optimal routine to calculate the conditional. Namely, calculate the conditional of the uncorrelated latent $g$ and afterwards project the mean and variance using the mixing matrix: $\\mu_f = W \\mu_g$ and $\\Sigma_f = W\\Sigma_g W^\\top$\n#\n# - $K_{uu} = L \\times M \\times M$\n# - $K_{uf} = L \\times M \\times N$\n\n# %%\n# Create list of kernels for each output\nkern_list = [gpf.kernels.SquaredExponential() + gpf.kernels.Linear() for _ in range(L)]\n# Create multi-output kernel from kernel list\nkernel = gpf.kernels.LinearCoregionalization(\n    kern_list, W=np.random.randn(P, L)\n)  # Notice that we initialise the mixing matrix W\n# initialisation of inducing input locations (M random points from the training inputs)\nZ = Zinit.copy()\n# create multi-output inducing variables from Z\niv = gpf.inducing_variables.SharedIndependentInducingVariables(\n    gpf.inducing_variables.InducingPoints(Z)\n)\n\n# %%\n# initialize mean of variational posterior to be of shape MxL\nq_mu = np.zeros((M, L))\n# initialize \\sqrt(\xce\xa3) of variational posterior to be of shape LxMxM\nq_sqrt = np.repeat(np.eye(M)[None, ...], L, axis=0) * 1.0\n\n# create SVGP model as usual and optimize\nm = gpf.models.SVGP(\n    kernel, gpf.likelihoods.Gaussian(), inducing_variable=iv, q_mu=q_mu, q_sqrt=q_sqrt\n)\n\n# %%\noptimize_model_with_scipy(m)\n\n# %%\nplot_model(m)\n\n\n# %% [markdown]\n# ## Illustration of GPflow\'s multi-output capabilities\n# This section shows the inheritance structure in GPflow\'s multi-output framework.\n\n# %% [markdown]\n# ### Multi-output kernels (MOK) class diagram\n# We include three multi-output kernels:\n# - `SharedIndependent`: This kernel is included mainly as an illustration of specifying a conditional using the multiple dispatch framework. The same functionality is provided by using a normal kernel and passing in multiple approximate posteriors by stacking `q_mu` and `q_sqrt`.\n# - `SeparateIndependent`: This kernel allows you to use different priors for each output GP.\n# - `LinearCoregionalization`: This kernel describes the prior of the linear model of coregionalization. As shown previously, this implementation supports various inducing point approximations.\n# ![Multi-output kernels](./multioutput_kernels.svg)\n#\n# We include several base classes. Two are noteworthy:\n# - `MultioutputKernel` is included to be the base class for all multi-output kernels.\n# - `IndepedentLatent` is the base class for all multi-output kernels which are constructed from independent latent processes. Including this kernel allows the specification of a default approximation method which, while not the most efficient, does take advantage of _some_ structure. It can be applied to _any_ kernel constructed from independent latent processes.\n#\n# There is a similarity in the meaning of `SeparateIndependent` and `IndependentLatent`. Both kernels indicate that independent processes are used, and that $\\mathbf{K}_{\\bf uu}$ can therefore be represented as a `[L, M, M]` tensor. It could therefore be suggested that `SeparateIndependent` be the parent class of all ""independent latent"" kernels, instead of having a separate `IndependentLatent` class. We decided against this because:\n# - this would increase the complexity in specifying `conditionals()` for the otherwise simple multi-output kernels `SeparateIndependent` and `SharedIndependent`.\n# - we did not want to specify too much of an implementation in `IndependentLatent`, leaving implementation details to child classes. Using `SeparateIndependent` as the base class would force all child classes to be a `Combination` kernel.\n\n# %% [markdown]\n# ### Multi-output inducing variables class diagram\n# ![Multi-output features](./multioutput_features.svg)\n#\n# #### Inducing points\n# The goal of this class is to provide inducing variables that can be used with _any_ kernel, even if the method ends up being slow.\n#\n# The multiouput framework extends `InducingPoints` to work with multi-output kernels. Just like for single-output kernels, we want `InducingPoints` to work for all `MultioutputKernel`s. We do this by defining `InducingPoints` to take _all_ outputs for specific inducing inputs as inducing variables.\n#\n# #### Fallback shared/separate independent inducing variables\n# The goal of these classes is to provide a reasonably efficient implementation for kernels that give exploitable independence structure in the prior of inducing variables (that is, subclasses of `IndependentLatent`), while only needing to implement `Kuu()` and `Kuf()` methods.\n#\n# #### Shared/separate independent inducing variables\n# The goal of these classes is to provide the most efficient code path for kernels that allow exploiting independence structure in the prior of inducing variables.\n#\n# For more specialized multi-output kernels (i.e. `{Shared|Separate}Independent` or `LinearCoregionalization`) we define `{Shared|Separate}IndependentInducingVariables`. These wrap (a list of) single-output inducing variables to define groups of a-priori independent inducing variables, which leads to a $\\mathbf{K}_{\\bf uu}$ that can be represented as a `[L, M, M]` tensor. We saw the use of these previously.\n#\n# `{Shared|Separate}IndependentInducingVariables` inherit from `Fallback{Shared|Separate}IndependentInducingVariables`, so the multiple dispatch will fall back on the slower but general implementation.\n\n# %% [markdown]\n# ### Implemented combinations\n# Multiple dispatch is applied to both `Kuu()`, `Kuf()`, and `conditional()`. The return values of the covariances can therefore be tailored to a specific implementation of `conditional()`. The following table lists combinations which are currently available in GPflow. Thanks to the multiple dispatch code, implementing your own outside of GPflow should require only a small amount of code!\n#\n# | Inducing variable class                                      | Kernel                  | Kuu           | Kuf           | conditional                         | note                                                                                                                                                                                                                                                                                           |\n# |----------------------------------------------|-------------------------|---------------|---------------|-------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n# | `InducingPoints`                               | `MultioutputKernel`       | `[M, P, M, P]` | `[M, P, N, P]` | `inducing_point_conditional()`, which calls `fully_correlated_conditional()`       | Works for all kernels, but might be very inefficient. In this case `q_mu` and `q_sqrt` should have shapes of `[1, MP]` and `[1, MP, MP]`  |\n# | `SharedIndependentInducingVariables`           | `SharedIndependent`       | `[M, M]`         | `[M, N]`         | `shared_independent_conditional()`, which calls `base_conditional()`                    | The combination of these two classes is in a sense redundant, because we can achieve the same behavior by using the single output Kernel and InducingVariable classes. They are added for illustrative purposes. Thanks to the conditional dispatch, the most efficient code path is used. |\n# | `SeparateIndependentInducingVariables`         | `SharedIndependent`       | `[P, M, M]`     | `[P, M, N]`     | `separate_independent_conditional()`, which calls `base_conditional()` P times               | We loop P times over the `base_conditional()`         |\n# | `SeparateIndependentInducingVariable`         | `SeparateIndependent`     | `[P, M, M]`     | `[P, M, N]`     |  `separate_independent_conditional()`, which calls `base_conditional()` P times                | We loop P times over the `base_conditional()`           |\n# | `SharedIndependentInducingVariables`           | `SeparateIndependent`     | `[P, M, M]`     | `[P, M, N]`     | `separate_independent_conditional()`, which calls `base_conditional()` P times                | We loop P times over the `base_conditional()`        |\n# | `FallbackSharedIndependentInducingVariables`   | `IndependentLatent`       | `[L, M, M]`     | `[M, L, N, P]` | `fallback_independent_latent_conditional()`, which calls `independent_interdomain_conditional()` | Implementation which only requires custom `Kuu()` and `Kuf()` |\n# | `FallbackSeparateIndependentInducingVariable` | `IndependentLatent`       | `[L, M, M]`     | `[M, L, N, P]` | `fallback_independent_latent_conditional()`, which calls `independent_interdomain_conditional()` | Implementation which only requires custom `Kuu()` and `Kuf()`  |\n# | `SharedIndependentInducingVariables`           | `LinearCoregionalization` | `[L, M, M]`     | `[L, M, N]`     | `coregionalization_conditional()`, which calls `base_conditional()`                    | This is the most efficient implementation for linear coregionalization. The inducing outputs live in g-space. Here we use the output of the base conditional and project the mean and covariance with the mixing matrix W.                                                                    |\n# | `SeparateIndependentInducingVariables`         | `LinearCoregionalization` | `[L, M, M]`     | `[L, M, N]`     | base_conditional                    | This is the most efficient implementation for linear coregionalization. The inducing outputs live in g-space. Here we use the output of the base conditional and project the mean and covariance with the mixing matrix W.                                                                    |\n\n# %% [markdown]\n# ## Debugging: introspect\n#\n# Given all these possibilities it can be hard to determine which conditional will be called for which set of kernel and inducing variable. The following method lets you proactively introspect which implementation will be executed. This can be useful when debugging new code.\n\n# %%\ndef inspect_conditional(inducing_variable_type, kernel_type):\n    """"""\n    Helper function returning the exact implementation called\n    by the multiple dispatch `conditional` given the type of\n    kernel and inducing variable.\n\n    :param inducing_variable_type:\n        Type of the inducing variable\n    :param kernel_type:\n        Type of the kernel\n\n    :return: String\n        Contains the name, the file and the linenumber of the\n        implementation.\n    """"""\n    import inspect\n    from gpflow.conditionals import conditional\n\n    implementation = conditional.dispatch(object, inducing_variable_type, kernel_type, object)\n    info = dict(inspect.getmembers(implementation))\n    return info[""__code__""]\n\n\n# Example:\ninspect_conditional(\n    gpf.inducing_variables.SharedIndependentInducingVariables, gpf.kernels.SharedIndependent\n)\n\n# %% [markdown]\n# ## Further Reading:\n# - [A simple demonstration of coregionalization](./coregionalisation.ipynb), which details other GPflow features for multi-output prediction without fully observed outputs.\n'"
doc/source/notebooks/advanced/natural_gradients.pct.py,14,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Natural gradients\n#\n# This notebook shows some basic usage of the natural gradient optimizer, both on its own and in combination with Adam optimizer.\n\n# %%\nimport warnings\nimport numpy as np\nimport gpflow\nimport tensorflow as tf\n\nfrom gpflow.ci_utils import ci_niter, ci_range\nfrom gpflow.models import VGP, GPR, SGPR, SVGP\nfrom gpflow.optimizers import NaturalGradient\nfrom gpflow.optimizers.natgrad import XiSqrtMeanVar\nfrom gpflow import set_trainable\n\n# %matplotlib inline\n# %precision 4\n\nnp.random.seed(0)\ntf.random.set_seed(0)\n\nN, D = 100, 2\nbatch_size = 50\n\n# inducing points\nM = 10\n\nx = np.random.uniform(size=(N, D))\ny = np.sin(10 * x[:, :1]) + 5 * x[:, 1:] ** 2\n\ndata = (x, y)\ninducing_variable = tf.random.uniform((M, D))\nadam_learning_rate = 0.01\niterations = ci_niter(5)\nautotune = tf.data.experimental.AUTOTUNE\n\n# %% [markdown]\n# ### VGP is a GPR\n\n# %% [markdown]\n# The following section demonstrates how natural gradients can turn VGP into GPR *in a single step, if the likelihood is Gaussian*.\n\n# %% [markdown]\n# Let\'s start by first creating a standard GPR model with Gaussian likelihood:\n\n# %%\ngpr = GPR(data, kernel=gpflow.kernels.Matern52())\n\n# %% [markdown]\n# The log marginal likelihood of the exact GP model is:\n\n# %%\ngpr.log_marginal_likelihood().numpy()\n\n# %% [markdown]\n# Now we will create an approximate model which approximates the true posterior via a variational Gaussian distribution.<br>We initialize the distribution to be zero mean and unit variance.\n\n# %%\nvgp = VGP(data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Gaussian())\n# (Note that GPflow\'s NaturalGradient optimizer does not implement diagonal covariance parametrization, i.e., it does not work for `q_diag=True`.)\n\n# %% [markdown]\n# The log marginal likelihood lower bound (evidence lower bound or ELBO) of the approximate GP model is:\n\n# %%\nvgp.elbo().numpy()\n\n# %% [markdown]\n# Obviously, our initial guess for the variational distribution is not correct, which results in a lower bound to the likelihood of the exact GPR model. We can optimize the variational parameters in order to get a tighter bound.\n\n# %% [markdown]\n# In fact, we only need to take **one step** in the natural gradient direction to recover the exact posterior:\n\n# %%\nnatgrad_opt = NaturalGradient(gamma=1.0)\nvariational_params = [(vgp.q_mu, vgp.q_sqrt)]\nnatgrad_opt.minimize(vgp.training_loss, var_list=variational_params)\n\n# %% [markdown]\n# The ELBO of the approximate GP model after a single NatGrad step:\n\n# %%\nvgp.elbo().numpy()\n\n# %% [markdown]\n# ### Optimize both variational parameters and kernel hyperparameters together\n#\n# In the Gaussian likelihood case we can iterate between an Adam update for the hyperparameters and a NatGrad update for the variational parameters. That way, we achieve optimization of hyperparameters as if the model were a GPR.\n\n# %% [markdown]\n# The trick is to forbid Adam from updating the variational parameters by setting them to not trainable.\n\n# %%\n# Stop Adam from optimizing the variational parameters\nset_trainable(vgp.q_mu, False)\nset_trainable(vgp.q_sqrt, False)\n\nadam_opt_for_vgp = tf.optimizers.Adam(adam_learning_rate)\nadam_opt_for_gpr = tf.optimizers.Adam(adam_learning_rate)\n\n# %%\nfor i in range(iterations):\n    adam_opt_for_gpr.minimize(gpr.training_loss, var_list=gpr.trainable_variables)\n    likelihood = gpr.log_marginal_likelihood()\n    tf.print(f""GPR with Adam: iteration {i + 1} likelihood {likelihood:.04f}"")\n\n# %%\nfor i in range(iterations):\n    natgrad_opt.minimize(vgp.training_loss, var_list=variational_params)\n    adam_opt_for_vgp.minimize(vgp.training_loss, var_list=vgp.trainable_variables)\n    likelihood = vgp.elbo()\n    tf.print(f""VGP with NaturalGradient and Adam: iteration {i + 1} likelihood {likelihood:.04f}"")\n\n# %% [markdown]\n# Compare GPR and VGP lengthscales after optimization:\n\n# %%\nprint(f""GPR lengthscales = {gpr.kernel.lengthscales.numpy():.04f}"")\nprint(f""VGP lengthscales = {vgp.kernel.lengthscales.numpy():.04f}"")\n\n# %% [markdown]\n# ### Natural gradients also work for the sparse model\n# Similarly, natural gradients turn SVGP into SGPR in the Gaussian likelihood case. <br>\n# We can again combine natural gradients with Adam to update both variational parameters and hyperparameters too.<br>\n# Here we\'ll just do a single natural step demonstration.\n\n# %%\nsvgp = SVGP(\n    kernel=gpflow.kernels.Matern52(),\n    likelihood=gpflow.likelihoods.Gaussian(),\n    inducing_variable=inducing_variable,\n)\nsgpr = SGPR(data, kernel=gpflow.kernels.Matern52(), inducing_variable=inducing_variable)\n\nfor model in svgp, sgpr:\n    model.likelihood.variance.assign(0.1)\n\n# %% [markdown]\n# Analytically optimal sparse model ELBO:\n\n# %%\nsgpr.elbo().numpy()\n\n# %% [markdown]\n# SVGP ELBO before natural gradient step:\n\n# %%\nsvgp.elbo(data).numpy()\n\n# %%\nvariational_params = [(svgp.q_mu, svgp.q_sqrt)]\n\nnatgrad_opt = NaturalGradient(gamma=1.0)\nnatgrad_opt.minimize(svgp.training_loss_closure(data), var_list=variational_params)\n\n# %% [markdown]\n# SVGP ELBO after a single natural gradient step:\n\n# %%\nsvgp.elbo(data).numpy()\n\n# %% [markdown]\n# ### Minibatches\n# A crucial property of the natural gradient method is that it still works with minibatches.\n# In practice though, we need to use a smaller gamma.\n\n# %%\nnatgrad_opt = NaturalGradient(gamma=0.1)\n\ndata_minibatch = (\n    tf.data.Dataset.from_tensor_slices(data)\n    .prefetch(autotune)\n    .repeat()\n    .shuffle(N)\n    .batch(batch_size)\n)\ndata_minibatch_it = iter(data_minibatch)\n\n\nsvgp_objective = svgp.training_loss_closure(data_minibatch_it)\nfor _ in range(ci_niter(100)):\n    natgrad_opt.minimize(svgp_objective, var_list=variational_params)\n\n# %% [markdown]\n# Minibatch SVGP ELBO after NatGrad optimization:\n\n# %%\nnp.average([svgp.elbo(next(data_minibatch_it)) for _ in ci_range(100)])\n\n# %% [markdown]\n# ### Comparison with ordinary gradients in the conjugate case\n#\n# ##### (Take home message: natural gradients are always better)\n#\n# Compared to SVGP with ordinary gradients with minibatches, the natural gradient optimizer is much faster in the Gaussian case.\n#\n# Here we\'ll do hyperparameter learning together with optimization of the variational parameters, comparing the interleaved natural gradient approach and the one using ordinary gradients for the hyperparameters and variational parameters jointly.\n#\n# **NOTE:** Again we need to compromise for smaller gamma value, which we\'ll keep *fixed* during the optimization.\n\n# %%\nsvgp_ordinary = SVGP(\n    kernel=gpflow.kernels.Matern52(),\n    likelihood=gpflow.likelihoods.Gaussian(),\n    inducing_variable=inducing_variable,\n)\nsvgp_natgrad = SVGP(\n    kernel=gpflow.kernels.Matern52(),\n    likelihood=gpflow.likelihoods.Gaussian(),\n    inducing_variable=inducing_variable,\n)\n\n# ordinary gradients with Adam for SVGP\nordinary_adam_opt = tf.optimizers.Adam(adam_learning_rate)\n\n# NatGrads and Adam for SVGP\n# Stop Adam from optimizing the variational parameters\nset_trainable(svgp_natgrad.q_mu, False)\nset_trainable(svgp_natgrad.q_sqrt, False)\n\n# Create the optimize_tensors for SVGP\nnatgrad_adam_opt = tf.optimizers.Adam(adam_learning_rate)\n\nnatgrad_opt = NaturalGradient(gamma=0.1)\nvariational_params = [(svgp_natgrad.q_mu, svgp_natgrad.q_sqrt)]\n\n# %% [markdown]\n# Let\'s optimize the models:\n\n# %%\ndata_minibatch = (\n    tf.data.Dataset.from_tensor_slices(data)\n    .prefetch(autotune)\n    .repeat()\n    .shuffle(N)\n    .batch(batch_size)\n)\ndata_minibatch_it = iter(data_minibatch)\n\n\nsvgp_ordinary_loss = svgp_ordinary.training_loss_closure(data_minibatch_it)\nsvgp_natgrad_loss = svgp_natgrad.training_loss_closure(data_minibatch_it)\n\n\nfor _ in range(ci_niter(100)):\n    ordinary_adam_opt.minimize(svgp_ordinary_loss, var_list=svgp_ordinary.trainable_variables)\n\n\nfor _ in range(ci_niter(100)):\n    natgrad_adam_opt.minimize(svgp_natgrad_loss, var_list=svgp_natgrad.trainable_variables)\n    natgrad_opt.minimize(svgp_natgrad_loss, var_list=variational_params)\n\n# %% [markdown]\n# SVGP ELBO after ordinary `Adam` optimization:\n\n# %%\nnp.average([svgp_ordinary.elbo(next(data_minibatch_it)) for _ in ci_range(100)])\n\n# %% [markdown]\n# SVGP ELBO after `NaturalGradient` and `Adam` optimization:\n\n# %%\nnp.average([svgp_natgrad.elbo(next(data_minibatch_it)) for _ in ci_range(100)])\n\n# %% [markdown]\n# ### Comparison with ordinary gradients in the non-conjugate case\n# #### Binary classification\n#\n# ##### (Take home message: natural gradients are usually better)\n#\n# We can use natural gradients even when the likelihood isn\'t Gaussian. It isn\'t guaranteed to be better, but it usually is better in practical situations.\n\n# %%\ny_binary = np.random.choice([1.0, -1], size=x.shape)\nvgp_data = (x, y_binary)\n\nvgp_bernoulli = VGP(\n    vgp_data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Bernoulli()\n)\nvgp_bernoulli_natgrad = VGP(\n    vgp_data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Bernoulli()\n)\n\n# ordinary gradients with Adam for VGP with Bernoulli likelihood\nadam_opt = tf.optimizers.Adam(adam_learning_rate)\n\n# NatGrads and Adam for VGP with Bernoulli likelihood\n# Stop Adam from optimizing the variational parameters\nset_trainable(vgp_bernoulli_natgrad.q_mu, False)\nset_trainable(vgp_bernoulli_natgrad.q_sqrt, False)\n\n# Create the optimize_tensors for VGP with natural gradients\nnatgrad_adam_opt = tf.optimizers.Adam(adam_learning_rate)\nnatgrad_opt = NaturalGradient(gamma=0.1)\nvariational_params = [(vgp_bernoulli_natgrad.q_mu, vgp_bernoulli_natgrad.q_sqrt)]\n\n# %%\n# Optimize vgp_bernoulli\nfor _ in range(ci_niter(100)):\n    adam_opt.minimize(vgp_bernoulli.training_loss, var_list=vgp_bernoulli.trainable_variables)\n\n# Optimize vgp_bernoulli_natgrad\nfor _ in range(ci_niter(100)):\n    adam_opt.minimize(\n        vgp_bernoulli_natgrad.training_loss, var_list=vgp_bernoulli_natgrad.trainable_variables\n    )\n    natgrad_opt.minimize(vgp_bernoulli_natgrad.training_loss, var_list=variational_params)\n\n# %% [markdown]\n# VGP ELBO after ordinary `Adam` optimization:\n\n# %%\nvgp_bernoulli.elbo().numpy()\n\n# %% [markdown]\n# VGP ELBO after `NaturalGradient` + `Adam` optimization:\n\n# %%\nvgp_bernoulli_natgrad.elbo().numpy()\n\n# %% [markdown]\n# We can also choose to run natural gradients in another parameterization.<br>\n# The sensible choice is the model parameters (q_mu, q_sqrt), which is already in GPflow.\n\n# %%\nvgp_bernoulli_natgrads_xi = VGP(\n    vgp_data, kernel=gpflow.kernels.Matern52(), likelihood=gpflow.likelihoods.Bernoulli()\n)\n\n# Stop Adam from optimizing the variational parameters\nset_trainable(vgp_bernoulli_natgrads_xi.q_mu, False)\nset_trainable(vgp_bernoulli_natgrads_xi.q_sqrt, False)\n\n# Create the optimize_tensors for VGP with Bernoulli likelihood\nadam_opt = tf.optimizers.Adam(adam_learning_rate)\nnatgrad_opt = NaturalGradient(gamma=0.01)\n\nvariational_params = [\n    (vgp_bernoulli_natgrads_xi.q_mu, vgp_bernoulli_natgrads_xi.q_sqrt, XiSqrtMeanVar())\n]\n\n# %%\n# Optimize vgp_bernoulli_natgrads_xi\nfor _ in range(ci_niter(100)):\n    adam_opt.minimize(\n        vgp_bernoulli_natgrads_xi.training_loss,\n        var_list=vgp_bernoulli_natgrads_xi.trainable_variables,\n    )\n\n    natgrad_opt.minimize(vgp_bernoulli_natgrads_xi.training_loss, var_list=variational_params)\n\n# %% [markdown]\n# VGP ELBO after `NaturalGradient` with `XiSqrtMeanVar` + `Adam` optimization:\n\n# %%\nvgp_bernoulli_natgrads_xi.elbo().numpy()\n\n# %% [markdown]\n# With sufficiently small steps, it shouldn\'t make a difference which transform is used, but for large\n# steps this can make a difference in practice.\n'"
doc/source/notebooks/advanced/optimisation.pct.py,0,"b""# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# Optimizers\n# --\n#\n# see svi_test 3/3.\n"""
doc/source/notebooks/advanced/ordinal_regression.pct.py,0,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# Ordinal regression\n# --\n# Ordinal regression aims to fit a model to some data $(X, Y)$, where $Y$ is an ordinal variable. To do so, we use a `VPG` model with a specific likelihood (`gpflow.likelihoods.Ordinal`).\n\n# %%\nimport gpflow\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\nplt.rcParams[""figure.figsize""] = (12, 6)\n\nnp.random.seed(123)  # for reproducibility\n\n\n# %%\n# make a one-dimensional ordinal regression problem\n\n# This function generates a set of inputs X,\n# quantitative output f (latent) and ordinal values Y\n\n\ndef generate_data(num_data):\n    # First generate random inputs\n    X = np.random.rand(num_data, 1)\n\n    # Now generate values of a latent GP\n    kern = gpflow.kernels.SquaredExponential(lengthscales=0.1)\n    K = kern(X)\n    f = np.random.multivariate_normal(mean=np.zeros(num_data), cov=K).reshape(-1, 1)\n\n    # Finally convert f values into ordinal values Y\n    Y = np.round((f + f.min()) * 3)\n    Y = Y - Y.min()\n    Y = np.asarray(Y, np.float64)\n\n    return X, f, Y\n\n\nnp.random.seed(1)\nnum_data = 20\nX, f, Y = generate_data(num_data)\n\nplt.figure(figsize=(11, 6))\nplt.plot(X, f, ""."")\nplt.ylabel(""latent function value"")\n\nplt.twinx()\nplt.plot(X, Y, ""kx"", mew=1.5)\nplt.ylabel(""observed data value"")\n\n# %%\n# construct ordinal likelihood - bin_edges is the same as unique(Y) but centered\nbin_edges = np.array(np.arange(np.unique(Y).size + 1), dtype=float)\nbin_edges = bin_edges - bin_edges.mean()\nlikelihood = gpflow.likelihoods.Ordinal(bin_edges)\n\n# build a model with this likelihood\nm = gpflow.models.VGP(data=(X, Y), kernel=gpflow.kernels.Matern32(), likelihood=likelihood)\n\n# fit the model\nopt = gpflow.optimizers.Scipy()\nopt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=100))\n\n# %%\n# here we\'ll plot the expected value of Y +- 2 std deviations, as if the distribution were Gaussian\nplt.figure(figsize=(11, 6))\nX_data, Y_data = (m.data[0].numpy(), m.data[1].numpy())\nXtest = np.linspace(X_data.min(), X_data.max(), 100).reshape(-1, 1)\nmu, var = m.predict_y(Xtest)\n(line,) = plt.plot(Xtest, mu, lw=2)\ncol = line.get_color()\nplt.plot(Xtest, mu + 2 * np.sqrt(var), ""--"", lw=2, color=col)\nplt.plot(Xtest, mu - 2 * np.sqrt(var), ""--"", lw=2, color=col)\nplt.plot(X_data, Y_data, ""kx"", mew=2)\n\n\n# %%\n## to see the predictive density, try predicting every possible discrete value for Y.\ndef pred_log_density(m):\n    Xtest = np.linspace(X_data.min(), X_data.max(), 100).reshape(-1, 1)\n    ys = np.arange(Y_data.max() + 1)\n    densities = []\n    for y in ys:\n        Ytest = np.full_like(Xtest, y)\n        # Predict the log density\n        densities.append(m.predict_log_density((Xtest, Ytest)))\n    return np.vstack(densities)\n\n\n# %%\nfig = plt.figure(figsize=(14, 6))\nplt.imshow(\n    np.exp(pred_log_density(m)),\n    interpolation=""nearest"",\n    extent=[X_data.min(), X_data.max(), -0.5, Y_data.max() + 0.5],\n    origin=""lower"",\n    aspect=""auto"",\n    cmap=plt.cm.viridis,\n)\nplt.colorbar()\nplt.plot(X, Y, ""kx"", mew=2, scalex=False, scaley=False)\n\n# %%\n# Predictive density for a single input x=0.5\nx_new = 0.5\nY_new = np.arange(np.max(Y_data + 1)).reshape([-1, 1])\nX_new = np.full_like(Y_new, x_new)\n# for predict_log_density x and y need to have the same number of rows\ndens_new = np.exp(m.predict_log_density((X_new, Y_new)))\nfig = plt.figure(figsize=(8, 4))\nplt.bar(x=Y_new.flatten(), height=dens_new.flatten())\n'"
doc/source/notebooks/advanced/variational_fourier_features.pct.py,65,"b'# -*- coding: utf-8 -*-\n# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Variational Fourier Features in the GPflow framework\n#\n# In this notebook we demonstrate how new types of inducing variables can easily be incorporated in the GPflow framework. As an example case, we use the variational Fourier features from [Hensman, Durrande, and Solin (JMLR 2018)](http://jmlr.csail.mit.edu/papers/v18/16-579). All equation and table references are to this paper.\n#\n# **Note:** we cannot yet use Fourier features within the multi-output framework, as `Kuu` and `Kuf` for SharedIndependent and SeparateIndependent inducing variables assume that the sub-inducing variable\'s covariances are simply computed as dense Tensors. Moreover, the `conditional` is not able to make use of the structure in `Kuu` and `Kuf` as it has to dispatch on the *arguments* to `Kuu` and `Kuf` instead...\n\n# %%\nimport tensorflow as tf\nimport numpy as np\nimport gpflow\nfrom gpflow.inducing_variables import InducingVariables\nfrom gpflow.base import TensorLike\nfrom gpflow.utilities import to_default_float\nfrom gpflow import covariances as cov\nfrom gpflow import kullback_leiblers as kl\nfrom gpflow.ci_utils import ci_niter\n\n# %%\n# VFF give structured covariance matrices that are computationally efficient.\n# We take advantage of this using TensorFlow\'s LinearOperators:\nBlockDiag = tf.linalg.LinearOperatorBlockDiag\nDiag = tf.linalg.LinearOperatorDiag\nLowRank = tf.linalg.LinearOperatorLowRankUpdate\n\n# %%\nimport matplotlib.pyplot as plt\n\n# %matplotlib notebook\n\n# %% [markdown]\n# The VFF inducing variables are defined as a projection $u_m = \\mathcal{P}_{\\phi_m}(f)$ (eq. (59)) of the GP $f(\\cdot)$ onto a truncated Fourier basis, $\\phi_m = [1, \\cos(\\omega_1(x-a)),\\dots,\\cos(\\omega_M(x-a)),\\sin(\\omega_1(x-a)),\\dots,\\sin(\\omega_M(x-a))]$ (eq. (47)). To represent this we define a new inducing variables class that derives from the `InducingVariables` base class.\n\n# %%\nclass FourierFeatures1D(InducingVariables):\n    def __init__(self, a, b, M):\n        # [a, b] defining the interval of the Fourier representation:\n        self.a = gpflow.Parameter(a, dtype=gpflow.default_float())\n        self.b = gpflow.Parameter(b, dtype=gpflow.default_float())\n        # integer array defining the frequencies, \xcf\x89_m = 2\xcf\x80 (b - a)/m:\n        self.ms = np.arange(M)\n\n    def __len__(self):\n        """""" number of inducing variables (defines dimensionality of q(u)) """"""\n        return 2 * len(self.ms) - 1  # M cosine and M-1 sine components\n\n\n# %% [markdown]\n# Next, we need to define how to compute $\\mathrm{K}_\\mathbf{uu} = \\operatorname{cov}(u_m, u_{m\'})$ (eq. (61)) and $\\mathrm{K}_\\mathbf{uf} = \\operatorname{cov}(u_m, f(x_n))$ (eq. (60)).\n\n# %%\n@cov.Kuu.register(FourierFeatures1D, gpflow.kernels.Matern12)\ndef Kuu_matern12_fourierfeatures1d(inducing_variable, kernel, jitter=None):\n    a, b, ms = (lambda u: (u.a, u.b, u.ms))(inducing_variable)\n    omegas = 2.0 * np.pi * ms / (b - a)\n\n    # Cosine block:\n    lamb = 1.0 / kernel.lengthscales\n    two_or_four = to_default_float(tf.where(omegas == 0, 2.0, 4.0))\n    d_cos = (\n        (b - a) * (tf.square(lamb) + tf.square(omegas)) / lamb / kernel.variance / two_or_four\n    )  # eq. (111)\n    v_cos = tf.ones_like(d_cos) / tf.sqrt(kernel.variance)  # eq. (110)\n    cosine_block = LowRank(Diag(d_cos), v_cos[:, None])\n\n    # Sine block:\n    omegas = omegas[tf.not_equal(omegas, 0)]  # the sine block does not include omega=0\n    d_sin = (\n        (b - a) * (tf.square(lamb) + tf.square(omegas)) / lamb / kernel.variance / 4.0\n    )  # eq. (113)\n    sine_block = Diag(d_sin)\n\n    return BlockDiag([cosine_block, sine_block])\n\n\n@cov.Kuf.register(FourierFeatures1D, gpflow.kernels.Matern12, TensorLike)\ndef Kuf_matern12_fourierfeatures1d(inducing_variable, kernel, X):\n    X = tf.squeeze(X, axis=1)\n    a, b, ms = (lambda u: (u.a, u.b, u.ms))(inducing_variable)\n\n    omegas = 2.0 * np.pi * ms / (b - a)\n    Kuf_cos = tf.cos(omegas[:, None] * (X[None, :] - a))\n    omegas_sin = omegas[tf.not_equal(omegas, 0)]  # don\'t compute zero frequency\n    Kuf_sin = tf.sin(omegas_sin[:, None] * (X[None, :] - a))\n\n    # correct Kuf outside [a, b] -- see Table 1\n    Kuf_sin = tf.where((X < a) | (X > b), tf.zeros_like(Kuf_sin), Kuf_sin)  # just zero\n\n    left_tail = tf.exp(-tf.abs(X - a) / kernel.lengthscales)[None, :]\n    right_tail = tf.exp(-tf.abs(X - b) / kernel.lengthscales)[None, :]\n    Kuf_cos = tf.where(X < a, left_tail, Kuf_cos)  # replace with left tail\n    Kuf_cos = tf.where(X > b, right_tail, Kuf_cos)  # replace with right tail\n\n    return tf.concat([Kuf_cos, Kuf_sin], axis=0)\n\n\n@cov.Kuu.register(FourierFeatures1D, gpflow.kernels.Matern32)\ndef Kuu_matern32_fourierfeatures1d(inducing_variable, kernel, jitter=None):\n    a, b, ms = (lambda u: (u.a, u.b, u.ms))(inducing_variable)\n    omegas = 2.0 * np.pi * ms / (b - a)\n\n    # Cosine block: eq. (114)\n    lamb = np.sqrt(3.0) / kernel.lengthscales\n    four_or_eight = to_default_float(tf.where(omegas == 0, 4.0, 8.0))\n    d_cos = (\n        (b - a)\n        * tf.square(tf.square(lamb) + tf.square(omegas))\n        / tf.pow(lamb, 3)\n        / kernel.variance\n        / four_or_eight\n    )\n    v_cos = tf.ones_like(d_cos) / tf.sqrt(kernel.variance)\n    cosine_block = LowRank(Diag(d_cos), v_cos[:, None])\n\n    # Sine block: eq. (115)\n    omegas = omegas[tf.not_equal(omegas, 0)]  # don\'t compute omega=0\n    d_sin = (\n        (b - a)\n        * tf.square(tf.square(lamb) + tf.square(omegas))\n        / tf.pow(lamb, 3)\n        / kernel.variance\n        / 8.0\n    )\n    v_sin = omegas / lamb / tf.sqrt(kernel.variance)\n    sine_block = LowRank(Diag(d_sin), v_sin[:, None])\n\n    return BlockDiag([cosine_block, sine_block])  # eq. (116)\n\n\n@cov.Kuf.register(FourierFeatures1D, gpflow.kernels.Matern32, TensorLike)\ndef Kuf_matern32_fourierfeatures1d(inducing_variable, kernel, X):\n    X = tf.squeeze(X, axis=1)\n    a, b, ms = (lambda u: (u.a, u.b, u.ms))(inducing_variable)\n    omegas = 2.0 * np.pi * ms / (b - a)\n\n    Kuf_cos = tf.cos(omegas[:, None] * (X[None, :] - a))\n    omegas_sin = omegas[tf.not_equal(omegas, 0)]  # don\'t compute zeros freq.\n    Kuf_sin = tf.sin(omegas_sin[:, None] * (X[None, :] - a))\n\n    # correct Kuf outside [a, b] -- see Table 1\n\n    def tail_cos(delta_X):\n        arg = np.sqrt(3) * tf.abs(delta_X) / kernel.lengthscales\n        return (1 + arg) * tf.exp(-arg)[None, :]\n\n    Kuf_cos = tf.where(X < a, tail_cos(X - a), Kuf_cos)\n    Kuf_cos = tf.where(X > b, tail_cos(X - b), Kuf_cos)\n\n    def tail_sin(delta_X):\n        arg = np.sqrt(3) * tf.abs(delta_X) / kernel.lengthscales\n        return delta_X[None, :] * tf.exp(-arg) * omegas_sin[:, None]\n\n    Kuf_sin = tf.where(X < a, tail_sin(X - a), Kuf_sin)\n    Kuf_sin = tf.where(X > b, tail_sin(X - b), Kuf_sin)\n\n    return tf.concat([Kuf_cos, Kuf_sin], axis=0)\n\n\n# %% [markdown]\n# In principle, this is all we need; however, to be able to take advantage of the structure of `Kuu`, we need to also implement new versions of the KL divergence from the prior to the approximate posterior (`prior_kl`) and the `conditional` computation:\n\n# %%\n@kl.prior_kl.register(FourierFeatures1D, gpflow.kernels.Kernel, TensorLike, TensorLike)\ndef prior_kl_vff(inducing_variable, kernel, q_mu, q_sqrt, whiten=False):\n    if whiten:\n        raise NotImplementedError\n    K = cov.Kuu(inducing_variable, kernel)\n    return gauss_kl_vff(q_mu, q_sqrt, K)\n\n\ndef gauss_kl_vff(q_mu, q_sqrt, K):\n    """"""\n    Compute the KL divergence from\n\n          q(x) = N(q_mu, q_sqrt^2)\n    to\n          p(x) = N(0, K)\n\n    q_mu is a vector [N, 1] that contains the mean.\n    q_sqrt is a matrix that is the lower triangular square-root matrix of the covariance of q.\n\n    K is a positive definite matrix: the covariance of p.\n    NOTE: K is a LinearOperator that provides efficient methjods\n        for solve(), log_abs_determinant(), and trace()\n    """"""\n    # KL(N\xe2\x82\x80 || N\xe2\x82\x81) = \xc2\xbd [tr(\xce\xa3\xe2\x82\x81\xe2\x81\xbb\xc2\xb9 \xce\xa3\xe2\x82\x80) + (\xce\xbc\xe2\x82\x81 - \xce\xbc\xe2\x82\x80)\xe1\xb5\x80 \xce\xa3\xe2\x82\x81\xe2\x81\xbb\xc2\xb9 (\xce\xbc\xe2\x82\x81 - \xce\xbc\xe2\x82\x80) - k + ln(det(\xce\xa3\xe2\x82\x81)/det(\xce\xa3\xe2\x82\x80))]\n    # N\xe2\x82\x80 = q; \xce\xbc\xe2\x82\x80 = q_mu, \xce\xa3\xe2\x82\x80 = q_sqrt q_sqrt\xe1\xb5\x80\n    # N\xe2\x82\x81 = p; \xce\xbc\xe2\x82\x81 = 0, \xce\xa3\xe2\x82\x81 = K\n    # KL(q || p) =\n    #     \xc2\xbd [tr(K\xe2\x81\xbb\xc2\xb9 q_sqrt q_sqrt\xe1\xb5\x80A + q_mu\xe1\xb5\x80 K\xe2\x81\xbb\xc2\xb9 q_mu - k + logdet(K) - logdet(q_sqrt q_sqrt\xe1\xb5\x80)]\n    # k = number of dimensions, if q_sqrt is m x m this is m\xc2\xb2\n    Kinv_q_mu = K.solve(q_mu)\n\n    mahalanobis_term = tf.squeeze(tf.matmul(q_mu, Kinv_q_mu, transpose_a=True))\n\n    # GPflow: q_sqrt is num_latent_gps x N x N\n    num_latent_gps = to_default_float(tf.shape(q_mu)[1])\n    logdet_prior = num_latent_gps * K.log_abs_determinant()\n\n    product_of_dimensions__int = tf.reduce_prod(tf.shape(q_sqrt)[:-1])  # dimensions are integers\n    constant_term = to_default_float(product_of_dimensions__int)\n\n    Lq = tf.linalg.band_part(q_sqrt, -1, 0)  # force lower triangle\n    logdet_q = tf.reduce_sum(tf.math.log(tf.square(tf.linalg.diag_part(Lq))))\n\n    # S = tf.matmul(q_sqrt, q_sqrt, transpose_b=True)\n    # trace_term = tf.trace(K.solve(S))\n    trace_term = tf.squeeze(\n        tf.reduce_sum(Lq * K.solve(Lq), axis=[-1, -2])\n    )  # [O(N\xc2\xb2) instead of O(N\xc2\xb3)\n\n    twoKL = trace_term + mahalanobis_term - constant_term + logdet_prior - logdet_q\n    return 0.5 * twoKL\n\n\n# %%\n@gpflow.conditionals.conditional.register(\n    TensorLike, FourierFeatures1D, gpflow.kernels.Kernel, TensorLike\n)\ndef conditional_vff(\n    Xnew,\n    inducing_variable,\n    kernel,\n    f,\n    *,\n    full_cov=False,\n    full_output_cov=False,\n    q_sqrt=None,\n    white=False,\n):\n    """"""\n     - Xnew are the points of the data or minibatch, size N x D (tf.array, 2d)\n     - feat is an instance of features.InducingFeature that provides `Kuu` and `Kuf` methods\n       for Fourier features, this contains the limits of the bounding box and the frequencies\n     - f is the value (or mean value) of the features (i.e. the weights)\n     - q_sqrt (default None) is the Cholesky factor of the uncertainty about f\n       (to be propagated through the conditional as per the GPflow inducing-point implementation)\n     - white (defaults False) specifies whether the whitening has been applied\n\n    Given the GP represented by the inducing points specified in `feat`, produce the mean and\n    (co-)variance of the GP at the points Xnew.\n\n       Xnew :: N x D\n       Kuu :: M x M\n       Kuf :: M x N\n       f :: M x K, K = 1\n       q_sqrt :: K x M x M, with K = 1\n    """"""\n    if full_output_cov:\n        raise NotImplementedError\n\n    # num_data = tf.shape(Xnew)[0]  # M\n    num_func = tf.shape(f)[1]  # K\n\n    Kuu = cov.Kuu(inducing_variable, kernel)  # this is now a LinearOperator\n    Kuf = cov.Kuf(inducing_variable, kernel, Xnew)  # still a Tensor\n\n    KuuInv_Kuf = Kuu.solve(Kuf)\n\n    # compute the covariance due to the conditioning\n    if full_cov:\n        fvar = kernel(Xnew) - tf.matmul(Kuf, KuuInv_Kuf, transpose_a=True)\n        shape = (num_func, 1, 1)\n    else:\n        KufT_KuuInv_Kuf_diag = tf.reduce_sum(Kuf * KuuInv_Kuf, axis=-2)\n        fvar = kernel(Xnew, full_cov=False) - KufT_KuuInv_Kuf_diag\n        shape = (num_func, 1)\n    fvar = tf.expand_dims(fvar, 0) * tf.ones(\n        shape, dtype=gpflow.default_float()\n    )  # K x N x N or K x N\n\n    # another backsubstitution in the unwhitened case\n    if white:\n        raise NotImplementedError\n\n    A = KuuInv_Kuf\n\n    # construct the conditional mean\n    fmean = tf.matmul(A, f, transpose_a=True)\n\n    if q_sqrt is not None:\n        if q_sqrt.get_shape().ndims == 2:\n            # LTA = A * tf.expand_dims(q_sqrt, 2)  # K x M x N\n            # won\'t work  # make ticket for this?\n            raise NotImplementedError\n        elif q_sqrt.get_shape().ndims == 3:\n            # L = tf.matrix_band_part(tf.transpose(q_sqrt, (2, 0, 1)), -1, 0)  # K x M x M\n\n            # K x M x N\n            # A_tiled = tf.expand_dims(A.get(), 0) * tf.ones((num_func, 1, 1), dtype=float_type)\n\n            # LTA = tf.matmul(L, A_tiled, transpose_a=True)  # K x M x N\n            # TODO the following won\'t work for K > 1\n            assert q_sqrt.shape[0] == 1\n            # LTA = (A.T @ DenseMatrix(q_sqrt[:,:,0])).T.get()[None, :, :]\n            ATL = tf.matmul(A, q_sqrt, transpose_a=True)\n        else:\n            raise ValueError(""Bad dimension for q_sqrt: %s"" % str(q_sqrt.get_shape().ndims))\n        if full_cov:\n            # fvar = fvar + tf.matmul(LTA, LTA, transpose_a=True)  # K x N x N\n            fvar = fvar + tf.matmul(ATL, ATL, transpose_b=True)  # K x N x N\n        else:\n            # fvar = fvar + tf.reduce_sum(tf.square(LTA), 1)  # K x N\n            fvar = fvar + tf.reduce_sum(tf.square(ATL), 2)  # K x N\n    fvar = tf.transpose(fvar)  # N x K or N x N x K\n\n    return fmean, fvar\n\n\n# %% [markdown]\n# We now demonstrate how to use these new types of inducing variables with the `SVGP` model class. First, let\'s create some toy data:\n\n# %%\nX = np.linspace(-2, 2, 510)\nXnew = np.linspace(-4, 4, 501)\n\n\ndef f(x):\n    return np.cos(2 * np.pi * x / 4 * 2)\n\n\nF = f(X)\nFnew = f(Xnew)\nnoise_scale = 0.1\nnp.random.seed(1)\nY = F + np.random.randn(*F.shape) * noise_scale\n\ndata = (X.reshape(-1, 1), Y.reshape(-1, 1))\n\n# %%\nplt.figure()\nplt.plot(X, F, label=""f(x)"")\nplt.plot(X, Y, ""."", label=""observations"")\nplt.legend()\nplt.show()\n\n# %% [markdown]\n# Setting up an SVGP model with variational Fourier feature inducing variables is as simple as replacing the `inducing_variable` argument:\n\n# %%\nMfreq = 9\nm = gpflow.models.SVGP(\n    kernel=gpflow.kernels.Matern32(),\n    likelihood=gpflow.likelihoods.Gaussian(variance=noise_scale ** 2),\n    inducing_variable=FourierFeatures1D(-4.5, 4.5, Mfreq),\n    num_data=len(X),\n    whiten=False,\n)\ngpflow.set_trainable(m.kernel, False)\ngpflow.set_trainable(m.likelihood, False)\ngpflow.set_trainable(m.inducing_variable, True)  # whether to optimize bounds [a, b]\n\n# %%\nopt = gpflow.optimizers.Scipy()\nopt.minimize(\n    m.training_loss_closure(data), m.trainable_variables, options=dict(maxiter=ci_niter(5000)),\n)\n\ngpflow.utilities.print_summary(m, fmt=""notebook"")\n\n# %% [markdown]\n# For comparison we also construct an SVGP model using inducing points and an exact GPR model:\n\n# %%\nm_ip = gpflow.models.SVGP(\n    kernel=gpflow.kernels.Matern32(),\n    likelihood=gpflow.likelihoods.Gaussian(variance=noise_scale ** 2),\n    inducing_variable=np.linspace(-2, 2, Mfreq * 2 - 1)[:, None],\n    num_data=len(X),\n    whiten=False,\n)\ngpflow.set_trainable(m_ip.kernel, False)\ngpflow.set_trainable(m_ip.likelihood, False)\ngpflow.set_trainable(m_ip.inducing_variable, True)  # whether to optimize inducing point locations\n\n# %%\nopt = gpflow.optimizers.Scipy()\nopt.minimize(\n    m_ip.training_loss_closure(data),\n    m_ip.trainable_variables,\n    options=dict(maxiter=ci_niter(5000)),\n)\n\ngpflow.utilities.print_summary(m_ip, fmt=""notebook"")\n\n# %%\nm_ref = gpflow.models.GPR((X.reshape(-1, 1), Y.reshape(-1, 1)), kernel=gpflow.kernels.Matern32())\nm_ref.likelihood.variance = np.array(noise_scale ** 2).astype(np.float64)\ngpflow.set_trainable(m_ref.kernel, False)\ngpflow.set_trainable(m_ref.likelihood, False)\n\n# Because we fixed the kernel and likelihood hyperparameters, we don\'t need to optimize anything.\n\ngpflow.utilities.print_summary(m_ref, fmt=""notebook"")\n\n# %%\nexact_gpr_lml = m_ref.log_marginal_likelihood().numpy().item()\nprint(""LML (exact GPR) ="", exact_gpr_lml)\nip_svgp_elbo = m_ip.elbo(data).numpy().item()\nprint(""ELBO (SVGP, inducing points) ="", ip_svgp_elbo)\nvff_svgp_elbo = m.elbo(data).numpy().item()\nprint(""ELBO (SVGP, Fourier features) ="", vff_svgp_elbo)\n\n# %%\ndef plot_gp(m, Xnew, name=""""):\n    Fmean, Fvar = m.predict_f(Xnew[:, None])\n    Fmean = Fmean.numpy().squeeze()\n    Fvar = Fvar.numpy().squeeze()\n    (p,) = plt.plot(Xnew, Fmean, label=name)\n    plt.fill_between(\n        Xnew, Fmean - 2 * np.sqrt(Fvar), Fmean + 2 * np.sqrt(Fvar), alpha=0.3, color=p.get_color()\n    )\n\n\ndef plot_data():\n    plt.plot(Xnew, Fnew, label=""f(x)"")\n    plt.plot(X, Y, ""."", label=""observations"")\n\n\nplt.figure(figsize=(15, 10))\nplot_data()\nplot_gp(m, Xnew, ""VFF [ELBO={:.3}]"".format(vff_svgp_elbo))\nplot_gp(m_ip, Xnew, ""inducing points [ELBO={:.3}]"".format(ip_svgp_elbo))\nplot_gp(m_ref, Xnew, ""exact [LML={:.3}]"".format(exact_gpr_lml))\nplt.legend(loc=""best"")\nplt.show()\n'"
doc/source/notebooks/advanced/varying_noise.pct.py,3,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Gaussian process regression with varying output noise\n\n# %% [markdown]\n# This notebook shows how to construct a Gaussian process model where different noise is assumed for different data points. The model is:\n#\n# \\begin{align}\n# f(\\cdot) &\\sim \\mathcal{GP}\\big(0, k(\\cdot, \\cdot)\\big) \\\\\n# y_i | f, x_i &\\sim \\mathcal N\\big(y_i; f(x_i), \\sigma^2_i\\big)\n# \\end{align}\n#\n# We\'ll demonstrate two methods. In the first demonstration, we\'ll assume that the noise variance is known for every data point. We\'ll incorporate the known noise variances $\\sigma^2_i$ into the data matrix $\\mathbf Y$, make a likelihood that can deal with this structure, and implement inference using variational GPs with natural gradients.\n#\n# In the second demonstration, we\'ll assume that the noise variance is not known, but we\'d like to estimate it for different groups of data. We\'ll show how to construct an appropriate likelihood for this task and set up inference similarly to the first demonstration, with optimization over the noise variances.\n#\n\n# %%\nimport numpy as np\nimport tensorflow as tf\nimport gpflow\nfrom gpflow.ci_utils import ci_niter\nfrom gpflow.optimizers import NaturalGradient\nfrom gpflow import set_trainable\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\n\n# %% [markdown]\n# ## Demo 1: known noise variances\n# ### Generate synthetic data\n# We create a utility function to generate synthetic data, including noise that varies amongst the data:\n\n# %%\nnp.random.seed(1)  # for reproducibility\n\n\ndef generate_data(N=80):\n    X = np.random.rand(N)[:, None] * 10 - 5  # Inputs, shape N x 1\n    F = 2.5 * np.sin(6 * X) + np.cos(3 * X)  # Mean function values\n    NoiseVar = 2 * np.exp(-((X - 2) ** 2) / 4) + 0.3  # Noise variances\n    Y = F + np.random.randn(N, 1) * np.sqrt(NoiseVar)  # Noisy data\n    return X, Y, NoiseVar\n\n\nX, Y, NoiseVar = generate_data()\n\n# %% [markdown]\n# Here\'s a plot of the data, with error bars representing two standard deviations:\n\n# %%\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\n_ = ax.errorbar(\n    X.squeeze(),\n    Y.squeeze(),\n    yerr=2 * (np.sqrt(NoiseVar)).squeeze(),\n    marker=""x"",\n    lw=0,\n    elinewidth=1.0,\n    color=""C1"",\n)\n\n# %% [markdown]\n# ### Make a Y matrix that includes the variances\n# We need to tell the GP model what the variance is for each data point. To do this, we\'ll concatenate the observations with the variances into a single data matrix:\n\n# %%\nY_data = np.hstack([Y, NoiseVar])\n\n\n# %% [markdown]\n# ### Make a new likelihood\n#\n# To cope with this data structure, we\'ll build a new likelihood. Note how the code extracts the observations `Y` and the variances `NoiseVar` from the data. For more information on creating new likelihoods, see [Likelihood design](../tailor/likelihood_design.ipynb). Here, we\'re implementing the `log_prob` function (which computes the log-probability of the data given the latent function) and `variational_expectations`, which computes the expected log-probability under a Gaussian distribution on the function, and is needed in the evaluation of the evidence lower bound (ELBO). Check out the docstring for the `Likelihood` object for more information on what these functions do.\n\n# %%\nclass HeteroskedasticGaussian(gpflow.likelihoods.Likelihood):\n    def __init__(self, **kwargs):\n        # this likelihood expects a single latent function F, and two columns in the data matrix Y:\n        super().__init__(latent_dim=1, observation_dim=2, **kwargs)\n\n    def _log_prob(self, F, Y):\n        # log_prob is used by the quadrature fallback of variational_expectations and predict_log_density.\n        # Because variational_expectations is implemented analytically below, this is not actually needed,\n        # but is included for pedagogical purposes.\n        # Note that currently relying on the quadrature would fail due to https://github.com/GPflow/GPflow/issues/966\n        Y, NoiseVar = Y[:, 0], Y[:, 1]\n        return gpflow.logdensities.gaussian(Y, F, NoiseVar)\n\n    def _variational_expectations(self, Fmu, Fvar, Y):\n        Y, NoiseVar = Y[:, 0], Y[:, 1]\n        return (\n            -0.5 * np.log(2 * np.pi)\n            - 0.5 * tf.math.log(NoiseVar)\n            - 0.5 * (tf.math.square(Y - Fmu) + Fvar) / NoiseVar\n        )\n\n    # The following two methods are abstract in the base class.\n    # They need to be implemented even if not used.\n\n    def _predict_log_density(self, Fmu, Fvar, Y):\n        raise NotImplementedError\n\n    def _predict_mean_and_var(self, Fmu, Fvar):\n        raise NotImplementedError\n\n\n# %% [markdown]\n# ### Put it together with Variational Gaussian Process (VGP)\n# Here we\'ll build a variational GP model with the previous likelihood on the dataset that we generated. We\'ll use the natural gradient optimizer (see [Natural gradients](natural_gradients.ipynb) for more information).\n#\n# The variational GP object is capable of variational inference with any GPflow-derived likelihood. Usually, the inference is an inexact (but pretty good) approximation, but in the special case considered here, where the noise is Gaussian, it will achieve exact inference. Optimizing over the variational parameters is easy using the natural gradients method, which provably converges in a single step.\n#\n# Note: We mark the variational parameters as not trainable so that they are not included in the `model.trainable_variables` when we optimize using the Adam optimizer. We train the variational parameters separately using the natural gradient method.\n\n# %%\n# model construction\nlikelihood = HeteroskedasticGaussian()\nkernel = gpflow.kernels.Matern52(lengthscales=0.5)\nmodel = gpflow.models.VGP((X, Y_data), kernel=kernel, likelihood=likelihood, num_latent_gps=1)\n\n\n# %% [markdown]\n# Notice that we specify num_latent_gps=1, as the VGP model would normally infer this from the shape of Y_data, but we handle the second column manually in the HeteroskedasticGaussian likelihood.\n\n# %%\nnatgrad = NaturalGradient(gamma=1.0)\nadam = tf.optimizers.Adam()\n\nset_trainable(model.q_mu, False)\nset_trainable(model.q_sqrt, False)\n\nfor _ in range(ci_niter(1000)):\n    natgrad.minimize(model.training_loss, [(model.q_mu, model.q_sqrt)])\n    adam.minimize(model.training_loss, model.trainable_variables)\n\n# %%\n# let\'s do some plotting!\nxx = np.linspace(-5, 5, 200)[:, None]\n\nmu, var = model.predict_f(xx)\n\nplt.figure(figsize=(12, 6))\nplt.plot(xx, mu, ""C0"")\nplt.plot(xx, mu + 2 * np.sqrt(var), ""C0"", lw=0.5)\nplt.plot(xx, mu - 2 * np.sqrt(var), ""C0"", lw=0.5)\n\nplt.errorbar(\n    X.squeeze(),\n    Y.squeeze(),\n    yerr=2 * (np.sqrt(NoiseVar)).squeeze(),\n    marker=""x"",\n    lw=0,\n    elinewidth=1.0,\n    color=""C1"",\n)\n_ = plt.xlim(-5, 5)\n\n# %% [markdown]\n# ### Questions for the reader\n# 1) What is the difference in meaning between the orange vertical bars and the blue regions in the prediction?\n#\n# 2) Why did we not implement `conditional_mean` and `conditional_var` in the HeteroskedasticGaussian likelihood? What could be done here?\n#\n# 2) What are some better kernel settings for this dataset? How could they be estimated?\n\n# %% [markdown]\n# ## Demo 2: grouped noise variances\n#\n# In this demo, we won\'t assume that the noise variances are known, but we will assume that they\'re known in two groups. This example represents a case where we might know that an instrument has varying fidelity for different regions, but we do not know what those fidelities are.\n#\n# Of course it would be straightforward to add more groups, or even one group per data point. We\'ll stick with two for simplicity.\n\n# %%\nnp.random.seed(1)  # for reproducibility and to make it independent from demo 1\n\n\n# %% [markdown]\n# ### Generate data\n\n# %%\ndef generate_data(N=100):\n    X = np.random.rand(N)[:, None] * 10 - 5  # Inputs, shape N x 1\n    F = 2.5 * np.sin(6 * X) + np.cos(3 * X)  # Mean function values\n    groups = np.where(X > 0, 0, 1)\n    NoiseVar = np.array([0.02, 0.5])[groups]  # Different variances for the two groups\n    Y = F + np.random.randn(N, 1) * np.sqrt(NoiseVar)  # Noisy data\n    return X, Y, groups\n\n\nX, Y, groups = generate_data()\n\n# %%\n# here\'s a plot of the raw data.\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\n_ = ax.plot(X, Y, ""kx"")\n\n# %% [markdown]\n# ### Data structure\n#\n# In this case, we need to let the model know which group each data point belongs to. We\'ll use a similar trick to the above, stacking the group identifier with the data:\n\n# %%\nY_data = np.hstack([Y, groups])\n\n# %% [markdown]\n# ### Build a likelihood\n#\n# This time, we\'ll use a builtin likelihood, `SwitchedLikelihood`, which is a container for other likelihoods, and applies them to the first `Y_data` column depending on the index in the second. We\'re able to access and optimize the parameters of those likelihoods. Here, we\'ll (incorrectly) initialize the variances of our likelihoods to 1, to demonstrate how we can recover reasonable values for these through maximum-likelihood estimation.\n\n# %%\nlikelihood = gpflow.likelihoods.SwitchedLikelihood(\n    [gpflow.likelihoods.Gaussian(variance=1.0), gpflow.likelihoods.Gaussian(variance=1.0)]\n)\n\n# %%\n# model construction (notice that num_latent_gps is 1)\nkernel = gpflow.kernels.Matern52(lengthscales=0.5)\nmodel = gpflow.models.VGP((X, Y_data), kernel=kernel, likelihood=likelihood, num_latent_gps=1)\n\n\n# %%\nfor _ in range(ci_niter(1000)):\n    natgrad.minimize(model.training_loss, [(model.q_mu, model.q_sqrt)])\n\n# %% [markdown]\n# We\'ve now fitted the VGP model to the data, but without optimizing over the hyperparameters. Plotting the data, we see that the fit is not terrible, but hasn\'t made use of our knowledge of the varying noise.\n\n# %%\n# let\'s do some plotting!\nxx = np.linspace(-5, 5, 200)[:, None]\n\nmu, var = model.predict_f(xx)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nax.plot(xx, mu, ""C0"")\nax.plot(xx, mu + 2 * np.sqrt(var), ""C0"", lw=0.5)\nax.plot(xx, mu - 2 * np.sqrt(var), ""C0"", lw=0.5)\n\nax.plot(X, Y, ""C1x"", mew=2)\n_ = ax.set_xlim(-5, 5)\n\n# %% [markdown]\n# ### Optimizing the noise variances\n# Here we\'ll optimize over both the noise variance and the variational parameters, applying natural gradients interleaved with the Adam optimizer.\n#\n# As before, we mark the variational parameters as not trainable, in order to train them separately with the natural gradient method.\n\n# %%\nlikelihood = gpflow.likelihoods.SwitchedLikelihood(\n    [gpflow.likelihoods.Gaussian(variance=1.0), gpflow.likelihoods.Gaussian(variance=1.0)]\n)\nkernel = gpflow.kernels.Matern52(lengthscales=0.5)\nmodel = gpflow.models.VGP((X, Y_data), kernel=kernel, likelihood=likelihood, num_latent_gps=1)\n\nset_trainable(model.q_mu, False)\nset_trainable(model.q_sqrt, False)\n\nfor _ in range(ci_niter(1000)):\n    natgrad.minimize(model.training_loss, [(model.q_mu, model.q_sqrt)])\n    adam.minimize(model.training_loss, model.trainable_variables)\n\n# %% [markdown]\n# ### Plotting the fitted model\n#\n# Now that the noise variances have been estimated, we can see the final model fit.\n# The predictive variance is higher on the left side of the plot, where we know that the data have different variance.\n# We\'ll plot the known underlying function in green to see how effectively we\'ve recovered the ground truth.\n# We can also print the model to examine the estimated noise variances:\n\n# %%\n# let\'s do some plotting!\nxx = np.linspace(-5, 5, 200)[:, None]\n\nmu, var = model.predict_f(xx)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nax.plot(xx, mu, ""C0"")\nax.plot(xx, mu + 2 * np.sqrt(var), ""C0"", lw=0.5)\nax.plot(xx, mu - 2 * np.sqrt(var), ""C0"", lw=0.5)\n\nax.plot(X, Y, ""C1x"", mew=2)\nax.set_xlim(-5, 5)\n_ = ax.plot(xx, 2.5 * np.sin(6 * xx) + np.cos(3 * xx), ""C2--"")\n\n# %%\nfrom gpflow.utilities import print_summary\n\nprint_summary(model, fmt=""notebook"")\n'"
doc/source/notebooks/basics/GPLVM.pct.py,5,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Bayesian Gaussian process latent variable model (Bayesian GPLVM)\n# This notebook shows how to use the Bayesian GPLVM model. This is an unsupervised learning method usually used for dimensionality reduction. For an in-depth overview of GPLVMs,see **[1, 2]**.\n\n# %%\nimport gpflow\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow.utilities import ops, print_summary\nfrom gpflow.config import set_default_float, default_float, set_default_summary_fmt\nfrom gpflow.ci_utils import ci_niter\n\nset_default_float(np.float64)\nset_default_summary_fmt(""notebook"")\n\n# %matplotlib inline\n\n# %% [markdown]\n# ## Data\n# We are using the ""three phase oil flow"" dataset used initially for demonstrating the Generative Topographic mapping from **[3]**.\n\n# %%\ndata = np.load(""./data/three_phase_oil_flow.npz"")\n\n# %% [markdown]\n# Following the GPflow notation we assume this dataset has a shape of `[num_data, output_dim]`\n\n# %%\nY = tf.convert_to_tensor(data[""Y""], dtype=default_float())\n\n# %% [markdown]\n# Integer in $[0, 2]$ indicating to which class the data point belongs (shape `[num_data,]`). Not used for model fitting, only for plotting afterwards.\n\n# %%\nlabels = tf.convert_to_tensor(data[""labels""])\n\n# %%\nprint(""Number of points: {} and Number of dimensions: {}"".format(Y.shape[0], Y.shape[1]))\n\n# %% [markdown]\n# ## Model construction\n#\n# We start by initializing the required variables:\n\n# %%\nlatent_dim = 2  # number of latent dimensions\nnum_inducing = 20  # number of inducing pts\nnum_data = Y.shape[0]  # number of data points\n\n# %% [markdown]\n# Initialize via PCA:\n\n# %%\nX_mean_init = ops.pca_reduce(Y, latent_dim)\nX_var_init = tf.ones((num_data, latent_dim), dtype=default_float())\n\n# %% [markdown]\n# Pick inducing inputs randomly from dataset initialization:\n\n# %%\nnp.random.seed(1)  # for reproducibility\ninducing_variable = tf.convert_to_tensor(\n    np.random.permutation(X_mean_init.numpy())[:num_inducing], dtype=default_float()\n)\n\n# %% [markdown]\n# We construct a Squared Exponential (SE) kernel operating on the two-dimensional latent space.\n# The `ARD` parameter stands for Automatic Relevance Determination, which in practice means that\n# we learn a different lengthscale for each of the input dimensions. See [Manipulating kernels](../advanced/kernels.ipynb) for more information.\n\n# %%\nlengthscales = tf.convert_to_tensor([1.0] * latent_dim, dtype=default_float())\nkernel = gpflow.kernels.RBF(lengthscales=lengthscales)\n\n# %% [markdown]\n# We have all the necessary ingredients to construct the model. GPflow contains an implementation of the Bayesian GPLVM:\n\n# %%\ngplvm = gpflow.models.BayesianGPLVM(\n    Y,\n    X_data_mean=X_mean_init,\n    X_data_var=X_var_init,\n    kernel=kernel,\n    inducing_variable=inducing_variable,\n)\n# Instead of passing an inducing_variable directly, we can also set the num_inducing_variables argument to an integer, which will randomly pick from the data.\n\n# %% [markdown]\n# We change the default likelihood variance, which is 1, to 0.01.\n\n# %%\ngplvm.likelihood.variance.assign(0.01)\n\n# %% [markdown]\n# Next we optimize the created model. Given that this model has a deterministic evidence lower bound (ELBO), we can use SciPy\'s BFGS optimizer.\n\n# %%\nopt = gpflow.optimizers.Scipy()\nmaxiter = ci_niter(1000)\n_ = opt.minimize(\n    gplvm.training_loss,\n    method=""BFGS"",\n    variables=gplvm.trainable_variables,\n    options=dict(maxiter=maxiter),\n)\n\n# %% [markdown]\n# ## Model analysis\n# GPflow allows you to inspect the learned model hyperparameters.\n\n# %%\nprint_summary(gplvm)\n\n# %% [markdown]\n# ## Plotting vs. Principle Component Analysis (PCA)\n# The reduction of the dimensionality of the dataset to two dimensions allows us to visualize the learned manifold.\n# We compare the Bayesian GPLVM\'s latent space to the deterministic PCA\'s one.\n\n# %%\nX_pca = ops.pca_reduce(Y, latent_dim).numpy()\ngplvm_X_mean = gplvm.X_data_mean.numpy()\n\nf, ax = plt.subplots(1, 2, figsize=(10, 6))\n\nfor i in np.unique(labels):\n    ax[0].scatter(X_pca[labels == i, 0], X_pca[labels == i, 1], label=i)\n    ax[1].scatter(gplvm_X_mean[labels == i, 0], gplvm_X_mean[labels == i, 1], label=i)\n    ax[0].set_title(""PCA"")\n    ax[1].set_title(""Bayesian GPLVM"")\n\n# %%\n\n# %% [markdown]\n# ## References\n# \\[1\\] Lawrence, Neil D. \'Gaussian process latent variable models for visualization of high dimensional data\'. *Advances in Neural Information Processing Systems*. 2004.\n#\n# \\[2\\] Titsias, Michalis, and Neil D. Lawrence. \'Bayesian Gaussian process latent variable model\'. *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics*. 2010.\n#\n# \\[3\\] Bishop, Christopher M., and Gwilym D. James. \'Analysis of multiphase flows using dual-energy gamma densitometry and neural networks\'. *Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment* 327.2-3 (1993): 580-593.\n'"
doc/source/notebooks/basics/classification.pct.py,1,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Basic (binary) GP classification model\n#\n#\n# This notebook shows how to build a GP classification model using variational inference.\n# Here we consider binary (two-class, 0 vs. 1) classification only (there is a separate notebook on [multiclass classification](../advanced/multiclass_classification.ipynb)).\n# We first look at a one-dimensional example, and then show how you can adapt this when the input space is two-dimensional.\n\n# %%\nimport numpy as np\nimport gpflow\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\n\nplt.rcParams[""figure.figsize""] = (8, 4)\n\n# %% [markdown]\n# ## One-dimensional example\n#\n# First of all, let\'s have a look at the data. `X` and `Y` denote the input and output values.\n# **NOTE:** `X` and `Y` must be two-dimensional NumPy arrays, $N \\times 1$ or $N \\times D$, where $D$ is the number of input dimensions/features, with the same number of rows as $N$ (one for each data point):\n\n# %%\nX = np.genfromtxt(""data/classif_1D_X.csv"").reshape(-1, 1)\nY = np.genfromtxt(""data/classif_1D_Y.csv"").reshape(-1, 1)\n\nplt.figure(figsize=(10, 6))\n_ = plt.plot(X, Y, ""C3x"", ms=8, mew=2)\n\n# %% [markdown]\n# ### Reminders on GP classification\n#\n# For a binary classification model using GPs, we can simply use a `Bernoulli` likelihood. The details of the generative model are as follows:\n#\n# __1. Define the latent GP:__ we start from a Gaussian process $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot\'))$:\n\n# %%\n# build the kernel and covariance matrix\nk = gpflow.kernels.Matern52(variance=20.0)\nx_grid = np.linspace(0, 6, 200).reshape(-1, 1)\nK = k(x_grid)\n\n# sample from a multivariate normal\nrng = np.random.RandomState(6)\n\nL = np.linalg.cholesky(K)\nf_grid = np.dot(L, rng.randn(200, 5))\nplt.plot(x_grid, f_grid, ""C0"", linewidth=1)\n_ = plt.plot(x_grid, f_grid[:, 1], ""C0"", linewidth=2)\n\n# %% [markdown]\n# __2. Squash them to $[0, 1]$:__ the samples of the GP are mapped to $[0, 1]$.\n# By default, GPflow uses the standard normal cumulative distribution function (inverse probit function): $p(x) = \\Phi(f(x)) = \\frac{1}{2} (1 + \\operatorname{erf}(x / \\sqrt{2}))$.\n# (This choice has the advantage that predictive mean, variance and density can be computed analytically, but any choice of invlink is possible, e.g. the logit $p(x) = \\frac{\\exp(f(x))}{1 + \\exp(f(x))}$. Simply pass another function as the `invlink` argument to the `Bernoulli` likelihood class.)\n\n# %%\ndef invlink(f):\n    return gpflow.likelihoods.Bernoulli().invlink(f).numpy()\n\n\np_grid = invlink(f_grid)\nplt.plot(x_grid, p_grid, ""C1"", linewidth=1)\n_ = plt.plot(x_grid, p_grid[:, 1], ""C1"", linewidth=2)\n\n# %% [markdown]\n# __3. Sample from a Bernoulli:__ for each observation point $X_i$, the class label $Y_i \\in \\{0, 1\\}$ is generated by sampling from a Bernoulli distribution $Y_i \\sim \\mathcal{B}(g(X_i))$.\n\n# %%\n# Select some input locations\nind = rng.randint(0, 200, (30,))\nX_gen = x_grid[ind]\n\n# evaluate probability and get Bernoulli draws\np = p_grid[ind, 1:2]\nY_gen = rng.binomial(1, p)\n\n# plot\nplt.plot(x_grid, p_grid[:, 1], ""C1"", linewidth=2)\nplt.plot(X_gen, p, ""C1o"", ms=6)\n_ = plt.plot(X_gen, Y_gen, ""C3x"", ms=8, mew=2)\n\n# %% [markdown]\n# ### Implementation with GPflow\n#\n# For the model described above, the posterior $f(x)|Y$ (say $p$) is not Gaussian any more and does not have a closed-form expression.\n# A common approach is then to look for the best approximation of this posterior by a tractable distribution (say $q$) such as a Gaussian distribution.\n# In variational inference, the quality of an approximation is measured by the Kullback-Leibler divergence $\\mathrm{KL}[q \\| p]$.\n# For more details on this model, see Nickisch and Rasmussen (2008).\n#\n# The inference problem is thus turned into an optimization problem: finding the best parameters for $q$.\n# In our case, we introduce $U \\sim \\mathcal{N}(q_\\mu, q_\\Sigma)$, and we choose $q$ to have the same distribution as $f | f(X) = U$.\n# The parameters $q_\\mu$ and $q_\\Sigma$ can be seen as parameters of $q$, which can be optimized in order to minimise  $\\mathrm{KL}[q \\| p]$.\n#\n# This variational inference model is called `VGP` in GPflow:\n\n# %%\nm = gpflow.models.VGP(\n    (X, Y), likelihood=gpflow.likelihoods.Bernoulli(), kernel=gpflow.kernels.Matern52()\n)\n\nopt = gpflow.optimizers.Scipy()\nopt.minimize(m.training_loss, variables=m.trainable_variables)\n\n# %% [markdown]\n# We can now inspect the result of the optimization with `gpflow.utilities.print_summary(m)`:\n\n# %%\ngpflow.utilities.print_summary(m, fmt=""notebook"")\n\n# %% [markdown]\n# In this table, the first two lines are associated with the kernel parameters, and the last two correspond to the variational parameters.\n# **NOTE:** In practice, $q_\\Sigma$ is actually parameterized by its lower-triangular square root $q_\\Sigma = q_\\text{sqrt} q_\\text{sqrt}^T$ in order to ensure its positive-definiteness.\n#\n# For more details on how to handle models in GPflow (getting and setting parameters, fixing some of them during optimization, using priors, and so on), see [Manipulating GPflow models](../understanding/models.ipynb).\n\n# %% [markdown]\n# ### Predictions\n#\n# Finally, we will see how to use model predictions to plot the resulting model.\n# We will replicate the figures of the generative model above, but using the approximate posterior distribution given by the model.\n\n# %%\nplt.figure(figsize=(12, 8))\n\n# bubble fill the predictions\nmu, var = m.predict_f(x_grid)\n\nplt.fill_between(\n    x_grid.flatten(),\n    np.ravel(mu + 2 * np.sqrt(var)),\n    np.ravel(mu - 2 * np.sqrt(var)),\n    alpha=0.3,\n    color=""C0"",\n)\n\n# plot samples\ntf.random.set_seed(6)\nsamples = m.predict_f_samples(x_grid, 10).numpy().squeeze().T\n\nplt.plot(x_grid, samples, ""C0"", lw=1)\n\n# plot p-samples\np = invlink(samples)\nplt.plot(x_grid, p, ""C1"", lw=1)\n\n# plot data\nplt.plot(X, Y, ""C3x"", ms=8, mew=2)\nplt.ylim((-3, 3))\n\n# %% [markdown]\n# ## Two-dimensional example\n#\n# In this section we will use the following data:\n\n# %%\nX = np.loadtxt(""data/banana_X_train"", delimiter="","")\nY = np.loadtxt(""data/banana_Y_train"", delimiter="","").reshape(-1, 1)\nmask = Y[:, 0] == 1\n\nplt.figure(figsize=(6, 6))\nplt.plot(X[mask, 0], X[mask, 1], ""oC0"", mew=0, alpha=0.5)\n_ = plt.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], ""oC1"", mew=0, alpha=0.5)\n\n# %% [markdown]\n# The model definition is the same as above; the only important difference is that we now specify that the kernel operates over a two-dimensional input space:\n\n# %%\nm = gpflow.models.VGP(\n    (X, Y), kernel=gpflow.kernels.SquaredExponential(), likelihood=gpflow.likelihoods.Bernoulli()\n)\n\nopt = gpflow.optimizers.Scipy()\nopt.minimize(\n    m.training_loss, variables=m.trainable_variables, options=dict(maxiter=25), method=""L-BFGS-B""\n)\n# in practice, the optimization needs around 250 iterations to converge\n\n# %% [markdown]\n# We can now plot the predicted decision boundary between the two classes.\n# To do so, we can equivalently plot the contour lines $E[f(x)|Y]=0$, or $E[g(f(x))|Y]=0.5$.\n# We will do the latter, because it allows us to introduce the `predict_y` function, which returns the mean and variance at test points:\n\n# %%\nx_grid = np.linspace(-3, 3, 40)\nxx, yy = np.meshgrid(x_grid, x_grid)\nXplot = np.vstack((xx.flatten(), yy.flatten())).T\n\np, _ = m.predict_y(Xplot)  # here we only care about the mean\nplt.figure(figsize=(7, 7))\nplt.plot(X[mask, 0], X[mask, 1], ""oC0"", mew=0, alpha=0.5)\nplt.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], ""oC1"", mew=0, alpha=0.5)\n\n_ = plt.contour(\n    xx,\n    yy,\n    p.numpy().reshape(*xx.shape),\n    [0.5],  # plot the p=0.5 contour line only\n    colors=""k"",\n    linewidths=1.8,\n    zorder=100,\n)\n\n# %% [markdown]\n# ## Further reading\n#\n# There are dedicated notebooks giving more details on how to manipulate [models](../understanding/models.ipynb) and [kernels](../advanced/kernels.ipynb).\n#\n# This notebook covers only very basic classification models. You might also be interested in:\n#   * [Multiclass classification](../advanced/multiclass_classification.ipynb) if you have more than two classes.\n#   * [Sparse models](../advanced/gps_for_big_data.ipynb). The models above have one inducing variable $U_i$ per observation point $X_i$, which does not scale to large datasets.   Sparse Variational GP (SVGP) is an efficient alternative where the variables $U_i$ are defined at some inducing input locations $Z_i$ that can also be optimized.\n#   * [Exact inference](../advanced/mcmc.ipynb). We have seen that variational inference provides an approximation to the posterior. GPflow also supports exact inference using Markov Chain Monte Carlo (MCMC) methods, and the kernel parameters can also be assigned prior distributions in order to avoid point estimates.\n#\n# ## References\n#\n# Hannes Nickisch and Carl Edward Rasmussen. \'Approximations for binary Gaussian process classification\'. *Journal of Machine Learning Research* 9(Oct):2035--2078, 2008.\n'"
doc/source/notebooks/basics/monitoring.pct.py,8,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.4.0\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Monitoring Optimisation\n#\n# In this notebook we cover how to monitor the model and certain metrics during optimisation.\n#\n# ## Setup\n\n# %%\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow.ci_utils import ci_niter\n\nnp.random.seed(0)\n\n# %% [markdown]\n# The monitoring functionality lives in `gpflow.monitor`.\n# For now, we import `ModelToTensorBoard`, `ImageToTensorBoard`, `ScalarToTensorBoard` monitoring tasks and `MonitorTaskGroup` and `Monitor`.\n\n# %%\nfrom gpflow.monitor import (\n    ImageToTensorBoard,\n    ModelToTensorBoard,\n    Monitor,\n    MonitorTaskGroup,\n    ScalarToTensorBoard,\n)\n\n# %% [markdown]\n# ## Set up data and model\n\n# %%\n# Define some configuration constants.\n\nnum_data = 100\nnoise_std = 0.1\noptimisation_steps = ci_niter(100)\n\n# %%\n# Create dummy data.\n\nX = np.random.randn(num_data, 1)  # [N, 2]\nY = np.sin(X) + 0.5 * np.cos(X) + np.random.randn(*X.shape) * noise_std  # [N, 1]\nplt.plot(X, Y, ""o"")\n\n# %%\n# Set up model and print\n\nkernel = gpflow.kernels.SquaredExponential(lengthscales=[1.0, 2.0]) + gpflow.kernels.Linear()\nmodel = gpflow.models.GPR((X, Y), kernel, noise_variance=noise_std ** 2)\nmodel\n\n\n# %%\n# We define a function that plots the model\'s prediction (in the form of samples) together with the data.\n# Importantly, this function has no other argument than `fig: matplotlib.figure.Figure` and `ax: matplotlib.figure.Axes`.\n\n\ndef plot_prediction(fig, ax):\n    Xnew = np.linspace(X.min() - 0.5, X.max() + 0.5, 100).reshape(-1, 1)\n    Ypred = model.predict_f_samples(Xnew, full_cov=True, num_samples=20)\n    ax.plot(Xnew.flatten(), np.squeeze(Ypred).T, ""C1"", alpha=0.2)\n    ax.plot(X, Y, ""o"")\n\n\n# Let\'s check if the function does the desired plotting\nfig = plt.figure()\nax = fig.subplots()\nplot_prediction(fig, ax)\nplt.show()\n\n# %% [markdown]\n# ## Set up monitoring tasks\n#\n# We now define the `MonitorTask`s that will be executed during the optimisation.\n# For this tutorial we set up three tasks:\n# - `ModelToTensorBoard`: writes the models hyper-parameters such as `likelihood.variance` and `kernel.lengthscales` to a TensorBoard.\n# - `ImageToTensorBoard`: writes custom matplotlib images to a TensorBoard.\n# - `ScalarToTensorBoard`: writes any scalar value to a TensorBoard. Here, we use it to write the model\'s training objective.\n\n# %%\nlog_dir = ""logs""  # Directory where TensorBoard files will be written.\nmodel_task = ModelToTensorBoard(log_dir, model)\nimage_task = ImageToTensorBoard(log_dir, plot_prediction, ""image_samples"")\nlml_task = ScalarToTensorBoard(log_dir, lambda: model.training_loss(), ""training_objective"")\n\n# %% [markdown]\n# We now group the tasks in a set of fast and slow tasks and pass them to the monitor.\n# This allows us to execute the groups at a different frequency.\n\n# %%\n# Plotting tasks can be quite slow. We want to run them less frequently.\n# We group them in a `MonitorTaskGroup` and set the period to 5.\nslow_tasks = MonitorTaskGroup(image_task, period=5)\n\n# The other tasks are fast. We run them at each iteration of the optimisation.\nfast_tasks = MonitorTaskGroup([model_task, lml_task], period=1)\n\n# Both groups are passed to the monitor.\n# `slow_tasks` will be run five times less frequently than `fast_tasks`.\nmonitor = Monitor(fast_tasks, slow_tasks)\n\n\n# %%\ntraining_loss = model.training_loss_closure(\n    compile=True\n)  # compile=True (default): compiles using tf.function\nopt = tf.optimizers.Adam()\n\nfor step in range(optimisation_steps):\n    opt.minimize(training_loss, model.trainable_variables)\n    monitor(step)  # <-- run the monitoring\n\n# %% [markdown]\n# TensorBoard is accessible through the browser, after launching the server by running `tensorboard --logdir ${logdir}`.\n# See the [TensorFlow documentation on TensorBoard](https://www.tensorflow.org/tensorboard/get_started) for more information.\n\n\n# %% [markdown]\n# For optimal performance, we can also wrap the monitor call inside `tf.function`:\n\n# %%\nopt = tf.optimizers.Adam()\n\nlog_dir = f""{log_dir}/compiled""\nmodel_task = ModelToTensorBoard(log_dir, model)\nlml_task = ScalarToTensorBoard(log_dir, lambda: model.training_loss(), ""training_objective"")\n# Note that the `ImageToTensorBoard` task cannot be compiled, and is omitted from the monitoring\nmonitor = Monitor(MonitorTaskGroup([model_task, lml_task]))\n\n\n# %% [markdown]\n# In the optimisation loop below we use `tf.range` (rather than Python\'s built-in range) to avoid re-tracing the `step` function each time.\n\n\n# %%\n@tf.function\ndef step(i):\n    opt.minimize(model.training_loss, model.trainable_variables)\n    monitor(i)\n\n\n# Notice the tf.range\nfor i in tf.range(optimisation_steps):\n    step(i)\n\n# %% [markdown]\n# When opening TensorBoard, you may need to use the command `tensorboard --logdir . --reload_multifile=true`, as multiple `FileWriter` objects are used.\n'"
doc/source/notebooks/basics/regression.pct.py,1,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.4.0\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Basic (Gaussian likelihood) GP regression model\n#\n#\n# This notebook shows the different steps for creating and using a standard GP regression model, including:\n#   - reading and formatting data\n#   - choosing a kernel function\n#   - choosing a mean function (optional)\n#   - creating the model\n#   - viewing, getting, and setting model parameters\n#   - optimizing the model parameters\n#   - making predictions\n#\n# We focus here on the implementation of the models in GPflow; for more intuition on these models, see [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/).\n#\n\n# %%\nimport gpflow\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom gpflow.utilities import print_summary\n\n# The lines below are specific to the notebook format\n# %matplotlib inline\nplt.rcParams[""figure.figsize""] = (12, 6)\n\n# %% [markdown]\n# `X` and `Y` denote the input and output values. **NOTE:** `X` and `Y` must be two-dimensional NumPy arrays, $N \\times 1$ or $N \\times D$, where $D$ is the number of input dimensions/features, with the same number of rows as $N$ (one for each data point):\n\n# %%\ndata = np.genfromtxt(""data/regression_1D.csv"", delimiter="","")\nX = data[:, 0].reshape(-1, 1)\nY = data[:, 1].reshape(-1, 1)\n\n_ = plt.plot(X, Y, ""kx"", mew=2)\n\n# %% [markdown]\n# We will consider the following probabilistic model:\n# \\begin{equation}\n# Y_i = f(X_i) + \\varepsilon_i\\,,\n# \\end{equation}\n# where $f \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot\'))$, and $\\varepsilon \\sim \\mathcal{N}(0, \\tau^2 I)$.\n\n# %% [markdown]\n# ## Choose a kernel\n# Several kernels (covariance functions) are implemented in GPflow. You can easily combine them to create new ones (see [Manipulating kernels](../advanced/kernels.ipynb)). You can also implement new covariance functions, as shown in the [Kernel design](../tailor/kernel_design.ipynb) notebook. Here, we will use a simple one:\n\n# %%\nk = gpflow.kernels.Matern52()\n\n# %% [markdown]\n# For more advanced kernels see the [advanced kernel notebook](../advanced/kernels.ipynb) (including kernels defined on subspaces). A summary of the kernel can be obtained by\n\n# %%\nprint_summary(k)\n\n# %% [markdown]\n# The Matern 5/2 kernel has two parameters: `lengthscales`, which encodes the ""wiggliness"" of the GP, and `variance`, which tunes the amplitude. They are both set to 1.0 as the default value. For more details on the meaning of the other columns, see [Manipulating kernels](../advanced/kernels.ipynb).\n\n# %% [markdown]\n# ## Choose a mean function (optional)\n# It is common to choose $\\mu = 0$, which is the GPflow default.\n#\n# However, if there is a clear pattern (such as a mean value of `Y` that is far away from 0, or a linear trend in the data), mean functions can  be beneficial. Some simple ones are provided in the `gpflow.mean_functions` module.\n#\n# Here\'s how to define a linear mean function:\n#\n# `meanf = gpflow.mean_functions.Linear()`\n\n# %% [markdown]\n# ## Construct a model\n# A GPflow model is created by instantiating one of the GPflow model classes, in this case GPR. We\'ll make a kernel `k` and instantiate a GPR object using the generated data and the kernel. We\'ll also set the variance of the likelihood to a sensible initial guess.\n\n# %%\nm = gpflow.models.GPR(data=(X, Y), kernel=k, mean_function=None)\n\n# %% [markdown]\n# A summary of the model can be obtained by\n\n# %%\nprint_summary(m)\n\n# %% [markdown]\n# The first two lines correspond to the kernel parameters, and the third one gives the likelihood parameter (the noise variance $\\tau^2$ in our model).\n#\n# You can access those values and manually set them to sensible initial guesses. For example:\n#\n\n# %%\nm.likelihood.variance.assign(0.01)\nm.kernel.lengthscales.assign(0.3)\n\n# %% [markdown]\n# ## Optimize the model parameters\n#\n# To obtain meaningful predictions, you need to tune the model parameters (that is, the parameters of the kernel, the likelihood, and the mean function if applicable) to the data at hand.\n#\n# There are several optimizers available in GPflow. Here we use the `Scipy` optimizer, which by default implements the L-BFGS-B algorithm. (You can select other algorithms by using the `method=` keyword argument to its `minimize` method; see [the SciPy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) for details of available options.)\n\n# %%\nopt = gpflow.optimizers.Scipy()\n\n\n# %% [markdown]\n# In order to train the model, we need to maximize the log marginal likelihood.\n# GPflow models define a `training_loss` that can be passed to the `minimize`\n# method of an optimizer; in this case it is simply the negative log marginal\n# likelihood. We also need to specify the variables to train with\n# `m.trainable_variables`, and the number of iterations.\n\n# %%\nopt_logs = opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=100))\nprint_summary(m)\n\n# %% [markdown]\n# Notice how the value column has changed.\n#\n# The local optimum found by Maximum Likelihood might not be the one you want (for example, it might be overfitting or oversmooth). This depends on the initial values of the hyperparameters, and is specific to each dataset. As an alternative to Maximum Likelihood, [Markov Chain Monte Carlo (MCMC)](../advanced/mcmc.ipynb) is also available.\n#\n# ## Make predictions\n#\n# We can now use the model to make some predictions at the new points `Xnew`. You might be interested in predicting two different quantities: the latent function values `f(Xnew)` (the denoised signal), or the values of new observations `y(Xnew)` (signal + noise). Because we are dealing with Gaussian probabilistic models, the predictions typically produce a mean and variance as output. Alternatively, you can obtain samples of `f(Xnew)` or the log density of the new data points `(Xnew, Ynew)`.\n#\n# GPflow models have several prediction methods:\n\n# %% [markdown]\n#  - `m.predict_f` returns the mean and marginal variance of $f$ at the points `Xnew`.\n#\n#  - `m.predict_f` with argument `full_cov=True` returns the mean and the full covariance matrix of $f$ at the points `Xnew`.\n#\n#  - `m.predict_f_samples` returns samples of the latent function.\n#\n#  - `m.predict_y` returns the mean and variance of a new data point (that is, it includes the noise variance).\n#\n#  - `m.predict_log_density` returns the log density of the observations `Ynew` at `Xnew`.\n#\n# We use `predict_f` and `predict_f_samples` to plot 95% confidence intervals and samples from the posterior distribution.\n\n# %%\n## generate test points for prediction\nxx = np.linspace(-0.1, 1.1, 100).reshape(100, 1)  # test points must be of shape (N, D)\n\n## predict mean and variance of latent GP at test points\nmean, var = m.predict_f(xx)\n\n## generate 10 samples from posterior\ntf.random.set_seed(1)  # for reproducibility\nsamples = m.predict_f_samples(xx, 10)  # shape (10, 100, 1)\n\n## plot\nplt.figure(figsize=(12, 6))\nplt.plot(X, Y, ""kx"", mew=2)\nplt.plot(xx, mean, ""C0"", lw=2)\nplt.fill_between(\n    xx[:, 0],\n    mean[:, 0] - 1.96 * np.sqrt(var[:, 0]),\n    mean[:, 0] + 1.96 * np.sqrt(var[:, 0]),\n    color=""C0"",\n    alpha=0.2,\n)\n\nplt.plot(xx, samples[:, :, 0].numpy().T, ""C0"", linewidth=0.5)\n_ = plt.xlim(-0.1, 1.1)\n\n\n# %% [markdown]\n# ## GP regression in higher dimensions\n#\n# Very little changes when the input space has more than one dimension. By default, the `lengthscales` is an isotropic (scalar) parameter. It is generally recommended that you allow to tune a different lengthscale for each dimension (Automatic Relevance Determination, ARD): simply initialize `lengthscales` with an array of length $D$ corresponding to the input dimension of `X`.  See [Manipulating kernels](../advanced/kernels.ipynb) for further information.\n\n\n# %% [markdown]\n# ## Further reading\n#\n#   - [Stochastic Variational Inference for scalability with SVGP](../advanced/gps_for_big_data.ipynb) for cases where there are a large number of observations.\n#   - [Ordinal regression](../advanced/ordinal_regression.ipynb) if the data is ordinal.\n#   - [Multi-output models and coregionalisation](../advanced/coregionalisation.ipynb) if `Y` is multidimensional.\n'"
doc/source/notebooks/tailor/external-mean-function.pct.py,11,"b'# -*- coding: utf-8 -*-\n# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Custom mean functions: metalearning with GPs\n# One of the advantages of Gaussian process is their flexibility as a modeling tool. For instance, if the modeler knows that there is an underlying trend in the data, they can specify a mean function that captures this trend.\n#\n# In this notebook, we illustrate how to use GPflow to construct a custom neural network mean function for GPs that can capture complex trends. We look at this functionality in the context of metalearning, where a number of metatasks are available at train time and the user wants to adapt a flexible model to new tasks at test time.\n#\n# For an in-depth discussion on this topic, see *(Fortuin and R\xc3\xa4tsch, 2019)*. This notebook reproduces section 4.2 of this paper.\n\n# %%\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport gpflow\nfrom gpflow.kernels import RBF\nfrom gpflow.likelihoods import Gaussian\nfrom gpflow.mean_functions import MeanFunction\nfrom gpflow.models import GPR\nfrom gpflow.base import Parameter\n\nfrom gpflow.ci_utils import ci_niter\n\n# for reproducibility of this notebook:\nnp.random.seed(1)\ntf.random.set_seed(2)\n\n# %matplotlib inline\n\n# %% [markdown]\n# ## Generate the tasks\n# To generate the meta and test tasks, we sample from a Gaussian process with an Squared Exponential covariance function and a sinusoidal mean function. Each task is a realization of this process.\n\n\n# %%\ndef generate_data(num_functions=10, N=500):\n    """"""\n    For each function, sample the value at `N` equally spaced\n    points in the [\xe2\x88\x925, 5] interval (Fortuin and R\xc3\xa4tsch, 2019).\n\n    Returns:\n        Tuple of np.arrays of size (N, 1) and (N, num_functions).\n    """"""\n    jitter = 1e-6\n    Xs = np.linspace(-5.0, 5.0, N)[:, None]\n    kernel = RBF(lengthscales=1.0)\n    cov = kernel(Xs)\n    L = np.linalg.cholesky(cov + np.eye(N) * jitter)\n    epsilon = np.random.randn(N, num_functions)\n    F = np.sin(Xs) + np.matmul(L, epsilon)\n    return Xs, F\n\n\n# %% [markdown]\n# We generate 10 tasks for illustration.\n\n# %%\nXs, F = generate_data(10)\n\n# %%\n_ = plt.plot(Xs, F)\n\n# %% [markdown]\n# We generate the meta and test tasks.\n\n\n# %%\ndef generate_meta_and_test_tasks(num_datapoints, num_meta, num_test, N):\n    """"""Generates meta-task datasets {D_i} = {x_i, y_i} and target task training\n    and test data {\\tilde{x}, \\tilde{y}} (Fortuin and R\xc3\xa4tsch, 2019).\n\n    Args:\n        num_datapoints: The number of training points, \\tilde{n} in Table S1.\n        num_meta: The number of meta-tasks (""sampled functions"").\n        num_test: The number of test tasks (""unseen functions"").\n        N: Number of sampled data points per function.\n\n    Returns:\n        A tuple (meta, test) where\n        - meta: List of num_meta pairs of arrays (X, Y) of size (n, 1) each.\n        - test: List of num_test pairs of pairs of arrays of sizes\n                (((num_datapoints, 1), (num_datapoints, 1)), \n                 ((N - num_datapoints, 1), (N - num_datapoints, 1))).\n    """"""\n    Xs, F = generate_data(num_functions=num_meta + num_test, N=N)\n    meta = []\n    sd = 1e-1  # Standard deviation for normally-distributed observation noise.\n    for i in range(num_meta):\n        # We always use all data points of the curve to train the mean function,\n        # i.e. n_i = N.\n        noise = sd * np.random.randn(N, 1)\n        Y = F[:, i][:, None] + noise\n        meta.append((Xs, Y))\n    test = []\n    rand_indices = np.random.permutation(N)\n    train_i = sorted(rand_indices[:num_datapoints])\n    test_i = sorted(rand_indices[num_datapoints:])\n    for i in range(num_test):\n        # Form target training set, \\tilde{D}_{train} (see Figure 1).\n        noise = sd * np.random.randn(num_datapoints, 1)\n        train_X = Xs[train_i]\n        train_Y = F[train_i, num_meta + i][:, None] + noise\n        # Form target test set, \\tilde{D}_{test}, which is disjoint from the\n        # target traininig set as it is in the original implementation.\n        test_X = Xs[test_i]\n        test_Y = F[test_i, num_meta + i][:, None]\n        # Form target tasks\' datasets \\tilde{D} as a pair of pairs,\n        # (\\tilde{X}, \\tilde{Y}) and (\\tilde{X}*, \\tilde{Y}*).\n        test.append(((train_X, train_Y), (test_X, test_Y)))\n    return meta, test\n\n\n# %%\nnum_datapoints = 5\n# Although the original experiment uses (1000, 200, 50) for the following\n# parameters, we will use (50, 10, 500) for computational reasons.\nnum_meta_tasks = 50\nnum_test_tasks = 10\nnum_data_per_task = 500\nmeta, test = generate_meta_and_test_tasks(\n    num_datapoints, num_meta_tasks, num_test_tasks, num_data_per_task\n)\n\n# %% [markdown]\n# ## Create the mean function\n# We will use a Keras model Deep Neural Network as mean function.\n\n# %%\nfrom tensorflow.python.keras import backend as K\nfrom gpflow.config import default_float\n\nK.set_floatx(""float64"")\nassert default_float() == np.float64\n\n\ndef build_mean_function():\n    inputs = tf.keras.layers.Input(shape=(1,))\n    x = tf.keras.layers.Dense(64, activation=""relu"")(inputs)\n    x = tf.keras.layers.Dense(64, activation=""relu"")(x)\n    outputs = tf.keras.layers.Dense(1)(x)\n    return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\n# %% [markdown]\n# ## Build the GP metamodel\n# Metalearning boils down to learning a good prior that can generalize to new tasks with a small number of data points. This framework is prevalent in GP modeling, where we usually maximize the marginal likelihood to learn a good set of hyperparameters that specify the GP prior.\n#\n# We perform the same optimization here, while sharing the hyperparameters across all the metatasks. For simplicity, we fix the kernel and likelihood parameters and learn those only for the mean function. Hence, our metalearning procedure is to cycle through the metatasks continuously, optimizing their marginal likelihood until a convergence criteria is reached (here, we just implement a fixed number of iterations over the tasks).\n#\n# To begin this process, first we create a utility function that takes in a task (X, Y) and a mean function and outputs a GP model.\n\n# %%\nfrom gpflow import set_trainable\n\n\ndef build_model(data, mean_function):\n    model = GPR(data, kernel=RBF(), mean_function=mean_function)\n    set_trainable(model.kernel, False)\n    model.likelihood.variance.assign(1e-2)\n    set_trainable(model.likelihood, False)\n    return model\n\n\n# %%\ndef create_optimization_step(optimizer, model: gpflow.models.GPR):\n    @tf.function\n    def optimization_step():\n        with tf.GradientTape(watch_accessed_variables=False) as tape:\n            tape.watch(model.trainable_variables)\n            objective = model.training_loss()\n        grads = tape.gradient(objective, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        return objective\n\n    return optimization_step\n\n\ndef run_adam(model, iterations):\n    """"""\n    Utility function running the Adam optimizer\n    \n    :param model: GPflow model\n    :param interations: number of iterations\n    """"""\n    # Create an Adam Optimizer action\n    logf = []\n    adam = tf.optimizers.Adam()\n    optimization_step = create_optimization_step(adam, model)\n    for step in range(iterations):\n        loss = optimization_step()\n        if step % 10 == 0:\n            # print(""num variables"", len(model.trainable_variables))\n            logf.append(loss.numpy())\n    return logf\n\n\n# %% [markdown]\n# Next, we define the training loop for metalearning.\n\n# %%\nimport time\n\n\ndef train_loop(meta_tasks, num_iter=5):\n    """"""\n    Metalearning training loop. Trained for 100 epochs in original experiment.\n    \n    :param meta_tasks: list of metatasks.\n    :param num_iter: number of iterations of tasks set\n    :returns: a mean function object\n    """"""\n    # Initialize mean function\n    mean_function = build_mean_function()\n    # Iterate for several passes over the tasks set\n    for iteration in range(num_iter):\n        ts = time.time()\n        print(""Currently in meta-iteration/epoch {}"".format(iteration))\n        # Iterate over tasks\n        for i, task in enumerate(meta_tasks):\n            data = task  # (X, Y)\n            model = build_model(data, mean_function=mean_function)\n            run_adam(model, ci_niter(100))\n\n        print("">>>> iteration took {:.2f} s"".format(time.time() - ts))\n    return mean_function\n\n\nmean_function_optimal = train_loop(meta)\n\n# %% [markdown]\n# Finally, we use the optimized mean function for all of the test tasks. **NOTE:** We do not do any further optimization for the hyperparameters in this step.\n\n# %%\ntest_models = [build_model(data, mean_function_optimal) for (data, _) in test]\n\n# %% [markdown]\n# ## Assess the model\n# We assess the performance of this procedure on the test tasks. For this, we use the mean squared error as a performance metric.\n\n\n# %%\ndef mean_squared_error(y, y_pred):\n    return np.mean((y - y_pred) ** 2)\n\n\n# %%\nmean_squared_errors = []\nfor i, test_task in enumerate(test):\n    plt.figure(figsize=(8, 4))\n    (train_X, train_Y), (Xs, F) = test_task\n    pred_mean, pred_var = test_models[i].predict_f(Xs)\n    plt.plot(Xs, pred_mean, label=""Prediction mean"", color=""blue"", linewidth=2)\n    plt.fill_between(\n        Xs.squeeze(1),\n        tf.squeeze(pred_mean - pred_var),\n        tf.squeeze(pred_mean + pred_var),\n        alpha=0.25,\n        facecolor=""blue"",\n        label=""Prediction variance"",\n    )\n    plt.plot(train_X, train_Y, ""ko"", label=""Training points"")\n    plt.plot(Xs, F, ""ko"", label=""Ground truth"", linewidth=2, markersize=1)\n    mse = mean_squared_error(F, pred_mean)\n    mean_squared_errors.append(mse)\n    plt.title(f""Test Task {i + 1} | MSE = {mse:.2f}"")\n    plt.legend()\n    plt.show()\n\n# %%\nmean_mse = np.mean(mean_squared_errors)\nstd_mse = np.std(mean_squared_errors) / np.sqrt(num_test_tasks)\nprint(f""The mean MSE over all {num_test_tasks} test tasks is {mean_mse:.2f} +/- {std_mse:.2f}"")\n\n# %% [markdown]\n# We achieve comparable results to those reported in the paper.\n#\n# **NOTE:** We use only 50 metatasks and 10 test tasks over 5 epochs for scalability, whereas the paper uses 1,000 and 200 respectively over 100 epochs. To compensate, we sample 500 points per curve, whereas the paper samples only 50 points. Hence, there might be some discrepancies in the results.\n\n# %% [markdown]\n# ## References\n# Fortuin, Vincent, and Gunnar R\xc3\xa4tsch. ""Deep Mean Functions for Meta-Learning in Gaussian Processes."" arXiv preprint arXiv:1901.08098 (2019).\n'"
doc/source/notebooks/tailor/gp_nn.pct.py,21,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Mixing TensorFlow models with GPflow\n#\n# This notebook explores the combination of Keras TensorFlow neural networks with GPflow models.\n\n# %%\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nimport gpflow\nfrom gpflow.ci_utils import ci_niter\nfrom scipy.cluster.vq import kmeans2\n\nfrom typing import Dict, Optional, Tuple\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport gpflow\nfrom gpflow.utilities import to_default_float\n\niterations = ci_niter(100)\n\n# %% [markdown]\n# ## Convolutional network inside a GPflow model\n\n# %%\noriginal_dataset, info = tfds.load(name=""mnist"", split=tfds.Split.TRAIN, with_info=True)\ntotal_num_data = info.splits[""train""].num_examples\nimage_shape = info.features[""image""].shape\nimage_size = tf.reduce_prod(image_shape)\nbatch_size = 32\n\n\ndef map_fn(input_slice: Dict[str, tf.Tensor]):\n    updated = input_slice\n    image = to_default_float(updated[""image""]) / 255.0\n    label = to_default_float(updated[""label""])\n    return tf.reshape(image, [-1, image_size]), label\n\n\nautotune = tf.data.experimental.AUTOTUNE\ndataset = (\n    original_dataset.shuffle(1024)\n    .batch(batch_size, drop_remainder=True)\n    .map(map_fn, num_parallel_calls=autotune)\n    .prefetch(autotune)\n    .repeat()\n)\n\n\n# %% [markdown]\n# Here we\'ll use the GPflow functionality, but put a non-GPflow model inside the kernel.\\\n# Vanilla ConvNet. This gets 97.3% accuracy on MNIST when used on its own (+ final linear layer) after 20K iterations\n\n# %%\nclass KernelWithConvNN(gpflow.kernels.Kernel):\n    def __init__(\n        self,\n        image_shape: Tuple,\n        output_dim: int,\n        base_kernel: gpflow.kernels.Kernel,\n        batch_size: Optional[int] = None,\n    ):\n        super().__init__()\n        with self.name_scope:\n            self.base_kernel = base_kernel\n            input_size = int(tf.reduce_prod(image_shape))\n            input_shape = (input_size,)\n\n            self.cnn = tf.keras.Sequential(\n                [\n                    tf.keras.layers.InputLayer(input_shape=input_shape, batch_size=batch_size),\n                    tf.keras.layers.Reshape(image_shape),\n                    tf.keras.layers.Conv2D(\n                        filters=32, kernel_size=image_shape[:-1], padding=""same"", activation=""relu""\n                    ),\n                    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2),\n                    tf.keras.layers.Conv2D(\n                        filters=64, kernel_size=(5, 5), padding=""same"", activation=""relu""\n                    ),\n                    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2),\n                    tf.keras.layers.Flatten(),\n                    tf.keras.layers.Dense(output_dim, activation=""relu""),\n                    tf.keras.layers.Lambda(to_default_float),\n                ]\n            )\n\n            self.cnn.build()\n\n    def K(self, a_input: tf.Tensor, b_input: Optional[tf.Tensor] = None) -> tf.Tensor:\n        transformed_a = self.cnn(a_input)\n        transformed_b = self.cnn(b_input) if b_input is not None else b_input\n        return self.base_kernel.K(transformed_a, transformed_b)\n\n    def K_diag(self, a_input: tf.Tensor) -> tf.Tensor:\n        transformed_a = self.cnn(a_input)\n        return self.base_kernel.K_diag(transformed_a)\n\n\n# %% [markdown]\n# $K_{uf}$ is in ConvNN output space, therefore we need to update `Kuf` multidispatch.\n\n# %%\nclass KernelSpaceInducingPoints(gpflow.inducing_variables.InducingPoints):\n    pass\n\n\n@gpflow.covariances.Kuu.register(KernelSpaceInducingPoints, KernelWithConvNN)\ndef Kuu(inducing_variable, kernel, jitter=None):\n    func = gpflow.covariances.Kuu.dispatch(\n        gpflow.inducing_variables.InducingPoints, gpflow.kernels.Kernel\n    )\n    return func(inducing_variable, kernel.base_kernel, jitter=jitter)\n\n\n@gpflow.covariances.Kuf.register(KernelSpaceInducingPoints, KernelWithConvNN, object)\ndef Kuf(inducing_variable, kernel, a_input):\n    return kernel.base_kernel(inducing_variable.Z, kernel.cnn(a_input))\n\n\n# %% [markdown]\n# Now we are ready to create and initialize the model:\n\n# %%\nnum_mnist_classes = 10\noutput_dim = 5\nnum_inducing_points = 100\nimages_subset, labels_subset = next(iter(dataset.batch(32)))\nimages_subset = tf.reshape(images_subset, [-1, image_size])\nlabels_subset = tf.reshape(labels_subset, [-1, 1])\n\nkernel = KernelWithConvNN(\n    image_shape, output_dim, gpflow.kernels.SquaredExponential(), batch_size=batch_size\n)\n\nlikelihood = gpflow.likelihoods.MultiClass(num_mnist_classes)\n\ninducing_variable_kmeans = kmeans2(images_subset.numpy(), num_inducing_points, minit=""points"")[0]\ninducing_variable_cnn = kernel.cnn(inducing_variable_kmeans)\ninducing_variable = KernelSpaceInducingPoints(inducing_variable_cnn)\n\nmodel = gpflow.models.SVGP(\n    kernel,\n    likelihood,\n    inducing_variable=inducing_variable,\n    num_data=total_num_data,\n    num_latent_gps=num_mnist_classes,\n)\n\n# %% [markdown]\n# And start optimization:\n\n# %%\ndata_iterator = iter(dataset)\nadam_opt = tf.optimizers.Adam(0.001)\n\ntraining_loss = model.training_loss_closure(data_iterator)\n\n\n@tf.function\ndef optimization_step():\n    adam_opt.minimize(training_loss, var_list=model.trainable_variables)\n\n\nfor _ in range(iterations):\n    optimization_step()\n\n# %% [markdown]\n# Let\'s do predictions after training. Don\'t expect that we will get a good accuracy, because we haven\'t run training for long enough.\n\n# %%\nm, v = model.predict_y(images_subset)\npreds = np.argmax(m, 1).reshape(labels_subset.numpy().shape)\ncorrect = preds == labels_subset.numpy().astype(int)\nacc = np.average(correct.astype(float)) * 100.0\n\nprint(""Accuracy is {:.4f}%"".format(acc))\n'"
doc/source/notebooks/tailor/kernel_design.pct.py,3,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Kernel Design\n#\n# It\'s easy to make new kernels in GPflow. To demonstrate, we\'ll have a look at the Brownian motion kernel, whose function is\n# \\begin{equation}\n# k(x, x\') = \\sigma^2 \\text{min}(x, x\')\n# \\end{equation}\n# where $\\sigma^2$ is a variance parameter.\n\n# %%\nimport gpflow\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom gpflow.utilities import print_summary, positive\n\nplt.style.use(""ggplot"")\n# %matplotlib inline\n\n# %% [markdown]\n# To make this new kernel class, we inherit from the base class `gpflow.kernels.Kernel` and implement the three functions below. **NOTE:** Depending on the kernel to be implemented, other classes can be more adequate. For example, if the kernel to be implemented is isotropic stationary, you can immediately subclass `gpflow.kernels.IsotropicStationary` (at which point you\n# only have to override `K_r` or `K_r2`; see the `IsotropicStationary` class docstring). Stationary but anisotropic kernels should subclass `gpflow.kernels.AnisotropicStationary` and override `K_d`.\n#\n# #### `__init__`\n# In this simple example, the constructor takes no argument (though it could, if that was convenient, for example to pass in an initial value for `variance`). It *must* call the constructor of the superclass with appropriate arguments. Brownian motion is only defined in one dimension, and we\'ll assume that the `active_dims` are `[0]`, for simplicity.\n#\n# We\'ve added a parameter to the kernel using the `Parameter` class. Using this class lets the parameter be used in computing the kernel function, and it will automatically be recognised for optimization (or MCMC). Here, the variance parameter is initialized at 1, and constrained to be positive.\n#\n# #### `K`\n# This is where you implement the kernel function itself. This takes two arguments, `X` and `X2`. By convention, we make the second argument optional (it defaults to `None`).\n#\n# Inside `K`, all the computation must be done with TensorFlow - here we\'ve used `tf.minimum`. When GPflow executes the `K` function, `X` and `X2` will be TensorFlow tensors, and parameters such as `self.variance` behave like TensorFlow tensors as well.\n#\n# #### `K_diag`\n# This convenience function allows GPflow to save memory at predict time. It\'s simply the diagonal of the `K` function, in the case where `X2` is `None`. It must return a one-dimensional vector, so we use TensorFlow\'s reshape command.\n\n# %%\nclass Brownian(gpflow.kernels.Kernel):\n    def __init__(self):\n        super().__init__(active_dims=[0])\n        self.variance = gpflow.Parameter(1.0, transform=positive())\n\n    def K(self, X, X2=None):\n        if X2 is None:\n            X2 = X\n        return self.variance * tf.minimum(X, tf.transpose(X2))  # this returns a 2D tensor\n\n    def K_diag(self, X):\n        return self.variance * tf.reshape(X, (-1,))  # this returns a 1D tensor\n\n\nk_brownian = Brownian()\nprint_summary(k_brownian, fmt=""notebook"")\n\n# %% [markdown]\n# We can now evaluate our new kernel function and draw samples from a Gaussian process with this covariance:\n\n# %%\nnp.random.seed(23)  # for reproducibility\n\n\ndef plotkernelsample(k, ax, xmin=0, xmax=3):\n    xx = np.linspace(xmin, xmax, 300)[:, None]\n    K = k(xx)\n    ax.plot(xx, np.random.multivariate_normal(np.zeros(300), K, 5).T)\n    ax.set_title(""Samples "" + k.__class__.__name__)\n\n\ndef plotkernelfunction(k, ax, xmin=0, xmax=3, other=0):\n    xx = np.linspace(xmin, xmax, 100)[:, None]\n    ax.plot(xx, k(xx, np.zeros((1, 1)) + other))\n    ax.set_title(k.__class__.__name__ + "" k(x, %.1f)"" % other)\n\n\nf, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\nplotkernelfunction(k_brownian, axes[0], other=2.0)\nplotkernelsample(k_brownian, axes[1])\n\n# %% [markdown]\n# ## Using the kernel in a model\n#\n# Because we\'ve inherited from the `Kernel` base class, this new kernel has all the properties needed to be used in GPflow. It also has some convenience features such as allowing the user to call\n#\n# `k(X, X2)`\n#\n# which computes the kernel matrix.\n#\n# To show that this kernel works, let\'s use it inside GP regression. We\'ll see that Brownian motion has quite interesting properties. To add a little flexibility, we\'ll add a `Constant` kernel to our `Brownian` kernel, and the `GPR` class will handle the noise.\n\n# %%\nnp.random.seed(42)\nX = np.random.rand(5, 1)\nY = np.sin(X * 6) + np.random.randn(*X.shape) * 0.001\n\nk1 = Brownian()\nk2 = gpflow.kernels.Constant()\nk = k1 + k2\n\nm = gpflow.models.GPR((X, Y), kernel=k)\n# m.likelihood.variance.assign(1e-6)\n\nopt = gpflow.optimizers.Scipy()\nopt.minimize(m.training_loss, variables=m.trainable_variables)\nprint_summary(m, fmt=""notebook"")\n\nxx = np.linspace(0, 1.1, 100).reshape(100, 1)\nmean, var = m.predict_y(xx)\nplt.plot(X, Y, ""kx"", mew=2)\n(line,) = plt.plot(xx, mean, lw=2)\n_ = plt.fill_between(\n    xx[:, 0],\n    mean[:, 0] - 2 * np.sqrt(var[:, 0]),\n    mean[:, 0] + 2 * np.sqrt(var[:, 0]),\n    color=line.get_color(),\n    alpha=0.2,\n)\n\n# %% [markdown]\n# ## See also\n#\n# For more details on how to manipulate existing kernels (or the one you just created!), we refer to the [Manipulating kernels](../advanced/kernels.ipynb) notebook.\n'"
doc/source/notebooks/tailor/likelihood_design.pct.py,0,"b""# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# Likelihood Design\n# --\n#\n# see models.ipynb (2/2), other option is to have heteroscedastic noise.\n"""
doc/source/notebooks/tailor/mdn_plotting.py,0,"b'import numpy as np\n\nfrom scipy.stats import norm\n\n\ndef make_grid(xx, yy):\n    """"""\n    Returns two n-by-n matrices. The first one contains all the x values \n    and the second all the y values of a cartesian product between `xx` and `yy`.\n    """"""\n    n = len(xx)\n    xx, yy = np.meshgrid(xx, yy)\n    grid = np.array([xx.ravel(), yy.ravel()]).T\n    x = grid[:, 0].reshape(n, n)\n    y = grid[:, 1].reshape(n, n)\n    return x, y\n\n\ndef plot(model, X, Y, axes, cmap, N_plot=100):\n    xx = np.linspace(X.min() - 1, X.max() + 1, N_plot)[:, None]\n    yy = np.linspace(Y.min() - 1, Y.max() + 1, N_plot)\n    pis, mus, sigmas = [v.numpy() for v in model.eval_network(xx)]\n\n    probs = norm.pdf(yy[:, None, None], loc=mus[None, :, :], scale=sigmas[None, :, :])\n    probs = np.sum(probs * pis[None, :, :], axis=-1)\n    plot_x, plot_y = make_grid(xx, yy)\n    axes[0].set_title(""Posterior density and trainings data"")\n    axes[0].contourf(plot_x, plot_y, np.log(probs), 500, cmap=cmap, vmin=-5, vmax=5)\n    axes[0].plot(X, Y, ""ro"", alpha=0.2, ms=3, label=""data"")\n    axes[0].legend(loc=4)\n    axes[1].set_title(r""$\\mu_m(x)$ and their relative contribution shown by size"")\n    axes[1].scatter(\n        np.repeat(xx.flatten(), repeats=mus.shape[1]), mus.flatten(), s=pis.flatten() * 20\n    )\n'"
doc/source/notebooks/tailor/mixture_density_network.pct.py,10,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Mixture Density Networks in GPflow\n#\n# In this notebook we explain how to implement a Mixture Density Network (MDN) [1] using GPflow. In theory, this is similar to [this blog post](http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/) from 2015, but instead of using TensorFlow directly we\'ll use GPflow. GPflow is typically used for building Gaussian Process-based models, but the framework contains many useful methods and classes that can be used to quickly prototype a wide variety of ML algorithms. Excellent for doing research!\n#\n# We start by explaining why MDNs can be useful. We then examine a GPflow implementation of the model and use it for a couple of toy experiments.\n#\n\n# %% [markdown]\n# ## Conditional Density Estimation models\n# Imagine we are interested in performing regression on the following dataset.\n\n# %%\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\n\nnp.random.seed(1)  # for reproducibility of this notebook\n\nCMAP = plt.get_cmap(""Blues"")\n\n# %%\nN = 200\nNOISE_STD = 5.0e-2\n\n\ndef sinusoidal_data(N, noise):\n    Y = np.linspace(-2, 2, N)[:, None]\n    X = np.sin(4 * Y) * 2.0 + Y * 0.5\n    X += np.random.randn(N, 1) * noise\n    Y += np.random.randn(N, 1) * noise\n    return X, Y\n\n\nX, Y = data = sinusoidal_data(N, NOISE_STD)\n\nplt.plot(X, Y, ""ro"", alpha=0.3)\nplt.xlabel(""$x$"")\n_ = plt.ylabel(""$y$"")\n\n# %% [markdown]\n# At first sight, this dataset doesn\'t seem overly complex. Both input and output have a single dimension, and the data has a clear sinusoidal pattern. However, notice that a single input $x$ can correspond to multiple output values $y$, so for example $x=0$ can yield any of the values $\\{-1.5, -3/4, 0, 0.8, 1.5\\}$. Typical regression algorithms such as Linear Regression, Gaussian Process regression and Multilayer Perceptrons (MLPs) struggle as they can only predict one output value for every input.\n#\n# To model this dataset we can use a Conditional Density Estimation (CDE) model. CDE models infer $p(f(x)|x)$ instead of just calculating the expectation $E[f(x) | x]$. Modeling the complete distribution $p(f(x)|x)$ is typically harder but it reveals more interesting properties, such as the modes, outlier boundaries and samples. A real-world example might be modeling taxi drop-offs, conditioned on the pick-up location. We would expect a taxi drop-off location to be multi-modal as passengers need to go to different destinations (airport/city center/suburbs and so on) and the density depends on the starting location [2].\n\n# %% [markdown]\n# ## Mixture Density Network models\n#\n# Mixture Density Networks (MDNs) are a parametric class of models that allow for conditional density estimation. They consist of two parts: a neural net and a Mixture of Gaussians (MoG). The neural net is responsible for producing the characteristics of the MoG. In practice, given that the MoG consists of $M$ Gaussians, the neural net will output a collection of $M$ means, variances and weights $\\{\\mu_m, \\sigma_m^2, \\pi_m\\}_{m=1}^M$. These means, variances and weights are used to define the conditional probability distribution function:\n# \\begin{equation}\n# p(Y = y\\,|\\,X = x) = \\sum_{m=1}^{M} \\pi_{m}(x)\\,\\mathcal{N}\\big(y\\, \\left|\\,\\mu_{m}(x), \\sigma_{m}^2(x)\\big)\\right.\n# \\end{equation}\n#\n# Each of the parameters  $\\pi_{m}(x), \\mu_{m}(x), \\sigma_{m}(x)$ of the distribution are determined by the neural net, as a function of the input $x$.\n#\n# We train the MDN\'s neural net by optimizing the model\'s likelihood:\n# \\begin{equation}\n# \\mathcal{L} \\triangleq \\text{argmax}_{\\Theta} \\prod_{n=1}^N p(Y = y_n | X = x_n)\n# \\end{equation}\n#\n# where $\\Theta$ collects the neural net\'s weights and biases and $\\{x_n, y_n\\}_{n=1}^N$ represents our training dataset.\n\n# %% [markdown]\n# ## A GPflow MDN implementation\n#\n# GPflow doesn\'t reinvent the wheel; most of what follows is just plain Python/TensorFlow code. We choose to use GPflow, however, because it provides us with functionality to easily define a model. Once we have a GPflow model, we can specify its objective function, parameters and dataset. This extra layer of abstraction makes interacting with the model much easier, for example optimizing or performing inference.\n#\n# We begin by importing the required packages from TensorFlow and GPflow.\n\n# %%\nimport tensorflow as tf\n\n# %%\nimport gpflow\nfrom gpflow.models import BayesianModel, ExternalDataTrainingLossMixin\nfrom gpflow.base import Parameter\n\n# %% [markdown]\n# Next, we create a `MDN` class that inherits from GPflow\'s `Model` class. We need to do the following:\n# 1. Store each of the feature and target matrices (X, Y) as a `DataHolder` object.\n# 2. Define our model\'s parameters using GPflow\'s `Parameter` and `ParamList` objects.\n# 3. Define the objective function using the `_build_likelihood` method. When we optimize the model the negative of this function will be minimized.\n\n# %%\nfrom typing import Callable, Optional, Tuple\n\n\nclass MDN(BayesianModel, ExternalDataTrainingLossMixin):\n    def __init__(\n        self,\n        num_mixtures: Optional[int] = 5,\n        inner_dims: Optional[list] = [10, 10,],\n        activation: Optional[Callable[[tf.Tensor], tf.Tensor]] = tf.keras.activations.relu,\n    ):\n        super().__init__()\n\n        # `self.dims` collects the neural net\'s input, hidden and output dimensions.\n        # The number of output dims `self.dims[-1]` equals `num_mixtures` means +\n        # `num _mixtures` variances + `num_mixtures` weights, a total of\n        # 3 times `num_mixtures` variables.\n        self.dims = [1,] + list(inner_dims) + [3 * num_mixtures]\n        self.activation = activation\n        self._create_network()\n\n    def _create_network(self):\n        self.Ws, self.bs = [], []\n\n        for dim_in, dim_out in zip(self.dims[:-1], self.dims[1:]):\n            init_xavier_std = (2.0 / (dim_in + dim_out)) ** 0.5\n            self.Ws.append(Parameter(np.random.randn(dim_in, dim_out) * init_xavier_std))\n            self.bs.append(Parameter(np.zeros(dim_out)))\n\n    def eval_network(self, X):\n        for i, (W, b) in enumerate(zip(self.Ws, self.bs)):\n            X = tf.matmul(X, W) + b\n            if i < len(self.bs) - 1:\n                X = self.activation(X)\n\n        pis, mus, sigmas = tf.split(X, 3, axis=1)\n        pis = tf.nn.softmax(pis)  # make sure they normalize to 1\n        sigmas = tf.exp(sigmas)  # make sure std. dev. are positive\n\n        return pis, mus, sigmas\n\n    def maximum_log_likelihood_objective(self, data: Tuple[tf.Tensor, tf.Tensor]):\n        x, y = data\n        pis, mus, sigmas = self.eval_network(x)\n        Z = (2 * np.pi) ** 0.5 * sigmas\n        log_probs_mog = (-0.5 * (mus - y) ** 2 / sigmas ** 2) - tf.math.log(Z) + tf.math.log(pis)\n        log_probs = tf.reduce_logsumexp(log_probs_mog, axis=1)\n        return tf.reduce_sum(log_probs)\n\n\n# %% [markdown]\n# ### Notes\n# - Given we are dealing with a MoG, the neural net output must comply with the following restrictions:\n#   \\begin{equation}\n#   \\sum_{m=1}^{M} \\pi_{m}(x) = 1, \\pi_m \\ge 0\\ \\text{and}\\ \\sigma_m\\ \\forall\\ m\n#   \\end{equation}\n#   We achieve this by applying the `softmax` operator to the $\\pi$\'s and by taking the `exp` to the $\\sigma$\'s.\n#\n# - We use the ""Xavier"" initialization for the neural net\'s weights. (Glorot and Bengio, 2010).\n#\n# - Instead of calculating the pdf of the Gaussians, we work with the pdf `log` and use `tf.reduce_logsumexp`. This is mainly for numerical stability.\n\n# %% [markdown]\n# ## Experiment 1: The sinusoidal dataset\n#\n# Let\'s see how our model works in practice with the sinusoidal dataset presented earlier. We do this by initializing a new instance of our MDN model, and then specifying the dataset $(X, Y)$, the number of hidden units of the MDN\'s neural net, and the number of mixture components $M$.\n\n# %%\nmodel = MDN(inner_dims=[100, 100], num_mixtures=5)\n\nfrom gpflow.utilities import print_summary\n\nprint_summary(model)\n\n# %% [markdown]\n# The objective function for MDN instances is the `maximum_log_likelihood_objective`, which we use for optimization of the parameters. GPflow ensures that only the variables stored in `Parameter` objects are optimized. For the MDN, the only parameters are the weights and the biases of the neural net.\n#\n# We use the `Scipy` optimizer, which is a wrapper around SciPy\'s L-BFGS optimization algorithm. Note that GPflow supports other TensorFlow optimizers such as `Adam`, `Adagrad`, and `Adadelta` as well.\n\n# %%\nfrom gpflow.optimizers import Scipy\nfrom gpflow.ci_utils import ci_niter\n\nScipy().minimize(\n    model.training_loss_closure(data, compile=True),\n    model.trainable_variables,\n    options=dict(maxiter=ci_niter(1500)),\n)\n\nprint(""Final Likelihood"", model.maximum_log_likelihood_objective(data).numpy())\n\n# %% [markdown]\n# To evaluate the validity of our model, we draw the posterior density. We also plot $\\mu(x)$ of the optimized neural net. Remember that for every $x$ the neural net outputs $M$ means $\\mu_m(x)$. These determine the location of the Gaussians. We plot all $M$ means and use their corresponding mixture weight $\\pi_m(X)$ to determine their size. Larger dots will have more impact in the Gaussian ensemble.\n\n# %%\ntry:\n    from mdn_plotting import plot\nexcept:\n    # VS CODE\'s root directory is GPflow\'s top-level directory\n    from doc.source.notebooks.tailor.mdn_plotting import plot\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nfor a in axes:\n    a.set_xlim(-4, 4)\n    a.set_ylim(-3, 3)\nplot(model, X, Y, axes, cmap=CMAP)\n\n# %% [markdown]\n# ## Experiment 2: The half moon dataset\n#\n# The half moon dataset is available in the `scikit-learn` package.\n\n# %%\nfrom sklearn.datasets import make_moons\n\n\ndef moon_data(N, noise):\n    data, _ = make_moons(n_samples=N, shuffle=True, noise=noise)\n    X, Y = data[:, 0].reshape(-1, 1), data[:, 1].reshape(-1, 1)\n    return X, Y\n\n\n# %%\nX, Y = data = moon_data(N, NOISE_STD)\nplt.plot(X, Y, ""ro"", alpha=0.3)\nplt.xlabel(""$x$"")\n_ = plt.ylabel(""$y$"")\n\n# %% [markdown]\n# The only difference in the MDN\'s setup is that we lower the number of mixture components.\n\n# %%\nmodel = MDN(inner_dims=[100, 100], num_mixtures=5)\n\n# %%\nScipy().minimize(\n    model.training_loss_closure(data, compile=True),\n    model.trainable_variables,\n    options=dict(maxiter=ci_niter(int(10e3))),\n)\n\nprint(""Final Likelihood"", model.maximum_log_likelihood_objective(data).numpy())\n\n# %%\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nfor a in axes:\n    a.set_xlim(-2, 3)\n    a.set_ylim(-1.5, 2)\nplot(model, X, Y, axes, cmap=CMAP)\n\n# %% [markdown]\n# ## References\n#\n# [1] Bishop, Christopher M. Mixture density networks. Technical Report NCRG/4288, Aston University, Birmingham, UK, 1994.\n#\n# [2] Dutordoir, Vincent, et al. ""Gaussian Process Conditional Density Estimation."" Advances in Neural Information Processing Systems. 2018.\n'"
doc/source/notebooks/tailor/models_with_latent_variables.pct.py,0,"b""# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# Models with observed and latent variables\n# --\n#\n# no material so far...\n"""
doc/source/notebooks/tailor/updating_models_with_new_data.pct.py,0,"b""# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# Updating model with new data\n# --\n#\n# No material so far...\n"""
doc/source/notebooks/theory/FITCvsVFE.pct.py,0,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Comparing FITC approximation to VFE approximation\n# This notebook examines why we prefer the Variational Free Energy (VFE) objective to the Fully Independent Training Conditional (FITC) approximation for our sparse approximations.\n#\n\n# %%\nimport gpflow\nimport tensorflow as tf\nfrom gpflow.ci_utils import ci_niter  # to speed up automated testing of this notebook\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\n\nfrom FITCvsVFE import (\n    getTrainingTestData,\n    printModelParameters,\n    plotPredictions,\n    repeatMinimization,\n    stretch,\n    plotComparisonFigure,\n)\n\nimport logging\n\n# logging.disable(logging.WARN)  # do not clutter up the notebook with optimization warnings\n\n# %% [markdown]\n# First, we load the training data and plot it together with the exact GP solution (using the `GPR` model):\n\n# %%\n# Load the training data:\nXtrain, Ytrain, Xtest, Ytest = getTrainingTestData()\n\n\ndef getKernel():\n    return gpflow.kernels.SquaredExponential()\n\n\n# Run exact inference on training data:\nexact_model = gpflow.models.GPR((Xtrain, Ytrain), kernel=getKernel())\n\nopt = gpflow.optimizers.Scipy()\nopt.minimize(\n    exact_model.training_loss,\n    exact_model.trainable_variables,\n    method=""L-BFGS-B"",\n    options=dict(maxiter=ci_niter(20000)),\n    tol=1e-11,\n)\n\nprint(""Exact model parameters:"")\nprintModelParameters(exact_model)\n\nfigA, ax = plt.subplots(1, 1)\nax.plot(Xtrain, Ytrain, ""ro"")\nplotPredictions(ax, exact_model, color=""g"")\n\n\n# %%\ndef initializeHyperparametersFromExactSolution(sparse_model):\n    sparse_model.likelihood.variance.assign(exact_model.likelihood.variance)\n    sparse_model.kernel.variance.assign(exact_model.kernel.variance)\n    sparse_model.kernel.lengthscales.assign(exact_model.kernel.lengthscales)\n\n\n# %% [markdown]\n# We now construct two sparse model using the VFE (`SGPR` model) and FITC (`GPRFITC` model) optimization objectives, with the inducing points being initialized on top of the training inputs, and the model hyperparameters (kernel variance and lengthscales, and likelihood variance) being initialized to the values obtained in the optimization of the exact `GPR` model:\n\n# %%\n# Train VFE model initialized from the perfect solution.\nVFEmodel = gpflow.models.SGPR((Xtrain, Ytrain), kernel=getKernel(), inducing_variable=Xtrain.copy())\n\ninitializeHyperparametersFromExactSolution(VFEmodel)\n\nVFEcb = repeatMinimization(VFEmodel, Xtest, Ytest)  # optimize with several restarts\nprint(""Sparse model parameters after VFE optimization:"")\nprintModelParameters(VFEmodel)\n\n# %%\n# Train FITC model initialized from the perfect solution.\nFITCmodel = gpflow.models.GPRFITC(\n    (Xtrain, Ytrain), kernel=getKernel(), inducing_variable=Xtrain.copy()\n)\n\ninitializeHyperparametersFromExactSolution(FITCmodel)\n\nFITCcb = repeatMinimization(FITCmodel, Xtest, Ytest)  # optimize with several restarts\nprint(""Sparse model parameters after FITC optimization:"")\nprintModelParameters(FITCmodel)\n\n# %% [markdown]\n# Plotting a comparison of the two algorithms, we see that VFE stays at the optimum of exact GPR, whereas the FITC approximation eventually ends up with several inducing points on top of each other, and a worse fit:\n\n# %%\nfigB, axes = plt.subplots(3, 2, figsize=(20, 16))\n\n# VFE optimization finishes after 10 iterations, so we stretch out the training and test\n# log-likelihood traces to make them comparable against FITC:\nVFEiters = FITCcb.n_iters\nVFElog_likelihoods = stretch(len(VFEiters), VFEcb.log_likelihoods)\nVFEhold_out_likelihood = stretch(len(VFEiters), VFEcb.hold_out_likelihood)\n\naxes[0, 0].set_title(""VFE"", loc=""center"", fontdict={""fontsize"": 22})\nplotComparisonFigure(\n    Xtrain,\n    VFEmodel,\n    exact_model,\n    axes[0, 0],\n    axes[1, 0],\n    axes[2, 0],\n    VFEiters,\n    VFElog_likelihoods,\n    VFEhold_out_likelihood,\n)\n\naxes[0, 1].set_title(""FITC"", loc=""center"", fontdict={""fontsize"": 22})\nplotComparisonFigure(\n    Xtrain,\n    FITCmodel,\n    exact_model,\n    axes[0, 1],\n    axes[1, 1],\n    axes[2, 1],\n    FITCcb.n_iters,\n    FITCcb.log_likelihoods,\n    FITCcb.hold_out_likelihood,\n)\n\n# %% [markdown]\n# A more detailed discussion of the comparison between these sparse approximations can be found in [Understanding Probabilistic Sparse Gaussian Process Approximations](http://papers.nips.cc/paper/6477-understanding-probabilistic-sparse-gaussian-process-approximations) by Bauer, van der Wilk, and Rasmussen (2017).\n'"
doc/source/notebooks/theory/FITCvsVFE.py,1,"b'import gpflow\nfrom gpflow.ci_utils import ci_niter\nimport tensorflow as tf\nimport numpy as np\n\nnRepeats = ci_niter(50)\n\npredict_limits = [-4.0, 4.0]\ninducing_points_limits = [-1.0, 9]\nhold_out_limits = [0.20, 0.60]\noptimization_limits = [18.0, 25.0]\n\n\ndef readCsvFile(fileName):\n    return np.loadtxt(fileName).reshape(-1, 1)\n\n\ndef getTrainingTestData():\n    overallX = readCsvFile(""data/snelson_train_inputs.dat"")\n    overallY = readCsvFile(""data/snelson_train_outputs.dat"")\n\n    trainIndices = []\n    testIndices = []\n\n    nPoints = overallX.shape[0]\n\n    for index in range(nPoints):\n        if index % 4 == 0:\n            trainIndices.append(index)\n        else:\n            testIndices.append(index)\n\n    Xtrain = overallX[trainIndices, :]\n    Xtest = overallX[testIndices, :]\n    Ytrain = overallY[trainIndices, :]\n    Ytest = overallY[testIndices, :]\n\n    return Xtrain, Ytrain, Xtest, Ytest\n\n\ndef getLogPredictiveDensities(targetValues, means, variances):\n    assert targetValues.shape == means.shape\n    assert variances.shape == means.shape\n\n    deltas = targetValues - means\n    mahalanobisTerms = -0.5 * deltas ** 2 / variances\n    normalizationTerms = -0.5 * np.log(variances) - 0.5 * np.log(2.0 * np.pi)\n    return mahalanobisTerms + normalizationTerms\n\n\ndef getKernel():\n    return gpflow.kernels.SquaredExponential()\n\n\ndef getRegressionModel(X, Y):\n    m = gpflow.models.GPR(X, Y, kern=getKernel())\n    m.likelihood.variance = 1.0\n    m.kern.lengthscales = 1.0\n    m.kern.variance = 1.0\n    return m\n\n\ndef getSparseModel(X, Y, isFITC=False):\n    if isFITC:\n        m = gpflow.models.GPRFITC((X, Y), kernel=getKernel(), inducing_variable=X.copy())\n    else:\n        m = gpflow.models.SGPR((X, Y), kernel=getKernel(), inducing_variable=X.copy())\n    return m\n\n\ndef printModelParameters(model):\n    print(""  Likelihood variance = {:.5g}"".format(model.likelihood.variance.numpy()))\n    print(""  Kernel variance     = {:.5g}"".format(model.kernel.variance.numpy()))\n    print(""  Kernel lengthscale  = {:.5g}"".format(model.kernel.lengthscales.numpy()))\n\n\ndef plotPredictions(ax, model, color, label=None):\n    Xtest = np.sort(readCsvFile(""data/snelson_test_inputs.dat""))\n    predMean, predVar = model.predict_y(Xtest)\n    ax.plot(Xtest, predMean, color, label=label)\n    ax.plot(Xtest, predMean + 2.0 * np.sqrt(predVar), color)\n    ax.plot(Xtest, predMean - 2.0 * np.sqrt(predVar), color)\n\n\ndef repeatMinimization(model, Xtest, Ytest):\n    callback = Callback(model, Xtest, Ytest)\n\n    opt = gpflow.optimizers.Scipy()\n    # print(""Optimising for {} repetitions"".format(nRepeats))\n    for repeatIndex in range(nRepeats):\n        # print(repeatIndex)\n        opt.minimize(\n            model.training_loss,\n            model.trainable_variables,\n            method=""L-BFGS-B"",\n            tol=1e-11,\n            options=dict(disp=False, maxiter=ci_niter(2000)),\n            step_callback=callback,\n            compile=True,\n        )\n    return callback\n\n\ndef trainSparseModel(Xtrain, Ytrain, exact_model, isFITC, Xtest, Ytest):\n    sparse_model = getSparseModel(Xtrain, Ytrain, isFITC)\n    sparse_model.likelihood.variance = exact_model.likelihood.variance.read_value().copy()\n    sparse_model.kern.lengthscales = exact_model.kern.lengthscales.read_value().copy()\n    sparse_model.kern.variance = exact_model.kern.variance.read_value().copy()\n    return sparse_model, repeatMinimization(sparse_model, Xtest, Ytest)\n\n\ndef plotComparisonFigure(\n    Xtrain,\n    sparse_model,\n    exact_model,\n    ax_predictions,\n    ax_inducing_points,\n    ax_optimization,\n    iterations,\n    log_likelihoods,\n    hold_out_likelihood,\n    title=None,\n):\n    plotPredictions(ax_predictions, exact_model, ""g"", label=""Exact"")\n    plotPredictions(ax_predictions, sparse_model, ""b"", label=""Approximate"")\n    ax_predictions.legend(loc=9)\n    ax_predictions.plot(\n        sparse_model.inducing_variable.Z.numpy(), -1.0 * np.ones(Xtrain.shape), ""ko""\n    )\n    ax_predictions.set_ylim(predict_limits)\n    ax_inducing_points.plot(Xtrain, sparse_model.inducing_variable.Z.numpy(), ""bo"")\n    xs = np.linspace(ax_inducing_points.get_xlim()[0], ax_inducing_points.get_xlim()[1], 200)\n    ax_inducing_points.plot(xs, xs, ""g"")\n    ax_inducing_points.set_xlabel(""Optimal inducing point position"")\n    ax_inducing_points.set_ylabel(""Learnt inducing point position"")\n    ax_inducing_points.set_ylim(inducing_points_limits)\n    ax_optimization.plot(iterations, -1.0 * np.array(log_likelihoods), ""g-"")\n    ax_optimization.set_ylim(optimization_limits)\n    ax2 = ax_optimization.twinx()\n    ax2.plot(iterations, -1.0 * np.array(hold_out_likelihood), ""b-"")\n    ax_optimization.set_xlabel(""Minimization iterations"")\n    ax_optimization.set_ylabel(""Minimization objective"", color=""g"")\n    ax2.set_ylim(hold_out_limits)\n    ax2.set_ylabel(""Hold out negative log likelihood"", color=""b"")\n\n\nclass Callback:\n    def __init__(self, model, Xtest, Ytest, holdout_interval=10):\n        self.model = model\n        self.holdout_interval = holdout_interval\n        self.Xtest = Xtest\n        self.Ytest = Ytest\n        self.log_likelihoods = []\n        self.hold_out_likelihood = []\n        self.n_iters = []\n        self.counter = 0\n\n    def __call__(self, step, variables, values):\n        # step will reset to zero between calls to minimize(), whereas counter will keep increasing\n\n        if (self.counter <= 10) or (self.counter % self.holdout_interval) == 0:\n            for var, val in zip(variables, values):\n                var.assign(val)\n\n            self.n_iters.append(self.counter)\n            self.log_likelihoods.append(self.model.maximum_log_likelihood_objective().numpy())\n\n            predictive_mean, predictive_variance = self.model.predict_y(self.Xtest)\n            test_log_likelihood = tf.reduce_mean(\n                getLogPredictiveDensities(self.Ytest, predictive_mean, predictive_variance)\n            )\n            self.hold_out_likelihood.append(test_log_likelihood.numpy())\n\n        self.counter += 1\n\n\ndef stretch(lenNIters, initialValues):\n    stretched = np.ones(lenNIters) * initialValues[-1]\n    stretched[0 : len(initialValues)] = initialValues\n    return stretched\n\n\ndef snelsonDemo():\n    from matplotlib import pyplot as plt\n    from IPython import embed\n\n    Xtrain, Ytrain, Xtest, Ytest = getTrainingTestData()\n\n    # run exact inference on training data.\n    exact_model = getRegressionModel(Xtrain, Ytrain)\n    opt = gpflow.train.ScipyOptimizer()\n    opt.minimize(exact_model, maxiter=ci_niter(2000000))\n\n    figA, axes = plt.subplots(1, 1)\n    inds = np.argsort(Xtrain.flatten())\n    axes.plot(Xtrain[inds, :], Ytrain[inds, :], ""ro"")\n    plotPredictions(axes, exact_model, ""g"", None)\n\n    figB, axes = plt.subplots(3, 2)\n\n    # run sparse model on training data initialized from exact optimal solution.\n    VFEmodel, VFEcb = trainSparseModel(Xtrain, Ytrain, exact_model, False, Xtest, Ytest)\n    FITCmodel, FITCcb = trainSparseModel(Xtrain, Ytrain, exact_model, True, Xtest, Ytest)\n\n    print(""Exact model parameters \\n"")\n    printModelParameters(exact_model)\n    print(""Sparse model parameters for VFE optimization \\n"")\n    printModelParameters(VFEmodel)\n    print(""Sparse model parameters for FITC optimization \\n"")\n    printModelParameters(FITCmodel)\n\n    VFEiters = FITCcb.n_iters\n    VFElog_likelihoods = stretch(len(VFEiters), VFEcb.log_likelihoods)\n    VFEhold_out_likelihood = stretch(len(VFEiters), VFEcb.hold_out_likelihood)\n\n    plotComparisonFigure(\n        Xtrain,\n        VFEmodel,\n        exact_model,\n        axes[0, 0],\n        axes[1, 0],\n        axes[2, 0],\n        VFEiters,\n        VFElog_likelihoods,\n        VFEhold_out_likelihood,\n        ""VFE"",\n    )\n    plotComparisonFigure(\n        Xtrain,\n        FITCmodel,\n        exact_model,\n        axes[0, 1],\n        axes[1, 1],\n        axes[2, 1],\n        FITCcb.n_iters,\n        FITCcb.log_likelihoods,\n        FITCcb.hold_out_likelihood,\n        ""FITC"",\n    )\n\n    axes[0, 0].set_title(""VFE"", loc=""center"", fontdict={""fontsize"": 22})\n    axes[0, 1].set_title(""FITC"", loc=""center"", fontdict={""fontsize"": 22})\n\n    embed()\n\n\nif __name__ == ""__main__"":\n    snelsonDemo()\n'"
doc/source/notebooks/theory/Sanity_check.pct.py,3,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# ## Sanity checking when model behaviours should overlap\n#\n# Many of the model classes in GPflow have overlapping behaviour in special cases. In this notebook, we fit some approximations to a model with a Gaussian likelihood, and make sure they\'re all the same.\n#\n# The models are:\n#  - `GPR`: Full Gaussian process regression.\n#\n#  - `VGP`: A Gaussian approximation with Variational Bayes.\n#    Approximating a Gaussian posterior with a Gaussian should be exact.\n#\n#  - `SVGP`: a sparse GP, with a Gaussian approximation. The inducing points are set to be at the data points, so again, should be exact.\n#\n#  - `SVGP` (with whitened representation): As above, but with a rotation applied to whiten the representation of the process.\n#\n#  - `SGPR`: A sparse GP with a *collapsed* posterior (Titsias 2009). Again, the inducing points are fixed to the data points.\n#\n#  - `GPRFITC`: The FITC approximation. Again, the inducing points are fixed to the data points.\n#\n# In all cases the parameters are estimated by the method of maximum likelihood (or approximate maximum likelihood, as appropriate). The parameter estimates should all be the same.\n\n# %%\nimport gpflow\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom gpflow import set_trainable\nfrom gpflow.models import maximum_log_likelihood_objective, training_loss_closure\nfrom gpflow.config import default_float\nfrom gpflow.ci_utils import ci_niter\n\n# %matplotlib inline\nplt.rcParams[""figure.figsize""] = (12, 6)\n\n# %%\nnp.random.seed(0)\nX = np.random.rand(20, 1) * 10\nY = np.sin(X) + 0.9 * np.cos(X * 1.6) + np.random.randn(*X.shape) * 0.4\nXtest = np.random.rand(10, 1) * 10\n_ = plt.plot(X, Y, ""kx"", mew=2)\n\n# %%\ndata = (\n    tf.convert_to_tensor(X, dtype=default_float()),\n    tf.convert_to_tensor(Y, dtype=default_float()),\n)\ninducing_variable = tf.convert_to_tensor(X, dtype=default_float())\n\nm1 = gpflow.models.GPR(data, kernel=gpflow.kernels.SquaredExponential())\nm2 = gpflow.models.VGP(\n    data, kernel=gpflow.kernels.SquaredExponential(), likelihood=gpflow.likelihoods.Gaussian()\n)\nm3 = gpflow.models.SVGP(\n    gpflow.kernels.SquaredExponential(),\n    gpflow.likelihoods.Gaussian(),\n    inducing_variable,\n    q_diag=False,\n)\nset_trainable(m3.inducing_variable, False)\n\nm4 = gpflow.models.SVGP(\n    gpflow.kernels.SquaredExponential(),\n    gpflow.likelihoods.Gaussian(),\n    inducing_variable,\n    q_diag=False,\n    whiten=True,\n)\nset_trainable(m4.inducing_variable, False)\n\nm5 = gpflow.models.SGPR(\n    data, kernel=gpflow.kernels.SquaredExponential(), inducing_variable=inducing_variable\n)\nset_trainable(m5.inducing_variable, False)\n\nm6 = gpflow.models.GPRFITC(\n    data, kernel=gpflow.kernels.SquaredExponential(), inducing_variable=inducing_variable\n)\nset_trainable(m6.inducing_variable, False)\n\nmodels = [m1, m2, m3, m4, m5, m6]\n\n# %% [markdown]\n# Now, we optimize the models. For `GPR`, `SVGP`, and `GPRFITC`, this simply optimizes the hyperparameters (since the inducing points are fixed). For the variational models, this jointly maximises the lower bound to the marginal likelihood (Evidence Lower Bound, ELBO) with respect to the variational parameters and the kernel and likelihood hyperparameters.\n\n# %%\nfor m in models:\n    opt = gpflow.optimizers.Scipy()\n    loss_closure = training_loss_closure(m, data)\n    opt.minimize(\n        loss_closure,\n        variables=m.trainable_variables,\n        options=dict(maxiter=ci_niter(1000)),\n        compile=True,\n    )\n\n\n# %% [markdown]\n# If everything worked as planned, the models should have the same:\n#\n#  - prediction functions\n#  - log (marginal) likelihood\n#  - kernel parameters\n#\n# For the variational models, where we use a ELBO in place of the likelihood, the ELBO should be tight to the likelihood in the cases studied here.\n\n# %%\ndef plot(m, color, ax):\n    xx = np.linspace(-1, 11, 100)[:, None]\n    mu, var = m.predict_y(xx)\n    ax.plot(xx, mu, color, lw=2)\n    ax.fill_between(\n        xx[:, 0],\n        mu[:, 0] - 2 * np.sqrt(var[:, 0]),\n        mu[:, 0] + 2 * np.sqrt(var[:, 0]),\n        color=color,\n        alpha=0.2,\n    )\n    ax.plot(X, Y, ""kx"", mew=2)\n    ax.set_xlim(-1, 11)\n\n\nf, ax = plt.subplots(3, 2, sharex=True, sharey=True, figsize=(12, 9))\nplot(m1, ""C0"", ax[0, 0])\nplot(m2, ""C1"", ax[1, 0])\nplot(m3, ""C2"", ax[0, 1])\nplot(m4, ""C3"", ax[1, 1])\nplot(m5, ""C4"", ax[2, 0])\nplot(m6, ""C5"", ax[2, 1])\n\n# %% [markdown]\n# Here are the kernels and likelihoods, which show the fitted kernel parameters and noise variance:\n\n# %%\nfor m in models:\n    print(m.__class__.__name__)\n    print(f""  kernel lengthscale  = {m.kernel.lengthscales.numpy():.5g}"")\n    print(f""  kernel variance     = {m.kernel.variance.numpy():.5}"")\n    print(f""  likelihood variance = {m.likelihood.variance.numpy():.5}"")\n\n# %% [markdown]\n# Here are the likelihoods (or ELBOs):\n\n# %%\nfor m in models:\n    print(f""{m.__class__.__name__:30}  {maximum_log_likelihood_objective(m, data)}"")\n'"
doc/source/notebooks/theory/upper_bound.pct.py,2,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Discussion of the GP marginal likelihood upper bound\n#\n# See the [`gp_upper` repository](https://github.com/markvdw/gp_upper) by Mark van der Wilk for code to tighten the upper bound through optimization, and a more comprehensive discussion.\n\n# %%\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\nplt.rcParams[""figure.figsize""] = (12, 6)\n\nimport numpy as np\nimport tensorflow as tf\n\nimport gpflow\nfrom gpflow import set_trainable\nfrom gpflow.utilities import print_summary\nfrom gpflow.ci_utils import ci_niter\n\nimport logging\n\nlogging.disable(logging.WARNING)\n\nnp.random.seed(1)\n\nfrom FITCvsVFE import getTrainingTestData\n\n# %%\nX, Y, Xt, Yt = getTrainingTestData()\n\n\n# %%\ndef plot_model(m, name=""""):\n    pX = np.linspace(-3, 9, 100)[:, None]\n    pY, pYv = m.predict_y(pX)\n    plt.plot(X, Y, ""x"")\n    plt.plot(pX, pY)\n    if not isinstance(m, gpflow.models.GPR):\n        Z = m.inducing_variable.Z.numpy()\n        plt.plot(Z, np.zeros_like(Z), ""o"")\n    two_sigma = (2.0 * pYv ** 0.5)[:, 0]\n    plt.fill_between(pX[:, 0], pY[:, 0] - two_sigma, pY[:, 0] + two_sigma, alpha=0.15)\n    lml = m.maximum_log_likelihood_objective().numpy()\n    plt.title(""%s (lml = %f)"" % (name, lml))\n    return lml\n\n\n# %% [markdown]\n# ## Full model\n\n# %%\ngpr = gpflow.models.GPR((X, Y), gpflow.kernels.SquaredExponential())\ngpflow.optimizers.Scipy().minimize(\n    gpr.training_loss, gpr.trainable_variables, options=dict(maxiter=ci_niter(1000))\n)\nfull_lml = plot_model(gpr)\n\n# %% [markdown]\n# ## Upper bounds for sparse variational models\n# As a first investigation, we compute the upper bound for models trained using the sparse variational GP approximation.\n\n# %%\nMs = np.arange(4, ci_niter(20, test_n=6), 1)\nvfe_lml = []\nvupper_lml = []\nvfe_hyps = []\nfor M in Ms:\n    Zinit = X[:M, :].copy()\n    vfe = gpflow.models.SGPR((X, Y), gpflow.kernels.SquaredExponential(), inducing_variable=Zinit)\n    gpflow.optimizers.Scipy().minimize(\n        vfe.training_loss,\n        vfe.trainable_variables,\n        options=dict(disp=False, maxiter=ci_niter(1000), compile=True),\n    )\n\n    vfe_lml.append(vfe.elbo().numpy())\n    vupper_lml.append(vfe.upper_bound().numpy())\n    vfe_hyps.append([(p.name, p.numpy()) for p in vfe.trainable_parameters])\n    print(""%i"" % M, end="" "")\n\n# %%\nplt.title(""LML bounds for models trained with SGPR"")\nplt.plot(Ms, vfe_lml, label=""lower"")\nplt.plot(Ms, vupper_lml, label=""upper"")\nplt.axhline(full_lml, label=""full"", alpha=0.3)\nplt.xlabel(""Number of inducing points"")\nplt.ylabel(""LML estimate"")\n_ = plt.legend()\n\n# %% [markdown]\n# We see that the lower bound increases as more inducing points are added. Note that the upper bound does _not_ monotonically decrease! This is because as we train the sparse model, we also get better estimates of the hyperparameters. The upper bound will be different for this different setting of the hyperparameters, and is sometimes looser. The upper bound also converges to the true lml slower than the lower bound.\n\n# %% [markdown]\n# ### Upper bounds for fixed hyperparameters\n# Here, we train sparse models with the hyperparameters fixed to the optimal value found previously.\n\n# %%\nfMs = np.arange(3, ci_niter(20, test_n=5), 1)\nfvfe_lml = []  # Fixed vfe lml\nfvupper_lml = []  # Fixed upper lml\n\ninit_params = gpflow.utilities.parameter_dict(vfe)\n\n# cannot copy this due to shape mismatch with different numbers of inducing points between models:\ndel init_params["".inducing_variable.Z""]\n\nfor M in fMs:\n    Zinit = vfe.inducing_variable.Z.numpy()[:M, :]\n    Zinit = np.vstack((Zinit, X[np.random.permutation(len(X))[: (M - len(Zinit))], :].copy()))\n\n    vfe = gpflow.models.SGPR((X, Y), gpflow.kernels.SquaredExponential(), inducing_variable=Zinit)\n\n    # copy hyperparameters (omitting inducing_variable.Z) from optimized model:\n    gpflow.utilities.multiple_assign(vfe, init_params)\n\n    set_trainable(vfe.kernel, False)\n    set_trainable(vfe.likelihood, False)\n\n    gpflow.optimizers.Scipy().minimize(\n        vfe.training_loss,\n        vfe.trainable_variables,\n        options=dict(disp=False, maxiter=ci_niter(1000)),\n        compile=True,\n    )\n\n    fvfe_lml.append(vfe.elbo().numpy())\n    fvupper_lml.append(vfe.upper_bound().numpy())\n    print(""%i"" % M, end="" "")\n\n# %%\nplt.plot(fMs, fvfe_lml, label=""lower"")\nplt.plot(fMs, fvupper_lml, label=""upper"")\nplt.axhline(full_lml, label=""full"", alpha=0.3)\nplt.xlabel(""Number of inducing points"")\nplt.ylabel(""LML estimate"")\n_ = plt.legend()\n\n# %%\nassert np.all(np.array(fvupper_lml) - np.array(fvfe_lml) > 0.0)\n\n# %% [markdown]\n# Now, as the hyperparameters are fixed, the bound _does_ monotonically decrease. We chose the optimal hyperparameters here, but the picture should be the same for any hyperparameter setting. This shows that we increasingly get a better estimate of the marginal likelihood as we add more inducing points.\n\n# %% [markdown]\n# ### A tight estimate bound does not imply a converged model\n\n# %%\nsingle_inducing_point = X[:1, :].copy()\nvfe = gpflow.models.SGPR(\n    (X, Y), gpflow.kernels.SquaredExponential(), inducing_variable=single_inducing_point\n)\nobjective = tf.function(vfe.training_loss)\ngpflow.optimizers.Scipy().minimize(\n    objective, vfe.trainable_variables, options=dict(maxiter=ci_niter(1000)), compile=False\n)\n# Note that we need to set compile=False here due to a discrepancy in compiling with tf.function\n# see https://github.com/GPflow/GPflow/issues/1260\n\nprint(""Lower bound: %f"" % vfe.elbo().numpy())\nprint(""Upper bound: %f"" % vfe.upper_bound().numpy())\n\n# %% [markdown]\n# In this case we show that for the hyperparameter setting, the bound is very tight. However, this does _not_ imply that we have enough inducing points, but simply that we have correctly identified the marginal likelihood for this particular hyperparameter setting. In this specific case, where we used a single inducing point, the model collapses to not using the GP at all (lengthscale is really long to model only the mean). The rest of the variance is explained by noise. This GP can be perfectly approximated with a single inducing point.\n\n# %%\nplot_model(vfe)\n\n# %%\nprint_summary(vfe, fmt=""notebook"")\n\n# %% [markdown]\n# This can be diagnosed by showing that there are other hyperparameter settings with higher upper bounds. This indicates that there might be better hyperparameter settings, but we cannot identify them due to the lack of inducing points. An example of this can be seen in the previous section.\n'"
doc/source/notebooks/understanding/architecture.pct.py,0,"b""# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# Architecture\n# --\n#\n#\n"""
doc/source/notebooks/understanding/models.pct.py,5,"b'# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: \'1.3\'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Manipulating GPflow models\n#\n# One of the key ingredients in GPflow is the model class, which enables you to carefully control parameters. This notebook shows how some of these parameter control features work, and how to build your own model with GPflow. First we\'ll look at:\n#\n#  - how to view models and parameters\n#  - how to set parameter values\n#  - how to constrain parameters (for example, variance > 0)\n#  - how to fix model parameters\n#  - how to apply priors to parameters\n#  - how to optimize models\n#\n# Then we\'ll show how to build a simple logistic regression model, demonstrating the ease of the parameter framework.\n#\n# GPy users should feel right at home, but there are some small differences.\n#\n# First, let\'s deal with the usual notebook boilerplate and make a simple GP regression model. See [Basic (Gaussian likelihood) GP regression model](../basics/regression.ipynb) for specifics of the model; we just want some parameters to play with.\n\n# %%\nimport numpy as np\nimport gpflow\nimport tensorflow_probability as tfp\nfrom gpflow.utilities import print_summary, set_trainable, to_default_float\n\n# %% [markdown]\n# We begin by creating a very simple GP regression model:\n\n# %%\n# generate toy data\nnp.random.seed(1)\nX = np.random.rand(20, 1)\nY = np.sin(12 * X) + 0.66 * np.cos(25 * X) + np.random.randn(20, 1) * 0.01\n\nm = gpflow.models.GPR((X, Y), kernel=gpflow.kernels.Matern32() + gpflow.kernels.Linear())\n\n# %% [markdown]\n# ## Viewing, getting, and setting parameters\n# You can display the state of the model in a terminal by using `print_summary(m)`. You can change the display format using the `fmt` keyword argument, e.g. `\'html\'`. In a notebook, you can also use `fmt=\'notebook\'` or set the default printing format as `notebook`:\n\n# %%\nprint_summary(m, fmt=""notebook"")\n\n# %%\ngpflow.config.set_default_summary_fmt(""notebook"")\n\n# %% [markdown]\n# This model has four parameters. The kernel is made of the sum of two parts. The first (counting from zero) is a Matern32 kernel that has a variance parameter and a lengthscales parameter; the second is a linear kernel that has only a variance parameter. There is also a parameter that controls the variance of the noise, as part of the likelihood.\n#\n# All the model variables have been initialized at `1.0`. You can access individual parameters in the same way that you display the state of the model in a terminal; for example, to see all the parameters that are part of the likelihood, run:\n\n# %%\nprint_summary(m.likelihood)\n\n# %% [markdown]\n# This gets more useful with more complex models!\n\n# %% [markdown]\n# To set the value of a parameter, just use `assign()`:\n\n# %%\nm.kernel.kernels[0].lengthscales.assign(0.5)\nm.likelihood.variance.assign(0.01)\nprint_summary(m, fmt=""notebook"")\n\n# %% [markdown]\n# ## Constraints and trainable variables\n#\n# GPflow helpfully creates an unconstrained representation of all the variables. In the previous example, all the variables are constrained positively (see the **transform** column in the table); the unconstrained representation is given by $\\alpha = \\log(\\exp(\\theta)-1)$. The `trainable_parameters` property returns the constrained values:\n\n# %%\nm.trainable_parameters\n\n# %% [markdown]\n# Each parameter has an `unconstrained_variable` attribute that enables you to access the unconstrained value as a TensorFlow `Variable`.\n\n# %%\np = m.kernel.kernels[0].lengthscales\np.unconstrained_variable\n\n# %% [markdown]\n# You can also check the unconstrained value as follows:\n\n# %%\np.transform.inverse(p)\n\n# %% [markdown]\n# Constraints are handled by the Bijector classes from the `tensorflow_probability` package. You might prefer to use the constraint $\\alpha = \\log(\\theta)$; this is easily done by replacing the parameter with one that has a different `transform` attribute (here we make sure to copy all other attributes across from the old parameter; this is not necessary when there is no `prior` and the `trainable` state is still the default of `True`):\n\n# %%\nold_parameter = m.kernel.kernels[0].lengthscales\nnew_parameter = gpflow.Parameter(\n    old_parameter,\n    trainable=old_parameter.trainable,\n    prior=old_parameter.prior,\n    name=old_parameter.name.split("":"")[0],  # tensorflow is weird and adds \':0\' to the name\n    transform=tfp.bijectors.Exp(),\n)\nm.kernel.kernels[0].lengthscales = new_parameter\n\n# %% [markdown]\n# Though the lengthscale itself remains the same, the unconstrained lengthscale has changed:\n\n# %%\np.transform.inverse(p)\n\n# %% [markdown]\n# You can also change the `transform` attribute in place:\n\n# %%\nm.kernel.kernels[0].variance.transform = tfp.bijectors.Exp()\n\n# %%\nprint_summary(m, fmt=""notebook"")\n\n# %% [markdown]\n# ## Changing whether a parameter will be trained in optimization\n#\n# Another helpful feature is the ability to fix parameters. To do this, simply set the `trainable` attribute to `False`; this is shown in the **trainable** column of the representation, and the corresponding variable is removed from the free state.\n\n# %%\nset_trainable(m.kernel.kernels[1].variance, False)\nprint_summary(m)\n\n# %%\nm.trainable_parameters\n\n# %% [markdown]\n# To unfix a parameter, just set the `trainable` attribute to `True` again.\n\n# %%\nset_trainable(m.kernel.kernels[1].variance, True)\nprint_summary(m)\n\n# %% [markdown]\n# **NOTE:** If you want to recursively change the `trainable` status of an object that *contains* parameters, you **must** use the `set_trainable()` utility function.\n#\n# A module (e.g. a model, kernel, likelihood, ... instance) does not have a `trainable` attribute:\n\n# %%\ntry:\n    m.kernel.trainable\nexcept AttributeError:\n    print(f""{m.kernel.__class__.__name__} does not have a trainable attribute"")\n\n# %%\nset_trainable(m.kernel, False)\nprint_summary(m)\n\n# %% [markdown]\n# ## Priors\n#\n# You can set priors in the same way as transforms and trainability, by using `tensorflow_probability` distribution objects. Let\'s set a Gamma prior on the variance of the Matern32 kernel.\n\n# %%\nk = gpflow.kernels.Matern32()\nk.variance.prior = tfp.distributions.Gamma(to_default_float(2), to_default_float(3))\n\nprint_summary(k)\n\n# %%\nm.kernel.kernels[0].variance.prior = tfp.distributions.Gamma(\n    to_default_float(2), to_default_float(3)\n)\nprint_summary(m)\n\n\n# %% [markdown]\n# ## Optimization\n#\n# To optimize your model, first create an instance of an optimizer (in this case, `gpflow.optimizers.Scipy`), which has optional arguments that are passed to `scipy.optimize.minimize` (we minimize the negative log likelihood). Then, call the `minimize` method of that optimizer, with your model as the optimization target. Variables that have priors are maximum a priori (MAP) estimated, that is, we add the log prior to the log likelihood, and otherwise use Maximum Likelihood.\n\n# %%\nopt = gpflow.optimizers.Scipy()\nopt.minimize(m.training_loss, variables=m.trainable_variables)\n\n# %% [markdown]\n# ## Building new models\n#\n# To build new models, you\'ll need to inherit from `gpflow.models.BayesianModel`.\n# Parameters are instantiated with `gpflow.Parameter`.\n# You might also be interested in `gpflow.Module` (a subclass of `tf.Module`), which acts as a \'container\' for `Parameter`s (for example, kernels are `gpflow.Module`s).\n#\n# In this very simple demo, we\'ll implement linear multiclass classification.\n#\n# There are two parameters: a weight matrix and a bias (offset). You can use\n# Parameter objects directly, like any TensorFlow tensor.\n#\n# The training objective depends on the type of model; it may be possible to\n# implement the exact (log)marginal likelihood, or only a lower bound to the\n# log marginal likelihood (ELBO). You need to implement this as the\n# `maximum_log_likelihood_objective` method. The `BayesianModel` parent class\n# provides a `log_posterior_density` method that returns the\n# `maximum_log_likelihood_objective` plus the sum of the log-density of any priors\n# on hyperparameters, which can be used for MCMC.\n# GPflow provides mixin classes that define a `training_loss` method\n# that returns the negative of (maximum likelihood objective + log prior\n# density) for MLE/MAP estimation to be passed to optimizer\'s `minimize`\n# method. Models that derive from `InternalDataTrainingLossMixin` are expected to store the data internally, and their `training_loss` does not take any arguments and can be passed directly to `minimize`.\n# Models that take data as an argument to their `maximum_log_likelihood_objective` method derive from `ExternalDataTrainingLossMixin`, which provides a `training_loss_closure` to take the data and return the appropriate closure for `optimizer.minimize`.\n# This is also discussed in the [GPflow with TensorFlow 2 notebook](../intro_to_gpflow2.ipynb).\n\n# %%\nimport tensorflow as tf\n\n\nclass LinearMulticlass(gpflow.models.BayesianModel, gpflow.models.InternalDataTrainingLossMixin):\n    # The InternalDataTrainingLossMixin provides the training_loss method.\n    # (There is also an ExternalDataTrainingLossMixin for models that do not encapsulate data.)\n\n    def __init__(self, X, Y, name=None):\n        super().__init__(name=name)  # always call the parent constructor\n\n        self.X = X.copy()  # X is a NumPy array of inputs\n        self.Y = Y.copy()  # Y is a 1-of-k (one-hot) representation of the labels\n\n        self.num_data, self.input_dim = X.shape\n        _, self.num_classes = Y.shape\n\n        # make some parameters\n        self.W = gpflow.Parameter(np.random.randn(self.input_dim, self.num_classes))\n        self.b = gpflow.Parameter(np.random.randn(self.num_classes))\n\n        # ^^ You must make the parameters attributes of the class for\n        # them to be picked up by the model. i.e. this won\'t work:\n        #\n        # W = gpflow.Parameter(...    <-- must be self.W\n\n    def maximum_log_likelihood_objective(self):\n        p = tf.nn.softmax(\n            tf.matmul(self.X, self.W) + self.b\n        )  # Parameters can be used like a tf.Tensor\n        return tf.reduce_sum(tf.math.log(p) * self.Y)  # be sure to return a scalar\n\n\n# %% [markdown]\n# ...and that\'s it. Let\'s build a really simple demo to show that it works.\n\n# %%\nnp.random.seed(123)\nX = np.vstack(\n    [\n        np.random.randn(10, 2) + [2, 2],\n        np.random.randn(10, 2) + [-2, 2],\n        np.random.randn(10, 2) + [2, -2],\n    ]\n)\nY = np.repeat(np.eye(3), 10, 0)\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(""ggplot"")\n# %matplotlib inline\n\nplt.rcParams[""figure.figsize""] = (12, 6)\n_ = plt.scatter(X[:, 0], X[:, 1], 100, np.argmax(Y, 1), lw=2, cmap=plt.cm.viridis)\n\n# %%\nm = LinearMulticlass(X, Y)\nm\n\n\n# %%\nopt = gpflow.optimizers.Scipy()\nopt.minimize(m.training_loss, variables=m.trainable_variables)\n\n# %%\nxx, yy = np.mgrid[-4:4:200j, -4:4:200j]\nX_test = np.vstack([xx.flatten(), yy.flatten()]).T\nf_test = np.dot(X_test, m.W.read_value()) + m.b.read_value()\np_test = np.exp(f_test)\np_test /= p_test.sum(1)[:, None]\n\n# %%\nplt.figure(figsize=(12, 6))\nfor i in range(3):\n    plt.contour(xx, yy, p_test[:, i].reshape(200, 200), [0.5], colors=""k"", linewidths=1)\n_ = plt.scatter(X[:, 0], X[:, 1], 100, np.argmax(Y, 1), lw=2, cmap=plt.cm.viridis)\n\n# %% [markdown]\n# That concludes the new model example and this notebook. You might want to see for yourself that the `LinearMulticlass` model and its parameters have all the functionality demonstrated here. You could also add some priors and run Hamiltonian Monte Carlo using the HMC optimizer `gpflow.train.HMC` and its `sample` method. See [Markov Chain Monte Carlo (MCMC)](../advanced/mcmc.ipynb) for more information on running the sampler.\n'"
doc/source/notebooks/understanding/utilities.pct.py,0,"b""# ---\n# jupyter:\n#   jupytext:\n#     formats: ipynb,.pct.py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.3.3\n#   kernelspec:\n#     display_name: Python 3\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# Utilities\n# --\n#\n#\n"""
