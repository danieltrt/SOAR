file_path,api_count,code
__init__.py,0,b''
actions/actions.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom rasa_core_sdk import Action\nfrom rasa_core_sdk.events import SlotSet\n\n\nsupport_search = [""\xe8\xaf\x9d\xe8\xb4\xb9"", ""\xe6\xb5\x81\xe9\x87\x8f""]\n\n\ndef extract_item(item):\n    if item is None:\n        return None\n    for name in support_search:\n        if name in item:\n            return name\n    return None\n\n\nclass ActionSearchConsume(Action):\n    def name(self):\n        return \'action_search_consume\'\n\n    def run(self, dispatcher, tracker, domain):\n        item = tracker.get_slot(""item"")\n        item = extract_item(item)\n        if item is None:\n            dispatcher.utter_message(""\xe6\x82\xa8\xe5\xa5\xbd\xef\xbc\x8c\xe6\x88\x91\xe7\x8e\xb0\xe5\x9c\xa8\xe5\x8f\xaa\xe4\xbc\x9a\xe6\x9f\xa5\xe8\xaf\x9d\xe8\xb4\xb9\xe5\x92\x8c\xe6\xb5\x81\xe9\x87\x8f"")\n            dispatcher.utter_message(""\xe4\xbd\xa0\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x99\xe6\xa0\xb7\xe9\x97\xae\xe6\x88\x91\xef\xbc\x9a\xe2\x80\x9c\xe5\xb8\xae\xe6\x88\x91\xe6\x9f\xa5\xe8\xaf\x9d\xe8\xb4\xb9\xe2\x80\x9d"")\n            return []\n\n        time = tracker.get_slot(""time"")\n        if time is None:\n            dispatcher.utter_message(""\xe6\x82\xa8\xe6\x83\xb3\xe6\x9f\xa5\xe8\xaf\xa2\xe5\x93\xaa\xe4\xb8\xaa\xe6\x9c\x88\xe7\x9a\x84\xe6\xb6\x88\xe8\xb4\xb9\xef\xbc\x9f"")\n            return []\n        # query database here using item and time as key. but you may normalize time format first.\n        dispatcher.utter_message(""\xe5\xa5\xbd\xef\xbc\x8c\xe8\xaf\xb7\xe7\xa8\x8d\xe7\xad\x89"")\n        if item == ""\xe6\xb5\x81\xe9\x87\x8f"":\n            dispatcher.utter_message(\n                ""\xe6\x82\xa8\xe5\xa5\xbd\xef\xbc\x8c\xe6\x82\xa8{}\xe5\x85\xb1\xe4\xbd\xbf\xe7\x94\xa8{}\xe4\xba\x8c\xe7\x99\xbe\xe5\x85\xab\xe5\x8d\x81\xe5\x85\x86\xef\xbc\x8c\xe5\x89\xa9\xe4\xbd\x99\xe4\xb8\x89\xe5\x8d\x81\xe5\x85\x86\xe3\x80\x82"".format(time, item))\n        else:\n            dispatcher.utter_message(""\xe6\x82\xa8\xe5\xa5\xbd\xef\xbc\x8c\xe6\x82\xa8{}\xe5\x85\xb1\xe6\xb6\x88\xe8\xb4\xb9\xe4\xba\x8c\xe5\x8d\x81\xe5\x85\xab\xe5\x85\x83\xe3\x80\x82"".format(time))\n        return []\n\n'"
components/__init__.py,0,b''
policy/__init__.py,0,b''
policy/attention_keras.py,0,"b""from keras import backend as K\nfrom keras.layers import Layer\n\n\nclass Position_Embedding(Layer):\n    def __init__(self, size=None, mode='sum', **kwargs):\n        self.size = size  # should be an even number\n        self.mode = mode\n        super(Position_Embedding, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            'size': self.size,\n            'mode': self.mode\n        }\n        base_config = super(Position_Embedding, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def call(self, x):\n        if (self.size == None) or (self.mode == 'sum'):\n            self.size = int(x.shape[-1])\n        # batch_size, seq_len = K.shape(x)[0], K.shape(x)[1]\n        position_j = 1. / K.pow(10000., 2 * K.arange(self.size / 2, dtype='float32') / self.size)\n        position_j = K.expand_dims(position_j, 0)\n        position_i = K.cumsum(K.ones_like(x[:, :, 0]), 1)-1\n        position_i = K.expand_dims(position_i, 2)\n\n        position_ij = K.dot(position_i, position_j)\n\n        position_ij = K.concatenate(\n            [K.cos(position_ij), K.sin(position_ij)], 2)\n        if self.mode == 'sum':\n            return position_ij + x\n        elif self.mode == 'concat':\n            return K.concatenate([position_ij, x], 2)\n\n    def compute_output_shape(self, input_shape):\n        if self.mode == 'sum':\n            return input_shape\n        elif self.mode == 'concat':\n            return (input_shape[0], input_shape[1], input_shape[2] + self.size)\n\n\nclass Attention(Layer):\n    def __init__(self, nb_head, size_per_head, **kwargs):\n        self.nb_head = nb_head\n        self.size_per_head = size_per_head\n        self.output_dim = nb_head*size_per_head\n\n        super(Attention, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            'nb_head': self.nb_head,\n            'size_per_head': self.size_per_head\n        }\n        base_config = super(Attention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        self.WQ = self.add_weight(name='WQ',\n                                  shape=(input_shape[0][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WK = self.add_weight(name='WK',\n                                  shape=(input_shape[1][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WV = self.add_weight(name='WV',\n                                  shape=(input_shape[2][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        super(Attention, self).build(input_shape)\n\n    def Mask(self, inputs, seq_len, mode='mul'):\n        if seq_len == None:\n            return inputs\n        else:\n            mask = K.one_hot(seq_len[:, 0], K.shape(inputs)[1])\n            mask = 1 - K.cumsum(mask, 1)\n            for _ in range(len(inputs.shape)-2):\n                mask = K.expand_dims(mask, 2)\n            if mode == 'mul':\n                return inputs * mask\n            if mode == 'add':\n                return inputs - (1 - mask) * 1e12\n\n    def call(self, x):\n        if len(x) == 3:\n            Q_seq, K_seq, V_seq = x\n            Q_len, V_len = None, None\n        elif len(x) == 5:\n            Q_seq, K_seq, V_seq, Q_len, V_len = x\n        Q_seq = K.dot(Q_seq, self.WQ)\n        Q_seq = K.reshape(\n            Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n        Q_seq = K.permute_dimensions(Q_seq, (0, 2, 1, 3))\n        K_seq = K.dot(K_seq, self.WK)\n        K_seq = K.reshape(\n            K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n        K_seq = K.permute_dimensions(K_seq, (0, 2, 1, 3))\n        V_seq = K.dot(V_seq, self.WV)\n        V_seq = K.reshape(\n            V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n        V_seq = K.permute_dimensions(V_seq, (0, 2, 1, 3))\n        A = K.batch_dot(Q_seq, K_seq, axes=[3, 3]) / self.size_per_head**0.5\n        A = K.permute_dimensions(A, (0, 3, 2, 1))\n        A = self.Mask(A, V_len, 'add')\n        A = K.permute_dimensions(A, (0, 3, 2, 1))\n        A = K.softmax(A)\n        O_seq = K.batch_dot(A, V_seq, axes=[3, 2])\n        O_seq = K.permute_dimensions(O_seq, (0, 2, 1, 3))\n        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n        O_seq = self.Mask(O_seq, Q_len, 'mul')\n        return O_seq\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n"""
policy/attention_policy.py,2,"b'import logging\n\nfrom typing import Text, Any\nimport json\nimport os\nimport tensorflow as tf\nimport warnings\n\nfrom rasa.core.policies.keras_policy import KerasPolicy\nimport rasa.utils.io\nfrom rasa.core import utils\nfrom rasa.core.featurizers import TrackerFeaturizer\n\nfrom .attention_keras import Attention, Position_Embedding\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\n\nlogger = logging.getLogger(__name__)\n\nclass AttentionPolicy(KerasPolicy):\n    def model_architecture(self, input_shape, output_shape):\n        from keras.models import Model, load_model\n        from keras.layers import Input, GlobalAveragePooling1D, Dropout, Dense\n\n        S_inputs = Input(shape=input_shape)\n\n        embeddings = S_inputs\n        # embeddings = Position_Embedding()(S_inputs)\n        """"""\n            nb_head = 8 \xe8\xb6\x85\xe5\x8f\x82\xe5\x8f\xaf\xe8\xae\xbe\xe5\xae\x9a\n            size_per_head = 16 \xe8\xb6\x85\xe5\x8f\x82\xe5\x8f\xaf\xe8\xae\xbe\xe5\xae\x9a\n        """"""\n        O_seq = Attention(16, 64)([embeddings, embeddings, embeddings])\n        O_seq = Attention(16, 64)([O_seq, O_seq, O_seq])\n        O_seq = GlobalAveragePooling1D()(O_seq)\n        O_seq = Dropout(0.5)(O_seq)\n        outputs = Dense(units=output_shape[-1], activation=\'softmax\')(O_seq)\n\n        model = Model(inputs=S_inputs, outputs=outputs)\n\n        model.compile(loss=\'categorical_crossentropy\',\n                    optimizer=\'adam\',\n                    metrics=[\'accuracy\'])\n\n        logger.debug(model.summary())\n        return model\n\n\n    @classmethod\n    def load(cls, path: Text) -> ""KerasPolicy"":\n        from keras.models import load_model\n\n        if os.path.exists(path):\n            featurizer = TrackerFeaturizer.load(path)\n            meta_file = os.path.join(path, ""keras_policy.json"")\n            if os.path.isfile(meta_file):\n                meta = json.loads(rasa.utils.io.read_file(meta_file))\n\n                tf_config_file = os.path.join(path, ""keras_policy.tf_config.pkl"")\n                with open(tf_config_file, ""rb"") as f:\n                    _tf_config = pickle.load(f)\n\n                model_file = os.path.join(path, meta[""model""])\n\n                graph = tf.Graph()\n                with graph.as_default():\n                    session = tf.Session(config=_tf_config)\n                    with session.as_default():\n                        with warnings.catch_warnings():\n                            warnings.simplefilter(""ignore"")\n                            model = load_model(model_file, custom_objects={\n                                \'Position_Embedding\': Position_Embedding,\n                                \'Attention\': Attention})\n\n                return cls(\n                    featurizer=featurizer,\n                    priority=meta[""priority""],\n                    model=model,\n                    graph=graph,\n                    session=session,\n                    current_epoch=meta[""epochs""],\n                )\n            else:\n                return cls(featurizer=featurizer)\n        else:\n            raise Exception(\n                ""Failed to load dialogue model. Path {} ""\n                ""doesn\'t exist"".format(os.path.abspath(path))\n            )\n'"
policy/mobile_policy.py,0,"b'import logging\n\nfrom rasa.core.policies.keras_policy import KerasPolicy\n\nlogger = logging.getLogger(__name__)\n\n\nclass MobilePolicy(KerasPolicy):\n    def model_architecture(self, input_shape, output_shape):\n        """"""Build a Keras model and return a compiled model.""""""\n        from keras.layers import LSTM, Activation, Masking, Dense\n        from keras.models import Sequential\n\n        from keras.layers import Masking, LSTM, Dense, TimeDistributed, Activation\n\n        # Build Model\n        model = Sequential()\n\n        # the shape of the y vector of the labels,\n        # determines which output from rnn will be used\n        # to calculate the loss\n        if len(output_shape) == 1:\n            # y is (num examples, num features) so\n            # only the last output from the rnn is used to\n            # calculate the loss\n            model.add(Masking(mask_value=-1, input_shape=input_shape))\n            model.add(LSTM(self.rnn_size))\n            model.add(Dense(input_dim=self.rnn_size, units=output_shape[-1]))\n        elif len(output_shape) == 2:\n            # y is (num examples, max_dialogue_len, num features) so\n            # all the outputs from the rnn are used to\n            # calculate the loss, therefore a sequence is returned and\n            # time distributed layer is used\n\n            # the first value in input_shape is max dialogue_len,\n            # it is set to None, to allow dynamic_rnn creation\n            # during prediction\n            model.add(Masking(mask_value=-1,\n                              input_shape=(None, input_shape[1])))\n            model.add(LSTM(self.rnn_size, return_sequences=True))\n            model.add(TimeDistributed(Dense(units=output_shape[-1])))\n        else:\n            raise ValueError(""Cannot construct the model because""\n                             ""length of output_shape = {} ""\n                             ""should be 1 or 2.""\n                             """".format(len(output_shape)))\n\n        model.add(Activation(\'softmax\'))\n\n        model.compile(loss=\'categorical_crossentropy\',\n                      optimizer=\'adam\',\n                      metrics=[\'accuracy\'])\n\n        logger.debug(model.summary())\n        return model\n\n'"
