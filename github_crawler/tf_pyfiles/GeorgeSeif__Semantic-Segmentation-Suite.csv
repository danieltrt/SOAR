file_path,api_count,code
predict.py,6,"b'import os,time,cv2, sys, math\nimport tensorflow as tf\nimport argparse\nimport numpy as np\n\nfrom utils import utils, helpers\nfrom builders import model_builder\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--image\', type=str, default=None, required=True, help=\'The image you want to predict on. \')\nparser.add_argument(\'--checkpoint_path\', type=str, default=None, required=True, help=\'The path to the latest checkpoint weights for your model.\')\nparser.add_argument(\'--crop_height\', type=int, default=512, help=\'Height of cropped input image to network\')\nparser.add_argument(\'--crop_width\', type=int, default=512, help=\'Width of cropped input image to network\')\nparser.add_argument(\'--model\', type=str, default=None, required=True, help=\'The model you are using\')\nparser.add_argument(\'--dataset\', type=str, default=""CamVid"", required=False, help=\'The dataset you are using\')\nargs = parser.parse_args()\n\nclass_names_list, label_values = helpers.get_label_info(os.path.join(args.dataset, ""class_dict.csv""))\n\nnum_classes = len(label_values)\n\nprint(""\\n***** Begin prediction *****"")\nprint(""Dataset -->"", args.dataset)\nprint(""Model -->"", args.model)\nprint(""Crop Height -->"", args.crop_height)\nprint(""Crop Width -->"", args.crop_width)\nprint(""Num Classes -->"", num_classes)\nprint(""Image -->"", args.image)\n\n# Initializing network\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess=tf.Session(config=config)\n\nnet_input = tf.placeholder(tf.float32,shape=[None,None,None,3])\nnet_output = tf.placeholder(tf.float32,shape=[None,None,None,num_classes]) \n\nnetwork, _ = model_builder.build_model(args.model, net_input=net_input,\n                                        num_classes=num_classes,\n                                        crop_width=args.crop_width,\n                                        crop_height=args.crop_height,\n                                        is_training=False)\n\nsess.run(tf.global_variables_initializer())\n\nprint(\'Loading model checkpoint weights\')\nsaver=tf.train.Saver(max_to_keep=1000)\nsaver.restore(sess, args.checkpoint_path)\n\n\nprint(""Testing image "" + args.image)\n\nloaded_image = utils.load_image(args.image)\nresized_image =cv2.resize(loaded_image, (args.crop_width, args.crop_height))\ninput_image = np.expand_dims(np.float32(resized_image[:args.crop_height, :args.crop_width]),axis=0)/255.0\n\nst = time.time()\noutput_image = sess.run(network,feed_dict={net_input:input_image})\n\nrun_time = time.time()-st\n\noutput_image = np.array(output_image[0,:,:,:])\noutput_image = helpers.reverse_one_hot(output_image)\n\nout_vis_image = helpers.colour_code_segmentation(output_image, label_values)\nfile_name = utils.filepath_to_name(args.image)\ncv2.imwrite(""%s_pred.png""%(file_name),cv2.cvtColor(np.uint8(out_vis_image), cv2.COLOR_RGB2BGR))\n\nprint("""")\nprint(""Finished!"")\nprint(""Wrote image "" + ""%s_pred.png""%(file_name))\n'"
test.py,6,"b'import os,time,cv2, sys, math\nimport tensorflow as tf\nimport argparse\nimport numpy as np\n\nfrom utils import utils, helpers\nfrom builders import model_builder\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--checkpoint_path\', type=str, default=None, required=True, help=\'The path to the latest checkpoint weights for your model.\')\nparser.add_argument(\'--crop_height\', type=int, default=512, help=\'Height of cropped input image to network\')\nparser.add_argument(\'--crop_width\', type=int, default=512, help=\'Width of cropped input image to network\')\nparser.add_argument(\'--model\', type=str, default=None, required=True, help=\'The model you are using\')\nparser.add_argument(\'--dataset\', type=str, default=""CamVid"", required=False, help=\'The dataset you are using\')\nargs = parser.parse_args()\n\n# Get the names of the classes so we can record the evaluation results\nprint(""Retrieving dataset information ..."")\nclass_names_list, label_values = helpers.get_label_info(os.path.join(args.dataset, ""class_dict.csv""))\nclass_names_string = """"\nfor class_name in class_names_list:\n    if not class_name == class_names_list[-1]:\n        class_names_string = class_names_string + class_name + "", ""\n    else:\n        class_names_string = class_names_string + class_name\n\nnum_classes = len(label_values)\n\n# Initializing network\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess=tf.Session(config=config)\n\nnet_input = tf.placeholder(tf.float32,shape=[None,None,None,3])\nnet_output = tf.placeholder(tf.float32,shape=[None,None,None,num_classes]) \n\nnetwork, _ = model_builder.build_model(args.model, net_input=net_input, num_classes=num_classes, crop_width=args.crop_width, crop_height=args.crop_height, is_training=False)\n\nsess.run(tf.global_variables_initializer())\n\nprint(\'Loading model checkpoint weights ...\')\nsaver=tf.train.Saver(max_to_keep=1000)\nsaver.restore(sess, args.checkpoint_path)\n\n# Load the data\nprint(""Loading the data ..."")\ntrain_input_names,train_output_names, val_input_names, val_output_names, test_input_names, test_output_names = utils.prepare_data(dataset_dir=args.dataset)\n\n# Create directories if needed\nif not os.path.isdir(""%s""%(""Test"")):\n        os.makedirs(""%s""%(""Test""))\n\ntarget=open(""%s/test_scores.csv""%(""Test""),\'w\')\ntarget.write(""test_name, test_accuracy, precision, recall, f1 score, mean iou, %s\\n"" % (class_names_string))\nscores_list = []\nclass_scores_list = []\nprecision_list = []\nrecall_list = []\nf1_list = []\niou_list = []\nrun_times_list = []\n\n# Run testing on ALL test images\nfor ind in range(len(test_input_names)):\n    sys.stdout.write(""\\rRunning test image %d / %d""%(ind+1, len(test_input_names)))\n    sys.stdout.flush()\n\n    input_image = np.expand_dims(np.float32(utils.load_image(test_input_names[ind])[:args.crop_height, :args.crop_width]),axis=0)/255.0\n    gt = utils.load_image(test_output_names[ind])[:args.crop_height, :args.crop_width]\n    gt = helpers.reverse_one_hot(helpers.one_hot_it(gt, label_values))\n\n    st = time.time()\n    output_image = sess.run(network,feed_dict={net_input:input_image})\n\n    run_times_list.append(time.time()-st)\n\n    output_image = np.array(output_image[0,:,:,:])\n    output_image = helpers.reverse_one_hot(output_image)\n    out_vis_image = helpers.colour_code_segmentation(output_image, label_values)\n\n    accuracy, class_accuracies, prec, rec, f1, iou = utils.evaluate_segmentation(pred=output_image, label=gt, num_classes=num_classes)\n\n    file_name = utils.filepath_to_name(test_input_names[ind])\n    target.write(""%s, %f, %f, %f, %f, %f""%(file_name, accuracy, prec, rec, f1, iou))\n    for item in class_accuracies:\n        target.write("", %f""%(item))\n    target.write(""\\n"")\n\n    scores_list.append(accuracy)\n    class_scores_list.append(class_accuracies)\n    precision_list.append(prec)\n    recall_list.append(rec)\n    f1_list.append(f1)\n    iou_list.append(iou)\n    \n    gt = helpers.colour_code_segmentation(gt, label_values)\n\n    cv2.imwrite(""%s/%s_pred.png""%(""Test"", file_name),cv2.cvtColor(np.uint8(out_vis_image), cv2.COLOR_RGB2BGR))\n    cv2.imwrite(""%s/%s_gt.png""%(""Test"", file_name),cv2.cvtColor(np.uint8(gt), cv2.COLOR_RGB2BGR))\n\n\ntarget.close()\n\navg_score = np.mean(scores_list)\nclass_avg_scores = np.mean(class_scores_list, axis=0)\navg_precision = np.mean(precision_list)\navg_recall = np.mean(recall_list)\navg_f1 = np.mean(f1_list)\navg_iou = np.mean(iou_list)\navg_time = np.mean(run_times_list)\nprint(""Average test accuracy = "", avg_score)\nprint(""Average per class test accuracies = \\n"")\nfor index, item in enumerate(class_avg_scores):\n    print(""%s = %f"" % (class_names_list[index], item))\nprint(""Average precision = "", avg_precision)\nprint(""Average recall = "", avg_recall)\nprint(""Average F1 score = "", avg_f1)\nprint(""Average mean IoU score = "", avg_iou)\nprint(""Average run time = "", avg_time)\n'"
train.py,10,"b'from __future__ import print_function\nimport os,time,cv2, sys, math\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport time, datetime\nimport argparse\nimport random\nimport os, sys\nimport subprocess\n\n# use \'Agg\' on matplotlib so that plots could be generated even without Xserver\n# running\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nfrom utils import utils, helpers\nfrom builders import model_builder\n\nimport matplotlib.pyplot as plt\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--num_epochs\', type=int, default=300, help=\'Number of epochs to train for\')\nparser.add_argument(\'--epoch_start_i\', type=int, default=0, help=\'Start counting epochs from this number\')\nparser.add_argument(\'--checkpoint_step\', type=int, default=5, help=\'How often to save checkpoints (epochs)\')\nparser.add_argument(\'--validation_step\', type=int, default=1, help=\'How often to perform validation (epochs)\')\nparser.add_argument(\'--image\', type=str, default=None, help=\'The image you want to predict on. Only valid in ""predict"" mode.\')\nparser.add_argument(\'--continue_training\', type=str2bool, default=False, help=\'Whether to continue training from a checkpoint\')\nparser.add_argument(\'--dataset\', type=str, default=""CamVid"", help=\'Dataset you are using.\')\nparser.add_argument(\'--crop_height\', type=int, default=512, help=\'Height of cropped input image to network\')\nparser.add_argument(\'--crop_width\', type=int, default=512, help=\'Width of cropped input image to network\')\nparser.add_argument(\'--batch_size\', type=int, default=1, help=\'Number of images in each batch\')\nparser.add_argument(\'--num_val_images\', type=int, default=20, help=\'The number of images to used for validations\')\nparser.add_argument(\'--h_flip\', type=str2bool, default=False, help=\'Whether to randomly flip the image horizontally for data augmentation\')\nparser.add_argument(\'--v_flip\', type=str2bool, default=False, help=\'Whether to randomly flip the image vertically for data augmentation\')\nparser.add_argument(\'--brightness\', type=float, default=None, help=\'Whether to randomly change the image brightness for data augmentation. Specifies the max bightness change as a factor between 0.0 and 1.0. For example, 0.1 represents a max brightness change of 10%% (+-).\')\nparser.add_argument(\'--rotation\', type=float, default=None, help=\'Whether to randomly rotate the image for data augmentation. Specifies the max rotation angle in degrees.\')\nparser.add_argument(\'--model\', type=str, default=""FC-DenseNet56"", help=\'The model you are using. See model_builder.py for supported models\')\nparser.add_argument(\'--frontend\', type=str, default=""ResNet101"", help=\'The frontend you are using. See frontend_builder.py for supported models\')\nargs = parser.parse_args()\n\n\ndef data_augmentation(input_image, output_image):\n    # Data augmentation\n    input_image, output_image = utils.random_crop(input_image, output_image, args.crop_height, args.crop_width)\n\n    if args.h_flip and random.randint(0,1):\n        input_image = cv2.flip(input_image, 1)\n        output_image = cv2.flip(output_image, 1)\n    if args.v_flip and random.randint(0,1):\n        input_image = cv2.flip(input_image, 0)\n        output_image = cv2.flip(output_image, 0)\n    if args.brightness:\n        factor = 1.0 + random.uniform(-1.0*args.brightness, args.brightness)\n        table = np.array([((i / 255.0) * factor) * 255 for i in np.arange(0, 256)]).astype(np.uint8)\n        input_image = cv2.LUT(input_image, table)\n    if args.rotation:\n        angle = random.uniform(-1*args.rotation, args.rotation)\n    if args.rotation:\n        M = cv2.getRotationMatrix2D((input_image.shape[1]//2, input_image.shape[0]//2), angle, 1.0)\n        input_image = cv2.warpAffine(input_image, M, (input_image.shape[1], input_image.shape[0]), flags=cv2.INTER_NEAREST)\n        output_image = cv2.warpAffine(output_image, M, (output_image.shape[1], output_image.shape[0]), flags=cv2.INTER_NEAREST)\n\n    return input_image, output_image\n\n\n# Get the names of the classes so we can record the evaluation results\nclass_names_list, label_values = helpers.get_label_info(os.path.join(args.dataset, ""class_dict.csv""))\nclass_names_string = """"\nfor class_name in class_names_list:\n    if not class_name == class_names_list[-1]:\n        class_names_string = class_names_string + class_name + "", ""\n    else:\n        class_names_string = class_names_string + class_name\n\nnum_classes = len(label_values)\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess=tf.Session(config=config)\n\n\n# Compute your softmax cross entropy loss\nnet_input = tf.placeholder(tf.float32,shape=[None,None,None,3])\nnet_output = tf.placeholder(tf.float32,shape=[None,None,None,num_classes])\n\nnetwork, init_fn = model_builder.build_model(model_name=args.model, frontend=args.frontend, net_input=net_input, num_classes=num_classes, crop_width=args.crop_width, crop_height=args.crop_height, is_training=True)\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network, labels=net_output))\n\nopt = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.995).minimize(loss, var_list=[var for var in tf.trainable_variables()])\n\nsaver=tf.train.Saver(max_to_keep=1000)\nsess.run(tf.global_variables_initializer())\n\nutils.count_params()\n\n# If a pre-trained ResNet is required, load the weights.\n# This must be done AFTER the variables are initialized with sess.run(tf.global_variables_initializer())\nif init_fn is not None:\n    init_fn(sess)\n\n# Load a previous checkpoint if desired\nmodel_checkpoint_name = ""checkpoints/latest_model_"" + args.model + ""_"" + args.dataset + "".ckpt""\nif args.continue_training:\n    print(\'Loaded latest model checkpoint\')\n    saver.restore(sess, model_checkpoint_name)\n\n# Load the data\nprint(""Loading the data ..."")\ntrain_input_names,train_output_names, val_input_names, val_output_names, test_input_names, test_output_names = utils.prepare_data(dataset_dir=args.dataset)\n\n\n\nprint(""\\n***** Begin training *****"")\nprint(""Dataset -->"", args.dataset)\nprint(""Model -->"", args.model)\nprint(""Crop Height -->"", args.crop_height)\nprint(""Crop Width -->"", args.crop_width)\nprint(""Num Epochs -->"", args.num_epochs)\nprint(""Batch Size -->"", args.batch_size)\nprint(""Num Classes -->"", num_classes)\n\nprint(""Data Augmentation:"")\nprint(""\\tVertical Flip -->"", args.v_flip)\nprint(""\\tHorizontal Flip -->"", args.h_flip)\nprint(""\\tBrightness Alteration -->"", args.brightness)\nprint(""\\tRotation -->"", args.rotation)\nprint("""")\n\navg_loss_per_epoch = []\navg_scores_per_epoch = []\navg_iou_per_epoch = []\n\n# Which validation images do we want\nval_indices = []\nnum_vals = min(args.num_val_images, len(val_input_names))\n\n# Set random seed to make sure models are validated on the same validation images.\n# So you can compare the results of different models more intuitively.\nrandom.seed(16)\nval_indices=random.sample(range(0,len(val_input_names)),num_vals)\n\n# Do the training here\nfor epoch in range(args.epoch_start_i, args.num_epochs):\n\n    current_losses = []\n\n    cnt=0\n\n    # Equivalent to shuffling\n    id_list = np.random.permutation(len(train_input_names))\n\n    num_iters = int(np.floor(len(id_list) / args.batch_size))\n    st = time.time()\n    epoch_st=time.time()\n    for i in range(num_iters):\n        # st=time.time()\n\n        input_image_batch = []\n        output_image_batch = []\n\n        # Collect a batch of images\n        for j in range(args.batch_size):\n            index = i*args.batch_size + j\n            id = id_list[index]\n            input_image = utils.load_image(train_input_names[id])\n            output_image = utils.load_image(train_output_names[id])\n\n            with tf.device(\'/cpu:0\'):\n                input_image, output_image = data_augmentation(input_image, output_image)\n\n\n                # Prep the data. Make sure the labels are in one-hot format\n                input_image = np.float32(input_image) / 255.0\n                output_image = np.float32(helpers.one_hot_it(label=output_image, label_values=label_values))\n\n                input_image_batch.append(np.expand_dims(input_image, axis=0))\n                output_image_batch.append(np.expand_dims(output_image, axis=0))\n\n        if args.batch_size == 1:\n            input_image_batch = input_image_batch[0]\n            output_image_batch = output_image_batch[0]\n        else:\n            input_image_batch = np.squeeze(np.stack(input_image_batch, axis=1))\n            output_image_batch = np.squeeze(np.stack(output_image_batch, axis=1))\n\n        # Do the training\n        _,current=sess.run([opt,loss],feed_dict={net_input:input_image_batch,net_output:output_image_batch})\n        current_losses.append(current)\n        cnt = cnt + args.batch_size\n        if cnt % 20 == 0:\n            string_print = ""Epoch = %d Count = %d Current_Loss = %.4f Time = %.2f""%(epoch,cnt,current,time.time()-st)\n            utils.LOG(string_print)\n            st = time.time()\n\n    mean_loss = np.mean(current_losses)\n    avg_loss_per_epoch.append(mean_loss)\n\n    # Create directories if needed\n    if not os.path.isdir(""%s/%04d""%(""checkpoints"",epoch)):\n        os.makedirs(""%s/%04d""%(""checkpoints"",epoch))\n\n    # Save latest checkpoint to same file name\n    print(""Saving latest checkpoint"")\n    saver.save(sess,model_checkpoint_name)\n\n    if val_indices != 0 and epoch % args.checkpoint_step == 0:\n        print(""Saving checkpoint for this epoch"")\n        saver.save(sess,""%s/%04d/model.ckpt""%(""checkpoints"",epoch))\n\n\n    if epoch % args.validation_step == 0:\n        print(""Performing validation"")\n        target=open(""%s/%04d/val_scores.csv""%(""checkpoints"",epoch),\'w\')\n        target.write(""val_name, avg_accuracy, precision, recall, f1 score, mean iou, %s\\n"" % (class_names_string))\n\n\n        scores_list = []\n        class_scores_list = []\n        precision_list = []\n        recall_list = []\n        f1_list = []\n        iou_list = []\n\n\n        # Do the validation on a small set of validation images\n        for ind in val_indices:\n\n            input_image = np.expand_dims(np.float32(utils.load_image(val_input_names[ind])[:args.crop_height, :args.crop_width]),axis=0)/255.0\n            gt = utils.load_image(val_output_names[ind])[:args.crop_height, :args.crop_width]\n            gt = helpers.reverse_one_hot(helpers.one_hot_it(gt, label_values))\n\n            # st = time.time()\n\n            output_image = sess.run(network,feed_dict={net_input:input_image})\n\n\n            output_image = np.array(output_image[0,:,:,:])\n            output_image = helpers.reverse_one_hot(output_image)\n            out_vis_image = helpers.colour_code_segmentation(output_image, label_values)\n\n            accuracy, class_accuracies, prec, rec, f1, iou = utils.evaluate_segmentation(pred=output_image, label=gt, num_classes=num_classes)\n\n            file_name = utils.filepath_to_name(val_input_names[ind])\n            target.write(""%s, %f, %f, %f, %f, %f""%(file_name, accuracy, prec, rec, f1, iou))\n            for item in class_accuracies:\n                target.write("", %f""%(item))\n            target.write(""\\n"")\n\n            scores_list.append(accuracy)\n            class_scores_list.append(class_accuracies)\n            precision_list.append(prec)\n            recall_list.append(rec)\n            f1_list.append(f1)\n            iou_list.append(iou)\n\n            gt = helpers.colour_code_segmentation(gt, label_values)\n\n            file_name = os.path.basename(val_input_names[ind])\n            file_name = os.path.splitext(file_name)[0]\n            cv2.imwrite(""%s/%04d/%s_pred.png""%(""checkpoints"",epoch, file_name),cv2.cvtColor(np.uint8(out_vis_image), cv2.COLOR_RGB2BGR))\n            cv2.imwrite(""%s/%04d/%s_gt.png""%(""checkpoints"",epoch, file_name),cv2.cvtColor(np.uint8(gt), cv2.COLOR_RGB2BGR))\n\n\n        target.close()\n\n        avg_score = np.mean(scores_list)\n        class_avg_scores = np.mean(class_scores_list, axis=0)\n        avg_scores_per_epoch.append(avg_score)\n        avg_precision = np.mean(precision_list)\n        avg_recall = np.mean(recall_list)\n        avg_f1 = np.mean(f1_list)\n        avg_iou = np.mean(iou_list)\n        avg_iou_per_epoch.append(avg_iou)\n\n        print(""\\nAverage validation accuracy for epoch # %04d = %f""% (epoch, avg_score))\n        print(""Average per class validation accuracies for epoch # %04d:""% (epoch))\n        for index, item in enumerate(class_avg_scores):\n            print(""%s = %f"" % (class_names_list[index], item))\n        print(""Validation precision = "", avg_precision)\n        print(""Validation recall = "", avg_recall)\n        print(""Validation F1 score = "", avg_f1)\n        print(""Validation IoU score = "", avg_iou)\n\n    epoch_time=time.time()-epoch_st\n    remain_time=epoch_time*(args.num_epochs-1-epoch)\n    m, s = divmod(remain_time, 60)\n    h, m = divmod(m, 60)\n    if s!=0:\n        train_time=""Remaining training time = %d hours %d minutes %d seconds\\n""%(h,m,s)\n    else:\n        train_time=""Remaining training time : Training completed.\\n""\n    utils.LOG(train_time)\n    scores_list = []\n\n\n    fig1, ax1 = plt.subplots(figsize=(11, 8))\n\n    ax1.plot(range(epoch+1), avg_scores_per_epoch)\n    ax1.set_title(""Average validation accuracy vs epochs"")\n    ax1.set_xlabel(""Epoch"")\n    ax1.set_ylabel(""Avg. val. accuracy"")\n\n\n    plt.savefig(\'accuracy_vs_epochs.png\')\n\n    plt.clf()\n\n    fig2, ax2 = plt.subplots(figsize=(11, 8))\n\n    ax2.plot(range(epoch+1), avg_loss_per_epoch)\n    ax2.set_title(""Average loss vs epochs"")\n    ax2.set_xlabel(""Epoch"")\n    ax2.set_ylabel(""Current loss"")\n\n    plt.savefig(\'loss_vs_epochs.png\')\n\n    plt.clf()\n\n    fig3, ax3 = plt.subplots(figsize=(11, 8))\n\n    ax3.plot(range(epoch+1), avg_iou_per_epoch)\n    ax3.set_title(""Average IoU vs epochs"")\n    ax3.set_xlabel(""Epoch"")\n    ax3.set_ylabel(""Current IoU"")\n\n    plt.savefig(\'iou_vs_epochs.png\')\n\n\n\n'"
builders/__init__.py,0,b''
builders/frontend_builder.py,0,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom frontends import resnet_v2\nfrom frontends import mobilenet_v2\nfrom frontends import inception_v4\nimport os \n\n\ndef build_frontend(inputs, frontend, is_training=True, pretrained_dir=""models""):\n    if frontend == \'ResNet50\':\n        with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n            logits, end_points = resnet_v2.resnet_v2_50(inputs, is_training=is_training, scope=\'resnet_v2_50\')\n            frontend_scope=\'resnet_v2_50\'\n            init_fn = slim.assign_from_checkpoint_fn(model_path=os.path.join(pretrained_dir, \'resnet_v2_50.ckpt\'), var_list=slim.get_model_variables(\'resnet_v2_50\'), ignore_missing_vars=True)\n    elif frontend == \'ResNet101\':\n        with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n            logits, end_points = resnet_v2.resnet_v2_101(inputs, is_training=is_training, scope=\'resnet_v2_101\')\n            frontend_scope=\'resnet_v2_101\'\n            init_fn = slim.assign_from_checkpoint_fn(model_path=os.path.join(pretrained_dir, \'resnet_v2_101.ckpt\'), var_list=slim.get_model_variables(\'resnet_v2_101\'), ignore_missing_vars=True)\n    elif frontend == \'ResNet152\':\n        with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n            logits, end_points = resnet_v2.resnet_v2_152(inputs, is_training=is_training, scope=\'resnet_v2_152\')\n            frontend_scope=\'resnet_v2_152\'\n            init_fn = slim.assign_from_checkpoint_fn(model_path=os.path.join(pretrained_dir, \'resnet_v2_152.ckpt\'), var_list=slim.get_model_variables(\'resnet_v2_152\'), ignore_missing_vars=True)\n    elif frontend == \'MobileNetV2\':\n        with slim.arg_scope(mobilenet_v2.training_scope()):\n            logits, end_points = mobilenet_v2.mobilenet(inputs, is_training=is_training, scope=\'mobilenet_v2\', base_only=True)\n            frontend_scope=\'mobilenet_v2\'\n            init_fn = slim.assign_from_checkpoint_fn(model_path=os.path.join(pretrained_dir, \'mobilenet_v2.ckpt\'), var_list=slim.get_model_variables(\'mobilenet_v2\'), ignore_missing_vars=True)\n    elif frontend == \'InceptionV4\':\n        with slim.arg_scope(inception_v4.inception_v4_arg_scope()):\n            logits, end_points = inception_v4.inception_v4(inputs, is_training=is_training, scope=\'inception_v4\')\n            frontend_scope=\'inception_v4\'\n            init_fn = slim.assign_from_checkpoint_fn(model_path=os.path.join(pretrained_dir, \'inception_v4.ckpt\'), var_list=slim.get_model_variables(\'inception_v4\'), ignore_missing_vars=True)\n    else:\n        raise ValueError(""Unsupported fronetnd model \'%s\'. This function only supports ResNet50, ResNet101, ResNet152, and MobileNetV2"" % (frontend))\n\n    return logits, end_points, frontend_scope, init_fn '"
builders/model_builder.py,0,"b'import sys, os\nimport tensorflow as tf\nimport subprocess\n\nsys.path.append(""models"")\nfrom models.FC_DenseNet_Tiramisu import build_fc_densenet\nfrom models.Encoder_Decoder import build_encoder_decoder\nfrom models.RefineNet import build_refinenet\nfrom models.FRRN import build_frrn\nfrom models.MobileUNet import build_mobile_unet\nfrom models.PSPNet import build_pspnet\nfrom models.GCN import build_gcn\nfrom models.DeepLabV3 import build_deeplabv3\nfrom models.DeepLabV3_plus import build_deeplabv3_plus\nfrom models.AdapNet import build_adaptnet\nfrom models.custom_model import build_custom\nfrom models.DenseASPP import build_dense_aspp\nfrom models.DDSC import build_ddsc\nfrom models.BiSeNet import build_bisenet\n\nSUPPORTED_MODELS = [""FC-DenseNet56"", ""FC-DenseNet67"", ""FC-DenseNet103"", ""Encoder-Decoder"", ""Encoder-Decoder-Skip"", ""RefineNet"",\n    ""FRRN-A"", ""FRRN-B"", ""MobileUNet"", ""MobileUNet-Skip"", ""PSPNet"", ""GCN"", ""DeepLabV3"", ""DeepLabV3_plus"", ""AdapNet"", \n    ""DenseASPP"", ""DDSC"", ""BiSeNet"", ""custom""]\n\nSUPPORTED_FRONTENDS = [""ResNet50"", ""ResNet101"", ""ResNet152"", ""MobileNetV2"", ""InceptionV4""]\n\ndef download_checkpoints(model_name):\n    subprocess.check_output([""python"", ""utils/get_pretrained_checkpoints.py"", ""--model="" + model_name])\n\n\n\ndef build_model(model_name, net_input, num_classes, crop_width, crop_height, frontend=""ResNet101"", is_training=True):\n\t# Get the selected model. \n\t# Some of them require pre-trained ResNet\n\n\tprint(""Preparing the model ..."")\n\n\tif model_name not in SUPPORTED_MODELS:\n\t\traise ValueError(""The model you selected is not supported. The following models are currently supported: {0}"".format(SUPPORTED_MODELS))\n\n\tif frontend not in SUPPORTED_FRONTENDS:\n\t\traise ValueError(""The frontend you selected is not supported. The following models are currently supported: {0}"".format(SUPPORTED_FRONTENDS))\n\n\tif ""ResNet50"" == frontend and not os.path.isfile(""models/resnet_v2_50.ckpt""):\n\t    download_checkpoints(""ResNet50"")\n\tif ""ResNet101"" == frontend and not os.path.isfile(""models/resnet_v2_101.ckpt""):\n\t    download_checkpoints(""ResNet101"")\n\tif ""ResNet152"" == frontend and not os.path.isfile(""models/resnet_v2_152.ckpt""):\n\t    download_checkpoints(""ResNet152"")\n\tif ""MobileNetV2"" == frontend and not os.path.isfile(""models/mobilenet_v2.ckpt.data-00000-of-00001""):\n\t    download_checkpoints(""MobileNetV2"")\n\tif ""InceptionV4"" == frontend and not os.path.isfile(""models/inception_v4.ckpt""):\n\t    download_checkpoints(""InceptionV4"") \n\n\tnetwork = None\n\tinit_fn = None\n\tif model_name == ""FC-DenseNet56"" or model_name == ""FC-DenseNet67"" or model_name == ""FC-DenseNet103"":\n\t    network = build_fc_densenet(net_input, preset_model = model_name, num_classes=num_classes)\n\telif model_name == ""RefineNet"":\n\t    # RefineNet requires pre-trained ResNet weights\n\t    network, init_fn = build_refinenet(net_input, preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""FRRN-A"" or model_name == ""FRRN-B"":\n\t    network = build_frrn(net_input, preset_model = model_name, num_classes=num_classes)\n\telif model_name == ""Encoder-Decoder"" or model_name == ""Encoder-Decoder-Skip"":\n\t    network = build_encoder_decoder(net_input, preset_model = model_name, num_classes=num_classes)\n\telif model_name == ""MobileUNet"" or model_name == ""MobileUNet-Skip"":\n\t    network = build_mobile_unet(net_input, preset_model = model_name, num_classes=num_classes)\n\telif model_name == ""PSPNet"":\n\t    # Image size is required for PSPNet\n\t    # PSPNet requires pre-trained ResNet weights\n\t    network, init_fn = build_pspnet(net_input, label_size=[crop_height, crop_width], preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""GCN"":\n\t    # GCN requires pre-trained ResNet weights\n\t    network, init_fn = build_gcn(net_input, preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""DeepLabV3"":\n\t    # DeepLabV requires pre-trained ResNet weights\n\t    network, init_fn = build_deeplabv3(net_input, preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""DeepLabV3_plus"":\n\t    # DeepLabV3+ requires pre-trained ResNet weights\n\t    network, init_fn = build_deeplabv3_plus(net_input, preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""DenseASPP"":\n\t    # DenseASPP requires pre-trained ResNet weights\n\t    network, init_fn = build_dense_aspp(net_input, preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""DDSC"":\n\t    # DDSC requires pre-trained ResNet weights\n\t    network, init_fn = build_ddsc(net_input, preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""BiSeNet"":\n\t\t# BiSeNet requires pre-trained ResNet weights\n\t\tnetwork, init_fn = build_bisenet(net_input, preset_model = model_name, frontend=frontend, num_classes=num_classes, is_training=is_training)\n\telif model_name == ""AdapNet"":\n\t    network = build_adaptnet(net_input, num_classes=num_classes)\n\telif model_name == ""custom"":\n\t    network = build_custom(net_input, num_classes)\n\telse:\n\t    raise ValueError(""Error: the model %d is not available. Try checking which models are available using the command python main.py --help"")\n\n\treturn network, init_fn'"
frontends/__init__.py,0,b''
frontends/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  project_activation_fn=tf.identity,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  padding=\'SAME\',\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    project_activation_fn: activation function for the project layer\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    padding: Padding type to use if `use_explicit_padding` is not set.\n    scope: optional scope.\n\n  Returns:\n    Tensor of depth num_outputs\n\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    if use_explicit_padding:\n      if padding != \'SAME\':\n        raise TypeError(\'`use_explicit_padding` should only be used with \'\n                        \'""SAME"" padding.\')\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=project_activation_fn)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')'"
frontends/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001,\n                        activation_fn=tf.nn.relu,\n                        batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n    activation_fn: Activation function for conv2d.\n    batch_norm_updates_collections: Collection for the update ops for\n      batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': batch_norm_updates_collections,\n      # use fused batch norm if possible.\n      \'fused\': None,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=activation_fn,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc'"
frontends/inception_v4.py,49,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom frontends import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n\n      end_points[""pool1""] = net\n\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n\n        end_points[""pool2""] = net\n\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n\n        end_points[""pool3""] = net\n\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n\n      end_points[""pool4""] = net\n\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n\n      end_points[""pool5""] = net\n\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    net: a Tensor with the logits (pre-softmax activations) if num_classes\n      is a non-zero integer, or the non-dropped input to the logits layer\n      if num_classes is 0 or None.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      # with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n      #                     stride=1, padding=\'SAME\'):\n      #   # Auxiliary Head logits\n      #   if create_aux_logits and num_classes:\n      #     with tf.variable_scope(\'AuxLogits\'):\n      #       # 17 x 17 x 1024\n      #       aux_logits = end_points[\'Mixed_6h\']\n      #       aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n      #                                    padding=\'VALID\',\n      #                                    scope=\'AvgPool_1a_5x5\')\n      #       aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n      #                                scope=\'Conv2d_1b_1x1\')\n      #       aux_logits = slim.conv2d(aux_logits, 768,\n      #                                aux_logits.get_shape()[1:3],\n      #                                padding=\'VALID\', scope=\'Conv2d_2a\')\n      #       aux_logits = slim.flatten(aux_logits)\n      #       aux_logits = slim.fully_connected(aux_logits, num_classes,\n      #                                         activation_fn=None,\n      #                                         scope=\'Aux_logits\')\n      #       end_points[\'AuxLogits\'] = aux_logits\n\n      #   # Final pooling and prediction\n      #   # TODO(sguada,arnoegw): Consider adding a parameter global_pool which\n      #   # can be set to False to disable pooling here (as in resnet_*()).\n      #   with tf.variable_scope(\'Logits\'):\n      #     # 8 x 8 x 1536\n      #     kernel_size = net.get_shape()[1:3]\n      #     if kernel_size.is_fully_defined():\n      #       net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n      #                             scope=\'AvgPool_1a\')\n      #     else:\n      #       net = tf.reduce_mean(net, [1, 2], keep_dims=True,\n      #                            name=\'global_pool\')\n      #     end_points[\'global_pool\'] = net\n      #     if not num_classes:\n      #       return net, end_points\n      #     # 1 x 1 x 1536\n      #     net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n      #     net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n      #     end_points[\'PreLogitsFlatten\'] = net\n      #     # 1536\n      #     logits = slim.fully_connected(net, num_classes, activation_fn=None,\n      #                                   scope=\'Logits\')\n      #     end_points[\'Logits\'] = logits\n      #     end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return net, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope'"
frontends/mobilenet_base.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = list(defaults.items())\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\nclass NoOpScope(object):\n  """"""No-op context manager.""""""\n\n  def __enter__(self):\n    return None\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    return False\n\n\ndef safe_arg_scope(funcs, **kwargs):\n  """"""Returns `slim.arg_scope` with all None arguments removed.\n\n  Arguments:\n    funcs: Functions to pass to `arg_scope`.\n    **kwargs: Arguments to pass to `arg_scope`.\n\n  Returns:\n    arg_scope or No-op context manager.\n\n  Note: can be useful if None value should be interpreted as ""do not overwrite\n    this parameter value"".\n  """"""\n  filtered_args = {name: value for name, value in kwargs.items()\n                   if value is not None}\n  if filtered_args:\n    return slim.arg_scope(funcs, **filtered_args)\n  else:\n    return NoOpScope()\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode). If this is set to None,\n      no arg_scope is added for slim.batch_norm\'s is_training parameter.\n\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      safe_arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n\n    #######################################\n    #######################################\n    #######################################\n    downsample_count = 0\n\n\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n\n      if layer_stride == 2:\n        downsample_count += 1\n        end_points[""pool"" + str(downsample_count)] = net\n\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n      set to non-training mode. This might be helpful for code that is reused\n      across both training/evaluation, but most of the time training_scope with\n      value False is not needed. If this is set to None, the parameters is not\n      added to the batch_norm arg_scope.\n\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability (not set if equals to None).\n    bn_decay: decay for the batch norm moving averages (not set if equals to\n      None).\n\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'decay\': bn_decay,\n      \'is_training\': is_training\n  }\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      safe_arg_scope([slim.batch_norm], **batch_norm_params), \\\n      safe_arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s'"
frontends/mobilenet_v2.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\n\nArchitecture: https://arxiv.org/abs/1801.04381\n\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport functools\n\nimport tensorflow as tf\n\nfrom frontends import conv_blocks as ops\nfrom frontends import mobilenet_base as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\nV2_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24),\n        op(ops.expanded_conv, stride=1, num_outputs=24),\n        op(ops.expanded_conv, stride=2, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=2, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=2, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=320),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n    ],\n)\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediction_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_classes=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\n\ndef wrapped_partial(func, *args, **kwargs):\n  partial_func = functools.partial(func, *args, **kwargs)\n  functools.update_wrapper(partial_func, func)\n  return partial_func\n\n\n# Wrappers for mobilenet v2 with depth-multipliers. Be noticed that\n# \'finegrain_classification_mode\' is set to True, which means the embedding\n# layer will not be shrinked when given a depth-multiplier < 1.0.\nmobilenet_v2_140 = wrapped_partial(mobilenet, depth_multiplier=1.4)\nmobilenet_v2_050 = wrapped_partial(mobilenet, depth_multiplier=0.50,\n                                   finegrain_classification_mode=True)\nmobilenet_v2_035 = wrapped_partial(mobilenet, depth_multiplier=0.35,\n                                   finegrain_classification_mode=True)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  with slim.\n\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']'"
frontends/resnet_utils.py,5,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, multi_grid, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            # Only uses atrous convolutions with multi-graid rates in the last (block4) block\n            if block.scope == ""block4"":\n              net = block.unit_fn(net, rate=rate * multi_grid[i], **dict(unit, stride=1))\n            else:\n              net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     is_training=True,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True,\n                     activation_fn=tf.nn.relu,\n                     use_batch_norm=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n    activation_fn: The activation function which is used in ResNet.\n    use_batch_norm: Whether or not to use batch normalization.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': None,\n      \'is_training\': is_training,\n      \'fused\': True,  # Use fused batch norm if possible.\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=activation_fn,\n      normalizer_fn=slim.batch_norm if use_batch_norm else None,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc'"
frontends/resnet_v1.py,4,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom frontends import resnet_utils\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n    """"""Bottleneck residual unit variant with BN after convolutions.\n    This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n    its definition. Note that we use here the bottleneck variant which has an\n    extra bottleneck layer.\n    When putting together two consecutive ResNet blocks that use this unit, one\n    should use stride = 2 in the last unit of the first block.\n    Args:\n      inputs: A tensor of size [batch, height, width, channels].\n      depth: The depth of the ResNet unit output.\n      depth_bottleneck: The depth of the bottleneck layers.\n      stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n        the units output compared to its input.\n      rate: An integer, rate for atrous convolution.\n      outputs_collections: Collection to add the ResNet unit output.\n      scope: Optional variable_scope.\n    Returns:\n      The ResNet unit\'s output.\n    """"""\n    with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                                   activation_fn=None, scope=\'shortcut\')\n        residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                               scope=\'conv1\')\n        residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                            rate=rate, scope=\'conv2\')\n        residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                               activation_fn=None, scope=\'conv3\')\n\n        output = tf.nn.relu(shortcut + residual)\n\n        return slim.utils.collect_named_outputs(outputs_collections,\n                                                sc.original_name_scope,\n                                                output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n    """"""Generator for v1 ResNet models.\n\n    This function generates a family of ResNet v1 models. See the resnet_v1_*()\n    methods for specific model instantiations, obtained by selecting different\n    block instantiations that produce ResNets of various depths.\n\n    Training for image classification on Imagenet is usually done with [224, 224]\n    inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n    block for the ResNets defined in [1] that have nominal stride equal to 32.\n    However, for dense prediction tasks we advise that one uses inputs with\n    spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n    this case the feature maps at the ResNet output will have spatial shape\n    [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n    and corners exactly aligned with the input image corners, which greatly\n    facilitates alignment of the features to the image. Using as input [225, 225]\n    images results in [8, 8] feature maps at the output of the last ResNet block.\n\n    For dense prediction tasks, the ResNet needs to run in fully-convolutional\n    (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n    have nominal stride equal to 32 and a good choice in FCN mode is to use\n    output_stride=16 in order to increase the density of the computed features at\n    small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n    Args:\n      inputs: A tensor of size [batch, height_in, width_in, channels].\n      blocks: A list of length equal to the number of ResNet blocks. Each element\n        is a resnet_utils.Block object describing the units in the block.\n      num_classes: Number of predicted classes for classification tasks. If None\n        we return the features before the logit layer.\n      is_training: whether is training or not.\n      global_pool: If True, we perform global average pooling before computing the\n        logits. Set to True for image classification, False for dense prediction.\n      output_stride: If None, then the output will be computed at the nominal\n        network stride. If output_stride is not None, it specifies the requested\n        ratio of input to output spatial resolution.\n      include_root_block: If True, include the initial convolution followed by\n        max-pooling, if False excludes it.\n      spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n          of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n\n    Returns:\n      net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n        If global_pool is False, then height_out and width_out are reduced by a\n        factor of output_stride compared to the respective height_in and width_in,\n        else both height_out and width_out equal one. If num_classes is None, then\n        net is the output of the last ResNet block, potentially after global\n        average pooling. If num_classes is not None, net contains the pre-softmax\n        activations.\n      end_points: A dictionary from components of the network to the corresponding\n        activation.\n\n    Raises:\n      ValueError: If the target output_stride is not valid.\n    """"""\n    with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n        end_points_collection = sc.name + \'_end_points\'\n        with slim.arg_scope([slim.conv2d, bottleneck,\n                             resnet_utils.stack_blocks_dense],\n                            outputs_collections=end_points_collection):\n            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n                net = inputs\n                if include_root_block:\n                    if output_stride is not None:\n                        if output_stride % 4 != 0:\n                            raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n                        output_stride /= 4\n                    net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n                    net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n\n                    net = slim.utils.collect_named_outputs(end_points_collection, \'pool2\', net)\n\n                net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n                end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n\n                end_points[\'pool3\'] = end_points[scope + \'/block1\']\n                end_points[\'pool4\'] = end_points[scope + \'/block2\']\n                end_points[\'pool5\'] = net\n                return net, end_points\n\n\nresnet_v1.default_image_size = 224\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n    """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n    """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n    """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n    """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n\n\nif __name__ == \'__main__\':\n    input = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name=\'input\')\n    with slim.arg_scope(resnet_arg_scope()) as sc:\n        logits = resnet_v1_50(input)'"
frontends/resnet_v2.py,5,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom frontends import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.name,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks.\n      If 0 or None, we return the features before the logit layer.\n    is_training: whether batch_norm layers are in training mode.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n        To use this parameter, the input images must be smaller than 300x300\n        pixels, in which case the output logit layer does not contain spatial\n        information and can be removed.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is 0 or None,\n      then net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is a non-zero integer, net contains the\n      pre-softmax activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n\n          net = slim.utils.collect_named_outputs(end_points_collection, \'pool2\', net)\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n\n        end_points[\'pool3\'] = end_points[scope + \'/block1\']\n        end_points[\'pool4\'] = end_points[scope + \'/block2\']\n        end_points[\'pool5\'] = net\n        return net, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v2 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v2 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
frontends/se_resnext.py,61,"b'import tensorflow as tf\n\nimport math\n\nUSE_FUSED_BN = True\nBN_EPSILON = 9.999999747378752e-06\nBN_MOMENTUM = 0.99\n\nVAR_LIST = []\n\n# input image order: BGR, range [0-255]\n# mean_value: 104, 117, 123\n# only subtract mean is used\ndef constant_xavier_initializer(shape, group, dtype=tf.float32, uniform=True):\n    """"""Initializer function.""""""\n    if not dtype.is_floating:\n      raise TypeError(\'Cannot create initializer for non-floating point type.\')\n    # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n    # This is the right thing for matrix multiply and convolutions.\n    if shape:\n      fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n      fan_out = float(shape[-1])/group\n    else:\n      fan_in = 1.0\n      fan_out = 1.0\n    for dim in shape[:-2]:\n      fan_in *= float(dim)\n      fan_out *= float(dim)\n\n    # Average number of inputs and output connections.\n    n = (fan_in + fan_out) / 2.0\n    if uniform:\n      # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n      limit = math.sqrt(3.0 * 1.0 / n)\n      return tf.random_uniform(shape, -limit, limit, dtype, seed=None)\n    else:\n      # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n      trunc_stddev = math.sqrt(1.3 * 1.0 / n)\n      return tf.truncated_normal(shape, 0.0, trunc_stddev, dtype, seed=None)\n\n# for root block, use dummy input_filters, e.g. 128 rather than 64 for the first block\ndef se_bottleneck_block(inputs, input_filters, name_prefix, is_training, group, data_format=\'channels_last\', need_reduce=True, is_root=False, reduced_scale=16):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    strides_to_use = 1\n    residuals = inputs\n    if need_reduce:\n        strides_to_use = 1 if is_root else 2\n        proj_mapping = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_proj\', strides=(strides_to_use, strides_to_use),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        residuals = tf.layers.batch_normalization(proj_mapping, momentum=BN_MOMENTUM,\n                                name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n\n    reduced_inputs = tf.layers.conv2d(inputs, input_filters // 2, (1, 1), use_bias=False,\n                            name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                            padding=\'valid\', data_format=data_format, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer())\n    reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=BN_MOMENTUM,\n                                        name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                        epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n    if data_format == \'channels_first\':\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters // 2]\n        weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n    else:\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters // 2]\n        weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n    convolved = [tf.nn.convolution(x, weight, padding=\'VALID\', strides=[strides_to_use, strides_to_use], name=name_prefix + \'_group_conv\',\n                    data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n    if data_format == \'channels_first\':\n        conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n    else:\n        conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n    conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=BN_MOMENTUM, name=name_prefix + \'_3x3/bn\',\n                                        axis=bn_axis, epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n    increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=BN_MOMENTUM,\n                                        name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                        epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n    down_inputs = tf.layers.conv2d(pooled_inputs, input_filters // reduced_scale, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n    up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n    rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n    pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n    return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n    #return tf.nn.relu(residuals + prob_outputs * increase_inputs_bn, name=name_prefix + \'/relu\')\n\ndef se_resnext(input_image, scope, is_training = False, group=16, data_format=\'channels_last\', net_depth=50):\n    end_points = dict()\n\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    # the input image should in BGR order, note that this is not the common case in Tensorflow\n    # convert from RGB to BGR\n    if data_format == \'channels_last\':\n        image_channels = tf.unstack(input_image, axis=-1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=-1)\n    else:\n        image_channels = tf.unstack(input_image, axis=1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=1)\n    #swaped_input_image = input_image\n\n    if net_depth not in [50, 101]:\n        raise TypeError(\'Only ResNeXt50 or ResNeXt101 are currently supported.\')\n    input_depth = [256, 512, 1024, 2048] # the input depth of the the first block is dummy input\n    num_units = [3, 4, 6, 3] if net_depth==50 else [3, 4, 23, 3]\n\n    block_name_prefix = [\'conv2_{}\', \'conv3_{}\', \'conv4_{}\', \'conv5_{}\']\n\n    if data_format == \'channels_first\':\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [0, 0], [3, 3], [3, 3]])\n    else:\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [3, 3], [3, 3], [0, 0]])\n\n    inputs_features = tf.layers.conv2d(swaped_input_image, input_depth[0]//4, (7, 7), use_bias=False,\n                                name=\'conv1/7x7_s2\', strides=(2, 2),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    VAR_LIST.append(\'conv1/7x7_s2\')\n\n    inputs_features = tf.layers.batch_normalization(inputs_features, momentum=BN_MOMENTUM,\n                                        name=\'conv1/7x7_s2/bn\', axis=bn_axis,\n                                        epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    inputs_features = tf.nn.relu(inputs_features, name=\'conv1/relu_7x7_s2\')\n\n    inputs_features = tf.layers.max_pooling2d(inputs_features, [3, 3], [2, 2], padding=\'same\', data_format=data_format, name=\'pool1/3x3_s2\')\n\n    is_root = True\n    for ind, num_unit in enumerate(num_units):\n        need_reduce = True\n        for unit_index in range(1, num_unit+1):\n            inputs_features = se_bottleneck_block(inputs_features, input_depth[ind], block_name_prefix[ind].format(unit_index), is_training=is_training, group=group, data_format=data_format, need_reduce=need_reduce, is_root=is_root)\n            need_reduce = False\n        end_points[\'pool\' + str(ind)] = inputs_features\n        is_root = False\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(inputs_features, [2, 3], name=\'pool5/7x7_s1\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(inputs_features, [1, 2], name=\'pool5/7x7_s1\', keep_dims=True)\n\n    pooled_inputs = tf.layers.flatten(pooled_inputs)\n\n    # logits_output = tf.layers.dense(pooled_inputs, num_classes,\n    #                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n    #                             bias_initializer=tf.zeros_initializer(), use_bias=True)\n\n    logits_output = None\n\n    return logits_output, end_points, VAR_LIST\n'"
models/AdapNet.py,28,"b'# coding=utf-8\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nfrom frontends import resnet_v2\nimport os, sys\n\n\ndef Upsampling(inputs,scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3], stride=1):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, n_filters, kernel_size, stride=stride, activation_fn=None, normalizer_fn=None)\n    return net\n\ndef ResNetBlock_1(inputs, filters_1, filters_2):\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, filters_1, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n    net = slim.conv2d(net, filters_1, [3, 3], activation_fn=None, normalizer_fn=None)\n\n    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n    net = slim.conv2d(net, filters_2, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    net = tf.add(inputs, net)\n\n    return net\n\ndef ResNetBlock_2(inputs, filters_1, filters_2, s=1):\n    net_1 = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net_1 = slim.conv2d(net_1, filters_1, [1, 1], stride=s, activation_fn=None, normalizer_fn=None)\n\n    net_1 = tf.nn.relu(slim.batch_norm(net_1, fused=True))\n    net_1 = slim.conv2d(net_1, filters_1, [3, 3], activation_fn=None, normalizer_fn=None)\n\n    net_1 = tf.nn.relu(slim.batch_norm(net_1, fused=True))\n    net_1 = slim.conv2d(net_1, filters_2, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    net_2 = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net_2 = slim.conv2d(net_2, filters_2, [1, 1], stride=s, activation_fn=None, normalizer_fn=None)\n\n    net = tf.add(net_1, net_2)\n\n    return net\n\n\ndef MultiscaleBlock_1(inputs, filters_1, filters_2, filters_3, p, d):\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, filters_1, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    scale_1 = tf.nn.relu(slim.batch_norm(net, fused=True))\n    scale_1 = slim.conv2d(scale_1, filters_3 // 2, [3, 3], rate=p, activation_fn=None, normalizer_fn=None)\n    scale_2 = tf.nn.relu(slim.batch_norm(net, fused=True))\n    scale_2 = slim.conv2d(scale_2, filters_3 // 2, [3, 3], rate=d, activation_fn=None, normalizer_fn=None)\n    net = tf.concat((scale_1, scale_2), axis=-1)\n\n    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n    net = slim.conv2d(net, filters_2, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    net = tf.add(inputs, net)\n\n    return net\n\n\ndef MultiscaleBlock_2(inputs, filters_1, filters_2, filters_3, p, d):\n    net_1 = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net_1 = slim.conv2d(net_1, filters_1, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    scale_1 = tf.nn.relu(slim.batch_norm(net_1, fused=True))\n    scale_1 = slim.conv2d(scale_1, filters_3 // 2, [3, 3], rate=p, activation_fn=None, normalizer_fn=None)\n    scale_2 = tf.nn.relu(slim.batch_norm(net_1, fused=True))\n    scale_2 = slim.conv2d(scale_2, filters_3 // 2, [3, 3], rate=d, activation_fn=None, normalizer_fn=None)\n    net_1 = tf.concat((scale_1, scale_2), axis=-1)\n\n    net_1 = tf.nn.relu(slim.batch_norm(net_1, fused=True))\n    net_1 = slim.conv2d(net_1, filters_2, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    net_2 = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net_2 = slim.conv2d(net_2, filters_2, [1, 1], activation_fn=None, normalizer_fn=None)\n\n    net = tf.add(net_1, net_2)\n\n    return net\n\n\n\n\n\n\ndef build_adaptnet(inputs, num_classes):\n    """"""\n    Builds the AdaptNet model. \n\n    Arguments:\n      inputs: The input tensor= \n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n\n    Returns:\n      AdaptNet model\n    """"""\n    net = ConvBlock(inputs, n_filters=64, kernel_size=[3, 3])\n    net = ConvBlock(net, n_filters=64, kernel_size=[7, 7], stride=2)\n    net = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\n    net = ResNetBlock_2(net, filters_1=64, filters_2=256, s=1)\n    net = ResNetBlock_1(net, filters_1=64, filters_2=256)\n    net = ResNetBlock_1(net, filters_1=64, filters_2=256)\n\n    net = ResNetBlock_2(net, filters_1=128, filters_2=512, s=2)\n    net = ResNetBlock_1(net, filters_1=128, filters_2=512)\n    net = ResNetBlock_1(net, filters_1=128, filters_2=512)\n\n    skip_connection = ConvBlock(net, n_filters=12, kernel_size=[1, 1])\n\n\n    net = MultiscaleBlock_1(net, filters_1=128, filters_2=512, filters_3=64, p=1, d=2)\n\n    net = ResNetBlock_2(net, filters_1=256, filters_2=1024, s=2)\n    net = ResNetBlock_1(net, filters_1=256, filters_2=1024)\n    net = MultiscaleBlock_1(net, filters_1=256, filters_2=1024, filters_3=64, p=1, d=2)\n    net = MultiscaleBlock_1(net, filters_1=256, filters_2=1024, filters_3=64, p=1, d=4)\n    net = MultiscaleBlock_1(net, filters_1=256, filters_2=1024, filters_3=64, p=1, d=8)\n    net = MultiscaleBlock_1(net, filters_1=256, filters_2=1024, filters_3=64, p=1, d=16)\n\n    net = MultiscaleBlock_2(net, filters_1=512, filters_2=2048, filters_3=512, p=2, d=4)\n    net = MultiscaleBlock_1(net, filters_1=512, filters_2=2048, filters_3=512, p=2, d=8)\n    net = MultiscaleBlock_1(net, filters_1=512, filters_2=2048, filters_3=512, p=2, d=16)\n\n    net = ConvBlock(net, n_filters=12, kernel_size=[1, 1])\n    net = Upsampling(net, scale=2)\n\n    net = tf.add(skip_connection, net)\n\n    net = Upsampling(net, scale=8)\n\n\n    \n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net\n\n\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)'"
models/BiSeNet.py,15,"b'# coding=utf-8\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom builders import frontend_builder\nimport numpy as np\nimport os, sys\n\ndef Upsampling(inputs,scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic conv transpose block for Encoder-Decoder upsampling\n    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d_transpose(net, n_filters, kernel_size=[3, 3], stride=[scale, scale], activation_fn=None)\n    return net\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3], strides=1):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = slim.conv2d(inputs, n_filters, kernel_size, stride=[strides, strides], activation_fn=None, normalizer_fn=None)\n    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n    return net\n\ndef AttentionRefinementModule(inputs, n_filters):\n\n    # Global average pooling\n    net = tf.reduce_mean(inputs, [1, 2], keep_dims=True)\n\n    net = slim.conv2d(net, n_filters, kernel_size=[1, 1])\n    net = slim.batch_norm(net, fused=True)\n    net = tf.sigmoid(net)\n\n    net = tf.multiply(inputs, net)\n\n    return net\n\ndef FeatureFusionModule(input_1, input_2, n_filters):\n    inputs = tf.concat([input_1, input_2], axis=-1)\n    inputs = ConvBlock(inputs, n_filters=n_filters, kernel_size=[3, 3])\n\n    # Global average pooling\n    net = tf.reduce_mean(inputs, [1, 2], keep_dims=True)\n    \n    net = slim.conv2d(net, n_filters, kernel_size=[1, 1])\n    net = tf.nn.relu(net)\n    net = slim.conv2d(net, n_filters, kernel_size=[1, 1])\n    net = tf.sigmoid(net)\n\n    net = tf.multiply(inputs, net)\n\n    net = tf.add(inputs, net)\n\n    return net\n\n\ndef build_bisenet(inputs, num_classes, preset_model=\'BiSeNet\', frontend=""ResNet101"", weight_decay=1e-5, is_training=True, pretrained_dir=""models""):\n    """"""\n    Builds the BiSeNet model. \n\n    Arguments:\n      inputs: The input tensor=\n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n\n    Returns:\n      BiSeNet model\n    """"""\n\n    ### The spatial path\n    ### The number of feature maps for each convolution is not specified in the paper\n    ### It was chosen here to be equal to the number of feature maps of a classification\n    ### model at each corresponding stage \n    spatial_net = ConvBlock(inputs, n_filters=64, kernel_size=[3, 3], strides=2)\n    spatial_net = ConvBlock(spatial_net, n_filters=128, kernel_size=[3, 3], strides=2)\n    spatial_net = ConvBlock(spatial_net, n_filters=256, kernel_size=[3, 3], strides=2)\n\n\n    ### Context path\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n    net_4 = AttentionRefinementModule(end_points[\'pool4\'], n_filters=512)\n\n    net_5 = AttentionRefinementModule(end_points[\'pool5\'], n_filters=2048)\n\n    global_channels = tf.reduce_mean(net_5, [1, 2], keep_dims=True)\n    net_5_scaled = tf.multiply(global_channels, net_5)\n\n    ### Combining the paths\n    net_4 = Upsampling(net_4, scale=2)\n    net_5_scaled = Upsampling(net_5_scaled, scale=4)\n\n    context_net = tf.concat([net_4, net_5_scaled], axis=-1)\n\n    net = FeatureFusionModule(input_1=spatial_net, input_2=context_net, n_filters=num_classes)\n\n\n    ### Final upscaling and finish\n    net = Upsampling(net, scale=8)\n    \n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net, init_fn\n\n'"
models/DDSC.py,10,"b'# coding=utf-8\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom builders import frontend_builder\nimport numpy as np\nimport os, sys\n\ndef Upsampling(inputs,scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic conv transpose block for Encoder-Decoder upsampling\n    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d_transpose(net, n_filters, kernel_size=[3, 3], stride=[scale, scale], activation_fn=None)\n    return net\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    return net\n\ndef GroupedConvolutionBlock(inputs, grouped_channels, cardinality=32):\n    group_list = []\n\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n\n    for c in range(cardinality):\n        x = net[:, :, :, c * grouped_channels:(c + 1) * grouped_channels]\n\n        x = slim.conv2d(x, grouped_channels, kernel_size=[3, 3])\n\n        group_list.append(x)\n\n    group_merge = tf.concat(group_list, axis=-1)\n\n    return group_merge\n\ndef ResNeXtBlock(inputs, n_filters_out, bottleneck_factor=2, cardinality=32):\n\n    assert not (n_filters_out // 2) % cardinality\n    grouped_channels = (n_filters_out // 2) // cardinality\n\n    net = ConvBlock(inputs, n_filters=n_filters_out / bottleneck_factor, kernel_size=[1, 1])\n    net = GroupedConvolutionBlock(net, grouped_channels, cardinality=32)\n    net = ConvBlock(net, n_filters=n_filters_out, kernel_size=[1, 1])\n\n\n    net = tf.add(inputs, net)\n\n    return net\n\ndef EncoderAdaptionBlock(inputs, n_filters, bottleneck_factor=2, cardinality=32):\n\n    net = ConvBlock(inputs, n_filters, kernel_size=[3, 3])\n    net = ResNeXtBlock(net, n_filters_out=n_filters, bottleneck_factor=bottleneck_factor)\n    net = ResNeXtBlock(net, n_filters_out=n_filters, bottleneck_factor=bottleneck_factor)\n    net = ResNeXtBlock(net, n_filters_out=n_filters, bottleneck_factor=bottleneck_factor)\n    net = ConvBlock(net, n_filters, kernel_size=[3, 3])\n\n    return net\n\n\ndef SemanticFeatureGenerationBlock(inputs, D_features, D_prime_features, O_features, bottleneck_factor=2, cardinality=32):\n\n    d_1 = ConvBlock(inputs, D_features, kernel_size=[3, 3])\n    pool_1 = slim.pool(d_1, [5, 5], stride=[1, 1], pooling_type=\'MAX\')\n    d_prime_1 = ConvBlock(pool_1, D_prime_features, kernel_size=[3, 3])\n\n    d_2 = ConvBlock(pool_1, D_features, kernel_size=[3, 3])\n    pool_2 = slim.pool(d_2, [5, 5], stride=[1, 1], pooling_type=\'MAX\')\n    d_prime_2 = ConvBlock(pool_2, D_prime_features, kernel_size=[3, 3])\n\n    d_3 = ConvBlock(pool_2, D_features, kernel_size=[3, 3])\n    pool_3 = slim.pool(d_3, [5, 5], stride=[1, 1], pooling_type=\'MAX\')\n    d_prime_3 = ConvBlock(pool_3, D_prime_features, kernel_size=[3, 3])\n\n    d_4 = ConvBlock(pool_3, D_features, kernel_size=[3, 3])\n    pool_4 = slim.pool(d_4, [5, 5], stride=[1, 1], pooling_type=\'MAX\')\n    d_prime_4 = ConvBlock(pool_4, D_prime_features, kernel_size=[3, 3])\n\n\n    net = tf.concat([d_prime_1, d_prime_2, d_prime_3, d_prime_4], axis=-1)\n\n    net = ConvBlock(net, n_filters=D_features, kernel_size=[3, 3])\n\n    net = ResNeXtBlock(net, n_filters_out=D_features, bottleneck_factor=bottleneck_factor)\n    net = ResNeXtBlock(net, n_filters_out=D_features, bottleneck_factor=bottleneck_factor)\n    net = ResNeXtBlock(net, n_filters_out=D_features, bottleneck_factor=bottleneck_factor)\n    net = ResNeXtBlock(net, n_filters_out=D_features, bottleneck_factor=bottleneck_factor)\n\n    net = ConvBlock(net, O_features, kernel_size=[3, 3])\n\n    return net\n\n\n\ndef build_ddsc(inputs, num_classes, preset_model=\'DDSC\', frontend=""ResNet101"", weight_decay=1e-5, is_training=True, pretrained_dir=""models""):\n    """"""\n    Builds the Dense Decoder Shortcut Connections model. \n\n    Arguments:\n      inputs: The input tensor=\n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n\n    Returns:\n      Dense Decoder Shortcut Connections model\n    """"""\n\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n    ### Adapting features for all stages\n    decoder_4 = EncoderAdaptionBlock(end_points[\'pool5\'], n_filters=1024)\n    decoder_3 = EncoderAdaptionBlock(end_points[\'pool4\'], n_filters=512)\n    decoder_2 = EncoderAdaptionBlock(end_points[\'pool3\'], n_filters=256)\n    decoder_1 = EncoderAdaptionBlock(end_points[\'pool2\'], n_filters=128)\n\n    decoder_4 = SemanticFeatureGenerationBlock(decoder_4, D_features=1024, D_prime_features = 1024 / 4, O_features=1024)\n\n    ### Fusing features from 3 and 4\n    decoder_4 = ConvBlock(decoder_4, n_filters=512, kernel_size=[3, 3])\n    decoder_4 = Upsampling(decoder_4, scale=2)\n\n    decoder_3 = ConvBlock(decoder_3, n_filters=512, kernel_size=[3, 3])\n\n    decoder_3 = tf.add_n([decoder_4, decoder_3])\n\n    decoder_3 = SemanticFeatureGenerationBlock(decoder_3, D_features=512, D_prime_features = 512 / 4, O_features=512)\n\n    ### Fusing features from 2, 3, 4\n    decoder_4 = ConvBlock(decoder_4, n_filters=256, kernel_size=[3, 3])\n    decoder_4 = Upsampling(decoder_4, scale=4)\n\n    decoder_3 = ConvBlock(decoder_3, n_filters=256, kernel_size=[3, 3])\n    decoder_3 = Upsampling(decoder_3, scale=2)\n\n    decoder_2 = ConvBlock(decoder_2, n_filters=256, kernel_size=[3, 3])\n\n    decoder_2 = tf.add_n([decoder_4, decoder_3, decoder_2])\n\n    decoder_2 = SemanticFeatureGenerationBlock(decoder_2, D_features=256, D_prime_features = 256 / 4, O_features=256)\n\n    ### Fusing features from 1, 2, 3, 4\n    decoder_4 = ConvBlock(decoder_4, n_filters=128, kernel_size=[3, 3])\n    decoder_4 = Upsampling(decoder_4, scale=8)\n\n    decoder_3 = ConvBlock(decoder_3, n_filters=128, kernel_size=[3, 3])\n    decoder_3 = Upsampling(decoder_3, scale=4)\n\n    decoder_2 = ConvBlock(decoder_2, n_filters=128, kernel_size=[3, 3])\n    decoder_2 = Upsampling(decoder_2, scale=2)\n\n    decoder_1 = ConvBlock(decoder_1, n_filters=128, kernel_size=[3, 3])\n\n    decoder_1 = tf.add_n([decoder_4, decoder_3, decoder_2, decoder_1])\n\n    decoder_1 = SemanticFeatureGenerationBlock(decoder_1, D_features=128, D_prime_features = 128 / 4, O_features=num_classes)\n\n\n    ### Final upscaling and finish\n    net = Upsampling(decoder_1, scale=4)\n    \n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net, init_fn\n\n'"
models/DeepLabV3.py,11,"b'# coding=utf-8\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nfrom builders import frontend_builder\nimport os, sys\n\ndef Upsampling(inputs,feature_map_shape):\n    return tf.image.resize_bilinear(inputs, size=feature_map_shape)\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic conv transpose block for Encoder-Decoder upsampling\n    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d_transpose(net, n_filters, kernel_size=[3, 3], stride=[scale, scale], activation_fn=None)\n    return net\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    return net\n\ndef AtrousSpatialPyramidPoolingModule(inputs, depth=256):\n    """"""\n\n    ASPP consists of (a) one 1\xc3\x971 convolution and three 3\xc3\x973 convolutions with rates = (6, 12, 18) when output stride = 16\n    (all with 256 filters and batch normalization), and (b) the image-level features as described in the paper\n\n    """"""\n\n    feature_map_size = tf.shape(inputs)\n\n    # Global average pooling\n    image_features = tf.reduce_mean(inputs, [1, 2], keep_dims=True)\n\n    image_features = slim.conv2d(image_features, depth, [1, 1], activation_fn=None)\n    image_features = tf.image.resize_bilinear(image_features, (feature_map_size[1], feature_map_size[2]))\n\n    atrous_pool_block_1 = slim.conv2d(inputs, depth, [1, 1], activation_fn=None)\n\n    atrous_pool_block_6 = slim.conv2d(inputs, depth, [3, 3], rate=6, activation_fn=None)\n\n    atrous_pool_block_12 = slim.conv2d(inputs, depth, [3, 3], rate=12, activation_fn=None)\n\n    atrous_pool_block_18 = slim.conv2d(inputs, depth, [3, 3], rate=18, activation_fn=None)\n\n    net = tf.concat((image_features, atrous_pool_block_1, atrous_pool_block_6, atrous_pool_block_12, atrous_pool_block_18), axis=3)\n    net = slim.conv2d(net, depth, [1, 1], scope=""conv_1x1_output"", activation_fn=None)\n\n    return net\n\n\n\n\n\ndef build_deeplabv3(inputs, num_classes, preset_model=\'DeepLabV3\', frontend=""Res101"", weight_decay=1e-5, is_training=True, pretrained_dir=""models""):\n    """"""\n    Builds the DeepLabV3 model. \n\n    Arguments:\n      inputs: The input tensor= \n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n\n    Returns:\n      DeepLabV3 model\n    """"""\n\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n    label_size = tf.shape(inputs)[1:3]\n\n    net = AtrousSpatialPyramidPoolingModule(end_points[\'pool4\'])\n\n    net = Upsampling(net, label_size)\n    \n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net, init_fn\n\n\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)'"
models/DeepLabV3_plus.py,15,"b'# coding=utf-8\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom builders import frontend_builder\nimport numpy as np\nimport os, sys\n\ndef Upsampling(inputs,feature_map_shape):\n    return tf.image.resize_bilinear(inputs, size=tf.cast(feature_map_shape, tf.int32))\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic conv transpose block for Encoder-Decoder upsampling\n    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d_transpose(net, n_filters, kernel_size=[3, 3], stride=[scale, scale], activation_fn=None)\n    return net\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    return net\n\ndef AtrousSpatialPyramidPoolingModule(inputs, depth=256):\n    """"""\n\n    ASPP consists of (a) one 1\xc3\x971 convolution and three 3\xc3\x973 convolutions with rates = (6, 12, 18) when output stride = 16\n    (all with 256 filters and batch normalization), and (b) the image-level features as described in the paper\n\n    """"""\n\n    feature_map_size = tf.shape(inputs)\n\n    # Global average pooling\n    image_features = tf.reduce_mean(inputs, [1, 2], keep_dims=True)\n\n    image_features = slim.conv2d(image_features, depth, [1, 1], activation_fn=None)\n    image_features = tf.image.resize_bilinear(image_features, (feature_map_size[1], feature_map_size[2]))\n\n    atrous_pool_block_1 = slim.conv2d(inputs, depth, [1, 1], activation_fn=None)\n\n    atrous_pool_block_6 = slim.conv2d(inputs, depth, [3, 3], rate=6, activation_fn=None)\n\n    atrous_pool_block_12 = slim.conv2d(inputs, depth, [3, 3], rate=12, activation_fn=None)\n\n    atrous_pool_block_18 = slim.conv2d(inputs, depth, [3, 3], rate=18, activation_fn=None)\n\n    net = tf.concat((image_features, atrous_pool_block_1, atrous_pool_block_6, atrous_pool_block_12, atrous_pool_block_18), axis=3)\n\n    return net\n\n\n\n\n\ndef build_deeplabv3_plus(inputs, num_classes, preset_model=\'DeepLabV3+\', frontend=""ResNet101"", weight_decay=1e-5, is_training=True, pretrained_dir=""models""):\n    """"""\n    Builds the DeepLabV3 model. \n\n    Arguments:\n      inputs: The input tensor= \n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n\n    Returns:\n      DeepLabV3 model\n    """"""\n\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n\n    label_size = tf.shape(inputs)[1:3]\n\n    encoder_features = end_points[\'pool2\']\n\n    net = AtrousSpatialPyramidPoolingModule(end_points[\'pool4\'])\n    net = slim.conv2d(net, 256, [1, 1], scope=""conv_1x1_output"", activation_fn=None)\n    decoder_features = Upsampling(net, label_size / 4)\n\n    encoder_features = slim.conv2d(encoder_features, 48, [1, 1], activation_fn=tf.nn.relu, normalizer_fn=None)\n\n    net = tf.concat((encoder_features, decoder_features), axis=3)\n\n    net = slim.conv2d(net, 256, [3, 3], activation_fn=tf.nn.relu, normalizer_fn=None)\n    net = slim.conv2d(net, 256, [3, 3], activation_fn=tf.nn.relu, normalizer_fn=None)\n\n    net = Upsampling(net, label_size)\n    \n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net, init_fn\n\n\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)\n'"
models/DenseASPP.py,7,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom builders import frontend_builder\nimport os, sys\n\n\ndef Upsampling(inputs,scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\n\n\ndef DilatedConvBlock(inputs, n_filters, rate=1, kernel_size=[3, 3]):\n    """"""\n    Basic dilated conv block \n    Apply successivly BatchNormalization, ReLU nonlinearity, dilated convolution \n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, n_filters, kernel_size, rate=rate, activation_fn=None, normalizer_fn=None)\n    return net\n\n\n\ndef build_dense_aspp(inputs, num_classes, preset_model=\'DenseASPP\', frontend=""ResNet101"", weight_decay=1e-5, is_training=True, pretrained_dir=""models""):\n    \n\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n    init_features = end_points[\'pool3\']\n\n    ### First block, rate = 3\n    d_3_features = DilatedConvBlock(init_features, n_filters=256, kernel_size=[1, 1])\n    d_3 = DilatedConvBlock(d_3_features, n_filters=64, rate=3, kernel_size=[3, 3])\n\n    ### Second block, rate = 6\n    d_4 = tf.concat([init_features, d_3], axis=-1)\n    d_4 = DilatedConvBlock(d_4, n_filters=256, kernel_size=[1, 1])\n    d_4 = DilatedConvBlock(d_4, n_filters=64, rate=6, kernel_size=[3, 3])\n\n    ### Third block, rate = 12\n    d_5 = tf.concat([init_features, d_3, d_4], axis=-1)\n    d_5 = DilatedConvBlock(d_5, n_filters=256, kernel_size=[1, 1])\n    d_5 = DilatedConvBlock(d_5, n_filters=64, rate=12, kernel_size=[3, 3])\n\n    ### Fourth block, rate = 18\n    d_6 = tf.concat([init_features, d_3, d_4, d_5], axis=-1)\n    d_6 = DilatedConvBlock(d_6, n_filters=256, kernel_size=[1, 1])\n    d_6 = DilatedConvBlock(d_6, n_filters=64, rate=18, kernel_size=[3, 3])\n\n    ### Fifth block, rate = 24\n    d_7 = tf.concat([init_features, d_3, d_4, d_5, d_6], axis=-1)\n    d_7 = DilatedConvBlock(d_7, n_filters=256, kernel_size=[1, 1])\n    d_7 = DilatedConvBlock(d_7, n_filters=64, rate=24, kernel_size=[3, 3])\n\n    full_block = tf.concat([init_features, d_3, d_4, d_5, d_6, d_7], axis=-1)\n    \n    net = slim.conv2d(full_block, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    net = Upsampling(net, scale=8)\n\n    return net, init_fn'"
models/Encoder_Decoder.py,6,"b'from __future__ import division\nimport os,time,cv2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\ndef conv_block(inputs, n_filters, kernel_size=[3, 3], dropout_p=0.0):\n\t""""""\n\tBasic conv block for Encoder-Decoder\n\tApply successivly Convolution, BatchNormalization, ReLU nonlinearity\n\tDropout (if dropout_p > 0) on the inputs\n\t""""""\n\tconv = slim.conv2d(inputs, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n\tout = tf.nn.relu(slim.batch_norm(conv, fused=True))\n\tif dropout_p != 0.0:\n\t  out = slim.dropout(out, keep_prob=(1.0-dropout_p))\n\treturn out\n\ndef conv_transpose_block(inputs, n_filters, kernel_size=[3, 3], dropout_p=0.0):\n\t""""""\n\tBasic conv transpose block for Encoder-Decoder upsampling\n\tApply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n\tDropout (if dropout_p > 0) on the inputs\n\t""""""\n\tconv = slim.conv2d_transpose(inputs, n_filters, kernel_size=[3, 3], stride=[2, 2], activation_fn=None)\n\tout = tf.nn.relu(slim.batch_norm(conv))\n\tif dropout_p != 0.0:\n\t  out = slim.dropout(out, keep_prob=(1.0-dropout_p))\n\treturn out\n\ndef build_encoder_decoder(inputs, num_classes, preset_model = ""Encoder-Decoder"", dropout_p=0.5, scope=None):\n\t""""""\n\tBuilds the Encoder-Decoder model. Inspired by SegNet with some modifications\n\tOptionally includes skip connections\n\n\tArguments:\n\t  inputs: the input tensor\n\t  n_classes: number of classes\n\t  dropout_p: dropout rate applied after each convolution (0. for not using)\n\n\tReturns:\n\t  Encoder-Decoder model\n\t""""""\n\n\n\tif preset_model == ""Encoder-Decoder"":\n\t\thas_skip = False\n\telif preset_model == ""Encoder-Decoder-Skip"":\n\t\thas_skip = True\n\telse:\n\t\traise ValueError(""Unsupported Encoder-Decoder model \'%s\'. This function only supports Encoder-Decoder and Encoder-Decoder-Skip"" % (preset_model))\n\n\t#####################\n\t# Downsampling path #\n\t#####################\n\tnet = conv_block(inputs, 64)\n\tnet = conv_block(net, 64)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_1 = net\n\n\tnet = conv_block(net, 128)\n\tnet = conv_block(net, 128)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_2 = net\n\n\tnet = conv_block(net, 256)\n\tnet = conv_block(net, 256)\n\tnet = conv_block(net, 256)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_3 = net\n\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_4 = net\n\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\n\n\t#####################\n\t# Upsampling path #\n\t#####################\n\tnet = conv_transpose_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 512)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_4)\n\n\tnet = conv_transpose_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 512)\n\tnet = conv_block(net, 256)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_3)\n\n\tnet = conv_transpose_block(net, 256)\n\tnet = conv_block(net, 256)\n\tnet = conv_block(net, 256)\n\tnet = conv_block(net, 128)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_2)\n\n\tnet = conv_transpose_block(net, 128)\n\tnet = conv_block(net, 128)\n\tnet = conv_block(net, 64)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_1)\n\n\tnet = conv_transpose_block(net, 64)\n\tnet = conv_block(net, 64)\n\tnet = conv_block(net, 64)\n\n\t#####################\n\t#      Softmax      #\n\t#####################\n\tnet = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\treturn net'"
models/FC_DenseNet_Tiramisu.py,8,"b'from __future__ import division\nimport os,time,cv2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\ndef preact_conv(inputs, n_filters, kernel_size=[3, 3], dropout_p=0.2):\n    """"""\n    Basic pre-activation layer for DenseNets\n    Apply successivly BatchNormalization, ReLU nonlinearity, Convolution and\n    Dropout (if dropout_p > 0) on the inputs\n    """"""\n    preact = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    conv = slim.conv2d(preact, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    if dropout_p != 0.0:\n      conv = slim.dropout(conv, keep_prob=(1.0-dropout_p))\n    return conv\n\ndef DenseBlock(stack, n_layers, growth_rate, dropout_p, scope=None):\n  """"""\n  DenseBlock for DenseNet and FC-DenseNet\n  Arguments:\n    stack: input 4D tensor\n    n_layers: number of internal layers\n    growth_rate: number of feature maps per internal layer\n  Returns:\n    stack: current stack of feature maps (4D tensor)\n    new_features: 4D tensor containing only the new feature maps generated\n      in this block\n  """"""\n  with tf.name_scope(scope) as sc:\n    new_features = []\n    for j in range(n_layers):\n      # Compute new feature maps\n      layer = preact_conv(stack, growth_rate, dropout_p=dropout_p)\n      new_features.append(layer)\n      # Stack new layer\n      stack = tf.concat([stack, layer], axis=-1)\n    new_features = tf.concat(new_features, axis=-1)\n    return stack, new_features\n\n\ndef TransitionDown(inputs, n_filters, dropout_p=0.2, scope=None):\n  """"""\n  Transition Down (TD) for FC-DenseNet\n  Apply 1x1 BN + ReLU + conv then 2x2 max pooling\n  """"""\n  with tf.name_scope(scope) as sc:\n    l = preact_conv(inputs, n_filters, kernel_size=[1, 1], dropout_p=dropout_p)\n    l = slim.pool(l, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n    return l\n\n\ndef TransitionUp(block_to_upsample, skip_connection, n_filters_keep, scope=None):\n  """"""\n  Transition Up for FC-DenseNet\n  Performs upsampling on block_to_upsample by a factor 2 and concatenates it with the skip_connection\n  """"""\n  with tf.name_scope(scope) as sc:\n    # Upsample\n    l = slim.conv2d_transpose(block_to_upsample, n_filters_keep, kernel_size=[3, 3], stride=[2, 2], activation_fn=None)\n    # Concatenate with skip connection\n    l = tf.concat([l, skip_connection], axis=-1)\n    return l\n\ndef build_fc_densenet(inputs, num_classes, preset_model=\'FC-DenseNet56\', n_filters_first_conv=48, n_pool=5, growth_rate=12, n_layers_per_block=4, dropout_p=0.2, scope=None):\n    """"""\n    Builds the FC-DenseNet model\n\n    Arguments:\n      inputs: the input tensor\n      preset_model: The model you want to use\n      n_classes: number of classes\n      n_filters_first_conv: number of filters for the first convolution applied\n      n_pool: number of pooling layers = number of transition down = number of transition up\n      growth_rate: number of new feature maps created by each layer in a dense block\n      n_layers_per_block: number of layers per block. Can be an int or a list of size 2 * n_pool + 1\n      dropout_p: dropout rate applied after each convolution (0. for not using)\n\n    Returns:\n      Fc-DenseNet model\n    """"""\n\n    if preset_model == \'FC-DenseNet56\':\n      n_pool=5\n      growth_rate=12\n      n_layers_per_block=4\n    elif preset_model == \'FC-DenseNet67\':\n      n_pool=5\n      growth_rate=16\n      n_layers_per_block=5\n    elif preset_model == \'FC-DenseNet103\':\n      n_pool=5\n      growth_rate=16\n      n_layers_per_block=[4, 5, 7, 10, 12, 15, 12, 10, 7, 5, 4]\n    else:\n      raise ValueError(""Unsupported FC-DenseNet model \'%s\'. This function only supports FC-DenseNet56, FC-DenseNet67, and FC-DenseNet103"" % (preset_model)) \n\n    if type(n_layers_per_block) == list:\n        assert (len(n_layers_per_block) == 2 * n_pool + 1)\n    elif type(n_layers_per_block) == int:\n        n_layers_per_block = [n_layers_per_block] * (2 * n_pool + 1)\n    else:\n        raise ValueError\n\n    with tf.variable_scope(scope, preset_model, [inputs]) as sc:\n\n      #####################\n      # First Convolution #\n      #####################\n      # We perform a first convolution.\n      stack = slim.conv2d(inputs, n_filters_first_conv, [3, 3], scope=\'first_conv\', activation_fn=None)\n\n      n_filters = n_filters_first_conv\n      \n      #####################\n      # Downsampling path #\n      #####################\n\n      skip_connection_list = []\n\n      for i in range(n_pool):\n        # Dense Block\n        stack, _ = DenseBlock(stack, n_layers_per_block[i], growth_rate, dropout_p, scope=\'denseblock%d\' % (i+1))\n        n_filters += growth_rate * n_layers_per_block[i]\n        # At the end of the dense block, the current stack is stored in the skip_connections list\n        skip_connection_list.append(stack)\n\n        # Transition Down\n        stack = TransitionDown(stack, n_filters, dropout_p, scope=\'transitiondown%d\'%(i+1))\n\n      skip_connection_list = skip_connection_list[::-1]\n\n      #####################\n      #     Bottleneck    #\n      #####################\n\n      # Dense Block\n      # We will only upsample the new feature maps\n      stack, block_to_upsample = DenseBlock(stack, n_layers_per_block[n_pool], growth_rate, dropout_p, scope=\'denseblock%d\' % (n_pool + 1))\n\n\n      #######################\n      #   Upsampling path   #\n      #######################\n\n      for i in range(n_pool):\n        # Transition Up ( Upsampling + concatenation with the skip connection)\n        n_filters_keep = growth_rate * n_layers_per_block[n_pool + i]\n        stack = TransitionUp(block_to_upsample, skip_connection_list[i], n_filters_keep, scope=\'transitionup%d\' % (n_pool + i + 1))\n\n        # Dense Block\n        # We will only upsample the new feature maps\n        stack, block_to_upsample = DenseBlock(stack, n_layers_per_block[n_pool + i + 1], growth_rate, dropout_p, scope=\'denseblock%d\' % (n_pool + i + 2))\n\n\n      #####################\n      #      Softmax      #\n      #####################\n      net = slim.conv2d(stack, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n      return net'"
models/FRRN.py,11,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\n\ndef Upsampling(inputs,scale):\n    return tf.image.resize_nearest_neighbor(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\ndef Unpooling(inputs,scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\ndef ResidualUnit(inputs, n_filters=48, filter_size=3):\n    """"""\n    A local residual unit\n\n    Arguments:\n      inputs: The input tensor\n      n_filters: Number of output feature maps for each conv\n      filter_size: Size of convolution kernel\n\n    Returns:\n      Output of local residual block\n    """"""\n\n    net = slim.conv2d(inputs, n_filters, filter_size, activation_fn=None)\n    net = slim.batch_norm(net, fused=True)\n    net = tf.nn.relu(net)\n    net = slim.conv2d(net, n_filters, filter_size, activation_fn=None)\n    net = slim.batch_norm(net, fused=True)\n\n    return net\n\ndef FullResolutionResidualUnit(pool_stream, res_stream, n_filters_3, n_filters_1, pool_scale):\n    """"""\n    A full resolution residual unit\n\n    Arguments:\n      pool_stream: The inputs from the pooling stream\n      res_stream: The inputs from the residual stream\n      n_filters_3: Number of output feature maps for each 3x3 conv\n      n_filters_1: Number of output feature maps for each 1x1 conv\n      pool_scale: scale of the pooling layer i.e window size and stride\n\n    Returns:\n      Output of full resolution residual block\n    """"""\n\n    G = tf.concat([pool_stream, slim.pool(res_stream, [pool_scale, pool_scale], stride=[pool_scale, pool_scale], pooling_type=\'MAX\')], axis=-1)\n\n    \n\n    net = slim.conv2d(G, n_filters_3, kernel_size=3, activation_fn=None)\n    net = slim.batch_norm(net, fused=True)\n    net = tf.nn.relu(net)\n    net = slim.conv2d(net, n_filters_3, kernel_size=3, activation_fn=None)\n    net = slim.batch_norm(net, fused=True)\n    pool_stream_out = tf.nn.relu(net)\n\n    net = slim.conv2d(pool_stream_out, n_filters_1, kernel_size=1, activation_fn=None)\n    net = Upsampling(net, scale=pool_scale)\n    res_stream_out = tf.add(res_stream, net)\n\n    return pool_stream_out, res_stream_out\n\n\n\ndef build_frrn(inputs, num_classes, preset_model=\'FRRN-A\'):\n    """"""\n    Builds the Full Resolution Residual Network model. \n\n    Arguments:\n      inputs: The input tensor\n      preset_model: Which model you want to use. Select FRRN-A or FRRN-B\n      num_classes: Number of classes\n\n    Returns:\n      FRRN model\n    """"""\n\n    if preset_model == \'FRRN-A\':\n\n        #####################\n        # Initial Stage   \n        #####################\n        net = slim.conv2d(inputs, 48, kernel_size=5, activation_fn=None)\n        net = slim.batch_norm(net, fused=True)\n        net = tf.nn.relu(net)\n\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n\n\n        #####################\n        # Downsampling Path \n        #####################\n        pool_stream = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n        res_stream = slim.conv2d(net, 32, kernel_size=1, activation_fn=None)\n\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n\n        pool_stream = slim.pool(pool_stream, [2, 2], stride=[2, 2], pooling_type=\'MAX\') \n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n\n        pool_stream = slim.pool(pool_stream, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=8)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=8)\n\n        pool_stream = slim.pool(pool_stream, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=16)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=16)\n\n        #####################\n        # Upsampling Path \n        #####################\n        pool_stream = Unpooling(pool_stream, 2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=8)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=8)\n\n        pool_stream = Unpooling(pool_stream, 2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n\n        pool_stream = Unpooling(pool_stream, 2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n\n        pool_stream = Unpooling(pool_stream, 2)\n\n        #####################\n        # Final Stage \n        #####################\n        net = tf.concat([pool_stream, res_stream], axis=-1)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n\n        net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n        return net\n\n        \n    elif preset_model == \'FRRN-B\':\n        #####################\n        # Initial Stage   \n        #####################\n        net = slim.conv2d(inputs, 48, kernel_size=5, activation_fn=None)\n        net = slim.batch_norm(net, fused=True)\n        net = tf.nn.relu(net)\n\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n\n\n        #####################\n        # Downsampling Path \n        #####################\n        pool_stream = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n        res_stream = slim.conv2d(net, 32, kernel_size=1, activation_fn=None)\n\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n\n        pool_stream = slim.pool(pool_stream, [2, 2], stride=[2, 2], pooling_type=\'MAX\') \n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n\n        pool_stream = slim.pool(pool_stream, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=8)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=8)\n\n        pool_stream = slim.pool(pool_stream, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=16)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=16)\n\n        pool_stream = slim.pool(pool_stream, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=32)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=384, n_filters_1=32, pool_scale=32)\n\n        #####################\n        # Upsampling Path \n        #####################\n        pool_stream = Unpooling(pool_stream, 2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=16)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=16)\n\n        pool_stream = Unpooling(pool_stream, 2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=8)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=8)\n\n        pool_stream = Unpooling(pool_stream, 2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=192, n_filters_1=32, pool_scale=4)\n\n        pool_stream = Unpooling(pool_stream, 2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n        pool_stream, res_stream = FullResolutionResidualUnit(pool_stream=pool_stream, res_stream=res_stream, n_filters_3=96, n_filters_1=32, pool_scale=2)\n\n        pool_stream = Unpooling(pool_stream, 2)\n\n        #####################\n        # Final Stage \n        #####################\n        net = tf.concat([pool_stream, res_stream], axis=-1)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n        net = ResidualUnit(net, n_filters=48, filter_size=3)\n\n        net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n        return net\n\n    else:\n        raise ValueError(""Unsupported FRRN model \'%s\'. This function only supports FRRN-A and FRRN-B"" % (preset_model)) \n'"
models/GCN.py,10,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom builders import frontend_builder\nimport os, sys\n\ndef Upsampling(inputs,scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic deconv block for GCN\n    Apply Transposed Convolution for feature map upscaling\n    """"""\n    net = slim.conv2d_transpose(inputs, n_filters, kernel_size=[3, 3], stride=[2, 2], activation_fn=None)\n    return net\n\ndef BoundaryRefinementBlock(inputs, n_filters, kernel_size=[3, 3]):\n    """"""\n    Boundary Refinement Block for GCN\n    """"""\n    net = slim.conv2d(inputs, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    net = tf.nn.relu(net)\n    net = slim.conv2d(net, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    net = tf.add(inputs, net)\n    return net\n\ndef GlobalConvBlock(inputs, n_filters=21, size=3):\n    """"""\n    Global Conv Block for GCN\n    """"""\n\n    net_1 = slim.conv2d(inputs, n_filters, [size, 1], activation_fn=None, normalizer_fn=None)\n    net_1 = slim.conv2d(net_1, n_filters, [1, size], activation_fn=None, normalizer_fn=None)\n\n    net_2 = slim.conv2d(inputs, n_filters, [1, size], activation_fn=None, normalizer_fn=None)\n    net_2 = slim.conv2d(net_2, n_filters, [size, 1], activation_fn=None, normalizer_fn=None)\n\n    net = tf.add(net_1, net_2)\n\n    return net\n\n\ndef build_gcn(inputs, num_classes, preset_model=\'GCN\', frontend=""ResNet101"", weight_decay=1e-5, is_training=True, upscaling_method=""bilinear"", pretrained_dir=""models""):\n    """"""\n    Builds the GCN model. \n\n    Arguments:\n      inputs: The input tensor\n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n\n    Returns:\n      GCN model\n    """"""\n\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n    \n\n\n    res = [end_points[\'pool5\'], end_points[\'pool4\'],\n         end_points[\'pool3\'], end_points[\'pool2\']]\n\n    down_5 = GlobalConvBlock(res[0], n_filters=21, size=3)\n    down_5 = BoundaryRefinementBlock(down_5, n_filters=21, kernel_size=[3, 3])\n    down_5 = ConvUpscaleBlock(down_5, n_filters=21, kernel_size=[3, 3], scale=2)\n\n    down_4 = GlobalConvBlock(res[1], n_filters=21, size=3)\n    down_4 = BoundaryRefinementBlock(down_4, n_filters=21, kernel_size=[3, 3])\n    down_4 = tf.add(down_4, down_5)\n    down_4 = BoundaryRefinementBlock(down_4, n_filters=21, kernel_size=[3, 3])\n    down_4 = ConvUpscaleBlock(down_4, n_filters=21, kernel_size=[3, 3], scale=2)\n\n    down_3 = GlobalConvBlock(res[2], n_filters=21, size=3)\n    down_3 = BoundaryRefinementBlock(down_3, n_filters=21, kernel_size=[3, 3])\n    down_3 = tf.add(down_3, down_4)\n    down_3 = BoundaryRefinementBlock(down_3, n_filters=21, kernel_size=[3, 3])\n    down_3 = ConvUpscaleBlock(down_3, n_filters=21, kernel_size=[3, 3], scale=2)\n\n    down_2 = GlobalConvBlock(res[3], n_filters=21, size=3)\n    down_2 = BoundaryRefinementBlock(down_2, n_filters=21, kernel_size=[3, 3])\n    down_2 = tf.add(down_2, down_3)\n    down_2 = BoundaryRefinementBlock(down_2, n_filters=21, kernel_size=[3, 3])\n    down_2 = ConvUpscaleBlock(down_2, n_filters=21, kernel_size=[3, 3], scale=2)\n\n    net = BoundaryRefinementBlock(down_2, n_filters=21, kernel_size=[3, 3])\n    net = ConvUpscaleBlock(net, n_filters=21, kernel_size=[3, 3], scale=2)\n    net = BoundaryRefinementBlock(net, n_filters=21, kernel_size=[3, 3])\n\n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net, init_fn\n\n\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)\n'"
models/ICNet.py,14,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nfrom frontends import frontend_builder\nimport os, sys\n\ndef Upsampling_by_shape(inputs, feature_map_shape):\n    return tf.image.resize_bilinear(inputs, size=feature_map_shape)\n\ndef Upsampling_by_scale(inputs, scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic conv transpose block for Encoder-Decoder upsampling\n    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = slim.conv2d_transpose(inputs, n_filters, kernel_size=[3, 3], stride=[2, 2], activation_fn=None)\n    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n    return net\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = slim.conv2d(inputs, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n    return net\n\ndef InterpBlock(net, level, feature_map_shape, pooling_type):\n    \n    # Compute the kernel and stride sizes according to how large the final feature map will be\n    # When the kernel size and strides are equal, then we can compute the final feature map size\n    # by simply dividing the current size by the kernel or stride size\n    # The final feature map sizes are 1x1, 2x2, 3x3, and 6x6. We round to the closest integer\n    kernel_size = [int(np.round(float(feature_map_shape[0]) / float(level))), int(np.round(float(feature_map_shape[1]) / float(level)))]\n    stride_size = kernel_size\n\n    net = slim.pool(net, kernel_size, stride=stride_size, pooling_type=\'MAX\')\n    net = slim.conv2d(net, 512, [1, 1], activation_fn=None)\n    net = slim.batch_norm(net, fused=True)\n    net = tf.nn.relu(net)\n    net = Upsampling_by_shape(net, feature_map_shape)\n    return net\n\ndef PyramidPoolingModule_ICNet(inputs, feature_map_shape, pooling_type):\n    """"""\n    Build the Pyramid Pooling Module.\n    """"""\n\n    interp_block1 = InterpBlock(inputs, 1, feature_map_shape, pooling_type)\n    interp_block2 = InterpBlock(inputs, 2, feature_map_shape, pooling_type)\n    interp_block3 = InterpBlock(inputs, 3, feature_map_shape, pooling_type)\n    interp_block6 = InterpBlock(inputs, 6, feature_map_shape, pooling_type)\n\n    res = tf.add([inputs, interp_block6, interp_block3, interp_block2, interp_block1])\n    return res\n\ndef CFFBlock(F1, F2, num_classes):\n    F1_big = Upsampling_by_scale(F1, scale=2)\n    F1_out = slim.conv2d(F1_big, num_classes, [1, 1], activation_fn=None)\n\n    F1_big = slim.conv2d(F1_big, 2048, [3, 3], rate=2, activation_fn=None)\n    F1_big = slim.batch_norm(F1_big, fused=True)\n\n    F2_proj = slim.conv2d(F2, 512, [1, 1], rate=1, activation_fn=None)\n    F2_proj = slim.batch_norm(F2_proj, fused=True)\n\n    F2_out = tf.add([F1_big, F2_proj])\n    F2_out = tf.nn.relu(F2_out)\n\n    return F1_out, F2_out\n\n\ndef build_icnet(inputs, label_size, num_classes, preset_model=\'ICNet\', pooling_type = ""MAX"",\n    frontend=""ResNet101"", weight_decay=1e-5, is_training=True, pretrained_dir=""models""):\n    """"""\n    Builds the ICNet model. \n\n    Arguments:\n      inputs: The input tensor\n      label_size: Size of the final label tensor. We need to know this for proper upscaling \n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n      pooling_type: Max or Average pooling\n\n    Returns:\n      ICNet model\n    """"""\n\n    inputs_4 = tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*4,  tf.shape(inputs)[2]*4])   \n    inputs_2 = tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*2,  tf.shape(inputs)[2]*2])\n    inputs_1 = inputs\n\n    if frontend == \'Res50\':\n        with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=weight_decay)):\n            logits_32, end_points_32 = resnet_v2.resnet_v2_50(inputs_4, is_training=is_training, scope=\'resnet_v2_50\')\n            logits_16, end_points_16 = resnet_v2.resnet_v2_50(inputs_2, is_training=is_training, scope=\'resnet_v2_50\')\n            logits_8, end_points_8 = resnet_v2.resnet_v2_50(inputs_1, is_training=is_training, scope=\'resnet_v2_50\')\n            resnet_scope=\'resnet_v2_50\'\n            # ICNet requires pre-trained ResNet weights\n            init_fn = slim.assign_from_checkpoint_fn(os.path.join(pretrained_dir, \'resnet_v2_50.ckpt\'), slim.get_model_variables(\'resnet_v2_50\'))\n    elif frontend == \'Res101\':\n        with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=weight_decay)):\n            logits_32, end_points_32 = resnet_v2.resnet_v2_101(inputs_4, is_training=is_training, scope=\'resnet_v2_101\')\n            logits_16, end_points_16 = resnet_v2.resnet_v2_101(inputs_2, is_training=is_training, scope=\'resnet_v2_101\')\n            logits_8, end_points_8 = resnet_v2.resnet_v2_101(inputs_1, is_training=is_training, scope=\'resnet_v2_101\')\n            resnet_scope=\'resnet_v2_101\'\n            # ICNet requires pre-trained ResNet weights\n            init_fn = slim.assign_from_checkpoint_fn(os.path.join(pretrained_dir, \'resnet_v2_101.ckpt\'), slim.get_model_variables(\'resnet_v2_101\'))\n    elif frontend == \'Res152\':\n        with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=weight_decay)):\n            logits_32, end_points_32 = resnet_v2.resnet_v2_152(inputs_4, is_training=is_training, scope=\'resnet_v2_152\')\n            logits_16, end_points_16 = resnet_v2.resnet_v2_152(inputs_2, is_training=is_training, scope=\'resnet_v2_152\')\n            logits_8, end_points_8 = resnet_v2.resnet_v2_152(inputs_1, is_training=is_training, scope=\'resnet_v2_152\')\n            resnet_scope=\'resnet_v2_152\'\n            # ICNet requires pre-trained ResNet weights\n            init_fn = slim.assign_from_checkpoint_fn(os.path.join(pretrained_dir, \'resnet_v2_152.ckpt\'), slim.get_model_variables(\'resnet_v2_152\'))\n    else:\n        raise ValueError(""Unsupported ResNet model \'%s\'. This function only supports ResNet 50, ResNet 101, and ResNet 152"" % (frontend))\n\n\n\n    feature_map_shape = [int(x / 32.0) for x in label_size]\n    block_32 = PyramidPoolingModule(end_points_32[\'pool3\'], feature_map_shape=feature_map_shape, pooling_type=pooling_type)\n\n    out_16, block_16 = CFFBlock(psp_32, end_points_16[\'pool3\'])\n    out_8, block_8 = CFFBlock(block_16, end_points_8[\'pool3\'])\n    out_4 = Upsampling_by_scale(out_8, scale=2)\n    out_4 = slim.conv2d(out_4, num_classes, [1, 1], activation_fn=None) \n\n    out_full = Upsampling_by_scale(out_4, scale=2)\n    \n    out_full = slim.conv2d(out_full, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    net = tf.concat([out_16, out_8, out_4, out_final])\n\n    return net, init_fn\n\n\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)'"
models/MobileUNet.py,8,"b'import os,time,cv2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n\t""""""\n\tBuilds the conv block for MobileNets\n\tApply successivly a 2D convolution, BatchNormalization relu\n\t""""""\n\t# Skip pointwise by setting num_outputs=Non\n\tnet = slim.conv2d(inputs, n_filters, kernel_size=[1, 1], activation_fn=None)\n\tnet = slim.batch_norm(net, fused=True)\n\tnet = tf.nn.relu(net)\n\treturn net\n\ndef DepthwiseSeparableConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n\t""""""\n\tBuilds the Depthwise Separable conv block for MobileNets\n\tApply successivly a 2D separable convolution, BatchNormalization relu, conv, BatchNormalization, relu\n\t""""""\n\t# Skip pointwise by setting num_outputs=None\n\tnet = slim.separable_convolution2d(inputs, num_outputs=None, depth_multiplier=1, kernel_size=[3, 3], activation_fn=None)\n\n\tnet = slim.batch_norm(net, fused=True)\n\tnet = tf.nn.relu(net)\n\tnet = slim.conv2d(net, n_filters, kernel_size=[1, 1], activation_fn=None)\n\tnet = slim.batch_norm(net, fused=True)\n\tnet = tf.nn.relu(net)\n\treturn net\n\ndef conv_transpose_block(inputs, n_filters, kernel_size=[3, 3]):\n\t""""""\n\tBasic conv transpose block for Encoder-Decoder upsampling\n\tApply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n\t""""""\n\tnet = slim.conv2d_transpose(inputs, n_filters, kernel_size=[3, 3], stride=[2, 2], activation_fn=None)\n\tnet = tf.nn.relu(slim.batch_norm(net))\n\treturn net\n\ndef build_mobile_unet(inputs, preset_model, num_classes):\n\n\thas_skip = False\n\tif preset_model == ""MobileUNet"":\n\t\thas_skip = False\n\telif preset_model == ""MobileUNet-Skip"":\n\t\thas_skip = True\n\telse:\n\t\traise ValueError(""Unsupported MobileUNet model \'%s\'. This function only supports MobileUNet and MobileUNet-Skip"" % (preset_model))\n\n    #####################\n\t# Downsampling path #\n\t#####################\n\tnet = ConvBlock(inputs, 64)\n\tnet = DepthwiseSeparableConvBlock(net, 64)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_1 = net\n\n\tnet = DepthwiseSeparableConvBlock(net, 128)\n\tnet = DepthwiseSeparableConvBlock(net, 128)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_2 = net\n\n\tnet = DepthwiseSeparableConvBlock(net, 256)\n\tnet = DepthwiseSeparableConvBlock(net, 256)\n\tnet = DepthwiseSeparableConvBlock(net, 256)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_3 = net\n\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\tskip_4 = net\n\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = slim.pool(net, [2, 2], stride=[2, 2], pooling_type=\'MAX\')\n\n\n\t#####################\n\t# Upsampling path #\n\t#####################\n\tnet = conv_transpose_block(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_4)\n\n\tnet = conv_transpose_block(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 512)\n\tnet = DepthwiseSeparableConvBlock(net, 256)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_3)\n\n\tnet = conv_transpose_block(net, 256)\n\tnet = DepthwiseSeparableConvBlock(net, 256)\n\tnet = DepthwiseSeparableConvBlock(net, 256)\n\tnet = DepthwiseSeparableConvBlock(net, 128)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_2)\n\n\tnet = conv_transpose_block(net, 128)\n\tnet = DepthwiseSeparableConvBlock(net, 128)\n\tnet = DepthwiseSeparableConvBlock(net, 64)\n\tif has_skip:\n\t\tnet = tf.add(net, skip_1)\n\n\tnet = conv_transpose_block(net, 64)\n\tnet = DepthwiseSeparableConvBlock(net, 64)\n\tnet = DepthwiseSeparableConvBlock(net, 64)\n\n\t#####################\n\t#      Softmax      #\n\t#####################\n\tnet = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\treturn net'"
models/PSPNet.py,9,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nfrom builders import frontend_builder\nimport os, sys\n\ndef Upsampling(inputs,feature_map_shape):\n    return tf.image.resize_bilinear(inputs, size=feature_map_shape)\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic conv transpose block for Encoder-Decoder upsampling\n    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d_transpose(net, n_filters, kernel_size=[3, 3], stride=[scale, scale], activation_fn=None)\n    return net\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    return net\n\ndef InterpBlock(net, level, feature_map_shape, pooling_type):\n    \n    # Compute the kernel and stride sizes according to how large the final feature map will be\n    # When the kernel size and strides are equal, then we can compute the final feature map size\n    # by simply dividing the current size by the kernel or stride size\n    # The final feature map sizes are 1x1, 2x2, 3x3, and 6x6. We round to the closest integer\n    kernel_size = [int(np.round(float(feature_map_shape[0]) / float(level))), int(np.round(float(feature_map_shape[1]) / float(level)))]\n    stride_size = kernel_size\n\n    net = slim.pool(net, kernel_size, stride=stride_size, pooling_type=\'MAX\')\n    net = slim.conv2d(net, 512, [1, 1], activation_fn=None)\n    net = slim.batch_norm(net, fused=True)\n    net = tf.nn.relu(net)\n    net = Upsampling(net, feature_map_shape)\n    return net\n\ndef PyramidPoolingModule(inputs, feature_map_shape, pooling_type):\n    """"""\n    Build the Pyramid Pooling Module.\n    """"""\n\n    interp_block1 = InterpBlock(inputs, 1, feature_map_shape, pooling_type)\n    interp_block2 = InterpBlock(inputs, 2, feature_map_shape, pooling_type)\n    interp_block3 = InterpBlock(inputs, 3, feature_map_shape, pooling_type)\n    interp_block6 = InterpBlock(inputs, 6, feature_map_shape, pooling_type)\n\n    res = tf.concat([inputs, interp_block6, interp_block3, interp_block2, interp_block1], axis=-1)\n    return res\n\n\n\ndef build_pspnet(inputs, label_size, num_classes, preset_model=\'PSPNet\', frontend=""ResNet101"", pooling_type = ""MAX"",\n    weight_decay=1e-5, upscaling_method=""conv"", is_training=True, pretrained_dir=""models""):\n    """"""\n    Builds the PSPNet model. \n\n    Arguments:\n      inputs: The input tensor\n      label_size: Size of the final label tensor. We need to know this for proper upscaling \n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n      pooling_type: Max or Average pooling\n\n    Returns:\n      PSPNet model\n    """"""\n\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n    feature_map_shape = [int(x / 8.0) for x in label_size]\n    print(feature_map_shape)\n    psp = PyramidPoolingModule(end_points[\'pool3\'], feature_map_shape=feature_map_shape, pooling_type=pooling_type)\n\n    net = slim.conv2d(psp, 512, [3, 3], activation_fn=None)\n    net = slim.batch_norm(net, fused=True)\n    net = tf.nn.relu(net)\n\n    if upscaling_method.lower() == ""conv"":\n        net = ConvUpscaleBlock(net, 256, kernel_size=[3, 3], scale=2)\n        net = ConvBlock(net, 256)\n        net = ConvUpscaleBlock(net, 128, kernel_size=[3, 3], scale=2)\n        net = ConvBlock(net, 128)\n        net = ConvUpscaleBlock(net, 64, kernel_size=[3, 3], scale=2)\n        net = ConvBlock(net, 64)\n    elif upscaling_method.lower() == ""bilinear"":\n        net = Upsampling(net, label_size)\n    \n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net, init_fn\n\n\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)'"
models/RefineNet.py,13,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom builders import frontend_builder\nimport os, sys\n\ndef Upsampling(inputs,scale):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*scale,  tf.shape(inputs)[2]*scale])\n\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n    """"""\n    Basic conv block for Encoder-Decoder\n    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d(net, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n    return net\n\ndef ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n    """"""\n    Basic conv transpose block for Encoder-Decoder upsampling\n    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n    """"""\n    net = tf.nn.relu(slim.batch_norm(inputs, fused=True))\n    net = slim.conv2d_transpose(net, n_filters, kernel_size=[3, 3], stride=[scale, scale], activation_fn=None)\n    return net\n\n\ndef ResidualConvUnit(inputs,n_filters=256,kernel_size=3):\n    """"""\n    A local residual unit designed to fine-tune the pretrained ResNet weights\n\n    Arguments:\n      inputs: The input tensor\n      n_filters: Number of output feature maps for each conv\n      kernel_size: Size of convolution kernel\n\n    Returns:\n      Output of local residual block\n    """"""\n    net=tf.nn.relu(inputs)\n    net=slim.conv2d(net, n_filters, kernel_size, activation_fn=None)\n    net=tf.nn.relu(net)\n    net=slim.conv2d(net,n_filters,kernel_size, activation_fn=None)\n    net=tf.add(net,inputs)\n    return net\n\ndef ChainedResidualPooling(inputs,n_filters=256):\n    """"""\n    Chained residual pooling aims to capture background \n    context from a large image region. This component is \n    built as a chain of 2 pooling blocks, each consisting \n    of one max-pooling layer and one convolution layer. One pooling\n    block takes the output of the previous pooling block as\n    input. The output feature maps of all pooling blocks are \n    fused together with the input feature map through summation \n    of residual connections.\n\n    Arguments:\n      inputs: The input tensor\n      n_filters: Number of output feature maps for each conv\n\n    Returns:\n      Double-pooled feature maps\n    """"""\n\n    net_relu=tf.nn.relu(inputs)\n    net=slim.max_pool2d(net_relu, [5, 5],stride=1,padding=\'SAME\')\n    net=slim.conv2d(net,n_filters,3, activation_fn=None)\n    net_sum_1=tf.add(net,net_relu)\n\n    net = slim.max_pool2d(net, [5, 5], stride=1, padding=\'SAME\')\n    net = slim.conv2d(net, n_filters, 3, activation_fn=None)\n    net_sum_2=tf.add(net,net_sum_1)\n\n    return net_sum_2\n\n\ndef MultiResolutionFusion(high_inputs=None,low_inputs=None,n_filters=256):\n    """"""\n    Fuse together all path inputs. This block first applies convolutions\n    for input adaptation, which generate feature maps of the same feature dimension \n    (the smallest one among the inputs), and then up-samples all (smaller) feature maps to\n    the largest resolution of the inputs. Finally, all features maps are fused by summation.\n\n    Arguments:\n      high_inputs: The input tensors that have the higher resolution\n      low_inputs: The input tensors that have the lower resolution\n      n_filters: Number of output feature maps for each conv\n\n    Returns:\n      Fused feature maps at higher resolution\n    \n    """"""\n\n    if high_inputs is None: # RefineNet block 4\n\n        fuse = slim.conv2d(low_inputs, n_filters, 3, activation_fn=None)\n\n        return fuse\n\n    else:\n\n        conv_low = slim.conv2d(low_inputs, n_filters, 3, activation_fn=None)\n        conv_high = slim.conv2d(high_inputs, n_filters, 3, activation_fn=None)\n\n        conv_low_up = Upsampling(conv_low,2)\n\n        return tf.add(conv_low_up, conv_high)\n\n\ndef RefineBlock(high_inputs=None,low_inputs=None):\n    """"""\n    A RefineNet Block which combines together the ResidualConvUnits,\n    fuses the feature maps using MultiResolutionFusion, and then gets\n    large-scale context with the ResidualConvUnit.\n\n    Arguments:\n      high_inputs: The input tensors that have the higher resolution\n      low_inputs: The input tensors that have the lower resolution\n\n    Returns:\n      RefineNet block for a single path i.e one resolution\n    \n    """"""\n\n    if low_inputs is None: # block 4\n        rcu_new_low= ResidualConvUnit(high_inputs, n_filters=512)\n        rcu_new_low = ResidualConvUnit(rcu_new_low, n_filters=512)\n\n        fuse = MultiResolutionFusion(high_inputs=None, low_inputs=rcu_new_low, n_filters=512)\n        fuse_pooling = ChainedResidualPooling(fuse, n_filters=512)\n        output = ResidualConvUnit(fuse_pooling, n_filters=512)\n        return output\n    else:\n        rcu_high= ResidualConvUnit(high_inputs, n_filters=256)\n        rcu_high = ResidualConvUnit(rcu_high, n_filters=256)\n\n        fuse = MultiResolutionFusion(rcu_high, low_inputs,n_filters=256)\n        fuse_pooling = ChainedResidualPooling(fuse, n_filters=256)\n        output = ResidualConvUnit(fuse_pooling, n_filters=256)\n        return output\n\n\n\ndef build_refinenet(inputs, num_classes, preset_model=\'RefineNet\', frontend=""ResNet101"", weight_decay=1e-5, upscaling_method=""bilinear"", pretrained_dir=""models"", is_training=True):\n    """"""\n    Builds the RefineNet model. \n\n    Arguments:\n      inputs: The input tensor\n      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n      num_classes: Number of classes\n\n    Returns:\n      RefineNet model\n    """"""\n\n    logits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, pretrained_dir=pretrained_dir, is_training=is_training)\n\n    \n\n\n    high = [end_points[\'pool5\'], end_points[\'pool4\'],\n         end_points[\'pool3\'], end_points[\'pool2\']]\n\n    low = [None, None, None, None]\n\n    # Get the feature maps to the proper size with bottleneck\n    high[0]=slim.conv2d(high[0], 512, 1)\n    high[1]=slim.conv2d(high[1], 256, 1)\n    high[2]=slim.conv2d(high[2], 256, 1)\n    high[3]=slim.conv2d(high[3], 256, 1)\n\n    # RefineNet\n    low[0]=RefineBlock(high_inputs=high[0],low_inputs=None) # Only input ResNet 1/32\n    low[1]=RefineBlock(high[1],low[0]) # High input = ResNet 1/16, Low input = Previous 1/16\n    low[2]=RefineBlock(high[2],low[1]) # High input = ResNet 1/8, Low input = Previous 1/8\n    low[3]=RefineBlock(high[3],low[2]) # High input = ResNet 1/4, Low input = Previous 1/4\n\n    # g[3]=Upsampling(g[3],scale=4)\n\n    net = low[3]\n\n    net = ResidualConvUnit(net)\n    net = ResidualConvUnit(net)\n\n    if upscaling_method.lower() == ""conv"":\n        net = ConvUpscaleBlock(net, 128, kernel_size=[3, 3], scale=2)\n        net = ConvBlock(net, 128)\n        net = ConvUpscaleBlock(net, 64, kernel_size=[3, 3], scale=2)\n        net = ConvBlock(net, 64)\n    elif upscaling_method.lower() == ""bilinear"":\n        net = Upsampling(net, scale=4)\n\n    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope=\'logits\')\n\n    return net, init_fn\n\n\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)\n'"
models/__init__.py,0,b''
models/custom_model.py,3,"b'from __future__ import division\nimport os,time,cv2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nfrom builders import frontend_builder\n\ndef conv_block(inputs, n_filters, filter_size=[3, 3], dropout_p=0.0):\n\t""""""\n\tBasic conv block for Encoder-Decoder\n\tApply successivly Convolution, BatchNormalization, ReLU nonlinearity\n\tDropout (if dropout_p > 0) on the inputs\n\t""""""\n\tconv = slim.conv2d(inputs, n_filters, filter_size, activation_fn=None, normalizer_fn=None)\n\tout = tf.nn.relu(slim.batch_norm(conv, fused=True))\n\tif dropout_p != 0.0:\n\t  out = slim.dropout(out, keep_prob=(1.0-dropout_p))\n\treturn out\n\ndef conv_transpose_block(inputs, n_filters, strides=2, filter_size=[3, 3], dropout_p=0.0):\n\t""""""\n\tBasic conv transpose block for Encoder-Decoder upsampling\n\tApply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n\tDropout (if dropout_p > 0) on the inputs\n\t""""""\n\tconv = slim.conv2d_transpose(inputs, n_filters, kernel_size=[3, 3], stride=[strides, strides])\n\tout = tf.nn.relu(slim.batch_norm(conv, fused=True))\n\tif dropout_p != 0.0:\n\t  out = slim.dropout(out, keep_prob=(1.0-dropout_p))\n\treturn out\n\ndef build_custom(inputs, num_classes, frontend=""ResNet101"", weight_decay=1e-5, is_training=True, pretrained_dir=""models""):\n\t\n\n\tlogits, end_points, frontend_scope, init_fn  = frontend_builder.build_frontend(inputs, frontend, is_training=is_training)\n\n\tup_1 = conv_transpose_block(end_points[""pool2""], strides=4, n_filters=64)\n\tup_2 = conv_transpose_block(end_points[""pool3""], strides=8, n_filters=64)\n\tup_3 = conv_transpose_block(end_points[""pool4""], strides=16, n_filters=64)\n\tup_4 = conv_transpose_block(end_points[""pool5""], strides=32, n_filters=64)\n\n\tfeatures = tf.concat([up_1, up_2, up_3, up_4], axis=-1)\n\n\tfeatures = conv_block(inputs=features, n_filters=256, filter_size=[1, 1])\n\n\tfeatures = conv_block(inputs=features, n_filters=64, filter_size=[3, 3])\n\tfeatures = conv_block(inputs=features, n_filters=64, filter_size=[3, 3])\n\tfeatures = conv_block(inputs=features, n_filters=64, filter_size=[3, 3])\n\n\n\tnet = slim.conv2d(features, num_classes, [1, 1], scope=\'logits\')\n\treturn net'"
utils/__init__.py,0,b''
utils/get_pretrained_checkpoints.py,0,"b'import subprocess\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model\', type=str, default=""ALL"", help=\'Which model weights to download\')\nargs = parser.parse_args()\n\n\nif args.model == ""ResNet50"" or args.model == ""ALL"":\n\tsubprocess.check_output([\'wget\',\'http://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz\', ""-P"", ""models""])\n\ttry:\n\t\tsubprocess.check_output([\'tar\', \'-xvf\', \'models/resnet_v2_50_2017_04_14.tar.gz\', ""-C"", ""models""])\n\t\tsubprocess.check_output([\'rm\', \'models/resnet_v2_50_2017_04_14.tar.gz\'])\n\texcept Exception as e:\n\t\tprint(e)\n\t\tpass\n\nif args.model == ""ResNet101"" or args.model == ""ALL"":\n\tsubprocess.check_output([\'wget\',\'http://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz\', ""-P"", ""models""])\n\ttry:\n\t\tsubprocess.check_output([\'tar\', \'-xvf\', \'models/resnet_v2_101_2017_04_14.tar.gz\', ""-C"", ""models""])\n\t\tsubprocess.check_output([\'rm\', \'models/resnet_v2_101_2017_04_14.tar.gz\'])\n\texcept Exception as e:\n\t\tprint(e)\n\t\tpass\n\nif args.model == ""ResNet152"" or args.model == ""ALL"":\n\tsubprocess.check_output([\'wget\',\'http://download.tensorflow.org/models/resnet_v2_152_2017_04_14.tar.gz\', ""-P"", ""models""])\n\ttry:\n\t\tsubprocess.check_output([\'tar\', \'-xvf\', \'models/resnet_v2_152_2017_04_14.tar.gz\', ""-C"", ""models""])\n\t\tsubprocess.check_output([\'rm\', \'models/resnet_v2_152_2017_04_14.tar.gz\'])\n\texcept Exception as e:\n\t\tprint(e)\n\t\tpass\n\nif args.model == ""MobileNetV2"" or args.model == ""ALL"":\n\tsubprocess.check_output([\'wget\',\'https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz\', ""-P"", ""models""])\n\ttry:\n\t\tsubprocess.check_output([\'tar\', \'-xvf\', \'models/mobilenet_v2_1.4_224.tgz\', ""-C"", ""models""])\n\t\tsubprocess.check_output([\'rm\', \'models/mobilenet_v2_1.4_224.tgz\'])\n\texcept Exception as e:\n\t\tprint(e)\n\t\tpass\n\nif args.model == ""InceptionV4"" or args.model == ""ALL"":\n\tsubprocess.check_output(\n\t\t[\'wget\', \'http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz\', ""-P"", ""models""])\n\ttry:\n\t\tsubprocess.check_output([\'tar\', \'-xvf\', \'models/inception_v4_2016_09_09.tar.gz\', ""-C"", ""models""])\n\t\tsubprocess.check_output([\'rm\', \'models/inception_v4_2016_09_09.tar.gz\'])\n\texcept Exception as e:\n\t\tprint(e)\n\t\tpass\n'"
utils/helpers.py,0,"b'import cv2\nimport numpy as np\nimport itertools\nimport operator\nimport os, csv\nimport tensorflow as tf\n\nimport time, datetime\n\ndef get_label_info(csv_path):\n    """"""\n    Retrieve the class names and label values for the selected dataset.\n    Must be in CSV format!\n\n    # Arguments\n        csv_path: The file path of the class dictionairy\n        \n    # Returns\n        Two lists: one for the class names and the other for the label values\n    """"""\n    filename, file_extension = os.path.splitext(csv_path)\n    if not file_extension == "".csv"":\n        return ValueError(""File is not a CSV!"")\n\n    class_names = []\n    label_values = []\n    with open(csv_path, \'r\') as csvfile:\n        file_reader = csv.reader(csvfile, delimiter=\',\')\n        header = next(file_reader)\n        for row in file_reader:\n            class_names.append(row[0])\n            label_values.append([int(row[1]), int(row[2]), int(row[3])])\n        # print(class_dict)\n    return class_names, label_values\n\n\ndef one_hot_it(label, label_values):\n    """"""\n    Convert a segmentation image label array to one-hot format\n    by replacing each pixel value with a vector of length num_classes\n\n    # Arguments\n        label: The 2D array segmentation image label\n        label_values\n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of num_classes\n    """"""\n    # st = time.time()\n    # w = label.shape[0]\n    # h = label.shape[1]\n    # num_classes = len(class_dict)\n    # x = np.zeros([w,h,num_classes])\n    # unique_labels = sortedlist((class_dict.values()))\n    # for i in range(0, w):\n    #     for j in range(0, h):\n    #         index = unique_labels.index(list(label[i][j][:]))\n    #         x[i,j,index]=1\n    # print(""Time 1 = "", time.time() - st)\n\n    # st = time.time()\n    # https://stackoverflow.com/questions/46903885/map-rgb-semantic-maps-to-one-hot-encodings-and-vice-versa-in-tensorflow\n    # https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n    semantic_map = []\n    for colour in label_values:\n        # colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis = -1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1)\n    # print(""Time 2 = "", time.time() - st)\n\n    return semantic_map\n    \ndef reverse_one_hot(image):\n    """"""\n    Transform a 2D array in one-hot format (depth is num_classes),\n    to a 2D array with only 1 channel, where each pixel value is\n    the classified class key.\n\n    # Arguments\n        image: The one-hot format image \n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of 1, where each pixel value is the classified \n        class key.\n    """"""\n    # w = image.shape[0]\n    # h = image.shape[1]\n    # x = np.zeros([w,h,1])\n\n    # for i in range(0, w):\n    #     for j in range(0, h):\n    #         index, value = max(enumerate(image[i, j, :]), key=operator.itemgetter(1))\n    #         x[i, j] = index\n\n    x = np.argmax(image, axis = -1)\n    return x\n\n\ndef colour_code_segmentation(image, label_values):\n    """"""\n    Given a 1-channel array of class keys, colour code the segmentation results.\n\n    # Arguments\n        image: single channel array where each value represents the class key.\n        label_values\n        \n    # Returns\n        Colour coded image for segmentation visualization\n    """"""\n\n    # w = image.shape[0]\n    # h = image.shape[1]\n    # x = np.zeros([w,h,3])\n    # colour_codes = label_values\n    # for i in range(0, w):\n    #     for j in range(0, h):\n    #         x[i, j, :] = colour_codes[int(image[i, j])]\n    \n    colour_codes = np.array(label_values)\n    x = colour_codes[image.astype(int)]\n\n    return x\n\n# class_dict = get_class_dict(""CamVid/class_dict.csv"")\n# gt = cv2.imread(""CamVid/test_labels/0001TP_007170_L.png"",-1)\n# gt = reverse_one_hot(one_hot_it(gt, class_dict))\n# gt = colour_code_segmentation(gt, class_dict)\n\n# file_name = ""gt_test.png""\n# cv2.imwrite(file_name,np.uint8(gt))'"
utils/utils.py,26,"b'from __future__ import print_function, division\nimport os,time,cv2, sys, math\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport time, datetime\nimport os, random\nfrom scipy.misc import imread\nimport ast\nfrom sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n\nfrom utils import helpers\n\ndef prepare_data(dataset_dir):\n    train_input_names=[]\n    train_output_names=[]\n    val_input_names=[]\n    val_output_names=[]\n    test_input_names=[]\n    test_output_names=[]\n    for file in os.listdir(dataset_dir + ""/train""):\n        cwd = os.getcwd()\n        train_input_names.append(cwd + ""/"" + dataset_dir + ""/train/"" + file)\n    for file in os.listdir(dataset_dir + ""/train_labels""):\n        cwd = os.getcwd()\n        train_output_names.append(cwd + ""/"" + dataset_dir + ""/train_labels/"" + file)\n    for file in os.listdir(dataset_dir + ""/val""):\n        cwd = os.getcwd()\n        val_input_names.append(cwd + ""/"" + dataset_dir + ""/val/"" + file)\n    for file in os.listdir(dataset_dir + ""/val_labels""):\n        cwd = os.getcwd()\n        val_output_names.append(cwd + ""/"" + dataset_dir + ""/val_labels/"" + file)\n    for file in os.listdir(dataset_dir + ""/test""):\n        cwd = os.getcwd()\n        test_input_names.append(cwd + ""/"" + dataset_dir + ""/test/"" + file)\n    for file in os.listdir(dataset_dir + ""/test_labels""):\n        cwd = os.getcwd()\n        test_output_names.append(cwd + ""/"" + dataset_dir + ""/test_labels/"" + file)\n    train_input_names.sort(),train_output_names.sort(), val_input_names.sort(), val_output_names.sort(), test_input_names.sort(), test_output_names.sort()\n    return train_input_names,train_output_names, val_input_names, val_output_names, test_input_names, test_output_names\n\ndef load_image(path):\n    image = cv2.cvtColor(cv2.imread(path,-1), cv2.COLOR_BGR2RGB)\n    return image\n\n# Takes an absolute file path and returns the name of the file without th extension\ndef filepath_to_name(full_name):\n    file_name = os.path.basename(full_name)\n    file_name = os.path.splitext(file_name)[0]\n    return file_name\n\n# Print with time. To console or file\ndef LOG(X, f=None):\n    time_stamp = datetime.datetime.now().strftime(""[%Y-%m-%d %H:%M:%S]"")\n    if not f:\n        print(time_stamp + "" "" + X)\n    else:\n        f.write(time_stamp + "" "" + X)\n\n\n# Count total number of parameters in the model\ndef count_params():\n    total_parameters = 0\n    for variable in tf.trainable_variables():\n        shape = variable.get_shape()\n        variable_parameters = 1\n        for dim in shape:\n            variable_parameters *= dim.value\n        total_parameters += variable_parameters\n    print(""This model has %d trainable parameters""% (total_parameters))\n\n# Subtracts the mean images from ImageNet\ndef mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n    inputs=tf.to_float(inputs)\n    num_channels = inputs.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)\n\ndef _lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\ndef _flatten_probas(probas, labels, ignore=None, order=\'BHWC\'):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    if order == \'BCHW\':\n        probas = tf.transpose(probas, (0, 2, 3, 1), name=""BCHW_to_BHWC"")\n        order = \'BHWC\'\n    if order != \'BHWC\':\n        raise NotImplementedError(\'Order {} unknown\'.format(order))\n    C = probas.shape[3]\n    probas = tf.reshape(probas, (-1, C))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return probas, labels\n    valid = tf.not_equal(labels, ignore)\n    vprobas = tf.boolean_mask(probas, valid, name=\'valid_probas\')\n    vlabels = tf.boolean_mask(labels, valid, name=\'valid_labels\')\n    return vprobas, vlabels\n\ndef _lovasz_softmax_flat(probas, labels, only_present=True):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    """"""\n    C = probas.shape[1]\n    losses = []\n    present = []\n    for c in range(C):\n        fg = tf.cast(tf.equal(labels, c), probas.dtype) # foreground for class c\n        if only_present:\n            present.append(tf.reduce_sum(fg) > 0)\n        errors = tf.abs(fg - probas[:, c])\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=""descending_sort_{}"".format(c))\n        fg_sorted = tf.gather(fg, perm)\n        grad = _lovasz_grad(fg_sorted)\n        losses.append(\n            tf.tensordot(errors_sorted, tf.stop_gradient(grad), 1, name=""loss_class_{}"".format(c))\n                      )\n    losses_tensor = tf.stack(losses)\n    if only_present:\n        present = tf.stack(present)\n        losses_tensor = tf.boolean_mask(losses_tensor, present)\n    return losses_tensor\n\ndef lovasz_softmax(probas, labels, only_present=True, per_image=False, ignore=None, order=\'BHWC\'):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, H, W, C] or [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n      order: use BHWC or BCHW\n    """"""\n    probas = tf.nn.softmax(probas, 3)\n    labels = helpers.reverse_one_hot(labels)\n\n    if per_image:\n        def treat_image(prob, lab):\n            prob, lab = tf.expand_dims(prob, 0), tf.expand_dims(lab, 0)\n            prob, lab = _flatten_probas(prob, lab, ignore, order)\n            return _lovasz_softmax_flat(prob, lab, only_present=only_present)\n        losses = tf.map_fn(treat_image, (probas, labels), dtype=tf.float32)\n    else:\n        losses = _lovasz_softmax_flat(*_flatten_probas(probas, labels, ignore, order), only_present=only_present)\n    return losses\n\n\n# Randomly crop the image to a specific size. For data augmentation\ndef random_crop(image, label, crop_height, crop_width):\n    if (image.shape[0] != label.shape[0]) or (image.shape[1] != label.shape[1]):\n        raise Exception(\'Image and label must have the same dimensions!\')\n        \n    if (crop_width <= image.shape[1]) and (crop_height <= image.shape[0]):\n        x = random.randint(0, image.shape[1]-crop_width)\n        y = random.randint(0, image.shape[0]-crop_height)\n        \n        if len(label.shape) == 3:\n            return image[y:y+crop_height, x:x+crop_width, :], label[y:y+crop_height, x:x+crop_width, :]\n        else:\n            return image[y:y+crop_height, x:x+crop_width, :], label[y:y+crop_height, x:x+crop_width]\n    else:\n        raise Exception(\'Crop shape (%d, %d) exceeds image dimensions (%d, %d)!\' % (crop_height, crop_width, image.shape[0], image.shape[1]))\n\n# Compute the average segmentation accuracy across all classes\ndef compute_global_accuracy(pred, label):\n    total = len(label)\n    count = 0.0\n    for i in range(total):\n        if pred[i] == label[i]:\n            count = count + 1.0\n    return float(count) / float(total)\n\n# Compute the class-specific segmentation accuracy\ndef compute_class_accuracies(pred, label, num_classes):\n    total = []\n    for val in range(num_classes):\n        total.append((label == val).sum())\n\n    count = [0.0] * num_classes\n    for i in range(len(label)):\n        if pred[i] == label[i]:\n            count[int(pred[i])] = count[int(pred[i])] + 1.0\n\n    # If there are no pixels from a certain class in the GT, \n    # it returns NAN because of divide by zero\n    # Replace the nans with a 1.0.\n    accuracies = []\n    for i in range(len(total)):\n        if total[i] == 0:\n            accuracies.append(1.0)\n        else:\n            accuracies.append(count[i] / total[i])\n\n    return accuracies\n\n\ndef compute_mean_iou(pred, label):\n\n    unique_labels = np.unique(label)\n    num_unique_labels = len(unique_labels);\n\n    I = np.zeros(num_unique_labels)\n    U = np.zeros(num_unique_labels)\n\n    for index, val in enumerate(unique_labels):\n        pred_i = pred == val\n        label_i = label == val\n\n        I[index] = float(np.sum(np.logical_and(label_i, pred_i)))\n        U[index] = float(np.sum(np.logical_or(label_i, pred_i)))\n\n\n    mean_iou = np.mean(I / U)\n    return mean_iou\n\n\ndef evaluate_segmentation(pred, label, num_classes, score_averaging=""weighted""):\n    flat_pred = pred.flatten()\n    flat_label = label.flatten()\n\n    global_accuracy = compute_global_accuracy(flat_pred, flat_label)\n    class_accuracies = compute_class_accuracies(flat_pred, flat_label, num_classes)\n\n    prec = precision_score(flat_pred, flat_label, average=score_averaging)\n    rec = recall_score(flat_pred, flat_label, average=score_averaging)\n    f1 = f1_score(flat_pred, flat_label, average=score_averaging)\n\n    iou = compute_mean_iou(flat_pred, flat_label)\n\n    return global_accuracy, class_accuracies, prec, rec, f1, iou\n\n    \ndef compute_class_weights(labels_dir, label_values):\n    \'\'\'\n    Arguments:\n        labels_dir(list): Directory where the image segmentation labels are\n        num_classes(int): the number of classes of pixels in all images\n\n    Returns:\n        class_weights(list): a list of class weights where each index represents each class label and the element is the class weight for that label.\n\n    \'\'\'\n    image_files = [os.path.join(labels_dir, file) for file in os.listdir(labels_dir) if file.endswith(\'.png\')]\n\n    num_classes = len(label_values)\n\n    class_pixels = np.zeros(num_classes) \n\n    total_pixels = 0.0\n\n    for n in range(len(image_files)):\n        image = imread(image_files[n])\n\n        for index, colour in enumerate(label_values):\n            class_map = np.all(np.equal(image, colour), axis = -1)\n            class_map = class_map.astype(np.float32)\n            class_pixels[index] += np.sum(class_map)\n\n            \n        print(""\\rProcessing image: "" + str(n) + "" / "" + str(len(image_files)), end="""")\n        sys.stdout.flush()\n\n    total_pixels = float(np.sum(class_pixels))\n    index_to_delete = np.argwhere(class_pixels==0.0)\n    class_pixels = np.delete(class_pixels, index_to_delete)\n\n    class_weights = total_pixels / class_pixels\n    class_weights = class_weights / np.sum(class_weights)\n\n    return class_weights\n\n# Compute the memory usage, for debugging\ndef memory():\n    import os\n    import psutil\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memoryUse = py.memory_info()[0]/2.**30  # Memory use in GB\n    print(\'Memory usage in GBs:\', memoryUse)\n\n'"
