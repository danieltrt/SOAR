file_path,api_count,code
fw/custom_GRU.py,40,"b'import tensorflow as tf\nimport numpy as np\n\nimport collections\nimport math\n\nfrom tensorflow.python.framework import (\n    ops,\n    tensor_shape,\n)\n\nfrom tensorflow.python.ops import (\n    array_ops,\n    clip_ops,\n    embedding_ops,\n    init_ops,\n    math_ops,\n    nn_ops,\n    partitioned_variables,\n    variable_scope as vs,\n)\n\nfrom tensorflow.python.ops.math_ops import (\n    sigmoid,\n    tanh,\n)\n\nfrom tensorflow.python.platform import (\n    tf_logging as logging,\n)\n\nfrom tensorflow.python.util import (\n    nest,\n)\n\n# LN funcition\ndef ln(inputs, epsilon=1e-5, scope=None):\n\n    """""" Computer LN given an input tensor. We get in an input of shape\n    [N X D] and with LN we compute the mean and var for each individual\n    training point across all it\'s hidden dimensions rather than across\n    the training batch as we do in BN. This gives us a mean and var of shape\n    [N X 1].\n    """"""\n    mean, var = tf.nn.moments(inputs, [1], keep_dims=True)\n    with tf.variable_scope(scope + \'LN\'):\n        scale = tf.get_variable(\'alpha\',\n                                shape=[inputs.get_shape()[1]],\n                                initializer=tf.constant_initializer(1))\n        shift = tf.get_variable(\'beta\',\n                                shape=[inputs.get_shape()[1]],\n                                initializer=tf.constant_initializer(0))\n    LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift\n\n    return LN\n\n# Modified from:\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\nclass GRUCell(tf.nn.rnn_cell.RNNCell):\n    """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""""""\n\n    def __init__(self, num_units, input_size=None, activation=tanh):\n        if input_size is not None:\n          logging.warn(""%s: The input_size parameter is deprecated."", self)\n        self._num_units = num_units\n        self._activation = activation\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Gated recurrent unit (GRU) with nunits cells.""""""\n        with vs.variable_scope(scope or type(self).__name__):  # ""GRUCell""\n          with vs.variable_scope(""Gates""):  # Reset gate and update gate.\n            # We start with bias of 1.0 to not reset and not update.\n            r, u = array_ops.split(1, 2, tf.nn.rnn_cell._linear([inputs, state],\n                           2 * self._num_units, True, 1.0))\n\n            # Apply Layer Normalization to the two gates\n            r = ln(r, scope = \'r/\')\n            u = ln(r, scope = \'u/\')\n\n            r, u = sigmoid(r), sigmoid(u)\n          with vs.variable_scope(""Candidate""):\n            c = self._activation(tf.nn.rnn_cell._linear([inputs, r * state],\n                                         self._num_units, True))\n          new_h = u * state + (1 - u) * c\n        return new_h, new_h\n\n\nclass gru_model(object):\n\n    def __init__(self, FLAGS):\n\n        self.X = tf.placeholder(tf.float32,\n            shape=[None, FLAGS.input_dim, FLAGS.num_classes], name=\'inputs_X\')\n        self.y = tf.placeholder(tf.float32,\n            shape=[None, FLAGS.num_classes], name=\'targets_y\')\n        self.l = tf.placeholder(tf.float32, [], # need [] for tf.scalar_mul\n            name=""learning_rate"")\n        self.e = tf.placeholder(tf.float32, [],\n            name=""decay_rate"")\n\n        with tf.variable_scope(""GRU""):\n\n            # input weights (proper initialization)\n            self.W_x = tf.Variable(tf.random_uniform(\n                [FLAGS.num_classes, FLAGS.num_hidden_units],\n                -np.sqrt(2.0/FLAGS.num_classes),\n                np.sqrt(2.0/FLAGS.num_classes)),\n                dtype=tf.float32)\n            self.b_x = tf.Variable(tf.zeros(\n                [FLAGS.num_hidden_units]),\n                dtype=tf.float32)\n\n            # hidden weights (See Hinton\'s video @ 21:20)\n            self.W_h = tf.Variable(\n                initial_value=0.05 * np.identity(FLAGS.num_hidden_units),\n                dtype=tf.float32)\n\n            # softmax weights (proper initialization)\n            self.W_softmax = tf.Variable(tf.random_uniform(\n                [FLAGS.num_hidden_units, FLAGS.num_classes],\n                -np.sqrt(2.0 / FLAGS.num_hidden_units),\n                np.sqrt(2.0 / FLAGS.num_hidden_units)),\n                dtype=tf.float32)\n            self.b_softmax = tf.Variable(tf.zeros(\n                [FLAGS.num_classes]),\n                dtype=tf.float32)\n\n        self.h = tf.zeros(\n            [FLAGS.batch_size, FLAGS.num_hidden_units],\n            dtype=tf.float32)\n\n        # GRU\n        self.gru = GRUCell(FLAGS.num_hidden_units)\n        with tf.variable_scope(""gru_step"") as scope:\n            for t in range(0, FLAGS.input_dim):\n                if t > 0:\n                    scope.reuse_variables()\n                self.outputs, self.h = self.gru(self.X[:, t, :], self.h)\n\n        # All inputs processed! Time for softmax\n        self.logits = tf.matmul(self.h, self.W_softmax) + self.b_softmax\n\n        # Loss\n        self.loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(self.logits, self.y))\n\n        # Optimization\n        self.lr = tf.Variable(0.0, trainable=False)\n        self.trainable_vars = tf.trainable_variables()\n        # clip the gradient to avoid vanishing or blowing up gradients\n        self.grads, self.norm = tf.clip_by_global_norm(\n            tf.gradients(self.loss, self.trainable_vars), FLAGS.max_gradient_norm)\n        optimizer = tf.train.AdamOptimizer(self.lr)\n        self.update = optimizer.apply_gradients(\n            zip(self.grads, self.trainable_vars))\n\n        # Accuracy\n        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.logits, 1),\n            tf.argmax(self.y, 1)), tf.float32))\n\n        # Components for model saving\n        self.global_step = tf.Variable(0, trainable=False) # won\'t step\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self, sess, batch_X, batch_y, l, e, forward_only):\n        """"""\n        Get results for training/validation.\n        """"""\n        input_feed = {self.X: batch_X, self.y: batch_y, self.l:l, self.e:e}\n\n        if not forward_only: # training\n            output_feed = [self.loss, self.accuracy, self.norm,\n            self.update]\n        elif forward_only: # validation\n            output_feed = [self.loss, self.accuracy]\n\n        # process outputs\n        outputs = sess.run(output_feed, input_feed)\n\n        if not forward_only:\n            return outputs[0], outputs[1], outputs[2], outputs[3]\n        elif forward_only:\n            return outputs[0], outputs[1]\n\n\n\n\n\n\n\n'"
fw/data_utils.py,0,"b'import numpy as np\nimport random\nimport cPickle\nimport os\n\n""""""\n    Creating the data set for fast weights implementation.\n    Data will minmic the synthetic dataset created in\n    https://arxiv.org/abs/1610.06258 Ba et al.\n\n    Ex.\n    c6a7s4??a = 7 (it asking for the value for the key a)\n    This is a very interesting dataset because\n    it requires the model to retrieve and use temporary memory\n    in order to accurately predict the proper value for the key.\n""""""\n\ndef get_three_letters():\n    """"""\n    Retrieve three random letters (a-z)\n    without replacement.\n    """"""\n    return np.random.choice(range(0,26), 3, replace=False)\n\ndef get_three_numbers():\n    """"""\n    Retrieve three random numbers (0-9)\n    with replacement.\n    """"""\n    return np.random.choice(range(26, 26+10), 3, replace=True)\n\ndef create_sequence():\n    """"""\n    Concatenate keys and values with\n    ?? and one of the keys.\n    Returns the input and output.\n    """"""\n    letters = get_three_letters()\n    numbers = get_three_numbers()\n    X = np.zeros((9))\n    y = np.zeros((1))\n    for i in range(0, 5, 2):\n        X[i] = letters[i/2]\n        X[i+1] = numbers[i/2]\n\n    # append ??\n    X[6] = 26+10\n    X[7] = 26+10\n\n    # last key and respective value (y)\n    index = np.random.choice(range(0,3), 1, replace=False)\n    X[8] = letters[index]\n    y = numbers[index]\n\n    # one hot encode X and y\n    X_one_hot = np.eye(26+10+1)[np.array(X).astype(\'int\')]\n    y_one_hot = np.eye(26+10+1)[y][0]\n\n    return X_one_hot, y_one_hot\n\ndef ordinal_to_alpha(sequence):\n    """"""\n    Convert from ordinal to alpha-numeric representations.\n    Just for funsies :)\n    """"""\n    corpus = [\'a\',\'b\',\'c\',\'d\',\'e\',\'f\',\'g\',\'h\',\'i\',\'j\',\'k\',\'l\',\n              \'m\',\'n\',\'o\',\'p\',\'q\',\'r\',\'s\',\'t\',\'u\',\'v\',\'w\',\'x\',\'y\',\'z\',\n               0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \'?\']\n\n    conversion = """"\n    for item in sequence:\n        conversion += str(corpus[int(item)])\n    return conversion\n\ndef create_data(num_samples):\n    """"""\n    Create a num_samples long set of X and y.\n    """"""\n    X = np.zeros([num_samples, 9, 26+10+1], dtype=np.int32)\n    y = np.zeros([num_samples, 26+10+1], dtype=np.int32)\n    for i in range(num_samples):\n        X[i], y[i] = create_sequence()\n    return X, y\n\ndef generate_epoch(X, y, num_epochs, batch_size):\n\n    for epoch_num in range(num_epochs):\n        yield generate_batch(X, y, batch_size)\n\ndef generate_batch(X, y, batch_size):\n\n    data_size = len(X)\n\n    num_batches = (data_size // batch_size)\n    for batch_num in range(num_batches):\n        start_index = batch_num * batch_size\n        end_index = min((batch_num + 1) * batch_size, data_size)\n        yield X[start_index:end_index], y[start_index:end_index]\n\nif __name__ == \'__main__\':\n\n    # Sampling\n    sample_X, sample_y = create_sequence()\n    print ""Sample:"", ordinal_to_alpha([np.argmax(X) for X in sample_X]), \\\n        ordinal_to_alpha([np.argmax(sample_y)])\n\n    # Train/valid sets\n    train_X, train_y = create_data(64000)\n    print ""train_X:"", np.shape(train_X), "",train_y:"", np.shape(train_y)\n    valid_X, valid_y = create_data(32000)\n    print ""valid_X:"", np.shape(valid_X), "",valid_y:"", np.shape(valid_y)\n\n    # Save data into pickle files\n    if not os.path.exists(\'data\'):\n        os.makedirs(\'data\')\n    with open(\'data/train.p\', \'wb\') as f:\n        cPickle.dump([train_X, train_y], f)\n    with open(\'data/valid.p\', \'wb\') as f:\n        cPickle.dump([valid_X, valid_y], f)\n\n\n'"
fw/model.py,51,"b'import tensorflow as tf\nimport numpy as np\n\nfrom custom_GRU import (\n    ln,\n)\n\nclass fast_weights_model(object):\n\n    def __init__(self, FLAGS):\n\n        self.X = tf.placeholder(tf.float32,\n            shape=[None, FLAGS.input_dim, FLAGS.num_classes], name=\'inputs_X\')\n        self.y = tf.placeholder(tf.float32,\n            shape=[None, FLAGS.num_classes], name=\'targets_y\')\n        self.l = tf.placeholder(tf.float32, [], # need [] for tf.scalar_mul\n            name=""learning_rate"")\n        self.e = tf.placeholder(tf.float32, [],\n            name=""decay_rate"")\n\n        with tf.variable_scope(""fast_weights""):\n\n            # input weights (proper initialization)\n            self.W_x = tf.Variable(tf.random_uniform(\n                [FLAGS.num_classes, FLAGS.num_hidden_units],\n                -np.sqrt(2.0/FLAGS.num_classes),\n                np.sqrt(2.0/FLAGS.num_classes)),\n                dtype=tf.float32)\n            self.b_x = tf.Variable(tf.zeros(\n                [FLAGS.num_hidden_units]),\n                dtype=tf.float32)\n\n            # hidden weights (See Hinton\'s video @ 21:20)\n            self.W_h = tf.Variable(\n                initial_value=0.05 * np.identity(FLAGS.num_hidden_units),\n                dtype=tf.float32)\n\n            # softmax weights (proper initialization)\n            self.W_softmax = tf.Variable(tf.random_uniform(\n                [FLAGS.num_hidden_units, FLAGS.num_classes],\n                -np.sqrt(2.0 / FLAGS.num_hidden_units),\n                np.sqrt(2.0 / FLAGS.num_hidden_units)),\n                dtype=tf.float32)\n            self.b_softmax = tf.Variable(tf.zeros(\n                [FLAGS.num_classes]),\n                dtype=tf.float32)\n\n            # scale and shift for layernorm\n            self.gain = tf.Variable(tf.ones(\n                [FLAGS.num_hidden_units]),\n                dtype=tf.float32)\n            self.bias = tf.Variable(tf.zeros(\n                [FLAGS.num_hidden_units]),\n                dtype=tf.float32)\n\n        # fast weights and hidden state initialization\n        self.A = tf.zeros(\n            [FLAGS.batch_size, FLAGS.num_hidden_units, FLAGS.num_hidden_units],\n            dtype=tf.float32)\n        self.h = tf.zeros(\n            [FLAGS.batch_size, FLAGS.num_hidden_units],\n            dtype=tf.float32)\n\n        # NOTE:inputs are batch-major\n        # Process batch by time-major\n        for t in range(0, FLAGS.input_dim):\n\n            # hidden state (preliminary vector)\n            self.h = tf.nn.relu((tf.matmul(self.X[:, t, :], self.W_x)+self.b_x) +\n                (tf.matmul(self.h, self.W_h)))\n\n            # Forward weight and layer normalization\n            if FLAGS.model_name == \'RNN-LN-FW\':\n\n                # Reshape h to use with a\n                self.h_s = tf.reshape(self.h,\n                    [FLAGS.batch_size, 1, FLAGS.num_hidden_units])\n\n                # Create the fixed A for this time step\n                self.A = tf.add(tf.scalar_mul(self.l, self.A),\n                    tf.scalar_mul(self.e, tf.batch_matmul(tf.transpose(\n                        self.h_s, [0, 2, 1]), self.h_s)))\n\n                # Loop for S steps\n                for _ in range(FLAGS.S):\n                    self.h_s = tf.reshape(\n                        tf.matmul(self.X[:, t, :], self.W_x)+self.b_x,\n                        tf.shape(self.h_s)) + tf.reshape(\n                        tf.matmul(self.h, self.W_h), tf.shape(self.h_s)) + \\\n                        tf.batch_matmul(self.h_s, self.A)\n\n                    # Apply layernorm\n                    mu = tf.reduce_mean(self.h_s, reduction_indices=2) # each sample\n                    sigma = tf.sqrt(tf.reduce_mean(tf.square(self.h_s - mu),\n                        reduction_indices=2))\n                    self.h_s = tf.div(tf.mul(self.gain, (self.h_s - mu)), sigma) + \\\n                        self.bias\n\n                    # Apply nonlinearity\n                    self.h_s = tf.nn.relu(self.h_s)\n\n                # Reshape h_s into h\n                self.h = tf.reshape(self.h_s,\n                    [FLAGS.batch_size, FLAGS.num_hidden_units])\n\n            elif FLAGS.model_name == \'RNN-LN\': # no fast weights but still LN\n\n                # Apply layer norm\n                with tf.variable_scope(\'just_ln\') as scope:\n                    if t > 0:\n                        scope.reuse_variables()\n                    self.h = ln(self.h, scope=\'h/\')\n\n            elif FLAGS.model_name == \'CONTROL\': # no fast weights or LN\n                pass\n\n        # All inputs processed! Time for softmax\n        self.logits = tf.matmul(self.h, self.W_softmax) + self.b_softmax\n\n        # Loss\n        self.loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(self.logits, self.y))\n\n        # Optimization\n        self.lr = tf.Variable(0.0, trainable=False)\n        self.trainable_vars = tf.trainable_variables()\n        # clip the gradient to avoid vanishing or blowing up gradients\n        self.grads, self.norm = tf.clip_by_global_norm(\n            tf.gradients(self.loss, self.trainable_vars), FLAGS.max_gradient_norm)\n        optimizer = tf.train.AdamOptimizer(self.lr)\n        self.update = optimizer.apply_gradients(\n            zip(self.grads, self.trainable_vars))\n\n        # Accuracy\n        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.logits, 1),\n            tf.argmax(self.y, 1)), tf.float32))\n\n        # Components for model saving\n        self.global_step = tf.Variable(0, trainable=False) # won\'t step\n        self.saver = tf.train.Saver(tf.all_variables())\n\n    def step(self, sess, batch_X, batch_y, l, e, forward_only=True):\n        """"""\n        Get results for training/validation.\n        """"""\n        input_feed = {self.X: batch_X, self.y: batch_y, self.l:l, self.e:e}\n\n        if not forward_only: # training\n            output_feed = [self.loss, self.accuracy, self.norm,\n            self.update]\n        elif forward_only: # validation\n            output_feed = [self.loss, self.accuracy]\n\n        # process outputs\n        outputs = sess.run(output_feed, input_feed)\n\n        if not forward_only:\n            return outputs[0], outputs[1], outputs[2], outputs[3]\n        elif forward_only:\n            return outputs[0], outputs[1]\n\n\n\n\n\n\n\n'"
fw/train.py,11,"b'import cPickle\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\nimport time\nimport sys\n\nfrom data_utils import (\n    generate_epoch,\n)\n\nfrom model import (\n    fast_weights_model,\n)\n\nfrom custom_GRU import (\n    gru_model,\n)\n\nclass parameters():\n\n    def __init__(self):\n\n        self.input_dim = 9\n        self.num_classes = 26+10+1\n        self.num_epochs = 1000\n        self.batch_size = 128\n\n        self.num_hidden_units = 50\n        self.l = 0.95 # decay lambda\n        self.e = 0.5 # learning rate eta\n        self.S = 1 # num steps to get to h_S(t+1)\n        self.learning_rate = 1e-4\n        self.learning_rate_decay_factor = 0.99 # don\'t use this decay\n        self.max_gradient_norm = 5.0\n\n        self.data_dir = \'data/\'\n        self.ckpt_dir = \'checkpoints/\'\n        self.save_every =  max(1, self.num_epochs//4) # save every 500 epochs\n\ndef create_model(sess, FLAGS):\n\n    if FLAGS.model_name == \'GRU-LN\':\n        fw_model = gru_model(FLAGS)\n    else:\n        fw_model = fast_weights_model(FLAGS)\n\n    ckpt = tf.train.get_checkpoint_state(FLAGS.ckpt_dir)\n    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n        print(""Restoring old model parameters from %s"" %\n                             ckpt.model_checkpoint_path)\n        fw_model.saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        print(""Created new model."")\n        sess.run(tf.initialize_all_variables())\n\n    return fw_model\n\ndef train(FLAGS):\n    """"""\n    Train the model on the associative retrieval task.\n    """"""\n\n    # Load the train/valid datasets\n    print ""Loading datasets:""\n    with open(os.path.join(FLAGS.data_dir, \'train.p\'), \'rb\') as f:\n        train_X, train_y = cPickle.load(f)\n        print ""train_X:"", np.shape(train_X), "",train_y:"", np.shape(train_y)\n    with open(os.path.join(FLAGS.data_dir, \'valid.p\'), \'rb\') as f:\n        valid_X, valid_y = cPickle.load(f)\n        print ""valid_X:"", np.shape(valid_X), "",valid_y:"", np.shape(valid_y)\n\n    with tf.Session() as sess:\n\n        # Load the model\n        model = create_model(sess, FLAGS)\n        start_time = time.time()\n\n        # Start training\n        train_epoch_loss = []; valid_epoch_loss = []\n        train_epoch_accuracy = []; valid_epoch_accuracy = []\n        train_epoch_gradient_norm = []\n        for train_epoch_num, train_epoch in enumerate(generate_epoch(\n            train_X, train_y, FLAGS.num_epochs, FLAGS.batch_size)):\n            print ""EPOCH:"", train_epoch_num\n\n            # Assign the learning rate\n            sess.run(tf.assign(model.lr, FLAGS.learning_rate))\n\n            #sess.run(tf.assign(model.lr, FLAGS.learning_rate))\n            # Decay the learning rate\n            #sess.run(tf.assign(model.lr, FLAGS.learning_rate * \\\n            #    (FLAGS.learning_rate_decay_factor ** epoch_num)))\n\n            #if epoch_num < 1000:\n            #    sess.run(tf.assign(model.lr, FLAGS.learning_rate))\n            #elif epoch_num >= 1000: # slow down now\n            #    sess.run(tf.assign(model.lr, 1e-4))\n\n            # Custom decay (empirically decided)\n            #if (epoch_num%1000 == 0):\n            #    sess.run(tf.assign(model.lr,\n            #        FLAGS.learning_rate/(10**(epoch_num//1000))))\n\n            # Train set\n            train_batch_loss = []\n            train_batch_accuracy = []\n            train_batch_gradient_norm = []\n            for train_batch_num, (batch_X, batch_y) in enumerate(train_epoch):\n\n                loss, accuracy, norm, _ = model.step(sess, batch_X, batch_y,\n                    FLAGS.l, FLAGS.e, forward_only=False)\n                train_batch_loss.append(loss)\n                train_batch_accuracy.append(accuracy)\n                train_batch_gradient_norm.append(norm)\n\n            train_epoch_loss.append(np.mean(train_batch_loss))\n            train_epoch_accuracy.append(np.mean(train_batch_accuracy))\n            train_epoch_gradient_norm.append(np.mean(train_batch_gradient_norm))\n            print (\'Epoch: [%i/%i] time: %.4f, loss: %.7f,\'\n                    \' acc: %.7f, norm: %.7f\' % (train_epoch_num, FLAGS.num_epochs,\n                        time.time() - start_time, train_epoch_loss[-1],\n                        train_epoch_accuracy[-1], train_epoch_gradient_norm[-1]))\n\n            # Validation set\n            valid_batch_loss = []\n            valid_batch_accuracy = []\n            for valid_epoch_num, valid_epoch in enumerate(generate_epoch(\n                valid_X, valid_y, num_epochs=1, batch_size=FLAGS.batch_size)):\n\n                for valid_batch_num, (batch_X, batch_y) in enumerate(valid_epoch):\n                    loss, accuracy = model.step(sess, batch_X, batch_y,\n                        FLAGS.l, FLAGS.e, forward_only=True)\n                    valid_batch_loss.append(loss)\n                    valid_batch_accuracy.append(accuracy)\n\n            valid_epoch_loss.append(np.mean(valid_batch_loss))\n            valid_epoch_accuracy.append(np.mean(valid_batch_accuracy))\n\n            # Save the model\n            if (train_epoch_num % FLAGS.save_every == 0 or\n                train_epoch_num == (FLAGS.num_epochs-1)) and \\\n                (train_epoch_num > 0):\n                if not os.path.isdir(FLAGS.ckpt_dir):\n                    os.makedirs(FLAGS.ckpt_dir)\n                checkpoint_path = os.path.join(FLAGS.ckpt_dir,\n                    ""%s.ckpt"" % model_name)\n                print ""Saving the model.""\n                model.saver.save(sess, checkpoint_path,\n                                 global_step=model.global_step)\n\n        plt.plot(train_epoch_accuracy, label=\'train accuracy\')\n        plt.plot(valid_epoch_accuracy, label=\'valid accuracy\')\n        plt.legend(loc=4)\n        plt.title(\'%s_Accuracy\' % FLAGS.model_name)\n        plt.show()\n\n        plt.plot(train_epoch_loss, label=\'train loss\')\n        plt.plot(valid_epoch_loss, label=\'valid loss\')\n        plt.legend(loc=3)\n        plt.title(\'%s_Loss\' % FLAGS.model_name)\n        plt.show()\n\n        plt.plot(train_epoch_gradient_norm, label=\'gradient norm\')\n        plt.legend(loc=4)\n        plt.title(\'%s_Gradient Norm\' % FLAGS.model_name)\n        plt.show()\n\n        # Store results for global plot\n        with open(\'%s_results.p\' % FLAGS.model_name, \'wb\') as f:\n            cPickle.dump([train_epoch_accuracy, valid_epoch_accuracy,\n                train_epoch_loss, valid_epoch_loss,\n                train_epoch_gradient_norm], f)\n\ndef test(FLAGS):\n    """"""\n    Sample inputs of your own.\n    """"""\n    # Corpus for indexing\n    corpus = [\'a\',\'b\',\'c\',\'d\',\'e\',\'f\',\'g\',\'h\',\'i\',\'j\',\'k\',\'l\',\n              \'m\',\'n\',\'o\',\'p\',\'q\',\'r\',\'s\',\'t\',\'u\',\'v\',\'w\',\'x\',\'y\',\'z\',\n               \'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'?\']\n\n    # Render the sample to proper input format\n    sample = \'g5o8k1??g\'\n    X = []\n    for item in sample:\n        X.append(corpus.index(item))\n    X_one_hot = np.eye(26+10+1)[np.array(X).astype(\'int\')]\n\n    with tf.Session() as sess:\n\n        if FLAGS.model_name == \'RNN-LN-FW\':\n\n            # Inputs need to real inputs of batch_size 128\n            # because we use A(t) which updates even during testing\n\n            # Load the model\n            model = create_model(sess, FLAGS)\n\n            # Load real samples\n            with open(os.path.join(FLAGS.data_dir, \'train.p\'), \'rb\') as f:\n                train_X, train_y = cPickle.load(f)\n            for train_epoch_num, train_epoch in enumerate(generate_epoch(\n                train_X, train_y, 1, FLAGS.batch_size)):\n                for train_batch_num, (batch_X, batch_y) in enumerate(train_epoch):\n                    batch_X[0] = X_one_hot\n                    logits = model.logits.eval(feed_dict={model.X: batch_X,\n                        model.l: FLAGS.l, model.e: FLAGS.e})\n\n                    print ""INPUT:"", sample\n                    print ""PREDICTION:"", corpus[np.argmax(logits[0])]\n\n                    return\n\n        else:\n            # Reset from train sizes to sample sizes\n            FLAGS.batch_size = 1\n\n            # Load the model\n            model = create_model(sess, FLAGS)\n            logits = model.logits.eval(feed_dict={model.X: [X_one_hot],\n                model.l: FLAGS.l, model.e: FLAGS.e})\n\n            print ""INPUT:"", sample\n            print ""PREDICTION:"", corpus[np.argmax(logits)]\n\n\ndef plot_all():\n    """"""\n    Plot the results.\n    """"""\n\n    with open(\'CONTROL_results.p\', \'rb\') as f:\n        control_results = cPickle.load(f)\n    with open(\'RNN-LN_results.p\', \'rb\') as f:\n        RNN_LN_results = cPickle.load(f)\n    with open(\'RNN-LN-FW_results.p\', \'rb\') as f:\n        RNN_LN_FW_results = cPickle.load(f)\n    with open(\'GRU-LN_results.p\', \'rb\') as f:\n        GRU_LN_results = cPickle.load(f)\n\n    # Plotting accuracy\n    fig = plt.figure()\n    plt.plot(control_results[1], label=\'Control accuracy\')\n    plt.plot(RNN_LN_results[1], label=\'RNN-LN accuracy\')\n    plt.plot(RNN_LN_FW_results[1], label=\'RNN-LN-FW accuracy\')\n    plt.plot(GRU_LN_results[1], label=\'GRU-LN accuracy\')\n    plt.title(\'Test Accuracy\')\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Accuracy\')\n    plt.legend(loc=4)\n    fig.savefig(\'accuracy.png\')\n    #plt.show()\n\n    # Plotting loss\n    fig = plt.figure()\n    plt.plot(control_results[3], label=\'Control loss\')\n    plt.plot(RNN_LN_results[3], label=\'RNN-LN loss\')\n    plt.plot(RNN_LN_FW_results[3], label=\'RNN-LN-FW loss\')\n    plt.plot(GRU_LN_results[3], label=\'GRU-LN loss\')\n    plt.title(\'Test Loss\')\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Loss\')\n    plt.legend(loc=1)\n    fig.savefig(\'loss.png\')\n    #plt.show()\n\n\nif __name__ == \'__main__\':\n\n    FLAGS = parameters()\n\n    if sys.argv[1] == \'train\':\n        model_name = sys.argv[2]\n        FLAGS.ckpt_dir = FLAGS.ckpt_dir + model_name\n        FLAGS.model_name = model_name\n        train(FLAGS)\n    elif sys.argv[1] == \'test\':\n        model_name = sys.argv[2]\n        FLAGS.ckpt_dir = FLAGS.ckpt_dir + model_name\n        FLAGS.model_name = model_name\n        test(FLAGS)\n    elif sys.argv[1] == \'plot\':\n        plot_all()\n\n\n\n'"
