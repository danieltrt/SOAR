file_path,api_count,code
00_Miscellaneous/model_optimisation/inference_test.py,4,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n"""""" Extract from notebook for Serving Optimization """"""\n\nfrom __future__ import print_function\n\nfrom googleapiclient import discovery\nfrom oauth2client.client import GoogleCredentials\nimport numpy as np\nimport tensorflow as tf\nfrom datetime import datetime\nimport requests\nimport sys\nimport json\n\nBATCH_SIZE = 100\n\nDISCOVERY_URL = \'https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json\'\n\nPROJECT = \'lramsey-goog-com-csa-ml\'\nMODEL_NAME = \'mnist_classifier\'\n\ncredentials = GoogleCredentials.get_application_default()\napi = discovery.build(\n    \'ml\', \'v1\',\n    credentials=credentials,\n    discoveryServiceUrl=DISCOVERY_URL\n)\n\n\ndef load_mnist_data():\n  mnist = tf.contrib.learn.datasets.load_dataset(\'mnist\')\n  train_data = mnist.train.images\n  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n  eval_data = mnist.test.images\n  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n  return train_data, train_labels, eval_data, eval_labels\n\n\ndef load_mnist_keras():\n  (train_data, train_labels), (eval_data, eval_labels) = tf.keras.datasets.mnist.load_data()\n  return train_data, train_labels, eval_data, eval_labels\n\n\ndef inference_tfserving(eval_data, batch=BATCH_SIZE, repeat=10, signature=\'predict\'):\n  url = \'http://localhost:8501/v1/models/mnist_classifier:predict\'\n\n  instances = [[float(i) for i in list(eval_data[img])] for img in range(batch)]\n\n  request_data = {\'signature_name\': signature,\n                  \'instances\': instances}\n\n  time_start = datetime.utcnow()\n  response = requests.post(url, data=json.dumps(request_data))\n  if response.status_code != 200:\n    raise Exception(""Bad response status from TF Serving instance: %d"" % response.status_code)\n  for i in range(repeat-1):\n    response = requests.post(url, data=json.dumps(request_data))\n  time_end = datetime.utcnow()\n  time_elapsed_sec = (time_end - time_start).total_seconds()\n\n  print(\'Total elapsed time: {} seconds\'.format(time_elapsed_sec))\n  print(\'Time for batch size {} repeated {} times\'.format(BATCH_SIZE, repeat))\n  print(\'Average latency per batch: {} seconds\'.format(time_elapsed_sec/repeat))\n\n\ndef predict(version, instances):\n  request_data = {\'instances\': instances}\n  model_url = \'projects/{}/models/{}/versions/{}\'.format(\n      PROJECT, MODEL_NAME, version)\n  response = api.projects().predict(\n      body=request_data, name=model_url).execute()\n  class_ids = None\n  try:\n    class_ids = [item[""class_ids""] for item in response[""predictions""]]\n  except:\n    print(response)\n  return class_ids\n\n\ndef inference_cmle(version, eval_data, batch=BATCH_SIZE, repeat=10):\n  instances = [\n    {\'input_image\': [float(i) for i in list(eval_data[img])]}\n    for img in range(batch)\n  ]\n\n  # warmup request\n  predict(version, instances[0])\n  print(\'Warm up request performed!\')\n  print(\'Timer started...\', \'\')\n\n  time_start = datetime.utcnow()\n  output = None\n  for i in range(repeat):\n    output = predict(version, instances)\n  time_end = datetime.utcnow()\n  time_elapsed_sec = (time_end - time_start).total_seconds()\n\n  print(\'Total elapsed time: {} seconds\'.format(time_elapsed_sec), \'\')\n  print(\'Time for batch size {} repeated {} times\'.format(BATCH_SIZE, repeat))\n  print(\'Average latency per batch: {} seconds\'.format(time_elapsed_sec/repeat))\n  print(\'Prediction output for the last instance: {}\'.format(output[0]))\n\n\ndef inference_test(saved_model_dir,\n                   eval_data,\n                   signature=\'predict\',\n                   repeat=10):\n  tf.logging.set_verbosity(tf.logging.ERROR)\n\n  # load model\n  time_start = datetime.utcnow()\n  for i in range(repeat/10):\n    predictor = tf.contrib.predictor.from_saved_model(\n        export_dir=saved_model_dir,\n        signature_def_key=signature\n    )\n  time_end = datetime.utcnow()\n\n  loading_time = (time_end - time_start).total_seconds()\n  print(\'\', \'Model loading time: {} seconds\'.format(\n      loading_time), \'\')\n\n  # serve a batch of\n  time_start = datetime.utcnow()\n  output = None\n  for i in range(repeat):\n    output = predictor(\n        {\'input_image\': eval_data[:BATCH_SIZE]}\n    )\n  time_end = datetime.utcnow()\n\n  print(\'Prediction produced for {} instances repeated {} times\'.format(\n      BATCH_SIZE, repeat), \'\')\n\n  serving_time = (time_end - time_start).total_seconds()\n  print(\'Inference elapsed time: {} seconds\'.format(\n      serving_time), \'\')\n\n  print(\'Prediction output for the last instance:\')\n  for key in output.keys():\n    print(\'{}: {}\'.format(key,output[key][0]))\n\n  return loading_time, serving_time\n\nif __name__ == \'__main__\':\n  _, _, eval_data, _ = load_mnist_data()\n  engine = sys.argv[1]\n  if engine == \'predictor\':\n    inference_test(sys.argv[2], eval_data)\n  elif engine == \'cmle\':\n    inference_cmle(sys.argv[2], eval_data)\n  elif engine == \'tfserving\':\n    if len(sys.argv) > 2:\n      print(\'Calling inference_tfserving with signature ""{}""\'.format(sys.argv[2]))\n      inference_tfserving(eval_data, repeat=1000, signature=sys.argv[2])\n    else:\n      print(\'Calling inference_tfserving with signature ""predict""\')\n      inference_tfserving(eval_data, repeat=1000)\n'"
00_Miscellaneous/model_optimisation/optimize_graph.py,43,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n"""""" Extract from notebook for Serving Optimization """"""\n\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport sh\nimport sys\nimport tensorflow as tf\nfrom tensorflow import data\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.tools import freeze_graph\nfrom tensorflow.python import ops\nfrom tensorflow.tools.graph_transforms import TransformGraph\n\nfrom inference_test import inference_test, load_mnist_data\n\nNUM_CLASSES = 10\nMODELS_LOCATION = \'models/mnist\'\nMODEL_NAME = \'cnn_classifier\'\n\n\ndef model_fn(features, labels, mode, params):\n  # conv layers\n  def _cnn_layers(input_layer, num_conv_layers, init_filters, mode):\n    inputs = input_layer\n    for i in range(num_conv_layers):\n      current_filters = init_filters * (2**i)\n      conv = tf.layers.conv2d(inputs=inputs, kernel_size=3,\n                              filters=current_filters, strides=1,\n                              padding=\'SAME\', name=\'conv{}\'.format(i+1))\n      pool = tf.layers.max_pooling2d(\n          inputs=conv, pool_size=2, strides=2,\n          padding=\'SAME\', name=\'pool{}\'.format(i+1))\n      batch_norm = tf.layers.batch_normalization(\n          pool, name=\'batch_norm{}\'.format(i+1))\n      if mode==tf.estimator.ModeKeys.TRAIN:\n          batch_norm = tf.nn.dropout(batch_norm, params.drop_out)\n      inputs = batch_norm\n    outputs = batch_norm\n    return outputs\n\n  # model body\n  def _body(features, mode, params):\n    input_layer = tf.reshape(features[\'input_image\'],\n                             [-1, 28, 28, 1], name=\'input_image\')\n    conv_outputs = _cnn_layers(input_layer,\n                               params.num_conv_layers,\n                               params.init_filters, mode)\n    flatten = tf.layers.flatten(inputs=conv_outputs, name=\'flatten\')\n    fully_connected = tf.contrib.layers.stack(\n        inputs=flatten,\n        layer=tf.contrib.layers.fully_connected,\n        stack_args=params.hidden_units,\n        activation_fn=tf.nn.relu)\n    # unused_layer\n    unused_layers = tf.layers.dense(flatten, units=100, name=\'unused\',\n                                    activation=tf.nn.relu)\n    logits = tf.layers.dense(fully_connected, units=NUM_CLASSES,\n                             name=\'logits\', activation=None)\n    return logits\n\n  # model head\n  head = tf.contrib.estimator.multi_class_head(n_classes=NUM_CLASSES)\n\n  return head.create_estimator_spec(\n      features=features,\n      mode=mode,\n      logits=_body(features, mode, params),\n      labels=labels,\n      optimizer=tf.train.AdamOptimizer(params.learning_rate)\n  )\n\n\ndef create_estimator(params, run_config):\n\n  def _metric_fn(labels, predictions):\n    metrics = {}\n    pred_class = predictions[\'class_ids\']\n    metrics[\'micro_accuracy\'] = tf.metrics.mean_per_class_accuracy(\n        labels=labels, predictions=pred_class, num_classes=NUM_CLASSES\n    )\n    return metrics\n\n  mnist_classifier = tf.estimator.Estimator(\n      model_fn=model_fn, params=params, config=run_config)\n\n  mnist_classifier = tf.contrib.estimator.add_metrics(\n      estimator=mnist_classifier, metric_fn=_metric_fn)\n\n  return mnist_classifier\n\n\n#### Run Experiment\n\ndef run_experiment(hparams, train_data, train_labels, run_config, create_estimator_fn=create_estimator):\n  train_spec = tf.estimator.TrainSpec(\n      input_fn = tf.estimator.inputs.numpy_input_fn(\n          x={\'input_image\': train_data},\n          y=train_labels,\n          batch_size=hparams.batch_size,\n          num_epochs=None,\n          shuffle=True),\n      max_steps=hparams.max_training_steps\n  )\n  eval_spec = tf.estimator.EvalSpec(\n      input_fn = tf.estimator.inputs.numpy_input_fn(\n          x={\'input_image\': train_data},\n          y=train_labels,\n          batch_size=hparams.batch_size,\n          num_epochs=1,\n          shuffle=False),\n      steps=None,\n      throttle_secs=hparams.eval_throttle_secs\n  )\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  time_start = datetime.utcnow()\n  print(\'Experiment started at {}\'.format(time_start.strftime(\'%H:%M:%S\')))\n  print(\'.......................................\')\n\n  estimator = create_estimator_fn(hparams, run_config)\n\n  tf.estimator.train_and_evaluate(\n      estimator=estimator,\n      train_spec=train_spec,\n      eval_spec=eval_spec\n  )\n\n  time_end = datetime.utcnow()\n  print(\'.......................................\')\n  print(\'Experiment finished at {}\'.format(time_end.strftime(\'%H:%M:%S\')))\n  print(\'\')\n  time_elapsed = time_end - time_start\n  print(\'Experiment elapsed time: {} seconds\'.format(time_elapsed.total_seconds()))\n\n  return estimator\n\n\n#### Train and Export Model\n\ndef train_and_export_model(train_data, train_labels):\n  model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n\n  hparams  = tf.contrib.training.HParams(\n      batch_size=100,\n      hidden_units=[1024],\n      num_conv_layers=2,\n      init_filters=64,\n      drop_out=0.85,\n      max_training_steps=50,\n      eval_throttle_secs=10,\n      learning_rate=1e-3,\n      debug=True\n  )\n\n  run_config = tf.estimator.RunConfig(\n      tf_random_seed=19830610,\n      save_checkpoints_steps=1000,\n      keep_checkpoint_max=3,\n      model_dir=model_dir\n  )\n\n  if tf.gfile.Exists(model_dir):\n      print(\'Removing previous artifacts...\')\n      tf.gfile.DeleteRecursively(model_dir)\n\n  estimator = run_experiment(hparams, train_data, train_labels, run_config)\n\n  def make_serving_input_receiver_fn():\n      inputs = {\'input_image\': tf.placeholder(\n          shape=[None,784], dtype=tf.float32, name=\'input_image\')}\n      return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n\n  export_dir = os.path.join(model_dir, \'export\')\n\n  if tf.gfile.Exists(export_dir):\n      tf.gfile.DeleteRecursively(export_dir)\n\n  estimator.export_savedmodel(\n      export_dir_base=export_dir,\n      serving_input_receiver_fn=make_serving_input_receiver_fn()\n  )\n\n  return export_dir\n\n\n#### Load GraphDef from a SavedModel Directory\n\ndef get_graph_def_from_saved_model(saved_model_dir):\n  with tf.Session() as session:\n      meta_graph_def = tf.saved_model.loader.load(\n          session,\n          tags=[tag_constants.SERVING],\n          export_dir=saved_model_dir\n      )\n  return meta_graph_def.graph_def\n\n\n#### Describe GraphDef\n\ndef describe_graph(graph_def, show_nodes=False):\n  print(\'Input Feature Nodes: {}\'.format([node.name for node in graph_def.node if node.op==\'Placeholder\']))\n  print(\'\')\n  print(\'Unused Nodes: {}\'.format([node.name for node in graph_def.node if \'unused\'  in node.name]))\n  print(\'\')\n  print(\'Output Nodes: {}\'.format(\n      [node.name for node in graph_def.node if (\'predictions\' in node.name or \'softmax\' in node.name)]))\n  print(\'\')\n  print(\'Quantization Nodes: {}\'.format( [node.name for node in graph_def.node if \'quant\' in node.name]))\n  print(\'\')\n  print(\'Constant Count: {}\'.format( len([node for node in graph_def.node if node.op==\'Const\'])))\n  print(\'\')\n  print(\'Variable Count: {}\'.format( len([node for node in graph_def.node if \'Variable\' in node.op])))\n  print(\'\')\n  print(\'Identity Count: {}\'.format( len([node for node in graph_def.node if node.op==\'Identity\'])))\n  print(\'\')\n  print(\'Total nodes: {}\'.format( len(graph_def.node)))\n  print(\'\')\n\n  if show_nodes==True:\n    for node in graph_def.node:\n      print(\'Op:{} - Name: {}\'.format(node.op, node.name))\n\n\n#### Get model size and metagraph\n\ndef get_size(model_dir, model_file=\'saved_model.pb\', include_vars=True):\n  model_file_path = os.path.join(model_dir, model_file)\n  print(model_file_path, \'\')\n  pb_size = os.path.getsize(model_file_path)\n  variables_size = 0\n  if include_vars and os.path.exists(os.path.join(model_dir,\'variables/variables.data-00000-of-00001\')):\n    variables_size = os.path.getsize(os.path.join(\n        model_dir,\'variables/variables.data-00000-of-00001\'))\n    variables_size += os.path.getsize(os.path.join(model_dir,\'variables/variables.index\'))\n  print(\'Model size: {} KB\'.format(round(pb_size/(1024.0),3)))\n  if include_vars:\n    print(\'Variables size: {} KB\'.format(round( variables_size/(1024.0),3)))\n    print(\'Total Size: {} KB\'.format(round((pb_size + variables_size)/(1024.0),3)))\n\n\ndef get_metagraph(model_dir):\n  output = sh.saved_model_cli(\'show\', \'--dir=%s\' % model_dir, \'--all\')\n  print(output)\n\n\n#### Get graph def from MetaGraphDef\n\ndef get_graph_def_from_file(graph_filepath):\n  with ops.Graph().as_default():\n    with tf.gfile.GFile(graph_filepath, \'rb\') as f:\n      graph_def = tf.GraphDef()\n      graph_def.ParseFromString(f.read())\n      return graph_def\n\n\ndef optimize_graph(model_dir, graph_filename, transforms, output_node):\n  input_names = []\n  output_names = [output_node]\n  if graph_filename is None:\n    graph_def = get_graph_def_from_saved_model(model_dir)\n  else:\n    graph_def = get_graph_def_from_file(os.path.join(model_dir, graph_filename))\n  optimized_graph_def = TransformGraph(\n      graph_def,\n      input_names,\n      output_names,\n      transforms)\n  tf.train.write_graph(optimized_graph_def,\n                      logdir=model_dir,\n                      as_text=False,\n                      name=\'optimized_model.pb\')\n  print(\'****************************************\')\n  print(\'Graph optimized!\')\n  print(\'****************************************\')\n\n\ndef freeze_model(saved_model_dir, output_node_names, output_filename):\n  output_graph_filename = os.path.join(saved_model_dir, output_filename)\n  initializer_nodes = \'\'\n  freeze_graph.freeze_graph(\n      input_saved_model_dir=saved_model_dir,\n      output_graph=output_graph_filename,\n      saved_model_tags = tag_constants.SERVING,\n      output_node_names=output_node_names,\n      initializer_nodes=initializer_nodes,\n      input_graph=None,\n      input_saver=False,\n      input_binary=False,\n      input_checkpoint=None,\n      restore_op_name=None,\n      filename_tensor_name=None,\n      clear_devices=False,\n      input_meta_graph=False,\n  )\n  print(\'****************************************\')\n  print(\'graph freezed!\')\n  print(\'****************************************\')\n\n\ndef convert_graph_def_to_saved_model(export_dir, graph_filepath, output_key, output_node_name):\n  if tf.gfile.Exists(export_dir):\n    tf.gfile.DeleteRecursively(export_dir)\n  graph_def = get_graph_def_from_file(graph_filepath)\n  with tf.Session(graph=tf.Graph()) as session:\n    tf.import_graph_def(graph_def, name=\'\')\n    tf.saved_model.simple_save(\n        session,\n        export_dir,\n        inputs={\n            node.name: session.graph.get_tensor_by_name(\n                \'{}:0\'.format(node.name))\n            for node in graph_def.node if node.op==\'Placeholder\'},\n        outputs={output_key: session.graph.get_tensor_by_name(\n            output_node_name)}\n    )\n    print(\'****************************************\')\n    print(\'Optimized graph converted to SavedModel!\')\n    print(\'****************************************\')\n\n\ndef setup_model():\n  train_data, train_labels, eval_data, eval_labels = load_mnist_data()\n  export_dir = train_and_export_model(train_data, train_labels)\n  return export_dir, eval_data\n\n\nTRANSFORMS = [\n    \'remove_nodes(op=Identity)\',\n    \'fold_constants(ignore_errors=true)\',\n    \'merge_duplicate_nodes\',\n    \'strip_unused_nodes\',\n    \'fold_batch_norms\'\n]\n\nQUANTIZE_TRANSFORMS = [\n    \'quantize_weights\',\n    \'quantize_nodes\'\n]\n\nNUM_TRIALS = 10\n\ndef main(args):\n  if len(args) > 1 and args[1] == \'--inference\':\n    export_dir = args[2]\n    _, _, eval_data, _ = load_mnist_data()\n\n    total_load_time = 0.0\n    total_serve_time = 0.0\n    saved_model_dir = os.path.join(\n        export_dir, [f for f in os.listdir(export_dir) if f.isdigit()][0])\n    for i in range(0, NUM_TRIALS):\n      load_time, serving_time = inference_test(saved_model_dir, eval_data, repeat=10000)\n      total_load_time += load_time\n      total_serve_time += serving_time\n\n    print(""****************************************"")\n    print(""*** Load time on original model: {:.2f}"".format(total_load_time / NUM_TRIALS))\n    print(""*** Serve time on original model: {:.2f}"".format(total_serve_time / NUM_TRIALS))\n    print(""****************************************"")\n\n    total_load_time = 0.0\n    total_serve_time = 0.0\n    optimized_export_dir = os.path.join(export_dir, \'optimized\')\n    for i in range(0, NUM_TRIALS):\n      load_time, serving_time = inference_test(optimized_export_dir, eval_data,\n                                               signature=\'serving_default\',\n                                               repeat=10000)\n      total_load_time += load_time\n      total_serve_time += serving_time\n    print(""****************************************"")\n    print(""*** Load time on optimized model: {:.2f}"".format(total_load_time / NUM_TRIALS))\n    print(""*** Serve time on optimized model: {:.2f}"".format(total_serve_time / NUM_TRIALS))\n    print(""****************************************"")\n\n\n  else:\n    # generate and output original model\n    export_dir, eval_data = setup_model()\n    saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1])\n    describe_graph(get_graph_def_from_saved_model(saved_model_dir))\n    get_size(saved_model_dir, \'saved_model.pb\')\n    get_metagraph(saved_model_dir)\n\n    # freeze model and describe it\n    freeze_model(saved_model_dir, \'head/predictions/class_ids\', \'frozen_model.pb\')\n    frozen_filepath = os.path.join(saved_model_dir, \'frozen_model.pb\')\n    describe_graph(get_graph_def_from_file(frozen_filepath))\n    get_size(saved_model_dir, \'frozen_model.pb\', include_vars=False)\n\n    # optimize model and describe it\n    optimize_graph(saved_model_dir, \'frozen_model.pb\', TRANSFORMS, \'head/predictions/class_ids\')\n    optimized_filepath = os.path.join(saved_model_dir, \'optimized_model.pb\')\n    describe_graph(get_graph_def_from_file(optimized_filepath))\n    get_size(saved_model_dir, \'optimized_model.pb\', include_vars=False)\n\n    # convert to saved model and output metagraph again\n    optimized_export_dir = os.path.join(export_dir, \'optimized\')\n    convert_graph_def_to_saved_model(optimized_export_dir, optimized_filepath, \'class_ids\',\n                                     \'head/predictions/class_ids:0\')\n    get_size(optimized_export_dir, \'saved_model.pb\')\n    get_metagraph(optimized_export_dir)\n\n\n\nif __name__ == \'__main__\':\n  main(sys.argv)\n'"
00_Miscellaneous/model_optimisation/optimize_graph_keras.py,22,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n"""""" Extract from notebook for Serving Optimization on Keras """"""\n\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport sh\nimport sys\nimport tensorflow as tf\nfrom tensorflow import data\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.tools import freeze_graph\nfrom tensorflow.python import ops\nfrom tensorflow.tools.graph_transforms import TransformGraph\n\nfrom inference_test import inference_test, load_mnist_keras\nfrom optimize_graph import (run_experiment, get_graph_def_from_saved_model,\n    describe_graph, get_size, get_metagraph, get_graph_def_from_file,\n    convert_graph_def_to_saved_model, freeze_model, optimize_graph, TRANSFORMS)\n\nNUM_CLASSES = 10\nMODELS_LOCATION = \'models/mnist\'\nMODEL_NAME = \'keras_classifier\'\n\n\ndef keras_model_fn(params):\n\n  inputs = tf.keras.layers.Input(shape=(28, 28), name=\'input_image\')\n  input_layer = tf.keras.layers.Reshape(target_shape=(28, 28, 1), name=\'reshape\')(inputs)\n\n  # convolutional layers\n  conv_inputs = input_layer\n  for i in range(params.num_conv_layers):\n    filters = params.init_filters * (2**i)\n    conv = tf.keras.layers.Conv2D(kernel_size=3, filters=filters, strides=1, padding=\'SAME\', activation=\'relu\')(conv_inputs)\n    max_pool = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding=\'SAME\')(conv)\n    batch_norm = tf.keras.layers.BatchNormalization()(max_pool)\n    conv_inputs = batch_norm\n\n  flatten = tf.keras.layers.Flatten(name=\'flatten\')(conv_inputs)\n\n  # fully-connected layers\n  dense_inputs = flatten\n  for i in range(len(params.hidden_units)):\n    dense = tf.keras.layers.Dense(units=params.hidden_units[i], activation=\'relu\')(dense_inputs)\n    dropout = tf.keras.layers.Dropout(params.dropout)(dense)\n    dense_inputs = dropout\n\n  # softmax classifier\n  logits = tf.keras.layers.Dense(units=NUM_CLASSES, name=\'logits\')(dense_inputs)\n  softmax = tf.keras.layers.Activation(\'softmax\', name=\'softmax\')(logits)\n\n  # keras model\n  model = tf.keras.models.Model(inputs, softmax)\n  return model\n\n\ndef create_estimator_keras(params, run_config):\n\n  keras_model = keras_model_fn(params)\n  print(keras_model.summary())\n\n  optimizer = tf.keras.optimizers.Adam(lr=params.learning_rate)\n  keras_model.compile(loss=\'sparse_categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n  mnist_classifier = tf.keras.estimator.model_to_estimator(\n      keras_model=keras_model,\n      config=run_config\n  )\n\n  return mnist_classifier\n\n\n#### Train and Export Model\n\ndef train_and_export_model(train_data, train_labels):\n  model_dir = os.path.join(MODELS_LOCATION, MODEL_NAME)\n\n  hparams  = tf.contrib.training.HParams(\n      batch_size=100,\n      hidden_units=[512, 512],\n      num_conv_layers=3,\n      init_filters=64,\n      dropout=0.2,\n      max_training_steps=50,\n      eval_throttle_secs=10,\n      learning_rate=1e-3,\n      debug=True\n  )\n\n  run_config = tf.estimator.RunConfig(\n      tf_random_seed=19830610,\n      save_checkpoints_steps=1000,\n      keep_checkpoint_max=3,\n      model_dir=model_dir\n  )\n\n  if tf.gfile.Exists(model_dir):\n      print(\'Removing previous artifacts...\')\n      tf.gfile.DeleteRecursively(model_dir)\n\n  os.makedirs(model_dir)\n\n  estimator = run_experiment(hparams, train_data, train_labels, run_config, create_estimator_keras)\n\n  def make_serving_input_receiver_fn():\n      inputs = {\'input_image\': tf.placeholder(\n          shape=[None,28,28], dtype=tf.float32, name=\'serving_input_image\')}\n      return tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)\n\n  export_dir = os.path.join(model_dir, \'export\')\n\n  if tf.gfile.Exists(export_dir):\n      tf.gfile.DeleteRecursively(export_dir)\n\n  estimator.export_savedmodel(\n      export_dir_base=export_dir,\n      serving_input_receiver_fn=make_serving_input_receiver_fn()\n  )\n\n  return export_dir\n\n\ndef setup_model():\n  train_data, train_labels, eval_data, eval_labels = load_mnist_keras()\n  export_dir = train_and_export_model(train_data, train_labels)\n  return export_dir, eval_data\n\n\nNUM_TRIALS = 10\n\ndef main(args):\n  if len(args) > 1 and args[1] == \'--inference\':\n    export_dir = args[2]\n    _, _, eval_data, _ = load_mnist_keras()\n\n    total_load_time = 0.0\n    total_serve_time = 0.0\n    saved_model_dir = os.path.join(\n        export_dir, [f for f in os.listdir(export_dir) if f.isdigit()][0])\n    for i in range(0, NUM_TRIALS):\n      load_time, serving_time = inference_test(saved_model_dir, eval_data, repeat=10000)\n      total_load_time += load_time\n      total_serve_time += serving_time\n\n    print(""****************************************"")\n    print(""*** Load time on original model: {:.2f}"".format(total_load_time / NUM_TRIALS))\n    print(""*** Serve time on original model: {:.2f}"".format(total_serve_time / NUM_TRIALS))\n    print(""****************************************"")\n\n    total_load_time = 0.0\n    total_serve_time = 0.0\n    optimized_export_dir = os.path.join(export_dir, \'optimized\')\n    for i in range(0, NUM_TRIALS):\n      load_time, serving_time = inference_test(optimized_export_dir, eval_data,\n                                               signature=\'serving_default\',\n                                               repeat=10000)\n      total_load_time += load_time\n      total_serve_time += serving_time\n    print(""****************************************"")\n    print(""*** Load time on optimized model: {:.2f}"".format(total_load_time / NUM_TRIALS))\n    print(""*** Serve time on optimized model: {:.2f}"".format(total_serve_time / NUM_TRIALS))\n    print(""****************************************"")\n\n  else:\n    # generate and output original model\n    export_dir, eval_data = setup_model()\n    saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1])\n    describe_graph(get_graph_def_from_saved_model(saved_model_dir))\n    get_size(saved_model_dir, \'saved_model.pb\')\n    get_metagraph(saved_model_dir)\n\n    # freeze model and describe it\n    freeze_model(saved_model_dir, \'softmax/Softmax\', \'frozen_model.pb\')\n    frozen_filepath = os.path.join(saved_model_dir, \'frozen_model.pb\')\n    describe_graph(get_graph_def_from_file(frozen_filepath))\n    get_size(saved_model_dir, \'frozen_model.pb\', include_vars=False)\n\n    # optimize model and describe it\n    optimize_graph(saved_model_dir, \'frozen_model.pb\', TRANSFORMS, \'softmax/Softmax\')\n    optimized_filepath = os.path.join(saved_model_dir, \'optimized_model.pb\')\n    describe_graph(get_graph_def_from_file(optimized_filepath))\n    get_size(saved_model_dir, \'optimized_model.pb\', include_vars=False)\n\n    # convert to saved model and output metagraph again\n    optimized_export_dir = os.path.join(export_dir, \'optimized\')\n    convert_graph_def_to_saved_model(optimized_export_dir, optimized_filepath,\n                                     \'softmax\', \'softmax/Softmax:0\')\n    get_size(optimized_export_dir, \'saved_model.pb\')\n    get_metagraph(optimized_export_dir)\n\n\nif __name__ == \'__main__\':\n  main(sys.argv)\n'"
00_Miscellaneous/tfx/setup.py,0,"b""from setuptools import find_packages\nfrom setuptools import setup\n\nREQUIRED_PACKAGES = [\n 'tensorflow==1.14.0',\n 'tensorflow-data-validation==0.11.0'\n]\n\nsetup(\n    name='tfdv-data-extraction',\n    version='0.1',\n    install_requires=REQUIRED_PACKAGES,\n    packages=find_packages(),\n    include_package_data=True\n)"""
00_Miscellaneous/model_evaluation_pipeline/evaluator/__init__.py,0,b''
00_Miscellaneous/model_evaluation_pipeline/evaluator/main.py,0,"b""from __future__ import absolute_import\n\nimport argparse\nimport datetime\nimport logging\n\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.options.pipeline_options import SetupOptions\n\nfrom process import pipeline\n\n\ndef main(argv=None):\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--year_from',\n                      dest='year_from',\n                      default=1969,\n                      help='A condition to extract test data from BigQuery.')\n  parser.add_argument('--year_to',\n                      dest='year_to',\n                      default=1973,\n                      help='A condition to extract test data from BigQuery.')\n  parser.add_argument('--datasize',\n                      dest='datasize',\n                      default=1000000,\n                      help='Maximum number of rows from BigQuery.')\n  parser.add_argument('--saved_model_dir',\n                      dest='saved_model_dir',\n                      default='gs://yaboo-cloud-training-demos-ml/babyweight/trained_model/1969-1973/export/exporter/1525182730',\n                      help='A path to a saved model directory on GCS.')\n  parser.add_argument('--output_table',\n                      dest='output_table',\n                      default='yaboo-sandbox:testing.prediction3',\n                      help='A name of a BQ table where you save eval results.')\n\n  known_args, pipeline_args = parser.parse_known_args(argv)\n  pipeline_options = PipelineOptions(pipeline_args)\n  setup_options = pipeline_options.view_as(SetupOptions)\n  setup_options.save_main_session = True\n\n  pipeline.run(pipeline_options,\n               known_args.saved_model_dir,\n               known_args.year_from,\n               known_args.year_to,\n               known_args.datasize,\n               known_args.output_table)\n\n\nif __name__ == '__main__':\n  logging.getLogger().setLevel(logging.INFO)\n  main()\n"""
00_Miscellaneous/model_evaluation_pipeline/evaluator/setup.py,0,"b""import setuptools\n\nsetuptools.setup(\n    name='model-evaluation-demo',\n    version='0.1',\n    install_requires=['tensorflow==1.8.0'],\n    packages=setuptools.find_packages(),\n    author='yaboo',\n    author_email='yaboo@google.com'\n)\n"""
00_Miscellaneous/model_evaluation_pipeline/trainer/__init__.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
00_Miscellaneous/model_evaluation_pipeline/trainer/model.py,34,"b'#!/usr/bin/env python\n\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport shutil\nimport numpy as np\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nBUCKET = None  # set from task.py\nPATTERN = \'of\' # gets all files\n\n# Determine CSV, label, and key columns\nCSV_COLUMNS = \'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key\'.split(\',\')\nLABEL_COLUMN = \'weight_pounds\'\nKEY_COLUMN = \'key\'\n\n# Set default values for each CSV column\nDEFAULTS = [[0.0], [\'null\'], [0.0], [\'null\'], [0.0], [\'nokey\']]\n\n# Define some hyperparameters\nTRAIN_STEPS = 10000\nEVAL_STEPS = None\nBATCH_SIZE = 512\nNEMBEDS = 3\nNNSIZE = [64, 16, 4]\n\n# Create an input function reading a file using the Dataset API\n# Then provide the results to the Estimator API\ndef read_dataset(prefix, mode, batch_size):\n    def _input_fn():\n        def decode_csv(value_column):\n            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n            features = dict(zip(CSV_COLUMNS, columns))\n            label = features.pop(LABEL_COLUMN)\n            return features, label\n        \n        # Use prefix to create file path\n        file_path = \'gs://{}/babyweight/preproc/{}*{}*\'.format(BUCKET, prefix, PATTERN)\n\n        # Create list of files that match pattern\n        file_list = tf.gfile.Glob(file_path)\n\n        # Create dataset from file list\n        dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n                    .map(decode_csv))  # Transform each elem by applying decode_csv fn\n      \n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None # indefinitely\n            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n        else:\n            num_epochs = 1 # end-of-input after this\n \n        dataset = dataset.repeat(num_epochs).batch(batch_size)\n        return dataset.make_one_shot_iterator().get_next()\n    return _input_fn\n\n# Define feature columns\ndef get_wide_deep():\n    # Define column types\n    is_male,mother_age,plurality,gestation_weeks = \\\n        [\\\n            tf.feature_column.categorical_column_with_vocabulary_list(\'is_male\', \n                        [\'True\', \'False\', \'Unknown\']),\n            tf.feature_column.numeric_column(\'mother_age\'),\n            tf.feature_column.categorical_column_with_vocabulary_list(\'plurality\',\n                        [\'Single(1)\', \'Twins(2)\', \'Triplets(3)\',\n                         \'Quadruplets(4)\', \'Quintuplets(5)\',\'Multiple(2+)\']),\n            tf.feature_column.numeric_column(\'gestation_weeks\')\n        ]\n\n    # Discretize\n    age_buckets = tf.feature_column.bucketized_column(mother_age, \n                        boundaries=np.arange(15,45,1).tolist())\n    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n                        boundaries=np.arange(17,47,1).tolist())\n      \n    # Sparse columns are wide, have a linear relationship with the output\n    wide = [is_male,\n            plurality,\n            age_buckets,\n            gestation_buckets]\n    \n    # Feature cross all the wide columns and embed into a lower dimension\n    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n    embed = tf.feature_column.embedding_column(crossed, NEMBEDS)\n    \n    # Continuous columns are deep, have a complex relationship with the output\n    deep = [mother_age,\n            gestation_weeks,\n            embed]\n    return wide, deep\n\n# Create serving input function to be able to serve predictions later using provided inputs\ndef serving_input_fn():\n    feature_placeholders = {\n        \'is_male\': tf.placeholder(tf.string, [None]),\n        \'mother_age\': tf.placeholder(tf.float32, [None]),\n        \'plurality\': tf.placeholder(tf.string, [None]),\n        \'gestation_weeks\': tf.placeholder(tf.float32, [None]),\n        KEY_COLUMN: tf.placeholder_with_default(tf.constant([\'nokey\']), [None])\n    }\n    features = {\n        key: tf.expand_dims(tensor, -1)\n        for key, tensor in feature_placeholders.items()\n    }\n    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n\n# create metric for hyperparameter tuning\ndef my_rmse(labels, predictions):\n    pred_values = predictions[\'predictions\']\n    return {\'rmse\': tf.metrics.root_mean_squared_error(labels, pred_values)}\n\n# forward to key-column to export\ndef forward_key_to_export(estimator):\n    estimator = tf.contrib.estimator.forward_features(estimator, KEY_COLUMN)\n    # return estimator\n\n    ## This shouldn\'t be necessary (I\'ve filed CL/187793590 to update extenders.py with this code)\n    config = estimator.config\n    def model_fn2(features, labels, mode):\n      estimatorSpec = estimator._call_model_fn(features, labels, mode, config=config)\n      if estimatorSpec.export_outputs:\n        for ekey in [\'predict\', \'serving_default\']:\n          if (ekey in estimatorSpec.export_outputs and\n              isinstance(estimatorSpec.export_outputs[ekey],\n                         tf.estimator.export.PredictOutput)):\n               estimatorSpec.export_outputs[ekey] = \\\n                 tf.estimator.export.PredictOutput(estimatorSpec.predictions)\n      return estimatorSpec\n    return tf.estimator.Estimator(model_fn=model_fn2, config=config)\n    ##\n\n# Create estimator to train and evaluate\ndef train_and_evaluate(output_dir):\n    wide, deep = get_wide_deep()\n    EVAL_INTERVAL = 300 # seconds\n    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n                                        keep_checkpoint_max = 3)\n    estimator = tf.estimator.DNNLinearCombinedRegressor(\n        model_dir = output_dir,\n        linear_feature_columns = wide,\n        dnn_feature_columns = deep,\n        dnn_hidden_units = NNSIZE,\n        config = run_config)\n    \n    estimator = tf.contrib.estimator.add_metrics(estimator, my_rmse)\n    estimator = forward_key_to_export(estimator)\n\n    train_spec = tf.estimator.TrainSpec(\n        input_fn = read_dataset(\'train\', tf.estimator.ModeKeys.TRAIN, BATCH_SIZE),\n        max_steps = TRAIN_STEPS)\n    exporter = tf.estimator.LatestExporter(\'exporter\', serving_input_fn, exports_to_keep=None)\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn = read_dataset(\'eval\', tf.estimator.ModeKeys.EVAL, 2**15),  # no need to batch in eval\n        steps = EVAL_STEPS,\n        start_delay_secs = 60, # start evaluating after N seconds\n        throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n        exporters = exporter)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n'"
00_Miscellaneous/model_evaluation_pipeline/trainer/task.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Example implementation of code to run on the Cloud ML service.\n""""""\n\nimport argparse\nimport json\nimport os\n\nimport model\n\nimport tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn import learn_runner\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--bucket\',\n        help = \'GCS path to data. We assume that data is in gs://BUCKET/babyweight/preproc/\',\n        required = True\n    )\n    parser.add_argument(\n        \'--output_dir\',\n        help = \'GCS location to write checkpoints and export models\',\n        required = True\n    )\n    parser.add_argument(\n        \'--train_examples\',\n        help = \'Number of examples (in thousands) to run the training job over. If this is more than actual # of examples available, it cycles through them. So specifying 1000 here when you have only 100k examples makes this 10 epochs.\',\n        type = int,\n        default = 5000\n    )\n    parser.add_argument(\n        \'--batch_size\',\n        help = \'Number of examples to compute gradient over.\',\n        type = int,\n        default = 512\n    )\n    parser.add_argument(\n        \'--nembeds\',\n        help = \'Embedding size of a cross of n key real-valued parameters\',\n        type = int,\n        default = 3\n    )\n    parser.add_argument(\n        \'--nnsize\',\n        help = \'Hidden layer sizes to use for DNN feature columns -- provide space-separated layers\',\n        nargs = \'+\',\n        type = int,\n        default=[128, 32, 4]\n    )\n    parser.add_argument(\n        \'--pattern\',\n        help = \'Specify a pattern that has to be in input files. For example 00001-of will process only one shard\',\n        default = \'of\'\n    )\n    parser.add_argument(\n        \'--job-dir\',\n        help = \'this model ignores this field, but it is required by gcloud\',\n        default = \'junk\'\n    )\n    parser.add_argument(\n        \'--eval_steps\',\n        help = \'Positive number of steps for which to evaluate model. Default to None, which means to evaluate until input_fn raises an end-of-input exception\',\n        type = int,       \n        default = None\n    )\n\n    args = parser.parse_args()\n    arguments = args.__dict__\n\n    # unused args provided by service\n    arguments.pop(\'job_dir\', None)\n    arguments.pop(\'job-dir\', None)\n\n    output_dir = arguments.pop(\'output_dir\')\n    model.BUCKET     = arguments.pop(\'bucket\')\n    model.BATCH_SIZE = arguments.pop(\'batch_size\')\n    model.TRAIN_STEPS = (arguments.pop(\'train_examples\') * 1000) / model.BATCH_SIZE\n    model.EVAL_STEPS = arguments.pop(\'eval_steps\')    \n    print (""Will train for {} steps using batch_size={}"".format(model.TRAIN_STEPS, model.BATCH_SIZE))\n    model.PATTERN = arguments.pop(\'pattern\')\n    model.NEMBEDS= arguments.pop(\'nembeds\')\n    model.NNSIZE = arguments.pop(\'nnsize\')\n    print (""Will use DNN size of {}"".format(model.NNSIZE))\n\n    # Append trial_id to path if we are doing hptuning\n    # This code can be removed if you are not using hyperparameter tuning\n    output_dir = os.path.join(\n        output_dir,\n        json.loads(\n            os.environ.get(\'TF_CONFIG\', \'{}\')\n        ).get(\'task\', {}).get(\'trial\', \'\')\n    )\n\n    # Run the training job\n    model.train_and_evaluate(output_dir)\n'"
00_Miscellaneous/text-similarity-analysis/etl/run_pipeline.py,1,"b'#!/usr/bin/python\n#\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport argparse\nimport datetime\nimport logging\n\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.options.pipeline_options import SetupOptions\nfrom apache_beam.options.pipeline_options import GoogleCloudOptions\n\nfrom modules import pipeline\n\n\ndef add_input_options(parser):\n  input_group = parser.add_argument_group(\'input options\')\n  input_group.add_argument(\'--file_pattern\', dest=\'file_pattern\',\n                           default=\'./data/*.sgm\',\n                           help=\'A directory location of input data\')\n\ndef add_transform_options(parser):\n  transform_group = parser.add_argument_group(\'transform options\')\n  transform_group.add_argument(\'--transform_temp_dir\',\n                               dest=\'transform_temp_dir\', default=\'tft_temp\',\n                               help=\'A temp directory used by tf.transform.\')\n  transform_group.add_argument(\'--transform_export_dir\',\n                               dest=\'transform_export_dir\', default=\'tft_out\',\n                               help=\'A directory where tft function is saved\')\n\ndef add_bigquery_options(parser):\n  bigquery_group = parser.add_argument_group(\'bigquery options\')\n  bigquery_group.add_argument(\'--bq_project\',\n                              dest=\'bq_project\', required=True,\n                              help=\'Project name used in BigQuery.\')\n  bigquery_group.add_argument(\'--bq_dataset\',\n                              dest=\'bq_dataset\', required=True,\n                              help=\'Dataset name used in BigQuery.\')\n  bigquery_group.add_argument(\'--bq_table\',\n                              dest=\'bq_table\', required=True,\n                              help=\'Table name used in BigQuery.\')\n\ndef add_tfrecord_options(parser):\n  tfrecord_group = parser.add_argument_group(\'tfrecord options\')\n  tfrecord_group.set_defaults(enable_tfrecord=False)\n  tfrecord_group.add_argument(\'--enable_tfrecord\',\n                              dest=\'enable_tfrecord\', action=\'store_true\',\n                              help=\'Export transformed data in tfrecords format.\')\n  tfrecord_group.add_argument(\'--tfrecord_export_dir\',\n                              dest=\'tfrecord_export_dir\', default=\'./export\',\n                              help=\'A directory where you save transform function\')\n\ndef add_debug_options(parser):\n  debug_group = parser.add_argument_group(\'debug options\')\n  debug_group.set_defaults(enable_debug=False)\n  debug_group.add_argument(\'--enable_debug\',\n                           dest=\'enable_debug\', action=\'store_true\',\n                           help=\'Enable debug options.\')\n  debug_group.add_argument(\'--debug_output_prefix\',\n                           dest=\'debug_output_prefix\', default=\'debug-output\',\n                           help=\'Specify prefix of debug output.\')\ndef main(argv=None):\n  parser = argparse.ArgumentParser()\n  add_input_options(parser)\n  add_transform_options(parser)\n  add_bigquery_options(parser)\n  add_tfrecord_options(parser)\n  add_debug_options(parser)\n  known_args, pipeline_args = parser.parse_known_args(argv)\n  pipeline_options = PipelineOptions(pipeline_args)\n  setup_options = pipeline_options.view_as(SetupOptions)\n  setup_options.save_main_session = True\n  pipeline.run(pipeline_options, known_args)\n\nif __name__ == \'__main__\':\n  logging.getLogger().setLevel(logging.ERROR)\n  main()\n'"
00_Miscellaneous/text-similarity-analysis/etl/setup.py,0,"b'#!/usr/bin/python\n#\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport setuptools\n\nREQUIRED_PACKAGES = [\n    \'tensorflow==1.7.0\',\n    \'tensorflow-hub==0.1.0\',\n    \'tensorflow-transform==0.6.0\',\n    \'bs4==0.0.1\',\n    \'nltk==3.3\']\n\nsetuptools.setup(\n    name=\'etl_pipeline\',\n    version=\'0.0.1\',\n    author=\'anonymous\',\n    author_email=\'anonymous@google.com\',\n    install_requires=REQUIRED_PACKAGES,\n    packages=setuptools.find_packages())\n'"
07_Image_Analysis/trainer/cnn-model-02/__init__.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
07_Image_Analysis/trainer/cnn-model-02/sample_model.py,54,"b'#!/usr/bin/env python\n\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport tensorflow as tf\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nIMAGE_HEIGHT = 32\nIMAGE_WIDTH = 32\nIMAGE_DEPTH = 3\nNUM_CLASSES = 10\n\n\ndef parse_record(serialized_example):\n  """"""Parsing CIFAR-10 dataset that is saved in TFRecord format.""""""\n  features = tf.parse_single_example(\n    serialized_example,\n    features={\n      \'image\': tf.FixedLenFeature([], tf.string),\n      \'label\': tf.FixedLenFeature([], tf.int64),\n    })\n\n  image = tf.decode_raw(features[\'image\'], tf.uint8)\n  image.set_shape([IMAGE_DEPTH * IMAGE_HEIGHT * IMAGE_WIDTH])\n  image = tf.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n  image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n\n  label = tf.cast(features[\'label\'], tf.int32)\n  label = tf.one_hot(label, NUM_CLASSES)\n\n  return image, label\n\n\ndef preprocess_image(image, is_training=False):\n  """"""Preprocess a single image of layout [height, width, depth].""""""\n  if is_training:\n    # Resize the image to add four extra pixels on each side.\n    image = tf.image.resize_image_with_crop_or_pad(\n        image, IMAGE_HEIGHT + 8, IMAGE_WIDTH + 8)\n\n    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n    image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n\n    # Randomly flip the image horizontally.\n    image = tf.image.random_flip_left_right(image)\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  image = tf.image.per_image_standardization(image)\n  return image\n\n\ndef generate_input_fn(filenames, mode=tf.estimator.ModeKeys.EVAL, batch_size=1):\n  """"""Input function for Estimator API.""""""\n  def _input_fn():\n    dataset = tf.data.TFRecordDataset(filenames=filenames)\n\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    if is_training:\n      buffer_size = batch_size * 2 + 1\n      dataset = dataset.shuffle(buffer_size=buffer_size)\n\n    dataset = dataset.map(parse_record)\n    dataset = dataset.map(\n      lambda image, label: (preprocess_image(image, is_training), label))\n\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(2 * batch_size)\n\n    images, labels = dataset.make_one_shot_iterator().get_next()\n\n    features = {\'images\': images}\n    return features, labels\n\n  return _input_fn\n\n\ndef get_feature_columns():\n  """"""Define feature columns.""""""\n  feature_columns = {\n    \'images\': tf.feature_column.numeric_column(\n        \'images\', (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)),\n  }\n  return feature_columns\n\n\ndef serving_input_fn():\n  """"""Define serving function.""""""\n  receiver_tensor = {\n      \'images\': tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)\n  }\n  features = {\n      \'images\': tf.map_fn(preprocess_image, receiver_tensor[\'images\'])\n  }\n  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)\n\n\ndef inference(images):\n  # 1st Convolutional Layer\n  conv1 = tf.layers.conv2d(\n      inputs=images, filters=64, kernel_size=[5, 5], padding=\'same\',\n      activation=tf.nn.relu, name=\'conv1\')\n  pool1 = tf.layers.max_pooling2d(\n      inputs=conv1, pool_size=[3, 3], strides=2, name=\'pool1\')\n  norm1 = tf.nn.lrn(\n      pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=\'norm1\')\n\n  # 2nd Convolutional Layer\n  conv2 = tf.layers.conv2d(\n      inputs=norm1, filters=64, kernel_size=[5, 5], padding=\'same\',\n      activation=tf.nn.relu, name=\'conv2\')\n  norm2 = tf.nn.lrn(\n      conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=\'norm2\')\n  pool2 = tf.layers.max_pooling2d(\n      inputs=norm2, pool_size=[3, 3], strides=2, name=\'pool2\')\n\n  # Flatten Layer\n  shape = pool2.get_shape()\n  pool2_ = tf.reshape(pool2, [-1, shape[1]*shape[2]*shape[3]])\n\n  # 1st Fully Connected Layer\n  dense1 = tf.layers.dense(\n      inputs=pool2_, units=384, activation=tf.nn.relu, name=\'dense1\')\n\n  # 2nd Fully Connected Layer\n  dense2 = tf.layers.dense(\n      inputs=dense1, units=192, activation=tf.nn.relu, name=\'dense2\')\n\n  # 3rd Fully Connected Layer (Logits)\n  logits = tf.layers.dense(\n      inputs=dense2, units=NUM_CLASSES, activation=tf.nn.relu, name=\'logits\')\n\n  return logits\n\n\ndef model_fn(features, labels, mode, params):\n  # Create the input layers from the features\n  feature_columns = list(get_feature_columns().values())\n\n  images = tf.feature_column.input_layer(\n    features=features, feature_columns=feature_columns)\n\n  images = tf.reshape(\n    images, shape=(-1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n\n  # Calculate logits through CNN\n  logits = inference(images)\n\n  if mode in (tf.estimator.ModeKeys.PREDICT, tf.estimator.ModeKeys.EVAL):\n    predicted_indices = tf.argmax(input=logits, axis=1)\n    probabilities = tf.nn.softmax(logits, name=\'softmax_tensor\')\n\n  if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n    global_step = tf.train.get_or_create_global_step()\n    label_indices = tf.argmax(input=labels, axis=1)\n    loss = tf.losses.softmax_cross_entropy(\n        onehot_labels=labels, logits=logits)\n    tf.summary.scalar(\'cross_entropy\', loss)\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    predictions = {\n        \'classes\': predicted_indices,\n        \'probabilities\': probabilities\n    }\n    export_outputs = {\n        \'predictions\': tf.estimator.export.PredictOutput(predictions)\n    }\n    return tf.estimator.EstimatorSpec(\n        mode, predictions=predictions, export_outputs=export_outputs)\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n    return tf.estimator.EstimatorSpec(\n        mode, loss=loss, train_op=train_op)\n\n  if mode == tf.estimator.ModeKeys.EVAL:\n    eval_metric_ops = {\n        \'accuracy\': tf.metrics.accuracy(label_indices, predicted_indices)\n    }\n    return tf.estimator.EstimatorSpec(\n        mode, loss=loss, eval_metric_ops=eval_metric_ops)\n'"
07_Image_Analysis/trainer/cnn-model-02/task.py,26,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Example implementation of code to run on the Cloud ML service.\n""""""\n\nimport fnmatch\nimport os\nimport tensorflow as tf\n\nfrom gcloud import storage\n\nimport sample_model as sm\n\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'bucket_name\', \'\', \'bucket_name.\')\ntf.app.flags.DEFINE_string(\n    \'output_dir\', \'\', \'GCS location to root directory for checkpoints and exported models.\')\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'cnn-model-02\', \'model name.\')\ntf.app.flags.DEFINE_string(\n    \'export_dir\', \'Servo\', \'model export directory.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 200, \'batch size for training.\')\ntf.app.flags.DEFINE_integer(\n    \'max_steps\', 1000, \'max_step for training.\')\ntf.app.flags.DEFINE_integer(\n    \'eval_steps\', 50, \'The number of steps that are used in evaluation phase.\')\ntf.app.flags.DEFINE_bool(\n    \'use_checkpoint\', True, \'True if use checkpoints.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_steps\', 500, \'\')\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 19851211, \'\')\ntf.app.flags.DEFINE_string(\n    \'train_data_pattern\', \'cifar-10/train*.tfrecord\', \'path to train dataset on GCS.\')\ntf.app.flags.DEFINE_string(\n    \'eval_data_pattern\', \'cifar-10/valid*.tfrecord\', \'path to eval dataset on GCS.\')\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef get_filenames(pattern):\n  storage_client = storage.Client()\n  bucket = storage_client.lookup_bucket(FLAGS.bucket_name)\n\n  prefix = \'/\'.join(pattern.split(\'/\')[:-1])\n  filenames = []\n\n  for blob in bucket.list_blobs(prefix=prefix):\n    if fnmatch.fnmatch(blob.name, pattern):\n      filenames.append(\'gs://\' + os.path.join(blob.bucket.name, blob.name))\n\n  tf.logging.info(filenames)\n  return filenames\n\n\ndef train_and_evaluate():\n  model_dir = os.path.join(FLAGS.output_dir, FLAGS.model_name)\n\n  # Create estimator from Keras model.\n  estimator = tf.estimator.Estimator(\n      model_fn=sm.model_fn,\n      model_dir=model_dir,\n      config=tf.estimator.RunConfig(\n          # save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n          keep_checkpoint_max=5,\n          tf_random_seed=FLAGS.tf_random_seed))\n\n  # Profile Hook.\n  profile_hook = tf.train.ProfilerHook(\n      save_steps=FLAGS.save_checkpoints_steps,\n      output_dir=model_dir,\n      show_dataflow=True,\n      show_memory=True)\n\n  # Specify training data paths, batch size and max steps.\n  train_spec = tf.estimator.TrainSpec(\n      input_fn=sm.generate_input_fn(filenames=get_filenames(FLAGS.train_data_pattern),\n                                    mode=tf.estimator.ModeKeys.TRAIN,\n                                    batch_size=FLAGS.batch_size),\n      max_steps=FLAGS.max_steps,\n      hooks=[profile_hook]\n  )\n\n  # Currently (2017.12.14) the latest tf only support exporter with keras models.\n  exporter = tf.estimator.LatestExporter(\n      name=FLAGS.export_dir, serving_input_receiver_fn=sm.serving_input_fn,\n      assets_extra=None, as_text=False, exports_to_keep=5)\n\n  # Specify validation data paths, steps for evaluation and exporter specs\n  eval_spec = tf.estimator.EvalSpec(\n      input_fn=sm.generate_input_fn(filenames=get_filenames(FLAGS.eval_data_pattern),\n                                    mode=tf.estimator.ModeKeys.EVAL,\n                                    batch_size=FLAGS.batch_size),\n      steps=FLAGS.eval_steps,\n      name=None,\n      hooks=None,\n      exporters=exporter # Iterable of Exporters, or single one or None.\n  )\n\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\ndef main(unused_argv=None):\n  tf.logging.info(tf.__version__)\n  train_and_evaluate()\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
00_Miscellaneous/model_evaluation_pipeline/evaluator/process/__init__.py,0,b''
00_Miscellaneous/model_evaluation_pipeline/evaluator/process/inference.py,2,"b""import os\nimport logging\n\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nSAVED_MODEL_DIR = 'model'\nPROJECT = 'yaboo-sandbox'\n\nexport_dir = None\npredictor_fn = None\n\ndef init_predictor(saved_model_dir):\n  global export_dir\n  global predictor_fn\n\n  if predictor_fn is None:\n    print('initializing predictor...')\n    if saved_model_dir == None:\n      dir_path = os.path.dirname(os.path.realpath(__file__))\n      export_dir = os.path.join(dir_path, SAVED_MODEL_DIR)\n    else:\n      export_dir = saved_model_dir\n\n    predictor_fn = tf.contrib.predictor.from_saved_model(\n        export_dir=export_dir, signature_def_key='predict')\n\n  return export_dir, predictor_fn\n\ndef estimate_local(instance, saved_model_dir=None):\n  export_dir, predictor_fn = init_predictor(saved_model_dir)\n  instance = dict((k, [v]) for k, v in instance.items())\n  value = predictor_fn(instance)['predictions'][0][0]\n  return export_dir, value\n"""
00_Miscellaneous/model_evaluation_pipeline/evaluator/process/pipeline.py,2,"b'from __future__ import absolute_import\n\nimport logging\nimport time\n\nimport apache_beam as beam\nimport tensorflow as tf\n\npredictor_fn = None\ntable_schema = None\ntf.logging.set_verbosity(tf.logging.ERROR)\n\n\ndef get_bigquery_query(year_from, year_to, datasize):\n  query = """"""\n  SELECT\n    is_male, mother_age, plurality, gestation_weeks, weight_pounds\n  FROM\n    publicdata.samples.natality\n  WHERE\n    year >= {0} AND year <= {1}\n    AND weight_pounds > 0\n    AND mother_age > 0\n    AND plurality > 0\n    AND gestation_weeks > 0\n    AND month > 0\n  LIMIT {2}\n  """""".format(year_from, year_to, datasize)\n  return query\n\n\ndef process_bigquery_row(bq_row):\n  # modify opaque numeric race code into human-readable data.\n  plurality = dict(zip([1, 2, 3, 4, 5, 6],\n                       [\'Single(1)\', \'Twins(2)\', \'Triplets(3)\',\n                        \'Quadruplets(4)\', \'Quintuplets(5)\', \'Multiple(2+)\']))\n  instance = dict()\n  instance[\'is_male\'] = str(bq_row[\'is_male\'])\n  instance[\'mother_age\'] = bq_row[\'mother_age\']\n  instance[\'plurality\'] = plurality[bq_row[\'plurality\']]\n  instance[\'gestation_weeks\'] = bq_row[\'gestation_weeks\']\n  instance[\'weight_true\'] = float(bq_row[\'weight_pounds\'])\n  return instance\n\n\ndef process_instance(instance, saved_model_dir, year_from, year_to):\n  # pop weight_true since it shouldn\'t be included in an instance.\n  weight_true = instance.pop(\'weight_true\')\n  weight_predicted, time_inference = predict_weight(instance, saved_model_dir)\n\n  # convert value to JSON serializable format.\n  instance[\'weight_true\'] = float(weight_true)\n  instance[\'weight_predicted\'] = float(weight_predicted)\n  instance[\'weight_residual\'] = (\n      instance[\'weight_true\'] - instance[\'weight_predicted\'])\n\n  # put test conditions to each instance.\n  instance[\'model\'] = saved_model_dir\n  instance[\'testdata\'] = \'{0}-{1}\'.format(year_from, year_to)\n  instance[\'time_inference\'] = 1000 * time_inference # sec to msec\n  return instance\n\n\ndef predict_weight(instance, saved_model_dir):\n  predictor_fn = init_predictor(saved_model_dir)\n  instance = dict((k, [v]) for k, v in instance.items())\n\n  tic = time.time()\n  weight_predicted = predictor_fn(instance)[\'predictions\'][0][0]\n  toc = time.time()\n  time_inference = toc - tic\n\n  return weight_predicted, time_inference\n\n\ndef init_predictor(saved_model_dir):\n  import tensorflow as tf\n\n  global predictor_fn\n  # Re-use predictor_fn once it\'s initialized.\n  if predictor_fn is None:\n    logging.info(\'initializing predictor...\')\n    predictor_fn = tf.contrib.predictor.from_saved_model(\n        export_dir=saved_model_dir, signature_def_key=\'predict\')\n  return predictor_fn\n\n\ndef get_output_table_schema():\n  global table_schema\n  if table_schema == None:\n    column_names = [\'is_male\', \'mother_age\', \'plurality\', \'gestation_weeks\',\n                    \'weight_true\', \'weight_predicted\', \'weight_residual\',\n                    \'model\', \'testdata\', \'time_inference\']\n    column_types = {\n        \'model\': \'STRING\',\n        \'testdata\': \'STRING\',\n        \'is_male\': \'STRING\',\n        \'mother_age\': \'INTEGER\',\n        \'plurality\': \'STRING\',\n        \'gestation_weeks\': \'INTEGER\',\n        \'weight_true\': \'FLOAT\',\n        \'weight_predicted\': \'FLOAT\',\n        \'weight_residual\': \'FLOAT\',\n        \'time_inference\': \'FLOAT\',\n    }\n    table_schema = \',\'.join(\n        [\'{0}:{1}\'.format(k, column_types[k]) for k in column_names])\n  return table_schema\n\ndef run(pipeline_options, saved_model_dir, year_from, year_to, datasize,\n        output_table):\n  pipeline = beam.Pipeline(options=pipeline_options)\n\n  (\n      pipeline\n      | \'Extract data\' >> beam.io.Read(beam.io.BigQuerySource(\n          query=get_bigquery_query(year_from, year_to, datasize),\n          use_standard_sql=True))\n      | \'Process rows\' >> beam.Map(process_bigquery_row)\n      | \'Predict weight\' >> beam.Map(lambda instance: process_instance(\n          instance, saved_model_dir, year_from, year_to))\n      | \'Write to table\' >> beam.io.Write(\n          beam.io.BigQuerySink(\n              output_table,\n              schema=get_output_table_schema(),\n              create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n              write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n  )\n\n  job = pipeline.run()\n\n  if pipeline_options.get_all_options()[\'runner\'] == \'DirectRunner\':\n    job.wait_until_finish()\n'"
00_Miscellaneous/text-similarity-analysis/etl/modules/__init__.py,0,"b'#!/usr/bin/python\n# \n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n'"
00_Miscellaneous/text-similarity-analysis/etl/modules/pipeline.py,10,"b'#!/usr/bin/python\n# \n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport apache_beam as beam\nimport tensorflow as tf\nimport tensorflow_transform as tft\nimport tensorflow_transform.coders as tft_coders\n\nfrom tensorflow_transform.beam import impl\nfrom tensorflow_transform.beam.tft_beam_io import transform_fn_io\nfrom tensorflow_transform.tf_metadata import dataset_schema\nfrom tensorflow_transform.tf_metadata import dataset_metadata\n\n\n###########################################################\n# Preprocess Reuter Dataset\n###########################################################\n\ndef get_paths(file_pattern, test_mode=False):\n  """"""\n  glob_pattern = \'./data/*.sgm\'\n  """"""\n  import tensorflow as tf\n\n  paths = tf.gfile.Glob(file_pattern)\n  if test_mode:\n    paths = paths[:1]\n\n  return paths\n\ndef get_articles(file_path):\n  """"""\n  file_path = \'./data/reut2-000.sgm\'\n  """"""\n  import bs4\n  import tensorflow as tf\n\n  data = tf.gfile.GFile(file_path).read()\n  soup = bs4.BeautifulSoup(data, ""html.parser"")\n  articles = []\n  for raw_article in soup.find_all(\'reuters\'):\n    article = {\n        \'title\': get_title(raw_article),\n        \'content\': get_content(raw_article),\n        \'topics\': get_topics(raw_article),\n    }\n    if None not in article.values():\n      if [] not in article.values():\n        articles.append(article)\n  return articles\n\ndef get_title(article):\n  title = article.find(\'text\').title\n  if title != None:\n    title = title.text.encode(\'ascii\', \'ignore\')\n  return title\n\ndef get_content(article):\n  import nltk\n  content = article.find(\'text\').body\n  if content != None:\n    content = content.text.encode(\'ascii\', \'ignore\')\n    content = content.replace(\'\\n Reuter\\n\\x03\', \'\')\n    content = content.replace(\'\\n\', \' \')\n    try:\n      content = \'\\n\'.join(nltk.sent_tokenize(content))\n    except LookupError:\n      nltk.download(\'punkt\')\n      content = \'\\n\'.join(nltk.sent_tokenize(content))\n  return content\n\ndef get_topics(article):\n  topics = []\n  for topic in article.topics.children:\n    topics.append(topic.text.encode(\'ascii\', \'ignore\'))\n  if len(topics) > 0:\n    return \',\'.join(topics)\n  else:\n    return \'\'\n\n###########################################################\n# TensorFlow Transform\n###########################################################\n\ndef get_metadata():\n  from tensorflow_transform.tf_metadata import dataset_schema\n  from tensorflow_transform.tf_metadata import dataset_metadata\n\n  metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema({\n    \'title\': dataset_schema.ColumnSchema(\n        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n    \'content\': dataset_schema.ColumnSchema(\n        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n    \'topics\': dataset_schema.ColumnSchema(\n        tf.string, [], dataset_schema.FixedColumnRepresentation()),\n  }))\n  return metadata\n\ndef preprocess_fn(input_features):\n  import tensorflow_transform as tft\n\n  title_embed = tft.apply_function(get_embed_content, input_features[\'content\'])\n  content_embed = tft.apply_function(get_embed_title, input_features[\'title\'])\n  output_features = {\n      \'topics\': input_features[\'topics\'],\n      \'title\': input_features[\'title\'],\n      \'content\': input_features[\'content\'],\n      \'title_embed\': title_embed,\n      \'content_embed\': content_embed,\n  }\n  return output_features\n\ndef get_embed_title(\n    title,\n    module_url=\'https://tfhub.dev/google/universal-sentence-encoder/1\'):\n  import tensorflow as tf\n  import tensorflow_hub as hub\n\n  module = hub.Module(module_url)\n  embed = module(title)\n  return embed\n\ndef get_embed_content(\n    content, delimiter=\'\\n\',\n    module_url=\'https://tfhub.dev/google/universal-sentence-encoder/1\'):\n  import tensorflow as tf\n  import tensorflow_hub as hub\n\n  module = hub.Module(module_url)\n\n  def _map_fn(t):\n    t = tf.cast(t, tf.string)\n    t = tf.string_split([t], delimiter).values\n    e = module(t)\n    e = tf.reduce_mean(e, axis=0)\n    return tf.squeeze(e)\n\n  embed = tf.map_fn(_map_fn, content, dtype=tf.float32)\n  return embed\n\n###########################################################\n# Write data to files or bq table\n###########################################################\n\ndef to_bq_row(entry):\n  # might not need to round...\n  entry[\'title_embed\'] = [round(float(e), 3) for e in entry[\'title_embed\']]\n  entry[\'content_embed\'] = [round(float(e), 3) for e in entry[\'content_embed\']]\n  return entry\n\ndef get_bigquery_schema():\n  """"""\n  Returns a bigquery schema.\n  """"""\n  from apache_beam.io.gcp.internal.clients import bigquery\n\n  table_schema = bigquery.TableSchema()\n  columns = ((\'topics\', \'string\', \'nullable\'),\n             (\'title\', \'string\', \'nullable\'),\n             (\'content\', \'string\', \'nullable\'),\n             (\'title_embed\', \'float\', \'repeated\'),\n             (\'content_embed\', \'float\', \'repeated\'))\n\n  for column in columns:\n    column_schema = bigquery.TableFieldSchema()\n    column_schema.name = column[0]\n    column_schema.type = column[1]\n    column_schema.mode = column[2]\n    table_schema.fields.append(column_schema)\n\n  return table_schema\n\n###########################################################\n# Dataflow Pipeline\n###########################################################\n\ndef run(pipeline_options, known_args):\n  pipeline = beam.Pipeline(options=pipeline_options)\n\n  with impl.Context(known_args.transform_temp_dir):\n    articles = (\n        pipeline\n        | \'Get Paths\' >> beam.Create(get_paths(known_args.file_pattern))\n        | \'Get Articles\' >> beam.Map(get_articles)\n        | \'Get Article\' >> beam.FlatMap(lambda x: x)\n    )\n\n    dataset = (articles, get_metadata())\n\n    transform_fn = (\n        dataset\n        | \'Analyse dataset\' >> impl.AnalyzeDataset(preprocess_fn)\n    )\n\n    transformed_data_with_meta = (\n        (dataset, transform_fn)\n        | \'Transform dataset\' >> impl.TransformDataset()\n    )\n\n    transformed_data, transformed_metadata = transformed_data_with_meta\n\n    transform_fn | \'Export Transform Fn\' >> transform_fn_io.WriteTransformFn(\n        known_args.transform_export_dir)\n\n    (\n        transformed_data\n        | \'Convert to Insertable data\' >> beam.Map(to_bq_row)\n        | \'Write to BigQuery table\' >> beam.io.WriteToBigQuery(\n            project=known_args.bq_project,\n            dataset=known_args.bq_dataset,\n            table=known_args.bq_table,\n            schema=get_bigquery_schema(),\n            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n    )\n\n    if known_args.enable_tfrecord:\n      transformed_data | \'Write TFRecords\' >> beam.io.tfrecordio.WriteToTFRecord(\n          file_path_prefix=\'{0}/{1}\'.format(known_args.tfrecord_export_dir, \'reuter\'),\n          file_name_suffix=\'.tfrecords\',\n          coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n\n    if known_args.enable_debug:\n      transformed_data | \'Debug Output\' >> beam.io.textio.WriteToText(\n          file_path_prefix=known_args.debug_output_prefix, file_name_suffix=\'.txt\')\n\n\n  job = pipeline.run()\n\n  if pipeline_options.get_all_options()[\'runner\'] == \'DirectRunner\':\n    job.wait_until_finish()\n'"
00_Miscellaneous/text-similarity-analysis/etl/modules/test_pipeline.py,0,"b'#!/usr/bin/python\n# \n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pipeline import get_articles\n\nclass TestPipeline(unittest.TestCase):\n  def test_get_content(self):\n    answer = """"""Total food aid needs in 69 of the least developed countries declined in 1986/87, as requirments fell in many countries in Africa, the Middle East and Asia, the U.S. Agriculture Department said.\\nIn a summary of its World Agriculture Report, the department said grain production in sub-Saharan Africa was a record high in 1986, with gains in almost every country.\\nHowever, food needs in Central America rose, worsened by drought-reduced crops and civil strife.\\nRecord wheat production in 1986/87 is pushing global wheat consumption for food to a new high, and higher yielding varieties have been particularly effective where spring wheat is a common crop, it said.\\nHowever, may developing countries in tropical climates, such as Sub-Saharan Africa, Southeast Asia, and Central America, are not well adapted for wheat production, and improved varieties are not the answer to rising food needs, the department said.\\nWorld per capita consumption of vegetable oil will rise in 1986/87 for the third straight year.\\nSoybean oil constitutes almost 30 pct of vegetable oil consumption, while palm oil is the most traded, the department said."""""" \n    response = get_articles(\'test_data/test.sgm\')[0][\'content\']\n    assert(answer == response)\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
Experimental/distribution/multi-node/mnist-keras-1/dataprep.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.flags.DEFINE_string(\n    ""output_dir"", default=None,\n    help=""The output directory to use for storing input data for TPU ""\n    ""training job."")\n\nFLAGS = tf.flags.FLAGS\n\ndef _bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef create_example(image, label):\n  feature={\n      ""image"": _bytes_feature(image.tobytes()),\n      ""label"": _bytes_feature(label.tobytes())}\n  return tf.train.Example(features=tf.train.Features(feature=feature))\n\ndef convert_to_tfrecord(images, labels, output_file):\n  with tf.python_io.TFRecordWriter(output_file) as record_writer:\n    for image, label in zip(images, labels):\n      example = create_example(image, label)\n      record_writer.write(example.SerializeToString())\n\ndef main(argv):\n  non_test, test = tf.keras.datasets.mnist.load_data()\n  X_train, y_train = non_test[0][:-5000], non_test[1][:-5000]\n  X_eval, y_eval = non_test[0][-5000:], non_test[1][:-5000]\n  X_test, y_test = test[0], test[1]\n\n  convert_to_tfrecord(X_train, y_train,\n                      ""{}/train.tfrecord"".format(FLAGS.output_dir))\n  convert_to_tfrecord(X_eval, y_eval,\n                      ""{}/eval.tfrecord"".format(FLAGS.output_dir))\n  convert_to_tfrecord(X_test, y_test,\n                      ""{}/test.tfrecord"".format(FLAGS.output_dir))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n\n'"
Experimental/distribution/multi-gpu/cmle/project/__init__.py,0,b''
Experimental/distribution/multi-gpu/cmle/project/setup.py,0,"b""from setuptools import find_packages\nfrom setuptools import setup\n\nREQUIRED_PACKAGES = []\n\nsetup(\n    name='trainer',\n    version='0.1',\n    install_requires=REQUIRED_PACKAGES,\n    packages=find_packages(),\n    include_package_data=True,\n    description='Generic example trainer package.',\n)\n"""
Experimental/distribution/multi-node/mnist-keras-1/trainer/__init__.py,0,b''
Experimental/distribution/multi-node/mnist-keras-1/trainer/main.py,39,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\n# Model specific parameters\ntf.flags.DEFINE_string(\n    ""data_dir"", default=""gs://your-bucket-name/keras-mnist/data"",\n    help=""Path to directory containing the MNIST dataset"")\ntf.flags.DEFINE_string(\n    ""model_dir"", default=""gs://your-bucket-name/keras-mnist/keras-model"",\n    help=""Estimator model_dir"")\ntf.flags.DEFINE_integer(\n    ""batch_size"", default=100,\n    help=""Mini-batch size for the training. Note that this is the global batch ""\n    ""size and not the per-shard batch."")\ntf.flags.DEFINE_float(\n    ""learning_rate"", default=0.005,\n    help=""Learning rate used to optimize your model."")\ntf.flags.DEFINE_integer(\n    ""train_steps"", default=1000,\n    help=""Total number of training steps."")\ntf.flags.DEFINE_string(\n    ""export_dir"", default=""gs://your-bucket-name/keras-mnist/keras-model/export"",\n    help=""The directory where the exported SavedModel will be stored."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef get_estimator():\n  model = tf.keras.Sequential()\n  model.add(tf.keras.layers.InputLayer(input_shape=[28*28]))\n  model.add(tf.keras.layers.Dense(300, activation=\'relu\'))\n  model.add(tf.keras.layers.Dense(100, activation=\'relu\'))\n  model.add(tf.keras.layers.Dense(10, activation=\'softmax\'))\n  model.compile(loss=\'categorical_crossentropy\',\n                optimizer=tf.keras.optimizers.SGD(lr=0.005),\n                metrics=[\'accuracy\'])\n  estimator = tf.keras.estimator.model_to_estimator(\n    model, model_dir=FLAGS.model_dir)\n\n  input_signature = model.input.name.split(\':\')[0]\n  return estimator, input_signature\n\ndef get_serving_input_fn(input_signature):\n  def preprocess(x):\n    return tf.reshape(x, [-1, 28*28]) / 255.0\n\n  def serving_input_fn():\n    receiver_tensor = {\'X\': tf.placeholder(tf.float32, shape=[None, 28, 28])}\n    features = {input_signature: tf.map_fn(preprocess, receiver_tensor[\'X\'])}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)\n  return serving_input_fn\n\ndef generate_input_fn(file_pattern, mode, batch_size, count=None):\n  def parse_record(serialized_example):\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            \'image\': tf.FixedLenFeature([], tf.string),\n            \'label\': tf.FixedLenFeature([], tf.string),            \n        })\n    # Normalize from [0, 255] to [0.0, 1.0]\n    image = tf.decode_raw(features[\'image\'], tf.uint8)\n    image = tf.cast(image, tf.float32)\n    image = tf.reshape(image, [28*28]) / 255.0\n    label = tf.decode_raw(features[\'label\'], tf.uint8)\n    label = tf.reshape(label, [])\n    label = tf.one_hot(label, 10, dtype=tf.int32)\n    return image, label\n\n  def input_fn():\n    files = tf.data.Dataset.list_files(file_pattern)\n    dataset = tf.data.TFRecordDataset(files)\n    \n    if mode == tf.estimator.ModeKeys.TRAIN:\n      dataset = dataset.cache()\n      dataset = dataset.shuffle(10000)\n      dataset = dataset.repeat(count=count)\n      \n    dataset = dataset.map(parse_record)\n    dataset = dataset.batch(batch_size)\n    \n    iterator = dataset.make_one_shot_iterator()\n    features, labels = iterator.get_next()\n\n    return features, labels\n  \n  return input_fn\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  estimator, input_signature = get_estimator()\n  train_input_fn = generate_input_fn(\n    file_pattern=\'{}/train.tfrecord\'.format(FLAGS.data_dir),\n    mode=tf.estimator.ModeKeys.TRAIN,\n    batch_size=FLAGS.batch_size, count=None)\n\n  eval_input_fn = generate_input_fn(\n    file_pattern=\'{}/eval.tfrecord\'.format(FLAGS.data_dir),\n    mode=tf.estimator.ModeKeys.EVAL,\n    batch_size=FLAGS.batch_size, count=None)\n\n  test_input_fn = generate_input_fn(\n    file_pattern=\'{}/test.tfrecord\'.format(FLAGS.data_dir),\n    mode=tf.estimator.ModeKeys.PREDICT,\n    batch_size=FLAGS.batch_size, count=None)\n\n  train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=FLAGS.train_steps)\n\n  exporter = tf.estimator.LatestExporter(\n    name=\'export\',\n    serving_input_receiver_fn=get_serving_input_fn(input_signature))\n\n  eval_spec = tf.estimator.EvalSpec(\n    input_fn=eval_input_fn,\n    steps=None,\n    start_delay_secs=60,\n    throttle_secs=60,\n    exporters=exporter)\n\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
Experimental/distribution/multi-node/mnist-keras-2/trainer/__init__.py,0,b''
Experimental/distribution/multi-node/mnist-keras-2/trainer/main.py,38,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\n# Model specific parameters\ntf.flags.DEFINE_string(\n    ""data_dir"", default=""gs://your-bucket-name/keras-mnist/data"",\n    help=""Path to directory containing the MNIST dataset"")\ntf.flags.DEFINE_string(\n    ""model_dir"", default=""gs://your-bucket-name/keras-mnist/keras-model"",\n    help=""Estimator model_dir"")\ntf.flags.DEFINE_integer(\n    ""batch_size"", default=100,\n    help=""Mini-batch size for the training. Note that this is the global batch ""\n    ""size and not the per-shard batch."")\ntf.flags.DEFINE_float(\n    ""learning_rate"", default=0.005,\n    help=""Learning rate used to optimize your model."")\ntf.flags.DEFINE_integer(\n    ""train_steps"", default=1000,\n    help=""Total number of training steps."")\ntf.flags.DEFINE_string(\n    ""export_dir"", default=""gs://your-bucket-name/keras-mnist/keras-model/export"",\n    help=""The directory where the exported SavedModel will be stored."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef get_estimator():\n  model = tf.keras.Sequential()\n  model.add(tf.keras.layers.Dense(300, activation=\'relu\', input_shape=[28*28]))\n  model.add(tf.keras.layers.Dense(100, activation=\'relu\'))  \n  model.add(tf.keras.layers.Dense(10, activation=\'softmax\'))\n  model.compile(loss=\'categorical_crossentropy\',\n                optimizer=tf.keras.optimizers.SGD(lr=0.005),\n                metrics=[\'accuracy\'])  \n  estimator = tf.keras.estimator.model_to_estimator(\n    model, model_dir=FLAGS.model_dir)\n\n  input_signature = model.input.name.split(\':\')[0]\n  return estimator, input_signature\n\ndef get_serving_input_fn(input_signature):\n  def preprocess(x):\n    return tf.reshape(x, [-1, 28*28]) / 255.0\n\n  def serving_input_fn():\n    receiver_tensor = {\'X\': tf.placeholder(tf.float32, shape=[None, 28, 28])}\n    features = {input_signature: tf.map_fn(preprocess, receiver_tensor[\'X\'])}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)\n  return serving_input_fn\n\ndef generate_input_fn(file_pattern, mode, batch_size, count=None):\n  def parse_record(serialized_example):\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            \'image\': tf.FixedLenFeature([], tf.string),\n            \'label\': tf.FixedLenFeature([], tf.string),            \n        })\n    # Normalize from [0, 255] to [0.0, 1.0]\n    image = tf.decode_raw(features[\'image\'], tf.uint8)\n    image = tf.cast(image, tf.float32)\n    image = tf.reshape(image, [28*28]) / 255.0\n    label = tf.decode_raw(features[\'label\'], tf.uint8)\n    label = tf.reshape(label, [])\n    label = tf.one_hot(label, 10, dtype=tf.int32)\n    return image, label\n\n  def input_fn():\n    files = tf.data.Dataset.list_files(file_pattern)\n    dataset = tf.data.TFRecordDataset(files)\n    \n    if mode == tf.estimator.ModeKeys.TRAIN:\n      dataset = dataset.cache()\n      dataset = dataset.shuffle(1000)\n      dataset = dataset.repeat(count=count)\n      \n    dataset = dataset.map(parse_record)\n    dataset = dataset.batch(batch_size)\n    \n    iterator = dataset.make_one_shot_iterator()\n    features, labels = iterator.get_next()\n\n    return features, labels\n  \n  return input_fn\n\ndef main(unused_argv):\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  estimator, input_signature = get_estimator()\n  train_input_fn = generate_input_fn(\n    file_pattern=\'{}/train.tfrecord\'.format(FLAGS.data_dir),\n    mode=tf.estimator.ModeKeys.TRAIN,\n    batch_size=FLAGS.batch_size, count=None)\n\n  eval_input_fn = generate_input_fn(\n    file_pattern=\'{}/eval.tfrecord\'.format(FLAGS.data_dir),\n    mode=tf.estimator.ModeKeys.EVAL,\n    batch_size=FLAGS.batch_size, count=None)\n\n  test_input_fn = generate_input_fn(\n    file_pattern=\'{}/test.tfrecord\'.format(FLAGS.data_dir),\n    mode=tf.estimator.ModeKeys.PREDICT,\n    batch_size=FLAGS.batch_size, count=None)\n\n  train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=FLAGS.train_steps)\n\n  exporter = tf.estimator.LatestExporter(\n    name=\'export\',\n    serving_input_receiver_fn=get_serving_input_fn(input_signature))\n\n  eval_spec = tf.estimator.EvalSpec(\n    input_fn=eval_input_fn,\n    steps=None,\n    start_delay_secs=60,\n    throttle_secs=60,\n    exporters=exporter)\n\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
Experimental/distribution/multi-gpu/cmle/project/trainer/__init__.py,0,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
Experimental/distribution/multi-gpu/cmle/project/trainer/sample_model.py,39,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tensorflow as tf\n\n\ndef _conv(x,kernel, name, log=False):\n    with tf.variable_scope(name):\n        W = tf.get_variable(initializer=tf.truncated_normal(shape=kernel, stddev=0.01), name=\'W\')\n        b = tf.get_variable(initializer=tf.constant(0.0, shape=[kernel[3]]), name=\'b\')\n        conv = tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\'SAME\')\n        activation = tf.nn.relu(tf.add(conv,b))\n        pool = tf.nn.max_pool(activation, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\'SAME\')\n        if log == True:\n            tf.summary.histogram(""weights"", W)\n            tf.summary.histogram(""biases"", b)\n            tf.summary.histogram(""activations"", activation)\n        return pool\n\n\ndef _dense(x,size_in,size_out,name,relu=False,log=False):\n    with tf.variable_scope(name):\n        flat = tf.reshape(x, [-1, size_in])\n        W = tf.get_variable(initializer=tf.truncated_normal([size_in,size_out], stddev=0.1), name=\'W\')\n        b = tf.get_variable(initializer=tf.constant(0.0, shape=[size_out]), name=\'b\')\n        activation = tf.add(tf.matmul(flat, W), b)\n        if relu==True:\n            activation = tf.nn.relu(activation)\n        if log==True:\n            tf.summary.histogram(""weights"", W)\n            tf.summary.histogram(""biases"", b)\n            tf.summary.histogram(""activations"", activation)\n        return activation\n    \n\ndef _model(features, mode, params):\n    input_layer = tf.reshape(features, [-1, 32, 32, 3])\n    conv1 = _conv(input_layer, kernel=[5,5,3,128], name=\'conv1\', log=params[\'log\'])\n    conv2 = _conv(conv1, kernel=[5,5,128,128], name=\'conv2\', log=params[\'log\'])\n    conv3 = _conv(conv2, kernel=[3,3,128,256], name=\'conv3\', log=params[\'log\'])\n    conv4 = _conv(conv3, kernel=[3,3,256,512], name=\'conv4\', log=params[\'log\'])\n    dense = _dense(conv4, size_in=2*2*512, size_out=params[\'dense_units\'],\n                   name=\'Dense\', relu=True, log=params[\'log\'])\n    \n    if mode==tf.estimator.ModeKeys.TRAIN:\n        dense = tf.nn.dropout(dense, params[\'drop_out\'])\n        \n    logits = _dense(dense, size_in=params[\'dense_units\'],\n                    size_out=10, name=\'Output\', relu=False, log=params[\'log\'])\n    return logits\n\n\ndef model_fn(features, labels, mode, params):\n    logits = _model(features, mode, params)\n    predictions = {""logits"": logits,\n                   ""classes"": tf.argmax(input=logits,axis=1),\n                   ""probabilities"": tf.nn.softmax(logits,name=\'softmax\')}\n    export_outputs = {\'predictions\': tf.estimator.export.PredictOutput(predictions)}\n    \n    if (mode==tf.estimator.ModeKeys.TRAIN or mode==tf.estimator.ModeKeys.EVAL):\n        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,logits=logits)\n    \n    if mode == tf.estimator.ModeKeys.TRAIN:\n        learning_rate = tf.train.exponential_decay(params[\'learning_rate\'],\n                                                   tf.train.get_global_step(),\n                                                   decay_steps=100000,\n                                                   decay_rate=0.96)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n        tf.summary.scalar(\'learning_rate\', learning_rate)\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n    \n    if mode == tf.estimator.ModeKeys.EVAL:\n        accuracy = tf.metrics.accuracy(\n            labels=labels, predictions=tf.argmax(logits, axis=1))\n        metrics = {\'accuracy\':accuracy}\n        return tf.estimator.EstimatorSpec(mode=mode,loss=loss, eval_metric_ops=metrics)\n        \n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n            mode=mode, predictions=predictions, export_outputs=export_outputs)\n'"
Experimental/distribution/multi-gpu/cmle/project/trainer/task.py,48,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport tensorflow as tf\n\nimport trainer.sample_model as sm\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer(\n    \'max_steps\', 1000, \'max_step for training.\')\ntf.app.flags.DEFINE_string(\n    \'output_dir\', \'\', \'GCS location to root directory for checkpoints and exported models.\')\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'sample_model\', \'model name.\')\ntf.app.flags.DEFINE_integer(\n    \'train_batch_size\', 200, \'batch size for training.\')\ntf.app.flags.DEFINE_integer(\n    \'eval_batch_size\', 200, \'batch size for evaluation.\')\ntf.app.flags.DEFINE_integer(\n    \'eval_steps\', 50, \'The number of steps that are used in evaluation phase.\')\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 19851211, \'\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_steps\', 500, \'\')\ntf.app.flags.DEFINE_string(\n    \'train_data_pattern\', \'cifar-10/train*.tfrecord\', \'path to train dataset on GCS.\')\ntf.app.flags.DEFINE_string(\n    \'eval_data_pattern\', \'cifar-10/valid*.tfrecord\', \'path to eval dataset on GCS.\')\ntf.app.flags.DEFINE_float(\n    \'learning_rate\', 1e-3, \'learning rate.\')\ntf.app.flags.DEFINE_integer(\n    \'num_gpus\', 1, \'num of gpus in single-node-multi-GPUs setting.\')\ntf.app.flags.DEFINE_integer(\n    \'num_gpus_per_worker\', 0, \'num of gpus for each node.\')\ntf.app.flags.DEFINE_bool(\n    \'auto_shard_dataset\', False,\n    \'whether to auto-shard the dataset when there are multiple workers.\')\ntf.app.flags.DEFINE_float(\n    \'drop_out_rate\', 1e-2, \'drop out rate\')\ntf.app.flags.DEFINE_integer(\n    \'dense_units\', 1024, \'units in dense layer.\')\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\ndef parse_tfrecord(example):\n    feature={\'label\': tf.FixedLenFeature((), tf.int64),\n             \'image\': tf.FixedLenFeature((), tf.string, default_value="""")}\n    parsed = tf.parse_single_example(example, feature)\n    image = tf.decode_raw(parsed[\'image\'],tf.float64)\n    image = tf.cast(image,tf.float32)\n    image = tf.reshape(image,[32,32,3])\n    return image, parsed[\'label\']\n\n\ndef image_scaling(x):\n    return tf.image.per_image_standardization(x)\n\ndef distort(x):\n    x = tf.image.resize_image_with_crop_or_pad(x, 40, 40)\n    x = tf.random_crop(x, [32, 32, 3])\n    x = tf.image.random_flip_left_right(x)\n    return x\n\ndef dataset_input_fn(params):\n    dataset = tf.data.TFRecordDataset(params[\'filenames\'],\n                                      num_parallel_reads=params[\'threads\'])\n    dataset = dataset.map(parse_tfrecord, num_parallel_calls=params[\'threads\'])\n    dataset = dataset.map(\n        lambda x,y: (image_scaling(x),y), num_parallel_calls=params[\'threads\'])\n    if params[\'mode\']==tf.estimator.ModeKeys.TRAIN:\n        dataset = dataset.map(\n            lambda x,y: (distort(x),y), num_parallel_calls=params[\'threads\'])\n        dataset = dataset.shuffle(buffer_size=params[\'shuffle_buff\'])\n    dataset = dataset.repeat()\n    dataset = dataset.batch(params[\'batch\'])\n    dataset = dataset.prefetch(8*params[\'batch\'])\n    return dataset\n\n\ndef train_dataset_input_fn(pattern):\n    files = tf.gfile.Glob(pattern)\n    params = {\'filenames\': files, \'mode\': tf.estimator.ModeKeys.TRAIN,\n              \'threads\': 16, \'shuffle_buff\': 100000, \'batch\': FLAGS.train_batch_size}\n    return dataset_input_fn(params)\n\n\ndef eval_dataset_input_fn(pattern):\n    files = tf.gfile.Glob(pattern)\n    params = {\'filenames\': tf.gfile.Glob(pattern), \'mode\': tf.estimator.ModeKeys.EVAL,\n              \'threads\': 16, \'batch\': FLAGS.eval_batch_size}\n    return dataset_input_fn(params)\n\n\ndef serving_input_fn():\n    receiver_tensor = {\'images\': tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)}\n    features = tf.map_fn(image_scaling, receiver_tensor[\'images\'])\n    return tf.estimator.export.TensorServingInputReceiver(features, receiver_tensor)\n\n\ndef train_and_evaluate():\n    model_dir = os.path.join(FLAGS.output_dir, FLAGS.model_name)\n\n    # MirroredStrategy\n    if FLAGS.num_gpus_per_worker > 0:\n        distribution = tf.contrib.distribute.MirroredStrategy(\n            num_gpus_per_worker=FLAGS.num_gpus_per_worker,\n            auto_shard_dataset=FLAGS.auto_shard_dataset)\n    elif FLAGS.num_gpus > 0:\n        distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.num_gpus)\n    else:\n        distribution = None\n\n    # Configuration for Estimator\n    config = tf.estimator.RunConfig(\n        save_checkpoints_secs=FLAGS.save_checkpoints_steps,\n        keep_checkpoint_max=5,\n        session_config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True),\n        train_distribute=distribution,\n        tf_random_seed=FLAGS.tf_random_seed)\n\n    model_params = {\n        \'drop_out\': FLAGS.drop_out_rate,\n        \'dense_units\': FLAGS.dense_units,\n        \'learning_rate\': FLAGS.learning_rate,\n        \'log\': True}\n\n    # Create Estimator.\n    estimator = tf.estimator.Estimator(\n        model_fn=sm.model_fn,\n        model_dir=model_dir,\n        params=model_params,\n        config=config)\n\n    # Specify training data paths, batch size and max steps.\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=lambda: train_dataset_input_fn(FLAGS.train_data_pattern),\n        max_steps=FLAGS.max_steps)\n    \n    # Configuration for model exportation\n    exporter = tf.estimator.LatestExporter(\n        name=\'export\',\n        serving_input_receiver_fn=serving_input_fn,\n        assets_extra=None, as_text=False, exports_to_keep=5)\n\n    # Specify validation data paths, steps for evaluation and exporter specs\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=lambda: eval_dataset_input_fn(FLAGS.eval_data_pattern),\n        steps=FLAGS.eval_steps, exporters=exporter)\n\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\ndef main(unused_argv=None):\n    tf.logging.info(tf.__version__)\n    train_and_evaluate()\n    \nif __name__ == \'__main__\':\n    tf.app.run()\n'"
