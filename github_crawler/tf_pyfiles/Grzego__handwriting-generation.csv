file_path,api_count,code
batch_generator.py,0,"b""import os\nimport random\nimport pickle\nimport numpy as np\n\n\nclass BatchGenerator(object):\n    def __init__(self, batch_size, seq_len):\n        self.batch_size = batch_size\n        self.seq_len = seq_len\n\n        dataset, labels, self.translation = self.load_dataset()\n        ndataset, nlabels = [], []\n        for i in range(len(dataset)):\n            if len(dataset[i]) >= seq_len + 1:\n                ndataset += [dataset[i]]\n                nlabels += [labels[i]]\n        del dataset, labels\n        self.dataset, labels = ndataset, nlabels\n\n        self.num_letters = len(self.translation)\n        # pad all labels to be the same length\n        max_len = max(map(lambda x: len(x), labels))\n        self.labels = np.array([np.concatenate([np.eye(self.num_letters, dtype=np.float32)[l],\n                                                np.zeros((max_len - len(l) + 1, self.num_letters),\n                                                         dtype=np.float32)],\n                                               axis=0)\n                                for l in labels])\n        self.max_len = self.labels.shape[1]\n        self.indices = np.random.choice(len(self.dataset), size=(batch_size,), replace=False)\n        self.batches = np.zeros((batch_size,), dtype=np.int32)\n\n    def next_batch(self):\n        coords = np.zeros((self.batch_size, self.seq_len + 1, 3), dtype=np.float32)\n        sequence = np.zeros((self.batch_size, self.max_len, self.num_letters), dtype=np.float32)\n        reset_states = np.ones((self.batch_size, 1), dtype=np.float32)\n        needed = False\n        for i in range(self.batch_size):\n            if self.batches[i] + self.seq_len + 1 > self.dataset[self.indices[i]].shape[0]:\n                ni = random.randint(0, len(self.dataset) - 1)\n                self.indices[i] = ni\n                self.batches[i] = 0\n                reset_states[i] = 0.\n                needed = True\n            coords[i, :, :] = self.dataset[self.indices[i]][self.batches[i]: self.batches[i] + self.seq_len + 1]\n            sequence[i] = self.labels[self.indices[i]]\n            self.batches[i] += self.seq_len\n\n        return coords, sequence, reset_states, needed\n\n    @staticmethod\n    def load_dataset():\n        dataset = np.load(os.path.join('data', 'dataset.npy'))\n        dataset = [np.array(d) for d in dataset]\n        temp = []\n        for d in dataset:\n            # dataset stores actual pen points, but we will train on differences between consecutive points\n            offs = d[1:, :2] - d[:-1, :2]\n            ends = d[1:, 2]\n            temp += [np.concatenate([[[0., 0., 1.]], np.concatenate([offs, ends[:, None]], axis=1)], axis=0)]\n        # because lines are of different length, we store them in python array (not numpy)\n        dataset = temp\n        labels = np.load(os.path.join('data', 'labels.npy'))\n        with open(os.path.join('data', 'translation.pkl'), 'rb') as file:\n            translation = pickle.load(file)\n\n        return dataset, labels, translation\n"""
generate.py,4,"b""import os\nimport pickle\nimport argparse\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.mlab as mlab\nfrom matplotlib import animation\nimport seaborn\nfrom collections import namedtuple\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', dest='model_path', type=str, default=os.path.join('pretrained', 'model-29'))\nparser.add_argument('--text', dest='text', type=str, default=None)\nparser.add_argument('--style', dest='style', type=int, default=None)\nparser.add_argument('--bias', dest='bias', type=float, default=1.)\nparser.add_argument('--force', dest='force', action='store_true', default=False)\nparser.add_argument('--animation', dest='animation', action='store_true', default=False)\nparser.add_argument('--noinfo', dest='info', action='store_false', default=True)\nparser.add_argument('--save', dest='save', type=str, default=None)\nargs = parser.parse_args()\n\n\ndef sample(e, mu1, mu2, std1, std2, rho):\n    cov = np.array([[std1 * std1, std1 * std2 * rho],\n                    [std1 * std2 * rho, std2 * std2]])\n    mean = np.array([mu1, mu2])\n\n    x, y = np.random.multivariate_normal(mean, cov)\n    end = np.random.binomial(1, e)\n    return np.array([x, y, end])\n\n\ndef split_strokes(points):\n    points = np.array(points)\n    strokes = []\n    b = 0\n    for e in range(len(points)):\n        if points[e, 2] == 1.:\n            strokes += [points[b: e + 1, :2].copy()]\n            b = e + 1\n    return strokes\n\n\ndef cumsum(points):\n    sums = np.cumsum(points[:, :2], axis=0)\n    return np.concatenate([sums, points[:, 2:]], axis=1)\n\n\ndef sample_text(sess, args_text, translation, style=None):\n    fields = ['coordinates', 'sequence', 'bias', 'e', 'pi', 'mu1', 'mu2', 'std1', 'std2',\n              'rho', 'window', 'kappa', 'phi', 'finish', 'zero_states']\n    vs = namedtuple('Params', fields)(\n        *[tf.get_collection(name)[0] for name in fields]\n    )\n\n    text = np.array([translation.get(c, 0) for c in args_text])\n    coord = np.array([0., 0., 1.])\n    coords = [coord]\n\n    # Prime the model with the author style if requested\n    prime_len, style_len = 0, 0\n    if style is not None:\n        # Priming consist of joining to a real pen-position and character sequences the synthetic sequence to generate\n        #   and set the synthetic pen-position to a null vector (the positions are sampled from the MDN)\n        style_coords, style_text = style\n        prime_len = len(style_coords)\n        style_len = len(style_text)\n        prime_coords = list(style_coords)\n        coord = prime_coords[0] # Set the first pen stroke as the first element to process\n        text = np.r_[style_text, text] # concatenate on 1 axis the prime text + synthesis character sequence\n        sequence_prime = np.eye(len(translation), dtype=np.float32)[style_text]\n        sequence_prime = np.expand_dims(np.concatenate([sequence_prime, np.zeros((1, len(translation)))]), axis=0)\n\n    sequence = np.eye(len(translation), dtype=np.float32)[text]\n    sequence = np.expand_dims(np.concatenate([sequence, np.zeros((1, len(translation)))]), axis=0)\n\n    phi_data, window_data, kappa_data, stroke_data = [], [], [], []\n    sess.run(vs.zero_states)\n    sequence_len = len(args_text) + style_len\n    for s in range(1, 60 * sequence_len + 1):\n        is_priming = s < prime_len\n\n        print('\\r[{:5d}] sampling... {}'.format(s, 'priming' if is_priming else 'synthesis'), end='')\n\n        e, pi, mu1, mu2, std1, std2, rho, \\\n        finish, phi, window, kappa = sess.run([vs.e, vs.pi, vs.mu1, vs.mu2,\n                                               vs.std1, vs.std2, vs.rho, vs.finish,\n                                               vs.phi, vs.window, vs.kappa],\n                                              feed_dict={\n                                                  vs.coordinates: coord[None, None, ...],\n                                                  vs.sequence: sequence_prime if is_priming else sequence,\n                                                  vs.bias: args.bias\n                                              })\n\n        if is_priming:\n            # Use the real coordinate if priming\n            coord = prime_coords[s]\n        else:\n            # Synthesis mode\n            phi_data += [phi[0, :]]\n            window_data += [window[0, :]]\n            kappa_data += [kappa[0, :]]\n            # ---\n            g = np.random.choice(np.arange(pi.shape[1]), p=pi[0])\n            coord = sample(e[0, 0], mu1[0, g], mu2[0, g],\n                           std1[0, g], std2[0, g], rho[0, g])\n            coords += [coord]\n            stroke_data += [[mu1[0, g], mu2[0, g], std1[0, g], std2[0, g], rho[0, g], coord[2]]]\n\n            if not args.force and finish[0, 0] > 0.8:\n                print('\\nFinished sampling!\\n')\n                break\n\n    coords = np.array(coords)\n    coords[-1, 2] = 1.\n\n    return phi_data, window_data, kappa_data, stroke_data, coords\n\n\ndef main():\n    with open(os.path.join('data', 'translation.pkl'), 'rb') as file:\n        translation = pickle.load(file)\n    rev_translation = {v: k for k, v in translation.items()}\n    charset = [rev_translation[i] for i in range(len(rev_translation))]\n    charset[0] = ''\n\n    config = tf.ConfigProto(\n        device_count={'GPU': 0}\n    )\n    with tf.Session(config=config) as sess:\n        saver = tf.train.import_meta_graph(args.model_path + '.meta')\n        saver.restore(sess, args.model_path)\n\n        while True:\n            if args.text is not None:\n                args_text = args.text\n            else:\n                args_text = input('What to generate: ')\n\n            style = None\n            if args.style is not None:\n                style = None\n                with open(os.path.join('data', 'styles.pkl'), 'rb') as file:\n                    styles = pickle.load(file)\n\n                if args.style > len(styles[0]):\n                    raise ValueError('Requested style is not in style list')\n\n                style = [styles[0][args.style], styles[1][args.style]]\n\n            phi_data, window_data, kappa_data, stroke_data, coords = sample_text(sess, args_text, translation, style)\n\n            strokes = np.array(stroke_data)\n            epsilon = 1e-8\n            strokes[:, :2] = np.cumsum(strokes[:, :2], axis=0)\n            minx, maxx = np.min(strokes[:, 0]), np.max(strokes[:, 0])\n            miny, maxy = np.min(strokes[:, 1]), np.max(strokes[:, 1])\n\n            if args.info:\n                delta = abs(maxx - minx) / 400.\n                x = np.arange(minx, maxx, delta)\n                y = np.arange(miny, maxy, delta)\n                x_grid, y_grid = np.meshgrid(x, y)\n                z_grid = np.zeros_like(x_grid)\n                for i in range(strokes.shape[0]):\n                    gauss = mlab.bivariate_normal(x_grid, y_grid, mux=strokes[i, 0], muy=strokes[i, 1],\n                                                  sigmax=strokes[i, 2], sigmay=strokes[i, 3],\n                                                  sigmaxy=0.)  # strokes[i, 4]\n                    z_grid += gauss * np.power(strokes[i, 2] + strokes[i, 3], 0.4) / (np.max(gauss) + epsilon)\n\n                fig, ax = plt.subplots(2, 2)\n\n                ax[0, 0].imshow(z_grid, interpolation='bilinear', aspect='auto', cmap=cm.jet)\n                ax[0, 0].grid(False)\n                ax[0, 0].set_title('Densities')\n                ax[0, 0].set_aspect('equal')\n\n                for stroke in split_strokes(cumsum(np.array(coords))):\n                    ax[0, 1].plot(stroke[:, 0], -stroke[:, 1])\n                ax[0, 1].set_title('Handwriting')\n                ax[0, 1].set_aspect('equal')\n\n                phi_img = np.vstack(phi_data).T[::-1, :]\n                ax[1, 0].imshow(phi_img, interpolation='nearest', aspect='auto', cmap=cm.jet)\n                ax[1, 0].set_yticks(np.arange(0, len(args_text) + 1))\n                ax[1, 0].set_yticklabels(list(' ' + args_text[::-1]), rotation='vertical', fontsize=8)\n                ax[1, 0].grid(False)\n                ax[1, 0].set_title('Phi')\n\n                window_img = np.vstack(window_data).T\n                ax[1, 1].imshow(window_img, interpolation='nearest', aspect='auto', cmap=cm.jet)\n                ax[1, 1].set_yticks(np.arange(0, len(charset)))\n                ax[1, 1].set_yticklabels(list(charset), rotation='vertical', fontsize=8)\n                ax[1, 1].grid(False)\n                ax[1, 1].set_title('Window')\n\n                plt.show()\n            else:\n                fig, ax = plt.subplots(1, 1)\n                for stroke in split_strokes(cumsum(np.array(coords))):\n                    plt.plot(stroke[:, 0], -stroke[:, 1])\n                ax.set_title('Handwriting')\n                ax.set_aspect('equal')\n                plt.show()\n\n            if args.animation:\n                fig, ax = plt.subplots(1, 1, frameon=False, figsize=(2 * (maxx - minx + 2) / (maxy - miny + 1), 2))\n                ax.set_xlim(minx - 1., maxx + 1.)\n                ax.set_ylim(-maxy - 0.5, -miny + 0.5)\n                ax.set_aspect('equal')\n                ax.axis('off')\n                # ax.hold(True)\n\n                plt.draw()\n                plt.show(False)\n\n                background = fig.canvas.copy_from_bbox(ax.bbox)\n\n                sumed = cumsum(coords)\n\n                def _update(i):\n                    c1, c2 = sumed[i: i+2]\n                    fig.canvas.restore_region(background)\n                    if c1[2] == 1. and c2[2] == 1.:\n                        line, = ax.plot([c2[0], c2[0]], [-c2[1], -c2[1]])\n                    elif c1[2] != 1.:\n                        line, = ax.plot([c1[0], c2[0]], [-c1[1], -c2[1]])\n                    else:\n                        line, = ax.plot([c1[0], c1[0]], [-c1[1], -c1[1]])\n                    fig.canvas.blit(ax.bbox)\n                    return line,\n\n                anim = animation.FuncAnimation(fig, _update, frames=len(sumed) - 2,\n                                               interval=16, blit=True, repeat=False)\n                if args.save is not None:\n                    anim.save(args.save, fps=60, extra_args=['-vcodec', 'libx264'])\n                plt.show()\n\n            if args.text is not None:\n                break\n\n\nif __name__ == '__main__':\n    main()\n"""
preprocess.py,0,"b'import os\nimport html\nimport pickle\nimport numpy as np\nimport xml.etree.cElementTree as ElementTree\n\n""""""\n- all points:\n>> [[x1, y1, e1], ..., [xn, yn, en]]\n- indexed values\n>> [h1, ... hn]\n""""""\n\n\ndef distance(p1, p2, axis=None):\n    return np.sqrt(np.sum(np.square(p1 - p2), axis=axis))\n\n\ndef clear_middle(pts):\n    to_remove = set()\n    for i in range(1, len(pts) - 1):\n        p1, p2, p3 = pts[i - 1: i + 2, :2]\n        dist = distance(p1, p2) + distance(p2, p3)\n        if dist > 1500:\n            to_remove.add(i)\n    npts = []\n    for i in range(len(pts)):\n        if i not in to_remove:\n            npts += [pts[i]]\n    return np.array(npts)\n\n\ndef separate(pts):\n    seps = []\n    for i in range(0, len(pts) - 1):\n        if distance(pts[i], pts[i+1]) > 600:\n            seps += [i + 1]\n    return [pts[b:e] for b, e in zip([0] + seps, seps + [len(pts)])]\n\n\ndef main():\n    data = []\n    charset = set()\n\n    file_no = 0\n    for root, dirs, files in os.walk(\'.\'):\n        for file in files:\n            file_name, extension = os.path.splitext(file)\n            if extension == \'.xml\':\n                file_no += 1\n                print(\'[{:5d}] File {} -- \'.format(file_no, os.path.join(root, file)), end=\'\')\n                xml = ElementTree.parse(os.path.join(root, file)).getroot()\n                transcription = xml.findall(\'Transcription\')\n                if not transcription:\n                    print(\'skipped\')\n                    continue\n                texts = [html.unescape(s.get(\'text\')) for s in transcription[0].findall(\'TextLine\')]\n                points = [s.findall(\'Point\') for s in xml.findall(\'StrokeSet\')[0].findall(\'Stroke\')]\n                strokes = []\n                mid_points = []\n                for ps in points:\n                    pts = np.array([[int(p.get(\'x\')), int(p.get(\'y\')), 0] for p in ps])\n                    pts[-1, 2] = 1\n\n                    pts = clear_middle(pts)\n                    if len(pts) == 0:\n                        continue\n\n                    seps = separate(pts)\n                    for pss in seps:\n                        if len(seps) > 1 and len(pss) == 1:\n                            continue\n                        pss[-1, 2] = 1\n\n                        xmax, ymax = max(pss, key=lambda x: x[0])[0], max(pss, key=lambda x: x[1])[1]\n                        xmin, ymin = min(pss, key=lambda x: x[0])[0], min(pss, key=lambda x: x[1])[1]\n\n                        strokes += [pss]\n                        mid_points += [[(xmax + xmin) / 2., (ymax + ymin) / 2.]]\n                distances = [-(abs(p1[0] - p2[0]) + abs(p1[1] - p2[1]))\n                             for p1, p2 in zip(mid_points, mid_points[1:])]\n                splits = sorted(np.argsort(distances)[:len(texts) - 1] + 1)\n                lines = []\n                for b, e in zip([0] + splits, splits + [len(strokes)]):\n                    lines += [[p for pts in strokes[b:e] for p in pts]]\n                print(\'lines = {:4d}; texts = {:4d}\'.format(len(lines), len(texts)))\n                charset |= set(\'\'.join(texts))\n                data += [(texts, lines)]\n    print(\'data = {}; charset = ({}) {}\'.format(len(data), len(charset), \'\'.join(sorted(charset))))\n\n    translation = {\'<NULL>\': 0}\n    for c in \'\'.join(sorted(charset)):\n        translation[c] = len(translation)\n\n    def translate(txt):\n        return list(map(lambda x: translation[x], txt))\n\n    dataset = []\n    labels = []\n    for texts, lines in data:\n        for text, line in zip(texts, lines):\n            line = np.array(line, dtype=np.float32)\n            line[:, 0] = line[:, 0] - np.min(line[:, 0])\n            line[:, 1] = line[:, 1] - np.mean(line[:, 1])\n\n            dataset += [line]\n            labels += [translate(text)]\n\n    whole_data = np.concatenate(dataset, axis=0)\n    std_y = np.std(whole_data[:, 1])\n    norm_data = []\n    for line in dataset:\n        line[:, :2] /= std_y\n        norm_data += [line]\n    dataset = norm_data\n\n    print(\'datset = {}; labels = {}\'.format(len(dataset), len(labels)))\n\n    try:\n        os.makedirs(\'data\')\n    except FileExistsError:\n        pass\n    np.save(os.path.join(\'data\', \'dataset\'), np.array(dataset))\n    np.save(os.path.join(\'data\', \'labels\'), np.array(labels))\n    with open(os.path.join(\'data\', \'translation.pkl\'), \'wb\') as file:\n        pickle.dump(translation, file)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,81,"b""import os\nimport argparse\nimport numpy as np\nimport tensorflow as tf\nfrom collections import namedtuple\n\nfrom utils import next_experiment_path\nfrom batch_generator import BatchGenerator\n\n\n# TODO: add help info\nparser = argparse.ArgumentParser()\nparser.add_argument('--seq_len', dest='seq_len', default=256, type=int)\nparser.add_argument('--batch_size', dest='batch_size', default=64, type=int)\nparser.add_argument('--epochs', dest='epochs', default=30, type=int)\nparser.add_argument('--window_mixtures', dest='window_mixtures', default=10, type=int)\nparser.add_argument('--output_mixtures', dest='output_mixtures', default=20, type=int)\nparser.add_argument('--lstm_layers', dest='lstm_layers', default=3, type=int)\nparser.add_argument('--units_per_layer', dest='units', default=400, type=int)\nparser.add_argument('--restore', dest='restore', default=None, type=str)\nargs = parser.parse_args()\n\nepsilon = 1e-8\n\n\nclass WindowLayer(object):\n    def __init__(self, num_mixtures, sequence, num_letters):\n        self.sequence = sequence  # one-hot encoded sequence of characters -- [batch_size, length, num_letters]\n        self.seq_len = tf.shape(sequence)[1]\n        self.num_mixtures = num_mixtures\n        self.num_letters = num_letters\n        self.u_range = -tf.expand_dims(tf.expand_dims(tf.range(0., tf.cast(self.seq_len, dtype=tf.float32)), axis=0),\n                                       axis=0)\n\n    def __call__(self, inputs, k, reuse=None):\n        with tf.variable_scope('window', reuse=reuse):\n            alpha = tf.layers.dense(inputs, self.num_mixtures, activation=tf.exp,\n                                    kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='alpha')\n            beta = tf.layers.dense(inputs, self.num_mixtures, activation=tf.exp,\n                                   kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='beta')\n            kappa = tf.layers.dense(inputs, self.num_mixtures, activation=tf.exp,\n                                    kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='kappa')\n\n            a = tf.expand_dims(alpha, axis=2)\n            b = tf.expand_dims(beta, axis=2)\n            k = tf.expand_dims(k + kappa, axis=2)\n\n            phi = tf.exp(-np.square(self.u_range + k) * b) * a  # [batch_size, mixtures, length]\n            phi = tf.reduce_sum(phi, axis=1, keep_dims=True)  # [batch_size, 1, length]\n\n            # whether or not network finished generating sequence\n            finish = tf.cast(phi[:, 0, -1] > tf.reduce_max(phi[:, 0, :-1], axis=1), tf.float32)\n\n            return tf.squeeze(tf.matmul(phi, self.sequence), axis=1), \\\n                   tf.squeeze(k, axis=2), \\\n                   tf.squeeze(phi, axis=1), \\\n                   tf.expand_dims(finish, axis=1)\n\n    @property\n    def output_size(self):\n        return [self.num_letters, self.num_mixtures, 1]\n\n\nclass MixtureLayer(object):\n    def __init__(self, input_size, output_size, num_mixtures):\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_mixtures = num_mixtures\n\n    def __call__(self, inputs, bias=0., reuse=None):\n        with tf.variable_scope('mixture_output', reuse=reuse):\n            e = tf.layers.dense(inputs, 1,\n                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='e')\n            pi = tf.layers.dense(inputs, self.num_mixtures,\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='pi')\n            mu1 = tf.layers.dense(inputs, self.num_mixtures,\n                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='mu1')\n            mu2 = tf.layers.dense(inputs, self.num_mixtures,\n                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='mu2')\n            std1 = tf.layers.dense(inputs, self.num_mixtures,\n                                   kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='std1')\n            std2 = tf.layers.dense(inputs, self.num_mixtures,\n                                   kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='std2')\n            rho = tf.layers.dense(inputs, self.num_mixtures,\n                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.075), name='rho')\n\n            return tf.nn.sigmoid(e), \\\n                   tf.nn.softmax(pi * (1. + bias)), \\\n                   mu1, mu2, \\\n                   tf.exp(std1 - bias), tf.exp(std2 - bias), \\\n                   tf.nn.tanh(rho)\n\n\nclass RNNModel(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, layers, num_units, input_size, num_letters, batch_size, window_layer):\n        super(RNNModel, self).__init__()\n        self.layers = layers\n        self.num_units = num_units\n        self.input_size = input_size\n        self.num_letters = num_letters\n        self.window_layer = window_layer\n        self.last_phi = None\n\n        with tf.variable_scope('rnn', reuse=None):\n            self.lstms = [tf.nn.rnn_cell.LSTMCell(num_units)\n                          for _ in range(layers)]\n            self.states = [tf.Variable(tf.zeros([batch_size, s]), trainable=False)\n                           for s in self.state_size]\n\n            self.zero_states = tf.group(*[sp.assign(sc)\n                                          for sp, sc in zip(self.states,\n                                                            self.zero_state(batch_size, dtype=tf.float32))])\n\n    @property\n    def state_size(self):\n        return [self.num_units] * self.layers * 2 + self.window_layer.output_size\n\n    @property\n    def output_size(self):\n        return [self.num_units]\n\n    def call(self, inputs, state, **kwargs):\n        # state[-3] --> window\n        # state[-2] --> k\n        # state[-1] --> finish\n        # state[2n] --> h\n        # state[2n+1] --> c\n        window, k, finish = state[-3:]\n        output_state = []\n        prev_output = []\n\n        for layer in range(self.layers):\n            x = tf.concat([inputs, window] + prev_output, axis=1)\n            with tf.variable_scope('lstm_{}'.format(layer)):\n                output, s = self.lstms[layer](x, tf.nn.rnn_cell.LSTMStateTuple(state[2 * layer],\n                                                                               state[2 * layer + 1]))\n                prev_output = [output]\n            output_state += [*s]\n\n            if layer == 0:\n                window, k, self.last_phi, finish = self.window_layer(output, k)\n\n        return output, output_state + [window, k, finish]\n\n\ndef create_graph(num_letters, batch_size,\n                 num_units=400, lstm_layers=3,\n                 window_mixtures=10, output_mixtures=20):\n    graph = tf.Graph()\n    with graph.as_default():\n        coordinates = tf.placeholder(tf.float32, shape=[None, None, 3])\n        sequence = tf.placeholder(tf.float32, shape=[None, None, num_letters])\n        reset = tf.placeholder(tf.float32, shape=[None, 1])\n        bias = tf.placeholder_with_default(tf.zeros(shape=[]), shape=[])\n\n        def create_model(generate=None):\n            in_coords = coordinates[:, :-1, :]\n            out_coords = coordinates[:, 1:, :]\n\n            _batch_size = 1 if generate else batch_size\n            if generate:\n                in_coords = coordinates\n\n            with tf.variable_scope('model', reuse=generate):\n                window = WindowLayer(num_mixtures=window_mixtures, sequence=sequence, num_letters=num_letters)\n\n                rnn_model = RNNModel(layers=lstm_layers, num_units=num_units,\n                                     input_size=3, num_letters=num_letters,\n                                     window_layer=window, batch_size=_batch_size)\n\n                reset_states = tf.group(*[state.assign(state * reset)\n                                          for state in rnn_model.states])\n\n                outs, states = tf.nn.dynamic_rnn(rnn_model, in_coords,\n                                                 initial_state=rnn_model.states)\n\n                output_layer = MixtureLayer(input_size=num_units, output_size=2,\n                                            num_mixtures=output_mixtures)\n\n                with tf.control_dependencies([sp.assign(sc) for sp, sc in zip(rnn_model.states, states)]):\n                    with tf.name_scope('prediction'):\n                        outs = tf.reshape(outs, [-1, num_units])\n                        e, pi, mu1, mu2, std1, std2, rho = output_layer(outs, bias)\n\n                    with tf.name_scope('loss'):\n                        coords = tf.reshape(out_coords, [-1, 3])\n                        xs, ys, es = tf.unstack(tf.expand_dims(coords, axis=2), axis=1)\n\n                        mrho = 1 - tf.square(rho)\n                        xms = (xs - mu1) / std1\n                        yms = (ys - mu2) / std2\n                        z = tf.square(xms) + tf.square(yms) - 2. * rho * xms * yms\n                        n = 1. / (2. * np.pi * std1 * std2 * tf.sqrt(mrho)) * tf.exp(-z / (2. * mrho))\n                        ep = es * e + (1. - es) * (1. - e)\n                        rp = tf.reduce_sum(pi * n, axis=1)\n\n                        loss = tf.reduce_mean(-tf.log(rp + epsilon) - tf.log(ep + epsilon))\n\n                    if generate:\n                        # save params for easier model loading and prediction\n                        for param in [('coordinates', coordinates),\n                                      ('sequence', sequence),\n                                      ('bias', bias),\n                                      ('e', e), ('pi', pi),\n                                      ('mu1', mu1), ('mu2', mu2),\n                                      ('std1', std1), ('std2', std2),\n                                      ('rho', rho),\n                                      ('phi', rnn_model.last_phi),\n                                      ('window', rnn_model.states[-3]),\n                                      ('kappa', rnn_model.states[-2]),\n                                      ('finish', rnn_model.states[-1]),\n                                      ('zero_states', rnn_model.zero_states)]:\n                            tf.add_to_collection(*param)\n\n                with tf.name_scope('training'):\n                    steps = tf.Variable(0.)\n                    learning_rate = tf.train.exponential_decay(0.001, steps, staircase=True,\n                                                               decay_steps=10000, decay_rate=0.5)\n\n                    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n                    grad, var = zip(*optimizer.compute_gradients(loss))\n                    grad, _ = tf.clip_by_global_norm(grad, 3.)\n                    train_step = optimizer.apply_gradients(zip(grad, var), global_step=steps)\n\n                with tf.name_scope('summary'):\n                    # TODO: add more summaries\n                    summary = tf.summary.merge([\n                        tf.summary.scalar('loss', loss)\n                    ])\n\n                return namedtuple('Model', ['coordinates', 'sequence', 'reset_states', 'reset', 'loss', 'train_step',\n                                            'learning_rate', 'summary'])(\n                           coordinates, sequence, reset_states, reset, loss, train_step, learning_rate, summary\n                       )\n        train_model = create_model(generate=None)\n        _ = create_model(generate=True)  # just to create ops for generation\n\n    return graph, train_model\n\n\ndef main():\n    restore_model = args.restore\n    seq_len = args.seq_len\n    batch_size = args.batch_size\n    num_epoch = args.epochs\n    batches_per_epoch = 1000\n\n    batch_generator = BatchGenerator(batch_size, seq_len)\n    g, vs = create_graph(batch_generator.num_letters, batch_size,\n                         num_units=args.units, lstm_layers=args.lstm_layers,\n                         window_mixtures=args.window_mixtures,\n                         output_mixtures=args.output_mixtures)\n\n    with tf.Session(graph=g) as sess:\n        model_saver = tf.train.Saver(max_to_keep=2)\n        if restore_model:\n            model_file = tf.train.latest_checkpoint(os.path.join(restore_model, 'models'))\n            experiment_path = restore_model\n            epoch = int(model_file.split('-')[-1]) + 1\n            model_saver.restore(sess, model_file)\n        else:\n            sess.run(tf.global_variables_initializer())\n            experiment_path = next_experiment_path()\n            epoch = 0\n\n        summary_writer = tf.summary.FileWriter(experiment_path, graph=g, flush_secs=10)\n        summary_writer.add_session_log(tf.SessionLog(status=tf.SessionLog.START),\n                                       global_step=epoch * batches_per_epoch)\n\n        for e in range(epoch, num_epoch):\n            print('\\nEpoch {}'.format(e))\n            for b in range(1, batches_per_epoch + 1):\n                coords, seq, reset, needed = batch_generator.next_batch()\n                if needed:\n                    sess.run(vs.reset_states, feed_dict={vs.reset: reset})\n                l, s, _ = sess.run([vs.loss, vs.summary, vs.train_step],\n                                   feed_dict={vs.coordinates: coords,\n                                              vs.sequence: seq})\n                summary_writer.add_summary(s, global_step=e * batches_per_epoch + b)\n                print('\\r[{:5d}/{:5d}] loss = {}'.format(b, batches_per_epoch, l), end='')\n\n            model_saver.save(sess, os.path.join(experiment_path, 'models', 'model'),\n                             global_step=e)\n\n\nif __name__ == '__main__':\n    main()\n"""
utils.py,0,"b'import os\nimport shutil\n\n\ndef next_experiment_path():\n    """"""\n    creates paths for new experiment\n    returns path for next experiment\n    """"""\n\n    idx = 0\n    path = os.path.join(\'summary\', \'experiment-{}\')\n    while os.path.exists(path.format(idx)):\n        idx += 1\n    path = path.format(idx)\n    os.makedirs(os.path.join(path, \'models\'))\n    os.makedirs(os.path.join(path, \'backup\'))\n    for file in filter(lambda x: x.endswith(\'.py\'), os.listdir(\'.\')):\n        shutil.copy2(file, os.path.join(path, \'backup\'))\n    return path\n'"
