file_path,api_count,code
TP_GAN-Mar6FS.py,226,"b'from __future__ import division\nfrom __future__ import print_function\nimport os,sys\nimport time\nfrom glob import glob\nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import xrange\n\nfrom ops import *\nfrom utils import *\n\nfrom net_input_everything_featparts import *\nfrom time import localtime, strftime\nimport random\nimport pickle\nimport subprocess\nrelu = tf.nn.relu\nimport scipy.misc\nimport shutil\nfrom utils import pp, visualize, to_json\n\nimport tensorflow as tf\n\n#These parameters should provide a good initialization, but if for specific refinement, you can adjust them during training.\n\nALPHA_ADVER = 2e1\nBELTA_FEATURE = 4e1 #3800 inital loss\nUPDATE_G = 2  #optimize D once and UPDATE_G times G\nUPDATE_D = 1\nPART_W = 3\nIDEN_W = 1e1\nTV_WEIGHT = 1e-3\nCOND_WEIGHT = 0.3\nL1_1_W = 1#0.5\nL1_2_W = 1\nL1_3_W = 1.5\nRANGE= 60\nRANGE_LOW = 0\nSYM_W =  3e-1\nCLIP_D = 0.1\nL1 = True\nMODE = \'fs60\'  #\'f\' feature loss enabled.        \'v\' -verification enanbled. \'o\' original, \'m\' masked is mandatory and no need to specify\nUPDATE_DV = 1 #optimize DV net\nDF = True   #local discriminator 4x4\nLOAD_60_LABEL = False #otherwise load frontal label\nWITHOUT_CODEMAP = True\nUSE_MASK = False\nENABLE_SELECTION = True\nRANDOM_VERIFY = False\nRANK_MUL = 6\nif WITHOUT_CODEMAP:\n    CHANNEL = 3\nelse:\n    CHANNEL = 6\n\nflags = tf.app.flags\nflags.DEFINE_integer(""epoch"", 250, ""Epoch to train [25]"")\nflags.DEFINE_float(""learning_rate"", 1e-4, ""Learning rate of for adam [0.0002]"")\nflags.DEFINE_float(""beta1"", 0.9, ""Momentum term of adam [0.5]"")\nflags.DEFINE_integer(""train_size"", np.inf, ""The size of train images [np.inf]"")\nflags.DEFINE_integer(""batch_size"", 20, ""The size of batch images [64]"")\nflags.DEFINE_integer(""image_size"", 128, ""The size of image to use (will be center cropped) [108]"")\nflags.DEFINE_integer(""output_size"", 128, ""The size of the output images to produce [64]"")\nflags.DEFINE_integer(""c_dim"", 3, ""Dimension of image color. [3]"")\nflags.DEFINE_string(""dataset"", ""MultiPIE"", ""The name of dataset [celebA, mnist, lsun]"")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoint60"", ""Directory name to save the checkpoints [checkpoint]"")\nflags.DEFINE_string(""sample_dir"", ""samples"", ""Directory name to save the image samples [samples]"")\nflags.DEFINE_boolean(""is_train"", True, ""True for training, False for testing [False]"")\nflags.DEFINE_boolean(""is_crop"", False, ""True for training, False for testing [False]"")\nflags.DEFINE_boolean(""visualize"", False, ""True for visualizing, False for nothing [False]"")\nFLAGS = flags.FLAGS\n\nclass DCGAN(object):\n    def __init__(self, sess, image_size=128, is_crop=True,\n                 batch_size=10, sample_size = 100, output_size=128,\n                 y_dim=None, z_dim=100, gf_dim=64, df_dim=64,\n                 gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name=\'MultiPIE\',\n                 checkpoint_dir=None, sample_dir=None):\n        """"""\n        Args:\n            sess: TensorFlow session\n            batch_size: The size of batch. Should be specified before training.\n            output_size: (optional) The resolution in pixels of the images. [64]\n            y_dim: (optional) Dimension of dim for y. [None]\n            z_dim: (optional) Dimension of dim for Z. [100]\n            gf_dim: (optional) Dimension of gen filters in first conv layer. [64]\n            df_dim: (optional) Dimension of discrim filters in first conv layer. [64]\n            gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024]\n            dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]\n            c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3]\n        """"""\n        self.test_batch_size = batch_size\n        self.save_interval = 300\n        self.sample_interval = 150\n        self.sess = sess\n        self.is_grayscale = (c_dim == 1)\n        self.batch_size = 10\n        self.sample_run_num = 15\n        self.testing = False\n        self.testingphase = \'FS\'\n        self.testimg = True\n        if self.testing:\n            #self.batch_size = 10\n            self.testingphase = \'60\'#\'gt50\'\n            self.sample_run_num = 99999999\n\n        self.test_batch_size = self.batch_size\n        self.image_size = image_size\n        self.sample_size = sample_size\n        self.output_size = output_size\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n\n        self.gfc_dim = gfc_dim\n        self.dfc_dim = dfc_dim\n        self.z_dim = 100\n        self.c_dim = c_dim\n\n        # batch normalization : deals with poor initialization helps gradient flow\n        random.seed()\n        self.DeepFacePath = \'/home/shu.zhang/ruihuang/data/DeepFace.pickle\'\n        self.dataset_name = dataset_name\n        self.checkpoint_dir = checkpoint_dir\n        self.loadDeepFace(self.DeepFacePath)\n        self.build_model()\n\n    def build_model(self):\n\n        #hold all four\n        #Note: not true, if WITHOUT_CODEMAP is true, then here is pure images without codemap and 3 channels\n        #mirror concatenate\n        mc = lambda left : tf.concat_v2([left, left[:,:,::-1,:]], 3)\n        self.images_with_code = tf.placeholder(tf.float32, [self.batch_size] + [self.output_size, self.output_size, CHANNEL], name=\'images_with_code\')\n        self.sample_images = tf.placeholder(tf.float32, [self.test_batch_size] + [self.output_size, self.output_size, CHANNEL], name=\'sample_images\')\n\n        if WITHOUT_CODEMAP:\n            self.images = self.images_with_code\n            self.sample_images_nocode = self.sample_images\n        else:\n            self.images = tf.split(3, 2, self.images_with_code)[0]\n            self.sample_images_nocode = tf.split(3, 2, self.sample_images)[0]\n\n        self.g_images = self.images #tf.reduce_mean(self.images, axis=3, keep_dims=True)\n        self.g_samples = self.sample_images_nocode #tf.reduce_mean(self.sample_images_nocode, axis=3, keep_dims=True)\n\n        self.g32_images_with_code = tf.image.resize_bilinear(self.images_with_code, [32, 32])\n        self.g64_images_with_code = tf.image.resize_bilinear(self.images_with_code, [64, 64])\n\n        self.g32_sampleimages_with_code = tf.image.resize_bilinear(self.sample_images, [32, 32])\n        self.g64_sampleimages_with_code = tf.image.resize_bilinear(self.sample_images, [64, 64])\n\n        self.labels = tf.placeholder(tf.float32, [self.batch_size] + [self.output_size, self.output_size, 3], name=\'label_images\')\n        self.poselabels = tf.placeholder(tf.int32, [self.batch_size])\n        self.idenlabels = tf.placeholder(tf.int32, [self.batch_size])\n        self.landmarklabels = tf.placeholder(tf.float32, [self.batch_size, 5*2])\n        self.g_labels = self.labels #tf.reduce_mean(self.labels, 3, keep_dims=True)\n        self.g8_labels = tf.image.resize_bilinear(self.g_labels, [8, 8])\n        self.g16_labels = tf.image.resize_bilinear(self.g_labels, [16, 16])\n        self.g32_labels = tf.image.resize_bilinear(self.g_labels, [32, 32])\n        self.g64_labels = tf.image.resize_bilinear(self.g_labels, [64, 64])\n\n        self.eyel = tf.placeholder(tf.float32, [self.batch_size, EYE_H, EYE_W, 3])\n        self.eyer = tf.placeholder(tf.float32, [self.batch_size, EYE_H, EYE_W, 3])\n        self.nose = tf.placeholder(tf.float32, [self.batch_size, NOSE_H, NOSE_W, 3])\n        self.mouth = tf.placeholder(tf.float32, [self.batch_size, MOUTH_H, MOUTH_W, 3])\n\n        self.eyel_label = tf.placeholder(tf.float32, [self.batch_size, EYE_H, EYE_W, 3])\n        self.eyer_label = tf.placeholder(tf.float32, [self.batch_size, EYE_H, EYE_W, 3])\n        self.nose_label = tf.placeholder(tf.float32, [self.batch_size, NOSE_H, NOSE_W, 3])\n        self.mouth_label = tf.placeholder(tf.float32, [self.batch_size, MOUTH_H, MOUTH_W, 3])\n\n        self.eyel_sam = tf.placeholder(tf.float32, [self.batch_size, EYE_H, EYE_W, 3])\n        self.eyer_sam = tf.placeholder(tf.float32, [self.batch_size, EYE_H, EYE_W, 3])\n        self.nose_sam = tf.placeholder(tf.float32, [self.batch_size, NOSE_H, NOSE_W, 3])\n        self.mouth_sam = tf.placeholder(tf.float32, [self.batch_size, MOUTH_H, MOUTH_W, 3])\n\n\n        #feats contains: self.feat128, self.feat64, self.feat32, self.feat16, self.feat8, self.feat\n        self.G_eyel,self.c_eyel = self.partRotator(self.eyel, ""PartRotator_eyel"")\n        self.G_eyer,self.c_eyer = self.partRotator(tf.concat_v2([self.eyer, self.eyel], axis=3), ""PartRotator_eyer"")\n        self.G_nose,self.c_nose = self.partRotator(self.nose, ""PartRotator_nose"")\n        self.G_mouth,self.c_mouth = self.partRotator(self.mouth, ""PartRotator_mouth"")\n\n        self.G_eyel_sam, self.c_eyel_sam = self.partRotator(self.eyel_sam, ""PartRotator_eyel"", reuse=True)\n        self.G_eyer_sam, self.c_eyer_sam = self.partRotator(tf.concat_v2([self.eyer_sam, self.eyel_sam],axis=3), ""PartRotator_eyer"", reuse=True)\n        self.G_nose_sam, self.c_nose_sam = self.partRotator(self.nose_sam, ""PartRotator_nose"", reuse=True)\n        self.G_mouth_sam, self.c_mouth_sam = self.partRotator(self.mouth_sam, ""PartRotator_mouth"", reuse=True)\n\n        self.z = tf.random_normal([self.batch_size, self.z_dim], mean=0.0, stddev=0.02, seed=2017)\n\n        #tf.placeholder(tf.float32, [self.batch_size, self.z_dim], name=\'z\')\n\n        self.feats = self.generator(mc(self.images_with_code), self.batch_size, name=""encoder"")\n        self.feats += (mc(self.images_with_code), mc(self.g64_images_with_code), mc(self.g32_images_with_code),\n                       self.G_eyel, self.G_eyer, self.G_nose, self.G_mouth,\n                       self.c_eyel, self.c_eyer, self.c_nose, self.c_mouth,)\n        self.check_sel128, self.check_sel64, self.check_sel32, self.check_sel16, self.check_sel8, self.G, self.G2, self.G3 = \\\n        self.decoder(*self.feats, batch_size=self.batch_size)\n        self.poselogits, self.identitylogits, self.Glandmark = self.FeaturePredict(self.feats[5])\n\n        sample_feats = self.generator(mc(self.sample_images),self.test_batch_size, name=""encoder"", reuse=True)\n        self.sample512 = sample_feats[-1]\n        sample_feats += (mc(self.sample_images), mc(self.g64_sampleimages_with_code), mc(self.g32_sampleimages_with_code),\n                         self.G_eyel_sam, self.G_eyer_sam, self.G_nose_sam, self.G_mouth_sam,\n                         self.c_eyel_sam, self.c_eyer_sam, self.c_nose_sam, self.c_mouth_sam,)\n        self.sample_generator = self.decoder(*sample_feats, batch_size=self.test_batch_size, reuse=True)\n        if not DF:\n            self.D, self.D_logits = self.discriminator(self.g_labels)\n            self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\n        else:\n            print(""Using local discriminator!"")\n            self.D, self.D_logits = self.discriminatorLocal(self.g_labels)\n            self.D_, self.D_logits_ = self.discriminatorLocal(self.G, reuse=True)\n        self.logfile = \'loss.txt\'\n\n        if \'f\' in MODE:\n            #self.verify_images_masked = tf.mul(self.verify_images, self.masks_binary)\n            #can not apply mask !!!\n            # self.Dv, self.Dv_logits = self.discriminatorVerify(self.labels, self.verify_images)\n            _,_,_,_, self.G_pool5, self.Gvector = self.FeatureExtractDeepFace(tf.reduce_mean(self.G, axis=3, keep_dims=True))\n            _,_,_,_, self.label_pool5, self.labelvector = self.FeatureExtractDeepFace(tf.reduce_mean(self.g_labels, axis=3, keep_dims=True), reuse=True)\n            _,_,_,_, _, self.samplevector = self.FeatureExtractDeepFace(tf.reduce_mean(self.sample_images_nocode, axis=3, keep_dims=True), reuse=True)\n            #self.Dv, self.Dv_logits = self.discriminatorClassify(self.Gvector)\n            #self.dv_loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(self.Dv_logits, self.verify_labels))\n            self.dv_loss = tf.reduce_mean(tf.abs(self.Gvector-self.labelvector))\n            self.dv_loss += tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(tf.abs(self.G_pool5-self.label_pool5),1),1))\n            self.logfile = \'loss_verify.txt\'\n            #self.dv_sum = histogram_summary(""dv_"", self.Dv)\n\n        # self.d__sum = histogram_summary(""d_"", self.D_)\n        # self.d_sum = histogram_summary(""d"", self.D)\n        # self.G_sum = image_summary(""G"", self.G)\n\n        #basic loss\n\n        # self.d_loss_real = tf.reduce_mean(self.D_logits)\n        # self.d_loss_fake = -tf.reduce_mean(self.D_logits_)\n        # self.g_loss_adver = -tf.reduce_mean(self.D_logits_)\n        self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D) * 0.9))\n        self.d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n        self.g_loss_adver = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_) * 0.9))\n\n        #self.mark_regression_loss = tf.reduce_mean(tf.square(tf.abs(self.landmarklabels-self.Glandmark)))\n        #self.poseloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.poselogits, self.poselabels))\n        self.idenloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.identitylogits, self.idenlabels))\n\n        self.eyel_loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(tf.abs(self.c_eyel - self.eyel_label), 1), 1))\n        self.eyer_loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(tf.abs(self.c_eyer - self.eyer_label), 1), 1))\n        self.nose_loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(tf.abs(self.c_nose - self.nose_label), 1), 1))\n        self.mouth_loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(tf.abs(self.c_mouth - self.mouth_label), 1), 1))\n        #rotation L1 / L2 loss in g_loss\n        # one8 = tf.ones([1,8,4,1],tf.float32)\n        # mask8 = tf.concat_v2([one8, one8], 2)\n        # mask16 = tf.image.resize_nearest_neighbor(mask8, size=[16, 16])\n        # mask32 = tf.image.resize_nearest_neighbor(mask8, size=[32, 32])\n        # mask64 = tf.image.resize_nearest_neighbor(mask8, size=[64, 64])\n        # mask128 = tf.image.resize_nearest_neighbor(mask8, size=[128, 128])\n        #use L2 for 128, L1 for others. mask emphasize left side.\n        errL1 = tf.abs(self.G - self.g_labels) #* mask128\n        errL1_2 = tf.abs(self.G2 - self.g64_labels) #* mask64\n        errL1_3 = tf.abs(self.G3 - self.g32_labels) #* mask32\n        #errcheck8 = tf.abs(self.check_sel8 - self.g8_labels) #* mask8\n        #errcheck16 = tf.abs(self.check_sel16 - self.g16_labels) #* mask16\n        errcheck32 = tf.abs(self.check_sel32 - self.g32_labels) #* mask32\n        errcheck64 = tf.abs(self.check_sel64 - self.g64_labels) #* mask64\n        errcheck128 = tf.abs(self.check_sel128 - self.g_labels) #* mask128\n\n        self.weightedErrL1 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errL1, 1), 1))\n        self.symErrL1 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(symL1(#self.processor(self.G)\n            tf.nn.avg_pool(self.G, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n           ), 1), 1))\n        self.weightedErrL2 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errL1_2, 1), 1))\n        self.symErrL2 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(symL1(self.processor(self.G2)), 1), 1))\n        self.weightedErrL3 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errL1_3, 1), 1))\n        self.symErrL3 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(symL1(self.processor(self.G3, reuse=True)), 1), 1))\n\n        cond_L12 = tf.abs(tf.image.resize_bilinear(self.G, [64,64]) - tf.stop_gradient(self.G2))\n        #self.condErrL12 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(cond_L12, 1), 1))\n        #cond_L23 = tf.abs(tf.image.resize_bilinear(self.G2, [32,32]) - tf.stop_gradient(self.G3))\n        #self.condErrL23 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(cond_L23, 1), 1))\n        self.tv_loss = tf.reduce_mean(total_variation(self.G))\n        #self.weightedErr_check8 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errcheck8, 1), 1))\n        #self.weightedErr_check16 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errcheck16, 1), 1))\n        # self.weightedErr_check32 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errcheck32, 1), 1))\n        # self.weightedErr_check64 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errcheck64, 1), 1))\n        # self.weightedErr_check128 = tf.reduce_mean(tf.reduce_sum(tf.reduce_sum(errcheck128, 1), 1))\n        # mean = tf.reduce_mean(tf.reduce_mean(self.G, 1,keep_dims=True), 2, keep_dims=True)\n        # self.stddev = tf.reduce_mean(tf.squared_difference(self.G, mean))\n\n        self.d_loss = self.d_loss_real + self.d_loss_fake\n        self.g_loss = L1_1_W * (self.weightedErrL1 + SYM_W * self.symErrL1) + L1_2_W * (self.weightedErrL2 + SYM_W * self.symErrL2) \\\n                      + L1_3_W * (self.weightedErrL3 + SYM_W * self.symErrL3)\n        self.g_loss += BELTA_FEATURE * self.dv_loss + ALPHA_ADVER * self.g_loss_adver + IDEN_W * self.idenloss + self.tv_loss * TV_WEIGHT\n\n        self.rot_loss = PART_W * (self.eyel_loss + self.eyer_loss + self.nose_loss + self.mouth_loss)\n        #self.sel_loss = self.weightedErr_check32 + self.weightedErr_check64 + self.weightedErr_check128\n        #self.g_loss += self.sel_loss\n\n        self.var_file = open(\'var_log.txt\', mode=\'a\')\n        t_vars = [var for var in tf.trainable_variables() if \'FeatureExtractDeepFace\' not in var.name \\\n                                                                                    and \'processor\' not in var.name]\n        def isTargetVar(name, tokens):\n            for token in tokens:\n                if token in name:\n                    return True\n            return False\n        dec128toks = [\'dec128\', \'recon128\', \'check_img128\']\n        self.d_vars = [var for var in t_vars if \'discriminatorLocal\' in var.name]\n        self.all_g_vars = [var for var in t_vars if \'discriminatorLocal\' not in var.name]\n        self.rot_vars = [var for var in t_vars if \'Rotator\' in var.name]\n        self.sel_vars = [var for var in t_vars if \'select\' in var.name]\n        self.dec_vars = [var for var in t_vars if \'decoder\' in var.name and \'select\' not in var.name]\n        self.enc_vars = [var for var in t_vars if \'encoder\' in var.name]\n        self.pre_vars = [var for var in t_vars if \'FeaturePredict\' in var.name]\n        #\n        self.se_vars = list(self.enc_vars); self.se_vars.extend(self.sel_vars)\n\n        self.ed_vars = list(self.dec_vars); self.ed_vars.extend(self.enc_vars);\n        self.ed_vars.extend(self.pre_vars); self.ed_vars.extend(self.rot_vars);\n        self.ed_vars.extend(self.sel_vars)\n\n        #self.rd_vars = list(self.dec_vars); self.rd_vars.extend([var for var in self.d_vars if isTargetVar(var.name, dec128toks)])\n\n        #print(""-----enc and dec ---->"", map(lambda x:x.name, self.ed_vars), sep=\'\\n\', file=var_file)\n        #print(""-----enc and sel ---->"", map(lambda x:x.name, self.se_vars), sep=\'\\n\',  file=var_file)\n        #print(""-----discrim ---->"", map(lambda x:x.name, self.d_vars),sep=\'\\n\',  file=var_file)\n\n        self.saver = tf.train.Saver(t_vars, max_to_keep=2)\n\n    def train(self, config):\n        """"""Train DCGAN""""""\n        #data = glob(os.path.join(""./data"", config.dataset, ""*.jpg""))\n        data = MultiPIE(LOAD_60_LABEL=LOAD_60_LABEL, GENERATE_MASK=USE_MASK, RANDOM_VERIFY=RANDOM_VERIFY, MIRROR_TO_ONE_SIDE = True, source = self.testingphase)\n        #np.random.shuffle(data)\n        config.sample_dir += \'{:05d}\'.format(random.randint(1,100000))\n\n        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n                          .minimize(self.d_loss, var_list=self.d_vars)\n        #clip_D = [p.assign(tf.clip_by_value(p, -CLIP_D, CLIP_D)) for p in self.d_vars]\n\n        g_dec_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n                           .minimize(self.g_loss, var_list=self.ed_vars)\n        #g_enc_optim = tf.train.AdamOptimizer(config.learning_rate * 0.001, beta1=config.beta1) \\\n        #                  .minimize(self.g_loss, var_list=self.enc_vars)\n        # s_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n        #                   .minimize(self.sel_loss, var_list=self.se_vars)\n        #g_sel_dec_optim = tf.train.RMSPropOptimizer(config.learning_rate) \\\n        #                 .minimize(self.g_loss + self.sel_loss + self.rot_loss, var_list=self.all_g_vars)\n        rot_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n                          .minimize(self.rot_loss, var_list=self.rot_vars)\n\n        init_op = tf.global_variables_initializer()\n        self.sess.run(init_op)\n\n        counter = random.randint(1,30)\n        start_time = time.time()\n\n        if self.load(self.checkpoint_dir):\n            print("" [*] Load SUCCESS"")\n        else:\n            print("" [!] Load failed..."")\n\n        sample_images, filenames ,sample_eyel, sample_eyer, sample_nose, sample_mouth, \\\n            sample_labels, sample_leyel, sample_leyer, sample_lnose, sample_lmouth, sample_iden = data.test_batch(self.test_batch_size * self.sample_run_num)\n        if not self.testing:\n            sample_imagesT, filenamesT ,sample_eyelT, sample_eyerT, sample_noseT, sample_mouthT, \\\n            sample_labelsT, sample_leyelT, sample_leyerT, sample_lnoseT, sample_lmouthT, sample_idenT = data.test_batch(self.test_batch_size * self.sample_run_num * RANK_MUL, Pose=RANGE)\n            if WITHOUT_CODEMAP:\n                sample_images = sample_images[..., 0:3]\n            #append loss log to file\n            self.f = open(self.logfile, mode=\'a\')\n            self.f.write(\'----\'+strftime(""%a, %d %b %Y %H:%M:%S +0000"", localtime())+\' BEGINS----MODE:\'+MODE+\'-----\\n\')\n            print(""start training!"")\n            for epoch in xrange(config.epoch):\n                #data = glob(os.path.join(""./data"", config.dataset, ""*.jpg""))\n                batch_idxs = min(data.size, config.train_size) // self.batch_size\n\n                for idx in xrange(0, batch_idxs):\n                    #load data from MultiPIE\n                    batch_images_with_code, batch_labels, batch_masks, verify_images, verify_labels, \\\n                    batch_pose, batch_iden, batch_landmarks,\\\n                    batch_eyel, batch_eyer, batch_nose, batch_mouth,\\\n                    batch_eyel_label, batch_eyer_label, batch_nose_label, batch_mouth_label \\\n                        = data.next_image_and_label_mask_batch(self.batch_size, imageRange=RANGE, imageRangeLow=RANGE_LOW)\n                    # batch_images = batch_images_with_code[:,:,:,0:3] #discard codes\n                    if WITHOUT_CODEMAP:\n                        batch_images_with_code = batch_images_with_code[..., 0:3]\n\n                    # needs self.G(needing images with code) and real images\n                    for _ in range(UPDATE_D):\n                        # Update D network\n                        _ = self.sess.run([d_optim,],\n                            feed_dict={ self.images_with_code: batch_images_with_code,\n                                        self.labels : batch_labels,\n                                        self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,\n                                        })\n                    for _ in range(UPDATE_G):\n                        # Update G network\n                        # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n                        _ = self.sess.run([rot_optim, g_dec_optim,],\n                        # _ = self.sess.run([g_sel_dec_optim],\n                            feed_dict={self.images_with_code: batch_images_with_code,\n                                       self.labels : batch_labels,\n                                       self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,\n                                       self.poselabels : batch_pose, self.idenlabels: batch_iden, self.landmarklabels: batch_landmarks,\n                                       self.eyel_label : batch_eyel_label, self.eyer_label : batch_eyer_label,\n                                       self.nose_label: batch_nose_label, self.mouth_label : batch_mouth_label\n                                       })\n\n                    counter += 1\n                    print(\'.\',end=\'\');sys.stdout.flush()\n                    if(counter % 5 == 0):\n                        self.evaluate(epoch, idx, batch_idxs, start_time, \'train\',\n                                       batch_images_with_code,  batch_eyel, batch_eyer, batch_nose, batch_mouth,\n                                       batch_labels, batch_eyel_label, batch_eyer_label, batch_nose_label, batch_mouth_label, batch_iden);\n                    if np.mod(counter, self.sample_interval) == self.sample_interval-1:\n                        for i in range(self.sample_run_num):\n                            print(i, end=\' \')\n                            currentBatchSamples = sample_images[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchEyel =  sample_eyel[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchEyer =  sample_eyer[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchNose = sample_nose[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchMouth = sample_mouth[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            samples = self.sess.run(\n                                self.sample_generator,\n                                feed_dict={ self.sample_images: currentBatchSamples,\n                                            self.eyel_sam : currentBatchEyel,\n                                            self.eyer_sam : currentBatchEyer,\n                                            self.nose_sam : currentBatchNose,\n                                            self.mouth_sam : currentBatchMouth\n                                            })\n                            savedtest = save_images(currentBatchSamples if WITHOUT_CODEMAP else currentBatchSamples[...,0:3], [100, 100],\n                                                    \'./{}/{:02d}_{:04d}/train{}_\'.format(config.sample_dir, epoch, idx, i), suffix=\'\')\n                            savedoutput = save_images(samples[5], [100, 100],\n                                                      \'./{}/{:02d}_{:04d}/train{}_\'.format(config.sample_dir, epoch, idx, i),suffix=\'_128\')\n                            savedoutput = save_images(samples[6], [100, 100],\n                                                      \'./{}/{:02d}_{:04d}/train{}_\'.format(config.sample_dir, epoch, idx, i),suffix=\'_64\')\n                            savedoutput = save_images(samples[7], [100, 100],\n                                                      \'./{}/{:02d}_{:04d}/train{}_\'.format(config.sample_dir, epoch, idx, i),suffix=\'_32\')\n                        print(""[{} completed{} and saved {}.]"".format(config.sample_dir, savedtest*self.sample_run_num, savedoutput*self.sample_run_num))\n\n                        #testing accuracy\n                        savedir = \'tem_test\'\n                        if not os.path.exists(savedir):\n                            os.mkdir(savedir)\n                        else:\n                            #subprocess.call([\'rm\' \'-f\' \'tem_test/*\'])\n                            shutil.rmtree(savedir, ignore_errors=True)\n                            os.mkdir(savedir)\n                            print(""cleaned tem_test!"")\n                        listfid = open(\'probef.txt\',\'w\')\n                        for i in range(self.sample_run_num * RANK_MUL):\n                            currentBatchSamples = sample_imagesT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchEyel =  sample_eyelT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchEyer =  sample_eyerT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchNose = sample_noseT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchMouth = sample_mouthT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchLSamples = sample_labelsT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchLEyel =  sample_leyelT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchLEyer =  sample_leyerT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchLNose = sample_lnoseT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchLMouth = sample_lmouthT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            currentBatchIden = sample_idenT[i*self.test_batch_size:(i+1)*self.test_batch_size,...]\n                            if i % (4 * RANK_MUL) == 0:\n                                self.evaluate(epoch, idx, batch_idxs, start_time, \'test\',\n                                              currentBatchSamples, currentBatchEyel, currentBatchEyer, currentBatchNose, currentBatchMouth,\n                                              currentBatchLSamples, currentBatchLEyel, currentBatchLEyer, currentBatchLNose, currentBatchLMouth, currentBatchIden);\n                            samples = self.sess.run(\n                                self.sample_generator,\n                                feed_dict={ self.sample_images: currentBatchSamples,\n                                            self.eyel_sam : currentBatchEyel,\n                                            self.eyer_sam : currentBatchEyer,\n                                            self.nose_sam : currentBatchNose,\n                                            self.mouth_sam : currentBatchMouth\n                                            })\n                            samplevectors = self.samplevector.eval({self.sample_images_nocode : samples[5]})\n                            for k in range(samplevectors.shape[0]):\n                                filename = filenamesT[i*self.test_batch_size + k]\n                                savefilename = filename.replace(\'.png\',\'.feat\')\n                                label = filename[0:3]\n                                listfid.write(savedir + \'/\' + filename +\' \'+ label + \'\\n\')\n                                result = samplevectors[k,:]\n                                f = open(savedir +\'/\'+ savefilename,\'wb\')\n                                f.write(result)\n                                f.close()\n                        listfid.close()\n                        print(""[{}  saved {} feats. calling comparision..]"".format(savedir, self.test_batch_size * self.sample_run_num))\n                        output, err = subprocess.Popen(\'./evaluation_rank.sh\', stdout=subprocess.PIPE, shell=True).communicate()\n                        tobePrint = \'-------!\' + \'\'.join([rank for rank in output.splitlines() if rank.startswith(\'Rank-1\')]) + \'!-------\'\n                        print(err, tobePrint)\n                        self.var_file.write(tobePrint)\n                        self.var_file.flush()\n                    if np.mod(counter, self.save_interval) == self.save_interval-1:\n                        self.save(config.checkpoint_dir, counter)\n        else:\n\n            print(\'test samples reading complete\')\n            batchnum = sample_images.shape[0] // self.test_batch_size #current test batch size\n            savedtest = 0\n            savedoutput = 0\n            sample_dir = \'testall\'\n            for i in range(batchnum):\n                print(\'generating test result batch{}\'.format(i))\n                ind = (i*self.test_batch_size, (i+1)*self.test_batch_size)\n                if self.testimg:#Save images\n                    samples = self.sess.run(\n                        self.sample_generator,\n                        feed_dict={ self.sample_images: sample_images[ind[0]:ind[1],:,:,:],\n                                    self.eyel_sam : sample_eyel[ind[0]:ind[1],...],\n                                    self.eyer_sam : sample_eyer[ind[0]:ind[1],...],\n                                    self.nose_sam : sample_nose[ind[0]:ind[1],...],\n                                    self.mouth_sam : sample_mouth[ind[0]:ind[1],...]}\n                    )\n                    colorgt = sample_images[ind[0]:ind[1],:,:,0:3]\n                    savedtest += save_images(colorgt, [128, 128],\n                                             \'./{}/\'.format(sample_dir),isOutput=False, filelist=filenames[ind[0]:ind[1]])\n                    savedoutput += save_images(samples[5], [128, 128],\n                                               \'./{}/\'.format(sample_dir),isOutput=True, filelist=filenames[ind[0]:ind[1]])\n                    print(""[{} completed{} and saved {}.]"".format(sample_dir, savedtest, savedoutput))\n                else:#save features\n                    savedir = \'testall_f\'#\'gt50_f\'\n                    samples = self.sess.run(\n                        self.sample512,\n                        feed_dict={ self.sample_images: sample_images[ind[0]:ind[1],:,:,:],})\n                    listfid = open(\'probef.txt\',\'a\')\n                    for j in range(self.test_batch_size):\n                        filename = filenames[ind[0]+j]\n                        savefilename = filename.replace(\'.png\',\'.feat\')\n                        label = filename[0:3]\n                        listfid.write(savedir + \'/\' + filename +\' \'+ label + \'\\n\')\n                        result = samples[j,0:448]\n                        if not os.path.exists(savedir):\n                            os.mkdir(savedir)\n                        f = open(savedir +\'/\'+ savefilename,\'wb\')\n                        f.write(result)\n                        f.close()\n                    print(""saved %d files!"" % (self.test_batch_size * (i+1)))\n                    listfid.close()\n\n\n    def processor(self, images, reuse=False):\n        #accept 3 channel images, output orginal 3 channels and 3 x 4 gradient map-> 15 channels\n        with tf.variable_scope(""processor"") as scope:\n            if reuse:\n                scope.reuse_variables()\n            input_dim = images.get_shape()[-1]\n            gradientKernel = gradientweight()\n            output_dim = gradientKernel.shape[-1]\n            print(""processor:"", output_dim)\n            k_hw = gradientKernel.shape[0]\n            init = tf.constant_initializer(value=gradientKernel, dtype=tf.float32)\n            w = tf.get_variable(\'w\', [k_hw, k_hw, input_dim, output_dim],\n                                initializer=init)\n            conv = tf.nn.conv2d(images, w, strides=[1, 1, 1, 1], padding=\'SAME\')\n            #conv = conv * 2\n            return tf.concat_v2([images, conv], 3)\n\n    def FeaturePredict(self, featvec, reuse=False):\n        with tf.variable_scope(""FeaturePredict"") as scope:\n            if reuse:\n                scope.reuse_variables()\n            identitylogits = linear(Dropout(featvec, keep_prob=0.3, is_training= not self.testing), output_size=340, scope=\'idenLinear\', bias_start=0.1, with_w=True)[0]\n            return None, identitylogits, None\n\n    def discriminatorLocal(self, images, reuse=False):\n        with tf.variable_scope(""discriminatorLocal"") as scope:\n            if reuse:\n                scope.reuse_variables()\n\n            h0 = lrelu(conv2d(images, self.df_dim, name=\'d_h0_conv\'))\n            #64\n            h1 = lrelu(batch_norm(conv2d(h0, self.df_dim*2, name=\'d_h1_conv\'), name=\'d_bn1\'))\n            #32\n            h2 = lrelu(batch_norm(conv2d(h1, self.df_dim*4, name=\'d_h2_conv\'), name=\'d_bn2\'))\n            #16\n            h3 = lrelu(batch_norm(conv2d(h2, self.df_dim*8, name=\'d_h3_conv\'), name=\'d_bn3\'))\n            # #8x8\n            h3r1 = resblock(h3, name = ""d_h3_conv_res1"")\n            h4 = lrelu(batch_norm(conv2d(h3r1, self.df_dim*8, name=\'d_h4_conv\'), name=\'d_bn4\'))\n            h4r1 = resblock(h4, name = ""d_h4_conv_res1"")\n            h5 = conv2d(h4r1, 1, k_h=1, k_w=1, d_h=1, d_w=1, name=\'d_h5_conv\')\n            h6 = tf.reshape(h5, [self.batch_size, -1])\n            #fusing 512 feature map to one layer prediction.\n            return h6, h6 #tf.nn.sigmoid(h6), h6\n\n\n    def decoder(self, feat128, feat64, feat32, feat16, feat8, featvec,\n                g128_images_with_code, g64_images_with_code, g32_images_with_code,\n                eyel, eyer, nose, mouth, c_eyel, c_eyer, c_nose, c_mouth, batch_size = 10, name=""decoder"", reuse = False):\n        sel_feat_capacity = self.gf_dim\n        with tf.variable_scope(name) as scope:\n            if reuse:\n                scope.reuse_variables()\n            initial_all = tf.concat_v2([featvec, self.z], 1)\n            initial_8 = relu(tf.reshape(linear(initial_all, output_size=8*8*self.gf_dim,scope=\'initial8\', bias_start=0.1, with_w=True)[0],\n                                        [batch_size, 8, 8, self.gf_dim]))\n            initial_32 = relu(deconv2d(initial_8, [batch_size, 32, 32, self.gf_dim // 2], d_h=4, d_w=4, name=""initial32""))\n            initial_64 = relu(deconv2d(initial_32, [batch_size, 64, 64, self.gf_dim // 4], name=""initial64""))\n            initial_128 = relu(deconv2d(initial_64, [batch_size, 128, 128, self.gf_dim // 8], name=""initial128""))\n\n            before_select8 = resblock(tf.concat_v2([initial_8, feat8], 3), k_h=2, k_w=2, name = ""select8_res_1"")\n            #selection T module\n            reconstruct8 = resblock(resblock(before_select8, k_h=2, k_w=2, name=""dec8_res1""), k_h=2, k_w=2, name=""dec8_res2"")\n\n            #selection F module\n            reconstruct16_deconv = relu(batch_norm(deconv2d(reconstruct8, [batch_size, 16, 16, self.gf_dim*8], name=""g_deconv16""), name=""g_bnd1""))\n            before_select16 = resblock(feat16, name = ""select16_res_1"")\n            reconstruct16 = resblock(resblock(tf.concat_v2([reconstruct16_deconv, before_select16], 3), name=""dec16_res1""), name=""dec16_res2"")\n\n            reconstruct32_deconv = relu(batch_norm(deconv2d(reconstruct16, [batch_size, 32, 32, self.gf_dim*4], name=""g_deconv32""), name=""g_bnd2""))\n            before_select32 = resblock(tf.concat_v2([feat32, g32_images_with_code, initial_32], 3), name = ""select32_res_1"")\n            reconstruct32 = resblock(resblock(tf.concat_v2([reconstruct32_deconv, before_select32], 3), name=""dec32_res1""), name=""dec32_res2"")\n            img32 = tf.nn.tanh(conv2d(reconstruct32, 3, d_h=1, d_w=1, name=""check_img32""))\n\n            reconstruct64_deconv = relu(batch_norm(deconv2d(reconstruct32, [batch_size, 64, 64, self.gf_dim*2], name=""g_deconv64""), name=""g_bnd3""))\n            before_select64 = resblock(tf.concat_v2([feat64, g64_images_with_code, initial_64], 3), k_h=5, k_w=5, name = ""select64_res_1"")\n            reconstruct64 = resblock(resblock(tf.concat_v2([reconstruct64_deconv, before_select64,\n                                                            tf.image.resize_bilinear(img32, [64,64])], 3), name=""dec64_res1""), name=""dec64_res2"")\n            img64 = tf.nn.tanh(conv2d(reconstruct64, 3, d_h=1, d_w=1, name=""check_img64""))\n\n            reconstruct128_deconv = relu(batch_norm(deconv2d(reconstruct64, [batch_size, 128, 128, self.gf_dim], name=""g_deconv128""), name=""g_bnd4""))\n            before_select128 = resblock(tf.concat_v2([feat128, initial_128, g128_images_with_code],3), k_h = 7, k_w = 7, name = ""select128_res_1"")\n            reconstruct128 = resblock(tf.concat_v2([reconstruct128_deconv, before_select128,\n                                                    self.partCombiner(eyel, eyer, nose, mouth),\n                                                    self.partCombiner(c_eyel, c_eyer, c_nose, c_mouth),\n                                                    tf.image.resize_bilinear(img64, [128,128])], 3), k_h=5, k_w=5, name=""dec128_res1"")\n            reconstruct128_1 = lrelu(batch_norm(conv2d(reconstruct128, self.gf_dim, k_h=5, k_w=5, d_h=1, d_w=1, name=""recon128_conv""), name=""recon128_bnc""))\n            reconstruct128_1_r = resblock(reconstruct128_1, name=""dec128_res2"")\n            reconstruct128_2 = lrelu(batch_norm(conv2d(reconstruct128_1_r, self.gf_dim/2, d_h=1, d_w=1, name=""recon128_conv2""),name=""recon128_bnc2""))\n            img128 = tf.nn.tanh(conv2d(reconstruct128_2, 3, d_h=1, d_w=1, name=""check_img128""))\n\n            return img128, img64, img32, img32, img32, img128, img64, img32\n\n    def generator(self, images, batch_size, name = ""generator"", reuse = False):\n        with tf.variable_scope(name) as scope:\n            if reuse:\n                scope.reuse_variables()\n        # imgs: input: IMAGE_SIZE x IMAGE_SIZE x CHANNEL\n        # return labels: IMAGE_SIZE x IMAGE_SIZE x 3\n        # U-Net structure, slightly different from the original on the location of relu/lrelu\n            #128x128\n            c0 = lrelu(conv2d(images, self.gf_dim, k_h=7, k_w=7, d_h=1, d_w=1, name=""g_conv0""))\n            c0r = resblock(c0, k_h=7, k_w=7, name=""g_conv0_res"")\n            c1 = lrelu(batch_norm(conv2d(c0r, self.gf_dim, k_h=5, k_w=5, name=""g_conv1""),name=""g_bnc1""))\n            #64x64\n            c1r = resblock(c1, k_h=5, k_w=5, name=""g_conv1_res"")\n            c2 = lrelu(batch_norm(conv2d(c1r, self.gf_dim*2, name=\'g_conv2\'),name=""g_bnc2""))\n            #32x32\n            c2r = resblock(c2, name=""g_conv2_res"")\n            c3 = lrelu(batch_norm(conv2d(c2r, self.gf_dim*4, name=\'g_conv3\'),name=""g_bnc3""))\n            #16x16\n            c3r = resblock(c3, name=""g_conv3_res"")\n            c4 = lrelu(batch_norm(conv2d(c3r, self.gf_dim*8, name=\'g_conv4\'),name=""g_bnc4""))\n            #8x8\n            c4r = resblock(c4, name=""g_conv4_res"")\n            # c5 = lrelu(batch_norm(conv2d(c4r, self.gf_dim*8, name=\'g_conv5\'),name=""g_bnc5""))\n            # #4x4\n            # #2x2\n            # c6r = resblock(c6,k_h=2, k_w=2, name=""g_conv6_res"")\n            c4r2 = resblock(c4r, name=""g_conv4_res2"")\n            c4r3 = resblock(c4r2, name=""g_conv4_res3"")\n            c4r4 = resblock(c4r3, name=""g_conv4_res4"")\n            c4r4_l = tf.reshape(c4r4,[batch_size, -1])\n            c7_l = linear(c4r4_l, output_size=512,scope=\'feature\', bias_start=0.1, with_w=True)[0]\n            c7_l_m = tf.maximum(c7_l[:, 0:256], c7_l[:, 256:])\n\n            return c0r, c1r, c2r, c3r, c4r4, c7_l_m\n\n    def partRotator(self, images, name, batch_size=10, reuse=False):\n        #HW 40x40, 32x40, 32x48\n        with tf.variable_scope(name) as scope:\n            if reuse:\n                scope.reuse_variables()\n            c0 = lrelu(conv2d(images, self.gf_dim, d_h=1, d_w=1, name=""p_conv0""))\n            c0r = resblock(c0, name=""p_conv0_res"")\n            c1 = lrelu(batch_norm(conv2d(c0r, self.gf_dim*2, name=""p_conv1""),name=""p_bnc1""))\n            #down1\n            c1r = resblock(c1, name=""p_conv1_res"")\n            c2 = lrelu(batch_norm(conv2d(c1r, self.gf_dim*4, name=\'p_conv2\'),name=""p_bnc2""))\n            #down2\n            c2r = resblock(c2, name=""p_conv2_res"")\n            c3 = lrelu(batch_norm(conv2d(c2r, self.gf_dim*8, name=\'p_conv3\'),name=""p_bnc3""))\n            #down3 5x5, 4x5, 4x6\n            c3r = resblock(c3, name=""p_conv3_res"")\n            c3r2 = resblock(c3r, name=""p_conv3_res2"")\n\n            shape = c3r2.get_shape().as_list()\n            d1 = lrelu(batch_norm(deconv2d(c3r2, [shape[0], shape[1] * 2, shape[2] * 2, self.gf_dim*4], name=""p_deconv1""), name=""p_bnd1""))\n            #up1\n            after_select_d1 = lrelu(batch_norm(conv2d(tf.concat_v2([d1, c2r], axis=3), self.gf_dim*4, d_h=1, d_w=1, name=""p_deconv1_s""),name=""p_bnd1_s""))\n            d1_r = resblock(after_select_d1, name=""p_deconv1_res"")\n            d2 = lrelu(batch_norm(deconv2d(d1_r, [shape[0], shape[1] * 4, shape[2] * 4, self.gf_dim*2], name=""p_deconv2""), name=""p_bnd2""))\n            #up2\n            after_select_d2 = lrelu(batch_norm(conv2d(tf.concat_v2([d2, c1r], axis=3), self.gf_dim*2, d_h=1, d_w=1, name=""p_deconv2_s""),name=""p_bnd2_s""))\n            d2_r = resblock(after_select_d2, name=""p_deconv2_res"")\n            d3 = lrelu(batch_norm(deconv2d(d2_r, [shape[0], shape[1] * 8, shape[2] * 8, self.gf_dim], name=""p_deconv3""), name=""p_bnd3""))\n            #up3\n            after_select_d3 = lrelu(batch_norm(conv2d(tf.concat_v2([d3, c0r], axis=3), self.gf_dim, d_h=1, d_w=1, name=""p_deconv3_s""),name=""p_bnd3_s""))\n            d3_r = resblock(after_select_d3, name=""p_deconv3_res"")\n\n            check_part = tf.nn.tanh(conv2d(d3_r, 3, d_h=1, d_w=1, name=""p_check""))\n\n        return d3_r, check_part\n\n    def partCombiner(self, eyel, eyer, nose, mouth):\n        \'\'\'\n        x         y\n        43.5823   41.0000\n        86.4177   41.0000\n        64.1165   64.7510\n        47.5863   88.8635\n        82.5904   89.1124\n        this is the mean locaiton of 5 landmarks\n        \'\'\'\n        eyel_p = tf.pad(eyel, [[0,0], [int(41 - EYE_H / 2 - 1), int(IMAGE_SIZE - (41+EYE_H/2 - 1))], [int(44 - EYE_W / 2 - 1), int(IMAGE_SIZE - (44+EYE_W/2-1))], [0,0]])\n        eyer_p = tf.pad(eyer, [[0,0], [int(41 - EYE_H / 2 - 1), int(IMAGE_SIZE - (41+EYE_H/2 - 1))], [int(86 - EYE_W / 2 - 1), int(IMAGE_SIZE - (86+EYE_W/2-1))], [0,0]])\n        nose_p = tf.pad(nose, [[0,0], [int(65 - NOSE_H / 2 - 1), int(IMAGE_SIZE - (65+NOSE_H/2 - 1))], [int(64 - NOSE_W / 2 - 1), int(IMAGE_SIZE - (64+NOSE_W/2-1))], [0,0]])\n        month_p = tf.pad(mouth, [[0,0], [int(89 - MOUTH_H / 2 - 1), int(IMAGE_SIZE - (89+MOUTH_H/2 - 1))], [int(65 - MOUTH_W / 2 - 1), int(IMAGE_SIZE - (65+MOUTH_W/2-1))], [0,0]])\n        eyes = tf.maximum(eyel_p, eyer_p)\n        eye_nose = tf.maximum(eyes, nose_p)\n        return tf.maximum(eye_nose, month_p)\n    def evaluate(self,epoch, idx, batch_idxs, start_time, mode,\n                 batch_images_with_code,  batch_eyel, batch_eyer, batch_nose, batch_mouth,\n                 batch_labels, batch_eyel_label, batch_eyer_label, batch_nose_label, batch_mouth_label, batch_iden):\n        errD = self.d_loss.eval({self.images_with_code: batch_images_with_code, self.labels: batch_labels,\n                                 self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n\n        errG_L = self.weightedErrL1.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels,\n                                          self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n        errG_L2 = self.weightedErrL2.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels,\n                                           self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n        errG_L3 = self.weightedErrL3.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels,\n                                           self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n        errG_adver = self.g_loss_adver.eval({self.images_with_code: batch_images_with_code,\n                                             self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n        errtv = self.tv_loss.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels,\n                                   self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n\n        errG_sym = self.symErrL1.eval({self.images_with_code: batch_images_with_code,\n                                       self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n        errG2_sym = self.symErrL2.eval({self.images_with_code: batch_images_with_code,\n                                        self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n        errG3_sym = self.symErrL3.eval({self.images_with_code: batch_images_with_code,\n                                        self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n\n        errcheck32 = 0#self.weightedErr_check32.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels, })\n        errcheck64 = 0#self.weightedErr_check64.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels, })\n        errcheck128 = 0#self.weightedErr_check128.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels, })\n\n        erreyel = self.eyel_loss.eval({self.eyel: batch_eyel, self.eyel_label: batch_eyel_label })\n        erreyer = self.eyer_loss.eval({self.eyel: batch_eyel, self.eyer: batch_eyer, self.eyer_label: batch_eyer_label})\n        errnose = self.nose_loss.eval({self.nose: batch_nose, self.nose_label: batch_nose_label})\n        errmouth = self.mouth_loss.eval({self.mouth : batch_mouth, self.mouth_label: batch_mouth_label})\n        erriden = self.idenloss.eval({self.images_with_code: batch_images_with_code, self.idenlabels : batch_iden,})\n\n\n        if \'f\' in MODE:\n            errDv = self.dv_loss.eval({self.images_with_code: batch_images_with_code, self.labels : batch_labels,\n                                       self.eyel : batch_eyel, self.eyer : batch_eyer, self.nose: batch_nose, self.mouth: batch_mouth,})\n        else:\n            errDv = 0\n        err_total_G = L1_1_W * (errG_L + errG_sym * SYM_W) + L1_2_W * (errG_L2 + errG2_sym * SYM_W) + L1_3_W * (errG_L3 + errG3_sym * SYM_W) \\\n                      + ALPHA_ADVER * errG_adver + errDv * BELTA_FEATURE + IDEN_W*erriden #+ COND_WEIGHT*(errCondL12 + errCondL23) + errtv * TV_WEIGHT\n        errfeat_total = errcheck32 + errcheck64+errcheck128 + PART_W * (erreyel + erreyer + errnose + errmouth)\n        tobePrint = ""%s Epo[%2d][%4d/%4d][t%4.2f] d_l:%.4f"" % (MODE + \'T\' if mode == \'test\' else \'\', epoch, idx, batch_idxs, time.time() - start_time, errD)\n        tobePrint += "" fstol:%.0f cg32:%.0f cg64:%.0f cg128:%.0f el:%.0f er:%.0f no:%.0f mo:%.0f\\n"" % \\\n                     (errfeat_total, errcheck32, errcheck64,errcheck128, erreyel, erreyer, errnose, errmouth)\n        tobePrint += ""g_l:%.0f gL1:%.0f(sym:%.0f) gL2:%.0f(sym:%.0f) gL3:%.0f(sym:%.0f) gadv:%.4f dv_l:%.2f iden:%.4f, tv:%.0f "" \\\n                     % (err_total_G, errG_L, errG_sym, errG_L2, errG2_sym, errG_L3, errG3_sym, errG_adver, errDv,  erriden, errtv)\n        if \'f\' in MODE:\n            tobePrint += \'L:{} G:{}\'.format(ALPHA_ADVER, BELTA_FEATURE)\n        self.f.write(tobePrint+\'\\n\')\n        self.f.flush()\n        print(tobePrint)\n    #DEEPFACE MODEL BEGINS---\n    def loadDeepFace(self, DeepFacePath):\n        if DeepFacePath is None:\n            path = sys.modules[self.__class__.__module__].__file__\n            # print path\n            path = os.path.abspath(os.path.join(path, os.pardir))\n            # print path\n            path = os.path.join(path, ""DeepFace.pickle"")\n            DeepFacePath = path\n            logging.info(""Load npy file from \'%s\'."", DeepFacePath)\n        if not os.path.isfile(DeepFacePath):\n            logging.error((""File \'%s\' not found. ""), DeepFacePath)\n            sys.exit(1)\n        with open(DeepFacePath,\'r\') as file:\n            self.data_dict = pickle.load(file)\n        print(""Deep Face pickle data file loaded"")\n\n    def FeatureExtractDeepFace(self, images, name = ""FeatureExtractDeepFace"", reuse=False):\n        #Preprocessing: from color to gray(reduce_mean)\n        with tf.variable_scope(name) as scope:\n            if reuse:\n                scope.reuse_variables()\n\n            conv1 = self._conv_layer(images, name=\'conv1\')\n            print(3, type(3))\n            slice1_1, slice1_2 = tf.split(3, 2, conv1)\n            eltwise1 = tf.maximum(slice1_1, slice1_2)\n            pool1 = tf.nn.max_pool(eltwise1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                                   padding=\'SAME\')\n            conv2_1 = self._conv_layer(pool1, name=\'conv2_1\')\n            slice2_1_1, slice2_1_2 = tf.split(3, 2, conv2_1)\n            eltwise2_1 = tf.maximum(slice2_1_1, slice2_1_2)\n\n            conv2_2 = self._conv_layer(eltwise2_1, name=\'conv2_2\')\n            slice2_2_1, slice2_2_2 = tf.split(3, 2, conv2_2)\n            eltwise2_2 = tf.maximum(slice2_2_1, slice2_2_2)\n\n            res2_1 = pool1 + eltwise2_2\n\n            conv2a = self._conv_layer(res2_1, name=\'conv2a\')\n            slice2a_1, slice2a_2 = tf.split(3, 2, conv2a)\n            eltwise2a = tf.maximum(slice2a_1, slice2a_2)\n\n            conv2 = self._conv_layer(eltwise2a, name=\'conv2\')\n            slice2_1, slice2_2 = tf.split(3, 2, conv2)\n            eltwise2 = tf.maximum(slice2_1, slice2_2)\n\n            pool2 = tf.nn.max_pool(eltwise2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                                   padding=\'SAME\')\n\n            conv3_1 = self._conv_layer(pool2, name=\'conv3_1\')\n            slice3_1_1, slice3_1_2 = tf.split(3, 2, conv3_1)\n            eltwise3_1 = tf.maximum(slice3_1_1, slice3_1_2)\n\n            conv3_2 = self._conv_layer(eltwise3_1, name=\'conv3_2\')\n            slice3_2_1, slice3_2_2 = tf.split(3, 2, conv3_2)\n            eltwise3_2 = tf.maximum(slice3_2_1, slice3_2_2)\n\n            res3_1 = pool2 + eltwise3_2\n\n            conv3_3 = self._conv_layer(res3_1, name=\'conv3_3\')\n            slice3_3_1, slice3_3_2 = tf.split(3, 2, conv3_3)\n            eltwise3_3 = tf.maximum(slice3_3_1, slice3_3_2)\n\n            conv3_4 = self._conv_layer(eltwise3_3, name=\'conv3_4\')\n            slice3_4_1, slice3_4_2 = tf.split(3, 2, conv3_4)\n            eltwise3_4 = tf.maximum(slice3_4_1, slice3_4_2)\n\n            res3_2 = res3_1 + eltwise3_4\n\n            conv3a = self._conv_layer(res3_2, name=\'conv3a\')\n            slice3a_1, slice3a_2 = tf.split(3, 2, conv3a)\n            eltwise3a = tf.maximum(slice3a_1, slice3a_2)\n\n            conv3 = self._conv_layer(eltwise3a, name=\'conv3\')\n            slice3_1, slice3_2 = tf.split(3, 2, conv3)\n            eltwise3 = tf.maximum(slice3_1, slice3_2)\n\n            pool3 = tf.nn.max_pool(eltwise3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                                   padding=\'SAME\')\n\n            conv4_1 = self._conv_layer(pool3, name=\'conv4_1\')\n            slice4_1_1, slice4_1_2 = tf.split(3, 2, conv4_1)\n            eltwise4_1 = tf.maximum(slice4_1_1, slice4_1_2)\n\n            conv4_2 = self._conv_layer(eltwise4_1, name=\'conv4_2\')\n            slice4_2_1, slice4_2_2 = tf.split(3, 2, conv4_2)\n            eltwise4_2 = tf.maximum(slice4_2_1, slice4_2_2)\n\n            res4_1 = pool3 + eltwise4_2\n\n            conv4_3 = self._conv_layer(res4_1, name=\'conv4_3\')\n            slice4_3_1, slice4_3_2 = tf.split(3, 2, conv4_3)\n            eltwise4_3 = tf.maximum(slice4_3_1, slice4_3_2)\n\n            conv4_4 = self._conv_layer(eltwise4_3, name=\'conv4_4\')\n            slice4_4_1, slice4_4_2 = tf.split(3, 2, conv4_4)\n            eltwise4_4 = tf.maximum(slice4_4_1, slice4_4_2)\n\n            res4_2 = res4_1 + eltwise4_4\n\n            conv4_5 = self._conv_layer(res4_2, name=\'conv4_5\')\n            slice4_5_1, slice4_5_2 = tf.split(3, 2, conv4_5)\n            eltwise4_5 = tf.maximum(slice4_5_1, slice4_5_2)\n\n            conv4_6 = self._conv_layer(eltwise4_5, name=\'conv4_6\')\n            slice4_6_1, slice4_6_2 = tf.split(3, 2, conv4_6)\n            eltwise4_6 = tf.maximum(slice4_6_1, slice4_6_2)\n\n            res4_3 = res4_2 + eltwise4_6\n\n            conv4a = self._conv_layer(res4_3, name=\'conv4a\')\n            slice4a_1, slice4a_2 = tf.split(3, 2, conv4a)\n            eltwise4a = tf.maximum(slice4a_1, slice4a_2)\n\n            conv4 = self._conv_layer(eltwise4a, name=\'conv4\')\n            slice4_1, slice4_2 = tf.split(3, 2, conv4)\n            eltwise4 = tf.maximum(slice4_1, slice4_2)\n\n            conv5_1 = self._conv_layer(eltwise4, name=\'conv5_1\')\n            slice5_1_1, slice5_1_2 = tf.split(3, 2, conv5_1)\n            eltwise5_1 = tf.maximum(slice5_1_1, slice5_1_2)\n\n            conv5_2 = self._conv_layer(eltwise5_1, name=\'conv5_2\')\n            slice5_2_1, slice5_2_2 = tf.split(3, 2, conv5_2)\n            eltwise5_2 = tf.maximum(slice5_2_1, slice5_2_2)\n\n            res5_1 = eltwise4 + eltwise5_2\n\n            conv5_3 = self._conv_layer(res5_1, name=\'conv5_3\')\n            slice5_3_1, slice5_3_2 = tf.split(3, 2, conv5_3)\n            eltwise5_3 = tf.maximum(slice5_3_1, slice5_3_2)\n\n            conv5_4 = self._conv_layer(eltwise5_3, name=\'conv5_4\')\n            slice5_4_1, slice5_4_2 = tf.split(3, 2, conv5_4)\n            eltwise5_4 = tf.maximum(slice5_4_1, slice5_4_2)\n\n            res5_2 = res5_1 + eltwise5_4\n\n            conv5_5 = self._conv_layer(res5_2, name=\'conv5_5\')\n            slice5_5_1, slice5_5_2 = tf.split(3, 2, conv5_5)\n            eltwise5_5 = tf.maximum(slice5_5_1, slice5_5_2)\n\n            conv5_6 = self._conv_layer(eltwise5_5, name=\'conv5_6\')\n            slice5_6_1, slice5_6_2 = tf.split(3, 2, conv5_6)\n            eltwise5_6 = tf.maximum(slice5_6_1, slice5_6_2)\n\n            res5_3 = res5_2 + eltwise5_6\n\n            conv5_7 = self._conv_layer(res5_3, name=\'conv5_7\')\n            slice5_7_1, slice5_7_2 = tf.split(3, 2, conv5_7)\n            eltwise5_7 = tf.maximum(slice5_7_1, slice5_7_2)\n\n            conv5_8 = self._conv_layer(eltwise5_7, name=\'conv5_8\')\n            slice5_8_1, slice5_8_2 = tf.split(3, 2, conv5_8)\n            eltwise5_8 = tf.maximum(slice5_8_1, slice5_8_2)\n\n            res5_4 = res5_3 + eltwise5_8\n\n            conv5a = self._conv_layer(res5_4, name=\'conv5a\')\n            slice5a_1, slice5a_2 = tf.split(3, 2, conv5a)\n            eltwise5a = tf.maximum(slice5a_1, slice5a_2)\n\n            conv5 = self._conv_layer(eltwise5a, name=\'conv5\')\n            slice5_1, slice5_2 = tf.split(3, 2, conv5)\n            eltwise5 = tf.maximum(slice5_1, slice5_2)\n            pool4 = tf.nn.max_pool(eltwise5, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                                   padding=\'SAME\')\n            pool4_transposed = tf.transpose(pool4, perm=[0, 3, 1, 2])\n            # pool4_reshaped = tf.reshape(pool4_transposed, shape=[tf.shape(pool4)[0],-1])\n            fc1 = self._fc_layer(pool4_transposed, name=""fc1"")\n            slice_fc1_1, slice_fc1_2 = tf.split(1, 2, fc1)\n            eltwise_fc1 = tf.maximum(slice_fc1_1, slice_fc1_2)\n\n            return eltwise1, eltwise2, eltwise3, eltwise5, pool4, eltwise_fc1\n        #DEEPFACE NET ENDS---\n\n        #DEEPFACE OPS BEGINS---\n    def _conv_layer(self, input_, output_dim=96,\n                    k_h=3, k_w=3, d_h=1, d_w=1, stddev=0.02,\n                    name=""conv2d""):\n        #Note: currently kernel size and input output channel num are decided by loaded filter weights.\n        #only strides are decided by calling param.\n        with tf.variable_scope(name) as scope:\n            filt = self.get_conv_filter(name)\n            conv = tf.nn.conv2d(input_, filt, strides=[1, d_h, d_w, 1], padding=\'SAME\')\n            conv_biases = self.get_bias(name)\n            return tf.nn.bias_add(conv, conv_biases)\n            return conv\n\n    def _fc_layer(self, bottom, name=""fc1"", num_classes=None):\n        with tf.variable_scope(name) as scope:\n            #shape = bottom.get_shape().as_list()\n            if name == \'fc1\':\n                filt = self.get_fc_weight(name)\n                bias = self.get_bias(name)\n            reshaped_bottom = tf.reshape(bottom,[tf.shape(bottom)[0],-1])\n            return tf.matmul(reshaped_bottom, filt) + bias\n\n    def get_conv_filter(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        var = tf.get_variable(name=""filter"", initializer=init, shape=shape)\n        return var\n\n    def get_bias(self, name, num_classes=None):\n        bias_wights = self.data_dict[name][1]\n        shape = self.data_dict[name][1].shape\n        init = tf.constant_initializer(value=bias_wights,\n                                       dtype=tf.float32)\n        var = tf.get_variable(name=""biases"", initializer=init, shape=shape)\n        return var\n\n    def get_fc_weight(self, name):\n        init = tf.constant_initializer(value=self.data_dict[name][0],\n                                       dtype=tf.float32)\n        shape = self.data_dict[name][0].shape\n        var = tf.get_variable(name=""weights"", initializer=init, shape=shape)\n        return var\n\n    #DEEPFACE OPS ENDS---\n\n    def save(self, checkpoint_dir, step):\n        model_name = ""DCGAN.model""\n        model_dir = ""%s_%s_%s"" % (self.dataset_name, self.batch_size, self.output_size)\n        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        print("" [*] Saving checkpoints...at step "" + str(step))\n        self.saver.save(self.sess,\n                        os.path.join(checkpoint_dir, model_name),\n                        global_step=step)\n\n    def load(self, checkpoint_dir):\n        print("" [*] Reading checkpoints..."")\n\n        model_dir = ""%s_%s_%s"" % (self.dataset_name, self.batch_size, self.output_size)\n        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n            print("" [*] Success to read {}"".format(ckpt_name))\n            return True\n        else:\n            print("" [*] Failed to find a checkpoint"")\n            return False\n\ndef main(_):\n    pp.pprint(flags.FLAGS.__flags)\n\n    if not os.path.exists(FLAGS.checkpoint_dir):\n        os.makedirs(FLAGS.checkpoint_dir)\n    if not os.path.exists(FLAGS.sample_dir):\n        os.makedirs(FLAGS.sample_dir)\n\n    with tf.Session() as sess:\n        dcgan = DCGAN(sess,\n                      image_size=FLAGS.image_size,\n                      batch_size=FLAGS.batch_size,\n                      output_size=FLAGS.output_size,\n                      c_dim=FLAGS.c_dim,\n                      dataset_name=FLAGS.dataset,\n                      is_crop=FLAGS.is_crop,\n                      checkpoint_dir=FLAGS.checkpoint_dir,\n                      sample_dir=FLAGS.sample_dir)\n        dcgan.train(FLAGS)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
net_input_everything_featparts.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Routine for decoding the CIFAR-10 binary file format.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport random\nimport re\nimport csv\n\n# Process images of this size. Note that this differs from the original CIFAR\n# image size of 32 x 32. If one alters this number, then the entire model\n# architecture will change and any model would need to be retrained.\nIMAGE_SIZE = 128\n# Global constants describing the CIFAR-10 data set.\nEYE_H = 40; EYE_W = 40;\nNOSE_H = 32; NOSE_W = 40;\nMOUTH_H = 32; MOUTH_W = 48;\nre_pose = re.compile(\'_\\d{3}_\')\nre_poseIllum = re.compile(\'_\\d{3}_\\d{2}_\')\n\nclass MultiPIE():\n    """"""Reads and parses examples from MultiPIE data filelist\n    """"""\n    def __init__(self, datasplit=\'train\', Random=True, LOAD_60_LABEL=False, MIRROR_TO_ONE_SIDE=True, RANDOM_VERIFY=False,\n                 GENERATE_MASK=False, source=\'without90\', testing = False):\n        self.dir = \'/home/ruihuang/data/FS_aligned/\'\n        self.csvpath = \'/home/ruihuang/data/{}/{}.csv\'\n        self.feat5ptDir = \'/home/shu.zhang/ruihuang/data/FS_t5pt/\'\n        self.test_dir = \'/home/shu.zhang/ruihuang/data/testlist/FS/{}.csv\'\n        self.testing = testing\n\n        self.split = datasplit\n        self.random = Random\n        self.seed = None\n        self.LOAD_60_LABEL = LOAD_60_LABEL\n        self.MIRROR_TO_ONE_SIDE = MIRROR_TO_ONE_SIDE\n        self.RANDOM_VERIFY = RANDOM_VERIFY\n        self.GENERATE_MASK = GENERATE_MASK\n        self.cameraPositions = {\'24_0\': (+90, \'10\'),\'01_0\' : (+75, \'08\'), \'20_0\' : (+60, \'08\'), \'19_0\' : (+45, \'09\'), \'04_1\' : (+30, \'07\'), \'05_0\' : (+15, \'06\'), #left\n                    \'05_1\' : (0,\'06\'), #center\n                    \'14_0\' : (-15,\'06\'), \'13_0\' : (-30, \'05\'), \'08_0\' : (-45, \'15\'),\'09_0\' : (-60, \'15\'),\'12_0\' : (-75, \'15\'),\'11_0\' : (-90, \'15\')} #right\n\n        if not testing:\n            split_f = self.csvpath.format(source, self.split)\n            split_f_test = self.csvpath.format(source, \'test\')\n            self.indices = open(split_f, \'r\').read().splitlines()\n            self.indices_test = open(split_f_test, \'r\').read().splitlines()\n            self.size = len(self.indices)\n            self.test_size = len(self.indices_test)\n            # make eval deterministic\n            if \'train\' not in self.split:\n                self.random = False\n            # randomization: seed and pick\n            if self.random:\n                random.seed(self.seed)\n                self.idx = random.randint(0, len(self.indices)-1)\n        else:#only load test images for a separate list file.\n            split_f_test = self.test_dir.format(source)\n            self.indices_test = open(split_f_test, \'r\').read().splitlines()\n            self.size = 0\n            self.test_size = len(self.indices_test)\n        self.idx = 0\n\n\n    def test_batch(self, test_batch_size=100,Random = True, Pose = -1):\n        test_batch_size = min(test_batch_size, len(self.indices_test))\n        images = np.empty([test_batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n        eyel = np.empty([test_batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n        eyer = np.empty([test_batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n        nose = np.empty([test_batch_size, NOSE_H, NOSE_W, 3], dtype=np.float32)\n        mouth = np.empty([test_batch_size, MOUTH_H, MOUTH_W, 3],dtype=np.float32)\n        if not self.testing:\n            idenlabels = np.empty([test_batch_size], dtype=np.int32)\n            leyel = np.empty([test_batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n            leyer = np.empty([test_batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n            lnose = np.empty([test_batch_size, NOSE_H, NOSE_W, 3], dtype=np.float32)\n            lmouth = np.empty([test_batch_size, MOUTH_H, MOUTH_W, 3],dtype=np.float32)\n            labels = np.empty([test_batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n        filenames = list()\n        if Random:\n            random.seed(2017)#make testing batch deterministic\n            random.shuffle(self.indices_test)\n            #resume randomeness for training\n            random.seed(self.seed)\n        j = 0\n        for i in range(test_batch_size):\n            if self.LOAD_60_LABEL:\n                while True:\n                    pose = abs(self.findPose(self.indices_test[j % len(self.indices_test)]))\n                    if pose >= 45:\n                            break\n                    j += 1\n            if Pose != -1:\n                while True:\n                    pose = abs(self.findPose(self.indices_test[j % len(self.indices_test)]))\n                    if pose == Pose:\n                        break\n                    j += 1\n            print(j, end=\' \')\n            images[i, ...], feats = self.load_image(self.indices_test[j % len(self.indices_test)])\n            eyel[i,...] = feats[1]\n            eyer[i,...] = feats[2]\n            nose[i,...] = feats[3]\n            mouth[i, ...] = feats[4]\n            filename = self.indices_test[j % len(self.indices_test)]\n            filenames.append(filename)\n            if not self.testing:\n                labels[i,...], _, leyel[i,...], leyer[i,...], lnose[i,...], lmouth[i, ...] = self.load_label_mask(filename)\n                identity = int(filename[0:3])\n                idenlabels[i] = identity\n            j += 1\n        print(\'\\n\')\n        if not self.testing:\n            return images, filenames, eyel, eyer, nose, mouth,\\\n            labels, leyel, leyer, lnose, lmouth, idenlabels\n        else:\n            return images, filenames, eyel, eyer, nose, mouth, None, None, None, None, None, None\n\n    def next_image_and_label_mask_batch(self, batch_size, imageRange=-1,imageRangeLow = 0, labelnum=None):\n        """"""Construct a batch of images and labels masks.\n\n        Args:\n        batch_size: Number of images per batch.\n        shuffle: boolean indicating whether to use a shuffling queue.\n        Returns:\n        ndarray feed.\n        images: Images. 4D of [batch_size, height, width, 6] size.\n        labels: Labels. 4D of [batch_size, height, width, 3] size.\n        masks: masks. 4D of [batch_size, height, width, 3] size.\n        verifyImages: Images. 4D of [batch_size, height, width, 3] size.\n        verifyLabels: 1D of [batch_size] 0 / 1 classification label\n        """"""\n        assert batch_size >= 1\n        images = np.empty([batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n        labels = np.empty([batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n\n        poselabels = np.empty([batch_size],dtype=np.int32)\n        idenlabels = np.empty([batch_size],dtype=np.int32)\n        landmarklabels = np.empty([batch_size, 5*2],dtype=np.float32)\n        eyel = np.empty([batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n        eyer = np.empty([batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n        nose = np.empty([batch_size, NOSE_H, NOSE_W, 3], dtype=np.float32)\n        mouth = np.empty([batch_size, MOUTH_H, MOUTH_W, 3],dtype=np.float32)\n        leyel = np.empty([batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n        leyer = np.empty([batch_size, EYE_H, EYE_W, 3], dtype=np.float32)\n        lnose = np.empty([batch_size, NOSE_H, NOSE_W, 3], dtype=np.float32)\n        lmouth = np.empty([batch_size, MOUTH_H, MOUTH_W, 3],dtype=np.float32)\n\n        if self.GENERATE_MASK:\n            masks = np.empty([batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n        else:\n            masks = None\n        if self.RANDOM_VERIFY:\n            verifyImages = np.empty([batch_size, IMAGE_SIZE, IMAGE_SIZE, 3])\n            #verifyLabels = np.empty([batch_size, 1], dtype=np.float32)\n            verifyLabels = np.empty([batch_size], dtype=np.int32)\n        else:\n            verifyImages = None; verifyLabels = None\n\n        for i in range(batch_size):\n            if imageRange != -1:\n                while True:\n                    pose = abs(self.findPose(self.indices[self.idx]))\n                    if not self.LOAD_60_LABEL:\n                        if  pose <= imageRange and pose >= imageRangeLow:\n                            break\n                    else:\n                        if pose <= imageRange and pose >= 45:\n                            break\n                    self.updateidx()\n            images[i, ...], feats = self.load_image(self.indices[self.idx])\n            filename = self.indices[self.idx]\n            labels[i,...], _, leyel[i,...], leyer[i,...], lnose[i,...], lmouth[i, ...] = self.load_label_mask(filename)\n\n            pose = abs(self.findPose(filename))\n            poselabels[i] = int(pose/15)\n            identity = int(filename[0:3])\n            idenlabels[i] = identity\n            landmarklabels[i,:] = feats[0].flatten()\n            eyel[i,...] = feats[1]\n            eyer[i,...] = feats[2]\n            nose[i,...] = feats[3]\n            mouth[i, ...] = feats[4]\n            self.updateidx()\n        return images, labels, masks, verifyImages, verifyLabels, poselabels, idenlabels, landmarklabels,\\\n               eyel, eyer, nose, mouth, leyel, leyer, lnose, lmouth\n\n    def updateidx(self):\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n    def load_image(self, filename):\n            """"""\n            Load input image & codemap and preprocess:\n            - cast to float\n            - subtract mean divide stdadv\n            - concatenate together\n            """"""\n            im = Image.open(self.dir + filename)\n            in_ = np.array(im, dtype=np.float32)\n            in_ /= 256\n            features = self.GetFeatureParts(in_, filename)\n            if self.MIRROR_TO_ONE_SIDE and self.findPose(filename) < 0:\n                in_ = in_[:,::-1,:]\n            return in_, features\n\n    def load_label_mask(self, filename, labelnum=-1):\n\n        _, labelname = self.findSameIllumCodeLabelpath(filename)\n        im = Image.open(self.dir + labelname)\n        if self.MIRROR_TO_ONE_SIDE and self.findPose(labelname) < 0:\n            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n            if self.GENERATE_MASK:\n                codeImg = codeImg.transpose(Image.FLIP_LEFT_RIGHT)\n\n        if self.GENERATE_MASK:\n            code = np.array(codeImg, dtype=np.float32)\n            mask = (code != np.zeros(code.shape, code.dtype))\n            mask = mask.astype(np.float32, copy=False)\n            RATIO = 0.1\n            mask *= 1-RATIO\n            mask += RATIO\n        else:\n            mask = None\n\n        label = np.array(im, dtype=np.float32)\n        #label -= label.mean()\n        #label /= label.std()\n        label /= 256\n        #label -= 1\n        feats = self.GetFeatureParts(label, labelname, label=True)\n        if not self.LOAD_60_LABEL:\n            if self.findPose(filename) < 0:#random.random() < 0.5:\n                #print(""fipping!"")\n                label = label[:,::-1,:]\n                feats[1][...] = feats[1][:,::-1,:]\n                feats[2][...] = feats[2][:,::-1,:]\n                feats[3][...] = feats[3][:,::-1,:]\n                feats[4][...] = feats[4][:,::-1,:]\n                return label, mask, feats[2], feats[1], feats[3], feats[4]\n        #print(""not flipping!"")\n        return label, mask, feats[1], feats[2], feats[3], feats[4]\n        #use coler code to generate mask\n        #background area weights 0.2, face area weights 1.0\n\n        return label, mask, feats\n\n    def load_label_mask_with_veri(self, filename, labelnum=-1):\n        label, mask = self.load_label_mask(filename, labelnum)\n\n        #if(random.random() > 0.5):#positive\n        if self.RANDOM_VERIFY:\n            if True:\n                return label, mask, label, int(filename[0:3])\n            else:\n                randomSubjectPath = self.indices[random.randint(0, len(self.indices)-1)]\n                _, veryPath = self.findCodeLabelpath(randomSubjectPath)\n                veryIm = Image.open(self.codeLabelDir + veryPath)\n                veryImArray = np.array(veryIm, dtype=np.float32)\n                veryImArray /= 256\n                #veryImArray -= 1\n                return label, mask, veryImArray, int(randomSubjectPath[0:3])\n        else:\n            return label, mask, None, None\n\n    def findBestIllumCodeImagepath(self, fullpath):\n        span = re_pose.search(fullpath).span()\n        camPos = list(fullpath[span[0]+1:span[1]-1])\n        camPos.insert(2,\'_\')\n        camPos = \'\'.join(camPos)\n        #get 01_0 like string\n        bestIllum = self.cameraPositions[camPos][1]\n\n        labelpath = list(fullpath)\n        labelpath[span[1]:span[1]+2] = bestIllum[:]\n        labelpath = \'\'.join(labelpath)\n        codepath = str(labelpath).replace(\'cropped\', \'code\')\n        return (codepath, labelpath)\n\n    def findCodeLabelpath(self, fullpath, labelnum):\n        span = re_poseIllum.search(fullpath).span()\n        #print span\n        #camPosIllu =fullpath[span[0]+1:span[1]-1]\n        #print camPosIllu\n        #labelpath = fullpath.replace(camPosIllu, \'051_06\')\n        tempath = list(fullpath)\n        if self.LOAD_60_LABEL:\n            camPos = fullpath[span[0]+1:span[0]+4]\n            if(camPos == \'240\' or camPos == \'010\'): #+90/75\n                tempath[span[0]+1:span[1]-1] = \'200_08\' #+60\n            elif (camPos == \'120\' or camPos == \'110\'): #-90/75\n                tempath[span[0]+1:span[1]-1] = \'090_15\' #-60\n            else:\n                tempath[span[0]+1:span[1]-1] = \'051_06\'\n        else:\n            tempath[span[0]+1:span[1]-1] = \'051_06\'\n        labelpath = \'\'.join(tempath)\n        codepath = str(labelpath).replace(\'cropped\', \'code\')\n        if labelnum != -1:\n            replace = None\n            for i in self.cameraPositions.items():\n                if i[1][0] == labelnum:\n                    replace = \'\'.join([i[0][0:2],i[0][3],\'_\',i[1][1]])\n                    tempath[span[0]+1:span[1]-1] = replace\n                    labelpath = \'\'.join(tempath)\n            if replace == None:\n                print(\'damn labelnum bug!\')\n        return (codepath, labelpath)\n\n    def findSameIllumCodeLabelpath(self, fullpath):\n        span = re_poseIllum.search(fullpath).span()\n        tempath = list(fullpath)\n        if self.LOAD_60_LABEL:\n            if self.findPose(fullpath) >= 0:\n                tempath[span[0]+1:span[0]+4] = \'190\' #+45\n            else:\n                tempath[span[0]+1:span[0]+4] = \'080\' #-45\n        else:\n            tempath[span[0]+1:span[0]+4] = \'051\'\n        labelpath = \'\'.join(tempath)\n        codepath = str(labelpath).replace(\'cropped\', \'code\')\n        return (codepath, labelpath)\n\n    def findPose(self, fullpath):\n        span = re_pose.search(fullpath).span()\n        camPos = list(fullpath[span[0]+1:span[1]-1])\n        camPos.insert(2,\'_\')\n        camPos = \'\'.join(camPos)\n        #get 01_0 like string\n        return self.cameraPositions[camPos][0]\n\n    def GetFeatureParts(self, img_resize, filename, label=False):\n        #crop four parts\n        trans_points = np.empty([5,2],dtype=np.int32)\n        if True:#not label:\n            featpath = self.feat5ptDir + filename[0:-15] + \'_trans.5pt\'\n            #print(filename)\n            with open(featpath, \'r\') as csvfile:\n                reader = csv.reader(csvfile, delimiter=\' \')\n                for ind,row in enumerate(reader):\n                    trans_points[ind,:] = row\n        #print(trans_points)\n        eyel_crop = np.zeros([EYE_H,EYE_W,3], dtype=np.float32);\n        crop_y = int(trans_points[0,1] - EYE_H / 2);\n        crop_y_end = crop_y + EYE_H;\n        crop_x = int(trans_points[0,0] - EYE_W / 2);\n        crop_x_end = crop_x + EYE_W;\n        eyel_crop[...] = img_resize[crop_y:crop_y_end,crop_x:crop_x_end,:];\n\n        eyer_crop = np.zeros([EYE_H,EYE_W,3], dtype=np.float32);\n        crop_y = int(trans_points[1,1] - EYE_H / 2)\n        crop_y_end = crop_y + EYE_H;\n        crop_x = int(trans_points[1,0] - EYE_W / 2);\n        crop_x_end = crop_x + EYE_W;\n        eyer_crop[...] = img_resize[crop_y:crop_y_end,crop_x:crop_x_end,:];\n\n\n        month_crop = np.zeros([MOUTH_H,MOUTH_W,3], dtype=np.float32);\n        crop_y = int((trans_points[3,1] + trans_points[4,1]) // 2 - MOUTH_H / 2);\n        crop_y_end = crop_y + MOUTH_H;\n        crop_x = int((trans_points[3,0] + trans_points[4,0]) // 2 - MOUTH_W / 2);\n        crop_x_end = crop_x + MOUTH_W;\n        month_crop[...] = img_resize[crop_y:crop_y_end,crop_x:crop_x_end,:];\n\n\n        nose_crop = np.zeros([NOSE_H,NOSE_W,3], dtype=np.float32);\n        \n        #pose specific crop for MultiPIE. But it doesn\'t affect much as long as you train and test with a consistant crop configuration.\n        # A general crop for nose is provicded below as comment.\n        pos_s = filename[-18:-15]\n        if label or pos_s == \'051\':#frontal\n            crop_y_end = trans_points[2,1] + 10 + 1;\n            crop_x = trans_points[2,0] - 20 + 1;\n        elif pos_s == \'050\':#+15 degrees\n            crop_y_end = trans_points[2,1] + 10 + 1;\n            crop_x = trans_points[2,0] - 25 + 1;\n        elif pos_s == \'140\':#-15 degrees\n            crop_y_end = trans_points[2,1] + 10 + 1;\n            crop_x = trans_points[2,0] - (NOSE_W-25) + 1;\n        elif pos_s ==  \'041\' or pos_s == \'190\':#+30/45 degrees\n            crop_y_end = trans_points[2,1] + 10 + 1;\n            crop_x = trans_points[2,0] - 30 + 1;\n        elif pos_s == \'130\' or pos_s == \'080\':#-30/45 degrees\n            crop_y_end = trans_points[2,1] + 10 + 1;\n            crop_x = trans_points[2,0] - (NOSE_W-30) + 1;\n        elif pos_s == \'010\' or pos_s ==\'200\' or pos_s == \'240\':#+60/75/90 degrees\n            crop_y_end = trans_points[2,1] + 10 + 1;\n            crop_x = trans_points[2,0] - 32 + 1;\n        elif pos_s == \'120\' or pos_s == \'090\' or pos_s == \'110\': #-60/75/90 degrees\n            crop_y_end = trans_points[2,1] + 10 + 1;\n            crop_x = trans_points[2,0] - (NOSE_W-32) + 1;\n        else:\n            print(""BUG from nose selection!"")\n\n        #Or you can just use this general position for nose crop, that works for most poses.\n        # crop_y_end = trans_points[2,1] + 10 + 1\n        # crop_x = trans_points[2,0] - 20 + 1\n        # crop_y_end = int(crop_y_end)\n        # crop_x = int(crop_x)\n        # crop_y = crop_y_end - NOSE_H;\n        # crop_x_end = crop_x + NOSE_W;\n\n\n        crop_y_end = int(crop_y_end)\n        crop_x = int(crop_x)\n        crop_y = crop_y_end - NOSE_H;\n        crop_x_end = crop_x + NOSE_W;\n        #import pdb; pdb.set_trace()\n        nose_crop[...] = img_resize[crop_y:crop_y_end,crop_x:crop_x_end,:];\n\n        if not label and self.MIRROR_TO_ONE_SIDE and self.findPose(filename) < 0:\n            teml = eyel_crop[:,::-1,:]\n            eyel_crop = eyer_crop[:,::-1,:]\n            eyer_crop = teml\n            month_crop = month_crop[:,::-1,:]\n            nose_crop = nose_crop[:,::-1,:]\n            trans_points[:,0] = IMAGE_SIZE - trans_points[:,0]\n            #exchange eyes and months\n            teml = trans_points[0,:].copy()\n            trans_points[0, :] = trans_points[1, :]\n            trans_points[1, :] = teml\n            teml = trans_points[3,:].copy()\n            trans_points[3, :] = trans_points[4, :]\n            trans_points[4, :] = teml\n\n        return trans_points, eyel_crop, eyer_crop, nose_crop, month_crop\n'"
ops.py,46,"b'import math\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\n\nfrom utils import *\n\n# try:\n#     image_summary = tf.image_summary\n#     scalar_summary = tf.scalar_summary\n#     histogram_summary = tf.histogram_summary\n#     merge_summary = tf.merge_summary\n#     SummaryWriter = tf.train.SummaryWriter\n# except:\nimage_summary = tf.summary.image\nscalar_summary = tf.summary.scalar\nhistogram_summary = tf.summary.histogram\nmerge_summary = tf.summary.merge\nSummaryWriter = tf.summary.FileWriter\n\n# class batch_norm(object):\n#     def __init__(self, epsilon=1e-5, momentum = 0.9, name=""batch_norm""):\n#         with tf.variable_scope(name):\n#             self.epsilon  = epsilon\n#             self.momentum = momentum\n#             self.name = name\n#\n#     def __call__(self, x, train=True):\n#         return tf.contrib.layers.batch_norm(x,\n#                                             decay=self.momentum,\n#                                             updates_collections=None,\n#                                             epsilon=self.epsilon,\n#                                             scale=True,\n#                                             is_training=train,\n#                                             scope=self.name)\ndef batch_norm(x,epsilon=1e-5, momentum = 0.9, name=""batch_norm"",train=True):\n    return tf.contrib.layers.batch_norm(x,\n                                        decay=momentum,\n                                        updates_collections=None,\n                                        epsilon=epsilon,\n                                        scale=True,\n                                        is_training=train,\n                                        scope=name)\n\n\n    def __call__(self, x, train=True):\n        return tf.contrib.layers.batch_norm(x,\n                                            decay=self.momentum,\n                                            updates_collections=None,\n                                            epsilon=self.epsilon,\n                                            scale=True,\n                                            is_training=train,\n                                            scope=self.name)\n\n\ndef binary_cross_entropy(preds, targets, name=None):\n    """"""Computes binary cross entropy given `preds`.\n\n    For brevity, let `x = `, `z = targets`.  The logistic loss is\n\n        loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\n    Args:\n        preds: A `Tensor` of type `float32` or `float64`.\n        targets: A `Tensor` of the same type and shape as `preds`.\n    """"""\n    eps = 1e-12\n    with ops.op_scope([preds, targets], name, ""bce_loss"") as name:\n        preds = ops.convert_to_tensor(preds, name=""preds"")\n        targets = ops.convert_to_tensor(targets, name=""targets"")\n        return tf.reduce_mean(-(targets * tf.log(preds + eps) +\n                              (1. - targets) * tf.log(1. - preds + eps)))\n\ndef conv_cond_concat(x, y):\n    """"""Concatenate conditioning vector on feature map axis.""""""\n    x_shapes = x.get_shape()\n    y_shapes = y.get_shape()\n    return tf.concat(3, [x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])])\n\ndef conv2d(input_, output_dim,\n           k_h=3, k_w=3, d_h=2, d_w=2, stddev=0.02,\n           name=""conv2d""):\n    with tf.variable_scope(name):\n        w = tf.get_variable(\'w\', [k_h, k_w, input_.get_shape()[-1], output_dim],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=\'SAME\')\n\n        biases = tf.get_variable(\'biases\', [output_dim], initializer=tf.constant_initializer(0.0))\n        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n        return conv\ndef resblock(input_, k_h=3, k_w=3, d_h=1, d_w=1, name = ""resblock""):\n    conv1 = lrelu(batch_norm(conv2d(input_, input_.get_shape()[-1],\n                                    k_h=k_h, k_w=k_w, d_h=d_h, d_w=d_w,\n                                    name=name+""_conv1""),name=name+""_bn1""))\n    conv2 = batch_norm(conv2d(conv1, input_.get_shape()[-1],\n                            k_h=k_h, k_w=k_w, d_h=d_h, d_w=d_w,\n                            name=name+""_conv2""),name=name+""_bn2"")\n    return lrelu(tf.add(input_, conv2))\n\ndef deconv2d(input_, output_shape,\n             k_h=3, k_w=3, d_h=2, d_w=2, stddev=0.02,\n             name=""deconv2d"", with_w=False):\n    with tf.variable_scope(name):\n        # filter : [height, width, output_channels, in_channels]\n        w = tf.get_variable(\'w\', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n                            initializer=tf.random_normal_initializer(stddev=stddev))\n\n        try:\n            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n                                strides=[1, d_h, d_w, 1])\n\n        # Support for verisons of TensorFlow before 0.7.0\n        except AttributeError:\n            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n                                strides=[1, d_h, d_w, 1])\n\n        biases = tf.get_variable(\'biases\', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n\n        if with_w:\n            return deconv, w, biases\n        else:\n            return deconv\n\ndef gradientweight():\n    kernels = []\n    # [np.array([\n    # [-1,0,1],\n    # [-1,0,1],\n    # [-1,0,1]])]\n    # kernels.append(np.array([\n    #     [-1,-1,-1],\n    #     [0,0,0],\n    #     [1,1,1]]))\n    # kernels.append(np.array([\n    #     [0,-1,0],\n    #     [-1,4,-1],\n    #     [0,-1,0]]))\n    kernels.append(np.array([\n        [-1,-1,-1],\n        [-1,8,-1],\n        [-1,-1,-1]]))\n    weights = []\n    for i in range(0,3): #input channel\n        weightPerChannel = []\n        for j in range(0,1):#kernel\n            weightPerKernel = []\n            for k in range(0,i):#before zero\n                weightPerKernel.append(np.zeros([3,3]))\n            weightPerKernel.append(kernels[j])\n            for k in range(i,3-1):#after zero\n                weightPerKernel.append(np.zeros([3,3]))\n            weightPerChannel.extend(weightPerKernel)\n        weights.append(weightPerChannel)\n    weights = np.array(weights).transpose(2,3,0,1)\n    return weights\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n  return tf.maximum(x, leak*x, name=name)\n\ndef Dropout(x, keep_prob=0.5, is_training=True):\n    """"""\n    :param is_training: if None, will use the current context by default.\n    """"""\n    keep_prob = tf.constant(keep_prob if is_training else 1.0)\n    return tf.nn.dropout(x, keep_prob)\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n    shape = input_.get_shape().as_list()\n\n    with tf.variable_scope(scope or ""Linear""):\n        matrix = tf.get_variable(""Matrix"", [shape[1], output_size], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        bias = tf.get_variable(""bias"", [output_size],\n            initializer=tf.constant_initializer(bias_start))\n        if with_w:\n            return tf.matmul(input_, matrix) + bias, matrix, bias\n        else:\n            return tf.matmul(input_, matrix) + bias\ndef symL1(img):\n    width = img.get_shape().as_list()[2]\n    right = img[:,:,width/2:,:]\n    left = tf.stop_gradient(img[:,:,0:width/2,:])\n    return tf.abs(left - right[:,:,::-1,:])\n\ndef total_variation(images, name=None):\n    """"""Calculate and return the Total Variation for one or more images.\n\n    The total variation is the sum of the absolute differences for neighboring\n    pixel-values in the input images. This measures how much noise is in the images.\n\n    This can be used as a loss-function during optimization so as to suppress noise\n    in images. If you have a batch of images, then you should calculate the scalar\n    loss-value as the sum: `loss = tf.reduce_sum(tf.image.total_variation(images))`\n\n    This implements the anisotropic 2-D version of the formula described here:\n\n    https://en.wikipedia.org/wiki/Total_variation_denoising\n\n    Args:\n        images: 4-D Tensor of shape `[batch, height, width, channels]` or\n                3-D Tensor of shape `[height, width, channels]`.\n\n        name: A name for the operation (optional).\n\n    Raises:\n        ValueError: if images.shape is not a 3-D or 4-D vector.\n\n    Returns:\n        The total variation of `images`.\n\n        If `images` was 4-D, a 1-D float Tensor of shape `[batch]` with the\n        total variation for each image in the batch.\n        If `images` was 3-D, a scalar float with the total variation for that image.\n    """"""\n\n    with ops.name_scope(name, \'total_variation\'):\n        ndims = images.get_shape().ndims\n\n        if ndims == 3:\n            # The input is a single image with shape [height, width, channels].\n\n            # Calculate the difference of neighboring pixel-values.\n            # The images are shifted one pixel along the height and width by slicing.\n            pixel_dif1 = images[1:,:,:] - images[:-1,:,:]\n            pixel_dif2 = images[:,1:,:] - images[:,:-1,:]\n\n            # Sum for all axis. (None is an alias for all axis.)\n            sum_axis = None\n        elif ndims == 4:\n            # The input is a batch of images with shape [batch, height, width, channels].\n\n            # Calculate the difference of neighboring pixel-values.\n            # The images are shifted one pixel along the height and width by slicing.\n            pixel_dif1 = images[:,1:,:,:] - images[:,:-1,:,:]\n            pixel_dif2 = images[:,:,1:,:] - images[:,:,:-1,:]\n\n            # Only sum for the last 3 axis.\n            # This results in a 1-D tensor with the total variation for each image.\n            sum_axis = [1, 2, 3]\n        else:\n            raise ValueError(\'\\\'images\\\' must be either 3 or 4-dimensional.\')\n\n        # Calculate the total variation by taking the absolute value of the\n        # pixel-differences and summing over the appropriate axis.\n        tot_var = tf.reduce_sum(tf.abs(pixel_dif1), axis=sum_axis) + \\\n                  tf.reduce_sum(tf.abs(pixel_dif2), axis=sum_axis)\n\n    return tot_var'"
utils.py,0,"b'""""""\nSome codes from https://github.com/Newmu/dcgan_code\n""""""\nfrom __future__ import division\nimport math\nimport json\nimport random\nimport pprint\nimport scipy.misc\nimport numpy as np\nfrom time import gmtime, strftime\n#from net_input_everything_featparts import MultiPIE\nimport os\n\npp = pprint.PrettyPrinter()\n\nget_stddev = lambda x, k_h, k_w: 1/math.sqrt(k_w*k_h*x.get_shape()[-1])\n\ndef get_image(image_path, image_size, is_crop=True, resize_w=64, is_grayscale = False):\n    return transform(imread(image_path, is_grayscale), image_size, is_crop, resize_w)\n\ndef save_images(images, size, image_path, suffix=None, isOutput=False, filelist = None):\n    #if isOutput:\n    #   images = mirrorLeftToFull(images)\n    return imsave(images, size, image_path, suffix, isOutput, filelist)\n\ndef mirrorLeftToFull(images):\n    width = images.shape[2]\n    leftImages = images[:,:,0:width/2,:]\n    reversedLeftImages = leftImages[:,:,::-1,:]\n    return np.concatenate((leftImages, reversedLeftImages), axis=2)\n\ndef imread(path, is_grayscale = False):\n    if (is_grayscale):\n        return scipy.misc.imread(path, flatten = True).astype(np.float)\n    else:\n        return scipy.misc.imread(path).astype(np.float)\n\ndef merge_images(images, size):\n    return inverse_transform(images)\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((h * size[0], w * size[1], 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[j*h:j*h+h, i*w:i*w+w, :] = image\n\n    return img\n\ndef imsave(images, size, path, suffix=None, isOutput=False, filelist=None):\n    num = images.shape[0]\n    for i in range(num):\n        if filelist is None:\n            filename = path+str(i)\n        else:\n            filename = path+filelist[i][:-4] #discard .png\n        if not isOutput:\n            filename += \'_test\'\n        if suffix is not None:\n            filename += suffix\n        filename += \'.png\'\n        dirName = os.path.dirname(filename)\n        if not os.path.exists(dirName):\n            os.makedirs(dirName)\n        if images.shape[-1] == 1:\n            scipy.misc.imsave(filename,images[i,:,:,0])\n        else:\n            scipy.misc.imsave(filename,images[i,:,:,:])\n    return num\n\ndef center_crop(x, crop_h, crop_w=None, resize_w=64):\n    if crop_w is None:\n        crop_w = crop_h\n    h, w = x.shape[:2]\n    j = int(round((h - crop_h)/2.))\n    i = int(round((w - crop_w)/2.))\n    return scipy.misc.imresize(x[j:j+crop_h, i:i+crop_w],\n                               [resize_w, resize_w])\n\ndef transform(image, npx=64, is_crop=True, resize_w=64):\n    # npx : # of pixels width/height of image\n    if is_crop:\n        cropped_image = center_crop(image, npx, resize_w=resize_w)\n    else:\n        cropped_image = image\n    return np.array(cropped_image)/127.5 - 1.\n\ndef inverse_transform(images):\n    return (images+1.)/2.\n\n\ndef to_json(output_path, *layers):\n    with open(output_path, ""w"") as layer_f:\n        lines = """"\n        for w, b, bn in layers:\n            layer_idx = w.name.split(\'/\')[0].split(\'h\')[1]\n\n            B = b.eval()\n\n            if ""lin/"" in w.name:\n                W = w.eval()\n                depth = W.shape[1]\n            else:\n                W = np.rollaxis(w.eval(), 2, 0)\n                depth = W.shape[0]\n\n            biases = {""sy"": 1, ""sx"": 1, ""depth"": depth, ""w"": [\'%.2f\' % elem for elem in list(B)]}\n            if bn != None:\n                gamma = bn.gamma.eval()\n                beta = bn.beta.eval()\n\n                gamma = {""sy"": 1, ""sx"": 1, ""depth"": depth, ""w"": [\'%.2f\' % elem for elem in list(gamma)]}\n                beta = {""sy"": 1, ""sx"": 1, ""depth"": depth, ""w"": [\'%.2f\' % elem for elem in list(beta)]}\n            else:\n                gamma = {""sy"": 1, ""sx"": 1, ""depth"": 0, ""w"": []}\n                beta = {""sy"": 1, ""sx"": 1, ""depth"": 0, ""w"": []}\n\n            if ""lin/"" in w.name:\n                fs = []\n                for w in W.T:\n                    fs.append({""sy"": 1, ""sx"": 1, ""depth"": W.shape[0], ""w"": [\'%.2f\' % elem for elem in list(w)]})\n\n                lines += """"""\n                    var layer_%s = {\n                        ""layer_type"": ""fc"",\n                        ""sy"": 1, ""sx"": 1,\n                        ""out_sx"": 1, ""out_sy"": 1,\n                        ""stride"": 1, ""pad"": 0,\n                        ""out_depth"": %s, ""in_depth"": %s,\n                        ""biases"": %s,\n                        ""gamma"": %s,\n                        ""beta"": %s,\n                        ""filters"": %s\n                    };"""""" % (layer_idx.split(\'_\')[0], W.shape[1], W.shape[0], biases, gamma, beta, fs)\n            else:\n                fs = []\n                for w_ in W:\n                    fs.append({""sy"": 5, ""sx"": 5, ""depth"": W.shape[3], ""w"": [\'%.2f\' % elem for elem in list(w_.flatten())]})\n\n                lines += """"""\n                    var layer_%s = {\n                        ""layer_type"": ""deconv"",\n                        ""sy"": 5, ""sx"": 5,\n                        ""out_sx"": %s, ""out_sy"": %s,\n                        ""stride"": 2, ""pad"": 1,\n                        ""out_depth"": %s, ""in_depth"": %s,\n                        ""biases"": %s,\n                        ""gamma"": %s,\n                        ""beta"": %s,\n                        ""filters"": %s\n                    };"""""" % (layer_idx, 2**(int(layer_idx)+2), 2**(int(layer_idx)+2),\n                             W.shape[0], W.shape[3], biases, gamma, beta, fs)\n        layer_f.write("" "".join(lines.replace(""\'"","""").split()))\n\ndef make_gif(images, fname, duration=2, true_image=False):\n  import moviepy.editor as mpy\n\n  def make_frame(t):\n    try:\n      x = images[int(len(images)/duration*t)]\n    except:\n      x = images[-1]\n\n    if true_image:\n      return x.astype(np.uint8)\n    else:\n      return ((x+1)/2*255).astype(np.uint8)\n\n  clip = mpy.VideoClip(make_frame, duration=duration)\n  clip.write_gif(fname, fps = len(images) / duration)\n\ndef visualize(sess, dcgan, config, option):\n  if option == 0:\n    data = MultiPIE(LOAD_60_LABEL=False)\n    sample_images, filenames = data.test_batch(9999999999, Random = False)\n    print(\'test samples reading complete\')\n    batchnum = sample_images.shape[0] // dcgan.test_batch_size #current test batch size\n    savedtest = 0\n    savedoutput = 0\n    sample_dir = \'testall\'\n    for i in range(batchnum):\n        print(\'generating test result batch{}\'.format(i))\n        ind = (i*dcgan.test_batch_size, (i+1)*dcgan.test_batch_size)\n        samples = sess.run(\n            dcgan.sample_generator,\n            feed_dict={ dcgan.sample_images: sample_images[ind[0]:ind[1],...]}\n        )\n        savedtest += save_images(sample_images[ind[0]:ind[1],:,:,0:3], [128, 128],\n                    \'./{}/\'.format(sample_dir),isOutput=False, filelist=filenames[ind[0]:ind[1]])\n        savedoutput += save_images(samples, [128, 128],\n                    \'./{}/\'.format(sample_dir),isOutput=True, filelist=filenames[ind[0]:ind[1]])\n        print(""[{} completed{} and saved {}.]"".format(sample_dir, savedtest, savedoutput))\n        #save_images(samples, [100, 100], \'./samples/test_%s.png\' % strftime(""%Y-%m-%d %H:%M:%S"", gmtime()))\n  elif option == 1:\n    data = MultiPIE(LOAD_60_LABEL=False)\n    sample_images, filenames = data.test_batch(9999999999, Random = False)\n    print(\'test samples reading complete\')\n    batchnum = sample_images.shape[0] // dcgan.test_batch_size #current test batch size\n    savedtest = 0\n    savedoutput = 0\n    sample_dir = \'testall\'\n    for i in range(batchnum):\n        print(\'generating test result batch{}\'.format(i))\n        ind = (i*dcgan.test_batch_size, (i+1)*dcgan.test_batch_size)\n        samples = sess.run(\n            dcgan.sample_generator,\n            feed_dict={ dcgan.sample_images: sample_images[ind[0]:ind[1],:,:,:]}\n        )\n        colorgt = sample_images[ind[0]:ind[1],:,:,0:3]\n        #colorgt.mean(axis=3, keepdims=True)\n        savedtest += save_images(colorgt, [128, 128],\n                    \'./{}/\'.format(sample_dir),isOutput=False, filelist=filenames[ind[0]:ind[1]])\n        #print(samples[5].shape)\n        savedoutput += save_images(samples[5], [128, 128],\n                    \'./{}/\'.format(sample_dir),isOutput=True, filelist=filenames[ind[0]:ind[1]])\n        print(""[{} completed{} and saved {}.]"".format(sample_dir, savedtest, savedoutput))\n        #save_images(samples, [100, 100], \'./samples/test_%s.png\' % strftime(""%Y-%m-%d %H:%M:%S"", gmtime()))\n  elif option == 2:\n      #patch version\n      data = MultiPIE(LOAD_60_LABEL=False)\n      sample_images, filenames,sample_eyel, sample_eyer, sample_nose, sample_mouth = data.test_batch(9999999999, Random = False)\n      print(\'test samples reading complete\')\n      batchnum = sample_images.shape[0] // dcgan.test_batch_size #current test batch size\n      savedtest = 0\n      savedoutput = 0\n      sample_dir = \'testall\'\n      for i in range(batchnum):\n          print(\'generating test result batch{}\'.format(i))\n          ind = (i*dcgan.test_batch_size, (i+1)*dcgan.test_batch_size)\n          samples = sess.run(\n              dcgan.sample_generator,\n              feed_dict={ dcgan.sample_images: sample_images[ind[0]:ind[1],:,:,:],\n                          dcgan.eyel_sam : sample_eyel[ind[0]:ind[1],...],\n                          dcgan.eyer_sam : sample_eyer[ind[0]:ind[1],...],\n                          dcgan.nose_sam : sample_nose[ind[0]:ind[1],...],\n                          dcgan.mouth_sam : sample_mouth[ind[0]:ind[1],...]}\n          )\n          colorgt = sample_images[ind[0]:ind[1],:,:,0:3]\n          #colorgt.mean(axis=3, keepdims=True)\n          savedtest += save_images(colorgt, [128, 128],\n                                   \'./{}/\'.format(sample_dir),isOutput=False, filelist=filenames[ind[0]:ind[1]])\n          #print(samples[5].shape)\n          savedoutput += save_images(samples[5], [128, 128],\n                                     \'./{}/\'.format(sample_dir),isOutput=True, filelist=filenames[ind[0]:ind[1]])\n          print(""[{} completed{} and saved {}.]"".format(sample_dir, savedtest, savedoutput))\n          #save_images(samples, [100, 100], \'./samples/test_%s.png\' % strftime(""%Y-%m-%d %H:%M:%S"", gmtime()))\n  elif option == 3:\n    values = np.arange(0, 1, 1./config.batch_size)\n    for idx in xrange(100):\n      print("" [*] %d"" % idx)\n      z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample):\n        z[idx] = values[kdx]\n\n      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n      make_gif(samples, \'./samples/test_gif_%s.gif\' % (idx))\n  elif option == 4:\n    image_set = []\n    values = np.arange(0, 1, 1./config.batch_size)\n\n    for idx in xrange(100):\n      print("" [*] %d"" % idx)\n      z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample): z[idx] = values[kdx]\n\n      image_set.append(sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample}))\n      make_gif(image_set[-1], \'./samples/test_gif_%s.gif\' % (idx))\n\n    new_image_set = [merge(np.array([images[idx] for images in image_set]), [10, 10]) \\\n        for idx in range(64) + range(63, -1, -1)]\n    make_gif(new_image_set, \'./samples/test_gif_merged.gif\', duration=8)\n'"
