file_path,api_count,code
data_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Feb 27 09:33:32 2016\n\n@author: Bing Liu (liubing@cmu.edu)\n\nPrepare data for multi-task RNN model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\n\nfrom tensorflow.python.platform import gfile\n\n# Special vocabulary symbols - we always put them at the start.\n_PAD = ""_PAD""\n_UNK = ""_UNK""\n_START_VOCAB = [_PAD, _UNK]\n\nSTART_VOCAB_dict = dict()\nSTART_VOCAB_dict[\'with_padding\'] = [_PAD, _UNK]\nSTART_VOCAB_dict[\'no_padding\'] = [_UNK]\n\n\nPAD_ID = 0\n\nUNK_ID_dict = dict()\nUNK_ID_dict[\'with_padding\'] = 1\nUNK_ID_dict[\'no_padding\'] = 0\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(r""\\d"")\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n  return [w for w in words if w]\n\ndef naive_tokenizer(sentence):\n  """"""Naive tokenizer: split the sentence by space into a list of tokens.""""""\n  return sentence.split()  \n\n\ndef create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n    vocab = {}\n    with gfile.GFile(data_path, mode=""r"") as f:\n      counter = 0\n      for line in f:\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  processing line %d"" % counter)\n        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n        for w in tokens:\n          word = re.sub(_DIGIT_RE, ""0"", w) if normalize_digits else w\n          if word in vocab:\n            vocab[word] += 1\n          else:\n            vocab[word] = 1\n      vocab_list = START_VOCAB_dict[\'with_padding\'] + \\\n                      sorted(vocab, key=vocab.get, reverse=True)\n      if len(vocab_list) > max_vocabulary_size:\n        vocab_list = vocab_list[:max_vocabulary_size]\n      with gfile.GFile(vocabulary_path, mode=""w"") as vocab_file:\n        for w in vocab_list:\n          vocab_file.write(w + ""\\n"")\n\n\ndef initialize_vocab(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n    with gfile.GFile(vocabulary_path, mode=""r"") as f:\n      rev_vocab.extend(f.readlines())\n    rev_vocab = [line.strip() for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary, UNK_ID,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: a string, the sentence to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  if not normalize_digits:\n    return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  return [vocabulary.get(re.sub(_DIGIT_RE, ""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path,\n                      tokenizer=None, normalize_digits=True, use_padding=True):\n  """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n  This function loads data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing data in %s"" % data_path)\n    vocab, _ = initialize_vocab(vocabulary_path)\n    with gfile.GFile(data_path, mode=""r"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          if use_padding:\n            UNK_ID = UNK_ID_dict[\'with_padding\']\n          else:\n            UNK_ID = UNK_ID_dict[\'no_padding\']\n          token_ids = sentence_to_token_ids(line, vocab, UNK_ID, tokenizer,\n                                            normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\n\ndef create_label_vocab(vocabulary_path, data_path):\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n    vocab = {}\n    with gfile.GFile(data_path, mode=""r"") as f:\n      counter = 0\n      for line in f:\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  processing line %d"" % counter)\n        label = line.strip()\n        vocab[label] = 1\n      label_list = START_VOCAB_dict[\'no_padding\'] + sorted(vocab)\n      with gfile.GFile(vocabulary_path, mode=""w"") as vocab_file:\n        for k in label_list:\n          vocab_file.write(k + ""\\n"")\n\ndef prepare_multi_task_data(data_dir, in_vocab_size, out_vocab_size):\n    train_path = data_dir + \'/train/train\'\n    dev_path = data_dir + \'/valid/valid\'\n    test_path = data_dir + \'/test/test\'\n    \n    # Create vocabularies of the appropriate sizes.\n    in_vocab_path = os.path.join(data_dir, ""in_vocab_%d.txt"" % in_vocab_size)\n    out_vocab_path = os.path.join(data_dir, ""out_vocab_%d.txt"" % out_vocab_size)\n    label_path = os.path.join(data_dir, ""label.txt"")\n    \n    create_vocabulary(in_vocab_path, \n                      train_path + "".seq.in"", \n                      in_vocab_size, \n                      tokenizer=naive_tokenizer)\n    create_vocabulary(out_vocab_path, \n                      train_path + "".seq.out"", \n                      out_vocab_size, \n                      tokenizer=naive_tokenizer)\n    create_label_vocab(label_path, train_path + "".label"")\n    \n    # Create token ids for the training data.\n    in_seq_train_ids_path = train_path + ("".ids%d.seq.in"" % in_vocab_size)\n    out_seq_train_ids_path = train_path + ("".ids%d.seq.out"" % out_vocab_size)\n    label_train_ids_path = train_path + ("".ids.label"")\n\n    data_to_token_ids(train_path + "".seq.in"", \n                      in_seq_train_ids_path, \n                      in_vocab_path, \n                      tokenizer=naive_tokenizer)\n    data_to_token_ids(train_path + "".seq.out"", \n                      out_seq_train_ids_path, \n                      out_vocab_path, \n                      tokenizer=naive_tokenizer)\n    data_to_token_ids(train_path + "".label"", \n                      label_train_ids_path, \n                      label_path, \n                      normalize_digits=False, \n                      use_padding=False)\n    \n    # Create token ids for the development data.\n    in_seq_dev_ids_path = dev_path + ("".ids%d.seq.in"" % in_vocab_size)\n    out_seq_dev_ids_path = dev_path + ("".ids%d.seq.out"" % out_vocab_size)\n    label_dev_ids_path = dev_path + ("".ids.label"")\n\n    data_to_token_ids(dev_path + "".seq.in"", \n                      in_seq_dev_ids_path, \n                      in_vocab_path, \n                      tokenizer=naive_tokenizer)\n    data_to_token_ids(dev_path + "".seq.out"", \n                      out_seq_dev_ids_path, \n                      out_vocab_path, \n                      tokenizer=naive_tokenizer)\n    data_to_token_ids(dev_path + "".label"", \n                      label_dev_ids_path, \n                      label_path, \n                      normalize_digits=False, \n                      use_padding=False)\n    \n    # Create token ids for the test data.\n    in_seq_test_ids_path = test_path + ("".ids%d.seq.in"" % in_vocab_size)\n    out_seq_test_ids_path = test_path + ("".ids%d.seq.out"" % out_vocab_size)\n    label_test_ids_path = test_path + ("".ids.label"")\n    \n    data_to_token_ids(test_path + "".seq.in"", \n                      in_seq_test_ids_path, \n                      in_vocab_path, \n                      tokenizer=naive_tokenizer)\n    data_to_token_ids(test_path + "".seq.out"", \n                      out_seq_test_ids_path, \n                      out_vocab_path, \n                      tokenizer=naive_tokenizer)\n    data_to_token_ids(test_path + "".label"", \n                      label_test_ids_path, \n                      label_path, \n                      normalize_digits=False, \n                      use_padding=False)\n    \n    return [(in_seq_train_ids_path,out_seq_train_ids_path,label_train_ids_path),\n            (in_seq_dev_ids_path, out_seq_dev_ids_path, label_dev_ids_path),\n            (in_seq_test_ids_path, out_seq_test_ids_path, label_test_ids_path),\n            (in_vocab_path, out_vocab_path, label_path)]'"
multi_task_model.py,27,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Feb 28 17:28:22 2016\n\n@author: Bing Liu (liubing@cmu.edu)\n\nMulti-task RNN model with an attention mechanism.\n  - Developped on top of the Tensorflow seq2seq_model.py example: \n    https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py\n  - Note that this example code does not include output label dependency modeling.\n    One may add a loop function as in the rnn_decoder function in tensorflow\n    seq2seq.py example to feed emitted label embedding back to RNN state.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\nimport data_utils\nimport seq_labeling\nimport seq_classification\nfrom tensorflow.contrib.rnn import BasicLSTMCell\nfrom tensorflow.contrib.rnn import MultiRNNCell\nfrom tensorflow.contrib.rnn import DropoutWrapper\nfrom tensorflow.contrib.rnn import static_rnn\nfrom tensorflow.contrib.rnn import static_bidirectional_rnn\n\n\nclass MultiTaskModel(object):\n  def __init__(self, \n               source_vocab_size, \n               tag_vocab_size, \n               label_vocab_size, \n               buckets, \n               word_embedding_size, \n               size, \n               num_layers, \n               max_gradient_norm, \n               batch_size, \n               dropout_keep_prob=1.0, \n               use_lstm=False, \n               bidirectional_rnn=True,\n               num_samples=1024, \n               use_attention=False, \n               task=None, \n               forward_only=False):\n    self.source_vocab_size = source_vocab_size\n    self.tag_vocab_size = tag_vocab_size\n    self.label_vocab_size = label_vocab_size\n    self.word_embedding_size = word_embedding_size\n    self.cell_size = size\n    self.num_layers = num_layers\n    self.buckets = buckets\n    self.batch_size = batch_size\n    self.bidirectional_rnn = bidirectional_rnn\n    self.global_step = tf.Variable(0, trainable=False)\n    \n    # If we use sampled softmax, we need an output projection.\n    softmax_loss_function = None\n\n    # Create the internal multi-layer cell for our RNN.\n    def create_cell():\n      if not forward_only and dropout_keep_prob < 1.0:\n        single_cell = lambda: BasicLSTMCell(self.cell_size)\n        cell = MultiRNNCell([single_cell() for _ in range(self.num_layers)])\n        cell = DropoutWrapper(cell,\n                                input_keep_prob=dropout_keep_prob,\n                                output_keep_prob=dropout_keep_prob)         \n      else:\n        single_cell = lambda: BasicLSTMCell(self.cell_size)\n        cell = MultiRNNCell([single_cell() for _ in range(self.num_layers)])\n      return cell\n  \n    self.cell_fw = create_cell()\n    self.cell_bw = create_cell()\n\n    # Feeds for inputs.\n    self.encoder_inputs = []\n    self.tags = []    \n    self.tag_weights = []    \n    self.labels = []    \n    self.sequence_length = tf.placeholder(tf.int32, [None], \n                                          name=""sequence_length"")\n    \n    for i in xrange(buckets[-1][0]):\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""encoder{0}"".format(i)))\n    for i in xrange(buckets[-1][1]):\n      self.tags.append(tf.placeholder(tf.float32, shape=[None], \n                                      name=""tag{0}"".format(i)))\n      self.tag_weights.append(tf.placeholder(tf.float32, shape=[None],\n                                                name=""weight{0}"".format(i)))\n    self.labels.append(tf.placeholder(tf.float32, shape=[None], name=""label""))\n\n    base_rnn_output = self.generate_rnn_output()\n    encoder_outputs, encoder_state, attention_states = base_rnn_output\n    \n    if task[\'tagging\'] == 1:\n       seq_labeling_outputs = seq_labeling.generate_sequence_output(\n                                   self.source_vocab_size,\n                                   encoder_outputs, \n                                   encoder_state, \n                                   self.tags, \n                                   self.sequence_length, \n                                   self.tag_vocab_size, \n                                   self.tag_weights,\n                                   buckets, \n                                   softmax_loss_function=softmax_loss_function, \n                                   use_attention=use_attention)\n       self.tagging_output, self.tagging_loss = seq_labeling_outputs\n    if task[\'intent\'] == 1:\n      seq_intent_outputs = seq_classification.generate_single_output(\n                                    encoder_state, \n                                    attention_states, \n                                    self.sequence_length, \n                                    self.labels, \n                                    self.label_vocab_size,\n                                    buckets, \n                                    softmax_loss_function=softmax_loss_function, \n                                    use_attention=use_attention)\n      self.classification_output, self.classification_loss = seq_intent_outputs\n    \n    if task[\'tagging\'] == 1:\n      self.loss = self.tagging_loss\n    elif task[\'intent\'] == 1:\n      self.loss = self.classification_loss\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    if not forward_only:\n      opt = tf.train.AdamOptimizer()\n      if task[\'joint\'] == 1:\n        # backpropagate the intent and tagging loss, one may further adjust \n        # the weights for the two costs.\n        gradients = tf.gradients([self.tagging_loss, self.classification_loss], \n                                 params)\n      elif task[\'tagging\'] == 1:\n        gradients = tf.gradients(self.tagging_loss, params)\n      elif task[\'intent\'] == 1:\n        gradients = tf.gradients(self.classification_loss, params)\n        \n      clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                       max_gradient_norm)\n      self.gradient_norm = norm\n      self.update = opt.apply_gradients(\n          zip(clipped_gradients, params), global_step=self.global_step)\n\n    self.saver = tf.train.Saver(tf.global_variables())\n\n  def generate_rnn_output(self):\n    """"""\n    Generate RNN state outputs with word embeddings as inputs\n    """"""\n    with tf.variable_scope(""generate_seq_output""):\n      if self.bidirectional_rnn:\n        embedding = tf.get_variable(""embedding"",\n                                    [self.source_vocab_size,\n                                     self.word_embedding_size])\n        encoder_emb_inputs = list()\n        encoder_emb_inputs = [tf.nn.embedding_lookup(embedding, encoder_input)\\\n                                for encoder_input in self.encoder_inputs]\n        rnn_outputs = static_bidirectional_rnn(self.cell_fw,\n                                               self.cell_bw, \n                                               encoder_emb_inputs, \n                                               sequence_length=self.sequence_length,\n                                               dtype=tf.float32)\n        encoder_outputs, encoder_state_fw, encoder_state_bw = rnn_outputs\n        # with state_is_tuple = True, if num_layers > 1, \n        # here we simply use the state from last layer as the encoder state\n        state_fw = encoder_state_fw[-1]\n        state_bw = encoder_state_bw[-1]\n        encoder_state = tf.concat([tf.concat(state_fw, 1),\n                                   tf.concat(state_bw, 1)], 1)\n        top_states = [tf.reshape(e, [-1, 1, self.cell_fw.output_size \\\n                                  + self.cell_bw.output_size])\n                      for e in encoder_outputs]\n        attention_states = tf.concat(top_states, 1)\n      else:\n        embedding = tf.get_variable(""embedding"", \n                                    [self.source_vocab_size,\n                                     self.word_embedding_size])\n        encoder_emb_inputs = list()\n        encoder_emb_inputs = [tf.nn.embedding_lookup(embedding, encoder_input)\\\n                              for encoder_input in self.encoder_inputs] \n        rnn_outputs = static_rnn(self.cell_fw,\n                                 encoder_emb_inputs,\n                                 sequence_length=self.sequence_length,\n                                 dtype=tf.float32)\n        encoder_outputs, encoder_state = rnn_outputs\n        # with state_is_tuple = True, if num_layers > 1, \n        # here we use the state from last layer as the encoder state\n        state = encoder_state[-1]\n        encoder_state = tf.concat(state, 1)\n        top_states = [tf.reshape(e, [-1, 1, self.cell_fw.output_size])\n                      for e in encoder_outputs]\n        attention_states = tf.concat(top_states, 1)\n      return encoder_outputs, encoder_state, attention_states\n  \n  def joint_step(self, session, encoder_inputs, tags, tag_weights, \n                 labels, batch_sequence_length,\n           bucket_id, forward_only):\n    """"""Run a step of the joint model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      tags: list of numpy int vectors to feed as decoder inputs.\n      tag_weights: list of numpy float vectors to feed as tag weights.\n      labels: list of numpy int vectors to feed as sequence class labels.\n      bucket_id: which bucket of the model to use.\n      batch_sequence_length: batch_sequence_length\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, output tags, and output class label.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, tag_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n    if len(tags) != tag_size:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(tags), tag_size))\n    if len(labels) != 1:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(labels), 1))\n\n    input_feed = {}\n    input_feed[self.sequence_length.name] = batch_sequence_length\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n      input_feed[self.tags[l].name] = tags[l]\n      input_feed[self.tag_weights[l].name] = tag_weights[l]\n    input_feed[self.labels[0].name] = labels[0]\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.update,  # Update Op that does SGD.\n                     self.gradient_norm,  # Gradient norm.\n                     self.loss] # Loss for this batch.\n      for i in range(tag_size):\n        output_feed.append(self.tagging_output[i])\n      output_feed.append(self.classification_output[0])\n    else:\n      output_feed = [self.loss]\n      for i in range(tag_size):\n        output_feed.append(self.tagging_output[i])\n      output_feed.append(self.classification_output[0])\n\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], outputs[3:3+tag_size], outputs[-1]\n    else:\n      return None, outputs[0], outputs[1:1+tag_size], outputs[-1]\n\n\n  def tagging_step(self, session, encoder_inputs, tags, tag_weights, \n                   batch_sequence_length, bucket_id, forward_only):\n    """"""Run a step of the tagging model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      tags: list of numpy int vectors to feed as decoder inputs.\n      tag_weights: list of numpy float vectors to feed as target weights.\n      batch_sequence_length: batch_sequence_length\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, and the output tags.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, tag_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n    if len(tags) != tag_size:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(tags), tag_size))\n\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n    input_feed = {}\n    input_feed[self.sequence_length.name] = batch_sequence_length\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n      input_feed[self.tags[l].name] = tags[l]\n      input_feed[self.tag_weights[l].name] = tag_weights[l]\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.update,  # Update Op that does SGD.\n                     self.gradient_norm,  # Gradient norm.\n                     self.loss] # Loss for this batch.\n      for i in range(tag_size):\n        output_feed.append(self.tagging_output[i])\n    else:\n      output_feed = [self.loss]\n      for i in range(tag_size):\n        output_feed.append(self.tagging_output[i])\n\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], outputs[3:3+tag_size]\n    else:\n      return None, outputs[0], outputs[1:1+tag_size]\n\n  def classification_step(self, session, encoder_inputs, labels, \n                          batch_sequence_length, bucket_id, forward_only):\n    """"""Run a step of the intent classification model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      labels: list of numpy int vectors to feed as sequence class labels.\n      batch_sequence_length: batch_sequence_length\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, and the output class label.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, target_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n    input_feed = {}\n    input_feed[self.sequence_length.name] = batch_sequence_length\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n    input_feed[self.labels[0].name] = labels[0]\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.update,  # Update Op that does SGD.\n                     self.gradient_norm,  # Gradient norm.\n                     self.loss,    # Loss for this batch.\n                     self.classification_output[0]]\n    else:\n      output_feed = [self.loss,\n                     self.classification_output[0],]\n\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], outputs[3]  # Gradient norm, loss, outputs.\n    else:\n      return None, outputs[0], outputs[1] # No gradient norm, loss, outputs.\n\n\n  def get_batch(self, data, bucket_id):\n    """"""Get a random batch of data from the specified bucket, prepare for step.\n\n    To feed data in step(..) it must be a list of batch-major vectors, while\n    data here contains single length-major cases. So the main logic of this\n    function is to re-index data cases to be in the proper format for feeding.\n\n    Args:\n      data: a tuple of size len(self.buckets) in which each element contains\n        lists of pairs of input and output data that we use to create a batch.\n      bucket_id: integer, which bucket to get the batch for.\n\n    Returns:\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\n      the constructed batch that has the proper format to call step(...) later.\n    """"""\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    encoder_inputs, decoder_inputs, labels = [], [], []\n\n    # Get a random batch of encoder and decoder inputs from data,\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\n    batch_sequence_length_list= list()\n    for _ in xrange(self.batch_size):\n      encoder_input, decoder_input, label = random.choice(data[bucket_id])\n      batch_sequence_length_list.append(len(encoder_input))\n\n      # Encoder inputs are padded and then reversed.\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n      #encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n      encoder_inputs.append(list(encoder_input + encoder_pad))\n\n      # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n      decoder_pad_size = decoder_size - len(decoder_input)\n      decoder_inputs.append(decoder_input +\n                            [data_utils.PAD_ID] * decoder_pad_size)\n      labels.append(label)\n\n    # Now we create batch-major vectors from the data selected above.\n    batch_encoder_inputs = []\n    batch_decoder_inputs = []\n    batch_weights = []\n    batch_labels = []\n\n    # Batch encoder inputs are just re-indexed encoder_inputs.\n    for length_idx in xrange(encoder_size):\n      batch_encoder_inputs.append(\n          np.array([encoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n    for length_idx in xrange(decoder_size):\n      batch_decoder_inputs.append(\n          np.array([decoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))                        \n      # Create target_weights to be 0 for targets that are padding.\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n      for batch_idx in xrange(self.batch_size):\n        # We set weight to 0 if the corresponding target is a PAD symbol.\n        # The corresponding target is decoder_input shifted by 1 forward.\n#        if length_idx < decoder_size - 1:\n#          target = decoder_inputs[batch_idx][length_idx + 1]\n#        print (length_idx)\n        if decoder_inputs[batch_idx][length_idx] == data_utils.PAD_ID:\n          batch_weight[batch_idx] = 0.0\n      batch_weights.append(batch_weight)\n    \n    batch_labels.append(\n      np.array([labels[batch_idx][0]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n                        \n    batch_sequence_length = np.array(batch_sequence_length_list, dtype=np.int32)\n    return (batch_encoder_inputs, batch_decoder_inputs, batch_weights, \n            batch_sequence_length, batch_labels)\n\n\n  def get_one(self, data, bucket_id, sample_id):\n    """"""Get a single sample data from the specified bucket, prepare for step.\n\n    To feed data in step(..) it must be a list of batch-major vectors, while\n    data here contains single length-major cases. So the main logic of this\n    function is to re-index data cases to be in the proper format for feeding.\n\n    Args:\n      data: a tuple of size len(self.buckets) in which each element contains\n        lists of pairs of input and output data that we use to create a batch.\n      bucket_id: integer, which bucket to get the batch for.\n\n    Returns:\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\n      the constructed batch that has the proper format to call step(...) later.\n    """"""\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    encoder_inputs, decoder_inputs, labels = [], [], []\n\n    # Get a random batch of encoder and decoder inputs from data,\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\n    batch_sequence_length_list= list()\n    #for _ in xrange(self.batch_size):\n    encoder_input, decoder_input, label = data[bucket_id][sample_id]\n    batch_sequence_length_list.append(len(encoder_input))\n\n      # Encoder inputs are padded and then reversed.\n    encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n      #encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n    encoder_inputs.append(list(encoder_input + encoder_pad))\n\n    # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n    decoder_pad_size = decoder_size - len(decoder_input)\n    decoder_inputs.append(decoder_input +\n                            [data_utils.PAD_ID] * decoder_pad_size)\n    labels.append(label)\n\n    # Now we create batch-major vectors from the data selected above.\n    batch_encoder_inputs = []\n    batch_decoder_inputs = []\n    batch_weights = []\n    batch_labels = []\n\n    # Batch encoder inputs are just re-indexed encoder_inputs.\n    for length_idx in xrange(encoder_size):\n      batch_encoder_inputs.append(\n          np.array([encoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(1)], dtype=np.int32))\n\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n    for length_idx in xrange(decoder_size):\n      batch_decoder_inputs.append(\n          np.array([decoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(1)], dtype=np.int32))\n\n      # Create target_weights to be 0 for targets that are padding.\n      batch_weight = np.ones(1, dtype=np.float32)\n      for batch_idx in xrange(1):\n        # We set weight to 0 if the corresponding target is a PAD symbol.\n        # The corresponding target is decoder_input shifted by 1 forward.\n#        if length_idx < decoder_size - 1:\n#          target = decoder_inputs[batch_idx][length_idx + 1]\n#        print (length_idx)\n        if decoder_inputs[batch_idx][length_idx] == data_utils.PAD_ID:\n          batch_weight[batch_idx] = 0.0\n      batch_weights.append(batch_weight)\n      \n    batch_labels.append(\n      np.array([labels[batch_idx][0]\n                for batch_idx in xrange(1)], dtype=np.int32))\n                    \n    batch_sequence_length = np.array(batch_sequence_length_list, dtype=np.int32)\n    return (batch_encoder_inputs, batch_decoder_inputs, batch_weights, \n            batch_sequence_length, batch_labels)'"
run_multi-task_rnn.py,32,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Feb 28 16:23:37 2016\n\n@author: Bing Liu (liubing@cmu.edu)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport sys\nimport time\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nimport data_utils\nimport multi_task_model\n\nimport subprocess\nimport stat\n\n\n#tf.app.flags.DEFINE_float(""learning_rate"", 0.1, ""Learning rate."")\n#tf.app.flags.DEFINE_float(""learning_rate_decay_factor"", 0.9,\n#                          ""Learning rate decays by this much."")\ntf.app.flags.DEFINE_float(""max_gradient_norm"", 5.0,\n                          ""Clip gradients to this norm."")\ntf.app.flags.DEFINE_integer(""batch_size"", 16,\n                            ""Batch size to use during training."")\ntf.app.flags.DEFINE_integer(""size"", 128, ""Size of each model layer."")\ntf.app.flags.DEFINE_integer(""word_embedding_size"", 128, ""word embedding size"")\ntf.app.flags.DEFINE_integer(""num_layers"", 1, ""Number of layers in the model."")\ntf.app.flags.DEFINE_integer(""in_vocab_size"", 10000, ""max vocab Size."")\ntf.app.flags.DEFINE_integer(""out_vocab_size"", 10000, ""max tag vocab Size."")\ntf.app.flags.DEFINE_string(""data_dir"", ""/tmp"", ""Data directory"")\ntf.app.flags.DEFINE_string(""train_dir"", ""/tmp"", ""Training directory."")\ntf.app.flags.DEFINE_integer(""max_train_data_size"", 0,\n                            ""Limit on the size of training data (0: no limit)"")\ntf.app.flags.DEFINE_integer(""steps_per_checkpoint"", 100,\n                            ""How many training steps to do per checkpoint."")\ntf.app.flags.DEFINE_integer(""max_training_steps"", 30000,\n                            ""Max training steps."")\ntf.app.flags.DEFINE_integer(""max_test_data_size"", 0,\n                            ""Max size of test set."")\ntf.app.flags.DEFINE_boolean(""use_attention"", True,\n                            ""Use attention based RNN"")\ntf.app.flags.DEFINE_integer(""max_sequence_length"", 0,\n                            ""Max sequence length."")\ntf.app.flags.DEFINE_float(""dropout_keep_prob"", 0.5,\n                          ""dropout keep cell input and output prob."")  \ntf.app.flags.DEFINE_boolean(""bidirectional_rnn"", True,\n                            ""Use birectional RNN"")\ntf.app.flags.DEFINE_string(""task"", None, ""Options: joint; intent; tagging"")\nFLAGS = tf.app.flags.FLAGS\n    \nif FLAGS.max_sequence_length == 0:\n    print (\'Please indicate max sequence length. Exit\')\n    exit()\n\nif FLAGS.task is None:\n    print (\'Please indicate task to run.\' + \n           \'Available options: intent; tagging; joint\')\n    exit()\n\ntask = dict({\'intent\':0, \'tagging\':0, \'joint\':0})\nif FLAGS.task == \'intent\':\n    task[\'intent\'] = 1\nelif FLAGS.task == \'tagging\':\n    task[\'tagging\'] = 1\nelif FLAGS.task == \'joint\':\n    task[\'intent\'] = 1\n    task[\'tagging\'] = 1\n    task[\'joint\'] = 1\n    \n_buckets = [(FLAGS.max_sequence_length, FLAGS.max_sequence_length)]\n#_buckets = [(3, 10), (10, 25)]\n\n# metrics function using conlleval.pl\ndef conlleval(p, g, w, filename):\n    \'\'\'\n    INPUT:\n    p :: predictions\n    g :: groundtruth\n    w :: corresponding words\n\n    OUTPUT:\n    filename :: name of the file where the predictions\n    are written. it will be the input of conlleval.pl script\n    for computing the performance in terms of precision\n    recall and f1 score\n    \'\'\'\n    out = \'\'\n    for sl, sp, sw in zip(g, p, w):\n        out += \'BOS O O\\n\'\n        for wl, wp, w in zip(sl, sp, sw):\n            out += w + \' \' + wl + \' \' + wp + \'\\n\'\n        out += \'EOS O O\\n\\n\'\n\n    f = open(filename, \'w\')\n    f.writelines(out[:-1]) # remove the ending \\n on last line\n    f.close()\n\n    return get_perf(filename)\n\ndef get_perf(filename):\n    \'\'\' run conlleval.pl perl script to obtain\n    precision/recall and F1 score \'\'\'\n    _conlleval = os.path.dirname(os.path.realpath(__file__)) + \'/conlleval.pl\'\n    os.chmod(_conlleval, stat.S_IRWXU)  # give the execute permissions\n\n    proc = subprocess.Popen([""perl"",\n                            _conlleval],\n                            stdin=subprocess.PIPE,\n                            stdout=subprocess.PIPE)\n\n    stdout, _ = proc.communicate(\'\'.join(open(filename).readlines()))\n    for line in stdout.split(\'\\n\'):\n        if \'accuracy\' in line:\n            out = line.split()\n            break\n\n    precision = float(out[6][:-2])\n    recall = float(out[8][:-2])\n    f1score = float(out[10])\n\n    return {\'p\': precision, \'r\': recall, \'f1\': f1score}\n\n\ndef read_data(source_path, target_path, label_path, max_size=None):\n  """"""Read data from source and target files and put into buckets.\n\n  Args:\n    source_path: path to the files with token-ids for the word sequence.\n    target_path: path to the file with token-ids for the tag sequence;\n      it must be aligned with the source file: n-th line contains the desired\n      output for n-th line from the source_path.\n    label_path: path to the file with token-ids for the intent label\n    max_size: maximum number of lines to read, all other will be ignored;\n      if 0 or None, data files will be read completely (no limit).\n\n  Returns:\n    data_set: a list of length len(_buckets); data_set[n] contains a list of\n      (source, target, label) tuple read from the provided data files that fit\n      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n      len(target) < _buckets[n][1];source, target, label are lists of token-ids\n  """"""\n  data_set = [[] for _ in _buckets]\n  with tf.gfile.GFile(source_path, mode=""r"") as source_file:\n    with tf.gfile.GFile(target_path, mode=""r"") as target_file:\n      with tf.gfile.GFile(label_path, mode=""r"") as label_file:\n        source = source_file.readline()\n        target = target_file.readline()\n        label = label_file.readline()\n        counter = 0\n        while source and target and label and (not max_size \\\n                                               or counter < max_size):\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  reading data line %d"" % counter)\n            sys.stdout.flush()\n          source_ids = [int(x) for x in source.split()]\n          target_ids = [int(x) for x in target.split()]\n          label_ids = [int(x) for x in label.split()]\n#          target_ids.append(data_utils.EOS_ID)\n          for bucket_id, (source_size, target_size) in enumerate(_buckets):\n            if len(source_ids) < source_size and len(target_ids) < target_size:\n              data_set[bucket_id].append([source_ids, target_ids, label_ids])\n              break\n          source = source_file.readline()\n          target = target_file.readline()\n          label = label_file.readline()\n  return data_set # 3 outputs in each unit: source_ids, target_ids, label_ids \n\ndef create_model(session, \n                 source_vocab_size, \n                 target_vocab_size, \n                 label_vocab_size):\n  """"""Create model and initialize or load parameters in session.""""""\n  with tf.variable_scope(""model"", reuse=None):\n    model_train = multi_task_model.MultiTaskModel(\n          source_vocab_size, \n          target_vocab_size, \n          label_vocab_size, \n          _buckets,\n          FLAGS.word_embedding_size, \n          FLAGS.size, FLAGS.num_layers, \n          FLAGS.max_gradient_norm, \n          FLAGS.batch_size,\n          dropout_keep_prob=FLAGS.dropout_keep_prob, \n          use_lstm=True,\n          forward_only=False, \n          use_attention=FLAGS.use_attention,\n          bidirectional_rnn=FLAGS.bidirectional_rnn,\n          task=task)\n  with tf.variable_scope(""model"", reuse=True):\n    model_test = multi_task_model.MultiTaskModel(\n          source_vocab_size, \n          target_vocab_size, \n          label_vocab_size, \n          _buckets,\n          FLAGS.word_embedding_size, \n          FLAGS.size, \n          FLAGS.num_layers, \n          FLAGS.max_gradient_norm, \n          FLAGS.batch_size,\n          dropout_keep_prob=FLAGS.dropout_keep_prob, \n          use_lstm=True,\n          forward_only=True, \n          use_attention=FLAGS.use_attention,\n          bidirectional_rnn=FLAGS.bidirectional_rnn,\n          task=task)\n\n  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n  if ckpt:\n    print(""Reading model parameters from %s"" % ckpt.model_checkpoint_path)\n    model_train.saver.restore(session, ckpt.model_checkpoint_path)\n  else:\n    print(""Created model with fresh parameters."")\n    session.run(tf.global_variables_initializer())\n  return model_train, model_test\n        \ndef train():\n  print (\'Applying Parameters:\')\n  for k,v in FLAGS.__dict__[\'__flags\'].iteritems():\n    print (\'%s: %s\' % (k, str(v)))\n  print(""Preparing data in %s"" % FLAGS.data_dir)\n  vocab_path = \'\'\n  tag_vocab_path = \'\'\n  label_vocab_path = \'\'\n  date_set = data_utils.prepare_multi_task_data(\n    FLAGS.data_dir, FLAGS.in_vocab_size, FLAGS.out_vocab_size)\n  in_seq_train, out_seq_train, label_train = date_set[0]\n  in_seq_dev, out_seq_dev, label_dev = date_set[1]\n  in_seq_test, out_seq_test, label_test = date_set[2]\n  vocab_path, tag_vocab_path, label_vocab_path = date_set[3]\n     \n  result_dir = FLAGS.train_dir + \'/test_results\'\n  if not os.path.isdir(result_dir):\n      os.makedirs(result_dir)\n\n  current_taging_valid_out_file = result_dir + \'/tagging.valid.hyp.txt\'\n  current_taging_test_out_file = result_dir + \'/tagging.test.hyp.txt\'\n\n  vocab, rev_vocab = data_utils.initialize_vocab(vocab_path)\n  tag_vocab, rev_tag_vocab = data_utils.initialize_vocab(tag_vocab_path)\n  label_vocab, rev_label_vocab = data_utils.initialize_vocab(label_vocab_path)\n\n  config = tf.ConfigProto(\n      gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.23),\n      #device_count = {\'gpu\': 2}\n  )\n    \n  with tf.Session(config=config) as sess:\n    # Create model.\n    print(""Max sequence length: %d."" % _buckets[0][0])\n    print(""Creating %d layers of %d units."" % (FLAGS.num_layers, FLAGS.size))\n    \n    model, model_test = create_model(sess, \n                                     len(vocab), \n                                     len(tag_vocab), \n                                     len(label_vocab))\n    print (""Creating model with "" + \n           ""source_vocab_size=%d, target_vocab_size=%d, label_vocab_size=%d."" \\\n           % (len(vocab), len(tag_vocab), len(label_vocab)))\n\n    # Read data into buckets and compute their sizes.\n    print (""Reading train/valid/test data (training set limit: %d).""\n           % FLAGS.max_train_data_size)\n    dev_set = read_data(in_seq_dev, out_seq_dev, label_dev)\n    test_set = read_data(in_seq_test, out_seq_test, label_test)\n    train_set = read_data(in_seq_train, out_seq_train, label_train)\n    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n    train_total_size = float(sum(train_bucket_sizes))\n\n    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n                           for i in xrange(len(train_bucket_sizes))]\n\n    # This is the training loop.\n    step_time, loss = 0.0, 0.0\n    current_step = 0\n\n    best_valid_score = 0\n    best_test_score = 0\n    while model.global_step.eval() < FLAGS.max_training_steps:\n      random_number_01 = np.random.random_sample()\n      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                       if train_buckets_scale[i] > random_number_01])\n\n      # Get a batch and make a step.\n      start_time = time.time()\n      batch_data = model.get_batch(train_set, bucket_id)\n      encoder_inputs,tags,tag_weights,batch_sequence_length,labels = batch_data\n      if task[\'joint\'] == 1:\n        step_outputs = model.joint_step(sess, \n                                        encoder_inputs, \n                                        tags, \n                                        tag_weights, \n                                        labels, \n                                        batch_sequence_length, \n                                        bucket_id, \n                                        False)\n        _, step_loss, tagging_logits, class_logits = step_outputs\n      elif task[\'tagging\'] == 1:\n        step_outputs = model.tagging_step(sess, \n                                          encoder_inputs,\n                                          tags,\n                                          tag_weights,\n                                          batch_sequence_length, \n                                          bucket_id, \n                                          False)\n        _, step_loss, tagging_logits = step_outputs\n      elif task[\'intent\'] == 1:\n        step_outputs = model.classification_step(sess, \n                                                 encoder_inputs, \n                                                 labels,\n                                                 batch_sequence_length, \n                                                 bucket_id, \n                                                 False)  \n        _, step_loss, class_logits = step_outputs\n\n      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n      loss += step_loss / FLAGS.steps_per_checkpoint\n      current_step += 1\n\n      # Once in a while, we save checkpoint, print statistics, and run evals.\n      if current_step % FLAGS.steps_per_checkpoint == 0:\n        perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\n        print (""global step %d step-time %.2f. Training perplexity %.2f"" \n            % (model.global_step.eval(), step_time, perplexity))\n        sys.stdout.flush()\n        # Save checkpoint and zero timer and loss.\n        checkpoint_path = os.path.join(FLAGS.train_dir, ""model.ckpt"")\n        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n        step_time, loss = 0.0, 0.0 \n        \n        def run_valid_test(data_set, mode): # mode: Eval, Test\n        # Run evals on development/test set and print the accuracy.\n            word_list = list() \n            ref_tag_list = list() \n            hyp_tag_list = list()\n            ref_label_list = list()\n            hyp_label_list = list()\n            correct_count = 0\n            accuracy = 0.0\n            tagging_eval_result = dict()\n            for bucket_id in xrange(len(_buckets)):\n              eval_loss = 0.0\n              count = 0\n              for i in xrange(len(data_set[bucket_id])):\n                count += 1\n                sample = model_test.get_one(data_set, bucket_id, i)\n                encoder_inputs,tags,tag_weights,sequence_length,labels = sample\n                tagging_logits = []\n                class_logits = []\n                if task[\'joint\'] == 1:\n                  step_outputs = model_test.joint_step(sess, \n                                                       encoder_inputs, \n                                                       tags, \n                                                       tag_weights, \n                                                       labels,\n                                                       sequence_length, \n                                                       bucket_id, \n                                                       True)\n                  _, step_loss, tagging_logits, class_logits = step_outputs\n                elif task[\'tagging\'] == 1:\n                  step_outputs = model_test.tagging_step(sess, \n                                                         encoder_inputs, \n                                                         tags, \n                                                         tag_weights,\n                                                         sequence_length, \n                                                         bucket_id, \n                                                         True)\n                  _, step_loss, tagging_logits = step_outputs\n                elif task[\'intent\'] == 1:\n                  step_outputs = model_test.classification_step(sess, \n                                                                encoder_inputs, \n                                                                labels,\n                                                                sequence_length, \n                                                                bucket_id, \n                                                                True) \n                  _, step_loss, class_logits = step_outputs\n                eval_loss += step_loss / len(data_set[bucket_id])\n                hyp_label = None\n                if task[\'intent\'] == 1:\n                  ref_label_list.append(rev_label_vocab[labels[0][0]])\n                  hyp_label = np.argmax(class_logits[0],0)\n                  hyp_label_list.append(rev_label_vocab[hyp_label])\n                  if labels[0] == hyp_label:\n                    correct_count += 1\n                if task[\'tagging\'] == 1:\n                  word_list.append([rev_vocab[x[0]] for x in \\\n                                    encoder_inputs[:sequence_length[0]]])\n                  ref_tag_list.append([rev_tag_vocab[x[0]] for x in \\\n                                       tags[:sequence_length[0]]])\n                  hyp_tag_list.append(\n                          [rev_tag_vocab[np.argmax(x)] for x in \\\n                                         tagging_logits[:sequence_length[0]]])\n\n            accuracy = float(correct_count)*100/count\n            if task[\'intent\'] == 1:\n              print(""  %s accuracy: %.2f %d/%d"" \\\n                    % (mode, accuracy, correct_count, count))\n              sys.stdout.flush()\n            if task[\'tagging\'] == 1:\n              if mode == \'Eval\':\n                  taging_out_file = current_taging_valid_out_file\n              elif mode == \'Test\':\n                  taging_out_file = current_taging_test_out_file\n              tagging_eval_result = conlleval(hyp_tag_list, \n                                              ref_tag_list, \n                                              word_list, \n                                              taging_out_file)\n              print(""  %s f1-score: %.2f"" % (mode, tagging_eval_result[\'f1\']))\n              sys.stdout.flush()\n            return accuracy, tagging_eval_result\n            \n        # valid\n        valid_accuracy, valid_tagging_result = run_valid_test(dev_set, \'Eval\')        \n        if task[\'tagging\'] == 1 \\\n            and valid_tagging_result[\'f1\'] > best_valid_score:\n          best_valid_score = valid_tagging_result[\'f1\']\n          # save the best output file\n          subprocess.call([\'mv\', \n                           current_taging_valid_out_file, \n                           current_taging_valid_out_file + \'.best_f1_%.2f\' \\\n                           % best_valid_score])\n        # test, run test after each validation for development purpose.\n        test_accuracy, test_tagging_result = run_valid_test(test_set, \'Test\')        \n        if task[\'tagging\'] == 1 \\\n            and test_tagging_result[\'f1\'] > best_test_score:\n          best_test_score = test_tagging_result[\'f1\']\n          # save the best output file\n          subprocess.call([\'mv\', \n                           current_taging_test_out_file, \n                           current_taging_test_out_file + \'.best_f1_%.2f\' \\\n                           % best_test_score])\n          \ndef main(_):\n    train()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
seq_classification.py,32,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Feb  28 15:28:44 2016\n\n@author: Bing Liu (liubing@cmu.edu)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n# We disable pylint because we need python3 compatibility.\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn_cell_impl\n\nlinear = rnn_cell_impl._linear\n\ndef attention_single_output_decoder(initial_state, \n                                    attention_states,\n                                    output_size=None,\n                                    num_heads=1,\n                                    dtype=tf.float32,\n                                    scope=None,\n                                    sequence_length=tf.ones([16]),\n                                    initial_state_attention=True,\n                                    use_attention=False):\n\n  if num_heads < 1:\n    raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  if not attention_states.get_shape()[1:2].is_fully_defined():\n    raise ValueError(""Shape[1] and [2] of attention_states must be known: %s""\n                     % attention_states.get_shape())\n\n  with tf.variable_scope(scope or ""decoder_single_output""):\n#    print (initial_state.eval().shape)\n    batch_size = tf.shape(initial_state)[0]  # Needed for reshaping.\n#    print (attention_states.get_shape())\n    attn_length = attention_states.get_shape()[1].value\n    attn_size = attention_states.get_shape()[2].value\n\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n    hidden = tf.reshape(\n        attention_states, [-1, attn_length, 1, attn_size])\n    hidden_features = []\n    v = []\n    attention_vec_size = attn_size  # Size of query vectors for attention.\n    for a in xrange(num_heads):\n      k = tf.get_variable(""AttnW_%d"" % a,\n                                      [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(tf.nn.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n      v.append(tf.get_variable(""AttnV_%d"" % a,\n                                           [attention_vec_size]))\n\n#     state = initial_state\n\n    def attention(query, use_attention=False):\n      """"""Put attention masks on hidden using hidden_features and query.""""""\n      attn_weights = []\n      ds = []  # Results of attention reads will be stored here.\n      for i in xrange(num_heads):\n        with tf.variable_scope(""Attention_%d"" % i):\n          # y = linear(query, attention_vec_size, True)\n          y = linear(query, attention_vec_size, True)\n          y = tf.reshape(y, [-1, 1, 1, attention_vec_size])\n          # Attention mask is a softmax of v^T * tanh(...).\n          s = tf.reduce_sum(\n              v[i] * tf.tanh(hidden_features[i] + y), [2, 3])\n          if use_attention is False: # apply mean pooling\n              weights = tf.tile(sequence_length, tf.stack([attn_length]))\n              weights = tf.reshape(weights, tf.shape(s))\n              a = tf.ones(tf.shape(s), dtype=dtype) / tf.to_float(weights)\n              # a = tf.ones(tf.shape(s), dtype=dtype) / tf.to_float(tf.shape(s)[1])\n          else:\n            a = tf.nn.softmax(s)\n          attn_weights.append(a)\n          # Now calculate the attention-weighted vector d.\n          d = tf.reduce_sum(\n              tf.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n              [1, 2])\n          ds.append(tf.reshape(d, [-1, attn_size]))\n      return attn_weights, ds\n\n    batch_attn_size = tf.stack([batch_size, attn_size])\n    attns = [tf.zeros(batch_attn_size, dtype=dtype)\n             for _ in xrange(num_heads)]\n    for a in attns:  # Ensure the second shape of attention vectors is set.\n      a.set_shape([None, attn_size])\n    if initial_state_attention:\n      attn_weights, attns = attention(initial_state, use_attention=use_attention)\n    \n    #with variable_scope.variable_scope(scope or ""Linear""):\n    matrix = tf.get_variable(""Out_Matrix"", [attn_size, output_size])\n    res = tf.matmul(attns[0], matrix) \n    # NOTE: here we temporarily assume num_head = 1\n    bias_start = 0.0\n    bias_term = tf.get_variable(""Out_Bias"", \n                                [output_size],\n                                initializer=tf.constant_initializer(bias_start))\n    output = res + bias_term\n  # NOTE: here we temporarily assume num_head = 1\n  return attention_states, attn_weights[0], attns[0], [output] \n  \ndef generate_single_output(encoder_state, attention_states, sequence_length, \n                           targets, num_classes, buckets, \n                           use_mean_attention=False,\n                           softmax_loss_function=None, per_example_loss=False, \n                           name=None, use_attention=False):\n  all_inputs = targets\n  with tf.name_scope(name, ""model_with_buckets"", all_inputs):\n    with tf.variable_scope(tf.get_variable_scope(),\n                                       reuse=None):\n      single_outputs = attention_single_output_decoder(encoder_state, \n                                                      attention_states, \n                                                      output_size=num_classes,\n                                                      num_heads=1,\n                                                      sequence_length=sequence_length,\n                                                      use_attention=use_attention)\n      _, _, _, bucket_outputs = single_outputs\n        \n      if softmax_loss_function is None:\n        assert len(bucket_outputs) == len(targets) == 1\n        # We need to make target and int64-tensor and set its shape.\n        bucket_target = tf.reshape(tf.to_int64(targets[0]), [-1])\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=bucket_outputs[0], labels=bucket_target)\n      else:\n        assert len(bucket_outputs) == len(targets) == 1\n        crossent = softmax_loss_function(bucket_outputs[0], targets[0])\n       \n      batch_size = tf.shape(targets[0])[0]\n      loss = tf.reduce_sum(crossent) / tf.cast(batch_size, tf.float32)\n\n  return bucket_outputs, loss'"
seq_labeling.py,36,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Feb  28 11:32:21 2016\n\n@author: Bing Liu (liubing@cmu.edu)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# We disable pylint because we need python3 compatibility.\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n#from six.moves import zip     # pylint: disable=redefined-builtin\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.contrib.legacy_seq2seq import sequence_loss_by_example\nfrom tensorflow.contrib.legacy_seq2seq import sequence_loss\n\nfrom tensorflow.python.ops import rnn_cell_impl\n\nlinear = rnn_cell_impl._linear\n\ndef _step(time, sequence_length, min_sequence_length, \n          max_sequence_length, zero_logit, generate_logit):\n  # Step 1: determine whether we need to call_cell or not\n  empty_update = lambda: zero_logit\n  logit = control_flow_ops.cond(\n      time < max_sequence_length, generate_logit, empty_update)\n\n  # Step 2: determine whether we need to copy through state and/or outputs\n  existing_logit = lambda: logit\n\n  def copy_through():\n    # Use broadcasting select to determine which values should get\n    # the previous state & zero output, and which values should get\n    # a calculated state & output.\n    copy_cond = (time >= sequence_length)\n    return tf.where(copy_cond, zero_logit, logit)\n\n  logit = control_flow_ops.cond(\n      time < min_sequence_length, existing_logit, copy_through)\n  logit.set_shape(zero_logit.get_shape())\n  return logit\n\ndef attention_RNN(encoder_outputs, \n                  encoder_state,\n                  num_decoder_symbols,\n                  sequence_length,\n                  num_heads=1,\n                  dtype=tf.float32,\n                  use_attention=True,\n                  loop_function=None,\n                  scope=None):\n  if use_attention:\n    print (\'Use the attention RNN model\')\n    if num_heads < 1:\n      raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n  \n    with tf.variable_scope(scope or ""attention_RNN""):\n      output_size = encoder_outputs[0].get_shape()[1].value\n      top_states = [tf.reshape(e, [-1, 1, output_size])\n                  for e in encoder_outputs]\n      attention_states = tf.concat(top_states, 1)\n      if not attention_states.get_shape()[1:2].is_fully_defined():\n        raise ValueError(""Shape[1] and [2] of attention_states must be known: %s""\n                       % attention_states.get_shape())\n  \n      batch_size = tf.shape(top_states[0])[0]  # Needed for reshaping.\n      attn_length = attention_states.get_shape()[1].value\n      attn_size = attention_states.get_shape()[2].value\n  \n      # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n      hidden = tf.reshape(\n          attention_states, [-1, attn_length, 1, attn_size])\n      hidden_features = []\n      v = []\n      attention_vec_size = attn_size  # Size of query vectors for attention.\n      for a in xrange(num_heads):\n        k = tf.get_variable(""AttnW_%d"" % a,\n                                        [1, 1, attn_size, attention_vec_size])\n        hidden_features.append(tf.nn.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n        v.append(tf.get_variable(""AttnV_%d"" % a,\n                                             [attention_vec_size]))\n  \n      def attention(query):\n        """"""Put attention masks on hidden using hidden_features and query.""""""\n        attn_weights = []\n        ds = []  # Results of attention reads will be stored here.\n        for i in xrange(num_heads):\n          with tf.variable_scope(""Attention_%d"" % i):\n            #y = linear(query, attention_vec_size, True)\n            y = linear(query, attention_vec_size, True)\n            y = tf.reshape(y, [-1, 1, 1, attention_vec_size])\n            # Attention mask is a softmax of v^T * tanh(...).\n            s = tf.reduce_sum(\n                v[i] * tf.tanh(hidden_features[i] + y), [2, 3])\n            a = tf.nn.softmax(s)\n            attn_weights.append(a)\n            # Now calculate the attention-weighted vector d.\n            d = tf.reduce_sum(\n                tf.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n                [1, 2])\n            ds.append(tf.reshape(d, [-1, attn_size]))\n        return attn_weights, ds\n  \n      batch_attn_size = tf.stack([batch_size, attn_size])\n      attns = [tf.zeros(batch_attn_size, dtype=dtype)\n               for _ in xrange(num_heads)]\n      for a in attns:  # Ensure the second shape of attention vectors is set.\n        a.set_shape([None, attn_size])\n  \n      # loop through the encoder_outputs\n      attention_encoder_outputs = list()\n      sequence_attention_weights = list()\n      for i in xrange(len(encoder_outputs)):\n        if i > 0:\n          tf.get_variable_scope().reuse_variables()\n        if i == 0:\n          with tf.variable_scope(""Initial_Decoder_Attention""):\n            initial_state = linear(encoder_state, output_size, True)\n          attn_weights, ds = attention(initial_state)\n        else:\n          attn_weights, ds = attention(encoder_outputs[i])\n        output = tf.concat([ds[0], encoder_outputs[i]], 1) \n        # NOTE: here we temporarily assume num_head = 1\n        with tf.variable_scope(""AttnRnnOutputProjection""):\n          logit = linear(output, num_decoder_symbols, True)\n        attention_encoder_outputs.append(logit) \n        # NOTE: here we temporarily assume num_head = 1\n        sequence_attention_weights.append(attn_weights[0]) \n        # NOTE: here we temporarily assume num_head = 1\n  else:\n    print (\'Use the NON attention RNN model\')\n    with tf.variable_scope(scope or ""non-attention_RNN""):\n      attention_encoder_outputs = list()\n      sequence_attention_weights = list()\n      \n      # copy over logits once out of sequence_length\n      if encoder_outputs[0].get_shape().ndims != 1:\n        (fixed_batch_size, output_size) = encoder_outputs[0].get_shape().with_rank(2)\n      else:\n        fixed_batch_size = encoder_outputs[0].get_shape().with_rank_at_least(1)[0]\n\n      if fixed_batch_size.value: \n        batch_size = fixed_batch_size.value\n      else:\n        batch_size = tf.shape(encoder_outputs[0])[0]\n      if sequence_length is not None:\n        sequence_length = tf.to_int32(sequence_length)\n      if sequence_length is not None:  # Prepare variables\n        zero_logit = tf.zeros(\n          tf.stack([batch_size, num_decoder_symbols]), encoder_outputs[0].dtype)\n        zero_logit.set_shape(\n          tensor_shape.TensorShape([fixed_batch_size.value, \n                                      num_decoder_symbols]))\n        min_sequence_length = tf.reduce_min(sequence_length)\n        max_sequence_length = tf.reduce_max(sequence_length)\n    \n      #reuse = False\n      for time, input_ in enumerate(encoder_outputs):\n        if time > 0: \n          tf.get_variable_scope().reuse_variables()\n          #reuse = True\n        # pylint: disable=cell-var-from-loop\n        # call_cell = lambda: cell(input_, state)\n        generate_logit = lambda: linear(encoder_outputs[time], \n                                        num_decoder_symbols, \n                                        True)\n        # pylint: enable=cell-var-from-loop\n        if sequence_length is not None:\n          logit = _step(time, sequence_length, min_sequence_length, \n                        max_sequence_length, zero_logit, generate_logit)\n        else:\n          logit = generate_logit\n        attention_encoder_outputs.append(logit)   \n        \n  return attention_encoder_outputs, sequence_attention_weights\n\n  \ndef generate_sequence_output(num_encoder_symbols,\n                             encoder_outputs, \n                             encoder_state, \n                             targets,\n                             sequence_length, \n                             num_decoder_symbols, \n                             weights,\n                             buckets, \n                             softmax_loss_function=None,\n                             per_example_loss=False, \n                             name=None, \n                             use_attention=False):\n  if len(targets) < buckets[-1][1]:\n    raise ValueError(""Length of targets (%d) must be at least that of last""\n                     ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n\n  all_inputs = encoder_outputs + targets + weights\n  with tf.name_scope(name, ""model_with_buckets"", all_inputs):\n    with tf.variable_scope(""decoder_sequence_output"", reuse=None):\n      logits, attention_weights = attention_RNN(encoder_outputs, \n                                                encoder_state,\n                                                num_decoder_symbols,\n                                                sequence_length,\n                                                use_attention=use_attention)\n      if per_example_loss is None:\n        assert len(logits) == len(targets)\n        # We need to make target and int64-tensor and set its shape.\n        bucket_target = [tf.reshape(tf.to_int64(x), [-1]) for x in targets]\n        crossent = sequence_loss_by_example(\n              logits, bucket_target, weights,\n              softmax_loss_function=softmax_loss_function)\n      else:\n        assert len(logits) == len(targets)\n        bucket_target = [tf.reshape(tf.to_int64(x), [-1]) for x in targets]\n        crossent = sequence_loss(\n              logits, bucket_target, weights,\n              softmax_loss_function=softmax_loss_function)\n\n  return logits, crossent\n'"
