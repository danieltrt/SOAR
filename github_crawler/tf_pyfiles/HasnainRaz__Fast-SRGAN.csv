file_path,api_count,code
dataloader.py,15,"b'import tensorflow as tf\nimport os\n\nfrom tensorflow.python.ops import array_ops, math_ops\n\n\nclass DataLoader(object):\n    """"""Data Loader for the SR GAN, that prepares a tf data object for training.""""""\n\n    def __init__(self, image_dir, hr_image_size):\n        """"""\n        Initializes the dataloader.\n        Args:\n            image_dir: The path to the directory containing high resolution images.\n            hr_image_size: Integer, the crop size of the images to train on (High\n                           resolution images will be cropped to this width and height).\n        Returns:\n            The dataloader object.\n        """"""\n        self.image_paths = [os.path.join(image_dir, x) for x in os.listdir(image_dir)]\n        self.image_size = hr_image_size\n\n    def _parse_image(self, image_path):\n        """"""\n        Function that loads the images given the path.\n        Args:\n            image_path: Path to an image file.\n        Returns:\n            image: A tf tensor of the loaded image.\n        """"""\n\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.convert_image_dtype(image, tf.float32)\n\n        # Check if image is large enough\n        if tf.keras.backend.image_data_format() == \'channels_last\':\n            shape = array_ops.shape(image)[:2]\n        else:\n            shape = array_ops.shape(image)[1:]\n        cond = math_ops.reduce_all(shape >= tf.constant(self.image_size))\n\n        image = tf.cond(cond, lambda: tf.identity(image),\n                        lambda: tf.image.resize(image, [self.image_size, self.image_size]))\n\n        return image\n\n    def _random_crop(self, image):\n        """"""\n        Function that crops the image according a defined width\n        and height.\n        Args:\n            image: A tf tensor of an image.\n        Returns:\n            image: A tf tensor of containing the cropped image.\n        """"""\n\n        image = tf.image.random_crop(image, [self.image_size, self.image_size, 3])\n\n        return image\n\n    def _high_low_res_pairs(self, high_res):\n        """"""\n        Function that generates a low resolution image given the \n        high resolution image. The downsampling factor is 4x.\n        Args:\n            high_res: A tf tensor of the high res image.\n        Returns:\n            low_res: A tf tensor of the low res image.\n            high_res: A tf tensor of the high res image.\n        """"""\n\n        low_res = tf.image.resize(high_res, \n                                  [self.image_size // 4, self.image_size // 4], \n                                  method=\'bicubic\')\n\n        return low_res, high_res\n\n    def _rescale(self, low_res, high_res):\n        """"""\n        Function that rescales the pixel values to the -1 to 1 range.\n        For use with the generator output tanh function.\n        Args:\n            low_res: The tf tensor of the low res image.\n            high_res: The tf tensor of the high res image.\n        Returns:\n            low_res: The tf tensor of the low res image, rescaled.\n            high_res: the tf tensor of the high res image, rescaled.\n        """"""\n        high_res = high_res * 2.0 - 1.0\n\n        return low_res, high_res\n\n    def dataset(self, batch_size, threads=4):\n        """"""\n        Returns a tf dataset object with specified mappings.\n        Args:\n            batch_size: Int, The number of elements in a batch returned by the dataset.\n            threads: Int, CPU threads to use for multi-threaded operation.\n        Returns:\n            dataset: A tf dataset object.\n        """"""\n\n        # Generate tf dataset from high res image paths.\n        dataset = tf.data.Dataset.from_tensor_slices(self.image_paths)\n\n        # Read the images\n        dataset = dataset.map(self._parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        # Crop out a piece for training\n        dataset = dataset.map(self._random_crop, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        # Generate low resolution by downsampling crop.\n        dataset = dataset.map(self._high_low_res_pairs, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        # Rescale the values in the input\n        dataset = dataset.map(self._rescale, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        # Batch the input, drop remainder to get a defined batch size.\n        # Prefetch the data for optimal GPU utilization.\n        dataset = dataset.shuffle(30).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n\n        return dataset\n'"
infer.py,0,"b""from argparse import ArgumentParser\nfrom tensorflow import keras\nimport numpy as np\nimport cv2\nimport os\n\nparser = ArgumentParser()\nparser.add_argument('--image_dir', type=str, help='Directory where images are kept.')\nparser.add_argument('--output_dir', type=str, help='Directory where to output high res images.')\n\n\ndef main():\n    args = parser.parse_args()\n\n    # Get all image paths\n    image_paths = [os.path.join(args.image_dir, x) for x in os.listdir(args.image_dir)]\n\n    # Change model input shape to accept all size inputs\n    model = keras.models.load_model('models/generator.h5')\n    inputs = keras.Input((None, None, 3))\n    output = model(inputs)\n    model = keras.models.Model(inputs, output)\n\n    # Loop over all images\n    for image_path in image_paths:\n        \n        # Read image\n        low_res = cv2.imread(image_path, 1)\n\n        # Convert to RGB (opencv uses BGR as default)\n        low_res = cv2.cvtColor(low_res, cv2.COLOR_BGR2RGB)\n\n        # Rescale to 0-1.\n        low_res = low_res / 255.0\n\n        # Get super resolution image\n        sr = model.predict(np.expand_dims(low_res, axis=0))[0]\n\n        # Rescale values in range 0-255\n        sr = ((sr + 1) / 2.) * 255\n\n        # Convert back to BGR for opencv\n        sr = cv2.cvtColor(sr, cv2.COLOR_RGB2BGR)\n\n        # Save the results:\n        cv2.imwrite(os.path.join(args.output_dir, os.path.basename(image_path)), sr)\n\n\nif __name__ == '__main__':\n    main()\n"""
main.py,22,"b'from argparse import ArgumentParser\nfrom dataloader import DataLoader\nfrom model import FastSRGAN\nimport tensorflow as tf\nimport os\n\nparser = ArgumentParser()\nparser.add_argument(\'--image_dir\', type=str, help=\'Path to high resolution image directory.\')\nparser.add_argument(\'--batch_size\', default=8, type=int, help=\'Batch size for training.\')\nparser.add_argument(\'--epochs\', default=1, type=int, help=\'Number of epochs for training\')\nparser.add_argument(\'--hr_size\', default=384, type=int, help=\'Low resolution input size.\')\nparser.add_argument(\'--lr\', default=1e-4, type=float, help=\'Learning rate for optimizers.\')\nparser.add_argument(\'--save_iter\', default=200, type=int,\n                    help=\'The number of iterations to save the tensorboard summaries and models.\')\n\n\n@tf.function\ndef pretrain_step(model, x, y):\n    """"""\n    Single step of generator pre-training.\n    Args:\n        model: A model object with a tf keras compiled generator.\n        x: The low resolution image tensor.\n        y: The high resolution image tensor.\n    """"""\n    with tf.GradientTape() as tape:\n        fake_hr = model.generator(x)\n        loss_mse = tf.keras.losses.MeanSquaredError()(y, fake_hr)\n\n    grads = tape.gradient(loss_mse, model.generator.trainable_variables)\n    model.gen_optimizer.apply_gradients(zip(grads, model.generator.trainable_variables))\n\n    return loss_mse\n\n\ndef pretrain_generator(model, dataset, writer):\n    """"""Function that pretrains the generator slightly, to avoid local minima.\n    Args:\n        model: The keras model to train.\n        dataset: A tf dataset object of low and high res images to pretrain over.\n        writer: A summary writer object.\n    Returns:\n        None\n    """"""\n    with writer.as_default():\n        iteration = 0\n        for _ in range(1):\n            for x, y in dataset:\n                loss = pretrain_step(model, x, y)\n                if iteration % 20 == 0:\n                    tf.summary.scalar(\'MSE Loss\', loss, step=tf.cast(iteration, tf.int64))\n                    writer.flush()\n                iteration += 1\n\n\n@tf.function\ndef train_step(model, x, y):\n    """"""Single train step function for the SRGAN.\n    Args:\n        model: An object that contains a tf keras compiled discriminator model.\n        x: The low resolution input image.\n        y: The desired high resolution output image.\n\n    Returns:\n        d_loss: The mean loss of the discriminator.\n    """"""\n    # Label smoothing for better gradient flow\n    valid = tf.ones((x.shape[0],) + model.disc_patch)\n    fake = tf.zeros((x.shape[0],) + model.disc_patch)\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # From low res. image generate high res. version\n        fake_hr = model.generator(x)\n\n        # Train the discriminators (original images = real / generated = Fake)\n        valid_prediction = model.discriminator(y)\n        fake_prediction = model.discriminator(fake_hr)\n\n        # Generator loss\n        content_loss = model.content_loss(y, fake_hr)\n        adv_loss = 1e-3 * tf.keras.losses.BinaryCrossentropy()(valid, fake_prediction)\n        mse_loss = tf.keras.losses.MeanSquaredError()(y, fake_hr)\n        perceptual_loss = content_loss + adv_loss + mse_loss\n\n        # Discriminator loss\n        valid_loss = tf.keras.losses.BinaryCrossentropy()(valid, valid_prediction)\n        fake_loss = tf.keras.losses.BinaryCrossentropy()(fake, fake_prediction)\n        d_loss = tf.add(valid_loss, fake_loss)\n\n    # Backprop on Generator\n    gen_grads = gen_tape.gradient(perceptual_loss, model.generator.trainable_variables)\n    model.gen_optimizer.apply_gradients(zip(gen_grads, model.generator.trainable_variables))\n\n    # Backprop on Discriminator\n    disc_grads = disc_tape.gradient(d_loss, model.discriminator.trainable_variables)\n    model.disc_optimizer.apply_gradients(zip(disc_grads, model.discriminator.trainable_variables))\n\n    return d_loss, adv_loss, content_loss, mse_loss\n\n\ndef train(model, dataset, log_iter, writer):\n    """"""\n    Function that defines a single training step for the SR-GAN.\n    Args:\n        model: An object that contains tf keras compiled generator and\n               discriminator models.\n        dataset: A tf data object that contains low and high res images.\n        log_iter: Number of iterations after which to add logs in \n                  tensorboard.\n        writer: Summary writer\n    """"""\n    with writer.as_default():\n        # Iterate over dataset\n        for x, y in dataset:\n            disc_loss, adv_loss, content_loss, mse_loss = train_step(model, x, y)\n            # Log tensorboard summaries if log iteration is reached.\n            if model.iterations % log_iter == 0:\n                tf.summary.scalar(\'Adversarial Loss\', adv_loss, step=model.iterations)\n                tf.summary.scalar(\'Content Loss\', content_loss, step=model.iterations)\n                tf.summary.scalar(\'MSE Loss\', mse_loss, step=model.iterations)\n                tf.summary.scalar(\'Discriminator Loss\', disc_loss, step=model.iterations)\n                tf.summary.image(\'Low Res\', tf.cast(255 * x, tf.uint8), step=model.iterations)\n                tf.summary.image(\'High Res\', tf.cast(255 * (y + 1.0) / 2.0, tf.uint8), step=model.iterations)\n                tf.summary.image(\'Generated\', tf.cast(255 * (model.generator.predict(x) + 1.0) / 2.0, tf.uint8),\n                                 step=model.iterations)\n                model.generator.save(\'models/generator.h5\')\n                model.discriminator.save(\'models/discriminator.h5\')\n                writer.flush()\n            model.iterations += 1\n\n\ndef main():\n    # Parse the CLI arguments.\n    args = parser.parse_args()\n\n    # create directory for saving trained models.\n    if not os.path.exists(\'models\'):\n        os.makedirs(\'models\')\n\n    # Create the tensorflow dataset.\n    ds = DataLoader(args.image_dir, args.hr_size).dataset(args.batch_size)\n\n    # Initialize the GAN object.\n    gan = FastSRGAN(args)\n\n    # Define the directory for saving pretrainig loss tensorboard summary.\n    pretrain_summary_writer = tf.summary.create_file_writer(\'logs/pretrain\')\n\n    # Run pre-training.\n    pretrain_generator(gan, ds, pretrain_summary_writer)\n\n    # Define the directory for saving the SRGAN training tensorbaord summary.\n    train_summary_writer = tf.summary.create_file_writer(\'logs/train\')\n\n    # Run training.\n    for _ in range(args.epochs):\n        train(gan, ds, args.save_iter, train_summary_writer)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
model.py,2,"b'from tensorflow import keras\nimport tensorflow as tf\n\n\nclass FastSRGAN(object):\n    """"""SRGAN for fast super resolution.""""""\n\n    def __init__(self, args):\n        """"""\n        Initializes the Mobile SRGAN class.\n        Args:\n            args: CLI arguments that dictate how to build the model.\n        Returns:\n            None\n        """"""\n        self.hr_height = args.hr_size\n        self.hr_width = args.hr_size\n        self.lr_height = self.hr_height // 4  # Low resolution height\n        self.lr_width = self.hr_width // 4  # Low resolution width\n        self.lr_shape = (self.lr_height, self.lr_width, 3)\n        self.hr_shape = (self.hr_height, self.hr_width, 3)\n        self.iterations = 0\n\n        # Number of inverted residual blocks in the mobilenet generator\n        self.n_residual_blocks = 6\n\n        # Define a learning rate decay schedule.\n        self.gen_schedule = keras.optimizers.schedules.ExponentialDecay(\n            args.lr,\n            decay_steps=100000,\n            decay_rate=0.1,\n            staircase=True\n        )\n\n        self.disc_schedule = keras.optimizers.schedules.ExponentialDecay(\n            args.lr * 5,  # TTUR - Two Time Scale Updates\n            decay_steps=100000,\n            decay_rate=0.1,\n            staircase=True\n        )\n\n        self.gen_optimizer = keras.optimizers.Adam(learning_rate=self.gen_schedule)\n        self.disc_optimizer = keras.optimizers.Adam(learning_rate=self.disc_schedule)\n\n        # We use a pre-trained VGG19 model to extract image features from the high resolution\n        # and the generated high resolution images and minimize the mse between them\n        self.vgg = self.build_vgg()\n        self.vgg.trainable = False\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.hr_height / 2 ** 4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 32  # Realtime Image Enhancement GAN Galteri et al.\n        self.df = 32\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build and compile the generator for pretraining.\n        self.generator = self.build_generator()\n\n    @tf.function\n    def content_loss(self, hr, sr):\n        sr = keras.applications.vgg19.preprocess_input(((sr + 1.0) * 255) / 2.0)\n        hr = keras.applications.vgg19.preprocess_input(((hr + 1.0) * 255) / 2.0)\n        sr_features = self.vgg(sr) / 12.75\n        hr_features = self.vgg(hr) / 12.75\n        return tf.keras.losses.MeanSquaredError()(hr_features, sr_features)\n\n    def build_vgg(self):\n        """"""\n        Builds a pre-trained VGG19 model that outputs image features extracted at the\n        third block of the model\n        """"""\n        # Get the vgg network. Extract features from Block 5, last convolution.\n        vgg = keras.applications.VGG19(weights=""imagenet"", input_shape=self.hr_shape, include_top=False)\n        vgg.trainable = False\n        for layer in vgg.layers:\n            layer.trainable = False\n\n        # Create model and compile\n        model = keras.models.Model(inputs=vgg.input, outputs=vgg.get_layer(""block5_conv4"").output)\n\n        return model\n\n    def build_generator(self):\n        """"""Build the generator that will do the Super Resolution task.\n        Based on the Mobilenet design. Idea from Galteri et al.""""""\n\n        def _make_divisible(v, divisor, min_value=None):\n                if min_value is None:\n                    min_value = divisor\n                new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n                # Make sure that round down does not go down by more than 10%.\n                if new_v < 0.9 * v:\n                    new_v += divisor\n                return new_v\n\n        def residual_block(inputs, filters, block_id, expansion=6, stride=1, alpha=1.0):\n            """"""Inverted Residual block that uses depth wise convolutions for parameter efficiency.\n            Args:\n                inputs: The input feature map.\n                filters: Number of filters in each convolution in the block.\n                block_id: An integer specifier for the id of the block in the graph.\n                expansion: Channel expansion factor.\n                stride: The stride of the convolution.\n                alpha: Depth expansion factor.\n            Returns:\n                x: The output of the inverted residual block.\n            """"""\n            channel_axis = 1 if keras.backend.image_data_format() == \'channels_first\' else -1\n\n            in_channels = keras.backend.int_shape(inputs)[channel_axis]\n            pointwise_conv_filters = int(filters * alpha)\n            pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n            x = inputs\n            prefix = \'block_{}_\'.format(block_id)\n\n            if block_id:\n                # Expand\n                x = keras.layers.Conv2D(expansion * in_channels,\n                                        kernel_size=1,\n                                        padding=\'same\',\n                                        use_bias=True,\n                                        activation=None,\n                                        name=prefix + \'expand\')(x)\n                x = keras.layers.BatchNormalization(axis=channel_axis,\n                                                    epsilon=1e-3,\n                                                    momentum=0.999,\n                                                    name=prefix + \'expand_BN\')(x)\n                x = keras.layers.Activation(\'relu\', name=prefix + \'expand_relu\')(x)\n            else:\n                prefix = \'expanded_conv_\'\n\n            # Depthwise\n            x = keras.layers.DepthwiseConv2D(kernel_size=3,\n                                             strides=stride,\n                                             activation=None,\n                                             use_bias=True,\n                                             padding=\'same\' if stride == 1 else \'valid\',\n                                             name=prefix + \'depthwise\')(x)\n            x = keras.layers.BatchNormalization(axis=channel_axis,\n                                                epsilon=1e-3,\n                                                momentum=0.999,\n                                                name=prefix + \'depthwise_BN\')(x)\n\n            x = keras.layers.Activation(\'relu\', name=prefix + \'depthwise_relu\')(x)\n\n            # Project\n            x = keras.layers.Conv2D(pointwise_filters,\n                                    kernel_size=1,\n                                    padding=\'same\',\n                                    use_bias=True,\n                                    activation=None,\n                                    name=prefix + \'project\')(x)\n            x = keras.layers.BatchNormalization(axis=channel_axis,\n                                                epsilon=1e-3,\n                                                momentum=0.999,\n                                                name=prefix + \'project_BN\')(x)\n\n            if in_channels == pointwise_filters and stride == 1:\n                return keras.layers.Add(name=prefix + \'add\')([inputs, x])\n            return x\n\n        def deconv2d(layer_input):\n            """"""Upsampling layer to increase height and width of the input.\n            Uses PixelShuffle for upsampling.\n            Args:\n                layer_input: The input tensor to upsample.\n            Returns:\n                u: Upsampled input by a factor of 2.\n            """"""\n            u = keras.layers.UpSampling2D(size=2, interpolation=\'bilinear\')(layer_input)\n            u = keras.layers.Conv2D(self.gf, kernel_size=3, strides=1, padding=\'same\')(u)\n            u = keras.layers.PReLU(shared_axes=[1, 2])(u)\n            return u\n\n        # Low resolution image input\n        img_lr = keras.Input(shape=self.lr_shape)\n\n        # Pre-residual block\n        c1 = keras.layers.Conv2D(self.gf, kernel_size=3, strides=1, padding=\'same\')(img_lr)\n        c1 = keras.layers.BatchNormalization()(c1)\n        c1 = keras.layers.PReLU(shared_axes=[1, 2])(c1)\n\n        # Propogate through residual blocks\n        r = residual_block(c1, self.gf, 0)\n        for idx in range(1, self.n_residual_blocks):\n            r = residual_block(r, self.gf, idx)\n\n        # Post-residual block\n        c2 = keras.layers.Conv2D(self.gf, kernel_size=3, strides=1, padding=\'same\')(r)\n        c2 = keras.layers.BatchNormalization()(c2)\n        c2 = keras.layers.Add()([c2, c1])\n        \n        # Upsampling\n        u1 = deconv2d(c2)\n        u2 = deconv2d(u1)\n\n        # Generate high resolution output\n        gen_hr = keras.layers.Conv2D(3, kernel_size=3, strides=1, padding=\'same\', activation=\'tanh\')(u2)\n\n        return keras.models.Model(img_lr, gen_hr)\n\n    def build_discriminator(self):\n        """"""Builds a discriminator network based on the SRGAN design.""""""\n\n        def d_block(layer_input, filters, strides=1, bn=True):\n            """"""Discriminator layer block.\n            Args:\n                layer_input: Input feature map for the convolutional block.\n                filters: Number of filters in the convolution.\n                strides: The stride of the convolution.\n                bn: Whether to use batch norm or not.\n            """"""\n            d = keras.layers.Conv2D(filters, kernel_size=3, strides=strides, padding=\'same\')(layer_input)\n            if bn:\n                d = keras.layers.BatchNormalization(momentum=0.8)(d)\n            d = keras.layers.LeakyReLU(alpha=0.2)(d)\n                \n            return d\n\n        # Input img\n        d0 = keras.layers.Input(shape=self.hr_shape)\n\n        d1 = d_block(d0, self.df, bn=False)\n        d2 = d_block(d1, self.df, strides=2)\n        d3 = d_block(d2, self.df)\n        d4 = d_block(d3, self.df, strides=2)\n        d5 = d_block(d4, self.df * 2)\n        d6 = d_block(d5, self.df * 2, strides=2)\n        d7 = d_block(d6, self.df * 2)\n        d8 = d_block(d7, self.df * 2, strides=2)\n\n        validity = keras.layers.Conv2D(1, kernel_size=1, strides=1, activation=\'sigmoid\', padding=\'same\')(d8)\n\n        return keras.models.Model(d0, validity)\n'"
