file_path,api_count,code
eval_ssd.py,128,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\nimport numpy as np\n\nfrom net import ssd_net\n\nfrom dataset import dataset_common\nfrom preprocessing import ssd_preprocessing\nfrom utility import anchor_manipulator\nfrom utility import scaffolds\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 8,\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 24,\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'./dataset/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_integer(\n    \'num_classes\', 21, \'Number of classes to use in the dataset.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs/\',\n    \'The directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are printed.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 500,\n    \'The frequency with which summaries are saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 300,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 1,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 1,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_last\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\ntf.app.flags.DEFINE_float(\n    \'negative_ratio\', 3., \'Negative ratio in the loss function.\')\ntf.app.flags.DEFINE_float(\n    \'match_threshold\', 0.5, \'Matching threshold in the loss function.\')\ntf.app.flags.DEFINE_float(\n    \'neg_threshold\', 0.5, \'Matching threshold for the negtive examples in the loss function.\')\ntf.app.flags.DEFINE_float(\n    \'select_threshold\', 0.01, \'Class-specific confidence score threshold for selecting a box.\')\ntf.app.flags.DEFINE_float(\n    \'min_size\', 0.03, \'The min size of bboxes to keep.\')\ntf.app.flags.DEFINE_float(\n    \'nms_threshold\', 0.45, \'Matching threshold in NMS algorithm.\')\ntf.app.flags.DEFINE_integer(\n    \'nms_topk\', 200, \'Number of total object to keep after NMS.\')\ntf.app.flags.DEFINE_integer(\n    \'keep_topk\', 400, \'Number of total object to keep for each image before nms.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 5e-4, \'The weight decay on the model weights.\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'model_scope\', \'ssd300\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\n\nFLAGS = tf.app.flags.FLAGS\n#CUDA_VISIBLE_DEVICES\n\ndef get_checkpoint():\n    if tf.train.latest_checkpoint(FLAGS.model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s\' % FLAGS.model_dir)\n        return None\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n        checkpoint_path = FLAGS.checkpoint_path\n\n    return checkpoint_path\n\n# couldn\'t find better way to pass params from input_fn to model_fn\n# some tensors used by model_fn must be created in input_fn to ensure they are in the same graph\n# but when we put these tensors to labels\'s dict, the replicate_model_fn will split them into each GPU\n# the problem is that they shouldn\'t be splited\nglobal_anchor_info = dict()\n\ndef input_pipeline(dataset_pattern=\'train-*\', is_training=True, batch_size=FLAGS.batch_size):\n    def input_fn():\n        out_shape = [FLAGS.train_image_size] * 2\n        anchor_creator = anchor_manipulator.AnchorCreator(out_shape,\n                                                    layers_shapes = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n                                                    anchor_scales = [(0.1,), (0.2,), (0.375,), (0.55,), (0.725,), (0.9,)],\n                                                    extra_anchor_scales = [(0.1414,), (0.2739,), (0.4541,), (0.6315,), (0.8078,), (0.9836,)],\n                                                    anchor_ratios = [(1., 2., .5), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., .5), (1., 2., .5)],\n                                                    #anchor_ratios = [(2., .5), (2., 3., .5, 0.3333), (2., 3., .5, 0.3333), (2., 3., .5, 0.3333), (2., .5), (2., .5)],\n                                                    layer_steps = [8, 16, 32, 64, 100, 300])\n        all_anchors, all_num_anchors_depth, all_num_anchors_spatial = anchor_creator.get_all_anchors()\n\n        num_anchors_per_layer = []\n        for ind in range(len(all_anchors)):\n            num_anchors_per_layer.append(all_num_anchors_depth[ind] * all_num_anchors_spatial[ind])\n\n        anchor_encoder_decoder = anchor_manipulator.AnchorEncoder(allowed_borders = [1.0] * 6,\n                                                            positive_threshold = FLAGS.match_threshold,\n                                                            ignore_threshold = FLAGS.neg_threshold,\n                                                            prior_scaling=[0.1, 0.1, 0.2, 0.2])\n\n        image_preprocessing_fn = lambda image_, labels_, bboxes_ : ssd_preprocessing.preprocess_image(image_, labels_, bboxes_, out_shape, is_training=is_training, data_format=FLAGS.data_format, output_rgb=False)\n        anchor_encoder_fn = lambda glabels_, gbboxes_: anchor_encoder_decoder.encode_all_anchors(glabels_, gbboxes_, all_anchors, all_num_anchors_depth, all_num_anchors_spatial)\n\n        image, filename, shape, loc_targets, cls_targets, match_scores = dataset_common.slim_get_batch(FLAGS.num_classes,\n                                                                                batch_size,\n                                                                                (\'train\' if is_training else \'val\'),\n                                                                                os.path.join(FLAGS.data_dir, dataset_pattern),\n                                                                                FLAGS.num_readers,\n                                                                                FLAGS.num_preprocessing_threads,\n                                                                                image_preprocessing_fn,\n                                                                                anchor_encoder_fn,\n                                                                                num_epochs=FLAGS.train_epochs,\n                                                                                is_training=is_training)\n        global global_anchor_info\n        global_anchor_info = {\'decode_fn\': lambda pred : anchor_encoder_decoder.decode_all_anchors(pred, num_anchors_per_layer),\n                            \'num_anchors_per_layer\': num_anchors_per_layer,\n                            \'all_num_anchors_depth\': all_num_anchors_depth }\n\n        return {\'image\': image, \'filename\': filename, \'shape\': shape, \'loc_targets\': loc_targets, \'cls_targets\': cls_targets, \'match_scores\': match_scores}, None\n    return input_fn\n\ndef modified_smooth_l1(bbox_pred, bbox_targets, bbox_inside_weights=1., bbox_outside_weights=1., sigma=1.):\n    """"""\n        ResultLoss = outside_weights * SmoothL1(inside_weights * (bbox_pred - bbox_targets))\n        SmoothL1(x) = 0.5 * (sigma * x)^2,    if |x| < 1 / sigma^2\n                      |x| - 0.5 / sigma^2,    otherwise\n    """"""\n    with tf.name_scope(\'smooth_l1\', [bbox_pred, bbox_targets]):\n        sigma2 = sigma * sigma\n\n        inside_mul = tf.multiply(bbox_inside_weights, tf.subtract(bbox_pred, bbox_targets))\n\n        smooth_l1_sign = tf.cast(tf.less(tf.abs(inside_mul), 1.0 / sigma2), tf.float32)\n        smooth_l1_option1 = tf.multiply(tf.multiply(inside_mul, inside_mul), 0.5 * sigma2)\n        smooth_l1_option2 = tf.subtract(tf.abs(inside_mul), 0.5 / sigma2)\n        smooth_l1_result = tf.add(tf.multiply(smooth_l1_option1, smooth_l1_sign),\n                                  tf.multiply(smooth_l1_option2, tf.abs(tf.subtract(smooth_l1_sign, 1.0))))\n\n        outside_mul = tf.multiply(bbox_outside_weights, smooth_l1_result)\n\n        return outside_mul\n\ndef select_bboxes(scores_pred, bboxes_pred, num_classes, select_threshold):\n    selected_bboxes = {}\n    selected_scores = {}\n    with tf.name_scope(\'select_bboxes\', [scores_pred, bboxes_pred]):\n        for class_ind in range(1, num_classes):\n            class_scores = scores_pred[:, class_ind]\n            select_mask = class_scores > select_threshold\n\n            select_mask = tf.cast(select_mask, tf.float32)\n            selected_bboxes[class_ind] = tf.multiply(bboxes_pred, tf.expand_dims(select_mask, axis=-1))\n            selected_scores[class_ind] = tf.multiply(class_scores, select_mask)\n\n    return selected_bboxes, selected_scores\n\ndef clip_bboxes(ymin, xmin, ymax, xmax, name):\n    with tf.name_scope(name, \'clip_bboxes\', [ymin, xmin, ymax, xmax]):\n        ymin = tf.maximum(ymin, 0.)\n        xmin = tf.maximum(xmin, 0.)\n        ymax = tf.minimum(ymax, 1.)\n        xmax = tf.minimum(xmax, 1.)\n\n        ymin = tf.minimum(ymin, ymax)\n        xmin = tf.minimum(xmin, xmax)\n\n        return ymin, xmin, ymax, xmax\n\ndef filter_bboxes(scores_pred, ymin, xmin, ymax, xmax, min_size, name):\n    with tf.name_scope(name, \'filter_bboxes\', [scores_pred, ymin, xmin, ymax, xmax]):\n        width = xmax - xmin\n        height = ymax - ymin\n\n        filter_mask = tf.logical_and(width > min_size, height > min_size)\n\n        filter_mask = tf.cast(filter_mask, tf.float32)\n        return tf.multiply(ymin, filter_mask), tf.multiply(xmin, filter_mask), \\\n                tf.multiply(ymax, filter_mask), tf.multiply(xmax, filter_mask), tf.multiply(scores_pred, filter_mask)\n\ndef sort_bboxes(scores_pred, ymin, xmin, ymax, xmax, keep_topk, name):\n    with tf.name_scope(name, \'sort_bboxes\', [scores_pred, ymin, xmin, ymax, xmax]):\n        cur_bboxes = tf.shape(scores_pred)[0]\n        scores, idxes = tf.nn.top_k(scores_pred, k=tf.minimum(keep_topk, cur_bboxes), sorted=True)\n\n        ymin, xmin, ymax, xmax = tf.gather(ymin, idxes), tf.gather(xmin, idxes), tf.gather(ymax, idxes), tf.gather(xmax, idxes)\n\n        paddings_scores = tf.expand_dims(tf.stack([0, tf.maximum(keep_topk-cur_bboxes, 0)], axis=0), axis=0)\n\n        return tf.pad(ymin, paddings_scores, ""CONSTANT""), tf.pad(xmin, paddings_scores, ""CONSTANT""),\\\n                tf.pad(ymax, paddings_scores, ""CONSTANT""), tf.pad(xmax, paddings_scores, ""CONSTANT""),\\\n                tf.pad(scores, paddings_scores, ""CONSTANT"")\n\ndef nms_bboxes(scores_pred, bboxes_pred, nms_topk, nms_threshold, name):\n    with tf.name_scope(name, \'nms_bboxes\', [scores_pred, bboxes_pred]):\n        idxes = tf.image.non_max_suppression(bboxes_pred, scores_pred, nms_topk, nms_threshold)\n        return tf.gather(scores_pred, idxes), tf.gather(bboxes_pred, idxes)\n\ndef parse_by_class(cls_pred, bboxes_pred, num_classes, select_threshold, min_size, keep_topk, nms_topk, nms_threshold):\n    with tf.name_scope(\'select_bboxes\', [cls_pred, bboxes_pred]):\n        scores_pred = tf.nn.softmax(cls_pred)\n        selected_bboxes, selected_scores = select_bboxes(scores_pred, bboxes_pred, num_classes, select_threshold)\n        for class_ind in range(1, num_classes):\n            ymin, xmin, ymax, xmax = tf.unstack(selected_bboxes[class_ind], 4, axis=-1)\n            #ymin, xmin, ymax, xmax = tf.split(selected_bboxes[class_ind], 4, axis=-1)\n            #ymin, xmin, ymax, xmax = tf.squeeze(ymin), tf.squeeze(xmin), tf.squeeze(ymax), tf.squeeze(xmax)\n            ymin, xmin, ymax, xmax = clip_bboxes(ymin, xmin, ymax, xmax, \'clip_bboxes_{}\'.format(class_ind))\n            ymin, xmin, ymax, xmax, selected_scores[class_ind] = filter_bboxes(selected_scores[class_ind],\n                                                ymin, xmin, ymax, xmax, min_size, \'filter_bboxes_{}\'.format(class_ind))\n            ymin, xmin, ymax, xmax, selected_scores[class_ind] = sort_bboxes(selected_scores[class_ind],\n                                                ymin, xmin, ymax, xmax, keep_topk, \'sort_bboxes_{}\'.format(class_ind))\n            selected_bboxes[class_ind] = tf.stack([ymin, xmin, ymax, xmax], axis=-1)\n            selected_scores[class_ind], selected_bboxes[class_ind] = nms_bboxes(selected_scores[class_ind], selected_bboxes[class_ind], nms_topk, nms_threshold, \'nms_bboxes_{}\'.format(class_ind))\n\n        return selected_bboxes, selected_scores\n\ndef ssd_model_fn(features, labels, mode, params):\n    """"""model_fn for SSD to be used with our Estimator.""""""\n    filename = features[\'filename\']\n    shape = features[\'shape\']\n    loc_targets = features[\'loc_targets\']\n    cls_targets = features[\'cls_targets\']\n    match_scores = features[\'match_scores\']\n    features = features[\'image\']\n\n    global global_anchor_info\n    decode_fn = global_anchor_info[\'decode_fn\']\n    num_anchors_per_layer = global_anchor_info[\'num_anchors_per_layer\']\n    all_num_anchors_depth = global_anchor_info[\'all_num_anchors_depth\']\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        backbone = ssd_net.VGG16Backbone(params[\'data_format\'])\n        feature_layers = backbone.forward(features, training=(mode == tf.estimator.ModeKeys.TRAIN))\n        #print(feature_layers)\n        location_pred, cls_pred = ssd_net.multibox_head(feature_layers, params[\'num_classes\'], all_num_anchors_depth, data_format=params[\'data_format\'])\n        if params[\'data_format\'] == \'channels_first\':\n            cls_pred = [tf.transpose(pred, [0, 2, 3, 1]) for pred in cls_pred]\n            location_pred = [tf.transpose(pred, [0, 2, 3, 1]) for pred in location_pred]\n\n        cls_pred = [tf.reshape(pred, [tf.shape(features)[0], -1, params[\'num_classes\']]) for pred in cls_pred]\n        location_pred = [tf.reshape(pred, [tf.shape(features)[0], -1, 4]) for pred in location_pred]\n\n        cls_pred = tf.concat(cls_pred, axis=1)\n        location_pred = tf.concat(location_pred, axis=1)\n\n        cls_pred = tf.reshape(cls_pred, [-1, params[\'num_classes\']])\n        location_pred = tf.reshape(location_pred, [-1, 4])\n\n    with tf.device(\'/cpu:0\'):\n        bboxes_pred = decode_fn(location_pred)\n        bboxes_pred = tf.concat(bboxes_pred, axis=0)\n        selected_bboxes, selected_scores = parse_by_class(cls_pred, bboxes_pred,\n                                                        params[\'num_classes\'], params[\'select_threshold\'], params[\'min_size\'],\n                                                        params[\'keep_topk\'], params[\'nms_topk\'], params[\'nms_threshold\'])\n\n    predictions = {\'filename\': filename, \'shape\': shape }\n    for class_ind in range(1, params[\'num_classes\']):\n        predictions[\'scores_{}\'.format(class_ind)] = tf.expand_dims(selected_scores[class_ind], axis=0)\n        predictions[\'bboxes_{}\'.format(class_ind)] = tf.expand_dims(selected_bboxes[class_ind], axis=0)\n\n    flaten_cls_targets = tf.reshape(cls_targets, [-1])\n    flaten_match_scores = tf.reshape(match_scores, [-1])\n    flaten_loc_targets = tf.reshape(loc_targets, [-1, 4])\n\n    # each positive examples has one label\n    positive_mask = flaten_cls_targets > 0\n    n_positives = tf.count_nonzero(positive_mask)\n\n    batch_n_positives = tf.count_nonzero(cls_targets, -1)\n\n    batch_negtive_mask = tf.equal(cls_targets, 0)#tf.logical_and(tf.equal(cls_targets, 0), match_scores > 0.)\n    batch_n_negtives = tf.count_nonzero(batch_negtive_mask, -1)\n\n    batch_n_neg_select = tf.cast(params[\'negative_ratio\'] * tf.cast(batch_n_positives, tf.float32), tf.int32)\n    batch_n_neg_select = tf.minimum(batch_n_neg_select, tf.cast(batch_n_negtives, tf.int32))\n\n    # hard negative mining for classification\n    predictions_for_bg = tf.nn.softmax(tf.reshape(cls_pred, [tf.shape(features)[0], -1, params[\'num_classes\']]))[:, :, 0]\n    prob_for_negtives = tf.where(batch_negtive_mask,\n                           0. - predictions_for_bg,\n                           # ignore all the positives\n                           0. - tf.ones_like(predictions_for_bg))\n    topk_prob_for_bg, _ = tf.nn.top_k(prob_for_negtives, k=tf.shape(prob_for_negtives)[1])\n    score_at_k = tf.gather_nd(topk_prob_for_bg, tf.stack([tf.range(tf.shape(features)[0]), batch_n_neg_select - 1], axis=-1))\n\n    selected_neg_mask = prob_for_negtives >= tf.expand_dims(score_at_k, axis=-1)\n\n    # include both selected negtive and all positive examples\n    final_mask = tf.stop_gradient(tf.logical_or(tf.reshape(tf.logical_and(batch_negtive_mask, selected_neg_mask), [-1]), positive_mask))\n    total_examples = tf.count_nonzero(final_mask)\n\n    cls_pred = tf.boolean_mask(cls_pred, final_mask)\n    location_pred = tf.boolean_mask(location_pred, tf.stop_gradient(positive_mask))\n    flaten_cls_targets = tf.boolean_mask(tf.clip_by_value(flaten_cls_targets, 0, params[\'num_classes\']), final_mask)\n    flaten_loc_targets = tf.stop_gradient(tf.boolean_mask(flaten_loc_targets, positive_mask))\n\n    # Calculate loss, which includes softmax cross entropy and L2 regularization.\n    #cross_entropy = (params[\'negative_ratio\'] + 1.) * tf.cond(n_positives > 0, lambda: tf.losses.sparse_softmax_cross_entropy(labels=glabels, logits=cls_pred), lambda: 0.)\n    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=flaten_cls_targets, logits=cls_pred) * (params[\'negative_ratio\'] + 1.)\n    # Create a tensor named cross_entropy for logging purposes.\n    tf.identity(cross_entropy, name=\'cross_entropy_loss\')\n    tf.summary.scalar(\'cross_entropy_loss\', cross_entropy)\n\n    #loc_loss = tf.cond(n_positives > 0, lambda: modified_smooth_l1(location_pred, tf.stop_gradient(flaten_loc_targets), sigma=1.), lambda: tf.zeros_like(location_pred))\n    loc_loss = modified_smooth_l1(location_pred, flaten_loc_targets, sigma=1.)\n    loc_loss = tf.reduce_mean(tf.reduce_sum(loc_loss, axis=-1), name=\'location_loss\')\n    tf.summary.scalar(\'location_loss\', loc_loss)\n    tf.losses.add_loss(loc_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    total_loss = tf.add(cross_entropy, loc_loss, name=\'total_loss\')\n\n    cls_accuracy = tf.metrics.accuracy(flaten_cls_targets, tf.argmax(cls_pred, axis=-1))\n\n    # Create a tensor named train_accuracy for logging purposes.\n    tf.identity(cls_accuracy[1], name=\'cls_accuracy\')\n    tf.summary.scalar(\'cls_accuracy\', cls_accuracy[1])\n\n    summary_hook = tf.train.SummarySaverHook(save_steps=params[\'save_summary_steps\'],\n                                        output_dir=params[\'summary_dir\'],\n                                        summary_op=tf.summary.merge_all())\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n              mode=mode,\n              predictions=predictions,\n              prediction_hooks=[summary_hook],\n              loss=None, train_op=None)\n    else:\n        raise ValueError(\'This script only support ""PREDICT"" mode!\')\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, intra_op_parallelism_threads=FLAGS.num_cpu_threads, inter_op_parallelism_threads=FLAGS.num_cpu_threads, gpu_options=gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=None).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=config)\n\n    summary_dir = os.path.join(FLAGS.model_dir, \'predict\')\n\n    ssd_detector = tf.estimator.Estimator(\n        model_fn=ssd_model_fn, model_dir=FLAGS.model_dir, config=run_config,\n        params={\n            \'select_threshold\': FLAGS.select_threshold,\n            \'min_size\': FLAGS.min_size,\n            \'nms_threshold\': FLAGS.nms_threshold,\n            \'nms_topk\': FLAGS.nms_topk,\n            \'keep_topk\': FLAGS.keep_topk,\n            \'data_format\': FLAGS.data_format,\n            \'batch_size\': FLAGS.batch_size,\n            \'model_scope\': FLAGS.model_scope,\n            \'save_summary_steps\': FLAGS.save_summary_steps,\n            \'summary_dir\': summary_dir,\n            \'num_classes\': FLAGS.num_classes,\n            \'negative_ratio\': FLAGS.negative_ratio,\n            \'match_threshold\': FLAGS.match_threshold,\n            \'neg_threshold\': FLAGS.neg_threshold,\n            \'weight_decay\': FLAGS.weight_decay,\n        })\n    tensors_to_log = {\n        \'ce\': \'cross_entropy_loss\',\n        \'loc\': \'location_loss\',\n        \'loss\': \'total_loss\',\n        \'acc\': \'cls_accuracy\',\n    }\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps,\n                                            formatter=lambda dicts: (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n    print(\'Starting a predict cycle.\')\n    pred_results = ssd_detector.predict(input_fn=input_pipeline(dataset_pattern=\'val-*\', is_training=False, batch_size=FLAGS.batch_size),\n                                    hooks=[logging_hook], checkpoint_path=get_checkpoint())#, yield_single_examples=False)\n\n    det_results = list(pred_results)\n    #print(list(det_results))\n\n    #[{\'bboxes_1\': array([[0.        , 0.        , 0.28459054, 0.5679505 ], [0.3158835 , 0.34792888, 0.7312541 , 1.        ]], dtype=float32), \'scores_17\': array([0.01333667, 0.01152573], dtype=float32), \'filename\': b\'000703.jpg\', \'shape\': array([334, 500,   3])}]\n    for class_ind in range(1, FLAGS.num_classes):\n        with open(os.path.join(summary_dir, \'results_{}.txt\'.format(class_ind)), \'wt\') as f:\n            for image_ind, pred in enumerate(det_results):\n                filename = pred[\'filename\']\n                shape = pred[\'shape\']\n                scores = pred[\'scores_{}\'.format(class_ind)]\n                bboxes = pred[\'bboxes_{}\'.format(class_ind)]\n                bboxes[:, 0] = (bboxes[:, 0] * shape[0]).astype(np.int32, copy=False) + 1\n                bboxes[:, 1] = (bboxes[:, 1] * shape[1]).astype(np.int32, copy=False) + 1\n                bboxes[:, 2] = (bboxes[:, 2] * shape[0]).astype(np.int32, copy=False) + 1\n                bboxes[:, 3] = (bboxes[:, 3] * shape[1]).astype(np.int32, copy=False) + 1\n\n                valid_mask = np.logical_and((bboxes[:, 2] - bboxes[:, 0] > 0), (bboxes[:, 3] - bboxes[:, 1] > 0))\n\n                for det_ind in range(valid_mask.shape[0]):\n                    if not valid_mask[det_ind]:\n                        continue\n                    f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(filename.decode(\'utf8\')[:-4], scores[det_ind],\n                                       bboxes[det_ind, 1], bboxes[det_ind, 0],\n                                       bboxes[det_ind, 3], bboxes[det_ind, 2]))\n\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
simple_ssd_demo.py,67,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\nfrom scipy.misc import imread, imsave, imshow, imresize\nimport numpy as np\n\nfrom net import ssd_net\n\nfrom dataset import dataset_common\nfrom preprocessing import ssd_preprocessing\nfrom utility import anchor_manipulator\nfrom utility import draw_toolbox\n\n# scaffold related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_classes\', 21, \'Number of classes to use in the dataset.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 300,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_last\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\ntf.app.flags.DEFINE_float(\n    \'select_threshold\', 0.2, \'Class-specific confidence score threshold for selecting a box.\')\ntf.app.flags.DEFINE_float(\n    \'min_size\', 0.03, \'The min size of bboxes to keep.\')\ntf.app.flags.DEFINE_float(\n    \'nms_threshold\', 0.45, \'Matching threshold in NMS algorithm.\')\ntf.app.flags.DEFINE_integer(\n    \'nms_topk\', 20, \'Number of total object to keep after NMS.\')\ntf.app.flags.DEFINE_integer(\n    \'keep_topk\', 200, \'Number of total object to keep for each image before nms.\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./logs\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'model_scope\', \'ssd300\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\n\nFLAGS = tf.app.flags.FLAGS\n#CUDA_VISIBLE_DEVICES\n\ndef get_checkpoint():\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n        checkpoint_path = FLAGS.checkpoint_path\n\n    return checkpoint_path\n\ndef select_bboxes(scores_pred, bboxes_pred, num_classes, select_threshold):\n    selected_bboxes = {}\n    selected_scores = {}\n    with tf.name_scope(\'select_bboxes\', [scores_pred, bboxes_pred]):\n        for class_ind in range(1, num_classes):\n            class_scores = scores_pred[:, class_ind]\n\n            select_mask = class_scores > select_threshold\n            select_mask = tf.cast(select_mask, tf.float32)\n            selected_bboxes[class_ind] = tf.multiply(bboxes_pred, tf.expand_dims(select_mask, axis=-1))\n            selected_scores[class_ind] = tf.multiply(class_scores, select_mask)\n\n    return selected_bboxes, selected_scores\n\ndef clip_bboxes(ymin, xmin, ymax, xmax, name):\n    with tf.name_scope(name, \'clip_bboxes\', [ymin, xmin, ymax, xmax]):\n        ymin = tf.maximum(ymin, 0.)\n        xmin = tf.maximum(xmin, 0.)\n        ymax = tf.minimum(ymax, 1.)\n        xmax = tf.minimum(xmax, 1.)\n\n        ymin = tf.minimum(ymin, ymax)\n        xmin = tf.minimum(xmin, xmax)\n\n        return ymin, xmin, ymax, xmax\n\ndef filter_bboxes(scores_pred, ymin, xmin, ymax, xmax, min_size, name):\n    with tf.name_scope(name, \'filter_bboxes\', [scores_pred, ymin, xmin, ymax, xmax]):\n        width = xmax - xmin\n        height = ymax - ymin\n\n        filter_mask = tf.logical_and(width > min_size, height > min_size)\n\n        filter_mask = tf.cast(filter_mask, tf.float32)\n        return tf.multiply(ymin, filter_mask), tf.multiply(xmin, filter_mask), \\\n                tf.multiply(ymax, filter_mask), tf.multiply(xmax, filter_mask), tf.multiply(scores_pred, filter_mask)\n\ndef sort_bboxes(scores_pred, ymin, xmin, ymax, xmax, keep_topk, name):\n    with tf.name_scope(name, \'sort_bboxes\', [scores_pred, ymin, xmin, ymax, xmax]):\n        cur_bboxes = tf.shape(scores_pred)[0]\n        scores, idxes = tf.nn.top_k(scores_pred, k=tf.minimum(keep_topk, cur_bboxes), sorted=True)\n\n        ymin, xmin, ymax, xmax = tf.gather(ymin, idxes), tf.gather(xmin, idxes), tf.gather(ymax, idxes), tf.gather(xmax, idxes)\n\n        paddings_scores = tf.expand_dims(tf.stack([0, tf.maximum(keep_topk-cur_bboxes, 0)], axis=0), axis=0)\n\n        return tf.pad(ymin, paddings_scores, ""CONSTANT""), tf.pad(xmin, paddings_scores, ""CONSTANT""),\\\n                tf.pad(ymax, paddings_scores, ""CONSTANT""), tf.pad(xmax, paddings_scores, ""CONSTANT""),\\\n                tf.pad(scores, paddings_scores, ""CONSTANT"")\n\ndef nms_bboxes(scores_pred, bboxes_pred, nms_topk, nms_threshold, name):\n    with tf.name_scope(name, \'nms_bboxes\', [scores_pred, bboxes_pred]):\n        idxes = tf.image.non_max_suppression(bboxes_pred, scores_pred, nms_topk, nms_threshold)\n        return tf.gather(scores_pred, idxes), tf.gather(bboxes_pred, idxes)\n\ndef parse_by_class(cls_pred, bboxes_pred, num_classes, select_threshold, min_size, keep_topk, nms_topk, nms_threshold):\n    with tf.name_scope(\'select_bboxes\', [cls_pred, bboxes_pred]):\n        scores_pred = tf.nn.softmax(cls_pred)\n        selected_bboxes, selected_scores = select_bboxes(scores_pred, bboxes_pred, num_classes, select_threshold)\n        for class_ind in range(1, num_classes):\n            ymin, xmin, ymax, xmax = tf.unstack(selected_bboxes[class_ind], 4, axis=-1)\n            #ymin, xmin, ymax, xmax = tf.squeeze(ymin), tf.squeeze(xmin), tf.squeeze(ymax), tf.squeeze(xmax)\n            ymin, xmin, ymax, xmax = clip_bboxes(ymin, xmin, ymax, xmax, \'clip_bboxes_{}\'.format(class_ind))\n            ymin, xmin, ymax, xmax, selected_scores[class_ind] = filter_bboxes(selected_scores[class_ind],\n                                                ymin, xmin, ymax, xmax, min_size, \'filter_bboxes_{}\'.format(class_ind))\n            ymin, xmin, ymax, xmax, selected_scores[class_ind] = sort_bboxes(selected_scores[class_ind],\n                                                ymin, xmin, ymax, xmax, keep_topk, \'sort_bboxes_{}\'.format(class_ind))\n            selected_bboxes[class_ind] = tf.stack([ymin, xmin, ymax, xmax], axis=-1)\n            selected_scores[class_ind], selected_bboxes[class_ind] = nms_bboxes(selected_scores[class_ind], selected_bboxes[class_ind], nms_topk, nms_threshold, \'nms_bboxes_{}\'.format(class_ind))\n\n        return selected_bboxes, selected_scores\n\ndef main(_):\n    with tf.Graph().as_default():\n        out_shape = [FLAGS.train_image_size] * 2\n\n        image_input = tf.placeholder(tf.uint8, shape=(None, None, 3))\n        shape_input = tf.placeholder(tf.int32, shape=(2,))\n\n        features = ssd_preprocessing.preprocess_for_eval(image_input, out_shape, data_format=FLAGS.data_format, output_rgb=False)\n        features = tf.expand_dims(features, axis=0)\n\n        anchor_creator = anchor_manipulator.AnchorCreator(out_shape,\n                                                    layers_shapes = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n                                                    anchor_scales = [(0.1,), (0.2,), (0.375,), (0.55,), (0.725,), (0.9,)],\n                                                    extra_anchor_scales = [(0.1414,), (0.2739,), (0.4541,), (0.6315,), (0.8078,), (0.9836,)],\n                                                    anchor_ratios = [(1., 2., .5), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., .5), (1., 2., .5)],\n                                                    #anchor_ratios = [(2., .5), (2., 3., .5, 0.3333), (2., 3., .5, 0.3333), (2., 3., .5, 0.3333), (2., .5), (2., .5)],\n                                                    layer_steps = [8, 16, 32, 64, 100, 300])\n        all_anchors, all_num_anchors_depth, all_num_anchors_spatial = anchor_creator.get_all_anchors()\n\n        anchor_encoder_decoder = anchor_manipulator.AnchorEncoder(allowed_borders = [1.0] * 6,\n                                                            positive_threshold = None,\n                                                            ignore_threshold = None,\n                                                            prior_scaling=[0.1, 0.1, 0.2, 0.2])\n\n        decode_fn = lambda pred : anchor_encoder_decoder.ext_decode_all_anchors(pred, all_anchors, all_num_anchors_depth, all_num_anchors_spatial)\n\n        with tf.variable_scope(FLAGS.model_scope, default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n            backbone = ssd_net.VGG16Backbone(FLAGS.data_format)\n            feature_layers = backbone.forward(features, training=False)\n            location_pred, cls_pred = ssd_net.multibox_head(feature_layers, FLAGS.num_classes, all_num_anchors_depth, data_format=FLAGS.data_format)\n            if FLAGS.data_format == \'channels_first\':\n                cls_pred = [tf.transpose(pred, [0, 2, 3, 1]) for pred in cls_pred]\n                location_pred = [tf.transpose(pred, [0, 2, 3, 1]) for pred in location_pred]\n\n            cls_pred = [tf.reshape(pred, [-1, FLAGS.num_classes]) for pred in cls_pred]\n            location_pred = [tf.reshape(pred, [-1, 4]) for pred in location_pred]\n\n            cls_pred = tf.concat(cls_pred, axis=0)\n            location_pred = tf.concat(location_pred, axis=0)\n\n        with tf.device(\'/cpu:0\'):\n            bboxes_pred = decode_fn(location_pred)\n            bboxes_pred = tf.concat(bboxes_pred, axis=0)\n            selected_bboxes, selected_scores = parse_by_class(cls_pred, bboxes_pred,\n                                                            FLAGS.num_classes, FLAGS.select_threshold, FLAGS.min_size,\n                                                            FLAGS.keep_topk, FLAGS.nms_topk, FLAGS.nms_threshold)\n\n            labels_list = []\n            scores_list = []\n            bboxes_list = []\n            for k, v in selected_scores.items():\n                labels_list.append(tf.ones_like(v, tf.int32) * k)\n                scores_list.append(v)\n                bboxes_list.append(selected_bboxes[k])\n            all_labels = tf.concat(labels_list, axis=0)\n            all_scores = tf.concat(scores_list, axis=0)\n            all_bboxes = tf.concat(bboxes_list, axis=0)\n\n        saver = tf.train.Saver()\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            saver.restore(sess, get_checkpoint())\n\n            np_image = imread(\'./demo/test.jpg\')\n            labels_, scores_, bboxes_ = sess.run([all_labels, all_scores, all_bboxes], feed_dict = {image_input : np_image, shape_input : np_image.shape[:-1]})\n\n            img_to_draw = draw_toolbox.bboxes_draw_on_img(np_image, labels_, scores_, bboxes_, thickness=2)\n            imsave(\'./demo/test_out.jpg\', img_to_draw)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
train_ssd.py,145,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom net import ssd_net\n\nfrom dataset import dataset_common\nfrom preprocessing import ssd_preprocessing\nfrom utility import anchor_manipulator\nfrom utility import scaffolds\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 8,\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 24,\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'./dataset/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_integer(\n    \'num_classes\', 21, \'Number of classes to use in the dataset.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs/\',\n    \'The directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are printed.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 500,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 7200,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 300,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', None,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'max_number_of_steps\', 120000,\n    \'The max number of steps to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\ntf.app.flags.DEFINE_float(\n    \'negative_ratio\', 3., \'Negative ratio in the loss function.\')\ntf.app.flags.DEFINE_float(\n    \'match_threshold\', 0.5, \'Matching threshold in the loss function.\')\ntf.app.flags.DEFINE_float(\n    \'neg_threshold\', 0.5, \'Matching threshold for the negtive examples in the loss function.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180503, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 5e-4, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 1e-3, \'Initial learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'500, 80000, 100000\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'0.1, 1, 0.1, 0.01\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'vgg_16\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    \'model_scope\', \'ssd300\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', \'ssd300/multibox_head, ssd300/additional_layers, ssd300/conv4_3_scale\',\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'multi_gpu\', True,\n    \'Whether there is GPU to use for training.\')\n\nFLAGS = tf.app.flags.FLAGS\n#CUDA_VISIBLE_DEVICES\ndef validate_batch_size_for_multi_gpu(batch_size):\n    """"""For multi-gpu, batch-size must be a multiple of the number of\n    available GPUs.\n\n    Note that this should eventually be handled by replicate_model_fn\n    directly. Multi-GPU support is currently experimental, however,\n    so doing the work here until that feature is in place.\n    """"""\n    if FLAGS.multi_gpu:\n        from tensorflow.python.client import device_lib\n\n        local_device_protos = device_lib.list_local_devices()\n        num_gpus = sum([1 for d in local_device_protos if d.device_type == \'GPU\'])\n        if not num_gpus:\n            raise ValueError(\'Multi-GPU mode was specified, but no GPUs \'\n                            \'were found. To use CPU, run --multi_gpu=False.\')\n\n        remainder = batch_size % num_gpus\n        if remainder:\n            err = (\'When running with multiple GPUs, batch size \'\n                    \'must be a multiple of the number of available GPUs. \'\n                    \'Found {} GPUs with a batch size of {}; try --batch_size={} instead.\'\n                    ).format(num_gpus, batch_size, batch_size - remainder)\n            raise ValueError(err)\n        return num_gpus\n    return 0\n\ndef get_init_fn():\n    return scaffolds.get_init_fn_for_scaffold(FLAGS.model_dir, FLAGS.checkpoint_path,\n                                            FLAGS.model_scope, FLAGS.checkpoint_model_scope,\n                                            FLAGS.checkpoint_exclude_scopes, FLAGS.ignore_missing_vars,\n                                            name_remap={\'/kernel\': \'/weights\', \'/bias\': \'/biases\'})\n\n# couldn\'t find better way to pass params from input_fn to model_fn\n# some tensors used by model_fn must be created in input_fn to ensure they are in the same graph\n# but when we put these tensors to labels\'s dict, the replicate_model_fn will split them into each GPU\n# the problem is that they shouldn\'t be splited\nglobal_anchor_info = dict()\n\ndef input_pipeline(dataset_pattern=\'train-*\', is_training=True, batch_size=FLAGS.batch_size):\n    def input_fn():\n        out_shape = [FLAGS.train_image_size] * 2\n        anchor_creator = anchor_manipulator.AnchorCreator(out_shape,\n                                                    layers_shapes = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n                                                    anchor_scales = [(0.1,), (0.2,), (0.375,), (0.55,), (0.725,), (0.9,)],\n                                                    extra_anchor_scales = [(0.1414,), (0.2739,), (0.4541,), (0.6315,), (0.8078,), (0.9836,)],\n                                                    anchor_ratios = [(1., 2., .5), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., .5), (1., 2., .5)],\n                                                    layer_steps = [8, 16, 32, 64, 100, 300])\n        all_anchors, all_num_anchors_depth, all_num_anchors_spatial = anchor_creator.get_all_anchors()\n\n        num_anchors_per_layer = []\n        for ind in range(len(all_anchors)):\n            num_anchors_per_layer.append(all_num_anchors_depth[ind] * all_num_anchors_spatial[ind])\n\n        anchor_encoder_decoder = anchor_manipulator.AnchorEncoder(allowed_borders = [1.0] * 6,\n                                                            positive_threshold = FLAGS.match_threshold,\n                                                            ignore_threshold = FLAGS.neg_threshold,\n                                                            prior_scaling=[0.1, 0.1, 0.2, 0.2])\n\n        image_preprocessing_fn = lambda image_, labels_, bboxes_ : ssd_preprocessing.preprocess_image(image_, labels_, bboxes_, out_shape, is_training=is_training, data_format=FLAGS.data_format, output_rgb=False)\n        anchor_encoder_fn = lambda glabels_, gbboxes_: anchor_encoder_decoder.encode_all_anchors(glabels_, gbboxes_, all_anchors, all_num_anchors_depth, all_num_anchors_spatial)\n\n        image, _, shape, loc_targets, cls_targets, match_scores = dataset_common.slim_get_batch(FLAGS.num_classes,\n                                                                                batch_size,\n                                                                                (\'train\' if is_training else \'val\'),\n                                                                                os.path.join(FLAGS.data_dir, dataset_pattern),\n                                                                                FLAGS.num_readers,\n                                                                                FLAGS.num_preprocessing_threads,\n                                                                                image_preprocessing_fn,\n                                                                                anchor_encoder_fn,\n                                                                                num_epochs=FLAGS.train_epochs,\n                                                                                is_training=is_training)\n        global global_anchor_info\n        global_anchor_info = {\'decode_fn\': lambda pred : anchor_encoder_decoder.decode_all_anchors(pred, num_anchors_per_layer),\n                            \'num_anchors_per_layer\': num_anchors_per_layer,\n                            \'all_num_anchors_depth\': all_num_anchors_depth }\n\n        return image, {\'shape\': shape, \'loc_targets\': loc_targets, \'cls_targets\': cls_targets, \'match_scores\': match_scores}\n    return input_fn\n\ndef modified_smooth_l1(bbox_pred, bbox_targets, bbox_inside_weights=1., bbox_outside_weights=1., sigma=1.):\n    """"""\n        ResultLoss = outside_weights * SmoothL1(inside_weights * (bbox_pred - bbox_targets))\n        SmoothL1(x) = 0.5 * (sigma * x)^2,    if |x| < 1 / sigma^2\n                      |x| - 0.5 / sigma^2,    otherwise\n    """"""\n    with tf.name_scope(\'smooth_l1\', [bbox_pred, bbox_targets]):\n        sigma2 = sigma * sigma\n\n        inside_mul = tf.multiply(bbox_inside_weights, tf.subtract(bbox_pred, bbox_targets))\n\n        smooth_l1_sign = tf.cast(tf.less(tf.abs(inside_mul), 1.0 / sigma2), tf.float32)\n        smooth_l1_option1 = tf.multiply(tf.multiply(inside_mul, inside_mul), 0.5 * sigma2)\n        smooth_l1_option2 = tf.subtract(tf.abs(inside_mul), 0.5 / sigma2)\n        smooth_l1_result = tf.add(tf.multiply(smooth_l1_option1, smooth_l1_sign),\n                                  tf.multiply(smooth_l1_option2, tf.abs(tf.subtract(smooth_l1_sign, 1.0))))\n\n        outside_mul = tf.multiply(bbox_outside_weights, smooth_l1_result)\n\n        return outside_mul\n\n\n# from scipy.misc import imread, imsave, imshow, imresize\n# import numpy as np\n# from utility import draw_toolbox\n\n# def save_image_with_bbox(image, labels_, scores_, bboxes_):\n#     if not hasattr(save_image_with_bbox, ""counter""):\n#         save_image_with_bbox.counter = 0  # it doesn\'t exist yet, so initialize it\n#     save_image_with_bbox.counter += 1\n\n#     img_to_draw = np.copy(image)\n\n#     img_to_draw = draw_toolbox.bboxes_draw_on_img(img_to_draw, labels_, scores_, bboxes_, thickness=2)\n#     imsave(os.path.join(\'./debug/{}.jpg\').format(save_image_with_bbox.counter), img_to_draw)\n#     return save_image_with_bbox.counter\n\ndef ssd_model_fn(features, labels, mode, params):\n    """"""model_fn for SSD to be used with our Estimator.""""""\n    shape = labels[\'shape\']\n    loc_targets = labels[\'loc_targets\']\n    cls_targets = labels[\'cls_targets\']\n    match_scores = labels[\'match_scores\']\n\n    global global_anchor_info\n    decode_fn = global_anchor_info[\'decode_fn\']\n    num_anchors_per_layer = global_anchor_info[\'num_anchors_per_layer\']\n    all_num_anchors_depth = global_anchor_info[\'all_num_anchors_depth\']\n\n    # bboxes_pred = decode_fn(loc_targets[0])\n    # bboxes_pred = [tf.reshape(preds, [-1, 4]) for preds in bboxes_pred]\n    # bboxes_pred = tf.concat(bboxes_pred, axis=0)\n    # save_image_op = tf.py_func(save_image_with_bbox,\n    #                         [ssd_preprocessing.unwhiten_image(features[0]),\n    #                         tf.clip_by_value(cls_targets[0], 0, tf.int64.max),\n    #                         match_scores[0],\n    #                         bboxes_pred],\n    #                         tf.int64, stateful=True)\n    # with tf.control_dependencies([save_image_op]):\n\n    #print(all_num_anchors_depth)\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        backbone = ssd_net.VGG16Backbone(params[\'data_format\'])\n        feature_layers = backbone.forward(features, training=(mode == tf.estimator.ModeKeys.TRAIN))\n        #print(feature_layers)\n        location_pred, cls_pred = ssd_net.multibox_head(feature_layers, params[\'num_classes\'], all_num_anchors_depth, data_format=params[\'data_format\'])\n\n        if params[\'data_format\'] == \'channels_first\':\n            cls_pred = [tf.transpose(pred, [0, 2, 3, 1]) for pred in cls_pred]\n            location_pred = [tf.transpose(pred, [0, 2, 3, 1]) for pred in location_pred]\n\n        cls_pred = [tf.reshape(pred, [tf.shape(features)[0], -1, params[\'num_classes\']]) for pred in cls_pred]\n        location_pred = [tf.reshape(pred, [tf.shape(features)[0], -1, 4]) for pred in location_pred]\n\n        cls_pred = tf.concat(cls_pred, axis=1)\n        location_pred = tf.concat(location_pred, axis=1)\n\n        cls_pred = tf.reshape(cls_pred, [-1, params[\'num_classes\']])\n        location_pred = tf.reshape(location_pred, [-1, 4])\n\n    with tf.device(\'/cpu:0\'):\n        with tf.control_dependencies([cls_pred, location_pred]):\n            with tf.name_scope(\'post_forward\'):\n                #bboxes_pred = decode_fn(location_pred)\n                bboxes_pred = tf.map_fn(lambda _preds : decode_fn(_preds),\n                                        tf.reshape(location_pred, [tf.shape(features)[0], -1, 4]),\n                                        dtype=[tf.float32] * len(num_anchors_per_layer), back_prop=False)\n                #cls_targets = tf.Print(cls_targets, [tf.shape(bboxes_pred[0]),tf.shape(bboxes_pred[1]),tf.shape(bboxes_pred[2]),tf.shape(bboxes_pred[3])])\n                bboxes_pred = [tf.reshape(preds, [-1, 4]) for preds in bboxes_pred]\n                bboxes_pred = tf.concat(bboxes_pred, axis=0)\n\n                flaten_cls_targets = tf.reshape(cls_targets, [-1])\n                flaten_match_scores = tf.reshape(match_scores, [-1])\n                flaten_loc_targets = tf.reshape(loc_targets, [-1, 4])\n\n                # each positive examples has one label\n                positive_mask = flaten_cls_targets > 0\n                n_positives = tf.count_nonzero(positive_mask)\n\n                batch_n_positives = tf.count_nonzero(cls_targets, -1)\n\n                batch_negtive_mask = tf.equal(cls_targets, 0)#tf.logical_and(tf.equal(cls_targets, 0), match_scores > 0.)\n                batch_n_negtives = tf.count_nonzero(batch_negtive_mask, -1)\n\n                batch_n_neg_select = tf.cast(params[\'negative_ratio\'] * tf.cast(batch_n_positives, tf.float32), tf.int32)\n                batch_n_neg_select = tf.minimum(batch_n_neg_select, tf.cast(batch_n_negtives, tf.int32))\n\n                # hard negative mining for classification\n                predictions_for_bg = tf.nn.softmax(tf.reshape(cls_pred, [tf.shape(features)[0], -1, params[\'num_classes\']]))[:, :, 0]\n                prob_for_negtives = tf.where(batch_negtive_mask,\n                                       0. - predictions_for_bg,\n                                       # ignore all the positives\n                                       0. - tf.ones_like(predictions_for_bg))\n                topk_prob_for_bg, _ = tf.nn.top_k(prob_for_negtives, k=tf.shape(prob_for_negtives)[1])\n                score_at_k = tf.gather_nd(topk_prob_for_bg, tf.stack([tf.range(tf.shape(features)[0]), batch_n_neg_select - 1], axis=-1))\n\n                selected_neg_mask = prob_for_negtives >= tf.expand_dims(score_at_k, axis=-1)\n\n                # include both selected negtive and all positive examples\n                final_mask = tf.stop_gradient(tf.logical_or(tf.reshape(tf.logical_and(batch_negtive_mask, selected_neg_mask), [-1]), positive_mask))\n                total_examples = tf.count_nonzero(final_mask)\n\n                cls_pred = tf.boolean_mask(cls_pred, final_mask)\n                location_pred = tf.boolean_mask(location_pred, tf.stop_gradient(positive_mask))\n                flaten_cls_targets = tf.boolean_mask(tf.clip_by_value(flaten_cls_targets, 0, params[\'num_classes\']), final_mask)\n                flaten_loc_targets = tf.stop_gradient(tf.boolean_mask(flaten_loc_targets, positive_mask))\n\n                predictions = {\n                            \'classes\': tf.argmax(cls_pred, axis=-1),\n                            \'probabilities\': tf.reduce_max(tf.nn.softmax(cls_pred, name=\'softmax_tensor\'), axis=-1),\n                            \'loc_predict\': bboxes_pred }\n\n                cls_accuracy = tf.metrics.accuracy(flaten_cls_targets, predictions[\'classes\'])\n                metrics = {\'cls_accuracy\': cls_accuracy}\n\n                # Create a tensor named train_accuracy for logging purposes.\n                tf.identity(cls_accuracy[1], name=\'cls_accuracy\')\n                tf.summary.scalar(\'cls_accuracy\', cls_accuracy[1])\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n    # Calculate loss, which includes softmax cross entropy and L2 regularization.\n    #cross_entropy = tf.cond(n_positives > 0, lambda: tf.losses.sparse_softmax_cross_entropy(labels=flaten_cls_targets, logits=cls_pred), lambda: 0.)# * (params[\'negative_ratio\'] + 1.)\n    #flaten_cls_targets=tf.Print(flaten_cls_targets, [flaten_loc_targets],summarize=50000)\n    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=flaten_cls_targets, logits=cls_pred) * (params[\'negative_ratio\'] + 1.)\n    # Create a tensor named cross_entropy for logging purposes.\n    tf.identity(cross_entropy, name=\'cross_entropy_loss\')\n    tf.summary.scalar(\'cross_entropy_loss\', cross_entropy)\n\n    #loc_loss = tf.cond(n_positives > 0, lambda: modified_smooth_l1(location_pred, tf.stop_gradient(flaten_loc_targets), sigma=1.), lambda: tf.zeros_like(location_pred))\n    loc_loss = modified_smooth_l1(location_pred, flaten_loc_targets, sigma=1.)\n    #loc_loss = modified_smooth_l1(location_pred, tf.stop_gradient(gtargets))\n    loc_loss = tf.reduce_mean(tf.reduce_sum(loc_loss, axis=-1), name=\'location_loss\')\n    tf.summary.scalar(\'location_loss\', loc_loss)\n    tf.losses.add_loss(loc_loss)\n\n    l2_loss_vars = []\n    for trainable_var in tf.trainable_variables():\n        if \'_bn\' not in trainable_var.name:\n            if \'conv4_3_scale\' not in trainable_var.name:\n                l2_loss_vars.append(tf.nn.l2_loss(trainable_var))\n            else:\n                l2_loss_vars.append(tf.nn.l2_loss(trainable_var) * 0.1)\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    total_loss = tf.add(cross_entropy + loc_loss, tf.multiply(params[\'weight_decay\'], tf.add_n(l2_loss_vars), name=\'l2_loss\'), name=\'total_loss\')\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'learning_rate\'] * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [int(_) for _ in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        # Create a tensor named learning_rate for logging purposes.\n        tf.summary.scalar(\'learning_rate\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n        optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(total_loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                              mode=mode,\n                              predictions=predictions,\n                              loss=total_loss,\n                              train_op=train_op,\n                              eval_metric_ops=metrics,\n                              scaffold=tf.train.Scaffold(init_fn=get_init_fn()))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, intra_op_parallelism_threads=FLAGS.num_cpu_threads, inter_op_parallelism_threads=FLAGS.num_cpu_threads, gpu_options=gpu_options)\n\n    num_gpus = validate_batch_size_for_multi_gpu(FLAGS.batch_size)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=config)\n\n    replicate_ssd_model_fn = tf.contrib.estimator.replicate_model_fn(ssd_model_fn, loss_reduction=tf.losses.Reduction.MEAN)\n    ssd_detector = tf.estimator.Estimator(\n        model_fn=replicate_ssd_model_fn, model_dir=FLAGS.model_dir, config=run_config,\n        params={\n            \'num_gpus\': num_gpus,\n            \'data_format\': FLAGS.data_format,\n            \'batch_size\': FLAGS.batch_size,\n            \'model_scope\': FLAGS.model_scope,\n            \'num_classes\': FLAGS.num_classes,\n            \'negative_ratio\': FLAGS.negative_ratio,\n            \'match_threshold\': FLAGS.match_threshold,\n            \'neg_threshold\': FLAGS.neg_threshold,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'decay_boundaries\': parse_comma_list(FLAGS.decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(FLAGS.lr_decay_factors),\n        })\n    tensors_to_log = {\n        \'lr\': \'learning_rate\',\n        \'ce\': \'cross_entropy_loss\',\n        \'loc\': \'location_loss\',\n        \'loss\': \'total_loss\',\n        \'l2\': \'l2_loss\',\n        \'acc\': \'post_forward/cls_accuracy\',\n    }\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps,\n                                            formatter=lambda dicts: (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n    #hook = tf.train.ProfilerHook(save_steps=50, output_dir=\'.\', show_memory=True)\n    print(\'Starting a training cycle.\')\n    ssd_detector.train(input_fn=input_pipeline(dataset_pattern=\'train-*\', is_training=True, batch_size=FLAGS.batch_size),\n                    hooks=[logging_hook], max_steps=FLAGS.max_number_of_steps)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n\n\n    # cls_targets = tf.reshape(cls_targets, [-1])\n    # match_scores = tf.reshape(match_scores, [-1])\n    # loc_targets = tf.reshape(loc_targets, [-1, 4])\n\n    # # each positive examples has one label\n    # positive_mask = cls_targets > 0\n    # n_positives = tf.count_nonzero(positive_mask)\n\n    # negtive_mask = tf.logical_and(tf.equal(cls_targets, 0), match_scores > 0.)\n    # n_negtives = tf.count_nonzero(negtive_mask)\n\n    # n_neg_to_select = tf.cast(params[\'negative_ratio\'] * tf.cast(n_positives, tf.float32), tf.int32)\n    # n_neg_to_select = tf.minimum(n_neg_to_select, tf.cast(n_negtives, tf.int32))\n\n    # # hard negative mining for classification\n    # predictions_for_bg = tf.nn.softmax(cls_pred)[:, 0]\n\n    # prob_for_negtives = tf.where(negtive_mask,\n    #                        0. - predictions_for_bg,\n    #                        # ignore all the positives\n    #                        0. - tf.ones_like(predictions_for_bg))\n    # topk_prob_for_bg, _ = tf.nn.top_k(prob_for_negtives, k=n_neg_to_select)\n    # selected_neg_mask = prob_for_negtives > topk_prob_for_bg[-1]\n\n    # # include both selected negtive and all positive examples\n    # final_mask = tf.stop_gradient(tf.logical_or(tf.logical_and(negtive_mask, selected_neg_mask), positive_mask))\n    # total_examples = tf.count_nonzero(final_mask)\n\n    # glabels = tf.boolean_mask(tf.clip_by_value(cls_targets, 0, FLAGS.num_classes), final_mask)\n    # cls_pred = tf.boolean_mask(cls_pred, final_mask)\n    # location_pred = tf.boolean_mask(location_pred, tf.stop_gradient(positive_mask))\n    # loc_targets = tf.boolean_mask(loc_targets, tf.stop_gradient(positive_mask))\n'"
voc_eval.py,0,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport numpy as np\nimport pickle\n\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\nfrom dataset import dataset_common\n\n\'\'\'\nVOC2007TEST\n    Annotations\n    ...\n    ImageSets\n\'\'\'\ndataset_path = \'/media/rs/7A0EE8880EE83EAF/Detections/PASCAL/VOC/VOC2007TEST\'\n# change above path according to your system settings\npred_path = \'./logs/predict\'\npred_file = \'results_{}.txt\' # from 1-num_classes\noutput_path = \'./logs/predict/eval_output\'\ncache_path = \'./logs/predict/eval_cache\'\nanno_files = \'Annotations/{}.xml\'\nall_images_file = \'ImageSets/Main/test.txt\'\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text) - 1,\n                              int(bbox.find(\'ymin\').text) - 1,\n                              int(bbox.find(\'xmax\').text) - 1,\n                              int(bbox.find(\'ymax\').text) - 1]\n        objects.append(obj_struct)\n\n    return objects\n\ndef do_python_eval(use_07=True):\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = use_07\n    print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_path):\n        os.mkdir(output_path)\n    for cls_name, cls_pair in dataset_common.VOC_LABELS.items():\n        if \'none\' in cls_name:\n            continue\n        cls_id = cls_pair[0]\n        filename = os.path.join(pred_path, pred_file.format(cls_id))\n        rec, prec, ap = voc_eval(filename, os.path.join(dataset_path, anno_files),\n                os.path.join(dataset_path, all_images_file), cls_name, cache_path,\n                ovthresh=0.5, use_07_metric=use_07_metric)\n        aps += [ap]\n        print(\'AP for {} = {:.4f}\'.format(cls_name, ap))\n        with open(os.path.join(output_path, cls_name + \'_pr.pkl\'), \'wb\') as f:\n            pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n    print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n    print(\'~~~~~~~~\')\n    print(\'Results:\')\n    for ap in aps:\n        print(\'{:.3f}\'.format(ap))\n    print(\'{:.3f}\'.format(np.mean(aps)))\n    print(\'~~~~~~~~\')\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'--------------------------------------------------------------\')\n\n\ndef voc_ap(rec, prec, use_07_metric=True):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=True):\n    """"""rec, prec, ap = voc_eval(detpath,\n                               annopath,\n                               imagesetfile,\n                               classname,\n                               [ovthresh],\n                               [use_07_metric])\n        Top level function that does the PASCAL VOC evaluation.\n        detpath: Path to detections\n           detpath.format(classname) should produce the detection results file.\n        annopath: Path to annotations\n           annopath.format(imagename) should be the xml annotations file.\n        imagesetfile: Text file containing the list of images, one image per line.\n        classname: Category name (duh)\n        cachedir: Directory for caching the annotations\n        [ovthresh]: Overlap threshold (default = 0.5)\n        [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n           (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print(\'Reading annotation for {:d}/{:d}\'.format(\n                   i + 1, len(imagenames)))\n        # save\n        print(\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            pickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = pickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n    # read dets\n    with open(detpath, \'r\') as f:\n        lines = f.readlines()\n\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(\' \') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin, 0.)\n                ih = np.maximum(iymax - iymin, 0.)\n                inters = iw * ih\n                uni = ((bb[2] - bb[0]) * (bb[3] - bb[1]) +\n                       (BBGT[:, 2] - BBGT[:, 0]) *\n                       (BBGT[:, 3] - BBGT[:, 1]) - inters)\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n        rec = -1.\n        prec = -1.\n        ap = -1.\n\n    return rec, prec, ap\n\nif __name__ == \'__main__\':\n        do_python_eval()\n'"
dataset/convert_tfrecords.py,27,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\nimport xml.etree.ElementTree as xml_tree\n\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nimport dataset_common\n\n\'\'\'How to organize your dataset folder:\n  VOCROOT/\n       |->VOC2007/\n       |    |->Annotations/\n       |    |->ImageSets/\n       |    |->...\n       |->VOC2012/\n       |    |->Annotations/\n       |    |->ImageSets/\n       |    |->...\n       |->VOC2007TEST/\n       |    |->Annotations/\n       |    |->...\n\'\'\'\ntf.app.flags.DEFINE_string(\'dataset_directory\', \'/media/rs/7A0EE8880EE83EAF/Detections/PASCAL/VOC\',\n                           \'All datas directory\')\ntf.app.flags.DEFINE_string(\'train_splits\', \'VOC2007, VOC2012\',\n                           \'Comma-separated list of the training data sub-directory\')\ntf.app.flags.DEFINE_string(\'validation_splits\', \'VOC2007TEST\',\n                           \'Comma-separated list of the validation data sub-directory\')\ntf.app.flags.DEFINE_string(\'output_directory\', \'/media/rs/7A0EE8880EE83EAF/Detections/SSD/dataset/tfrecords\',\n                           \'Output data directory\')\ntf.app.flags.DEFINE_integer(\'train_shards\', 16,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'validation_shards\', 16,\n                            \'Number of shards in validation TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'num_threads\', 8,\n                            \'Number of threads to preprocess the images.\')\nRANDOM_SEED = 180428\n\nFLAGS = tf.app.flags.FLAGS\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n  """"""Wrapper for inserting float features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _bytes_list_feature(value):\n    """"""Wrapper for inserting a list of bytes features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  if isinstance(value, six.string_types):\n    value = six.binary_type(value, encoding=\'utf-8\')\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _convert_to_example(filename, image_name, image_buffer, bboxes, labels, labels_text,\n                        difficult, truncated, height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    bboxes: List of bounding boxes for each image\n    labels: List of labels for bounding box\n    labels_text: List of labels\' name for bounding box\n    difficult: List of ints indicate the difficulty of that bounding box\n    truncated: List of ints indicate the truncation of that bounding box\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  ymin = []\n  xmin = []\n  ymax = []\n  xmax = []\n  for b in bboxes:\n    assert len(b) == 4\n    # pylint: disable=expression-not-assigned\n    [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n    # pylint: enable=expression-not-assigned\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n            \'image/height\': _int64_feature(height),\n            \'image/width\': _int64_feature(width),\n            \'image/channels\': _int64_feature(channels),\n            \'image/shape\': _int64_feature([height, width, channels]),\n            \'image/object/bbox/xmin\': _float_feature(xmin),\n            \'image/object/bbox/xmax\': _float_feature(xmax),\n            \'image/object/bbox/ymin\': _float_feature(ymin),\n            \'image/object/bbox/ymax\': _float_feature(ymax),\n            \'image/object/bbox/label\': _int64_feature(labels),\n            \'image/object/bbox/label_text\': _bytes_list_feature(labels_text),\n            \'image/object/bbox/difficult\': _int64_feature(difficult),\n            \'image/object/bbox/truncated\': _int64_feature(truncated),\n            \'image/format\': _bytes_feature(image_format),\n            \'image/filename\': _bytes_feature(image_name.encode(\'utf8\')),\n            \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'rb\') as f:\n    image_data = f.read()\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\ndef _find_image_bounding_boxes(directory, cur_record):\n  """"""Find the bounding boxes for a given image file.\n\n  Args:\n    directory: string; the path of all datas.\n    cur_record: list of strings; the first of which is the sub-directory of cur_record, the second is the image filename.\n  Returns:\n    bboxes: List of bounding boxes for each image.\n    labels: List of labels for bounding box.\n    labels_text: List of labels\' name for bounding box.\n    difficult: List of ints indicate the difficulty of that bounding box.\n    truncated: List of ints indicate the truncation of that bounding box.\n  """"""\n  anna_file = os.path.join(directory, cur_record[0], \'Annotations\', cur_record[1].replace(\'jpg\', \'xml\'))\n\n  tree = xml_tree.parse(anna_file)\n  root = tree.getroot()\n\n  # Image shape.\n  size = root.find(\'size\')\n  shape = [int(size.find(\'height\').text),\n           int(size.find(\'width\').text),\n           int(size.find(\'depth\').text)]\n  # Find annotations.\n  bboxes = []\n  labels = []\n  labels_text = []\n  difficult = []\n  truncated = []\n  for obj in root.findall(\'object\'):\n      label = obj.find(\'name\').text\n      labels.append(int(dataset_common.VOC_LABELS[label][0]))\n      labels_text.append(label.encode(\'ascii\'))\n\n      isdifficult = obj.find(\'difficult\')\n      if isdifficult is not None:\n          difficult.append(int(isdifficult.text))\n      else:\n          difficult.append(0)\n\n      istruncated = obj.find(\'truncated\')\n      if istruncated is not None:\n          truncated.append(int(istruncated.text))\n      else:\n          truncated.append(0)\n\n      bbox = obj.find(\'bndbox\')\n      bboxes.append((float(bbox.find(\'ymin\').text) / shape[0],\n                     float(bbox.find(\'xmin\').text) / shape[1],\n                     float(bbox.find(\'ymax\').text) / shape[0],\n                     float(bbox.find(\'xmax\').text) / shape[1]\n                     ))\n  return bboxes, labels, labels_text, difficult, truncated\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, directory, all_records, num_shards):\n  """"""Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    directory: string; the path of all datas\n    all_records: list of string tuples; the first of each tuple is the sub-directory of the record, the second is the image filename.\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in range(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      cur_record = all_records[i]\n      filename = os.path.join(directory, cur_record[0], \'JPEGImages\', cur_record[1])\n\n      bboxes, labels, labels_text, difficult, truncated = _find_image_bounding_boxes(directory, cur_record)\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, cur_record[1], image_buffer, bboxes, labels, labels_text,\n                                    difficult, truncated, height, width)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print(\'%s [thread %d]: Wrote %d images to %s\' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()\n\ndef _process_image_files(name, directory, all_records, num_shards):\n  """"""Process and save list of images as TFRecord of Example protos.\n\n  Args:\n    name: string, unique identifier specifying the data set\n    directory: string; the path of all datas\n    all_records: list of string tuples; the first of each tuple is the sub-directory of the record, the second is the image filename.\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(all_records), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  threads = []\n  for i in range(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i + 1]])\n\n  # Launch a thread for each batch.\n  print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in range(len(ranges)):\n    args = (coder, thread_index, ranges, name, directory, all_records, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(\'%s: Finished writing all %d images in data set.\' %\n        (datetime.now(), len(all_records)))\n  sys.stdout.flush()\n\ndef _process_dataset(name, directory, all_splits, num_shards):\n  """"""Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    all_splits: list of strings, sub-path to the data set.\n    num_shards: integer number of shards for this data set.\n  """"""\n  all_records = []\n  for split in all_splits:\n    jpeg_file_path = os.path.join(directory, split, \'JPEGImages\')\n    images = tf.gfile.ListDirectory(jpeg_file_path)\n    jpegs = [im_name for im_name in images if im_name.strip()[-3:]==\'jpg\']\n    all_records.extend(list(zip([split] * len(jpegs), jpegs)))\n\n  shuffled_index = list(range(len(all_records)))\n  random.seed(RANDOM_SEED)\n  random.shuffle(shuffled_index)\n  all_records = [all_records[i] for i in shuffled_index]\n  _process_image_files(name, directory, all_records, num_shards)\n\ndef parse_comma_list(args):\n    return [s.strip() for s in args.split(\',\')]\n\ndef main(unused_argv):\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with \'\n      \'FLAGS.validation_shards\')\n  print(\'Saving results to %s\' % FLAGS.output_directory)\n\n  # Run it!\n  _process_dataset(\'val\', FLAGS.dataset_directory, parse_comma_list(FLAGS.validation_splits), FLAGS.validation_shards)\n  _process_dataset(\'train\', FLAGS.dataset_directory, parse_comma_list(FLAGS.train_splits), FLAGS.train_shards)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
dataset/dataset_common.py,23,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\nVOC_LABELS = {\n    \'none\': (0, \'Background\'),\n    \'aeroplane\': (1, \'Vehicle\'),\n    \'bicycle\': (2, \'Vehicle\'),\n    \'bird\': (3, \'Animal\'),\n    \'boat\': (4, \'Vehicle\'),\n    \'bottle\': (5, \'Indoor\'),\n    \'bus\': (6, \'Vehicle\'),\n    \'car\': (7, \'Vehicle\'),\n    \'cat\': (8, \'Animal\'),\n    \'chair\': (9, \'Indoor\'),\n    \'cow\': (10, \'Animal\'),\n    \'diningtable\': (11, \'Indoor\'),\n    \'dog\': (12, \'Animal\'),\n    \'horse\': (13, \'Animal\'),\n    \'motorbike\': (14, \'Vehicle\'),\n    \'person\': (15, \'Person\'),\n    \'pottedplant\': (16, \'Indoor\'),\n    \'sheep\': (17, \'Animal\'),\n    \'sofa\': (18, \'Indoor\'),\n    \'train\': (19, \'Vehicle\'),\n    \'tvmonitor\': (20, \'Indoor\'),\n}\n\nCOCO_LABELS = {\n    ""bench"":  (14, \'outdoor\') ,\n    ""skateboard"":  (37, \'sports\') ,\n    ""toothbrush"":  (80, \'indoor\') ,\n    ""person"":  (1, \'person\') ,\n    ""donut"":  (55, \'food\') ,\n    ""none"":  (0, \'background\') ,\n    ""refrigerator"":  (73, \'appliance\') ,\n    ""horse"":  (18, \'animal\') ,\n    ""elephant"":  (21, \'animal\') ,\n    ""book"":  (74, \'indoor\') ,\n    ""car"":  (3, \'vehicle\') ,\n    ""keyboard"":  (67, \'electronic\') ,\n    ""cow"":  (20, \'animal\') ,\n    ""microwave"":  (69, \'appliance\') ,\n    ""traffic light"":  (10, \'outdoor\') ,\n    ""tie"":  (28, \'accessory\') ,\n    ""dining table"":  (61, \'furniture\') ,\n    ""toaster"":  (71, \'appliance\') ,\n    ""baseball glove"":  (36, \'sports\') ,\n    ""giraffe"":  (24, \'animal\') ,\n    ""cake"":  (56, \'food\') ,\n    ""handbag"":  (27, \'accessory\') ,\n    ""scissors"":  (77, \'indoor\') ,\n    ""bowl"":  (46, \'kitchen\') ,\n    ""couch"":  (58, \'furniture\') ,\n    ""chair"":  (57, \'furniture\') ,\n    ""boat"":  (9, \'vehicle\') ,\n    ""hair drier"":  (79, \'indoor\') ,\n    ""airplane"":  (5, \'vehicle\') ,\n    ""pizza"":  (54, \'food\') ,\n    ""backpack"":  (25, \'accessory\') ,\n    ""kite"":  (34, \'sports\') ,\n    ""sheep"":  (19, \'animal\') ,\n    ""umbrella"":  (26, \'accessory\') ,\n    ""stop sign"":  (12, \'outdoor\') ,\n    ""truck"":  (8, \'vehicle\') ,\n    ""skis"":  (31, \'sports\') ,\n    ""sandwich"":  (49, \'food\') ,\n    ""broccoli"":  (51, \'food\') ,\n    ""wine glass"":  (41, \'kitchen\') ,\n    ""surfboard"":  (38, \'sports\') ,\n    ""sports ball"":  (33, \'sports\') ,\n    ""cell phone"":  (68, \'electronic\') ,\n    ""dog"":  (17, \'animal\') ,\n    ""bed"":  (60, \'furniture\') ,\n    ""toilet"":  (62, \'furniture\') ,\n    ""fire hydrant"":  (11, \'outdoor\') ,\n    ""oven"":  (70, \'appliance\') ,\n    ""zebra"":  (23, \'animal\') ,\n    ""tv"":  (63, \'electronic\') ,\n    ""potted plant"":  (59, \'furniture\') ,\n    ""parking meter"":  (13, \'outdoor\') ,\n    ""spoon"":  (45, \'kitchen\') ,\n    ""bus"":  (6, \'vehicle\') ,\n    ""laptop"":  (64, \'electronic\') ,\n    ""cup"":  (42, \'kitchen\') ,\n    ""bird"":  (15, \'animal\') ,\n    ""sink"":  (72, \'appliance\') ,\n    ""remote"":  (66, \'electronic\') ,\n    ""bicycle"":  (2, \'vehicle\') ,\n    ""tennis racket"":  (39, \'sports\') ,\n    ""baseball bat"":  (35, \'sports\') ,\n    ""cat"":  (16, \'animal\') ,\n    ""fork"":  (43, \'kitchen\') ,\n    ""suitcase"":  (29, \'accessory\') ,\n    ""snowboard"":  (32, \'sports\') ,\n    ""clock"":  (75, \'indoor\') ,\n    ""apple"":  (48, \'food\') ,\n    ""mouse"":  (65, \'electronic\') ,\n    ""bottle"":  (40, \'kitchen\') ,\n    ""frisbee"":  (30, \'sports\') ,\n    ""carrot"":  (52, \'food\') ,\n    ""bear"":  (22, \'animal\') ,\n    ""hot dog"":  (53, \'food\') ,\n    ""teddy bear"":  (78, \'indoor\') ,\n    ""knife"":  (44, \'kitchen\') ,\n    ""train"":  (7, \'vehicle\') ,\n    ""vase"":  (76, \'indoor\') ,\n    ""banana"":  (47, \'food\') ,\n    ""motorcycle"":  (4, \'vehicle\') ,\n    ""orange"":  (50, \'food\')\n  }\n\n# use dataset_inspect.py to get these summary\ndata_splits_num = {\n    \'train\': 22136,\n    \'val\': 4952,\n}\n\ndef slim_get_batch(num_classes, batch_size, split_name, file_pattern, num_readers, num_preprocessing_threads, image_preprocessing_fn, anchor_encoder, num_epochs=None, is_training=True):\n    """"""Gets a dataset tuple with instructions for reading Pascal VOC dataset.\n\n    Args:\n      num_classes: total class numbers in dataset.\n      batch_size: the size of each batch.\n      split_name: \'train\' of \'val\'.\n      file_pattern: The file pattern to use when matching the dataset sources (full path).\n      num_readers: the max number of reader used for reading tfrecords.\n      num_preprocessing_threads: the max number of threads used to run preprocessing function.\n      image_preprocessing_fn: the function used to dataset augumentation.\n      anchor_encoder: the function used to encoder all anchors.\n      num_epochs: total epoches for iterate this dataset.\n      is_training: whether we are in traing phase.\n\n    Returns:\n      A batch of [image, shape, loc_targets, cls_targets, match_scores].\n    """"""\n    if split_name not in data_splits_num:\n        raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/filename\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/shape\': tf.FixedLenFeature([3], tf.int64),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/label\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/difficult\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/truncated\': tf.VarLenFeature(dtype=tf.int64),\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'filename\': slim.tfexample_decoder.Tensor(\'image/filename\'),\n        \'shape\': slim.tfexample_decoder.Tensor(\'image/shape\'),\n        \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n        \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/bbox/label\'),\n        \'object/difficult\': slim.tfexample_decoder.Tensor(\'image/object/bbox/difficult\'),\n        \'object/truncated\': slim.tfexample_decoder.Tensor(\'image/object/bbox/truncated\'),\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n    labels_to_names = {}\n    for name, pair in VOC_LABELS.items():\n        labels_to_names[pair[0]] = name\n\n    dataset = slim.dataset.Dataset(\n                data_sources=file_pattern,\n                reader=tf.TFRecordReader,\n                decoder=decoder,\n                num_samples=data_splits_num[split_name],\n                items_to_descriptions=None,\n                num_classes=num_classes,\n                labels_to_names=labels_to_names)\n\n    with tf.name_scope(\'dataset_data_provider\'):\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n            dataset,\n            num_readers=num_readers,\n            common_queue_capacity=32 * batch_size,\n            common_queue_min=8 * batch_size,\n            shuffle=is_training,\n            num_epochs=num_epochs)\n\n    [org_image, filename, shape, glabels_raw, gbboxes_raw, isdifficult] = provider.get([\'image\', \'filename\', \'shape\',\n                                                                     \'object/label\',\n                                                                     \'object/bbox\',\n                                                                     \'object/difficult\'])\n\n    if is_training:\n        # if all is difficult, then keep the first one\n        isdifficult_mask =tf.cond(tf.count_nonzero(isdifficult, dtype=tf.int32) < tf.shape(isdifficult)[0],\n                                lambda : isdifficult < tf.ones_like(isdifficult),\n                                lambda : tf.one_hot(0, tf.shape(isdifficult)[0], on_value=True, off_value=False, dtype=tf.bool))\n\n        glabels_raw = tf.boolean_mask(glabels_raw, isdifficult_mask)\n        gbboxes_raw = tf.boolean_mask(gbboxes_raw, isdifficult_mask)\n\n    # Pre-processing image, labels and bboxes.\n\n    if is_training:\n        image, glabels, gbboxes = image_preprocessing_fn(org_image, glabels_raw, gbboxes_raw)\n    else:\n        image = image_preprocessing_fn(org_image, glabels_raw, gbboxes_raw)\n        glabels, gbboxes = glabels_raw, gbboxes_raw\n\n    gt_targets, gt_labels, gt_scores = anchor_encoder(glabels, gbboxes)\n\n    return tf.train.batch([image, filename, shape, gt_targets, gt_labels, gt_scores],\n                    dynamic_pad=False,\n                    batch_size=batch_size,\n                    allow_smaller_final_batch=(not is_training),\n                    num_threads=num_preprocessing_threads,\n                    capacity=64 * batch_size)\n'"
dataset/dataset_inspect.py,3,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\ndef count_split_examples(split_path, file_prefix=\'.tfrecord\'):\n    # Count the total number of examples in all of these shard\n    num_samples = 0\n    tfrecords_to_count = tf.gfile.Glob(os.path.join(split_path, file_prefix))\n    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    for tfrecord_file in tfrecords_to_count:\n        for record in tf.python_io.tf_record_iterator(tfrecord_file):#, options = opts):\n            num_samples += 1\n    return num_samples\n\nif __name__ == \'__main__\':\n    print(\'train:\', count_split_examples(\'/media/rs/7A0EE8880EE83EAF/Detections/SSD/dataset/tfrecords\', \'train-?????-of-?????\'))\n    print(\'val:\', count_split_examples(\'/media/rs/7A0EE8880EE83EAF/Detections/SSD/dataset/tfrecords\', \'val-?????-of-?????\'))\n'"
net/ssd_net.py,51,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\n_USE_FUSED_BN = True\n\n# vgg_16/conv2/conv2_1/biases\n# vgg_16/conv4/conv4_3/biases\n# vgg_16/conv1/conv1_1/biases\n# vgg_16/fc6/weights\n# vgg_16/conv3/conv3_2/biases\n# vgg_16/conv5/conv5_3/biases\n# vgg_16/conv3/conv3_1/weights\n# vgg_16/conv4/conv4_2/weights\n# vgg_16/conv1/conv1_1/weights\n# vgg_16/conv5/conv5_3/weights\n# vgg_16/conv4/conv4_1/weights\n# vgg_16/conv3/conv3_3/weights\n# vgg_16/conv5/conv5_2/biases\n# vgg_16/conv3/conv3_2/weights\n# vgg_16/conv4/conv4_2/biases\n# vgg_16/conv5/conv5_2/weights\n# vgg_16/conv3/conv3_1/biases\n# vgg_16/conv2/conv2_2/weights\n# vgg_16/fc7/weights\n# vgg_16/conv5/conv5_1/biases\n# vgg_16/conv1/conv1_2/biases\n# vgg_16/conv2/conv2_2/biases\n# vgg_16/conv4/conv4_1/biases\n# vgg_16/fc7/biases\n# vgg_16/fc6/biases\n# vgg_16/conv4/conv4_3/weights\n# vgg_16/conv2/conv2_1/weights\n# vgg_16/conv5/conv5_1/weights\n# vgg_16/conv3/conv3_3/biases\n# vgg_16/conv1/conv1_2/weights\n\nclass ReLuLayer(tf.layers.Layer):\n    def __init__(self, name, **kwargs):\n        super(ReLuLayer, self).__init__(name=name, trainable=trainable, **kwargs)\n        self._name = name\n    def build(self, input_shape):\n        self._relu = lambda x : tf.nn.relu(x, name=self._name)\n        self.built = True\n\n    def call(self, inputs):\n        return self._relu(inputs)\n\n    def compute_output_shape(self, input_shape):\n        return tf.TensorShape(input_shape)\n\ndef forward_module(m, inputs, training=False):\n    if isinstance(m, tf.layers.BatchNormalization) or isinstance(m, tf.layers.Dropout):\n        return m.apply(inputs, training=training)\n    return m.apply(inputs)\n\nclass VGG16Backbone(object):\n    def __init__(self, data_format=\'channels_first\'):\n        super(VGG16Backbone, self).__init__()\n        self._data_format = data_format\n        self._bn_axis = -1 if data_format == \'channels_last\' else 1\n        #initializer = tf.glorot_uniform_initializer  glorot_normal_initializer\n        self._conv_initializer = tf.glorot_uniform_initializer\n        self._conv_bn_initializer = tf.glorot_uniform_initializer#lambda : tf.truncated_normal_initializer(mean=0.0, stddev=0.005)\n        # VGG layers\n        self._conv1_block = self.conv_block(2, 64, 3, (1, 1), \'conv1\')\n        self._pool1 = tf.layers.MaxPooling2D(2, 2, padding=\'same\', data_format=self._data_format, name=\'pool1\')\n        self._conv2_block = self.conv_block(2, 128, 3, (1, 1), \'conv2\')\n        self._pool2 = tf.layers.MaxPooling2D(2, 2, padding=\'same\', data_format=self._data_format, name=\'pool2\')\n        self._conv3_block = self.conv_block(3, 256, 3, (1, 1), \'conv3\')\n        self._pool3 = tf.layers.MaxPooling2D(2, 2, padding=\'same\', data_format=self._data_format, name=\'pool3\')\n        self._conv4_block = self.conv_block(3, 512, 3, (1, 1), \'conv4\')\n        self._pool4 = tf.layers.MaxPooling2D(2, 2, padding=\'same\', data_format=self._data_format, name=\'pool4\')\n        self._conv5_block = self.conv_block(3, 512, 3, (1, 1), \'conv5\')\n        self._pool5 = tf.layers.MaxPooling2D(3, 1, padding=\'same\', data_format=self._data_format, name=\'pool5\')\n        self._conv6 = tf.layers.Conv2D(filters=1024, kernel_size=3, strides=1, padding=\'same\', dilation_rate=6,\n                            data_format=self._data_format, activation=tf.nn.relu, use_bias=True,\n                            kernel_initializer=self._conv_initializer(),\n                            bias_initializer=tf.zeros_initializer(),\n                            name=\'fc6\', _scope=\'fc6\', _reuse=None)\n        self._conv7 = tf.layers.Conv2D(filters=1024, kernel_size=1, strides=1, padding=\'same\',\n                            data_format=self._data_format, activation=tf.nn.relu, use_bias=True,\n                            kernel_initializer=self._conv_initializer(),\n                            bias_initializer=tf.zeros_initializer(),\n                            name=\'fc7\', _scope=\'fc7\', _reuse=None)\n        # SSD layers\n        with tf.variable_scope(\'additional_layers\') as scope:\n            self._conv8_block = self.ssd_conv_block(256, 2, \'conv8\')\n            self._conv9_block = self.ssd_conv_block(128, 2, \'conv9\')\n            self._conv10_block = self.ssd_conv_block(128, 1, \'conv10\', padding=\'valid\')\n            self._conv11_block = self.ssd_conv_block(128, 1, \'conv11\', padding=\'valid\')\n\n    def l2_normalize(self, x, name):\n        with tf.name_scope(name, ""l2_normalize"", [x]) as name:\n            axis = -1 if self._data_format == \'channels_last\' else 1\n            square_sum = tf.reduce_sum(tf.square(x), axis, keep_dims=True)\n            x_inv_norm = tf.rsqrt(tf.maximum(square_sum, 1e-10))\n            return tf.multiply(x, x_inv_norm, name=name)\n\n    def forward(self, inputs, training=False):\n        # inputs should in BGR\n        feature_layers = []\n        # forward vgg layers\n        for conv in self._conv1_block:\n            inputs = forward_module(conv, inputs, training=training)\n        inputs = self._pool1.apply(inputs)\n        for conv in self._conv2_block:\n            inputs = forward_module(conv, inputs, training=training)\n        inputs = self._pool2.apply(inputs)\n        for conv in self._conv3_block:\n            inputs = forward_module(conv, inputs, training=training)\n        inputs = self._pool3.apply(inputs)\n        for conv in self._conv4_block:\n            inputs = forward_module(conv, inputs, training=training)\n        # conv4_3\n        with tf.variable_scope(\'conv4_3_scale\') as scope:\n            weight_scale = tf.Variable([20.] * 512, trainable=training, name=\'weights\')\n            if self._data_format == \'channels_last\':\n                weight_scale = tf.reshape(weight_scale, [1, 1, 1, -1], name=\'reshape\')\n            else:\n                weight_scale = tf.reshape(weight_scale, [1, -1, 1, 1], name=\'reshape\')\n\n            feature_layers.append(tf.multiply(weight_scale, self.l2_normalize(inputs, name=\'norm\'), name=\'rescale\')\n                                )\n        inputs = self._pool4.apply(inputs)\n        for conv in self._conv5_block:\n            inputs = forward_module(conv, inputs, training=training)\n        inputs = self._pool5.apply(inputs)\n        # forward fc layers\n        inputs = self._conv6.apply(inputs)\n        inputs = self._conv7.apply(inputs)\n        # fc7\n        feature_layers.append(inputs)\n        # forward ssd layers\n        for layer in self._conv8_block:\n            inputs = forward_module(layer, inputs, training=training)\n        # conv8\n        feature_layers.append(inputs)\n        for layer in self._conv9_block:\n            inputs = forward_module(layer, inputs, training=training)\n        # conv9\n        feature_layers.append(inputs)\n        for layer in self._conv10_block:\n            inputs = forward_module(layer, inputs, training=training)\n        # conv10\n        feature_layers.append(inputs)\n        for layer in self._conv11_block:\n            inputs = forward_module(layer, inputs, training=training)\n        # conv11\n        feature_layers.append(inputs)\n\n        return feature_layers\n\n    def conv_block(self, num_blocks, filters, kernel_size, strides, name, reuse=None):\n        with tf.variable_scope(name):\n            conv_blocks = []\n            for ind in range(1, num_blocks + 1):\n                conv_blocks.append(\n                        tf.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=\'same\',\n                            data_format=self._data_format, activation=tf.nn.relu, use_bias=True,\n                            kernel_initializer=self._conv_initializer(),\n                            bias_initializer=tf.zeros_initializer(),\n                            name=\'{}_{}\'.format(name, ind), _scope=\'{}_{}\'.format(name, ind), _reuse=None)\n                    )\n            return conv_blocks\n\n    def ssd_conv_block(self, filters, strides, name, padding=\'same\', reuse=None):\n        with tf.variable_scope(name):\n            conv_blocks = []\n            conv_blocks.append(\n                    tf.layers.Conv2D(filters=filters, kernel_size=1, strides=1, padding=padding,\n                        data_format=self._data_format, activation=tf.nn.relu, use_bias=True,\n                        kernel_initializer=self._conv_initializer(),\n                        bias_initializer=tf.zeros_initializer(),\n                        name=\'{}_1\'.format(name), _scope=\'{}_1\'.format(name), _reuse=None)\n                )\n            conv_blocks.append(\n                    tf.layers.Conv2D(filters=filters * 2, kernel_size=3, strides=strides, padding=padding,\n                        data_format=self._data_format, activation=tf.nn.relu, use_bias=True,\n                        kernel_initializer=self._conv_initializer(),\n                        bias_initializer=tf.zeros_initializer(),\n                        name=\'{}_2\'.format(name), _scope=\'{}_2\'.format(name), _reuse=None)\n                )\n            return conv_blocks\n\n    def ssd_conv_bn_block(self, filters, strides, name, reuse=None):\n        with tf.variable_scope(name):\n            conv_bn_blocks = []\n            conv_bn_blocks.append(\n                    tf.layers.Conv2D(filters=filters, kernel_size=1, strides=1, padding=\'same\',\n                        data_format=self._data_format, activation=None, use_bias=False,\n                        kernel_initializer=self._conv_bn_initializer(),\n                        bias_initializer=None,\n                        name=\'{}_1\'.format(name), _scope=\'{}_1\'.format(name), _reuse=None)\n                )\n            conv_bn_blocks.append(\n                    tf.layers.BatchNormalization(axis=self._bn_axis, momentum=BN_MOMENTUM, epsilon=BN_EPSILON, fused=USE_FUSED_BN,\n                        name=\'{}_bn1\'.format(name), _scope=\'{}_bn1\'.format(name), _reuse=None)\n                )\n            conv_bn_blocks.append(\n                    ReLuLayer(\'{}_relu1\'.format(name), _scope=\'{}_relu1\'.format(name), _reuse=None)\n                )\n            conv_bn_blocks.append(\n                    tf.layers.Conv2D(filters=filters * 2, kernel_size=3, strides=strides, padding=\'same\',\n                        data_format=self._data_format, activation=None, use_bias=False,\n                        kernel_initializer=self._conv_bn_initializer(),\n                        bias_initializer=None,\n                        name=\'{}_2\'.format(name), _scope=\'{}_2\'.format(name), _reuse=None)\n                )\n            conv_bn_blocks.append(\n                    tf.layers.BatchNormalization(axis=self._bn_axis, momentum=BN_MOMENTUM, epsilon=BN_EPSILON, fused=USE_FUSED_BN,\n                        name=\'{}_bn2\'.format(name), _scope=\'{}_bn2\'.format(name), _reuse=None)\n                )\n            conv_bn_blocks.append(\n                    ReLuLayer(\'{}_relu2\'.format(name), _scope=\'{}_relu2\'.format(name), _reuse=None)\n                )\n            return conv_bn_blocks\n\ndef multibox_head(feature_layers, num_classes, num_anchors_depth_per_layer, data_format=\'channels_first\'):\n    with tf.variable_scope(\'multibox_head\'):\n        cls_preds = []\n        loc_preds = []\n        for ind, feat in enumerate(feature_layers):\n            loc_preds.append(tf.layers.conv2d(feat, num_anchors_depth_per_layer[ind] * 4, (3, 3), use_bias=True,\n                        name=\'loc_{}\'.format(ind), strides=(1, 1),\n                        padding=\'same\', data_format=data_format, activation=None,\n                        kernel_initializer=tf.glorot_uniform_initializer(),\n                        bias_initializer=tf.zeros_initializer()))\n            cls_preds.append(tf.layers.conv2d(feat, num_anchors_depth_per_layer[ind] * num_classes, (3, 3), use_bias=True,\n                        name=\'cls_{}\'.format(ind), strides=(1, 1),\n                        padding=\'same\', data_format=data_format, activation=None,\n                        kernel_initializer=tf.glorot_uniform_initializer(),\n                        bias_initializer=tf.zeros_initializer()))\n\n        return loc_preds, cls_preds\n\n\n'"
preprocessing/preprocessing_unittest.py,27,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\nfrom scipy.misc import imread, imsave, imshow, imresize\nimport numpy as np\nimport sys; sys.path.insert(0, ""."")\nfrom utility import draw_toolbox\nimport ssd_preprocessing\n\nslim = tf.contrib.slim\n\ndef save_image_with_bbox(image, labels_, scores_, bboxes_):\n    if not hasattr(save_image_with_bbox, ""counter""):\n        save_image_with_bbox.counter = 0  # it doesn\'t exist yet, so initialize it\n    save_image_with_bbox.counter += 1\n\n    img_to_draw = np.copy(image)\n\n    img_to_draw = draw_toolbox.bboxes_draw_on_img(img_to_draw, labels_, scores_, bboxes_, thickness=2)\n    imsave(os.path.join(\'./debug/{}.jpg\').format(save_image_with_bbox.counter), img_to_draw)\n    return save_image_with_bbox.counter\n\ndef slim_get_split(file_pattern=\'{}_????\'):\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/filename\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/shape\': tf.FixedLenFeature([3], tf.int64),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/label\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/difficult\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/truncated\': tf.VarLenFeature(dtype=tf.int64),\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'filename\': slim.tfexample_decoder.Tensor(\'image/filename\'),\n        \'shape\': slim.tfexample_decoder.Tensor(\'image/shape\'),\n        \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n        \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/bbox/label\'),\n        \'object/difficult\': slim.tfexample_decoder.Tensor(\'image/object/bbox/difficult\'),\n        \'object/truncated\': slim.tfexample_decoder.Tensor(\'image/object/bbox/truncated\'),\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n    dataset = slim.dataset.Dataset(\n                data_sources=file_pattern,\n                reader=tf.TFRecordReader,\n                decoder=decoder,\n                num_samples=100,\n                items_to_descriptions=None,\n                num_classes=21,\n                labels_to_names=None)\n\n    with tf.name_scope(\'dataset_data_provider\'):\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n                    dataset,\n                    num_readers=2,\n                    common_queue_capacity=32,\n                    common_queue_min=8,\n                    shuffle=True,\n                    num_epochs=1)\n\n    [org_image, filename, shape, glabels_raw, gbboxes_raw, isdifficult] = provider.get([\'image\', \'filename\', \'shape\',\n                                                                         \'object/label\',\n                                                                         \'object/bbox\',\n                                                                         \'object/difficult\'])\n    image, glabels, gbboxes = ssd_preprocessing.preprocess_image(org_image, glabels_raw, gbboxes_raw, [300, 300], is_training=True, data_format=\'channels_first\', output_rgb=True)\n\n    image = tf.transpose(image, perm=(1, 2, 0))\n    save_image_op = tf.py_func(save_image_with_bbox,\n                            [ssd_preprocessing.unwhiten_image(image),\n                            tf.clip_by_value(glabels, 0, tf.int64.max),\n                            tf.ones_like(glabels),\n                            gbboxes],\n                            tf.int64, stateful=True)\n    return save_image_op\n\nif __name__ == \'__main__\':\n    save_image_op = slim_get_split(\'/media/rs/7A0EE8880EE83EAF/Detections/SSD/dataset/tfrecords/*\')\n    # Create the graph, etc.\n    init_op = tf.group([tf.local_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer()])\n\n    # Create a session for running operations in the Graph.\n    sess = tf.Session()\n    # Initialize the variables (like the epoch counter).\n    sess.run(init_op)\n\n    # Start input enqueue threads.\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    try:\n        while not coord.should_stop():\n            # Run training steps or whatever\n            print(sess.run(save_image_op))\n\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # When done, ask the threads to stop.\n        coord.request_stop()\n\n    # Wait for threads to finish.\n    coord.join(threads)\n    sess.close()\n'"
preprocessing/ssd_preprocessing.py,150,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\ndef _ImageDimensions(image, rank = 3):\n  """"""Returns the dimensions of an image tensor.\n\n  Args:\n    image: A rank-D Tensor. For 3-D  of shape: `[height, width, channels]`.\n    rank: The expected rank of the image\n\n  Returns:\n    A list of corresponding to the dimensions of the\n    input image.  Dimensions that are statically known are python integers,\n    otherwise they are integer scalar tensors.\n  """"""\n  if image.get_shape().is_fully_defined():\n    return image.get_shape().as_list()\n  else:\n    static_shape = image.get_shape().with_rank(rank).as_list()\n    dynamic_shape = tf.unstack(tf.shape(image), rank)\n    return [s if s is not None else d\n            for s, d in zip(static_shape, dynamic_shape)]\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\ndef ssd_random_sample_patch(image, labels, bboxes, ratio_list=[0.1, 0.3, 0.5, 0.7, 0.9, 1.], name=None):\n  \'\'\'ssd_random_sample_patch.\n  select one min_iou\n  sample _width and _height from [0-width] and [0-height]\n  check if the aspect ratio between 0.5-2.\n  select left_top point from (width - _width, height - _height)\n  check if this bbox has a min_iou with all ground_truth bboxes\n  keep ground_truth those center is in this sampled patch, if none then try again\n  \'\'\'\n  def sample_width_height(width, height):\n    with tf.name_scope(\'sample_width_height\'):\n      index = 0\n      max_attempt = 10\n      sampled_width, sampled_height = width, height\n\n      def condition(index, sampled_width, sampled_height, width, height):\n        return tf.logical_or(tf.logical_and(tf.logical_or(tf.greater(sampled_width, sampled_height * 2),\n                                                        tf.greater(sampled_height, sampled_width * 2)),\n                                            tf.less(index, max_attempt)),\n                            tf.less(index, 1))\n\n      def body(index, sampled_width, sampled_height, width, height):\n        sampled_width = tf.random_uniform([1], minval=0.3, maxval=0.999, dtype=tf.float32)[0] * width\n        sampled_height = tf.random_uniform([1], minval=0.3, maxval=0.999, dtype=tf.float32)[0] *height\n\n        return index+1, sampled_width, sampled_height, width, height\n\n      [index, sampled_width, sampled_height, _, _] = tf.while_loop(condition, body,\n                                         [index, sampled_width, sampled_height, width, height], parallel_iterations=4, back_prop=False, swap_memory=True)\n\n      return tf.cast(sampled_width, tf.int32), tf.cast(sampled_height, tf.int32)\n\n  def jaccard_with_anchors(roi, bboxes):\n    with tf.name_scope(\'jaccard_with_anchors\'):\n      int_ymin = tf.maximum(roi[0], bboxes[:, 0])\n      int_xmin = tf.maximum(roi[1], bboxes[:, 1])\n      int_ymax = tf.minimum(roi[2], bboxes[:, 2])\n      int_xmax = tf.minimum(roi[3], bboxes[:, 3])\n      h = tf.maximum(int_ymax - int_ymin, 0.)\n      w = tf.maximum(int_xmax - int_xmin, 0.)\n      inter_vol = h * w\n      union_vol = (roi[3] - roi[1]) * (roi[2] - roi[0]) + ((bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1]) - inter_vol)\n      jaccard = tf.div(inter_vol, union_vol)\n      return jaccard\n\n  def areas(bboxes):\n    with tf.name_scope(\'bboxes_areas\'):\n      vol = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n      return vol\n\n  def check_roi_center(width, height, labels, bboxes):\n    with tf.name_scope(\'check_roi_center\'):\n      index = 0\n      max_attempt = 20\n      roi = [0., 0., 0., 0.]\n      float_width = tf.cast(width, tf.float32)\n      float_height = tf.cast(height, tf.float32)\n      mask = tf.cast(tf.zeros_like(labels, dtype=tf.uint8), tf.bool)\n      center_x, center_y = (bboxes[:, 1] + bboxes[:, 3]) / 2, (bboxes[:, 0] + bboxes[:, 2]) / 2\n\n      def condition(index, roi, mask):\n        return tf.logical_or(tf.logical_and(tf.reduce_sum(tf.cast(mask, tf.int32)) < 1,\n                                          tf.less(index, max_attempt)),\n                            tf.less(index, 1))\n\n      def body(index, roi, mask):\n        sampled_width, sampled_height = sample_width_height(float_width, float_height)\n\n        x = tf.random_uniform([], minval=0, maxval=width - sampled_width, dtype=tf.int32)\n        y = tf.random_uniform([], minval=0, maxval=height - sampled_height, dtype=tf.int32)\n\n        roi = [tf.cast(y, tf.float32) / float_height,\n              tf.cast(x, tf.float32) / float_width,\n              tf.cast(y + sampled_height, tf.float32) / float_height,\n              tf.cast(x + sampled_width, tf.float32) / float_width]\n\n        mask_min = tf.logical_and(tf.greater(center_y, roi[0]), tf.greater(center_x, roi[1]))\n        mask_max = tf.logical_and(tf.less(center_y, roi[2]), tf.less(center_x, roi[3]))\n        mask = tf.logical_and(mask_min, mask_max)\n\n        return index + 1, roi, mask\n\n      [index, roi, mask] = tf.while_loop(condition, body, [index, roi, mask], parallel_iterations=10, back_prop=False, swap_memory=True)\n\n      mask_labels = tf.boolean_mask(labels, mask)\n      mask_bboxes = tf.boolean_mask(bboxes, mask)\n\n      return roi, mask_labels, mask_bboxes\n  def check_roi_overlap(width, height, labels, bboxes, min_iou):\n    with tf.name_scope(\'check_roi_overlap\'):\n      index = 0\n      max_attempt = 50\n      roi = [0., 0., 1., 1.]\n      mask_labels = labels\n      mask_bboxes = bboxes\n\n      def condition(index, roi, mask_labels, mask_bboxes):\n        return tf.logical_or(tf.logical_or(tf.logical_and(tf.reduce_sum(tf.cast(jaccard_with_anchors(roi, mask_bboxes) < min_iou, tf.int32)) > 0,\n                                                        tf.less(index, max_attempt)),\n                                          tf.less(index, 1)),\n                            tf.less(tf.shape(mask_labels)[0], 1))\n\n      def body(index, roi, mask_labels, mask_bboxes):\n        roi, mask_labels, mask_bboxes = check_roi_center(width, height, labels, bboxes)\n        return index+1, roi, mask_labels, mask_bboxes\n\n      [index, roi, mask_labels, mask_bboxes] = tf.while_loop(condition, body, [index, roi, mask_labels, mask_bboxes], parallel_iterations=16, back_prop=False, swap_memory=True)\n\n      return tf.cond(tf.greater(tf.shape(mask_labels)[0], 0),\n                  lambda : (tf.cast([roi[0] * tf.cast(height, tf.float32),\n                            roi[1] * tf.cast(width, tf.float32),\n                            (roi[2] - roi[0]) * tf.cast(height, tf.float32),\n                            (roi[3] - roi[1]) * tf.cast(width, tf.float32)], tf.int32), mask_labels, mask_bboxes),\n                  lambda : (tf.cast([0, 0, height, width], tf.int32), labels, bboxes))\n\n\n  def sample_patch(image, labels, bboxes, min_iou):\n    with tf.name_scope(\'sample_patch\'):\n      height, width, depth = _ImageDimensions(image, rank=3)\n\n      roi_slice_range, mask_labels, mask_bboxes = check_roi_overlap(width, height, labels, bboxes, min_iou)\n\n      scale = tf.cast(tf.stack([height, width, height, width]), mask_bboxes.dtype)\n      mask_bboxes = mask_bboxes * scale\n\n      # Add offset.\n      offset = tf.cast(tf.stack([roi_slice_range[0], roi_slice_range[1], roi_slice_range[0], roi_slice_range[1]]), mask_bboxes.dtype)\n      mask_bboxes = mask_bboxes - offset\n\n      cliped_ymin = tf.maximum(0., mask_bboxes[:, 0])\n      cliped_xmin = tf.maximum(0., mask_bboxes[:, 1])\n      cliped_ymax = tf.minimum(tf.cast(roi_slice_range[2], tf.float32), mask_bboxes[:, 2])\n      cliped_xmax = tf.minimum(tf.cast(roi_slice_range[3], tf.float32), mask_bboxes[:, 3])\n\n      mask_bboxes = tf.stack([cliped_ymin, cliped_xmin, cliped_ymax, cliped_xmax], axis=-1)\n      # Rescale to target dimension.\n      scale = tf.cast(tf.stack([roi_slice_range[2], roi_slice_range[3],\n                                roi_slice_range[2], roi_slice_range[3]]), mask_bboxes.dtype)\n\n      return tf.cond(tf.logical_or(tf.less(roi_slice_range[2], 1), tf.less(roi_slice_range[3], 1)),\n                  lambda: (image, labels, bboxes),\n                  lambda: (tf.slice(image, [roi_slice_range[0], roi_slice_range[1], 0], [roi_slice_range[2], roi_slice_range[3], -1]),\n                                  mask_labels, mask_bboxes / scale))\n\n  with tf.name_scope(\'ssd_random_sample_patch\'):\n    image = tf.convert_to_tensor(image, name=\'image\')\n\n    min_iou_list = tf.convert_to_tensor(ratio_list)\n    samples_min_iou = tf.multinomial(tf.log([[1. / len(ratio_list)] * len(ratio_list)]), 1)\n\n    sampled_min_iou = min_iou_list[tf.cast(samples_min_iou[0][0], tf.int32)]\n\n    return tf.cond(tf.less(sampled_min_iou, 1.), lambda: sample_patch(image, labels, bboxes, sampled_min_iou), lambda: (image, labels, bboxes))\n\ndef ssd_random_expand(image, bboxes, ratio=2., name=None):\n  with tf.name_scope(\'ssd_random_expand\'):\n    image = tf.convert_to_tensor(image, name=\'image\')\n    if image.get_shape().ndims != 3:\n      raise ValueError(\'\\\'image\\\' must have 3 dimensions.\')\n\n    height, width, depth = _ImageDimensions(image, rank=3)\n\n    float_height, float_width = tf.to_float(height), tf.to_float(width)\n\n    canvas_width, canvas_height = tf.to_int32(float_width * ratio), tf.to_int32(float_height * ratio)\n\n    mean_color_of_image = [_R_MEAN/255., _G_MEAN/255., _B_MEAN/255.]#tf.reduce_mean(tf.reshape(image, [-1, 3]), 0)\n\n    x = tf.random_uniform([], minval=0, maxval=canvas_width - width, dtype=tf.int32)\n    y = tf.random_uniform([], minval=0, maxval=canvas_height - height, dtype=tf.int32)\n\n    paddings = tf.convert_to_tensor([[y, canvas_height - height - y], [x, canvas_width - width - x]])\n\n    big_canvas = tf.stack([tf.pad(image[:, :, 0], paddings, ""CONSTANT"", constant_values = mean_color_of_image[0]),\n                          tf.pad(image[:, :, 1], paddings, ""CONSTANT"", constant_values = mean_color_of_image[1]),\n                          tf.pad(image[:, :, 2], paddings, ""CONSTANT"", constant_values = mean_color_of_image[2])], axis=-1)\n\n    scale = tf.cast(tf.stack([height, width, height, width]), bboxes.dtype)\n    absolute_bboxes = bboxes * scale + tf.cast(tf.stack([y, x, y, x]), bboxes.dtype)\n\n    return big_canvas, absolute_bboxes / tf.cast(tf.stack([canvas_height, canvas_width, canvas_height, canvas_width]), bboxes.dtype)\n\n# def ssd_random_sample_patch_wrapper(image, labels, bboxes):\n#   with tf.name_scope(\'ssd_random_sample_patch_wrapper\'):\n#     orgi_image, orgi_labels, orgi_bboxes = image, labels, bboxes\n#     def check_bboxes(bboxes):\n#       areas = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n#       return tf.logical_and(tf.logical_and(areas < 0.9, areas > 0.001),\n#                             tf.logical_and((bboxes[:, 3] - bboxes[:, 1]) > 0.025, (bboxes[:, 2] - bboxes[:, 0]) > 0.025))\n\n#     index = 0\n#     max_attempt = 3\n#     def condition(index, image, labels, bboxes):\n#       return tf.logical_or(tf.logical_and(tf.reduce_sum(tf.cast(check_bboxes(bboxes), tf.int64)) < 1, tf.less(index, max_attempt)), tf.less(index, 1))\n\n#     def body(index, image, labels, bboxes):\n#       image, bboxes = tf.cond(tf.random_uniform([], minval=0., maxval=1., dtype=tf.float32) < 0.5,\n#                       lambda: (image, bboxes),\n#                       lambda: ssd_random_expand(image, bboxes, tf.random_uniform([1], minval=1.1, maxval=4., dtype=tf.float32)[0]))\n#       # Distort image and bounding boxes.\n#       random_sample_image, labels, bboxes = ssd_random_sample_patch(image, labels, bboxes, ratio_list=[-0.1, 0.1, 0.3, 0.5, 0.7, 0.9, 1.])\n#       random_sample_image.set_shape([None, None, 3])\n#       return index+1, random_sample_image, labels, bboxes\n\n#     [index, image, labels, bboxes] = tf.while_loop(condition, body, [index, orgi_image, orgi_labels, orgi_bboxes], parallel_iterations=4, back_prop=False, swap_memory=True)\n\n#     valid_mask = check_bboxes(bboxes)\n#     labels, bboxes = tf.boolean_mask(labels, valid_mask), tf.boolean_mask(bboxes, valid_mask)\n#     return tf.cond(tf.less(index, max_attempt),\n#                 lambda : (image, labels, bboxes),\n#                 lambda : (orgi_image, orgi_labels, orgi_bboxes))\n\ndef ssd_random_sample_patch_wrapper(image, labels, bboxes):\n  with tf.name_scope(\'ssd_random_sample_patch_wrapper\'):\n    orgi_image, orgi_labels, orgi_bboxes = image, labels, bboxes\n    def check_bboxes(bboxes):\n      areas = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n      return tf.logical_and(tf.logical_and(areas < 0.9, areas > 0.001),\n                            tf.logical_and((bboxes[:, 3] - bboxes[:, 1]) > 0.025, (bboxes[:, 2] - bboxes[:, 0]) > 0.025))\n\n    index = 0\n    max_attempt = 3\n    def condition(index, image, labels, bboxes, orgi_image, orgi_labels, orgi_bboxes):\n      return tf.logical_or(tf.logical_and(tf.reduce_sum(tf.cast(check_bboxes(bboxes), tf.int64)) < 1, tf.less(index, max_attempt)), tf.less(index, 1))\n\n    def body(index, image, labels, bboxes, orgi_image, orgi_labels, orgi_bboxes):\n      image, bboxes = tf.cond(tf.random_uniform([], minval=0., maxval=1., dtype=tf.float32) < 0.5,\n                      lambda: (orgi_image, orgi_bboxes),\n                      lambda: ssd_random_expand(orgi_image, orgi_bboxes, tf.random_uniform([1], minval=1.1, maxval=4., dtype=tf.float32)[0]))\n      # Distort image and bounding boxes.\n      random_sample_image, labels, bboxes = ssd_random_sample_patch(image, orgi_labels, bboxes, ratio_list=[-0.1, 0.1, 0.3, 0.5, 0.7, 0.9, 1.])\n      random_sample_image.set_shape([None, None, 3])\n      return index+1, random_sample_image, labels, bboxes, orgi_image, orgi_labels, orgi_bboxes\n\n    [index, image, labels, bboxes, orgi_image, orgi_labels, orgi_bboxes] = tf.while_loop(condition, body, [index,  image, labels, bboxes, orgi_image, orgi_labels, orgi_bboxes], parallel_iterations=4, back_prop=False, swap_memory=True)\n\n    valid_mask = check_bboxes(bboxes)\n    labels, bboxes = tf.boolean_mask(labels, valid_mask), tf.boolean_mask(bboxes, valid_mask)\n    return tf.cond(tf.less(index, max_attempt),\n                lambda : (image, labels, bboxes),\n                lambda : (orgi_image, orgi_labels, orgi_bboxes))\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\ndef unwhiten_image(image):\n  means=[_R_MEAN, _G_MEAN, _B_MEAN]\n  num_channels = image.get_shape().as_list()[-1]\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] += means[i]\n  return tf.concat(axis=2, values=channels)\n\ndef random_flip_left_right(image, bboxes):\n  with tf.name_scope(\'random_flip_left_right\'):\n    uniform_random = tf.random_uniform([], 0, 1.0)\n    mirror_cond = tf.less(uniform_random, .5)\n    # Flip image.\n    result = tf.cond(mirror_cond, lambda: tf.image.flip_left_right(image), lambda: image)\n    # Flip bboxes.\n    mirror_bboxes = tf.stack([bboxes[:, 0], 1 - bboxes[:, 3],\n                              bboxes[:, 2], 1 - bboxes[:, 1]], axis=-1)\n    bboxes = tf.cond(mirror_cond, lambda: mirror_bboxes, lambda: bboxes)\n    return result, bboxes\n\ndef preprocess_for_train(image, labels, bboxes, out_shape, data_format=\'channels_first\', scope=\'ssd_preprocessing_train\', output_rgb=True):\n  """"""Preprocesses the given image for training.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    labels: A `Tensor` containing all labels for all bboxes of this image.\n    bboxes: A `Tensor` containing all bboxes of this image, in range [0., 1.] with shape [num_bboxes, 4].\n    out_shape: The height and width of the image after preprocessing.\n    data_format: The data_format of the desired output image.\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'ssd_preprocessing_train\', [image, labels, bboxes]):\n    if image.get_shape().ndims != 3:\n      raise ValueError(\'Input must be of size [height, width, C>0]\')\n    # Convert to float scaled [0, 1].\n    orig_dtype = image.dtype\n    if orig_dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distort_image = apply_with_random_selector(image,\n                                          lambda x, ordering: distort_color(x, ordering, True),\n                                          num_cases=4)\n\n    random_sample_image, labels, bboxes = ssd_random_sample_patch_wrapper(distort_image, labels, bboxes)\n    # image, bboxes = tf.cond(tf.random_uniform([1], minval=0., maxval=1., dtype=tf.float32)[0] < 0.25,\n    #                     lambda: (image, bboxes),\n    #                     lambda: ssd_random_expand(image, bboxes, tf.random_uniform([1], minval=2, maxval=4, dtype=tf.int32)[0]))\n\n    # # Distort image and bounding boxes.\n    # random_sample_image, labels, bboxes = ssd_random_sample_patch(image, labels, bboxes, ratio_list=[0.1, 0.3, 0.5, 0.7, 0.9, 1.])\n\n    # Randomly flip the image horizontally.\n    random_sample_flip_image, bboxes = random_flip_left_right(random_sample_image, bboxes)\n    # Rescale to VGG input scale.\n    random_sample_flip_resized_image = tf.image.resize_images(random_sample_flip_image, out_shape, method=tf.image.ResizeMethod.BILINEAR, align_corners=False)\n    random_sample_flip_resized_image.set_shape([None, None, 3])\n\n    final_image = tf.to_float(tf.image.convert_image_dtype(random_sample_flip_resized_image, orig_dtype, saturate=True))\n    final_image = _mean_image_subtraction(final_image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n    final_image.set_shape(out_shape + [3])\n    if not output_rgb:\n      image_channels = tf.unstack(final_image, axis=-1, name=\'split_rgb\')\n      final_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=-1, name=\'merge_bgr\')\n    if data_format == \'channels_first\':\n      final_image = tf.transpose(final_image, perm=(2, 0, 1))\n    return final_image, labels, bboxes\n\ndef preprocess_for_eval(image, out_shape, data_format=\'channels_first\', scope=\'ssd_preprocessing_eval\', output_rgb=True):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    out_shape: The height and width of the image after preprocessing.\n    data_format: The data_format of the desired output image.\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'ssd_preprocessing_eval\', [image]):\n    image = tf.to_float(image)\n    image = tf.image.resize_images(image, out_shape, method=tf.image.ResizeMethod.BILINEAR, align_corners=False)\n    image.set_shape(out_shape + [3])\n\n    image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n    if not output_rgb:\n      image_channels = tf.unstack(image, axis=-1, name=\'split_rgb\')\n      image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=-1, name=\'merge_bgr\')\n    # Image data format.\n    if data_format == \'channels_first\':\n      image = tf.transpose(image, perm=(2, 0, 1))\n    return image\n\ndef preprocess_image(image, labels, bboxes, out_shape, is_training=False, data_format=\'channels_first\', output_rgb=True):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    labels: A `Tensor` containing all labels for all bboxes of this image.\n    bboxes: A `Tensor` containing all bboxes of this image, in range [0., 1.] with shape [num_bboxes, 4].\n    out_shape: The height and width of the image after preprocessing.\n    is_training: Wether we are in training phase.\n    data_format: The data_format of the desired output image.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, labels, bboxes, out_shape, data_format=data_format, output_rgb=output_rgb)\n  else:\n    return preprocess_for_eval(image, out_shape, data_format=data_format, output_rgb=output_rgb)\n'"
utility/anchor_manipulator.py,92,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport math\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.contrib.image.python.ops import image_ops\n\ndef areas(gt_bboxes):\n    with tf.name_scope(\'bboxes_areas\', [gt_bboxes]):\n        ymin, xmin, ymax, xmax = tf.split(gt_bboxes, 4, axis=1)\n        return (xmax - xmin) * (ymax - ymin)\n\ndef intersection(gt_bboxes, default_bboxes):\n    with tf.name_scope(\'bboxes_intersection\', [gt_bboxes, default_bboxes]):\n        # num_anchors x 1\n        ymin, xmin, ymax, xmax = tf.split(gt_bboxes, 4, axis=1)\n        # 1 x num_anchors\n        gt_ymin, gt_xmin, gt_ymax, gt_xmax = [tf.transpose(b, perm=[1, 0]) for b in tf.split(default_bboxes, 4, axis=1)]\n        # broadcast here to generate the full matrix\n        int_ymin = tf.maximum(ymin, gt_ymin)\n        int_xmin = tf.maximum(xmin, gt_xmin)\n        int_ymax = tf.minimum(ymax, gt_ymax)\n        int_xmax = tf.minimum(xmax, gt_xmax)\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n\n        return h * w\ndef iou_matrix(gt_bboxes, default_bboxes):\n    with tf.name_scope(\'iou_matrix\', [gt_bboxes, default_bboxes]):\n        inter_vol = intersection(gt_bboxes, default_bboxes)\n        # broadcast\n        union_vol = areas(gt_bboxes) + tf.transpose(areas(default_bboxes), perm=[1, 0]) - inter_vol\n\n        return tf.where(tf.equal(union_vol, 0.0),\n                        tf.zeros_like(inter_vol), tf.truediv(inter_vol, union_vol))\n\ndef do_dual_max_match(overlap_matrix, low_thres, high_thres, ignore_between=True, gt_max_first=True):\n    \'\'\'\n    overlap_matrix: num_gt * num_anchors\n    \'\'\'\n    with tf.name_scope(\'dual_max_match\', [overlap_matrix]):\n        # first match from anchors\' side\n        anchors_to_gt = tf.argmax(overlap_matrix, axis=0)\n        # the matching degree\n        match_values = tf.reduce_max(overlap_matrix, axis=0)\n\n        #positive_mask = tf.greater(match_values, high_thres)\n        less_mask = tf.less(match_values, low_thres)\n        between_mask = tf.logical_and(tf.less(match_values, high_thres), tf.greater_equal(match_values, low_thres))\n        negative_mask = less_mask if ignore_between else between_mask\n        ignore_mask = between_mask if ignore_between else less_mask\n        # fill all negative positions with -1, all ignore positions is -2\n        match_indices = tf.where(negative_mask, -1 * tf.ones_like(anchors_to_gt), anchors_to_gt)\n        match_indices = tf.where(ignore_mask, -2 * tf.ones_like(match_indices), match_indices)\n\n        # negtive values has no effect in tf.one_hot, that means all zeros along that axis\n        # so all positive match positions in anchors_to_gt_mask is 1, all others are 0\n        anchors_to_gt_mask = tf.one_hot(tf.clip_by_value(match_indices, -1, tf.cast(tf.shape(overlap_matrix)[0], tf.int64)),\n                                        tf.shape(overlap_matrix)[0], on_value=1, off_value=0, axis=0, dtype=tf.int32)\n        # match from ground truth\'s side\n        gt_to_anchors = tf.argmax(overlap_matrix, axis=1)\n\n        if gt_max_first:\n            # the max match from ground truth\'s side has higher priority\n            left_gt_to_anchors_mask = tf.one_hot(gt_to_anchors, tf.shape(overlap_matrix)[1], on_value=1, off_value=0, axis=1, dtype=tf.int32)\n        else:\n            # the max match from anchors\' side has higher priority\n            # use match result from ground truth\'s side only when the the matching degree from anchors\' side is lower than position threshold\n            left_gt_to_anchors_mask = tf.cast(tf.logical_and(tf.reduce_max(anchors_to_gt_mask, axis=1, keep_dims=True) < 1,\n                                                            tf.one_hot(gt_to_anchors, tf.shape(overlap_matrix)[1],\n                                                                        on_value=True, off_value=False, axis=1, dtype=tf.bool)\n                                                            ), tf.int64)\n        # can not use left_gt_to_anchors_mask here, because there are many ground truthes match to one anchor, we should pick the highest one even when we are merging matching from ground truth side\n        left_gt_to_anchors_scores = overlap_matrix * tf.to_float(left_gt_to_anchors_mask)\n        # merge matching results from ground truth\'s side with the original matching results from anchors\' side\n        # then select all the overlap score of those matching pairs\n        selected_scores = tf.gather_nd(overlap_matrix,  tf.stack([tf.where(tf.reduce_max(left_gt_to_anchors_mask, axis=0) > 0,\n                                                                            tf.argmax(left_gt_to_anchors_scores, axis=0),\n                                                                            anchors_to_gt),\n                                                                    tf.range(tf.cast(tf.shape(overlap_matrix)[1], tf.int64))], axis=1))\n        # return the matching results for both foreground anchors and background anchors, also with overlap scores\n        return tf.where(tf.reduce_max(left_gt_to_anchors_mask, axis=0) > 0,\n                        tf.argmax(left_gt_to_anchors_scores, axis=0),\n                        match_indices), selected_scores\n\n# def save_anchors(bboxes, labels, anchors_point):\n#     if not hasattr(save_image_with_bbox, ""counter""):\n#         save_image_with_bbox.counter = 0  # it doesn\'t exist yet, so initialize it\n#     save_image_with_bbox.counter += 1\n\n#     np.save(\'./debug/bboxes_{}.npy\'.format(save_image_with_bbox.counter), np.copy(bboxes))\n#     np.save(\'./debug/labels_{}.npy\'.format(save_image_with_bbox.counter), np.copy(labels))\n#     np.save(\'./debug/anchors_{}.npy\'.format(save_image_with_bbox.counter), np.copy(anchors_point))\n#     return save_image_with_bbox.counter\n\nclass AnchorEncoder(object):\n    def __init__(self, allowed_borders, positive_threshold, ignore_threshold, prior_scaling, clip=False):\n        super(AnchorEncoder, self).__init__()\n        self._all_anchors = None\n        self._allowed_borders = allowed_borders\n        self._positive_threshold = positive_threshold\n        self._ignore_threshold = ignore_threshold\n        self._prior_scaling = prior_scaling\n        self._clip = clip\n\n    def center2point(self, center_y, center_x, height, width):\n        return center_y - height / 2., center_x - width / 2., center_y + height / 2., center_x + width / 2.,\n\n    def point2center(self, ymin, xmin, ymax, xmax):\n        height, width = (ymax - ymin), (xmax - xmin)\n        return ymin + height / 2., xmin + width / 2., height, width\n\n    def encode_all_anchors(self, labels, bboxes, all_anchors, all_num_anchors_depth, all_num_anchors_spatial, debug=False):\n        # y, x, h, w are all in range [0, 1] relative to the original image size\n        # shape info:\n        # y_on_image, x_on_image: layers_shapes[0] * layers_shapes[1]\n        # h_on_image, w_on_image: num_anchors\n        assert (len(all_num_anchors_depth)==len(all_num_anchors_spatial)) and (len(all_num_anchors_depth)==len(all_anchors)), \'inconsist num layers for anchors.\'\n        with tf.name_scope(\'encode_all_anchors\'):\n            num_layers = len(all_num_anchors_depth)\n            list_anchors_ymin = []\n            list_anchors_xmin = []\n            list_anchors_ymax = []\n            list_anchors_xmax = []\n            tiled_allowed_borders = []\n            for ind, anchor in enumerate(all_anchors):\n                anchors_ymin_, anchors_xmin_, anchors_ymax_, anchors_xmax_ = self.center2point(anchor[0], anchor[1], anchor[2], anchor[3])\n\n                list_anchors_ymin.append(tf.reshape(anchors_ymin_, [-1]))\n                list_anchors_xmin.append(tf.reshape(anchors_xmin_, [-1]))\n                list_anchors_ymax.append(tf.reshape(anchors_ymax_, [-1]))\n                list_anchors_xmax.append(tf.reshape(anchors_xmax_, [-1]))\n\n                tiled_allowed_borders.extend([self._allowed_borders[ind]] * all_num_anchors_depth[ind] * all_num_anchors_spatial[ind])\n\n            anchors_ymin = tf.concat(list_anchors_ymin, 0, name=\'concat_ymin\')\n            anchors_xmin = tf.concat(list_anchors_xmin, 0, name=\'concat_xmin\')\n            anchors_ymax = tf.concat(list_anchors_ymax, 0, name=\'concat_ymax\')\n            anchors_xmax = tf.concat(list_anchors_xmax, 0, name=\'concat_xmax\')\n\n            if self._clip:\n                anchors_ymin = tf.clip_by_value(anchors_ymin, 0., 1.)\n                anchors_xmin = tf.clip_by_value(anchors_xmin, 0., 1.)\n                anchors_ymax = tf.clip_by_value(anchors_ymax, 0., 1.)\n                anchors_xmax = tf.clip_by_value(anchors_xmax, 0., 1.)\n\n            anchor_allowed_borders = tf.stack(tiled_allowed_borders, 0, name=\'concat_allowed_borders\')\n\n            inside_mask = tf.logical_and(tf.logical_and(anchors_ymin > -anchor_allowed_borders * 1.,\n                                                        anchors_xmin > -anchor_allowed_borders * 1.),\n                                        tf.logical_and(anchors_ymax < (1. + anchor_allowed_borders * 1.),\n                                                        anchors_xmax < (1. + anchor_allowed_borders * 1.)))\n\n            anchors_point = tf.stack([anchors_ymin, anchors_xmin, anchors_ymax, anchors_xmax], axis=-1)\n\n            # save_anchors_op = tf.py_func(save_anchors,\n            #                 [bboxes,\n            #                 labels,\n            #                 anchors_point],\n            #                 tf.int64, stateful=True)\n\n            # with tf.control_dependencies([save_anchors_op]):\n            overlap_matrix = iou_matrix(bboxes, anchors_point) * tf.cast(tf.expand_dims(inside_mask, 0), tf.float32)\n            matched_gt, gt_scores = do_dual_max_match(overlap_matrix, self._ignore_threshold, self._positive_threshold)\n            # get all positive matching positions\n            matched_gt_mask = matched_gt > -1\n            matched_indices = tf.clip_by_value(matched_gt, 0, tf.int64.max)\n            # the labels here maybe chaos at those non-positive positions\n            gt_labels = tf.gather(labels, matched_indices)\n            # filter the invalid labels\n            gt_labels = gt_labels * tf.cast(matched_gt_mask, tf.int64)\n            # set those ignored positions to -1\n            gt_labels = gt_labels + (-1 * tf.cast(matched_gt < -1, tf.int64))\n\n            gt_ymin, gt_xmin, gt_ymax, gt_xmax = tf.unstack(tf.gather(bboxes, matched_indices), 4, axis=-1)\n\n            # transform to center / size.\n            gt_cy, gt_cx, gt_h, gt_w = self.point2center(gt_ymin, gt_xmin, gt_ymax, gt_xmax)\n            anchor_cy, anchor_cx, anchor_h, anchor_w = self.point2center(anchors_ymin, anchors_xmin, anchors_ymax, anchors_xmax)\n            # encode features.\n            # the prior_scaling (in fact is 5 and 10) is use for balance the regression loss of center and with(or height)\n            gt_cy = (gt_cy - anchor_cy) / anchor_h / self._prior_scaling[0]\n            gt_cx = (gt_cx - anchor_cx) / anchor_w / self._prior_scaling[1]\n            gt_h = tf.log(gt_h / anchor_h) / self._prior_scaling[2]\n            gt_w = tf.log(gt_w / anchor_w) / self._prior_scaling[3]\n            # now gt_localizations is our regression object, but also maybe chaos at those non-positive positions\n            if debug:\n                gt_targets = tf.stack([anchors_ymin, anchors_xmin, anchors_ymax, anchors_xmax], axis=-1)\n            else:\n                gt_targets = tf.stack([gt_cy, gt_cx, gt_h, gt_w], axis=-1)\n            # set all targets of non-positive positions to 0\n            gt_targets = tf.expand_dims(tf.cast(matched_gt_mask, tf.float32), -1) * gt_targets\n            self._all_anchors = (anchor_cy, anchor_cx, anchor_h, anchor_w)\n            return gt_targets, gt_labels, gt_scores\n\n    # return a list, of which each is:\n    #   shape: [feature_h, feature_w, num_anchors, 4]\n    #   order: ymin, xmin, ymax, xmax\n    def decode_all_anchors(self, pred_location, num_anchors_per_layer):\n        assert self._all_anchors is not None, \'no anchors to decode.\'\n        with tf.name_scope(\'decode_all_anchors\', [pred_location]):\n            anchor_cy, anchor_cx, anchor_h, anchor_w = self._all_anchors\n\n            pred_h = tf.exp(pred_location[:, -2] * self._prior_scaling[2]) * anchor_h\n            pred_w = tf.exp(pred_location[:, -1] * self._prior_scaling[3]) * anchor_w\n            pred_cy = pred_location[:, 0] * self._prior_scaling[0] * anchor_h + anchor_cy\n            pred_cx = pred_location[:, 1] * self._prior_scaling[1] * anchor_w + anchor_cx\n\n            return tf.split(tf.stack(self.center2point(pred_cy, pred_cx, pred_h, pred_w), axis=-1), num_anchors_per_layer, axis=0)\n\n    def ext_decode_all_anchors(self, pred_location, all_anchors, all_num_anchors_depth, all_num_anchors_spatial):\n        assert (len(all_num_anchors_depth)==len(all_num_anchors_spatial)) and (len(all_num_anchors_depth)==len(all_anchors)), \'inconsist num layers for anchors.\'\n        with tf.name_scope(\'ext_decode_all_anchors\', [pred_location]):\n            num_anchors_per_layer = []\n            for ind in range(len(all_anchors)):\n                num_anchors_per_layer.append(all_num_anchors_depth[ind] * all_num_anchors_spatial[ind])\n\n            num_layers = len(all_num_anchors_depth)\n            list_anchors_ymin = []\n            list_anchors_xmin = []\n            list_anchors_ymax = []\n            list_anchors_xmax = []\n            tiled_allowed_borders = []\n            for ind, anchor in enumerate(all_anchors):\n                anchors_ymin_, anchors_xmin_, anchors_ymax_, anchors_xmax_ = self.center2point(anchor[0], anchor[1], anchor[2], anchor[3])\n\n                list_anchors_ymin.append(tf.reshape(anchors_ymin_, [-1]))\n                list_anchors_xmin.append(tf.reshape(anchors_xmin_, [-1]))\n                list_anchors_ymax.append(tf.reshape(anchors_ymax_, [-1]))\n                list_anchors_xmax.append(tf.reshape(anchors_xmax_, [-1]))\n\n            anchors_ymin = tf.concat(list_anchors_ymin, 0, name=\'concat_ymin\')\n            anchors_xmin = tf.concat(list_anchors_xmin, 0, name=\'concat_xmin\')\n            anchors_ymax = tf.concat(list_anchors_ymax, 0, name=\'concat_ymax\')\n            anchors_xmax = tf.concat(list_anchors_xmax, 0, name=\'concat_xmax\')\n\n            anchor_cy, anchor_cx, anchor_h, anchor_w = self.point2center(anchors_ymin, anchors_xmin, anchors_ymax, anchors_xmax)\n\n            pred_h = tf.exp(pred_location[:,-2] * self._prior_scaling[2]) * anchor_h\n            pred_w = tf.exp(pred_location[:, -1] * self._prior_scaling[3]) * anchor_w\n            pred_cy = pred_location[:, 0] * self._prior_scaling[0] * anchor_h + anchor_cy\n            pred_cx = pred_location[:, 1] * self._prior_scaling[1] * anchor_w + anchor_cx\n\n            return tf.split(tf.stack(self.center2point(pred_cy, pred_cx, pred_h, pred_w), axis=-1), num_anchors_per_layer, axis=0)\n\nclass AnchorCreator(object):\n    def __init__(self, img_shape, layers_shapes, anchor_scales, extra_anchor_scales, anchor_ratios, layer_steps):\n        super(AnchorCreator, self).__init__()\n        # img_shape -> (height, width)\n        self._img_shape = img_shape\n        self._layers_shapes = layers_shapes\n        self._anchor_scales = anchor_scales\n        self._extra_anchor_scales = extra_anchor_scales\n        self._anchor_ratios = anchor_ratios\n        self._layer_steps = layer_steps\n        self._anchor_offset = [0.5] * len(self._layers_shapes)\n\n    def get_layer_anchors(self, layer_shape, anchor_scale, extra_anchor_scale, anchor_ratio, layer_step, offset = 0.5):\n        \'\'\' assume layer_shape[0] = 6, layer_shape[1] = 5\n        x_on_layer = [[0, 1, 2, 3, 4],\n                       [0, 1, 2, 3, 4],\n                       [0, 1, 2, 3, 4],\n                       [0, 1, 2, 3, 4],\n                       [0, 1, 2, 3, 4],\n                       [0, 1, 2, 3, 4]]\n        y_on_layer = [[0, 0, 0, 0, 0],\n                       [1, 1, 1, 1, 1],\n                       [2, 2, 2, 2, 2],\n                       [3, 3, 3, 3, 3],\n                       [4, 4, 4, 4, 4],\n                       [5, 5, 5, 5, 5]]\n        \'\'\'\n        with tf.name_scope(\'get_layer_anchors\'):\n            x_on_layer, y_on_layer = tf.meshgrid(tf.range(layer_shape[1]), tf.range(layer_shape[0]))\n\n            y_on_image = (tf.cast(y_on_layer, tf.float32) + offset) * layer_step / self._img_shape[0]\n            x_on_image = (tf.cast(x_on_layer, tf.float32) + offset) * layer_step / self._img_shape[1]\n\n            num_anchors_along_depth = len(anchor_scale) * len(anchor_ratio) + len(extra_anchor_scale)\n            num_anchors_along_spatial = layer_shape[1] * layer_shape[0]\n\n            list_h_on_image = []\n            list_w_on_image = []\n\n            global_index = 0\n            # for square anchors\n            for _, scale in enumerate(extra_anchor_scale):\n                list_h_on_image.append(scale)\n                list_w_on_image.append(scale)\n                global_index += 1\n            # for other aspect ratio anchors\n            for scale_index, scale in enumerate(anchor_scale):\n                for ratio_index, ratio in enumerate(anchor_ratio):\n                    list_h_on_image.append(scale / math.sqrt(ratio))\n                    list_w_on_image.append(scale * math.sqrt(ratio))\n                    global_index += 1\n            # shape info:\n            # y_on_image, x_on_image: layers_shapes[0] * layers_shapes[1]\n            # h_on_image, w_on_image: num_anchors_along_depth\n            return tf.expand_dims(y_on_image, axis=-1), tf.expand_dims(x_on_image, axis=-1), \\\n                    tf.constant(list_h_on_image, dtype=tf.float32), \\\n                    tf.constant(list_w_on_image, dtype=tf.float32), num_anchors_along_depth, num_anchors_along_spatial\n\n    def get_all_anchors(self):\n        all_anchors = []\n        all_num_anchors_depth = []\n        all_num_anchors_spatial = []\n        for layer_index, layer_shape in enumerate(self._layers_shapes):\n            anchors_this_layer = self.get_layer_anchors(layer_shape,\n                                                        self._anchor_scales[layer_index],\n                                                        self._extra_anchor_scales[layer_index],\n                                                        self._anchor_ratios[layer_index],\n                                                        self._layer_steps[layer_index],\n                                                        self._anchor_offset[layer_index])\n            all_anchors.append(anchors_this_layer[:-2])\n            all_num_anchors_depth.append(anchors_this_layer[-2])\n            all_num_anchors_spatial.append(anchors_this_layer[-1])\n        return all_anchors, all_num_anchors_depth, all_num_anchors_spatial\n\n'"
utility/anchor_manipulator_unittest.py,30,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\nfrom scipy.misc import imread, imsave, imshow, imresize\nimport numpy as np\nimport sys; sys.path.insert(0, ""."")\nfrom utility import draw_toolbox\nfrom utility import anchor_manipulator\nfrom preprocessing import ssd_preprocessing\n\nslim = tf.contrib.slim\n\ndef save_image_with_bbox(image, labels_, scores_, bboxes_):\n    if not hasattr(save_image_with_bbox, ""counter""):\n        save_image_with_bbox.counter = 0  # it doesn\'t exist yet, so initialize it\n    save_image_with_bbox.counter += 1\n\n    img_to_draw = np.copy(image)\n\n    img_to_draw = draw_toolbox.bboxes_draw_on_img(img_to_draw, labels_, scores_, bboxes_, thickness=2)\n    imsave(os.path.join(\'./debug/{}.jpg\').format(save_image_with_bbox.counter), img_to_draw)\n    return save_image_with_bbox.counter\n\ndef slim_get_split(file_pattern=\'{}_????\'):\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/shape\': tf.FixedLenFeature([3], tf.int64),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/label\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/difficult\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/truncated\': tf.VarLenFeature(dtype=tf.int64),\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'shape\': slim.tfexample_decoder.Tensor(\'image/shape\'),\n        \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n        \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/bbox/label\'),\n        \'object/difficult\': slim.tfexample_decoder.Tensor(\'image/object/bbox/difficult\'),\n        \'object/truncated\': slim.tfexample_decoder.Tensor(\'image/object/bbox/truncated\'),\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n    dataset = slim.dataset.Dataset(\n                data_sources=file_pattern,\n                reader=tf.TFRecordReader,\n                decoder=decoder,\n                num_samples=100,\n                items_to_descriptions=None,\n                num_classes=21,\n                labels_to_names=None)\n\n    with tf.name_scope(\'dataset_data_provider\'):\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n                    dataset,\n                    num_readers=2,\n                    common_queue_capacity=32,\n                    common_queue_min=8,\n                    shuffle=True,\n                    num_epochs=1)\n\n    [org_image, shape, glabels_raw, gbboxes_raw, isdifficult] = provider.get([\'image\', \'shape\',\n                                                                         \'object/label\',\n                                                                         \'object/bbox\',\n                                                                         \'object/difficult\'])\n    image, glabels, gbboxes = ssd_preprocessing.preprocess_image(org_image, glabels_raw, gbboxes_raw, [300, 300], is_training=True, data_format=\'channels_last\', output_rgb=True)\n\n    anchor_creator = anchor_manipulator.AnchorCreator([300] * 2,\n                                                    layers_shapes = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n                                                    anchor_scales = [(0.1,), (0.2,), (0.375,), (0.55,), (0.725,), (0.9,)],\n                                                    extra_anchor_scales = [(0.1414,), (0.2739,), (0.4541,), (0.6315,), (0.8078,), (0.9836,)],\n                                                    anchor_ratios = [(2., .5), (2., 3., .5, 0.3333), (2., 3., .5, 0.3333), (2., 3., .5, 0.3333), (2., .5), (2., .5)],\n                                                    layer_steps = [8, 16, 32, 64, 100, 300])\n\n    all_anchors, all_num_anchors_depth, all_num_anchors_spatial = anchor_creator.get_all_anchors()\n\n    num_anchors_per_layer = []\n    for ind in range(len(all_anchors)):\n        num_anchors_per_layer.append(all_num_anchors_depth[ind] * all_num_anchors_spatial[ind])\n\n    anchor_encoder_decoder = anchor_manipulator.AnchorEncoder(allowed_borders=[1.0] * 6,\n                                                        positive_threshold = 0.5,\n                                                        ignore_threshold = 0.5,\n                                                        prior_scaling=[0.1, 0.1, 0.2, 0.2])\n\n    gt_targets, gt_labels, gt_scores = anchor_encoder_decoder.encode_all_anchors(glabels, gbboxes, all_anchors, all_num_anchors_depth, all_num_anchors_spatial, True)\n\n    anchors = anchor_encoder_decoder._all_anchors\n    # split by layers\n    gt_targets, gt_labels, gt_scores, anchors = tf.split(gt_targets, num_anchors_per_layer, axis=0),\\\n                                                tf.split(gt_labels, num_anchors_per_layer, axis=0),\\\n                                                tf.split(gt_scores, num_anchors_per_layer, axis=0),\\\n                                                [tf.split(anchor, num_anchors_per_layer, axis=0) for anchor in anchors]\n\n    save_image_op = tf.py_func(save_image_with_bbox,\n                            [ssd_preprocessing.unwhiten_image(image),\n                            tf.clip_by_value(tf.concat(gt_labels, axis=0), 0, tf.int64.max),\n                            tf.concat(gt_scores, axis=0),\n                            tf.concat(gt_targets, axis=0)],\n                            tf.int64, stateful=True)\n    return save_image_op\n\nif __name__ == \'__main__\':\n    save_image_op = slim_get_split(\'/media/rs/7A0EE8880EE83EAF/Detections/SSD/dataset/tfrecords/train*\')\n    # Create the graph, etc.\n    init_op = tf.group([tf.local_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer()])\n\n    # Create a session for running operations in the Graph.\n    sess = tf.Session()\n    # Initialize the variables (like the epoch counter).\n    sess.run(init_op)\n\n    # Start input enqueue threads.\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    try:\n        while not coord.should_stop():\n            # Run training steps or whatever\n            print(sess.run(save_image_op))\n\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # When done, ask the threads to stop.\n        coord.request_stop()\n\n    # Wait for threads to finish.\n    coord.join(threads)\n    sess.close()\n'"
utility/checkpint_inspect.py,0,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.python import pywrap_tensorflow\n\ndef print_tensors_in_checkpoint_file(file_name, tensor_name, all_tensors):\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        if all_tensors:\n            var_to_shape_map = reader.get_variable_to_shape_map()\n            for key in var_to_shape_map:\n                print(""tensor_name: "", key)\n                print(reader.get_tensor(key))\n        elif not tensor_name:\n            print(reader.debug_string().decode(""utf-8""))\n        else:\n            print(""tensor_name: "", tensor_name)\n            print(reader.get_tensor(tensor_name))\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(""It\'s likely that your checkpoint file has been compressed ""\n                  ""with SNAPPY."")\n\ndef print_all_tensors_name(file_name):\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        var_to_shape_map = reader.get_variable_to_shape_map()\n        for key in var_to_shape_map:\n            print(key)\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(""It\'s likely that your checkpoint file has been compressed ""\n                  ""with SNAPPY."")\n\nif __name__ == ""__main__"":\n    print_all_tensors_name(\'./model/vgg16_reducedfc.ckpt\')\n'"
utility/draw_toolbox.py,0,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport cv2\nimport matplotlib.cm as mpcm\n\nfrom dataset import dataset_common\n\ndef gain_translate_table():\n    label2name_table = {}\n    for class_name, labels_pair in dataset_common.VOC_LABELS.items():\n        label2name_table[labels_pair[0]] = class_name\n    return label2name_table\n\nlabel2name_table = gain_translate_table()\n\ndef colors_subselect(colors, num_classes=21):\n    dt = len(colors) // num_classes\n    sub_colors = []\n    for i in range(num_classes):\n        color = colors[i*dt]\n        if isinstance(color[0], float):\n            sub_colors.append([int(c * 255) for c in color])\n        else:\n            sub_colors.append([c for c in color])\n    return sub_colors\n\ncolors = colors_subselect(mpcm.plasma.colors, num_classes=21)\ncolors_tableau = [(255, 255, 255), (31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\n                 (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),\n                 (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),\n                 (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),\n                 (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]\n\ndef bboxes_draw_on_img(img, classes, scores, bboxes, thickness=2):\n    shape = img.shape\n    scale = 0.4\n    text_thickness = 1\n    line_type = 8\n    for i in range(bboxes.shape[0]):\n        if classes[i] < 1: continue\n        bbox = bboxes[i]\n        color = colors_tableau[classes[i]]\n        # Draw bounding boxes\n        p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))\n        p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))\n        if (p2[0] - p1[0] < 1) or (p2[1] - p1[1] < 1):\n            continue\n\n        cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n        # Draw text\n        s = \'%s/%.1f%%\' % (label2name_table[classes[i]], scores[i]*100)\n        # text_size is (width, height)\n        text_size, baseline = cv2.getTextSize(s, cv2.FONT_HERSHEY_SIMPLEX, scale, text_thickness)\n        p1 = (p1[0] - text_size[1], p1[1])\n\n        cv2.rectangle(img, (p1[1] - thickness//2, p1[0] - thickness - baseline), (p1[1] + text_size[0], p1[0] + text_size[1]), color, -1)\n\n        cv2.putText(img, s, (p1[1], p1[0] + baseline), cv2.FONT_HERSHEY_SIMPLEX, scale, (255,255,255), text_thickness, line_type)\n\n    return img\n\n'"
utility/scaffolds.py,9,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\n\ndef get_init_fn_for_scaffold(model_dir, checkpoint_path, model_scope, checkpoint_model_scope, checkpoint_exclude_scopes, ignore_missing_vars, name_remap=None):\n    if tf.train.latest_checkpoint(model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s.\' % model_dir)\n        return None\n    exclusion_scopes = []\n    if checkpoint_exclude_scopes:\n        exclusion_scopes = [scope.strip() for scope in checkpoint_exclude_scopes.split(\',\')]\n\n    variables_to_restore = []\n    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n        excluded = False\n        for exclusion in exclusion_scopes:\n            if exclusion in var.op.name:#.startswith(exclusion):\n                excluded = True\n                break\n        if not excluded:\n            variables_to_restore.append(var)\n    if checkpoint_model_scope is not None:\n        if checkpoint_model_scope.strip() == \'\':\n            variables_to_restore = {var.op.name.replace(model_scope + \'/\', \'\'): var for var in variables_to_restore}\n        else:\n            variables_to_restore = {var.op.name.replace(model_scope, checkpoint_model_scope.strip()): var for var in variables_to_restore}\n        if name_remap is not None:\n            renamed_variables_to_restore = dict()\n            for var_name, var in variables_to_restore.items():\n                found = False\n                for k, v in name_remap.items():\n                    if k in var_name:\n                        renamed_variables_to_restore[var_name.replace(k, v)] = var\n                        found = True\n                        break\n                if not found:\n                    renamed_variables_to_restore[var_name] = var\n            variables_to_restore = renamed_variables_to_restore\n\n    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path) if tf.gfile.IsDirectory(checkpoint_path) else checkpoint_path\n\n    tf.logging.info(\'Fine-tuning from %s. Ignoring missing vars: %s.\' % (checkpoint_path, ignore_missing_vars))\n\n    if not variables_to_restore:\n        raise ValueError(\'variables_to_restore cannot be empty\')\n    if ignore_missing_vars:\n        reader = tf.train.NewCheckpointReader(checkpoint_path)\n        if isinstance(variables_to_restore, dict):\n            var_dict = variables_to_restore\n        else:\n            var_dict = {var.op.name: var for var in variables_to_restore}\n        available_vars = {}\n        for var in var_dict:\n            if reader.has_tensor(var):\n                available_vars[var] = var_dict[var]\n            else:\n                tf.logging.warning(\'Variable %s missing in checkpoint %s.\', var, checkpoint_path)\n        variables_to_restore = available_vars\n    if variables_to_restore:\n        saver = tf.train.Saver(variables_to_restore, reshape=False)\n        saver.build()\n        def callback(scaffold, session):\n            saver.restore(session, checkpoint_path)\n        return callback\n    else:\n        tf.logging.warning(\'No Variables to restore.\')\n        return None\n'"
