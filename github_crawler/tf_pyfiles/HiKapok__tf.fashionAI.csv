file_path,api_count,code
config.py,0,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport os\n\nDEBUG = False\nPRED_DEBUG = False\n\nDATA_DIR = \'../Datasets\'\nRECORDS_DATA_DIR = \'../Datasets/tfrecords\'\nTEST_RECORDS_DATA_DIR = \'../Datasets/tfrecords_test\'\nTEST_RECORDS_STAGE2 = \'../Datasets/tfrecords_test_stage2\'\n\nCATEGORIES = [\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\']\nSPLITS = [\'test_0\', \'train_1\', \'train_2\', \'train_3\']#\'train_0\',\nWARM_UP_SPLITS = [\'train_0\']\n\nDEBUG_DIR = \'../Debug\'\nEVAL_DEBUG_DIR = \'../Eval_Debug\'\n\n\n#all_keys = sorted(list(set(blouse_key + outwear_key + trousers_key + skirt_key + dress_key)))\n#print(dict(zip(all_keys, list(range(len(all_keys))))))\n#print(all_keys, len(all_keys))\ncategory2ind = dict(zip(sorted(CATEGORIES), list(range(len(CATEGORIES)))))\nind2category = dict(zip(list(range(len(CATEGORIES))), sorted(CATEGORIES)))\n\n\n#armpit_left|armpit_right; waistband_left,waistband_right--trousers, skirt\xef\xbc\x89\nnormalize_point_name = {\n    \'blouse\': (\'armpit_left\', \'armpit_right\'),\n    \'outwear\': (\'armpit_left\', \'armpit_right\'),\n    \'trousers\': (\'waistband_left\', \'waistband_right\'),\n    \'skirt\': (\'waistband_left\', \'waistband_right\'),\n    \'dress\': (\'armpit_left\', \'armpit_right\')\n}\n\nnormalize_point_ind = {\n    \'blouse\': ([5, 6], [6, 7]),\n    \'outwear\': ([4, 5], [6, 7]),\n    \'trousers\': ([0, 1], [16, 17]),\n    \'skirt\': ([0, 1], [16, 17]),\n    \'dress\': ([5, 6], [6, 7])\n}\nnormalize_point_ind_by_id = {}\n\nlocal_norm_key = []\nlocal_norm_lvalues = []\nlocal_norm_rvalues = []\nglobal_norm_key = []\nglobal_norm_lvalues = []\nglobal_norm_rvalues = []\n\nfor k, v in normalize_point_ind.items():\n    normalize_point_ind_by_id[category2ind[k]] = v\n    local_norm_key.append(category2ind[k])\n    local_norm_lvalues.append(v[0][0])\n    local_norm_rvalues.append(v[0][1])\n    global_norm_key.append(category2ind[k])\n    global_norm_lvalues.append(v[1][0])\n    global_norm_rvalues.append(v[1][1])\n\n# key2ind = {\'bottom_right_in\': 4, \'bottom_left_out\': 3, \'waistline_left\': 22, \'neckline_left\': 14, \'cuff_left_out\': 9, \'bottom_right_out\': 5, \'waistband_left\': 20, \'top_hem_right\': 19, \'top_hem_left\': 18, \'cuff_right_in\': 10, \'armpit_left\': 0, \'bottom_left_in\': 2, \'cuff_left_in\': 8, \'cuff_right_out\': 11, \'hemline_left\': 12, \'neckline_right\': 15, \'shoulder_right\': 17, \'hemline_right\': 13, \'waistband_right\': 21, \'armpit_right\': 1, \'waistline_right\': 23, \'shoulder_left\': 16, \'center_front\': 6, \'crotch\': 7}\nkey2ind = {\'neckline_left\': 1, \'neckline_right\': 2, \'center_front\': 3, \'shoulder_left\': 4, \'shoulder_right\': 5, \'armpit_left\': 6, \'armpit_right\': 7, \'waistline_left\': 8, \'waistline_right\': 9, \'cuff_left_in\': 10, \'cuff_left_out\': 11, \'cuff_right_in\': 12, \'cuff_right_out\': 13, \'top_hem_left\': 14, \'top_hem_right\': 15, \'waistband_left\': 16, \'waistband_right\': 17, \'hemline_left\': 18, \'hemline_right\': 19, \'crotch\': 20, \'bottom_left_in\': 21, \'bottom_left_out\': 22, \'bottom_right_in\': 23, \'bottom_right_out\': 24}\n\nall_keys = [\'neckline_left\', \'neckline_right\', \'center_front\', \'shoulder_left\', \'shoulder_right\', \'armpit_left\', \'armpit_right\', \'waistline_left\', \'waistline_right\', \'cuff_left_in\', \'cuff_left_out\', \'cuff_right_in\', \'cuff_right_out\', \'top_hem_left\', \'top_hem_right\', \'waistband_left\', \'waistband_right\', \'hemline_left\', \'hemline_right\', \'crotch\', \'bottom_left_in\', \'bottom_left_out\', \'bottom_right_in\', \'bottom_right_out\']\n\ninverse_key2ind = {}\nfor k, v in key2ind.items():\n    inverse_key2ind[v] = k\n\n\nclass_num_joints = {\n    \'*\': 24,\n    \'blouse\': 13,\n    \'dress\': 15,\n    \'outwear\': 14,\n    \'skirt\': 4,\n    \'trousers\': 7\n}\n\n# |0|1|2|3|4|\n# |---|---|---|---|---|\n# |neckline_left|neckline_right|shoulder_left|shoulder_right|center_front|\n\n# |5|6|7|8|\n# |---|---|---|---|\n# |armpit_left|armpit_right|top_hem_left|top_hem_right|\n\n# |9|10|11|12|\n# |---|---|---|---|\n# |cuff_left_in|cuff_left_out|cuff_right_in|cuff_right_out|\n## Blouse 13\nblouse_keymap = {\'neckline_left\': 0,\n            \'neckline_right\': 1,\n            \'shoulder_left\': 2,\n            \'shoulder_right\': 3,\n            \'center_front\': 4,\n            \'armpit_left\': 5,\n            \'armpit_right\': 6,\n            \'top_hem_left\': 7,\n            \'top_hem_right\': 8,\n            \'cuff_left_in\': 9,\n            \'cuff_left_out\': 10,\n            \'cuff_right_in\': 11,\n            \'cuff_right_out\': 12}\n\ninverse_blouse_keymap = {}\nfor k, v in blouse_keymap.items():\n    inverse_blouse_keymap[v] = k\n\nblouse_global_ind = []\nfor i in range(len(inverse_blouse_keymap)):\n    blouse_global_ind.append(key2ind[inverse_blouse_keymap[i]]-1)\n\n\n# ## Outwear 14\n## Outwear\n# | 0 | 1 | 2 | 3 | 4 |\n# | --- | --- | --- | --- | --- |\n# | neckline_left | neckline_right | shoulder_left | shoulder_right | armpit_left |\n\n# | 5 | 6 | 7 | 8 | 9 |\n# | --- | --- | --- | --- | --- |\n# | armpit_right | waistline_left | waistline_right | cuff_left_in | cuff_left_out|\n\n# | 10 | 11 | 12 | 13 |\n# | --- | --- | --- | --- |\n# | cuff_right_in | cuff_right_out | top_hem_left |top_hem_right  |\n\noutwear_keymap = {\'neckline_left\': 0,\n            \'neckline_right\': 1,\n            \'shoulder_left\': 2,\n            \'shoulder_right\': 3,\n            \'armpit_left\': 4,\n            \'armpit_right\': 5,\n            \'waistline_left\': 6,\n            \'waistline_right\': 7,\n            \'cuff_left_in\': 8,\n            \'cuff_left_out\': 9,\n            \'cuff_right_in\': 10,\n            \'cuff_right_out\': 11,\n            \'top_hem_left\': 12,\n            \'top_hem_right\': 13}\n\ninverse_outwear_keymap = {}\nfor k, v in outwear_keymap.items():\n    inverse_outwear_keymap[v] = k\n\noutwear_global_ind = []\nfor i in range(len(inverse_outwear_keymap)):\n    outwear_global_ind.append(key2ind[inverse_outwear_keymap[i]]-1)\n\n# ## Trousers 7\n## Trousers\n# | 0 | 1 | 2 |\n# | --- | --- | --- |\n# | waistband_left | waistband_right | crotch |\n\n# |3| 4 | 5 | 6 |\n# |--- | --- | --- |--- |\n# | bottom_left_in | bottom_left_out | bottom_right_in | bottom_right_out |\ntrousers_keymap = {\'waistband_left\': 0,\n                \'waistband_right\': 1,\n                \'crotch\': 2,\n                \'bottom_left_in\': 3,\n                \'bottom_left_out\': 4,\n                \'bottom_right_in\': 5,\n                \'bottom_right_out\': 6}\n\ninverse_trousers_keymap = {}\nfor k, v in trousers_keymap.items():\n    inverse_trousers_keymap[v] = k\n\ntrousers_global_ind = []\nfor i in range(len(inverse_trousers_keymap)):\n    trousers_global_ind.append(key2ind[inverse_trousers_keymap[i]]-1)\n\n\n# ## Skirt 4\n## Skirt\n# | 0 | 1 | 2 | 3 |\n# | --- | --- | --- | --- |\n# | waistband_left | waistband_right | hemline_left | hemline_right |\nskirt_keymap = {\'waistband_left\': 0,\n            \'waistband_right\': 1,\n            \'hemline_left\': 2,\n            \'hemline_right\': 3}\n\ninverse_skirt_keymap = {}\nfor k, v in skirt_keymap.items():\n    inverse_skirt_keymap[v] = k\n\nskirt_global_ind = []\nfor i in range(len(inverse_skirt_keymap)):\n    skirt_global_ind.append(key2ind[inverse_skirt_keymap[i]]-1)\n\n# ## Dress 15\n## Dress\n# | 0 | 1 | 2 | 3 | 4 |\n# | --- | --- | --- | --- | --- |\n# | neckline_left | neckline_right | shoulder_left | shoulder_right | center_front |\n\n\n# | 5 | 6 | 7 | 8 | 9  |\n# | --- | --- | --- | --- | --- | --- |\n# | armpit_left | armpit_right |waistline_left | waistline_right | cuff_left_in|\n\n\n# | 10| 11 | 12 | 13 | 14 |\n# | --- | --- | --- | --- |--- |\n# | cuff_left_out | cuff_right_in | cuff_right_out |hemline_left | hemline_right |\ndress_keymap = {\'neckline_left\': 0,\n            \'neckline_right\': 1,\n            \'shoulder_left\': 2,\n            \'shoulder_right\': 3,\n            \'center_front\': 4,\n            \'armpit_left\': 5,\n            \'armpit_right\': 6,\n            \'waistline_left\': 7,\n            \'waistline_right\': 8,\n            \'cuff_left_in\': 9,\n            \'cuff_left_out\': 10,\n            \'cuff_right_in\': 11,\n            \'cuff_right_out\': 12,\n            \'hemline_left\': 13,\n            \'hemline_right\': 14}\n\ninverse_dress_keymap = {}\nfor k, v in dress_keymap.items():\n    inverse_dress_keymap[v] = k\n\ndress_global_ind = []\nfor i in range(len(inverse_dress_keymap)):\n    dress_global_ind.append(key2ind[inverse_dress_keymap[i]]-1)\n\n# whick global ind is this position belongs to\nclass2global_ind_map = {\n    \'*\': list(range(24)),\n    \'blouse\': blouse_global_ind,\n    \'dress\': dress_global_ind,\n    \'outwear\': outwear_global_ind,\n    \'skirt\': skirt_global_ind,\n    \'trousers\': trousers_global_ind\n}\n\n#print(class2global_ind_map)\n\nleft_right_remap = {\n    \'*\': [1, 0, 2, 4, 3, 6, 5, 8, 7, 11, 12, 9, 10, 14, 13, 16, 15, 18, 17, 19, 22, 23, 20, 21],\n    \'blouse\': [1, 0, 3, 2, 4, 6, 5, 8, 7, 11, 12, 9, 10],\n    \'outwear\': [1, 0, 3, 2, 5, 4, 7, 6, 10, 11, 8, 9, 13, 12],\n    \'trousers\': [1, 0, 2, 5, 6, 3, 4],\n    \'skirt\': [1, 0, 3, 2],\n    \'dress\': [1, 0, 3, 2, 4, 6, 5, 8, 7, 11, 12, 9, 10, 14, 13]\n}\n\n# left keypoint index, right keypoint index, center keypoint index\nleft_right_group_map = {\n    \'*\': ([0, 3, 5, 7, 9, 10, 13, 15, 17, 20, 21],\n            [1, 4, 6, 8, 11, 12, 14, 16, 18, 22, 23],\n            [2, 19]),\n    \'blouse\': ([0, 2, 5, 7, 9, 10],\n                [1, 3, 6, 8, 11, 12],\n                [4]),\n    \'outwear\': ([0, 2, 4, 6, 8, 9, 12],\n                [1, 3, 5, 7, 10, 11, 13],\n                []),\n    \'trousers\': ([0, 3, 4],\n                [1, 5, 6],\n                [2]),\n    \'skirt\': ([0, 2],\n            [1, 3],\n            []),\n    \'dress\': ([0, 2, 5, 7, 9, 10, 13],\n                [1, 3, 6, 8, 11, 12, 14],\n                [4])\n}\n# train {\'blouse\': 10155, \'outwear\': 7734, \'dress\': 7224, \'skirt\': 9910, \'trousers\': 9142} 220825\n# test {\'trousers\': 1958, \'outwear\': 2043, \'skirt\': 1980, \'blouse\': 1977, \'dress\': 2038} 49980\n# test_b {\'blouse\': 1974, \'outwear\': 1947, \'trousers\': 1946, \'skirt\': 2051, \'dress\': 2052} 9970\n\n# round 2\n# train {\'outwear\': 9586, \'blouse\': 11109, \'dress\': 9002, \'trousers\': 10251, \'skirt\': 11649} 51597\n# warm-up {\'trousers\': 2795, \'dress\': 2312, \'skirt\': 2292, \'outwear\': 2138, \'blouse\': 2997} 12534\n# val blouse 182 outwear 152 dress 137 skirt 186 trousers 160 all 817\n# test_a {\'outwear\': 2508, \'blouse\': 2586, \'trousers\': 2631, \'skirt\': 2683, \'dress\': 2693} 13101\n\n# new round 2\n# {\'trousers\': 10251, \'skirt\': 11649, \'blouse\': 11109, \'dress\': 9002, \'outwear\': 9586} 51597\n# warm-up {\'trousers\': 2795, \'skirt\': 2292, \'blouse\': 2997, \'dress\': 2312, \'outwear\': 2138} 12534\n# test_a {\'trousers\': 2631, \'skirt\': 2683, \'blouse\': 2586, \'dress\': 2693, \'outwear\': 2508} 13101\n# test_b {\'outwear\': 10906, \'trousers\': 10618, \'dress\': 11096, \'skirt\': 11154, \'blouse\': 10670} 54444\nsplit_size = {\n            \'*\': {\'train\': 51597+12534,\n                \'val\': 0,\n                \'test\': 54444,\n                \'test_a\': 13101},\n            \'blouse\': {\'train\': 11109+2997,\n                \'val\': 0,\n                \'test\': 10670,\n                \'test_a\': 2586},\n            \'dress\': {\'train\': 9002+2312,\n                \'val\': 0,\n                \'test\': 11096,\n                \'test_a\': 2693},\n            \'outwear\': {\'train\': 9586+2138,\n                \'val\': 0,\n                \'test\': 10906,\n                \'test_a\': 2508},\n            \'skirt\': {\'train\': 11649+2292,\n                \'val\': 0,\n                \'test\': 11154,\n                \'test_a\': 2683},\n            \'trousers\': {\'train\': 10251+2795,\n                \'val\': 0,\n                \'test\': 10618,\n                \'test_a\': 2631},\n            }\n\n'"
convert_tfrecords.py,31,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport config\n\nimport os\nimport sys\nimport re\nimport random\nfrom scipy import misc\n\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\n\n# TFRecords convertion parameters.\nRANDOM_SEED = 4242\nSAMPLES_PER_FILES = 2500\n\ndef int64_feature(value):\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef float_feature(value):\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef bytes_feature(value):\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef _process_image(filename):\n    image_data = tf.gfile.FastGFile(filename, \'rb\').read()\n    return image_data, misc.imread(filename).shape\n\ndef _convert_to_example(image_data, shape, image_file, class_id, keypoint_x, keypoint_y, keypoint_v, keypoint_id, keypoint_global_id):\n    image_format = b\'JPEG\'\n    example = tf.train.Example(features=tf.train.Features(feature={\n            \'image/height\': int64_feature(shape[0]),\n            \'image/width\': int64_feature(shape[1]),\n            \'image/channels\': int64_feature(shape[2]),\n            \'image/classid\': int64_feature(class_id),\n            \'image/keypoint/x\': int64_feature(keypoint_x),\n            \'image/keypoint/y\': int64_feature(keypoint_y),\n            \'image/keypoint/v\': int64_feature(keypoint_v),\n            \'image/keypoint/id\': int64_feature(keypoint_id),\n            \'image/keypoint/gid\': int64_feature(keypoint_global_id),\n            \'image/format\': bytes_feature(image_format),\n            \'image/filename\': bytes_feature(image_file.encode(\'utf8\')),\n            \'image/encoded\': bytes_feature(image_data)}))\n    return example\n\ndef _add_to_tfrecord(tfrecord_writer, image_path, image_file, class_id, keypoint_x, keypoint_y, keypoint_v, keypoint_id, keypoint_global_id):\n    image_data, shape = _process_image(image_path)\n    example = _convert_to_example(image_data, shape, image_file, class_id, keypoint_x, keypoint_y, keypoint_v, keypoint_id, keypoint_global_id)\n    tfrecord_writer.write(example.SerializeToString())\n\ndef _test_add_to_tfrecord(tfrecord_writer, image_path, image_file, class_id):\n    image_data, shape = _process_image(image_path)\n    image_format = b\'JPEG\'\n    example = tf.train.Example(features=tf.train.Features(feature={\n            \'image/height\': int64_feature(shape[0]),\n            \'image/width\': int64_feature(shape[1]),\n            \'image/channels\': int64_feature(shape[2]),\n            \'image/classid\': int64_feature(class_id),\n            \'image/format\': bytes_feature(image_format),\n            \'image/filename\': bytes_feature(image_file.encode(\'utf8\')),\n            \'image/encoded\': bytes_feature(image_data)}))\n    tfrecord_writer.write(example.SerializeToString())\n\ndef test_dataset():\n\n    filename_queue = tf.train.string_input_producer([\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords/blouse_0000.tfrecord\'], num_epochs=None)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(serialized_example,\n            features={\n                \'image/height\': tf.FixedLenFeature([1], tf.int64),\n                \'image/width\': tf.FixedLenFeature([1], tf.int64),\n                \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n                \'image/classid\': tf.FixedLenFeature([1], tf.int64),\n                \'image/keypoint/x\': tf.VarLenFeature(dtype=tf.int64),\n                \'image/keypoint/y\': tf.VarLenFeature(dtype=tf.int64),\n                \'image/keypoint/v\': tf.VarLenFeature(dtype=tf.int64),\n                \'image/keypoint/id\': tf.VarLenFeature(dtype=tf.int64),\n                \'image/keypoint/gid\': tf.VarLenFeature(dtype=tf.int64),\n                \'image/format\': tf.FixedLenFeature([], tf.string, default_value=\'jpeg\'),\n                \'image/filename\': tf.FixedLenFeature([], tf.string, default_value=\'\'),\n                \'image/encoded\': tf.FixedLenFeature([], tf.string, default_value=\'\')\n            }\n        )\n\n    sess = tf.Session()\n    init = tf.initialize_all_variables()\n    sess.run(init)\n    tf.train.start_queue_runners(sess=sess)\n    eval_features = sess.run(features)\n    eval_features = sess.run(features)\n    eval_features = sess.run(features)\n    eval_features = sess.run(features)\n    eval_features = sess.run(features)\n    eval_features = sess.run(features)\n\n    print(\'image/height\', eval_features[\'image/height\'])\n    print(\'image/width\', eval_features[\'image/width\'])\n    print(\'image/channels\', eval_features[\'image/channels\'])\n    print(\'image/classid\', eval_features[\'image/classid\'])\n    print(\'image/keypoint/x\', eval_features[\'image/keypoint/x\'])\n    print(\'image/keypoint/y\', eval_features[\'image/keypoint/y\'])\n    print(\'image/keypoint/v\', eval_features[\'image/keypoint/v\'])\n    print(\'image/keypoint/id\', eval_features[\'image/keypoint/id\'])\n    print(\'image/keypoint/gid\', eval_features[\'image/keypoint/gid\'])\n    print(\'image/format\', eval_features[\'image/format\'])\n    print(\'image/filename\', eval_features[\'image/filename\'].decode(\'utf8\'))\n    #print(\'image/encoded\', eval_features[\'image/encoded\'])\n\n\n# print(blouse_keymap)\n# print(inverse_blouse_keymap)\n# print(outwear_keymap)\n# print(inverse_outwear_keymap)\n# print(trousers_keymap)\n# print(inverse_trousers_keymap)\n# print(skirt_keymap)\n# print(inverse_skirt_keymap)\n# print(dress_keymap)\n# print(inverse_dress_keymap)\n# print(key2ind)\n# print(inverse_key2ind)\nkeymap_factory = {\'blouse\': config.blouse_keymap,\n                 \'dress\': config.dress_keymap,\n                 \'outwear\': config.outwear_keymap,\n                 \'skirt\': config.skirt_keymap,\n                 \'trousers\': config.trousers_keymap}\n\ndef convert_train(output_dir, val_per=0.015, all_splits=config.SPLITS, file_idx_start=0):\n    class_hist = {\'blouse\': 0,\n                 \'dress\': 0,\n                 \'outwear\': 0,\n                 \'skirt\': 0,\n                 \'trousers\': 0}\n\n    start_file_idx = {\'blouse\': 5,\n                 \'dress\': 3,\n                 \'outwear\': 4,\n                 \'skirt\': 4,\n                 \'trousers\': 4}\n\n    for cat in config.CATEGORIES:\n        total_examples = 0\n        # TODO: create tfrecorder writer here\n        sys.stdout.write(\'\\nprocessing category: {}...\'.format(cat))\n        sys.stdout.flush()\n        file_idx = file_idx_start#start_file_idx[cat]\n        record_idx = 0\n        tf_filename = os.path.join(output_dir, \'%s_%04d.tfrecord\' % (cat, file_idx))\n        tfrecord_writer = tf.python_io.TFRecordWriter(tf_filename)\n\n        tf_val_filename = os.path.join(output_dir, \'%s_%04d_val.tfrecord\' % (cat, 0))\n        val_tfrecord_writer = tf.python_io.TFRecordWriter(tf_val_filename)\n        this_key_map = keymap_factory[cat]\n\n        for split in all_splits:\n            if \'test\' in split: continue\n            sys.stdout.write(\'\\nprocessing split: {}...\\n\'.format(split))\n            sys.stdout.flush()\n            split_path = os.path.join(config.DATA_DIR, split)\n            anna_root = os.path.join(split_path, \'Annotations\')\n            anna_file = os.path.join(anna_root, os.listdir(anna_root)[0])\n            anna_pd = pd.read_csv(anna_file)\n            anna_pd = anna_pd.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n            this_nums = len(anna_pd.index)\n            total_examples += this_nums\n            all_columns_name = list(anna_pd.columns)\n            #print(all_columns_name)\n            all_columns_name = sorted([s.strip() for s in all_columns_name[2:]])\n            #print(all_columns_name)\n            # print(anna_pd)\n            # print(all_columns_name)\n            for index, row in anna_pd.iterrows():\n                sys.stdout.write(\'\\r>> Converting image %d/%d\' % (index+1, this_nums))\n                sys.stdout.flush()\n                category = row[\'image_category\']\n                if not (cat in category): continue\n                class_hist[category] += 1\n                image_file = row[\'image_id\']\n                full_file_path = os.path.join(split_path, image_file)\n                #print(len(all_columns_name))\n                class_id = config.category2ind[category]\n                keypoint_x = []\n                keypoint_y = []\n                keypoint_v = []\n                keypoint_id = []\n                keypoint_global_id = []\n\n                for keys in config.all_keys:\n                    if keys in this_key_map:\n                        keypoint_id.append(this_key_map[keys])\n                    else:\n                        keypoint_id.append(-1)\n                    keypoint_global_id.append(config.key2ind[keys] - 1)\n                    keypoint_info = row[keys].strip().split(\'_\')\n                    keypoint_x.append(int(keypoint_info[0]))\n                    keypoint_y.append(int(keypoint_info[1]))\n                    keypoint_v.append(int(keypoint_info[2]))\n                    #print(row[keys].strip().split(\'_\'))\n                if np.random.random_sample() > val_per:\n                    _add_to_tfrecord(tfrecord_writer, full_file_path, image_file, class_id, keypoint_x, keypoint_y, keypoint_v, keypoint_id, keypoint_global_id)\n                else:\n                    _add_to_tfrecord(val_tfrecord_writer, full_file_path, image_file, class_id, keypoint_x, keypoint_y, keypoint_v, keypoint_id, keypoint_global_id)\n                record_idx += 1\n                if record_idx > SAMPLES_PER_FILES:\n                    record_idx = 0\n                    file_idx += 1\n                    tf_filename = os.path.join(output_dir, \'%s_%04d.tfrecord\' % (cat, file_idx))\n                    tfrecord_writer.flush()\n                    tfrecord_writer.close()\n                    tfrecord_writer = tf.python_io.TFRecordWriter(tf_filename)\n                    #print(keypoint_id)\n                    #print(keypoint_global_id)\n                    # print(keypoint_x)\n                    # print(keypoint_y)\n                    # print(keypoint_v)\n                    #keymap_factory[category](full_file_path, image_file)\n                    #[(col, row[col]) for col in all_columns_name]\n                    #pass#print(row[\'c1\'], row[\'c2\'])\n        val_tfrecord_writer.flush()\n        val_tfrecord_writer.close()\n    print(\'\\nFinished converting the whole dataset!\')\n    print(class_hist, total_examples)\n    return class_hist, total_examples\n\ndef convert_test(output_dir, splits=config.SPLITS):\n\n    class_hist = {\'blouse\': 0,\n                 \'dress\': 0,\n                 \'outwear\': 0,\n                 \'skirt\': 0,\n                 \'trousers\': 0}\n\n    for cat in config.CATEGORIES:\n        total_examples = 0\n        # TODO: create tfrecorder writer here\n        sys.stdout.write(\'\\nprocessing category: {}...\'.format(cat))\n        sys.stdout.flush()\n        file_idx = 0\n        record_idx = 0\n        tf_filename = os.path.join(output_dir, \'%s_%04d.tfrecord\' % (cat, file_idx))\n        tfrecord_writer = tf.python_io.TFRecordWriter(tf_filename)\n        this_key_map = keymap_factory[cat]\n\n        for split in splits:\n            if \'train\' in split: continue\n            sys.stdout.write(\'\\nprocessing split: {}...\\n\'.format(split))\n            sys.stdout.flush()\n            split_path = os.path.join(config.DATA_DIR, split)\n            anna_file = os.path.join(split_path, \'test.csv\')\n            anna_pd = pd.read_csv(anna_file)\n            this_nums = len(anna_pd.index)\n            total_examples += this_nums\n            for index, row in anna_pd.iterrows():\n                sys.stdout.write(\'\\r>> Converting image %d/%d\' % (index+1, this_nums))\n                sys.stdout.flush()\n                category = row[\'image_category\']\n                if not (cat in category): continue\n                class_hist[category] += 1\n                image_file = row[\'image_id\']\n                full_file_path = os.path.join(split_path, image_file)\n                #print(len(all_columns_name))\n                class_id = config.category2ind[category]\n\n                _test_add_to_tfrecord(tfrecord_writer, full_file_path, image_file, class_id)\n                record_idx += 1\n                if record_idx > SAMPLES_PER_FILES:\n                    record_idx = 0\n                    file_idx += 1\n                    tf_filename = os.path.join(output_dir, \'%s_%04d.tfrecord\' % (cat, file_idx))\n                    tfrecord_writer.flush()\n                    tfrecord_writer.close()\n                    tfrecord_writer = tf.python_io.TFRecordWriter(tf_filename)\n    print(\'\\nFinished converting the whole test dataset!\')\n    print(class_hist, total_examples)\n    return class_hist, total_examples\n\ndef count_split_examples(split_path, file_pattern=\'\'):\n    # Count the total number of examples in all of these shard\n    num_samples = 0\n    tfrecords_to_count = [os.path.join(split_path, file) for file in os.listdir(split_path) if file_pattern in file]\n    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    for tfrecord_file in tfrecords_to_count:\n        for record in tf.python_io.tf_record_iterator(tfrecord_file):#, options = opts):\n            num_samples += 1\n            #print(num_samples)\n    return num_samples\n\nif __name__ == \'__main__\':\n    np.random.seed(RANDOM_SEED)\n    convert_test(config.TEST_RECORDS_STAGE2, splits=[\'test_1\'])\n    print(\'blouse\', count_split_examples(config.TEST_RECORDS_STAGE2, file_pattern=\'blouse\')\n    , \'outwear\', count_split_examples(config.TEST_RECORDS_STAGE2, file_pattern=\'outwear\')\n    , \'dress\', count_split_examples(config.TEST_RECORDS_STAGE2, file_pattern=\'dress\')\n    , \'skirt\', count_split_examples(config.TEST_RECORDS_STAGE2, file_pattern=\'skirt\')\n    , \'trousers\', count_split_examples(config.TEST_RECORDS_STAGE2, file_pattern=\'trousers\')\n    , \'all\', count_split_examples(config.TEST_RECORDS_STAGE2, file_pattern=\'_\'))\n\n    # os.mkdir(config.RECORDS_DATA_DIR)\n    # convert_train(config.RECORDS_DATA_DIR, val_per=0.)\n    # convert_train(config.RECORDS_DATA_DIR, val_per=0., all_splits=config.WARM_UP_SPLITS, file_idx_start=1000)\n    # os.mkdir(config.TEST_RECORDS_DATA_DIR)\n    # convert_test(config.TEST_RECORDS_DATA_DIR)\n    # print(\'blouse\', count_split_examples(config.RECORDS_DATA_DIR, file_pattern=\'blouse_0000_val\')\n    # , \'outwear\', count_split_examples(config.RECORDS_DATA_DIR, file_pattern=\'outwear_0000_val\')\n    # , \'dress\', count_split_examples(config.RECORDS_DATA_DIR, file_pattern=\'dress_0000_val\')\n    # , \'skirt\', count_split_examples(config.RECORDS_DATA_DIR, file_pattern=\'skirt_0000_val\')\n    # , \'trousers\', count_split_examples(config.RECORDS_DATA_DIR, file_pattern=\'trousers_0000_val\')\n    # , \'all\', count_split_examples(config.RECORDS_DATA_DIR, file_pattern=\'val\'))\n    # test_dataset()\n\n'"
depth_conv2d.py,0,"b'# -*- coding: utf-8 -*-\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# modified from tensorflow/contrib/layers/python/layers/layers.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib.framework.python.ops import variables\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom tensorflow.contrib.layers.python.layers import utils\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import variable_scope\n\nDATA_FORMAT_NCHW = \'NCHW\'\nDATA_FORMAT_NHWC = \'NHWC\'\nDATA_FORMAT_NCDHW = \'NCDHW\'\nDATA_FORMAT_NDHWC = \'NDHWC\'\n\ndef _model_variable_getter(getter,\n                           name,\n                           shape=None,\n                           dtype=None,\n                           initializer=None,\n                           regularizer=None,\n                           trainable=True,\n                           collections=None,\n                           caching_device=None,\n                           partitioner=None,\n                           rename=None,\n                           use_resource=None,\n                           **_):\n  """"""Getter that uses model_variable for compatibility with core layers.""""""\n  short_name = name.split(\'/\')[-1]\n  if rename and short_name in rename:\n    name_components = name.split(\'/\')\n    name_components[-1] = rename[short_name]\n    name = \'/\'.join(name_components)\n  return variables.model_variable(\n      name,\n      shape=shape,\n      dtype=dtype,\n      initializer=initializer,\n      regularizer=regularizer,\n      collections=collections,\n      trainable=trainable,\n      caching_device=caching_device,\n      partitioner=partitioner,\n      custom_getter=getter,\n      use_resource=use_resource)\n\n\ndef _build_variable_getter(rename=None):\n  """"""Build a model variable getter that respects scope getter and renames.""""""\n\n  # VariableScope will nest the getters\n  def layer_variable_getter(getter, *args, **kwargs):\n    kwargs[\'rename\'] = rename\n    return _model_variable_getter(getter, *args, **kwargs)\n\n  return layer_variable_getter\n\ndef depth_conv2d(\n    inputs,\n    kernel_size,\n    stride=1,\n    channel_multiplier=1,\n    padding=\'SAME\',\n    data_format=DATA_FORMAT_NHWC,\n    rate=1,\n    activation_fn=nn.relu,\n    normalizer_fn=None,\n    normalizer_params=None,\n    weights_initializer=initializers.xavier_initializer(),\n    weights_regularizer=None,\n    biases_initializer=init_ops.zeros_initializer(),\n    biases_regularizer=None,\n    reuse=None,\n    variables_collections=None,\n    outputs_collections=None,\n    trainable=True,\n    scope=None):\n\n    if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n        raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n    layer_variable_getter = _build_variable_getter({\n      \'bias\': \'biases\',\n      \'depthwise_kernel\': \'depthwise_weights\'\n    })\n\n    with variable_scope.variable_scope(\n            scope,\n            \'SeparableConv2d\', [inputs],\n            reuse=reuse,\n            custom_getter=layer_variable_getter) as sc:\n        inputs = ops.convert_to_tensor(inputs)\n\n        df = (\'channels_first\'\n              if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n\n        # Actually apply depthwise conv instead of separable conv.\n        dtype = inputs.dtype.base_dtype\n        kernel_h, kernel_w = utils.two_element_tuple(kernel_size)\n        stride_h, stride_w = utils.two_element_tuple(stride)\n        num_filters_in = utils.channel_dimension(\n            inputs.get_shape(), df, min_rank=4)\n        weights_collections = utils.get_variable_collections(\n            variables_collections, \'weights\')\n\n        depthwise_shape = [kernel_h, kernel_w, num_filters_in, channel_multiplier]\n        depthwise_weights = variables.model_variable(\n            \'depthwise_weights\',\n            shape=depthwise_shape,\n            dtype=dtype,\n            initializer=weights_initializer,\n            regularizer=weights_regularizer,\n            trainable=trainable,\n            collections=weights_collections)\n        strides = [1, 1, stride_h, stride_w] if data_format.startswith(\'NC\') else [1, stride_h, stride_w, 1]\n\n        outputs = nn.depthwise_conv2d(\n            inputs,\n            depthwise_weights,\n            strides,\n            padding,\n            rate=utils.two_element_tuple(rate),\n            data_format=data_format)\n        num_outputs = num_filters_in\n\n        if normalizer_fn is not None:\n            normalizer_params = normalizer_params or {}\n            outputs = normalizer_fn(outputs, **normalizer_params)\n        else:\n            if biases_initializer is not None:\n                biases_collections = utils.get_variable_collections(\n                  variables_collections, \'biases\')\n                biases = variables.model_variable(\n                    \'biases\',\n                    shape=[\n                      num_outputs,\n                    ],\n                    dtype=dtype,\n                    initializer=biases_initializer,\n                    regularizer=biases_regularizer,\n                    trainable=trainable,\n                    collections=biases_collections)\n                outputs = nn.bias_add(outputs, biases, data_format=data_format)\n\n        if activation_fn is not None:\n            outputs = activation_fn(outputs)\n        return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n'"
ensemble_from_csv.py,0,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport config\n\nsubs_dir = \'../Submit/ensemble\'\n# ensemble_subs = [\'cpn_320_160_1e-3_half_epoch.csv\',\n# \'cpn_320_160_blur_half_epoch_2e-5.csv\',\n# \'hg_8_256_v2_half_epoch.csv\',\n# \'sub_2_cpn_320_100_1e-3-half_epoch.csv\',\n# \'sub_2_hg_4_256_64-half_epoch.csv\',\n# \'sub_2_hg_8_256_64_v1-half_epoch.csv\']#[\'cpn_2_320_160_1e-3.csv\', \'sub_2_hg_4_256_64.csv\', \'sub_2_cpn_320_100_1e-3.csv\', \'sub_2_hg_8_256_64.csv\']\n\nensemble_subs = [\'large_seresnext_cpn_sub.csv\', \'large_detnext_cpn_sub.csv\']\n\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef mean_ensemble():\n    # all test images will be put into this dict\n    all_test_items = {}\n    # extract all predict items\n    for sub_file in ensemble_subs:\n        sub_file_path = os.path.join(subs_dir, sub_file)\n        df = pd.read_csv(sub_file_path, header=0)\n        #print(df.values.tolist())\n        all_predict = df.values.tolist()\n\n        for records in all_predict:\n            file_id = records[0]\n            preds = records[1:]\n            if file_id in all_test_items:\n                all_test_items[file_id].append(preds)\n            else:\n                all_test_items[file_id] = [preds]\n\n    #print(all_test_items)\n    cur_record = 0\n    df = pd.DataFrame(columns=[\'image_id\', \'image_category\'] + config.all_keys)\n    num_keypoints_plus = len(config.all_keys) + 1\n    for k, v in all_test_items.items():\n        #print(v)\n        temp_list = []\n        len_pred = len(v) * 1.\n        # iterate all the predictions\n        for pred_ind in range(1, num_keypoints_plus):\n            pred_x, pred_y, pred_v = 0., 0., 1\n            if v[0][pred_ind].strip() == \'-1_-1_-1\':\n                temp_list.append(\'-1_-1_-1\')\n                #print(temp_list)\n                continue\n            for _pred in v:\n                _pred_x, _pred_y, _pred_v = _pred[pred_ind].strip().split(\'_\')\n                _pred_x, _pred_y, _pred_v = float(_pred_x), float(_pred_y), int(_pred_v)\n                #print(_pred_x, _pred_y)\n                pred_x = pred_x + _pred_x/len_pred\n                pred_y = pred_y + _pred_y/len_pred\n            temp_list.append(\'{}_{}_{}\'.format(round(pred_x), round(pred_y), pred_v))\n        #print(temp_list)\n            #break\n        #break\n        df.loc[cur_record] = [k, v[0][0]] + temp_list\n        cur_record = cur_record + 1\n    df.sort_values(\'image_id\').to_csv(os.path.join(subs_dir, \'ensmeble.csv\'), encoding=\'utf-8\', index=False)\n\nif __name__ == \'__main__\':\n    mean_ensemble()\n'"
eval_all_cpn_onepass.py,124,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nimport sys\nimport numpy as np\nimport pandas as pd\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import detnet_cpn\nfrom net import detxt_cpn\nfrom net import seresnet_cpn\nfrom net import cpn\nfrom net import simple_xt\n\nfrom utility import train_helper\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n#--num_readers=2 --num_preprocessing_threads=2 --data_dir=/media/disk/keypoint/tfrecords --model_to_train=all, blouse\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords_test_stage2\',#tfrecords_test tfrecords_test_stage1_b tfrecords_test_stage2\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_*.tfrecord\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'.\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_string(\n    \'backbone\', \'detnet50_cpn\',\n    \'The backbone network to use for feature extraction.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_last\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_boolean(\n    \'flip_on_test\', True,\n    \'Wether we will average predictions of left-right fliped image.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', \'blouse\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', False,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_eval\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to eval (comma-separated list).\')\n\n#--model_scope=blouse --checkpoint_path=./logs/blouse\nFLAGS = tf.app.flags.FLAGS\n\n#print(FLAGS.data_dir)\nall_models = {\n  \'resnet50_cpn\': {\'backbone\': cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_cpn\'},\n  \'detnet50_cpn\': {\'backbone\': detnet_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_detnet_cpn\'},\n  \'seresnet50_cpn\': {\'backbone\': seresnet_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_se_cpn\'},\n  \'seresnext50_cpn\': {\'backbone\': seresnet_cpn.xt_cascaded_pyramid_net, \'logs_sub_dir\': \'logs_sext_cpn\'},\n  \'detnext50_cpn\': {\'backbone\': detxt_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_detxt_cpn\'},\n  \'large_seresnext_cpn\': {\'backbone\': lambda inputs, output_channals, heatmap_size, istraining, data_format : seresnet_cpn.xt_cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=101),\n                        \'logs_sub_dir\': \'logs_large_sext_cpn\'},\n  \'large_detnext_cpn\': {\'backbone\': lambda inputs, output_channals, heatmap_size, istraining, data_format : detxt_cpn.cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=101),\n                        \'logs_sub_dir\': \'logs_large_detxt_cpn\'},\n  \'simple_net\': {\'backbone\': lambda inputs, output_channals, heatmap_size, istraining, data_format : simple_xt.simple_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=101),\n                        \'logs_sub_dir\': \'logs_simple_net\'},\n  \'head_seresnext50_cpn\': {\'backbone\': seresnet_cpn.head_xt_cascaded_pyramid_net, \'logs_sub_dir\': \'logs_head_sext_cpn\'},\n}\n\ndef input_pipeline(model_scope=FLAGS.model_scope):\n    preprocessing_fn = lambda org_image, file_name, shape: preprocessing.preprocess_for_test_raw_output(org_image, file_name, shape, FLAGS.train_image_size, FLAGS.train_image_size, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, file_name, classid, offsets = dataset.slim_test_get_split(FLAGS.data_dir, None, FLAGS.num_readers, FLAGS.num_preprocessing_threads, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None, dynamic_pad=True)\n\n    return {\'images\': images, \'shape\': shape, \'classid\': classid, \'file_name\': file_name, \'pred_offsets\': offsets}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, heatmap, predictions, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 120\n      #print(img_to_save)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(heatmap[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(heatmap[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(heatmap[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'with_heatmap_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.EVAL_DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      predictions = np.array(predictions.tolist())\n      #print(predictions.shape)\n      for ind in range(predictions.shape[0]):\n        img = predictions[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.EVAL_DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, predictions, heatmap_size, height, width, category, clip_at_zero=False, data_format=\'channels_last\', name=None):\n    # expand_border = 10\n    # pad_pred = tf.pad(predictions, tf.constant([[0, 0], [0, 0], [expand_border, expand_border], [expand_border, expand_border]]),\n    #               mode=\'CONSTANT\', name=\'pred_padding\', constant_values=0)\n\n    # blur_pred = gaussian_blur(pad_pred, config.class_num_joints[category], 3.5, \'channels_first\', \'pred_blur\')\n\n    # predictions = tf.slice(blur_pred, [0, 0, expand_border, expand_border], [1, config.class_num_joints[category], heatmap_size, heatmap_size])\n\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_max_indices = tf.argmax(predictions, axis=-1)\n    pred_max_x, pred_max_y = tf.cast(tf.floormod(pred_max_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_max_indices, heatmap_size), tf.float32)\n    # mask the max elements to zero\n    mask_predictions = predictions * tf.one_hot(pred_max_indices, heatmap_size*heatmap_size, on_value=0., off_value=1., dtype=tf.float32)\n    # get the second max prediction\n    pred_next_max = tf.reduce_max(mask_predictions, axis=-1)\n    pred_next_max_indices = tf.argmax(mask_predictions, axis=-1)\n    pred_next_max_x, pred_next_max_y = tf.cast(tf.floormod(pred_next_max_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_next_max_indices, heatmap_size), tf.float32)\n\n    dist = tf.pow(tf.pow(pred_next_max_x - pred_max_x, 2.) + tf.pow(pred_next_max_y - pred_max_y, 2.), .5)\n\n    pred_x = tf.where(dist < 1e-3, pred_max_x, pred_max_x + (pred_next_max_x - pred_max_x) * 0.25 / dist)\n    pred_y = tf.where(dist < 1e-3, pred_max_y, pred_max_y + (pred_next_max_y - pred_max_y) * 0.25 / dist)\n\n    pred_indices_ = tf.squeeze(tf.cast(pred_x, tf.int64) + tf.cast(pred_y, tf.int64) * heatmap_size)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    width_ratio, height_ratio = width / tf.cast(heatmap_size, tf.float32), height / tf.cast(heatmap_size, tf.float32)\n\n    pred_x, pred_y = pred_x * width_ratio, pred_y * height_ratio\n    #pred_x, pred_y = pred_x * width_ratio + width_ratio/2., pred_y * height_ratio + height_ratio/2.\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=255, off_value=0, axis=-1, dtype=tf.int32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  pred_heatmap,\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\nbackbone_ = all_models[FLAGS.backbone.strip()][\'backbone\']\n\ndef keypoint_model_fn(features, labels, mode, params):\n    #print(features)\n    shape = features[\'shape\']\n    classid = features[\'classid\']\n    file_name = features[\'file_name\']\n    features = features[\'images\']\n\n    file_name = tf.identity(file_name, name=\'current_file\')\n\n    image = preprocessing.preprocess_for_test_raw_output(features, params[\'train_image_size\'], params[\'train_image_size\'], data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), scope=\'first_stage\')\n\n    if not params[\'flip_on_test\']:\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[image], reuse=tf.AUTO_REUSE):\n            pred_outputs = backbone_(image, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n        if params[\'data_format\'] == \'channels_last\':\n            pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n        pred_x_first_stage, pred_y_first_stage = get_keypoint(image, pred_outputs[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n    else:\n        # test augumentation on the fly\n        if params[\'data_format\'] == \'channels_last\':\n            double_features = tf.reshape(tf.stack([image, tf.map_fn(tf.image.flip_left_right, image, back_prop=False)], axis = 1), [-1, params[\'train_image_size\'], params[\'train_image_size\'], 3])\n        else:\n            double_features = tf.reshape(tf.stack([image, tf.transpose(tf.map_fn(tf.image.flip_left_right, tf.transpose(image, [0, 2, 3, 1], name=\'nchw2nhwc\'), back_prop=False), [0, 3, 1, 2], name=\'nhwc2nchw\')], axis = 1), [-1, 3, params[\'train_image_size\'], params[\'train_image_size\']])\n\n        num_joints = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')]\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[double_features], reuse=tf.AUTO_REUSE):\n            pred_outputs = backbone_(double_features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n        if params[\'data_format\'] == \'channels_last\':\n            pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n        row_indices = tf.tile(tf.reshape(tf.stack([tf.range(0, tf.shape(double_features)[0], delta=2), tf.range(1, tf.shape(double_features)[0], delta=2)], axis=0), [-1, 1]), [1, num_joints])\n        col_indices = tf.reshape(tf.tile(tf.reshape(tf.stack([tf.range(num_joints), tf.constant(config.left_right_remap[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')])], axis=0), [2, -1]), [1, tf.shape(features)[0]]), [-1, num_joints])\n        flip_indices=tf.stack([row_indices, col_indices], axis=-1)\n\n        #flip_indices = tf.Print(flip_indices, [flip_indices], summarize=500)\n        pred_outputs = [tf.gather_nd(pred_outputs[ind], flip_indices, name=\'gather_nd_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n        def cond_flip(heatmap_ind):\n            return tf.cond(heatmap_ind[1] < tf.shape(features)[0], lambda : heatmap_ind[0], lambda : tf.transpose(tf.image.flip_left_right(tf.transpose(heatmap_ind[0], [1, 2, 0], name=\'pred_nchw2nhwc\')), [2, 0, 1], name=\'pred_nhwc2nchw\'))\n        # all the heatmap of the fliped image should also be fliped back\n        pred_outputs = [tf.map_fn(cond_flip, [pred_outputs[ind], tf.range(tf.shape(double_features)[0])], dtype=tf.float32, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=\'map_fn_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n        pred_outputs = [tf.split(_, 2) for _ in pred_outputs]\n        pred_outputs_1 = [_[0] for _ in pred_outputs]\n        pred_outputs_2 = [_[1] for _ in pred_outputs]\n        pred_x_first_stage1, pred_y_first_stage1 = get_keypoint(image, pred_outputs_1[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n        pred_x_first_stage2, pred_y_first_stage2 = get_keypoint(image, pred_outputs_2[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n\n        dist = tf.pow(tf.pow(pred_x_first_stage1 - pred_x_first_stage2, 2.) + tf.pow(pred_y_first_stage1 - pred_y_first_stage2, 2.), .5)\n\n        pred_x_first_stage = tf.where(dist < 1e-3, pred_x_first_stage1, pred_x_first_stage1 + (pred_x_first_stage2 - pred_x_first_stage1) * 0.25 / dist)\n        pred_y_first_stage = tf.where(dist < 1e-3, pred_y_first_stage1, pred_y_first_stage1 + (pred_y_first_stage2 - pred_y_first_stage1) * 0.25 / dist)\n\n    xmin = tf.cast(tf.reduce_min(pred_x_first_stage), tf.int64)\n    xmax = tf.cast(tf.reduce_max(pred_x_first_stage), tf.int64)\n    ymin = tf.cast(tf.reduce_min(pred_y_first_stage), tf.int64)\n    ymax = tf.cast(tf.reduce_max(pred_y_first_stage), tf.int64)\n\n    xmin, ymin, xmax, ymax = xmin - 100, ymin - 80, xmax + 100, ymax + 80\n\n    xmin = tf.clip_by_value(xmin, 0, shape[0][1][0]-1)\n    ymin = tf.clip_by_value(ymin, 0, shape[0][0][0]-1)\n    xmax = tf.clip_by_value(xmax, 0, shape[0][1][0]-1)\n    ymax = tf.clip_by_value(ymax, 0, shape[0][0][0]-1)\n\n    bbox_h = ymax - ymin\n    bbox_w = xmax - xmin\n    areas = bbox_h * bbox_w\n\n    offsets=tf.stack([xmin, ymin], axis=0)\n    crop_shape = tf.stack([bbox_h, bbox_w, shape[0][2][0]], axis=0)\n\n    ymin, xmin, bbox_h, bbox_w = tf.cast(ymin, tf.int32), tf.cast(xmin, tf.int32), tf.cast(bbox_h, tf.int32), tf.cast(bbox_w, tf.int32)\n\n    single_image = tf.squeeze(features, [0])\n    crop_image = tf.image.crop_to_bounding_box(single_image, ymin, xmin, bbox_h, bbox_w)\n    crop_image = tf.expand_dims(crop_image, 0)\n\n    image, shape, offsets = tf.cond(areas > 0, lambda : (crop_image, crop_shape, offsets),\n                                    lambda : (features, shape, tf.constant([0, 0], tf.int64)))\n    offsets.set_shape([2])\n    offsets = tf.to_float(offsets)\n    shape = tf.reshape(shape, [1, 3])\n\n    image = preprocessing.preprocess_for_test_raw_output(image, params[\'train_image_size\'], params[\'train_image_size\'], data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), scope=\'second_stage\')\n\n    if not params[\'flip_on_test\']:\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[image], reuse=True):\n            pred_outputs = backbone_(image, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n        with tf.name_scope(""refine_prediction""):\n            if params[\'data_format\'] == \'channels_last\':\n                pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n            pred_x, pred_y = get_keypoint(image, pred_outputs[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n    else:\n        # test augumentation on the fly\n        with tf.name_scope(""refine_prediction""):\n            if params[\'data_format\'] == \'channels_last\':\n                double_features = tf.reshape(tf.stack([image, tf.map_fn(tf.image.flip_left_right, image, back_prop=False)], axis = 1), [-1, params[\'train_image_size\'], params[\'train_image_size\'], 3])\n            else:\n                double_features = tf.reshape(tf.stack([image, tf.transpose(tf.map_fn(tf.image.flip_left_right, tf.transpose(image, [0, 2, 3, 1], name=\'nchw2nhwc\'), back_prop=False), [0, 3, 1, 2], name=\'nhwc2nchw\')], axis = 1), [-1, 3, params[\'train_image_size\'], params[\'train_image_size\']])\n\n        num_joints = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')]\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[double_features], reuse=True):\n            pred_outputs = backbone_(double_features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n        with tf.name_scope(""refine_prediction""):\n            if params[\'data_format\'] == \'channels_last\':\n                pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n            row_indices = tf.tile(tf.reshape(tf.stack([tf.range(0, tf.shape(double_features)[0], delta=2), tf.range(1, tf.shape(double_features)[0], delta=2)], axis=0), [-1, 1]), [1, num_joints])\n            col_indices = tf.reshape(tf.tile(tf.reshape(tf.stack([tf.range(num_joints), tf.constant(config.left_right_remap[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')])], axis=0), [2, -1]), [1, tf.shape(features)[0]]), [-1, num_joints])\n            flip_indices=tf.stack([row_indices, col_indices], axis=-1)\n\n            #flip_indices = tf.Print(flip_indices, [flip_indices], summarize=500)\n            pred_outputs = [tf.gather_nd(pred_outputs[ind], flip_indices, name=\'gather_nd_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n            def cond_flip(heatmap_ind):\n                return tf.cond(heatmap_ind[1] < tf.shape(features)[0], lambda : heatmap_ind[0], lambda : tf.transpose(tf.image.flip_left_right(tf.transpose(heatmap_ind[0], [1, 2, 0], name=\'pred_nchw2nhwc\')), [2, 0, 1], name=\'pred_nhwc2nchw\'))\n            # all the heatmap of the fliped image should also be fliped back\n            pred_outputs = [tf.map_fn(cond_flip, [pred_outputs[ind], tf.range(tf.shape(double_features)[0])], dtype=tf.float32, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=\'map_fn_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n            pred_outputs = [tf.split(_, 2) for _ in pred_outputs]\n            pred_outputs_1 = [_[0] for _ in pred_outputs]\n            pred_outputs_2 = [_[1] for _ in pred_outputs]\n            #pred_outputs_1[-1] = tf.Print(pred_outputs_1[-1], [pred_outputs_1[-1]], summarize=10000)\n            pred_x_first_stage1, pred_y_first_stage1 = get_keypoint(image, pred_outputs_1[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n            pred_x_first_stage2, pred_y_first_stage2 = get_keypoint(image, pred_outputs_2[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n\n            dist = tf.pow(tf.pow(pred_x_first_stage1 - pred_x_first_stage2, 2.) + tf.pow(pred_y_first_stage1 - pred_y_first_stage2, 2.), .5)\n\n            pred_x = tf.where(dist < 1e-3, pred_x_first_stage1, pred_x_first_stage1 + (pred_x_first_stage2 - pred_x_first_stage1) * 0.25 / dist)\n            pred_y = tf.where(dist < 1e-3, pred_y_first_stage1, pred_y_first_stage1 + (pred_y_first_stage2 - pred_y_first_stage1) * 0.25 / dist)\n    # for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):#TRAINABLE_VARIABLES):\n    #   print(var.op.name)\n\n    predictions = {\'pred_x\': pred_x + offsets[0], \'pred_y\': pred_y + offsets[1], \'file_name\': file_name}\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                              mode=mode,\n                              predictions=predictions,\n                              loss=None, train_op=None)\n    else:\n        raise ValueError(\'Only ""PREDICT"" mode is supported.\')\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef eval_each(model_fn, model_dir, model_scope, run_config):\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'model_scope\': model_scope,\n            \'flip_on_test\': FLAGS.flip_on_test,\n        })\n    #tf.logging.info(\'params recv: %s\', FLAGS.flag_values_dict())\n\n    tensors_to_log = {\n        \'cur_file\': \'current_file\'\n    }\n\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \', \'.join([\'%s=%s\' % (k, v) for k, v in dicts.items()]))\n    tf.logging.info(\'Starting to predict model {}.\'.format(model_scope))\n    pred_results = fashionAI.predict(input_fn=lambda : input_pipeline(model_scope), hooks=[logging_hook], checkpoint_path=train_helper.get_latest_checkpoint_for_evaluate_(model_dir, model_dir))\n    #tf.logging.info()\n    return list(pred_results)\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=None).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    model_to_eval = [s.strip() for s in FLAGS.model_to_eval.split(\',\')]\n\n    full_model_dir = os.path.join(FLAGS.model_dir, all_models[FLAGS.backbone.strip()][\'logs_sub_dir\'])\n    for m in model_to_eval:\n        if m == \'\': continue\n        pred_results = eval_each(keypoint_model_fn, os.path.join(full_model_dir, m), m, run_config)\n        #print(pred_results)\n        # collect result\n        df = pd.DataFrame(columns=[\'image_id\', \'image_category\'] + config.all_keys)\n        cur_record = 0\n        gloabl2local_ind = dict(zip(config.class2global_ind_map[m], list(range(len(config.class2global_ind_map[m]))) ))\n        #print(gloabl2local_ind)\n        for pred_item in pred_results:\n            temp_list = []\n            index = 0\n            x = pred_item[\'pred_x\'].tolist()\n            y = pred_item[\'pred_y\'].tolist()\n            filename = pred_item[\'file_name\'].decode(\'utf8\')\n            for ind in list(range(config.class_num_joints[\'*\'])):\n                if ind in gloabl2local_ind:\n                    temp_list.append(\'{}_{}_1\'.format(round(x[gloabl2local_ind[ind]]), round(y[gloabl2local_ind[ind]])))\n                else:\n                    temp_list.append(\'-1_-1_-1\')\n            #Images/blouse/ab669925e96490ec698af976586f0b2f.jpg\n            df.loc[cur_record] = [filename, m] + temp_list\n            cur_record = cur_record + 1\n        df.to_csv(\'./{}_{}.csv\'.format(FLAGS.backbone.strip(), m), encoding=\'utf-8\', index=False)\n\n    # merge dataframe\n    df_list = [pd.read_csv(\'./{}_{}.csv\'.format(FLAGS.backbone.strip(), model_to_eval[0]), encoding=\'utf-8\')]\n    for m in model_to_eval[1:]:\n        if m == \'\': continue\n        df_list.append(pd.read_csv(\'./{}_{}.csv\'.format(FLAGS.backbone.strip(), m), encoding=\'utf-8\'))\n\n    time_stamps = int(time.time())\n    pd.concat(df_list, ignore_index=True).to_csv(\'./{}_sub_{}.csv\'.format(FLAGS.backbone.strip(), time_stamps), encoding=\'utf-8\', index=False)\n\n    if FLAGS.run_on_cloud:\n        tf.gfile.Copy(\'./{}_sub_{}.csv\'.format(FLAGS.backbone.strip(), time_stamps), os.path.join(full_model_dir, \'{}_sub_{}.csv\'.format(FLAGS.backbone.strip(), time_stamps)), overwrite=True)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
eval_all_cpn_simple.py,83,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import detnet_cpn\nfrom net import detxt_cpn\nfrom net import seresnet_cpn\nfrom net import cpn\nfrom net import simple_xt\n\nfrom utility import train_helper\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n#--num_readers=2 --num_preprocessing_threads=2 --data_dir=/media/disk/keypoint/tfrecords --model_to_train=all, blouse\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords_test_stage2\',#tfrecords_test tfrecords_test_stage1_b tfrecords_test_stage2\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_*.tfrecord\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'.\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_string(\n    \'backbone\', \'detnet50_cpn\',\n    \'The backbone network to use for feature extraction.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_last\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_boolean(\n    \'flip_on_test\', True,\n    \'Wether we will average predictions of left-right fliped image.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', \'blouse\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', False,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_eval\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to eval (comma-separated list).\')\n\n#--model_scope=blouse --checkpoint_path=./logs/blouse\nFLAGS = tf.app.flags.FLAGS\n\nall_models = {\n  \'resnet50_cpn\': {\'backbone\': cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_cpn\'},\n  \'detnet50_cpn\': {\'backbone\': detnet_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_detnet_cpn\'},\n  \'seresnet50_cpn\': {\'backbone\': seresnet_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_se_cpn\'},\n  \'seresnext50_cpn\': {\'backbone\': seresnet_cpn.xt_cascaded_pyramid_net, \'logs_sub_dir\': \'logs_sext_cpn\'},\n  \'detnext50_cpn\': {\'backbone\': detxt_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'logs_detxt_cpn\'},\n  \'large_seresnext_cpn\': {\'backbone\': lambda inputs, output_channals, heatmap_size, istraining, data_format : seresnet_cpn.xt_cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=101),\n                        \'logs_sub_dir\': \'logs_large_sext_cpn\'},\n  \'large_detnext_cpn\': {\'backbone\': lambda inputs, output_channals, heatmap_size, istraining, data_format : detxt_cpn.cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=101),\n                        \'logs_sub_dir\': \'logs_large_detxt_cpn\'},\n  \'simple_net\': {\'backbone\': lambda inputs, output_channals, heatmap_size, istraining, data_format : simple_xt.simple_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=101),\n                        \'logs_sub_dir\': \'logs_simple_net\'},\n  \'head_seresnext50_cpn\': {\'backbone\': seresnet_cpn.head_xt_cascaded_pyramid_net, \'logs_sub_dir\': \'logs_head_sext_cpn\'},\n}\n\ndef input_pipeline(model_scope=FLAGS.model_scope):\n    preprocessing_fn = lambda org_image, file_name, shape: preprocessing.preprocess_for_test_raw_output(org_image, file_name, shape, FLAGS.train_image_size, FLAGS.train_image_size, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, file_name, classid, offsets = dataset.slim_test_get_split(FLAGS.data_dir, None, FLAGS.num_readers, FLAGS.num_preprocessing_threads, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None, dynamic_pad=True)\n\n    return {\'images\': images, \'shape\': shape, \'classid\': classid, \'file_name\': file_name, \'pred_offsets\': offsets}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, heatmap, predictions, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 120\n      #print(img_to_save)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(heatmap[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(heatmap[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(heatmap[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'with_heatmap_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.EVAL_DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      predictions = np.array(predictions.tolist())\n      #print(predictions.shape)\n      for ind in range(predictions.shape[0]):\n        img = predictions[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.EVAL_DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, predictions, heatmap_size, height, width, category, clip_at_zero=False, data_format=\'channels_last\', name=None):\n    # expand_border = 10\n    # pad_pred = tf.pad(predictions, tf.constant([[0, 0], [0, 0], [expand_border, expand_border], [expand_border, expand_border]]),\n    #               mode=\'CONSTANT\', name=\'pred_padding\', constant_values=0)\n\n    # blur_pred = gaussian_blur(pad_pred, config.class_num_joints[category], 3.5, \'channels_first\', \'pred_blur\')\n\n    # predictions = tf.slice(blur_pred, [0, 0, expand_border, expand_border], [1, config.class_num_joints[category], heatmap_size, heatmap_size])\n\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_max_indices = tf.argmax(predictions, axis=-1)\n    pred_max_x, pred_max_y = tf.cast(tf.floormod(pred_max_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_max_indices, heatmap_size), tf.float32)\n    # mask the max elements to zero\n    mask_predictions = predictions * tf.one_hot(pred_max_indices, heatmap_size*heatmap_size, on_value=0., off_value=1., dtype=tf.float32)\n    # get the second max prediction\n    pred_next_max = tf.reduce_max(mask_predictions, axis=-1)\n    pred_next_max_indices = tf.argmax(mask_predictions, axis=-1)\n    pred_next_max_x, pred_next_max_y = tf.cast(tf.floormod(pred_next_max_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_next_max_indices, heatmap_size), tf.float32)\n\n    dist = tf.pow(tf.pow(pred_next_max_x - pred_max_x, 2.) + tf.pow(pred_next_max_y - pred_max_y, 2.), .5)\n\n    pred_x = tf.where(dist < 1e-3, pred_max_x, pred_max_x + (pred_next_max_x - pred_max_x) * 0.25 / dist)\n    pred_y = tf.where(dist < 1e-3, pred_max_y, pred_max_y + (pred_next_max_y - pred_max_y) * 0.25 / dist)\n\n    pred_indices_ = tf.squeeze(tf.cast(pred_x, tf.int64) + tf.cast(pred_y, tf.int64) * heatmap_size)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    width_ratio, height_ratio = width / tf.cast(heatmap_size, tf.float32), height / tf.cast(heatmap_size, tf.float32)\n\n    pred_x, pred_y = pred_x * width_ratio, pred_y * height_ratio\n    #pred_x, pred_y = pred_x * width_ratio + width_ratio/2., pred_y * height_ratio + height_ratio/2.\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=255, off_value=0, axis=-1, dtype=tf.int32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  pred_heatmap,\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\nbackbone_ = all_models[FLAGS.backbone.strip()][\'backbone\']\n\ndef keypoint_model_fn(features, labels, mode, params):\n    #print(features)\n    shape = features[\'shape\']\n    classid = features[\'classid\']\n    file_name = features[\'file_name\']\n    features = features[\'images\']\n\n    file_name = tf.identity(file_name, name=\'current_file\')\n\n    image = preprocessing.preprocess_for_test_raw_output(features, params[\'train_image_size\'], params[\'train_image_size\'], data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), scope=\'first_stage\')\n\n    if not params[\'flip_on_test\']:\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[image], reuse=tf.AUTO_REUSE):\n            pred_outputs = backbone_(image, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n        if params[\'data_format\'] == \'channels_last\':\n            pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n        pred_x, pred_y = get_keypoint(image, pred_outputs[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n    else:\n        # test augumentation on the fly\n        if params[\'data_format\'] == \'channels_last\':\n            double_features = tf.reshape(tf.stack([image, tf.map_fn(tf.image.flip_left_right, image, back_prop=False)], axis = 1), [-1, params[\'train_image_size\'], params[\'train_image_size\'], 3])\n        else:\n            double_features = tf.reshape(tf.stack([image, tf.transpose(tf.map_fn(tf.image.flip_left_right, tf.transpose(image, [0, 2, 3, 1], name=\'nchw2nhwc\'), back_prop=False), [0, 3, 1, 2], name=\'nhwc2nchw\')], axis = 1), [-1, 3, params[\'train_image_size\'], params[\'train_image_size\']])\n\n        num_joints = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')]\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[double_features], reuse=tf.AUTO_REUSE):\n            pred_outputs = backbone_(double_features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n        if params[\'data_format\'] == \'channels_last\':\n            pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n        row_indices = tf.tile(tf.reshape(tf.stack([tf.range(0, tf.shape(double_features)[0], delta=2), tf.range(1, tf.shape(double_features)[0], delta=2)], axis=0), [-1, 1]), [1, num_joints])\n        col_indices = tf.reshape(tf.tile(tf.reshape(tf.stack([tf.range(num_joints), tf.constant(config.left_right_remap[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')])], axis=0), [2, -1]), [1, tf.shape(features)[0]]), [-1, num_joints])\n        flip_indices=tf.stack([row_indices, col_indices], axis=-1)\n\n        #flip_indices = tf.Print(flip_indices, [flip_indices], summarize=500)\n        pred_outputs = [tf.gather_nd(pred_outputs[ind], flip_indices, name=\'gather_nd_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n        def cond_flip(heatmap_ind):\n            return tf.cond(heatmap_ind[1] < tf.shape(features)[0], lambda : heatmap_ind[0], lambda : tf.transpose(tf.image.flip_left_right(tf.transpose(heatmap_ind[0], [1, 2, 0], name=\'pred_nchw2nhwc\')), [2, 0, 1], name=\'pred_nhwc2nchw\'))\n        # all the heatmap of the fliped image should also be fliped back\n        pred_outputs = [tf.map_fn(cond_flip, [pred_outputs[ind], tf.range(tf.shape(double_features)[0])], dtype=tf.float32, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=\'map_fn_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n        pred_outputs = [tf.split(_, 2) for _ in pred_outputs]\n        pred_outputs_1 = [_[0] for _ in pred_outputs]\n        pred_outputs_2 = [_[1] for _ in pred_outputs]\n        pred_x_first_stage1, pred_y_first_stage1 = get_keypoint(image, pred_outputs_1[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n        pred_x_first_stage2, pred_y_first_stage2 = get_keypoint(image, pred_outputs_2[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=False, data_format=params[\'data_format\'])\n\n        dist = tf.pow(tf.pow(pred_x_first_stage1 - pred_x_first_stage2, 2.) + tf.pow(pred_y_first_stage1 - pred_y_first_stage2, 2.), .5)\n\n        pred_x = tf.where(dist < 1e-3, pred_x_first_stage1, pred_x_first_stage1 + (pred_x_first_stage2 - pred_x_first_stage1) * 0.25 / dist)\n        pred_y = tf.where(dist < 1e-3, pred_y_first_stage1, pred_y_first_stage1 + (pred_y_first_stage2 - pred_y_first_stage1) * 0.25 / dist)\n\n    # for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):#TRAINABLE_VARIABLES):\n    #   print(var.op.name)\n\n    predictions = {\'pred_x\': pred_x, \'pred_y\': pred_y, \'file_name\': file_name}\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                              mode=mode,\n                              predictions=predictions,\n                              loss=None, train_op=None)\n    else:\n        raise ValueError(\'Only ""PREDICT"" mode is supported.\')\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef eval_each(model_fn, model_dir, model_scope, run_config):\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'model_scope\': model_scope,\n            \'flip_on_test\': FLAGS.flip_on_test,\n        })\n    #tf.logging.info(\'params recv: %s\', FLAGS.flag_values_dict())\n\n    tensors_to_log = {\n        \'cur_file\': \'current_file\'\n    }\n\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \', \'.join([\'%s=%s\' % (k, v) for k, v in dicts.items()]))\n    tf.logging.info(\'Starting to predict model {}.\'.format(model_scope))\n    pred_results = fashionAI.predict(input_fn=lambda : input_pipeline(model_scope), hooks=[logging_hook], checkpoint_path=train_helper.get_latest_checkpoint_for_evaluate_(model_dir, model_dir))\n    #tf.logging.info()\n    return list(pred_results)\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=None).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    model_to_eval = [s.strip() for s in FLAGS.model_to_eval.split(\',\')]\n\n    full_model_dir = os.path.join(FLAGS.model_dir, all_models[FLAGS.backbone.strip()][\'logs_sub_dir\'])\n    for m in model_to_eval:\n        if m == \'\': continue\n        pred_results = eval_each(keypoint_model_fn, os.path.join(full_model_dir, m), m, run_config)\n        #print(pred_results)\n        # collect result\n        df = pd.DataFrame(columns=[\'image_id\', \'image_category\'] + config.all_keys)\n        cur_record = 0\n        gloabl2local_ind = dict(zip(config.class2global_ind_map[m], list(range(len(config.class2global_ind_map[m]))) ))\n        #print(gloabl2local_ind)\n        for pred_item in pred_results:\n            temp_list = []\n            index = 0\n            x = pred_item[\'pred_x\'].tolist()\n            y = pred_item[\'pred_y\'].tolist()\n            filename = pred_item[\'file_name\'].decode(\'utf8\')\n            for ind in list(range(config.class_num_joints[\'*\'])):\n                if ind in gloabl2local_ind:\n                    temp_list.append(\'{}_{}_1\'.format(round(x[gloabl2local_ind[ind]]), round(y[gloabl2local_ind[ind]])))\n                else:\n                    temp_list.append(\'-1_-1_-1\')\n            #Images/blouse/ab669925e96490ec698af976586f0b2f.jpg\n            df.loc[cur_record] = [filename, m] + temp_list\n            cur_record = cur_record + 1\n        df.to_csv(\'./{}_{}.csv\'.format(FLAGS.backbone.strip(), m), encoding=\'utf-8\', index=False)\n\n    # merge dataframe\n    df_list = [pd.read_csv(\'./{}_{}.csv\'.format(FLAGS.backbone.strip(), model_to_eval[0]), encoding=\'utf-8\')]\n    for m in model_to_eval[1:]:\n        if m == \'\': continue\n        df_list.append(pd.read_csv(\'./{}_{}.csv\'.format(FLAGS.backbone.strip(), m), encoding=\'utf-8\'))\n    pd.concat(df_list, ignore_index=True).to_csv(\'./{}_sub.csv\'.format(FLAGS.backbone.strip()), encoding=\'utf-8\', index=False)\n\n    if FLAGS.run_on_cloud:\n        tf.gfile.Copy(\'./{}_sub.csv\'.format(FLAGS.backbone.strip()), os.path.join(full_model_dir, \'{}_sub.csv\'.format(FLAGS.backbone.strip())), overwrite=True)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
eval_hg_subnet.py,154,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import hourglass as hg\nfrom utility import train_helper\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n#--num_readers=2 --num_preprocessing_threads=2 --data_dir=/media/disk/keypoint/tfrecords --model_to_train=all, blouse\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords_test_stage1_b\',# tfrecords_test_stage1_b tfrecords_test\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_*.tfrecord\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_hg/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_integer(\'feats_channals\', 256, \'Number of features in the hourglass.\')\ntf.app.flags.DEFINE_integer(\'num_stacks\', 4, \'Number of hourglasses to stack.\')#8\ntf.app.flags.DEFINE_integer(\'num_modules\', 1, \'Number of residual modules at each location in the hourglass.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_last\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180406, \'Random seed for TensorFlow initializers.\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_boolean(\n    \'flip_on_test\', False,\n    \'Wether we will average predictions of left-right fliped image.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', \'all\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_eval\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to eval (comma-separated list).\')\n\n#--model_scope=blouse --checkpoint_path=./logs/blouse\nFLAGS = tf.app.flags.FLAGS\n\ndef input_pipeline(model_scope=FLAGS.model_scope):\n    #preprocessing_fn = lambda org_image, shape: preprocessing.preprocess_for_test(org_image, shape, FLAGS.train_image_size, FLAGS.train_image_size, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n    preprocessing_fn = lambda org_image, file_name, shape: preprocessing.preprocess_for_test_raw_output(org_image, file_name, shape, FLAGS.train_image_size, FLAGS.train_image_size, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, file_name, classid, offsets = dataset.slim_test_get_split(FLAGS.data_dir, None, FLAGS.num_readers, FLAGS.num_preprocessing_threads, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None, dynamic_pad=True)\n\n    return {\'images\': images, \'shape\': shape, \'classid\': classid, \'file_name\': file_name, \'pred_offsets\': offsets}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, heatmap, predictions, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 120\n      #print(img_to_save)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(heatmap[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(heatmap[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(heatmap[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'with_heatmap_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.EVAL_DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      predictions = np.array(predictions.tolist())\n      #print(predictions.shape)\n      for ind in range(predictions.shape[0]):\n        img = predictions[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.EVAL_DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\ndef get_keypoint(image, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    # expand_border = 10\n\n    # pad_pred = tf.pad(predictions, tf.constant([[0, 0], [0, 0], [expand_border, expand_border], [expand_border, expand_border]]),\n    #               mode=\'CONSTANT\', name=\'pred_padding\', constant_values=0)\n\n    # blur_pred = gaussian_blur(pad_pred, config.class_num_joints[category], 3.5, \'channels_first\', \'pred_blur\')\n\n    # predictions = tf.slice(blur_pred, [0, 0, expand_border, expand_border], [1, config.class_num_joints[category], heatmap_size, heatmap_size])\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_max_indices = tf.argmax(predictions, axis=-1)\n    pred_max_x, pred_max_y = tf.cast(tf.floormod(pred_max_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_max_indices, heatmap_size), tf.float32)\n    # mask the max elements to zero\n    mask_predictions = predictions * tf.one_hot(pred_max_indices, heatmap_size*heatmap_size, on_value=0., off_value=1., dtype=tf.float32)\n    # get the second max prediction\n    pred_next_max = tf.reduce_max(mask_predictions, axis=-1)\n    pred_next_max_indices = tf.argmax(mask_predictions, axis=-1)\n    pred_next_max_x, pred_next_max_y = tf.cast(tf.floormod(pred_next_max_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_next_max_indices, heatmap_size), tf.float32)\n\n    dist = tf.pow(tf.pow(pred_next_max_x - pred_max_x, 2.) + tf.pow(pred_next_max_y - pred_max_y, 2.), .5)\n\n    pred_x = tf.where(dist < 1e-3, pred_max_x, pred_max_x + (pred_next_max_x - pred_max_x) * 0.25 / dist)\n    pred_y = tf.where(dist < 1e-3, pred_max_y, pred_max_y + (pred_next_max_y - pred_max_y) * 0.25 / dist)\n\n    pred_indices_ = tf.squeeze(tf.cast(pred_x, tf.int64) + tf.cast(pred_y, tf.int64) * heatmap_size)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    width_ratio, height_ratio = width / tf.cast(heatmap_size, tf.float32), height / tf.cast(heatmap_size, tf.float32)\n\n    pred_x, pred_y = pred_x * width_ratio, pred_y * height_ratio\n    #pred_x, pred_y = pred_x * width_ratio + width_ratio/2., pred_y * height_ratio + height_ratio/2.\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=255, off_value=0, axis=-1, dtype=tf.int32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  pred_heatmap,\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef get_keypoint_v0(image, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=255, off_value=0, axis=-1, dtype=tf.int32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  pred_heatmap,\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef keypoint_model_fn(features, labels, mode, params):\n    #print(features)\n    shape = features[\'shape\']\n    classid = features[\'classid\']\n    pred_offsets = tf.to_float(features[\'pred_offsets\'])\n    file_name = features[\'file_name\']\n    features = features[\'images\']\n\n    file_name = tf.identity(file_name, name=\'current_file\')\n\n    image = preprocessing.preprocess_for_test_raw_output(features, params[\'train_image_size\'], params[\'train_image_size\'], data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), scope=\'first_stage\')\n\n    if not params[\'flip_on_test\']:\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[image], reuse=tf.AUTO_REUSE):\n            pred_outputs = hg.create_model(image, params[\'num_stacks\'], params[\'feats_channals\'],\n                                config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'num_modules\'],\n                                (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n        if params[\'data_format\'] == \'channels_last\':\n            pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n        pred_x_first_stage, pred_y_first_stage = get_keypoint(image, pred_outputs[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n    else:\n        # test augumentation on the fly\n        if params[\'data_format\'] == \'channels_last\':\n            double_features = tf.reshape(tf.stack([image, tf.map_fn(tf.image.flip_left_right, image, back_prop=False)], axis = 1), [-1, params[\'train_image_size\'], params[\'train_image_size\'], 3])\n        else:\n            double_features = tf.reshape(tf.stack([image, tf.transpose(tf.map_fn(tf.image.flip_left_right, tf.transpose(image, [0, 2, 3, 1], name=\'nchw2nhwc\'), back_prop=False), [0, 3, 1, 2], name=\'nhwc2nchw\')], axis = 1), [-1, 3, params[\'train_image_size\'], params[\'train_image_size\']])\n\n        num_joints = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')]\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[double_features], reuse=tf.AUTO_REUSE):\n            pred_outputs = hg.create_model(double_features, params[\'num_stacks\'], params[\'feats_channals\'],\n                                config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'num_modules\'],\n                                (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n        if params[\'data_format\'] == \'channels_last\':\n            pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n        row_indices = tf.tile(tf.reshape(tf.stack([tf.range(0, tf.shape(double_features)[0], delta=2), tf.range(1, tf.shape(double_features)[0], delta=2)], axis=0), [-1, 1]), [1, num_joints])\n        col_indices = tf.reshape(tf.tile(tf.reshape(tf.stack([tf.range(num_joints), tf.constant(config.left_right_remap[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')])], axis=0), [2, -1]), [1, tf.shape(features)[0]]), [-1, num_joints])\n        flip_indices=tf.stack([row_indices, col_indices], axis=-1)\n\n        #flip_indices = tf.Print(flip_indices, [flip_indices], summarize=500)\n        pred_outputs = [tf.gather_nd(pred_outputs[ind], flip_indices, name=\'gather_nd_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n        def cond_flip(heatmap_ind):\n            return tf.cond(heatmap_ind[1] < tf.shape(features)[0], lambda : heatmap_ind[0], lambda : tf.transpose(tf.image.flip_left_right(tf.transpose(heatmap_ind[0], [1, 2, 0], name=\'pred_nchw2nhwc\')), [2, 0, 1], name=\'pred_nhwc2nchw\'))\n        # all the heatmap of the fliped image should also be fliped back\n        pred_outputs = [tf.map_fn(cond_flip, [pred_outputs[ind], tf.range(tf.shape(double_features)[0])], dtype=tf.float32, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=\'map_fn_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n        pred_outputs = [tf.split(_, 2) for _ in pred_outputs]\n        pred_outputs_1 = [_[0] for _ in pred_outputs]\n        pred_outputs_2 = [_[1] for _ in pred_outputs]\n        pred_x_first_stage1, pred_y_first_stage1 = get_keypoint(image, pred_outputs_1[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n        pred_x_first_stage2, pred_y_first_stage2 = get_keypoint(image, pred_outputs_2[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n        dist = tf.pow(tf.pow(pred_x_first_stage1 - pred_x_first_stage2, 2.) + tf.pow(pred_y_first_stage1 - pred_y_first_stage2, 2.), .5)\n\n        pred_x_first_stage = tf.where(dist < 1e-3, pred_x_first_stage1, pred_x_first_stage1 + (pred_x_first_stage2 - pred_x_first_stage1) * 0.25 / dist)\n        pred_y_first_stage = tf.where(dist < 1e-3, pred_y_first_stage1, pred_y_first_stage1 + (pred_y_first_stage2 - pred_y_first_stage1) * 0.25 / dist)\n\n    xmin = tf.cast(tf.reduce_min(pred_x_first_stage), tf.int64)\n    xmax = tf.cast(tf.reduce_max(pred_x_first_stage), tf.int64)\n    ymin = tf.cast(tf.reduce_min(pred_y_first_stage), tf.int64)\n    ymax = tf.cast(tf.reduce_max(pred_y_first_stage), tf.int64)\n\n    xmin, ymin, xmax, ymax = xmin - 100, ymin - 80, xmax + 100, ymax + 80\n\n    xmin = tf.clip_by_value(xmin, 0, shape[0][1][0]-1)\n    ymin = tf.clip_by_value(ymin, 0, shape[0][0][0]-1)\n    xmax = tf.clip_by_value(xmax, 0, shape[0][1][0]-1)\n    ymax = tf.clip_by_value(ymax, 0, shape[0][0][0]-1)\n\n    bbox_h = ymax - ymin\n    bbox_w = xmax - xmin\n    areas = bbox_h * bbox_w\n\n    offsets=tf.stack([xmin, ymin], axis=0)\n    crop_shape = tf.stack([bbox_h, bbox_w, shape[0][2][0]], axis=0)\n\n    ymin, xmin, bbox_h, bbox_w = tf.cast(ymin, tf.int32), tf.cast(xmin, tf.int32), tf.cast(bbox_h, tf.int32), tf.cast(bbox_w, tf.int32)\n\n    single_image = tf.squeeze(features, [0])\n    crop_image = tf.image.crop_to_bounding_box(single_image, ymin, xmin, bbox_h, bbox_w)\n    crop_image = tf.expand_dims(crop_image, 0)\n\n    image, shape, offsets = tf.cond(areas > 0, lambda : (crop_image, crop_shape, offsets),\n                                    lambda : (features, shape, tf.constant([0, 0], tf.int64)))\n    offsets.set_shape([2])\n    offsets = tf.to_float(offsets)\n    shape = tf.reshape(shape, [1, 3])\n\n    image = preprocessing.preprocess_for_test_raw_output(image, params[\'train_image_size\'], params[\'train_image_size\'], data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), scope=\'second_stage\')\n\n    if not params[\'flip_on_test\']:\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[image], reuse=True):\n            pred_outputs = hg.create_model(image, params[\'num_stacks\'], params[\'feats_channals\'],\n                                config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'num_modules\'],\n                                (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n        with tf.name_scope(""refine_prediction""):\n            if params[\'data_format\'] == \'channels_last\':\n                pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n            pred_x, pred_y = get_keypoint(image, pred_outputs[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n    else:\n        # test augumentation on the fly\n        with tf.name_scope(""refine_prediction""):\n            if params[\'data_format\'] == \'channels_last\':\n                double_features = tf.reshape(tf.stack([image, tf.map_fn(tf.image.flip_left_right, image, back_prop=False)], axis = 1), [-1, params[\'train_image_size\'], params[\'train_image_size\'], 3])\n            else:\n                double_features = tf.reshape(tf.stack([image, tf.transpose(tf.map_fn(tf.image.flip_left_right, tf.transpose(image, [0, 2, 3, 1], name=\'nchw2nhwc\'), back_prop=False), [0, 3, 1, 2], name=\'nhwc2nchw\')], axis = 1), [-1, 3, params[\'train_image_size\'], params[\'train_image_size\']])\n\n        num_joints = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')]\n        with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[double_features], reuse=True):\n            pred_outputs = hg.create_model(double_features, params[\'num_stacks\'], params[\'feats_channals\'],\n                                config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'num_modules\'],\n                                (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n        with tf.name_scope(""refine_prediction""):\n            if params[\'data_format\'] == \'channels_last\':\n                pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n            row_indices = tf.tile(tf.reshape(tf.stack([tf.range(0, tf.shape(double_features)[0], delta=2), tf.range(1, tf.shape(double_features)[0], delta=2)], axis=0), [-1, 1]), [1, num_joints])\n            col_indices = tf.reshape(tf.tile(tf.reshape(tf.stack([tf.range(num_joints), tf.constant(config.left_right_remap[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')])], axis=0), [2, -1]), [1, tf.shape(features)[0]]), [-1, num_joints])\n            flip_indices=tf.stack([row_indices, col_indices], axis=-1)\n\n            #flip_indices = tf.Print(flip_indices, [flip_indices], summarize=500)\n            pred_outputs = [tf.gather_nd(pred_outputs[ind], flip_indices, name=\'gather_nd_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n            def cond_flip(heatmap_ind):\n                return tf.cond(heatmap_ind[1] < tf.shape(features)[0], lambda : heatmap_ind[0], lambda : tf.transpose(tf.image.flip_left_right(tf.transpose(heatmap_ind[0], [1, 2, 0], name=\'pred_nchw2nhwc\')), [2, 0, 1], name=\'pred_nhwc2nchw\'))\n            # all the heatmap of the fliped image should also be fliped back\n            pred_outputs = [tf.map_fn(cond_flip, [pred_outputs[ind], tf.range(tf.shape(double_features)[0])], dtype=tf.float32, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=\'map_fn_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n            pred_outputs = [tf.split(_, 2) for _ in pred_outputs]\n            pred_outputs_1 = [_[0] for _ in pred_outputs]\n            pred_outputs_2 = [_[1] for _ in pred_outputs]\n            pred_x_first_stage1, pred_y_first_stage1 = get_keypoint(image, pred_outputs_1[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n            pred_x_first_stage2, pred_y_first_stage2 = get_keypoint(image, pred_outputs_2[-1], params[\'heatmap_size\'], shape[0][0], shape[0][1], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n            dist = tf.pow(tf.pow(pred_x_first_stage1 - pred_x_first_stage2, 2.) + tf.pow(pred_y_first_stage1 - pred_y_first_stage2, 2.), .5)\n\n            pred_x = tf.where(dist < 1e-3, pred_x_first_stage1, pred_x_first_stage1 + (pred_x_first_stage2 - pred_x_first_stage1) * 0.25 / dist)\n            pred_y = tf.where(dist < 1e-3, pred_y_first_stage1, pred_y_first_stage1 + (pred_y_first_stage2 - pred_y_first_stage1) * 0.25 / dist)\n    # for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):#TRAINABLE_VARIABLES):\n    #   print(var.op.name)\n\n    predictions = {\'pred_x\': pred_x + offsets[0], \'pred_y\': pred_y + offsets[1], \'file_name\': file_name}\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n                              mode=mode,\n                              predictions=predictions,\n                              loss=None, train_op=None)\n    else:\n        raise ValueError(\'Only ""PREDICT"" mode is supported.\')\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef eval_each(model_fn, model_dir, model_scope, run_config):\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'feats_channals\': FLAGS.feats_channals,\n            \'num_stacks\': FLAGS.num_stacks,\n            \'num_modules\': FLAGS.num_modules,\n            \'data_format\': FLAGS.data_format,\n            \'model_scope\': model_scope,\n            \'flip_on_test\': FLAGS.flip_on_test,\n        })\n    #tf.logging.info(\'params recv: %s\', FLAGS.flag_values_dict())\n\n    tensors_to_log = {\n        \'cur_file\': \'current_file\'\n    }\n\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \', \'.join([\'%s=%s\' % (k, v) for k, v in dicts.items()]))\n    tf.logging.info(\'Starting to predict model {}.\'.format(model_scope))\n    pred_results = fashionAI.predict(input_fn=lambda : input_pipeline(model_scope), hooks=[logging_hook], checkpoint_path=train_helper.get_latest_checkpoint_for_evaluate_(model_dir, model_dir))\n    #tf.logging.info()\n    return list(pred_results)\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=None).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    model_to_eval = [s.strip() for s in FLAGS.model_to_eval.split(\',\')]\n    for m in model_to_eval:\n        if m == \'\': continue\n        pred_results = eval_each(keypoint_model_fn, os.path.join(FLAGS.model_dir, m), m, run_config)\n        #print(pred_results)\n        # collect result\n        df = pd.DataFrame(columns=[\'image_id\', \'image_category\'] + config.all_keys)\n        cur_record = 0\n        gloabl2local_ind = dict(zip(config.class2global_ind_map[m], list(range(len(config.class2global_ind_map[m]))) ))\n        #print(gloabl2local_ind)\n        for pred_item in pred_results:\n            temp_list = []\n            index = 0\n            x = pred_item[\'pred_x\'].tolist()\n            y = pred_item[\'pred_y\'].tolist()\n            filename = pred_item[\'file_name\'].decode(\'utf8\')\n            for ind in list(range(config.class_num_joints[\'*\'])):\n                if ind in gloabl2local_ind:\n                    temp_list.append(\'{}_{}_1\'.format(round(x[gloabl2local_ind[ind]]), round(y[gloabl2local_ind[ind]])))\n                else:\n                    temp_list.append(\'-1_-1_-1\')\n            #Images/blouse/ab669925e96490ec698af976586f0b2f.jpg\n            df.loc[cur_record] = [filename, m] + temp_list\n            cur_record = cur_record + 1\n        df.to_csv(\'./{}.csv\'.format(m), encoding=\'utf-8\', index=False)\n\n    # merge dataframe\n    df_list = [pd.read_csv(\'./{}.csv\'.format(model_to_eval[0]), encoding=\'utf-8\')]\n    for m in model_to_eval[1:]:\n        if m == \'\': continue\n        df_list.append(pd.read_csv(\'./{}.csv\'.format(m), encoding=\'utf-8\'))\n    pd.concat(df_list, ignore_index=True).to_csv(\'./sub.csv\', encoding=\'utf-8\', index=False)\n\n    if FLAGS.run_on_cloud:\n        tf.gfile.Copy(\'./sub.csv\', os.path.join(FLAGS.model_dir, \'sub.csv\'), overwrite=True)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
inspect_checkpoint.py,0,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.python import pywrap_tensorflow\n\ndef print_tensors_in_checkpoint_file(file_name, tensor_name, all_tensors):\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        if all_tensors:\n            var_to_shape_map = reader.get_variable_to_shape_map()\n            for key in var_to_shape_map:\n                print(""tensor_name: "", key)\n                print(reader.get_tensor(key))\n        elif not tensor_name:\n            print(reader.debug_string().decode(""utf-8""))\n        else:\n            print(""tensor_name: "", tensor_name)\n            print(reader.get_tensor(tensor_name))\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(""It\'s likely that your checkpoint file has been compressed ""\n                  ""with SNAPPY."")\n\ndef print_all_tensors_name(file_name):\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        var_to_shape_map = reader.get_variable_to_shape_map()\n        for key in var_to_shape_map:\n            print(key)\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(""It\'s likely that your checkpoint file has been compressed ""\n                  ""with SNAPPY."")\n\nif __name__ == ""__main__"":\n    print_all_tensors_name(\'/media/rs/0E06CD1706CD0127/Kapok/Chi/fashionAI/Codes/model/seresnet50/se_resnet50.ckpt\')\n'"
run_local_mertric.py,0,"b'import os\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport math\n\nimport config as cfg\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\nparser = argparse.ArgumentParser(\n    description=\'The Normarlized Error Mertric Calculation For FashionAI Keypoint Detection Script.\')\ntrain_set = parser.add_mutually_exclusive_group()\nparser.add_argument(\'--prediction\', default=\'\',\n                    help=\'The path of file containing the prediction of keypoints.\')\nparser.add_argument(\'--cat\', type=lambda s: s.lower() in [\'True\', \'true\', \'t\', \'yes\', \'1\'], help=""whether print Normarlized Error for each catgory"")\nparser.add_argument(\'--gt\', default=\'./stage1_testb_gt.csv\',\n                    help=\'The path of file containing the ground truth of keypoints.\')\n\nargs = parser.parse_args()\n\ndef run():\n    if args.prediction.strip() == \'\' or args.gt.strip() == \'\':\n        parser.error(\'Must specify the file path of the prediction and ground truth.\')\n\n    pred_df = pd.read_csv(args.prediction, encoding=\'utf-8\')\n    gt_df = pd.read_csv(args.gt, encoding=\'utf-8\').set_index(\'image_id\')\n\n\n    num_v = 0.\n    sum_dist = 0.\n    for index, row in pred_df.iterrows():\n        gt = gt_df.loc[row[\'image_id\']]\n        img_cat = gt[\'image_category\']\n        gt_points = {}\n        pred_points = {}\n\n        for kp in cfg.all_keys:\n            pred_kp = row[kp].strip().split(\'_\')\n            gt_kp = gt[kp].strip().split(\'_\')\n            pred_points[kp] = [int(_) for _ in pred_kp]\n            gt_points[kp] = [int(_) for _ in gt_kp]\n\n        lnorm_name, rnorm_name = cfg.normalize_point_name[img_cat]\n        lnorm, rnorm = gt_points[lnorm_name][:-1], gt_points[rnorm_name][:-1]\n        norm_value = math.pow(math.pow(lnorm[0] - rnorm[0], 2.) + math.pow(lnorm[1] - rnorm[1], 2.), 0.5)\n\n\n        for kp in cfg.all_keys:\n            if gt_points[kp][-1] == -1 or norm_value < 1e-3:\n                continue\n            num_v += 1.\n\n            dist = math.pow(math.pow(pred_points[kp][0] - gt_points[kp][0], 2.) + math.pow(pred_points[kp][1] - gt_points[kp][1], 2.), 0.5)\n            sum_dist += dist/norm_value\n\n    sum_dist = sum_dist/num_v\n    print(sum_dist)\n\ndef run_by_cat():\n    if args.prediction.strip() == \'\' or args.gt.strip() == \'\':\n        parser.error(\'Must specify the file path of the prediction and ground truth.\')\n\n    pred_df = pd.read_csv(args.prediction, encoding=\'utf-8\')\n    gt_df = pd.read_csv(args.gt, encoding=\'utf-8\').set_index(\'image_id\')\n\n    for cat_ in cfg.CATEGORIES:\n        num_v = 0.\n        sum_dist = 0.\n        for index, row in pred_df.iterrows():\n            gt = gt_df.loc[row[\'image_id\']]\n            img_cat = gt[\'image_category\']\n            if cat_ not in img_cat:\n                continue\n            gt_points = {}\n            pred_points = {}\n\n            for kp in cfg.all_keys:\n                pred_kp = row[kp].strip().split(\'_\')\n                gt_kp = gt[kp].strip().split(\'_\')\n                pred_points[kp] = [int(_) for _ in pred_kp]\n                gt_points[kp] = [int(_) for _ in gt_kp]\n\n            lnorm_name, rnorm_name = cfg.normalize_point_name[img_cat]\n            lnorm, rnorm = gt_points[lnorm_name][:-1], gt_points[rnorm_name][:-1]\n            norm_value = math.pow(math.pow(lnorm[0] - rnorm[0], 2.) + math.pow(lnorm[1] - rnorm[1], 2.), 0.5)\n\n\n            for kp in cfg.all_keys:\n                if gt_points[kp][-1] == -1 or norm_value < 1e-3:\n                    continue\n                num_v += 1.\n\n                dist = math.pow(math.pow(pred_points[kp][0] - gt_points[kp][0], 2.) + math.pow(pred_points[kp][1] - gt_points[kp][1], 2.), 0.5)\n                sum_dist += dist/norm_value\n\n        sum_dist = sum_dist/num_v\n        print(\'{}:\'.format(cat_), sum_dist)\n\nif __name__ == \'__main__\':\n    if not args.cat:\n        run()\n    else:\n        run_by_cat()\n'"
swa_moving_average.py,15,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Maintain moving averages of parameters.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.training import slot_creator\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# TODO(touts): switch to variables.Variable.\ndef assign_moving_average(variable, value, decay, zero_debias=True, name=None):\n  """"""Compute the moving average of a variable.\n\n  The moving average of \'variable\' updated with \'value\' is:\n    variable * decay + value * (1 - decay)\n\n  The returned Operation sets \'variable\' to the newly computed moving average.\n\n  The new value of \'variable\' can be set with the \'AssignSub\' op as:\n     variable -= (1 - decay) * (variable - value)\n\n  Since variables that are initialized to a `0` value will be `0` biased,\n  `zero_debias` optionally enables scaling by the mathematically correct\n  debiasing factor of\n    1 - decay ** num_updates\n  See `ADAM: A Method for Stochastic Optimization` Section 3 for more details\n  (https://arxiv.org/abs/1412.6980).\n\n  The names of the debias shadow variables, by default, include both the scope\n  they were created in and the scope of the variables they debias. They are also\n  given a uniqifying-suffix.\n\n  E.g.:\n\n  ```\n    with tf.variable_scope(\'scope1\'):\n      with tf.variable_scope(\'scope2\'):\n        var = tf.get_variable(\'foo\')\n        tf.assign_moving_average(var, 0.0, 1.0)\n        tf.assign_moving_average(var, 0.0, 0.9)\n\n    # var.name: \'scope1/scope2/foo\'\n    # shadow var names: \'scope1/scope2/scope1/scope2/foo/biased\'\n    #                   \'scope1/scope2/scope1/scope2/foo/biased_1\'\n  ```\n\n  Args:\n    variable: A Variable.\n    value: A tensor with the same shape as \'variable\'.\n    decay: A float Tensor or float value.  The moving average decay.\n    zero_debias: A python bool. If true, assume the variable is 0-initialized\n      and unbias it, as in https://arxiv.org/abs/1412.6980. See docstring in\n      `_zero_debias` for more details.\n    name: Optional name of the returned operation.\n\n  Returns:\n    A reference to the input \'variable\' tensor with the newly computed\n    moving average.\n  """"""\n  with ops.name_scope(name, ""AssignMovingAvg"",\n                      [variable, value, decay]) as scope:\n    with ops.colocate_with(variable):\n      decay = ops.convert_to_tensor(1.0 - decay, name=""decay"")\n      if decay.dtype != variable.dtype.base_dtype:\n        decay = math_ops.cast(decay, variable.dtype.base_dtype)\n      if zero_debias:\n        update_delta = _zero_debias(variable, value, decay)\n      else:\n        update_delta = (variable - value) * decay\n      return state_ops.assign_sub(variable, update_delta, name=scope)\n\n\ndef weighted_moving_average(value,\n                            decay,\n                            weight,\n                            truediv=True,\n                            collections=None,\n                            name=None):\n  """"""Compute the weighted moving average of `value`.\n\n  Conceptually, the weighted moving average is:\n    `moving_average(value * weight) / moving_average(weight)`,\n  where a moving average updates by the rule\n    `new_value = decay * old_value + (1 - decay) * update`\n  Internally, this Op keeps moving average variables of both `value * weight`\n  and `weight`.\n\n  Args:\n    value: A numeric `Tensor`.\n    decay: A float `Tensor` or float value.  The moving average decay.\n    weight:  `Tensor` that keeps the current value of a weight.\n      Shape should be able to multiply `value`.\n    truediv:  Boolean, if `True`, dividing by `moving_average(weight)` is\n      floating point division.  If `False`, use division implied by dtypes.\n    collections:  List of graph collections keys to add the internal variables\n      `value * weight` and `weight` to.\n      Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n    name: Optional name of the returned operation.\n      Defaults to ""WeightedMovingAvg"".\n\n  Returns:\n    An Operation that updates and returns the weighted moving average.\n  """"""\n  # Unlike assign_moving_average, the weighted moving average doesn\'t modify\n  # user-visible variables. It is the ratio of two internal variables, which are\n  # moving averages of the updates.  Thus, the signature of this function is\n  # quite different than assign_moving_average.\n  if collections is None:\n    collections = [ops.GraphKeys.GLOBAL_VARIABLES]\n  with variable_scope.variable_scope(name, ""WeightedMovingAvg"",\n                                     [value, weight, decay]) as scope:\n    value_x_weight_var = variable_scope.get_variable(\n        ""value_x_weight"",\n        shape=value.get_shape(),\n        dtype=value.dtype,\n        initializer=init_ops.zeros_initializer(),\n        trainable=False,\n        collections=collections)\n    weight_var = variable_scope.get_variable(\n        ""weight"",\n        shape=weight.get_shape(),\n        dtype=weight.dtype,\n        initializer=init_ops.zeros_initializer(),\n        trainable=False,\n        collections=collections)\n    numerator = assign_moving_average(\n        value_x_weight_var, value * weight, decay, zero_debias=False)\n    denominator = assign_moving_average(\n        weight_var, weight, decay, zero_debias=False)\n\n    if truediv:\n      return math_ops.truediv(numerator, denominator, name=scope.name)\n    else:\n      return math_ops.div(numerator, denominator, name=scope.name)\n\n\ndef _zero_debias(unbiased_var, value, decay):\n  """"""Compute the delta required for a debiased Variable.\n\n  All exponential moving averages initialized with Tensors are initialized to 0,\n  and therefore are biased to 0. Variables initialized to 0 and used as EMAs are\n  similarly biased. This function creates the debias updated amount according to\n  a scale factor, as in https://arxiv.org/abs/1412.6980.\n\n  To demonstrate the bias the results from 0-initialization, take an EMA that\n  was initialized to `0` with decay `b`. After `t` timesteps of seeing the\n  constant `c`, the variable have the following value:\n\n  ```\n    EMA = 0*b^(t) + c*(1 - b)*b^(t-1) + c*(1 - b)*b^(t-2) + ...\n        = c*(1 - b^t)\n  ```\n\n  To have the true value `c`, we would divide by the scale factor `1 - b^t`.\n\n  In order to perform debiasing, we use two shadow variables. One keeps track of\n  the biased estimate, and the other keeps track of the number of updates that\n  have occurred.\n\n  Args:\n    unbiased_var: A Variable representing the current value of the unbiased EMA.\n    value: A Tensor representing the most recent value.\n    decay: A Tensor representing `1-decay` for the EMA.\n\n  Returns:\n    The amount that the unbiased variable should be updated. Computing this\n    tensor will also update the shadow variables appropriately.\n  """"""\n  with variable_scope.variable_scope(\n      unbiased_var.op.name, values=[unbiased_var, value, decay]) as scope:\n    with ops.colocate_with(unbiased_var):\n      with ops.init_scope():\n        biased_initializer = init_ops.zeros_initializer(\n            dtype=unbiased_var.dtype)(unbiased_var.get_shape())\n        local_step_initializer = init_ops.zeros_initializer()\n      def _maybe_get_unique(name):\n        """"""Get name for a unique variable, if not `reuse=True`.""""""\n        if variable_scope.get_variable_scope().reuse:\n          return name\n        vs_vars = [x.op.name for x in\n                   variable_scope.get_variable_scope().global_variables()]\n        full_name = variable_scope.get_variable_scope().name + ""/"" + name\n        if full_name not in vs_vars: return name\n        idx = 1\n        while full_name + (""_%d"" % idx) in vs_vars:\n          idx += 1\n        return name + (""_%d"" % idx)\n      biased_var = variable_scope.get_variable(\n          _maybe_get_unique(""biased""), initializer=biased_initializer,\n          trainable=False)\n      local_step = variable_scope.get_variable(\n          _maybe_get_unique(""local_step""),\n          shape=[],\n          dtype=unbiased_var.dtype,\n          initializer=local_step_initializer,\n          trainable=False)\n\n      # Get an update ops for both shadow variables.\n      update_biased = state_ops.assign_sub(biased_var,\n                                           (biased_var - value) * decay,\n                                           name=scope.name)\n      update_local_step = local_step.assign_add(1)\n\n      # Compute the value of the delta to update the unbiased EMA. Make sure to\n      # use the new values of the biased variable and the local step.\n      with ops.control_dependencies([update_biased, update_local_step]):\n        # This function gets `1 - decay`, so use `1.0 - decay` in the exponent.\n        unbiased_ema_delta = (unbiased_var - biased_var.read_value() /\n                              (1 - math_ops.pow(\n                                  1.0 - decay, local_step.read_value())))\n\n      return unbiased_ema_delta\n\n\nclass SWAMovingAverage(object):\n  """"""Maintains moving averages of variables by employing an exponential decay.\n\n  When training a model, it is often beneficial to maintain moving averages of\n  the trained parameters.  Evaluations that use averaged parameters sometimes\n  produce significantly better results than the final trained values.\n\n  The `apply()` method adds shadow copies of trained variables and add ops that\n  maintain a moving average of the trained variables in their shadow copies.\n  It is used when building the training model.  The ops that maintain moving\n  averages are typically run after each training step.\n  The `average()` and `average_name()` methods give access to the shadow\n  variables and their names.  They are useful when building an evaluation\n  model, or when restoring a model from a checkpoint file.  They help use the\n  moving averages in place of the last trained values for evaluations.\n\n  The moving averages are computed using exponential decay.  You specify the\n  decay value when creating the `ExponentialMovingAverage` object.  The shadow\n  variables are initialized with the same initial values as the trained\n  variables.  When you run the ops to maintain the moving averages, each\n  shadow variable is updated with the formula:\n\n    `shadow_variable -= (1 - decay) * (shadow_variable - variable)`\n\n  This is mathematically equivalent to the classic formula below, but the use\n  of an `assign_sub` op (the `""-=""` in the formula) allows concurrent lockless\n  updates to the variables:\n\n    `shadow_variable = decay * shadow_variable + (1 - decay) * variable`\n\n  Reasonable values for `decay` are close to 1.0, typically in the\n  multiple-nines range: 0.999, 0.9999, etc.\n\n  Example usage when creating a training model:\n\n  ```python\n  # Create variables.\n  var0 = tf.Variable(...)\n  var1 = tf.Variable(...)\n  # ... use the variables to build a training model...\n  ...\n  # Create an op that applies the optimizer.  This is what we usually\n  # would use as a training op.\n  opt_op = opt.minimize(my_loss, [var0, var1])\n\n  # Create an ExponentialMovingAverage object\n  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n\n  with tf.control_dependencies([opt_op]):\n      # Create the shadow variables, and add ops to maintain moving averages\n      # of var0 and var1. This also creates an op that will update the moving\n      # averages after each training step.  This is what we will use in place\n      # of the usual training op.\n      training_op = ema.apply([var0, var1])\n\n  ...train the model by running training_op...\n  ```\n\n  There are two ways to use the moving averages for evaluations:\n\n  *  Build a model that uses the shadow variables instead of the variables.\n     For this, use the `average()` method which returns the shadow variable\n     for a given variable.\n  *  Build a model normally but load the checkpoint files to evaluate by using\n     the shadow variable names.  For this use the `average_name()` method.  See\n     the @{tf.train.Saver} for more\n     information on restoring saved variables.\n\n  Example of restoring the shadow variable values:\n\n  ```python\n  # Create a Saver that loads variables from their saved shadow values.\n  shadow_var0_name = ema.average_name(var0)\n  shadow_var1_name = ema.average_name(var1)\n  saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})\n  saver.restore(...checkpoint filename...)\n  # var0 and var1 now hold the moving average values\n  ```\n  """"""\n\n  def __init__(self, num_updates, zero_debias=False,\n               name=""SWAMovingAverage""):\n    """"""Creates a new ExponentialMovingAverage object.\n\n    The `apply()` method has to be called to create shadow variables and add\n    ops to maintain moving averages.\n\n    The optional `num_updates` parameter allows one to tweak the decay rate\n    dynamically. It is typical to pass the count of training steps, usually\n    kept in a variable that is incremented at each step, in which case the\n    decay rate is lower at the start of training.  This makes moving averages\n    move faster.  If passed, the actual decay rate used is:\n\n      `num_updates / (1 + num_updates)`\n\n    Args:\n      decay: Float.  The decay to use.\n      num_updates: Optional count of number of updates applied to variables.\n      zero_debias: If `True`, zero debias moving-averages that are initialized\n        with tensors.\n      name: String. Optional prefix name to use for the name of ops added in\n        `apply()`.\n    """"""\n    self._num_updates = num_updates\n    self._zero_debias = zero_debias\n    self._name = name\n    self._averages = {}\n\n  def apply(self, var_list=None):\n    """"""Maintains moving averages of variables.\n\n    `var_list` must be a list of `Variable` or `Tensor` objects.  This method\n    creates shadow variables for all elements of `var_list`.  Shadow variables\n    for `Variable` objects are initialized to the variable\'s initial value.\n    They will be added to the `GraphKeys.MOVING_AVERAGE_VARIABLES` collection.\n    For `Tensor` objects, the shadow variables are initialized to 0 and zero\n    debiased (see docstring in `assign_moving_average` for more details).\n\n    shadow variables are created with `trainable=False` and added to the\n    `GraphKeys.ALL_VARIABLES` collection.  They will be returned by calls to\n    `tf.global_variables()`.\n\n    Returns an op that updates all shadow variables as described above.\n\n    Note that `apply()` can be called multiple times with different lists of\n    variables.\n\n    Args:\n      var_list: A list of Variable or Tensor objects. The variables\n        and Tensors must be of types float16, float32, or float64.\n\n    Returns:\n      An Operation that updates the moving averages.\n\n    Raises:\n      TypeError: If the arguments are not all float16, float32, or float64.\n      ValueError: If the moving average of one of the variables is already\n        being computed.\n    """"""\n    # TODO(touts): op_scope\n    if var_list is None:\n      var_list = variables.trainable_variables()\n    zero_debias_true = set()  # set of vars to set `zero_debias=True`\n    for var in var_list:\n      if var.dtype.base_dtype not in [dtypes.float16, dtypes.float32,\n                                      dtypes.float64]:\n        raise TypeError(""The variables must be half, float, or double: %s"" %\n                        var.name)\n      if var in self._averages:\n        raise ValueError(""Moving average already computed for: %s"" % var.name)\n\n      # For variables: to lower communication bandwidth across devices we keep\n      # the moving averages on the same device as the variables. For other\n      # tensors, we rely on the existing device allocation mechanism.\n      with ops.init_scope():\n        if isinstance(var, variables.Variable):\n          avg = slot_creator.create_slot(var,\n                                         var.initialized_value(),\n                                         self._name,\n                                         colocate_with_primary=True)\n          # NOTE(mrry): We only add `tf.Variable` objects to the\n          # `MOVING_AVERAGE_VARIABLES` collection.\n          ops.add_to_collection(ops.GraphKeys.MOVING_AVERAGE_VARIABLES, var)\n        else:\n          avg = slot_creator.create_zeros_slot(\n              var,\n              self._name,\n              colocate_with_primary=(var.op.type in [""Variable"",\n                                                     ""VariableV2"",\n                                                     ""VarHandleOp""]))\n          if self._zero_debias:\n            zero_debias_true.add(avg)\n      self._averages[var] = avg\n\n    with ops.name_scope(self._name) as scope:\n      num_updates = math_ops.cast(self._num_updates,\n                                  dtypes.float32,\n                                  name=""num_updates"")\n      decay = num_updates / (1. + num_updates)\n      decay = array_ops.identity(decay, name=\'decay\')\n\n      updates = []\n      for var in var_list:\n        zero_debias = self._averages[var] in zero_debias_true\n        updates.append(assign_moving_average(\n            self._averages[var], var, decay, zero_debias=zero_debias))\n      return control_flow_ops.group(*updates, name=scope)\n\n  def average(self, var):\n    """"""Returns the `Variable` holding the average of `var`.\n\n    Args:\n      var: A `Variable` object.\n\n    Returns:\n      A `Variable` object or `None` if the moving average of `var`\n      is not maintained.\n    """"""\n    return self._averages.get(var, None)\n\n  def average_name(self, var):\n    """"""Returns the name of the `Variable` holding the average for `var`.\n\n    The typical scenario for `ExponentialMovingAverage` is to compute moving\n    averages of variables during training, and restore the variables from the\n    computed moving averages during evaluations.\n\n    To restore variables, you have to know the name of the shadow variables.\n    That name and the original variable can then be passed to a `Saver()` object\n    to restore the variable from the moving average value with:\n      `saver = tf.train.Saver({ema.average_name(var): var})`\n\n    `average_name()` can be called whether or not `apply()` has been called.\n\n    Args:\n      var: A `Variable` object.\n\n    Returns:\n      A string: The name of the variable that will be used or was used\n      by the `ExponentialMovingAverage class` to hold the moving average of\n      `var`.\n    """"""\n    if var in self._averages:\n      return self._averages[var].op.name\n    return ops.get_default_graph().unique_name(\n        var.op.name + ""/"" + self._name, mark_as_used=False)\n\n  def variables_to_restore(self, moving_avg_variables=None):\n    """"""Returns a map of names to `Variables` to restore.\n\n    If a variable has a moving average, use the moving average variable name as\n    the restore name; otherwise, use the variable name.\n\n    For example,\n\n    ```python\n      variables_to_restore = ema.variables_to_restore()\n      saver = tf.train.Saver(variables_to_restore)\n    ```\n\n    Below is an example of such mapping:\n\n    ```\n      conv/batchnorm/gamma/ExponentialMovingAverage: conv/batchnorm/gamma,\n      conv_4/conv2d_params/ExponentialMovingAverage: conv_4/conv2d_params,\n      global_step: global_step\n    ```\n    Args:\n      moving_avg_variables: a list of variables that require to use of the\n        moving variable name to be restored. If None, it will default to\n        variables.moving_average_variables() + variables.trainable_variables()\n\n    Returns:\n      A map from restore_names to variables. The restore_name can be the\n      moving_average version of the variable name if it exist, or the original\n      variable name.\n    """"""\n    name_map = {}\n    if moving_avg_variables is None:\n      # Include trainable variables and variables which have been explicitly\n      # added to the moving_average_variables collection.\n      moving_avg_variables = variables.trainable_variables()\n      moving_avg_variables += variables.moving_average_variables()\n    # Remove duplicates\n    moving_avg_variables = set(moving_avg_variables)\n    # Collect all the variables with moving average,\n    for v in moving_avg_variables:\n      name_map[self.average_name(v)] = v\n    # Make sure we restore variables without moving averages as well.\n    moving_avg_variable_names = set([v.name for v in moving_avg_variables])\n    for v in list(set(variables.global_variables())):\n      if v.name not in moving_avg_variable_names and v.op.name not in name_map:\n        name_map[v.op.name] = v\n    return name_map\n'"
swa_train_cpn.py,143,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import detnet_cpn\nfrom net import detxt_cpn\nfrom net import seresnet_cpn\nfrom net import cpn\n\nimport swa_moving_average\n\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_string(\n    \'backbone\', \'detnet50_cpn\',\n    \'The backbone network to use for feature extraction.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 10,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 10,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 1e-5, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'high_learning_rate\', 8e-5, \'The maximal learning rate used by SWA.\')#1e-3\ntf.app.flags.DEFINE_float(\n    \'low_learning_rate\', 1e-6,\n    \'The minimal learning rate used by SWA.\')\ntf.app.flags.DEFINE_boolean(\n    \'dummy_train\', False,\n    \'training with zero learning rate to get batch norm statistics.\')\n# tf.app.flags.DEFINE_string(\n#     \'steps_per_epoch\', \'1125, 905, 935, 1114, 1040\',\n#     \'Learning rate decay boundaries by global_step (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', None,\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n\nall_models = {\n  \'resnet50_cpn\': {\'backbone\': cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'swa_logs_cpn\', \'checkpoint_root\': \'logs_cpn\'},\n  \'detnet50_cpn\': {\'backbone\': detnet_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'swa_logs_detnet_cpn\', \'checkpoint_root\': \'logs_detnet_cpn\'},\n  \'seresnet50_cpn\': {\'backbone\': seresnet_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'swa_logs_se_cpn\', \'checkpoint_root\': \'logs_se_cpn\'},\n  \'seresnext50_cpn\': {\'backbone\': seresnet_cpn.xt_cascaded_pyramid_net, \'logs_sub_dir\': \'swa_logs_sext_cpn\', \'checkpoint_root\': \'logs_sext_cpn\'},\n  \'detnext50_cpn\': {\'backbone\': detxt_cpn.cascaded_pyramid_net, \'logs_sub_dir\': \'swa_logs_detxt_cpn\', \'checkpoint_root\': \'logs_detxt_cpn\'},\n}\n\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=FLAGS.train_epochs):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\nbackbone_ = all_models[FLAGS.backbone.strip()][\'backbone\']\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = backbone_(features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n    #print(pred_outputs)\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n    blur_list = [1., 1.37, 1.73, 2.4, None]#[1., 1.5, 2., 3., None]\n    #blur_list = [None, None, None, None, None]\n\n    targets_list = []\n    for sigma in blur_list:\n        if sigma is None:\n            targets_list.append(targets)\n        else:\n            # always channels first foe targets\n            targets_list.append(gaussian_blur(targets, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], sigma, params[\'data_format\'], \'blur_{}\'.format(sigma)))\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n    # filter all invisible keypoint maybe better for this task\n    # all_visible = tf.logical_and(key_v>0, isvalid>0)\n    # targets_list = [tf.boolean_mask(targets_list[ind], all_visible) for ind in list(range(len(targets_list)))]\n    # pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets_list = [targets_list[ind] * all_visible for ind in list(range(len(targets_list)))]\n    pred_outputs = [pred_outputs[ind] * all_visible for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        for pred_ind in list(range(len(pred_outputs) - 1)):\n            mse_loss_list.append(0.5 * tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets_list[-1], pred_outputs[-1], weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets_list[-1], gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs[-1], gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(len(pred_outputs) - 1),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        for pred_ind in list(range(len(pred_outputs))):\n            mse_loss_list.append(tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        if not params[\'dummy_train\']:\n            step_remainder = tf.floormod(global_step - 1, params[\'steps_per_epoch\'])\n            range_scale = tf.to_float(step_remainder + 1) / tf.to_float(params[\'steps_per_epoch\'])\n            learning_rate = tf.add((1 - range_scale) * params[\'high_learning_rate\'], range_scale * params[\'low_learning_rate\'], name=\'learning_rate\')\n            tf.summary.scalar(\'lr\', learning_rate)\n\n            should_update = tf.equal(step_remainder, params[\'steps_per_epoch\'] - 2)\n            optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=params[\'momentum\'])\n\n            # Batch norm requires update_ops to be added as a train_op dependency.\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                opt_op = optimizer.minimize(loss, global_step)\n\n            variables_to_train = []\n            for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n                variables_to_train.append(var)\n\n            # Create an ExponentialMovingAverage object\n            ema = swa_moving_average.SWAMovingAverage(tf.floordiv(global_step, params[\'steps_per_epoch\']))\n            with tf.control_dependencies([opt_op]):\n                train_op = tf.cond(should_update, lambda : ema.apply(variables_to_train), lambda : tf.no_op())\n\n            _init_fn = train_helper.get_raw_init_fn_for_scaffold(params[\'checkpoint_path\'], params[\'model_dir\'])\n        else:\n            learning_rate = tf.constant(0., name=\'learning_rate\')\n            tf.summary.scalar(\'lr\', learning_rate)\n            optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.)\n\n            variables_to_train = []\n            for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n                variables_to_train.append(var)\n            ema = swa_moving_average.SWAMovingAverage(tf.floordiv(global_step, params[\'steps_per_epoch\']))\n            # Batch norm requires update_ops to be added as a train_op dependency.\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                train_op = optimizer.minimize(loss, global_step)\n            _init_fn = train_helper.swa_get_init_fn_for_scaffold(params[\'checkpoint_path\'], params[\'model_dir\'], variables_to_train, ema)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=_init_fn, saver=None))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, high_learning_rate, low_learning_rate, checkpoint_path=None):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // FLAGS.batch_size\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'model_scope\': model_scope,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'batch_size\': FLAGS.batch_size,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'dummy_train\': FLAGS.dummy_train,\n            \'high_learning_rate\': high_learning_rate,\n            \'low_learning_rate\': low_learning_rate,\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    tensors_to_log = {\n        \'lr\': \'learning_rate\',\n        \'loss\': \'total_loss\',\n        \'mse\': \'mse_loss\',\n        \'ne\': \'ne_mertric\',\n    }\n\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n    tf.logging.info(\'Starting a training cycle.\')\n    fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, train_epochs), hooks=[logging_hook], max_steps=(steps_per_epoch*((train_epochs+1) if FLAGS.dummy_train else train_epochs)))\n\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    full_model_dir = os.path.join(FLAGS.model_dir, all_models[FLAGS.backbone.strip()][\'logs_sub_dir\'])\n    checkpoint_model_dir = os.path.join(FLAGS.model_dir, all_models[FLAGS.backbone.strip()][\'checkpoint_root\'])\n\n    detail_params = {\n        \'blouse\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'blouse\'),\n            \'train_epochs\': FLAGS.train_epochs,\n            \'model_scope\': \'blouse\',\n            \'high_learning_rate\': FLAGS.high_learning_rate,\n            \'low_learning_rate\': FLAGS.low_learning_rate,\n            \'checkpoint_path\': os.path.join(checkpoint_model_dir, \'blouse\'),\n        },\n        \'dress\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'dress\'),\n            \'train_epochs\': FLAGS.train_epochs,\n            \'model_scope\': \'dress\',\n            \'high_learning_rate\': FLAGS.high_learning_rate,\n            \'low_learning_rate\': FLAGS.low_learning_rate,\n            \'checkpoint_path\': os.path.join(checkpoint_model_dir, \'dress\'),\n        },\n        \'outwear\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'outwear\'),\n            \'train_epochs\': FLAGS.train_epochs,\n            \'model_scope\': \'outwear\',\n            \'high_learning_rate\': FLAGS.high_learning_rate,\n            \'low_learning_rate\': FLAGS.low_learning_rate,\n            \'checkpoint_path\': os.path.join(checkpoint_model_dir, \'outwear\'),\n        },\n        \'skirt\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'skirt\'),\n            \'train_epochs\': FLAGS.train_epochs,\n            \'model_scope\': \'skirt\',\n            \'high_learning_rate\': FLAGS.high_learning_rate,\n            \'low_learning_rate\': FLAGS.low_learning_rate,\n            \'checkpoint_path\': os.path.join(checkpoint_model_dir, \'skirt\'),\n        },\n        \'trousers\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'trousers\'),\n            \'train_epochs\': FLAGS.train_epochs,\n            \'high_learning_rate\': FLAGS.high_learning_rate,\n            \'low_learning_rate\': FLAGS.low_learning_rate,\n            \'model_scope\': \'trousers\',\n            \'checkpoint_path\': os.path.join(checkpoint_model_dir, \'trousers\'),\n        },\n    }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n\n    # import datetime\n    # import time\n    # while True:\n    #     time.sleep(1600)\n    #     if \'8\' in datetime.datetime.now().time().strftime(\'%H\'):\n    #         break\n\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'high_learning_rate\'], detail_params[m][\'low_learning_rate\'], detail_params[m][\'checkpoint_path\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
tf_replicate_model_fn.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities to replicate model_fn\'s over local GPUs.\n\nThis file contains util that allow to replicate `Estimator.model_fn` over\nGPUs.  Replicated version of a `model_fn` is returned that can subsequently\nbe used with `Estimator`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nimport copy\n\nimport six\n\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.estimator import model_fn as model_fn_lib\nfrom tensorflow.python.estimator import util\nfrom tensorflow.python.estimator.export import export_output as export_output_lib\nfrom tensorflow.python.framework import device as framework_device\nfrom tensorflow.python.framework import ops as ops_lib\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops.losses import losses\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.training import device_setter as device_setter_lib\nfrom tensorflow.python.training import optimizer as optimizer_lib\n\n\ndef replicate_model_fn(model_fn,\n                       loss_reduction=losses.Reduction.SUM_BY_NONZERO_WEIGHTS,\n                       devices=None):\n  """"""Replicate `Estimator.model_fn` over GPUs.\n\n  The given `model_fn` specifies a single forward pass of a model.  To replicate\n  such a model over GPUs, each GPU gets its own instance of the forward pass\n  (a.k.a. a tower).  The input features and labels get sharded into the chunks\n  that correspond to the number of GPUs.  Each tower computes a loss based\n  on its input.  For each such loss, gradients are computed.  After that, the\n  available losses are aggregated to form aggregated loss.  Available\n  gradients are summed.  Then, they update weights using the specified\n  optimizer.\n\n  If `devices` are `None`, then all available GPUs are going to be used for\n  replication.  If no GPUs are available, then the model is going to be\n  placed on the CPU.\n\n  Two modes of local replication over available GPUs are supported:\n    1)  If exactly 1 GPU is detected, then variables and operations are placed\n        onto the GPU.\n    2)  If more than 1 GPU is detected, then variables are going to be placed on\n        the CPU.  Replicas of operations are placed on each individual GPU.\n\n  Here is an example of how one might use their `model_fn` to run over GPUs:\n    ```python\n       ...\n       def model_fn(...):  # See `model_fn` in `Estimator`.\n         loss = ...\n         optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n         optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)\n         if mode == tf.estimator.ModeKeys.TRAIN:\n           #  See the section below on `EstimatorSpec.train_op`.\n           return EstimatorSpec(mode=mode, loss=loss,\n                                train_op=optimizer.minimize(loss))\n\n         #  No change for `ModeKeys.EVAL` or `ModeKeys.PREDICT`.\n         return EstimatorSpec(...)\n       ...\n       classifier = tf.estimator.Estimator(\n         model_fn=tf.contrib.estimator.replicate_model_fn(model_fn))\n    ```\n\n  Please see `DNNClassifierIntegrationTest` for an example with a canned\n  Estimator.\n\n  On `EstimatorSpec.train_op`:\n  `model_fn` returns `EstimatorSpec.train_op` for\n  `tf.estimator.GraphKeys.TRAIN`. It is typically derived using an optimizer.\n  Towers are expected to populate it in the same way.  Gradients from all towers\n  are reduced and applied in the last tower.  To achieve that in the case of\n  multiple towers, `TowerOptimizer` needs to be used.  See `TowerOptimizer`.\n\n  On sharding input features and labels:\n  Input features and labels are split for consumption by each tower. They are\n  split across the dimension 0.  Features and labels need to be batch major.\n\n  On reduction algorithms:\n  Certain algorithms were chosen for aggregating results of computations on\n  multiple towers:\n    - Losses from all towers are reduced according to `loss_reduction`.\n    - Gradients are reduced using sum for each trainable variable.\n    - `eval_metrics_ops` are reduced per metric using `reduce_mean`.\n    - `EstimatorSpec.predictions` and `EstimatorSpec.export_outputs` are\n      reduced using concatenation.\n    - For all other fields of `EstimatorSpec` the values of the first tower\n      are taken.\n\n  On distribution of variables:\n  Variables are not duplicated between towers.  Instead, they are placed on a\n  single device as defined above and shared across towers.\n\n  On overhead:\n  If only one device is specified, then aggregation of loss and gradients\n  doesn\'t happen. Replication consists of placing `model_fn` onto the\n  specified device.\n\n  On current limitations:\n    - `predictions` are not supported for `ModeKeys.EVAL`.  They are required\n       for `tf.contrib.estimator.add_metrics`.\n\n  Args:\n    model_fn: `model_fn` as defined in `Estimator`.  See the section above about\n      the train_op argument of `EstimatorSpec`.\n    loss_reduction: controls whether losses are summed or averaged.\n    devices: Optional list of devices to replicate the model across.  This\n      argument can be used to replice only on the subset of available GPUs.\n      If `None`, then all available GPUs are going to be used for replication.\n      If no GPUs are available, then the model is going to be placed on the CPU.\n\n  Raises:\n    ValueError: if there is no `loss_reduction` or if TowerOptimizer is\n      mis-used.\n\n  Returns:\n    A replicated version of the supplied `model_fn`. Returned function that\n      conforms to the requirements of `Estimator`\'s `model_fn` and can be used\n      instead of the supplied `model_fn`.\n  """"""\n  return _replicate_model_fn_with_mode(\n      model_fn,\n      loss_reduction,\n      devices,\n      # TODO(isaprykin): Query the system configuration to choose modes other\n      # than `SHARED_LOCAL_PARAMETER_SERVER`, even though it is often\n      # appropriate.\n      mode=_VariableDistributionMode.SHARED_LOCAL_PARAMETER_SERVER)\n\n\nclass _VariableDistributionMode(object):\n  """"""Modes for variable distribution used for forcing a particular one.\n\n  Forcing a mode is meant for performance experimentation purposes rather than\n  for general use cases.\n  """"""\n\n  SHARED_LOCAL_PARAMETER_SERVER = 1\n  """"""Variables are placed on a single device and shared across all devices.\n\n  Two ways to achieve this distribution over available GPUs are supported:\n    1)  If exactly 1 GPU is detected, then variables and operations are placed\n        onto GPU.\n    2)  If more than 1 GPU is detected, then variables are going to be placed on\n        the CPU.  Replicas of operations are placed on each individual GPU.\n  """"""\n\n  SHARED_ROUND_ROBIN = 2\n  """"""Variables are placed on all devices in a round-robin fashion.\n\n  Every subsequent variable is placed on the next device.  There is only one\n  copy of each variable that is shared across all devices.\n  """"""\n\n\ndef _replicate_model_fn_with_mode(\n    model_fn,\n    loss_reduction,\n    devices=None,\n    mode=_VariableDistributionMode.SHARED_LOCAL_PARAMETER_SERVER):\n  """"""A version of `replicate_model_fn` that allows to specify a `mode`.""""""\n  if loss_reduction == losses.Reduction.NONE:\n    raise ValueError(\'Tower losses need to be reduced in some way, yet {} \'\n                     \'reduction is specified.\'.format(loss_reduction))\n  if not devices:\n    devices = _get_local_devices(\'GPU\') or _get_local_devices(\'CPU\')\n\n  is_a_single_gpu_case = len(devices) == 1 and \'GPU\' in devices[0]\n  consolidation_device = devices[0] if is_a_single_gpu_case else \'/CPU:0\'\n\n  ps_devices = [consolidation_device]\n  if mode == _VariableDistributionMode.SHARED_ROUND_ROBIN:\n    ps_devices = devices\n\n  tf_logging.info(\'Replicating the `model_fn` across {}.  Variables are going \'\n                  \'to be placed on {}.  Consolidation device is going to be {}.\'\n                  .format(devices, ps_devices, consolidation_device))\n\n  def single_device_model_fn(features, labels, mode, params=None, config=None):\n    """"""`model_fn` on a single device without reduction overhead.""""""\n    return _get_loss_towers(\n        model_fn=model_fn,\n        mode=mode,\n        features=[features],\n        labels=[labels],\n        params=params,\n        loss_reduction=loss_reduction,\n        config=config,\n        devices=devices,\n        local_ps_devices=ps_devices)[0]  # One device, so one spec is out.\n\n  def replicated_model_fn(features, labels, mode, params=None, config=None):\n    """"""Replicated version of `model_fn` to be used instead.""""""\n    feature_shards, label_shards = _split_batch(\n        features, labels, len(devices), device=consolidation_device)\n    tower_specs = _get_loss_towers(\n        model_fn=model_fn,\n        mode=mode,\n        features=feature_shards,\n        labels=label_shards,\n        params=params,\n        loss_reduction=loss_reduction,\n        config=config,\n        devices=devices,\n        local_ps_devices=ps_devices)\n\n    if mode == model_fn_lib.ModeKeys.TRAIN:\n      train_op = _minimize_towers(tower_specs)\n      return _train_spec(\n          tower_specs, train_op, aggregation_device=consolidation_device)\n    elif mode == model_fn_lib.ModeKeys.EVAL:\n      return _eval_spec(tower_specs, aggregation_device=consolidation_device)\n    elif mode == model_fn_lib.ModeKeys.PREDICT:\n      return _predict_spec(tower_specs, aggregation_device=consolidation_device)\n\n  if len(devices) == 1:\n    return single_device_model_fn\n  else:\n    return replicated_model_fn\n\n\nclass TowerOptimizer(optimizer_lib.Optimizer):\n  """"""Gathers gradients from all towers and reduces them in the last one.""""""\n\n  COLLECTION_FOR_GRAPH_STATES = \'replicate_model_fn_graph_states\'\n\n  def __init__(self, optimizer_or_optimizer_fn):\n    """"""Wrap an existing optimizer for gathering gradients across towers.\n\n    Each invocation of model_fn has to call the same optimizers in the same\n    order.\n\n    Multiple optimizers that use the same or different losses are supported.\n\n    If TowerOptimizer is used but `replicate_model_fn` isn\'t, then no\n    aggregation will happen.  All calls will simply be forwarded to the\n    underlying optimizer. The behavior is similar if there is only one tower.\n\n    If TowerOptimizer is used together with SyncReplicasOptimizer that wraps\n    the user\'s optimizer, then it\'s the SyncReplicasOptimizer that needs to be\n    wrapped with TowerOptimizer.\n\n    Args:\n      optimizer_or_optimizer_fn: an instance of optimizer to wrap.  That\n        instance is going to be used for optimizer-specific logic.  This can\n        also be a no-argument function that returns such an optimizer instance.\n    """"""\n    self._optimizer_or_optimizer_fn = optimizer_or_optimizer_fn\n\n  @staticmethod\n  def has_been_used():\n    return TowerOptimizer._graph_state().has_tower_optimizer_been_used\n\n  def get_slot(self, *args, **kwargs):\n    return self._get_optimizer().get_slot(*args, **kwargs)\n\n  def get_slot_names(self, *args, **kwargs):\n    return self._get_optimizer().get_slot_names(*args, **kwargs)\n\n  def get_name(self, *args, **kwargs):\n    return self._get_optimizer().get_name(*args, **kwargs)\n\n  def variables(self, *args, **kwargs):\n    return self._get_optimizer().variables(*args, **kwargs)\n\n  def compute_gradients(self, loss, *args, **kwargs):\n    """"""Compute gradients, but first, if needed, scale the loss.""""""\n    loss = _scale_loss(loss,\n                       self._graph_state().loss_reduction,\n                       self._graph_state().number_of_towers)\n    return self._get_optimizer().compute_gradients(loss, *args, **kwargs)\n\n  def apply_gradients(self, grads_and_vars, global_step=None, **kwargs):\n    """"""Collect gradients updates to apply them with the last tower.""""""\n    if self._graph_state().number_of_towers == 1:\n      # Avoid the overhead of reduction if there\'s only one tower.\n      #\n      # There assumed to be only one tower if aggregation-related methods were\n      # not called by `_get_loss_towers`, for example if the model_fn uses\n      # TowerEstimator, but `replicate_model_fn` isn\'t used.\n      return self._get_optimizer().apply_gradients(grads_and_vars, global_step,\n                                                   **kwargs)\n\n    self._graph_state().collect_gradients(grads_and_vars)\n\n    if not self._graph_state().is_the_last_tower:\n      with ops_lib.control_dependencies(_extract_tensors(grads_and_vars)):\n        return self._construct_no_op_train_op()\n    else:\n      # Gradients need to be gathered and applied in the scope of the first\n      # tower, so that the tensors are accessible via names without prefixes.\n      var_scope, name_scope = self._graph_state().scopes_of_the_first_tower\n      with variable_scope.variable_scope(var_scope):\n        with ops_lib.name_scope(name_scope):\n          return self._apply_gathered_gradients(global_step, **kwargs)\n\n  def _apply_gathered_gradients(self, global_step, **kwargs):\n    graph_state = self._graph_state()\n    optimizer = self._get_optimizer()\n\n    grad_lists = {}\n    for grad, var in graph_state.get_latest_gradients_from_all_towers():\n      if grad is not None:\n        grad_lists.setdefault(var, []).append(grad)\n\n    aggregated_grads = []\n    with ops_lib.name_scope(\'gradient_aggregating\'):\n      for var, grads in six.iteritems(grad_lists):\n        grad = _compute_sum_on_device(grads, var.device)\n        aggregated_grads.append((grad, var))\n    return optimizer.apply_gradients(\n        aggregated_grads, global_step=global_step, **kwargs)\n\n  def _get_optimizer(self):\n    if callable(self._optimizer_or_optimizer_fn):\n      # If optimizer is given as a function then we need to wait till we are\n      # under the right graph context before constructing it.  That\'s why the\n      # optimizer is constructed in _get_optimizer() rather than __init__().\n      self._optimizer_or_optimizer_fn = self._optimizer_or_optimizer_fn()\n    self._graph_state().has_tower_optimizer_been_used = True\n    return self._optimizer_or_optimizer_fn\n\n  def _construct_no_op_train_op(self):\n    return control_flow_ops.no_op(name=\'train_op_placeholder\')\n\n  @staticmethod\n  def _graph_state():\n    graph_states = ops_lib.get_default_graph().get_collection_ref(\n        TowerOptimizer.COLLECTION_FOR_GRAPH_STATES)\n    if not graph_states:\n      graph_states.append(TowerOptimizer._PerGraphState())\n    return graph_states[-1]\n\n  @staticmethod\n  def _did_towers_have_same_optimizer_calls():\n    graph_state = TowerOptimizer._graph_state()\n    return graph_state.did_towers_have_same_optimizer_calls()\n\n  @staticmethod\n  def _clear_graph_state():\n    # Clearing the Graph collection will prevent _PerGraphState from being\n    # serialized.\n    ops_lib.get_default_graph().clear_collection(\n        TowerOptimizer.COLLECTION_FOR_GRAPH_STATES)\n\n  class _PerGraphState(object):\n    """"""Gradient reduction related state of a Tensorflow graph.""""""\n\n    def __init__(self):\n      self._collected_grads_and_vars = defaultdict(list)\n      self._current_tower_index = 0\n      self._number_of_towers = 1\n      self._loss_reduction = None\n      # Scopes of the first tower that don\'t have a prefix:\n      self._variable_scope = None\n      self._name_scope = None\n      # If needed, alert that TowerOptimizer needs to be used with model_fn.\n      self._has_tower_optimizer_been_used = False\n\n    def collect_gradients(self, grads_and_vars):\n      self._collected_grads_and_vars[self._current_tower_index].append(\n          grads_and_vars)\n\n    def get_latest_gradients_from_all_towers(self):\n      """"""Get gradients across towers for the last called optimizer.""""""\n      grads_and_vars = []\n      index_of_last_gradients = len(\n          self._collected_grads_and_vars[self._current_tower_index]) - 1\n      for tower_id in range(self._current_tower_index + 1):\n        grads_and_vars.extend(\n            self._collected_grads_and_vars[tower_id][index_of_last_gradients])\n      return grads_and_vars\n\n    def set_reduction_across_towers(self, loss_reduction, number_of_towers):\n      self._loss_reduction = loss_reduction\n      self._number_of_towers = number_of_towers\n\n    @contextmanager\n    def tower(self, tower_id, var_scope, name_scope):\n      if tower_id == 0:\n        self._variable_scope = var_scope\n        self._name_scope = name_scope\n      self._current_tower_index = tower_id\n      yield\n\n    @property\n    def scopes_of_the_first_tower(self):\n      return self._variable_scope, self._name_scope\n\n    @property\n    def is_the_last_tower(self):\n      return self._current_tower_index == (self._number_of_towers - 1)\n\n    @property\n    def number_of_towers(self):\n      return self._number_of_towers\n\n    @property\n    def loss_reduction(self):\n      return self._loss_reduction\n\n    @property\n    def has_tower_optimizer_been_used(self):\n      return self._has_tower_optimizer_been_used\n\n    @has_tower_optimizer_been_used.setter\n    def has_tower_optimizer_been_used(self, value):\n      self._has_tower_optimizer_been_used = value\n\n    def did_towers_have_same_optimizer_calls(self):\n      total_number_of_grads = sum([\n          len(grads)\n          for _, grads in six.iteritems(self._collected_grads_and_vars)\n      ])\n      return total_number_of_grads % self._number_of_towers == 0\n\n\ndef _get_local_devices(device_type):\n  local_device_protos = device_lib.list_local_devices()\n  return [\n      device.name\n      for device in local_device_protos\n      if device.device_type == device_type\n  ]\n\n\ndef _split_batch(features, labels, number_of_shards, device):\n  """"""Split input features and labes into batches.""""""\n\n  def ensure_divisible_by_shards(sequence):\n    batch_size = ops_lib.convert_to_tensor(sequence).get_shape()[0]\n    if batch_size % number_of_shards != 0:\n      raise ValueError(\n          \'Batch size {} needs to be divisible by the number of GPUs, which \'\n          \'is {}.\'.format(batch_size, number_of_shards))\n\n  def split_dictionary(dictionary):\n    """"""Split a dictionary into shards.""""""\n    shards = [{} for _ in range(number_of_shards)]\n    for name, tensor in six.iteritems(dictionary):\n      if isinstance(tensor, sparse_tensor.SparseTensor):\n        for i, shard in enumerate(\n            sparse_ops.sparse_split(\n                sp_input=tensor, num_split=number_of_shards, axis=0)):\n          shards[i][name] = shard\n      else:\n        ensure_divisible_by_shards(tensor)\n        for i, shard in enumerate(array_ops.split(tensor, number_of_shards)):\n          shards[i][name] = shard\n    return shards\n\n  with ops_lib.name_scope(\'split_inputs\'):\n    with ops_lib.device(device):\n      if isinstance(features, dict):\n        feature_shards = split_dictionary(features)\n      else:\n        ensure_divisible_by_shards(features)\n        feature_shards = array_ops.split(features, number_of_shards)\n\n      if labels is None:\n        label_shards = None\n      elif isinstance(labels, dict):\n        label_shards = split_dictionary(labels)\n      else:\n        ensure_divisible_by_shards(labels)\n        label_shards = array_ops.split(labels, number_of_shards)\n  return feature_shards, label_shards\n\n\n_DEFAULT_NAME_SCOPE_PATTERN = \'tower_{}\'\n\n\ndef _get_loss_towers(model_fn,\n                     mode,\n                     features,\n                     labels,\n                     params,\n                     config,\n                     devices,\n                     local_ps_devices,\n                     loss_reduction,\n                     name_scope_pattern=_DEFAULT_NAME_SCOPE_PATTERN):\n  """"""Replicate the loss computation across devices.""""""\n  tower_specs = []\n\n  model_fn_args = util.fn_args(model_fn)\n  optional_params = {}\n  if \'params\' in model_fn_args:\n    optional_params[\'params\'] = copy.deepcopy(params)\n  if \'config\' in model_fn_args:\n    optional_params[\'config\'] = copy.deepcopy(config)\n\n  # pylint: disable=protected-access\n  round_robin_strategy = device_setter_lib._RoundRobinStrategy(\n      num_tasks=len(local_ps_devices))\n  TowerOptimizer._graph_state().set_reduction_across_towers(\n      loss_reduction, len(devices))\n\n  for i, device in enumerate(devices):\n    is_the_first_tower = (i == 0)\n\n    device_setter = _local_device_setter(\n        worker_device=device,\n        ps_devices=local_ps_devices,\n        ps_strategy=round_robin_strategy)\n\n    # We would like to preserve the names of the variables and ops that the user\n    # might be relying on. Names without a prefix are going to resolve to\n    # variables and ops of the first tower.\n    name_scope = name_scope_pattern\n    if is_the_first_tower:\n      name_scope = \'\'\n\n    with variable_scope.variable_scope(\n        \'\', reuse=not is_the_first_tower) as var_scope:\n      with ops_lib.name_scope(name_scope.format(i)) as name_scope:\n        with TowerOptimizer._graph_state().tower(\n            tower_id=i, var_scope=var_scope, name_scope=name_scope):\n          with ops_lib.device(device_setter):\n            labels_shard = None\n            if labels:\n              labels_shard = labels[i]\n\n            tower_spec = model_fn(\n                mode=mode,\n                features=features[i],\n                labels=labels_shard,\n                **optional_params)\n\n            if (tower_spec.train_op is not None and len(devices) > 1 and\n                not TowerOptimizer.has_been_used()):\n              raise ValueError(\'Please wrap optimizers with TowerOptimizer\'\n                               \' in order to use replicate_model_fn with\'\n                               \' multiple `devices`.\')\n\n            # Scaling the loss here doesn\'t actually affect gradients.  Another\n            # instance of scaling happens inside the TowerOptimizer.\n            tower_spec = _scale_tower_loss(\n                tower_spec, loss_reduction, number_of_towers=len(devices))\n            tower_specs.append(tower_spec)\n\n  if not TowerOptimizer._did_towers_have_same_optimizer_calls():\n    raise ValueError(\'Each invocation of model_fn was supposed to make the same\'\n                     \' optimizer calls.\')\n  TowerOptimizer._clear_graph_state()\n  # pylint: enable=protected-access\n  return tower_specs\n\n\ndef _local_device_setter(worker_device, ps_devices, ps_strategy):\n  """"""A device setter that puts distributes Var/Ops to PS/workers.""""""\n  ps_ops = [\'Variable\', \'VariableV2\', \'VarHandleOp\']\n\n  def local_device_chooser(op):\n    current_device = framework_device.DeviceSpec.from_string(op.device or \'\')\n\n    node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n    if node_def.op in ps_ops:\n      ps_device_spec = framework_device.DeviceSpec.from_string(\n          \'{}\'.format(ps_devices[ps_strategy(op)]))\n\n      ps_device_spec.merge_from(current_device)\n      return ps_device_spec.to_string()\n    else:\n      worker_device_spec = framework_device.DeviceSpec.from_string(\n          worker_device or \'\')\n      worker_device_spec.merge_from(current_device)\n      return worker_device_spec.to_string()\n\n  return local_device_chooser\n\n\ndef _scale_tower_loss(tower_spec, loss_reduction, number_of_towers):\n  """"""Produce an EstimatorSpec with approproriately scaled loss.""""""\n  if tower_spec.loss is None:\n    return tower_spec\n\n  estimator_spec = _asdict(tower_spec)\n  estimator_spec[\'loss\'] = _scale_loss(tower_spec.loss, loss_reduction,\n                                       number_of_towers)\n  return model_fn_lib.EstimatorSpec(**estimator_spec)\n\n\ndef _scale_loss(loss, loss_reduction, number_of_towers):\n  """"""If needed, scale down the loss for averaging loss by summing.""""""\n  if loss is None:\n    return None\n  if number_of_towers == 1:\n    return loss\n\n  if loss_reduction != losses.Reduction.SUM:\n    return math_ops.div(loss, 1.0 * number_of_towers, name=\'averaged_loss\')\n  else:\n    return loss\n\n\ndef _minimize_towers(tower_specs):\n  """"""`train_op` of the last tower applies aggregated gradients.""""""\n  return tower_specs[-1].train_op\n\n\ndef _compute_sum_on_device(values, device, name=None):\n  with ops_lib.device(device):\n    if isinstance(values[0], ops_lib.IndexedSlices):\n      if name:\n        raise ValueError(\'The name {} is not expected to be given to \'\n                         \'IndexedSlices {}\'.format(name, values))\n\n      values_concat = array_ops.concat([v.values for v in values], axis=0)\n      indices_concat = array_ops.concat([v.indices for v in values], axis=0)\n      return ops_lib.IndexedSlices(values_concat, indices_concat,\n                                   values[0].dense_shape)\n    else:\n      return math_ops.add_n(values, name=name)\n\n\ndef _train_spec(tower_specs,\n                train_op,\n                aggregation_device,\n                aggregated_loss_name=\'loss\'):\n  """"""Populate replicated EstimatorSpec for `GraphKeys.TRAIN`.""""""\n  # Spec of the last tower is used as the template for the final spec, because\n  # some `EstimatorSpec.training_hooks` rely on calls made in model_fn.  For\n  # example, `SyncReplicasOptimizerHook` validates the\n  # `SyncReplicasOptimizer.apply_gradients` call. `TowerEstimator` makes that\n  # call only in the last tower.\n  estimator_spec = _asdict(tower_specs[-1])\n  estimator_spec[\'mode\'] = model_fn_lib.ModeKeys.TRAIN\n  estimator_spec[\'train_op\'] = train_op\n  estimator_spec[\'loss\'] = _compute_sum_on_device(\n      [spec.loss for spec in tower_specs], aggregation_device,\n      aggregated_loss_name)\n  return model_fn_lib.EstimatorSpec(**estimator_spec)\n\n\ndef _eval_spec(tower_specs, aggregation_device, aggregated_loss_name=\'loss\'):\n  """"""Populate replicated EstimatorSpec for `GraphKeys.EVAL`.""""""\n  estimator_spec = _asdict(tower_specs[0])\n  estimator_spec[\'mode\'] = model_fn_lib.ModeKeys.EVAL\n  estimator_spec[\'loss\'] = _compute_sum_on_device(\n      [spec.loss for spec in tower_specs], aggregation_device,\n      aggregated_loss_name)\n\n  update_ops = []\n  for tower_spec in tower_specs:\n    for name, (_, update_op) in six.iteritems(tower_spec.eval_metric_ops):\n      update_ops.append(update_op)\n\n  with ops_lib.control_dependencies(update_ops):\n    reduced_update_op = _reduce_metric_variables(len(tower_specs))\n\n  eval_metric_ops = {}\n  for name, (metric_tensor, _) in six.iteritems(tower_specs[0].eval_metric_ops):\n    eval_metric_ops[name] = (metric_tensor, reduced_update_op)\n  estimator_spec[\'eval_metric_ops\'] = eval_metric_ops\n  return model_fn_lib.EstimatorSpec(**estimator_spec)\n\n\ndef _reduce_metric_variables(number_of_towers):\n  """"""Aggregate local variables used in metrics into the first tower.""""""\n  if number_of_towers == 1:\n    return control_flow_ops.no_op(name=\'no_eval_metric_reduction\')\n\n  metric_variables = ops_lib.get_collection(ops_lib.GraphKeys.METRIC_VARIABLES)\n  variables_per_tower = len(metric_variables) // number_of_towers\n\n  if len(metric_variables) % number_of_towers != 0:\n    raise ValueError(\n        \'Different `EstimatorSpec.eval_metric_ops` across `model_fn()` calls.\'\n        \' Expected {} local variables, but got {} instead.\'.format(\n            variables_per_tower * number_of_towers, len(metric_variables)))\n\n  # `metric_variables` has the size of `variables_per_tower` x\n  #  number_of_towers.  Each tower is produced by calling the same model_fn.\n  #  First `variables_per_tower` correspond to the first tower.  Each such\n  #  variable has an replica at the `(variables_per_tower * i)` position, where\n  #  `i` is `[1.. number_of_towers]`.  We are going to add values from replicas\n  #  to each variable of the first tower.  We then zero out replica values, so\n  #  that `_reduce_metric_variables` operation is idempotent.  If a metric\n  #  is then computed based on local variables from the first tower, then the\n  #  resulting metric is an estimate for all `number_of_towers` towers.\n  ops = []\n  for i in range(0, variables_per_tower):\n    next_replica_id = i + variables_per_tower\n    replicas = [\n        metric_variables[replica_id]\n        for replica_id in range(next_replica_id, len(metric_variables),\n                                variables_per_tower)\n    ]  #  `replicas` doesn\'t contain the first-tower variable.\n\n    reduce_op = state_ops.assign_add(metric_variables[i],\n                                     math_ops.add_n(replicas))\n\n    with ops_lib.control_dependencies([reduce_op]):\n      for replica in replicas:\n        zeros_for_replica = array_ops.zeros(\n            array_ops.shape(replica), dtype=replica.dtype)\n        zero_out_replica_op = state_ops.assign(replica, zeros_for_replica)\n        ops.append(zero_out_replica_op)\n\n  return control_flow_ops.group(*ops)\n\n\ndef _predict_spec(tower_specs, aggregation_device):\n  """"""Populate replicated EstimatorSpec for `GraphKeys.PREDICT`.""""""\n  estimator_spec = _asdict(tower_specs[0])\n  estimator_spec[\'mode\'] = model_fn_lib.ModeKeys.PREDICT\n\n  with ops_lib.device(aggregation_device):\n    estimator_spec[\'predictions\'] = _concat_tensor_dicts(\n        *[tower_spec.predictions for tower_spec in tower_specs])\n\n    export_outputs_dict = _dict_concat(\n        *[tower_spec.export_outputs for tower_spec in tower_specs])\n\n    export_outputs = {}\n    for name, export_output_list in six.iteritems(export_outputs_dict):\n      if isinstance(export_output_list[0], export_output_lib.PredictOutput):\n        export_outputs[name] = export_output_lib.PredictOutput(\n            outputs=_concat_tensor_dicts(*[\n                export_output.outputs for export_output in export_output_list\n            ]))\n      elif isinstance(export_output_list[0],\n                      export_output_lib.RegressionOutput):\n        export_outputs[name] = export_output_lib.RegressionOutput(\n            value=array_ops.concat(\n                [export_output.value for export_output in export_output_list],\n                axis=0))\n      elif isinstance(export_output_list[0],\n                      export_output_lib.ClassificationOutput):\n        scores = None\n        if export_output_list[0].scores is not None:\n          scores = array_ops.concat(\n              [export_output.scores for export_output in export_output_list],\n              axis=0)\n\n        classes = None\n        if export_output_list[0].classes is not None:\n          classes = array_ops.stack(\n              [export_output.classes for export_output in export_output_list],\n              axis=0)\n\n        export_outputs[name] = export_output_lib.ClassificationOutput(\n            scores=scores, classes=classes)\n\n  estimator_spec[\'export_outputs\'] = export_outputs\n  return model_fn_lib.EstimatorSpec(**estimator_spec)\n\n\ndef _concat_tensor_dicts(*tensor_dicts):\n  return {\n      name: array_ops.concat(tensors, axis=0, name=name)\n      for name, tensors in six.iteritems(_dict_concat(*tensor_dicts))\n  }\n\n\ndef _extract_tensors(tensors_and_vars):\n  tensors = []\n  for tensor_and_var in tensors_and_vars:\n    tensor, _ = tensor_and_var\n    if isinstance(tensor, ops_lib.IndexedSlices):\n      tensors.append(tensor.values)\n    else:\n      tensors.append(tensor)\n  return tensors\n\n\ndef _dict_concat(*dicts):\n  list_dict = {}\n  for d in dicts:\n    if d is None:\n      continue\n\n    for k, v in six.iteritems(d):\n      list_dict.setdefault(k, []).append(v)\n  return list_dict\n\n\ndef _asdict(namedtuple):\n  """"""Returns a namedtuple as a dictionary.\n\n  This is required because `_asdict()` in Python 3.x.x is broken in classes\n  that inherit from `collections.namedtuple`. See\n  https://bugs.python.org/issue24931 for more details.\n\n  Args:\n    namedtuple: An object that inherits from `collections.namedtuple`.\n\n  Returns:\n    A dictionary version of the tuple.\n  """"""\n  return {k: getattr(namedtuple, k) for k in namedtuple._fields}\n'"
train_cpn_onebyone.py,147,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import cpn as cpn\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_cpn/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 50,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'epochs_per_eval\', 20,\n    \'The number of training epochs to run between evaluations.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 10,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 1e-5, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 1e-4, \'Initial learning rate.\')#1e-3\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model/resnet50\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', None,\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_string(\n    \'cloud_checkpoint_path\', \'resnet50\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_boolean(\n    \'seq_train\', False,\n    \'Wether we will train a sequence model.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=FLAGS.epochs_per_eval):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = cpn.cascaded_pyramid_net(features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n    #print(pred_outputs)\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n    blur_list = [1., 1.37, 1.73, 2.4, None]#[1., 1.5, 2., 3., None]\n    #blur_list = [None, None, None, None, None]\n\n    targets_list = []\n    for sigma in blur_list:\n        if sigma is None:\n            targets_list.append(targets)\n        else:\n            # always channels first foe targets\n            targets_list.append(gaussian_blur(targets, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], sigma, params[\'data_format\'], \'blur_{}\'.format(sigma)))\n\n    # print(key_v)\n    #targets = tf.reshape(255.*tf.one_hot(tf.ones_like(key_v,tf.int64)*(params[\'heatmap_size\']*params[\'heatmap_size\']//2+params[\'heatmap_size\']), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n    #norm_value = tf.ones_like(norm_value)\n    # score_map = tf.reshape(tf.one_hot(tf.ones_like(key_v,tf.int64)*(31*64+31), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n    # filter all invisible keypoint maybe better for this task\n    # all_visible = tf.logical_and(key_v>0, isvalid>0)\n    # targets_list = [tf.boolean_mask(targets_list[ind], all_visible) for ind in list(range(len(targets_list)))]\n    # pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets_list = [targets_list[ind] * all_visible for ind in list(range(len(targets_list)))]\n    pred_outputs = [pred_outputs[ind] * all_visible for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    base_learning_rate = params[\'learning_rate\']\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        base_learning_rate = 1. * base_learning_rate\n        for pred_ind in list(range(len(pred_outputs) - 1)):\n            mse_loss_list.append(0.5 * tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets_list[-1], pred_outputs[-1], weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets_list[-1], gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs[-1], gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(len(pred_outputs) - 1),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        for pred_ind in list(range(len(pred_outputs))):\n            mse_loss_list.append(tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets_list[pred_ind]/255., name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [base_learning_rate * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold_(params[\'checkpoint_path\'], params[\'model_dir\'], params[\'checkpoint_exclude_scopes\'], params[\'model_scope\'], params[\'checkpoint_model_scope\'], params[\'ignore_missing_vars\'])))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, epochs_per_eval, lr_decay_factors, decay_boundaries, checkpoint_path=None, checkpoint_exclude_scopes=\'\', checkpoint_model_scope=\'\', ignore_missing_vars=True):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // FLAGS.batch_size\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'checkpoint_exclude_scopes\': checkpoint_exclude_scopes,\n            \'model_scope\': model_scope,\n            \'checkpoint_model_scope\': checkpoint_model_scope,\n            \'ignore_missing_vars\': ignore_missing_vars,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'batch_size\': FLAGS.batch_size,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(lr_decay_factors),\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    for _ in range(train_epochs // epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, epochs_per_eval), hooks=[logging_hook], max_steps=(steps_per_epoch*train_epochs))\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, model_scope, 1))\n        tf.logging.info(eval_results)\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    if FLAGS.seq_train:\n        detail_params = {\n            \'all\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'all\'),\n                \'train_epochs\': 6,\n                \'epochs_per_eval\': 4,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'3, 4\',\n                \'model_scope\': \'all\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'blouse/feature_pyramid/conv_heatmap, blouse/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'dress/feature_pyramid/conv_heatmap, dress/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'outwear/feature_pyramid/conv_heatmap, outwear/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'skirt/feature_pyramid/conv_heatmap, skirt/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'trousers/feature_pyramid/conv_heatmap, trousers/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    else:\n        detail_params = {\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'blouse/feature_pyramid, blouse/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'dress/feature_pyramid, dress/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'outwear/feature_pyramid, outwear/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'skirt/feature_pyramid, skirt/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'trousers/feature_pyramid, trousers/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n\n    # import datetime\n    # import time\n    # while True:\n    #     time.sleep(1600)\n    #     if \'8\' in datetime.datetime.now().time().strftime(\'%H\'):\n    #         break\n\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'epochs_per_eval\'], detail_params[m][\'lr_decay_factors\'], detail_params[m][\'decay_boundaries\'], detail_params[m][\'checkpoint_path\'], detail_params[m][\'checkpoint_exclude_scopes\'], detail_params[m][\'checkpoint_model_scope\'], detail_params[m][\'ignore_missing_vars\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
train_detnet_cpn_onebyone.py,147,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import detnet_cpn as cpn\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_detnet_cpn/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 50,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'epochs_per_eval\', 20,\n    \'The number of training epochs to run between evaluations.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 10,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 1e-5, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 1e-4, \'Initial learning rate.\')#1e-3\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model/resnet50\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', None,\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_string(\n    \'cloud_checkpoint_path\', \'resnet50\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_boolean(\n    \'seq_train\', False,\n    \'Wether we will train a sequence model.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=FLAGS.epochs_per_eval):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = cpn.cascaded_pyramid_net(features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n    #print(pred_outputs)\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n    blur_list = [1., 1.37, 1.73, 2.4, None]#[1., 1.5, 2., 3., None]\n    #blur_list = [None, None, None, None, None]\n\n    targets_list = []\n    for sigma in blur_list:\n        if sigma is None:\n            targets_list.append(targets)\n        else:\n            # always channels first foe targets\n            targets_list.append(gaussian_blur(targets, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], sigma, params[\'data_format\'], \'blur_{}\'.format(sigma)))\n\n    # print(key_v)\n    #targets = tf.reshape(255.*tf.one_hot(tf.ones_like(key_v,tf.int64)*(params[\'heatmap_size\']*params[\'heatmap_size\']//2+params[\'heatmap_size\']), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n    #norm_value = tf.ones_like(norm_value)\n    # score_map = tf.reshape(tf.one_hot(tf.ones_like(key_v,tf.int64)*(31*64+31), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n    # filter all invisible keypoint maybe better for this task\n    # all_visible = tf.logical_and(key_v>0, isvalid>0)\n    # targets_list = [tf.boolean_mask(targets_list[ind], all_visible) for ind in list(range(len(targets_list)))]\n    # pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets_list = [targets_list[ind] * all_visible for ind in list(range(len(targets_list)))]\n    pred_outputs = [pred_outputs[ind] * all_visible for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    base_learning_rate = params[\'learning_rate\']\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        base_learning_rate = 1. * base_learning_rate\n        for pred_ind in list(range(len(pred_outputs) - 1)):\n            mse_loss_list.append(0.5 * tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets_list[-1], pred_outputs[-1], weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets_list[-1], gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs[-1], gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(len(pred_outputs) - 1),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        for pred_ind in list(range(len(pred_outputs))):\n            mse_loss_list.append(tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets_list[pred_ind]/255., name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [base_learning_rate * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold_(params[\'checkpoint_path\'], params[\'model_dir\'], params[\'checkpoint_exclude_scopes\'], params[\'model_scope\'], params[\'checkpoint_model_scope\'], params[\'ignore_missing_vars\'])))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, epochs_per_eval, lr_decay_factors, decay_boundaries, checkpoint_path=None, checkpoint_exclude_scopes=\'\', checkpoint_model_scope=\'\', ignore_missing_vars=True):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // FLAGS.batch_size\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'checkpoint_exclude_scopes\': checkpoint_exclude_scopes,\n            \'model_scope\': model_scope,\n            \'checkpoint_model_scope\': checkpoint_model_scope,\n            \'ignore_missing_vars\': ignore_missing_vars,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'batch_size\': FLAGS.batch_size,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(lr_decay_factors),\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    for _ in range(train_epochs // epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, epochs_per_eval), hooks=[logging_hook], max_steps=(steps_per_epoch*train_epochs))\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, model_scope, 1))\n        tf.logging.info(eval_results)\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    if FLAGS.seq_train:\n        detail_params = {\n            \'all\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'all\'),\n                \'train_epochs\': 6,\n                \'epochs_per_eval\': 4,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'3, 4\',\n                \'model_scope\': \'all\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'blouse/additional_layer, blouse/feature_pyramid/conv_heatmap, blouse/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'dress/additional_layer, dress/feature_pyramid/conv_heatmap, dress/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'outwear/additional_layer, outwear/feature_pyramid/conv_heatmap, outwear/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'skirt/additional_layer, skirt/feature_pyramid/conv_heatmap, skirt/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'trousers/additional_layer, trousers/feature_pyramid/conv_heatmap, trousers/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    else:\n        detail_params = {\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'blouse/additional_layer, blouse/feature_pyramid, blouse/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'dress/additional_layer, dress/feature_pyramid, dress/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'outwear/additional_layer, outwear/feature_pyramid, outwear/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'skirt/additional_layer, skirt/feature_pyramid, skirt/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'trousers/additional_layer, trousers/feature_pyramid, trousers/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n\n    # import datetime\n    # import time\n    # while True:\n    #     time.sleep(1600)\n    #     if \'8\' in datetime.datetime.now().time().strftime(\'%H\'):\n    #         break\n\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'epochs_per_eval\'], detail_params[m][\'lr_decay_factors\'], detail_params[m][\'decay_boundaries\'], detail_params[m][\'checkpoint_path\'], detail_params[m][\'checkpoint_exclude_scopes\'], detail_params[m][\'checkpoint_model_scope\'], detail_params[m][\'ignore_missing_vars\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n\n\n# 0.04623710017640076\n# blouse: 0.04324096205446997\n# dress: 0.04344861161278246\n# outwear: 0.04659189066954178\n# skirt: 0.05859064040038493\n# trousers: 0.049271125570990434\n'"
train_detxt_cpn_onebyone.py,147,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import detxt_cpn as cpn\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_detxt_cpn/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 50,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'epochs_per_eval\', 20,\n    \'The number of training epochs to run between evaluations.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 10,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 1e-5, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 1e-4, \'Initial learning rate.\')#1e-3\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model/seresnext50\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', None,\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_string(\n    \'cloud_checkpoint_path\', \'seresnext50\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_boolean(\n    \'seq_train\', False,\n    \'Wether we will train a sequence model.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=FLAGS.epochs_per_eval):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = cpn.cascaded_pyramid_net(features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n    #print(pred_outputs)\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n    blur_list = [1., 1.37, 1.73, 2.4, None]#[1., 1.5, 2., 3., None]\n    #blur_list = [None, None, None, None, None]\n\n    targets_list = []\n    for sigma in blur_list:\n        if sigma is None:\n            targets_list.append(targets)\n        else:\n            # always channels first foe targets\n            targets_list.append(gaussian_blur(targets, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], sigma, params[\'data_format\'], \'blur_{}\'.format(sigma)))\n\n    # print(key_v)\n    #targets = tf.reshape(255.*tf.one_hot(tf.ones_like(key_v,tf.int64)*(params[\'heatmap_size\']*params[\'heatmap_size\']//2+params[\'heatmap_size\']), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n    #norm_value = tf.ones_like(norm_value)\n    # score_map = tf.reshape(tf.one_hot(tf.ones_like(key_v,tf.int64)*(31*64+31), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n    # filter all invisible keypoint maybe better for this task\n    # all_visible = tf.logical_and(key_v>0, isvalid>0)\n    # targets_list = [tf.boolean_mask(targets_list[ind], all_visible) for ind in list(range(len(targets_list)))]\n    # pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets_list = [targets_list[ind] * all_visible for ind in list(range(len(targets_list)))]\n    pred_outputs = [pred_outputs[ind] * all_visible for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    base_learning_rate = params[\'learning_rate\']\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        base_learning_rate = 1. * base_learning_rate\n        for pred_ind in list(range(len(pred_outputs) - 1)):\n            mse_loss_list.append(0.5 * tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets_list[-1], pred_outputs[-1], weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets_list[-1], gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs[-1], gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(len(pred_outputs) - 1),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        for pred_ind in list(range(len(pred_outputs))):\n            mse_loss_list.append(tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets_list[pred_ind]/255., name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [base_learning_rate * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold_(params[\'checkpoint_path\'], params[\'model_dir\'], params[\'checkpoint_exclude_scopes\'], params[\'model_scope\'], params[\'checkpoint_model_scope\'], params[\'ignore_missing_vars\'])))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, epochs_per_eval, lr_decay_factors, decay_boundaries, checkpoint_path=None, checkpoint_exclude_scopes=\'\', checkpoint_model_scope=\'\', ignore_missing_vars=True):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // FLAGS.batch_size\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'checkpoint_exclude_scopes\': checkpoint_exclude_scopes,\n            \'model_scope\': model_scope,\n            \'checkpoint_model_scope\': checkpoint_model_scope,\n            \'ignore_missing_vars\': ignore_missing_vars,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'batch_size\': FLAGS.batch_size,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(lr_decay_factors),\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    for _ in range(train_epochs // epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, epochs_per_eval), hooks=[logging_hook], max_steps=(steps_per_epoch*train_epochs))\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, model_scope, 1))\n        tf.logging.info(eval_results)\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    if FLAGS.seq_train:\n        detail_params = {\n            \'all\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'all\'),\n                \'train_epochs\': 6,\n                \'epochs_per_eval\': 4,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'3, 4\',\n                \'model_scope\': \'all\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'blouse/additional_layer, blouse/feature_pyramid/conv_heatmap, blouse/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'dress/additional_layer, dress/feature_pyramid/conv_heatmap, dress/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'outwear/additional_layer, outwear/feature_pyramid/conv_heatmap, outwear/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'skirt/additional_layer, skirt/feature_pyramid/conv_heatmap, skirt/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'trousers/additional_layer, trousers/feature_pyramid/conv_heatmap, trousers/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    else:\n        detail_params = {\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'blouse/additional_layer, blouse/feature_pyramid, blouse/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'dress/additional_layer, dress/feature_pyramid, dress/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'outwear/additional_layer, outwear/feature_pyramid, outwear/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'skirt/additional_layer, skirt/feature_pyramid, skirt/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'trousers/additional_layer, trousers/feature_pyramid, trousers/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'epochs_per_eval\'], detail_params[m][\'lr_decay_factors\'], detail_params[m][\'decay_boundaries\'], detail_params[m][\'checkpoint_path\'], detail_params[m][\'checkpoint_exclude_scopes\'], detail_params[m][\'checkpoint_model_scope\'], detail_params[m][\'ignore_missing_vars\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n\n# 0.0433054647096145\n# blouse: 0.04104559849382152\n# dress: 0.040321287576354434\n# outwear: 0.04271434626552231\n# skirt: 0.054697498510954054\n# trousers: 0.04762229379563965\n'"
train_hg_onebyone.py,139,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import hourglass as hg\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_hg/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_integer(\'feats_channals\', 256, \'Number of features in the hourglass.\')\ntf.app.flags.DEFINE_integer(\'num_stacks\', 4, \'Number of hourglasses to stack.\')#8\ntf.app.flags.DEFINE_integer(\'num_modules\', 1, \'Number of residual modules at each location in the hourglass.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 50,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'epochs_per_eval\', 20,\n    \'The number of training epochs to run between evaluations.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 6,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180406, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00000, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.0,#0.9\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 5e-3, \'Initial learning rate.\')#2.5e-4\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'all\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', \'all\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_boolean(\n    \'seq_train\', False,\n    \'Wether we will train a sequence model.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=FLAGS.epochs_per_eval):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = hg.create_model(features, params[\'num_stacks\'], params[\'feats_channals\'],\n                            config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'num_modules\'],\n                            (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n\n    # print(key_v)\n    #targets = tf.reshape(255.*tf.one_hot(tf.ones_like(key_v,tf.int64)*(32*64+32), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n    #norm_value = tf.ones_like(norm_value)\n    # score_map = tf.reshape(tf.one_hot(tf.ones_like(key_v,tf.int64)*(31*64+31), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n\n    # filter all invisible keypoint maybe better for this task\n    # all_visible = tf.logical_and(key_v>0, isvalid>0)\n    # targets = tf.boolean_mask(targets, all_visible)\n    # pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets = targets * all_visible\n    pred_outputs = [pred_outputs[ind] * all_visible for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    base_learning_rate = params[\'learning_rate\']\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        base_learning_rate = 1.5 * base_learning_rate\n        for pred_ind in list(range(len(pred_outputs) - 1)):\n            mse_loss_list.append(0.6 * tf.losses.mean_squared_error(targets, pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets, pred_outputs[-1], weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets, gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs[-1], gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(len(pred_outputs) - 1),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        for pred_ind in list(range(len(pred_outputs))):\n            mse_loss_list.append(tf.losses.mean_squared_error(targets, pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets, name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [base_learning_rate * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold_(params[\'checkpoint_path\'], params[\'model_dir\'], params[\'checkpoint_exclude_scopes\'], params[\'model_scope\'], params[\'checkpoint_model_scope\'], params[\'ignore_missing_vars\'])))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, epochs_per_eval, lr_decay_factors, decay_boundaries, checkpoint_path=None, checkpoint_exclude_scopes=\'\', checkpoint_model_scope=\'\', ignore_missing_vars=True):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // FLAGS.batch_size\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'checkpoint_exclude_scopes\': checkpoint_exclude_scopes,\n            \'model_scope\': model_scope,\n            \'checkpoint_model_scope\': checkpoint_model_scope,\n            \'ignore_missing_vars\': ignore_missing_vars,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'feats_channals\': FLAGS.feats_channals,\n            \'num_stacks\': FLAGS.num_stacks,\n            \'num_modules\': FLAGS.num_modules,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'batch_size\': FLAGS.batch_size,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(lr_decay_factors),\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    for _ in range(train_epochs // epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, epochs_per_eval), hooks=[logging_hook], max_steps=(steps_per_epoch*train_epochs))\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, model_scope, 1))\n        tf.logging.info(eval_results)\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    if FLAGS.seq_train:\n        detail_params = {\n            \'all\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'all\'),\n                \'train_epochs\': 6,\n                \'epochs_per_eval\': 4,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'3, 4\',\n                \'model_scope\': \'all\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'blouse/hg_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'dress/hg_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'outwear/hg_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'skirt/hg_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'trousers/hg_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    else:\n        detail_params = {\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 40,\n                \'epochs_per_eval\': 15,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 40,\n                \'epochs_per_eval\': 15,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 40,\n                \'epochs_per_eval\': 15,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 40,\n                \'epochs_per_eval\': 15,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 40,\n                \'epochs_per_eval\': 15,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'epochs_per_eval\'], detail_params[m][\'lr_decay_factors\'], detail_params[m][\'decay_boundaries\'], detail_params[m][\'checkpoint_path\'], detail_params[m][\'checkpoint_exclude_scopes\'], detail_params[m][\'checkpoint_model_scope\'], detail_params[m][\'ignore_missing_vars\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
train_hg_seqnet.py,37,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport tensorflow as tf\nimport config\n\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'*\'\n    \'dataset_split_name\', \'blouse\', \'The name of the train/test split.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 256,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 64,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_integer(\'feats_channals\', 256, \'Number of features in the hourglass.\')\ntf.app.flags.DEFINE_integer(\'num_stacks\', 8, \'Number of hourglasses to stack.\')#8\ntf.app.flags.DEFINE_integer(\'num_modules\', 1, \'Number of residual modules at each location in the hourglass.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 5,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'epochs_per_eval\', 1,\n    \'The number of training epochs to run between evaluations.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 6,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180406, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00000, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1.0, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.0,#0.9\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 2.5e-4, \'Initial learning rate.\')#2.5e-4\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', None,\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', \'all\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,#\'all/hg_heatmap\',#\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_boolean(\n    \'seq_train\', True,\n    \'Wether we will train a sequence model.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'all, blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n\ntotal_params = {\n    \'--data_dir\': FLAGS.data_dir,\n    \'--dataset_name\': FLAGS.dataset_name,\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'*\'\n    \'--model_dir\': FLAGS.model_dir,\n    \'--save_checkpoints_secs\': FLAGS.save_checkpoints_secs,\n    \'--train_image_size\': FLAGS.train_image_size,\n    \'--heatmap_size\': FLAGS.heatmap_size,\n    \'--heatmap_sigma\': FLAGS.heatmap_sigma,\n    \'--feats_channals\': FLAGS.feats_channals,\n    \'--num_stacks\': FLAGS.num_stacks,\n    \'--num_modules\': FLAGS.num_modules,\n    \'--bbox_border\': FLAGS.bbox_border,\n    \'--train_epochs\': FLAGS.train_epochs,\n    \'--epochs_per_eval\': FLAGS.epochs_per_eval,\n    \'--batch_size\': FLAGS.batch_size,\n    \'--data_format\': FLAGS.data_format,\n    \'--tf_random_seed\': FLAGS.tf_random_seed,\n    \'--weight_decay\': FLAGS.weight_decay,\n    \'--mse_weight\': FLAGS.mse_weight,\n    \'--momentum\': FLAGS.momentum,\n    \'--learning_rate\': FLAGS.learning_rate,\n    \'--end_learning_rate\': FLAGS.end_learning_rate,\n    \'--warmup_learning_rate\': FLAGS.warmup_learning_rate,\n    \'--warmup_steps\': FLAGS.warmup_steps,\n    \'--decay_boundaries\': FLAGS.decay_boundaries,\n    \'--lr_decay_factors\': FLAGS.lr_decay_factors,\n    \'--checkpoint_path\': FLAGS.checkpoint_path,\n    \'--checkpoint_model_scope\': FLAGS.checkpoint_model_scope,\n    \'--model_scope\': FLAGS.model_scope,\n    \'--checkpoint_exclude_scopes\': FLAGS.checkpoint_exclude_scopes,\n    \'--run_on_cloud\': FLAGS.run_on_cloud\n    }\n\nif FLAGS.seq_train:\n    detail_params = {\n        \'all\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'all\'),\n            \'train_epochs\': 6,\n            \'epochs_per_eval\': 3,\n            \'decay_boundaries\': \'3, 4\',\n            \'model_scope\': \'all\',\n        },\n        \'blouse\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n            \'train_epochs\': 50,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'15, 30\',\n            \'model_scope\': \'blouse\',\n            \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n            \'checkpoint_model_scope\': \'all\',\n            \'checkpoint_exclude_scopes\': \'blouse/hg_heatmap\',\n        },\n        \'dress\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n            \'train_epochs\': 50,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'15, 30\',\n            \'model_scope\': \'dress\',\n            \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n            \'checkpoint_model_scope\': \'all\',\n            \'checkpoint_exclude_scopes\': \'dress/hg_heatmap\',\n        },\n        \'outwear\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n            \'train_epochs\': 50,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'15, 30\',\n            \'model_scope\': \'outwear\',\n            \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n            \'checkpoint_model_scope\': \'all\',\n            \'checkpoint_exclude_scopes\': \'outwear/hg_heatmap\',\n        },\n        \'skirt\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n            \'train_epochs\': 50,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'15, 30\',\n            \'model_scope\': \'skirt\',\n            \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n            \'checkpoint_model_scope\': \'all\',\n            \'checkpoint_exclude_scopes\': \'skirt/hg_heatmap\',\n        },\n        \'trousers\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n            \'train_epochs\': 50,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'15, 30\',\n            \'model_scope\': \'trousers\',\n            \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n            \'checkpoint_model_scope\': \'all\',\n            \'checkpoint_exclude_scopes\': \'trousers/hg_heatmap\',\n        },\n    }\nelse:\n    detail_params = {\n        \'blouse\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n            \'train_epochs\': 60,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'20, 40\',\n            \'model_scope\': \'blouse\',\n        },\n        \'dress\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n            \'train_epochs\': 60,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'20, 40\',\n            \'model_scope\': \'dress\',\n        },\n        \'outwear\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n            \'train_epochs\': 60,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'20, 40\',\n            \'model_scope\': \'outwear\',\n        },\n        \'skirt\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n            \'train_epochs\': 60,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'20, 40\',\n            \'model_scope\': \'skirt\',\n        },\n        \'trousers\': {\n            \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n            \'train_epochs\': 60,\n            \'epochs_per_eval\': 20,\n            \'decay_boundaries\': \'20, 40\',\n            \'model_scope\': \'trousers\',\n        },\n    }\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\ndef parse_str_comma_list(args):\n    return [s.strip() for s in args.split(\',\')]\n\ndef main(_):\n    import subprocess\n    import copy\n\n    #[\'skirt\', \'dress\', \'outwear\', \'trousers\']#\n    all_category = parse_str_comma_list(FLAGS.model_to_train)\n\n    for cat in all_category:\n        tf.gfile.MakeDirs(os.path.join(FLAGS.model_dir, cat))\n\n    for cat in all_category:\n        temp_params = copy.deepcopy(total_params)\n        for k, v in total_params.items():\n            if k[2:] in detail_params[cat]:\n                temp_params[k] = detail_params[cat][k[2:]]\n\n        params_str = []\n        for k, v in temp_params.items():\n            if v is not None:\n                params_str.append(k)\n                params_str.append(str(v))\n        print(\'params send: \', params_str)\n        train_process = subprocess.Popen([\'python\', \'./train_subnet.py\'] + params_str, stdout=subprocess.PIPE, cwd=os.getcwd())\n        output, _ = train_process.communicate()\n        print(output)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
train_hg_subnet.py,117,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import hourglass as hg\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 256,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 64,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_integer(\'feats_channals\', 256, \'Number of features in the hourglass.\')\ntf.app.flags.DEFINE_integer(\'num_stacks\', 8, \'Number of hourglasses to stack.\')#8\ntf.app.flags.DEFINE_integer(\'num_modules\', 1, \'Number of residual modules at each location in the hourglass.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 50,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'epochs_per_eval\', 20,\n    \'The number of training epochs to run between evaluations.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 6,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180406, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00000, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.0,#0.9\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 2.5e-3, \'Initial learning rate.\')#2.5e-4\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'all\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', \'all\',\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\ndef input_pipeline(is_training=True, num_epochs=FLAGS.epochs_per_eval):\n    if \'all\' in FLAGS.model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(FLAGS.model_scope if \'all\' not in FLAGS.model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(FLAGS.model_scope if \'all\' not in FLAGS.model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 120\n      #print(img_to_save)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=255, off_value=0, axis=-1, dtype=tf.int32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(targets * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = hg.create_model(features, params[\'num_stacks\'], params[\'feats_channals\'],\n                            config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'num_modules\'],\n                            (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n\n    all_visible = tf.logical_and(key_v>0, isvalid>0)\n    targets = tf.boolean_mask(targets, all_visible)\n    pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n\n    mse_loss_list = []\n    for pred_ind in list(range(len(pred_outputs))):\n        mse_loss_list.append(tf.losses.mean_squared_error(targets, pred_outputs[pred_ind],\n                            weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                            scope=\'loss_{}\'.format(pred_ind),\n                            loss_collection=None,#tf.GraphKeys.LOSSES,\n                            reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets, name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n(\n                              [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n                               if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [params[\'learning_rate\'] * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold(FLAGS)))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    fashionAI = tf.estimator.Estimator(\n        model_fn=keypoint_model_fn, model_dir=FLAGS.model_dir, config=run_config,\n        params={\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'feats_channals\': FLAGS.feats_channals,\n            \'num_stacks\': FLAGS.num_stacks,\n            \'num_modules\': FLAGS.num_modules,\n            \'data_format\': FLAGS.data_format,\n            \'model_scope\': FLAGS.model_scope,\n            \'steps_per_epoch\': config.split_size[(FLAGS.model_scope if \'all\' not in FLAGS.model_scope else \'*\')][\'train\'] // FLAGS.batch_size,\n            \'batch_size\': FLAGS.batch_size,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(FLAGS.decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(FLAGS.lr_decay_factors),\n        })\n    if not FLAGS.run_on_cloud:\n        tf.logging.info(\'params recv: %s\', FLAGS.flag_values_dict())\n    tf.gfile.MakeDirs(FLAGS.model_dir)\n    for _ in range(FLAGS.train_epochs // FLAGS.epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \', \'.join([\'%s=%.7f\' % (k, v) for k, v in dicts.items()]))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True), hooks=[logging_hook])\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, 1))\n        tf.logging.info(eval_results)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
train_large_xt_cpn_onebyone.py,148,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nimport tf_replicate_model_fn\n\nfrom net import detxt_cpn\nfrom net import seresnet_cpn\n\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_large_{}_cpn/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_steps\', 8000,\n    \'The frequency with which the model is saved, in steps.\')\n# model related configuration\ntf.app.flags.DEFINE_string(\n    \'backbone\', \'sext\', # \'detxt\' or \'sext\'\n    \'The backbone network to use for feature extraction.\')\ntf.app.flags.DEFINE_integer(\n    \'net_depth\', 101,\n    \'The depth of the backbone network for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 4,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 1e-5, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 7e-5, \'Initial learning rate.\')#1e-3\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model/seresnext{}\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', None,\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_boolean(\n    \'multi_gpu\', True,\n    \'Wether we will use multi-GPUs to train.\')\ntf.app.flags.DEFINE_string(\n    \'cloud_checkpoint_path\', \'seresnext{}\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\n\ndef validate_batch_size_for_multi_gpu(batch_size):\n    """"""For multi-gpu, batch-size must be a multiple of the number of\n    available GPUs.\n\n    Note that this should eventually be handled by replicate_model_fn\n    directly. Multi-GPU support is currently experimental, however,\n    so doing the work here until that feature is in place.\n    """"""\n    if not FLAGS.multi_gpu:\n        return 0\n\n    from tensorflow.python.client import device_lib\n\n    local_device_protos = device_lib.list_local_devices()\n    num_gpus = sum([1 for d in local_device_protos if d.device_type == \'GPU\'])\n    if not num_gpus:\n        raise ValueError(\'Multi-GPU mode was specified, but no GPUs \'\n                        \'were found. To use CPU, run without --multi_gpu=False.\')\n\n    remainder = batch_size % num_gpus\n    if remainder:\n        err = (\'When running with multiple GPUs, batch size \'\n                \'must be a multiple of the number of available GPUs. \'\n                \'Found {} GPUs with a batch size of {}; try --batch_size={} instead.\'\n                ).format(num_gpus, batch_size, batch_size - remainder)\n        raise ValueError(err)\n    return num_gpus\n\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=None):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\nbackbone_ = seresnet_cpn.xt_cascaded_pyramid_net if \'sext\' in FLAGS.backbone.strip() else detxt_cpn.cascaded_pyramid_net\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = backbone_(features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'], net_depth=params[\'net_depth\'])\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n    blur_list = [1., 1.37, 1.73, 2.4, None]#[1., 1.5, 2., 3., None]\n    #blur_list = [None, None, None, None, None]\n\n    targets_list = []\n    for sigma in blur_list:\n        if sigma is None:\n            targets_list.append(targets)\n        else:\n            # always channels first foe targets\n            targets_list.append(gaussian_blur(targets, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], sigma, params[\'data_format\'], \'blur_{}\'.format(sigma)))\n\n    # print(key_v)\n    #targets = tf.reshape(255.*tf.one_hot(tf.ones_like(key_v,tf.int64)*(params[\'heatmap_size\']*params[\'heatmap_size\']//2+params[\'heatmap_size\']), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n    #norm_value = tf.ones_like(norm_value)\n    # score_map = tf.reshape(tf.one_hot(tf.ones_like(key_v,tf.int64)*(31*64+31), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n    # filter all invisible keypoint maybe better for this task\n    # all_visible = tf.logical_and(key_v>0, isvalid>0)\n    # targets_list = [tf.boolean_mask(targets_list[ind], all_visible) for ind in list(range(len(targets_list)))]\n    # pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets_list = [targets_list[ind] * all_visible for ind in list(range(len(targets_list)))]\n    pred_outputs = [pred_outputs[ind] * all_visible for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    base_learning_rate = params[\'learning_rate\']\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        base_learning_rate = 1. * base_learning_rate\n        for pred_ind in list(range(len(pred_outputs) - 1)):\n            mse_loss_list.append(0.5 * tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets_list[-1], pred_outputs[-1], weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets_list[-1], gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs[-1], gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(len(pred_outputs) - 1),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        for pred_ind in list(range(len(pred_outputs))):\n            mse_loss_list.append(tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets_list[pred_ind]/255., name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [base_learning_rate * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        optimizer = tf_replicate_model_fn.TowerOptimizer(optimizer)\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold_(params[\'checkpoint_path\'], params[\'model_dir\'], params[\'checkpoint_exclude_scopes\'], params[\'model_scope\'], params[\'checkpoint_model_scope\'], params[\'ignore_missing_vars\'])))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, epochs_per_eval, lr_decay_factors, decay_boundaries, checkpoint_path=None, checkpoint_exclude_scopes=\'\', checkpoint_model_scope=\'\', ignore_missing_vars=True):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // FLAGS.batch_size\n\n    _replicate_model_fn = tf_replicate_model_fn.replicate_model_fn(model_fn, loss_reduction=tf.losses.Reduction.MEAN)\n\n    fashionAI = tf.estimator.Estimator(\n        model_fn=_replicate_model_fn, model_dir=model_dir, config=run_config.replace(save_checkpoints_steps=2*steps_per_epoch),\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'checkpoint_exclude_scopes\': checkpoint_exclude_scopes,\n            \'model_scope\': model_scope,\n            \'checkpoint_model_scope\': checkpoint_model_scope,\n            \'ignore_missing_vars\': ignore_missing_vars,\n            \'net_depth\': FLAGS.net_depth,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'batch_size\': FLAGS.batch_size,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(lr_decay_factors),\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    for _ in range(train_epochs // epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, epochs_per_eval), hooks=[logging_hook], max_steps=(steps_per_epoch*train_epochs))\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, model_scope, 1))\n        tf.logging.info(eval_results)\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=None).replace(\n                                        save_checkpoints_steps=FLAGS.save_checkpoints_steps).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    num_gpus = validate_batch_size_for_multi_gpu(FLAGS.batch_size)\n\n    full_model_dir = FLAGS.model_dir if FLAGS.run_on_cloud else FLAGS.model_dir.format(FLAGS.backbone.strip())\n    detail_params = {\n        \'blouse\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'blouse\'),\n            \'train_epochs\': 25,\n            \'epochs_per_eval\': 5,\n            \'lr_decay_factors\': \'1, 0.5, 0.1\',\n            \'decay_boundaries\': \'15, 20\',\n            \'model_scope\': \'blouse\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path.format(FLAGS.net_depth)) if FLAGS.run_on_cloud else FLAGS.checkpoint_path.format(FLAGS.net_depth),\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'blouse/additional_layer, blouse/feature_pyramid, blouse/global_net\',\n            \'ignore_missing_vars\': True,\n        },\n        \'dress\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'dress\'),\n            \'train_epochs\': 25,\n            \'epochs_per_eval\': 5,\n            \'lr_decay_factors\': \'1, 0.5, 0.1\',\n            \'decay_boundaries\': \'15, 20\',\n            \'model_scope\': \'dress\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path.format(FLAGS.net_depth)) if FLAGS.run_on_cloud else FLAGS.checkpoint_path.format(FLAGS.net_depth),\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'dress/additional_layer, dress/feature_pyramid, dress/global_net\',\n            \'ignore_missing_vars\': True,\n        },\n        \'outwear\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'outwear\'),\n            \'train_epochs\': 25,\n            \'epochs_per_eval\': 5,\n            \'lr_decay_factors\': \'1, 0.5, 0.1\',\n            \'decay_boundaries\': \'15, 20\',\n            \'model_scope\': \'outwear\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path.format(FLAGS.net_depth)) if FLAGS.run_on_cloud else FLAGS.checkpoint_path.format(FLAGS.net_depth),\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'outwear/additional_layer, outwear/feature_pyramid, outwear/global_net\',\n            \'ignore_missing_vars\': True,\n        },\n        \'skirt\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'skirt\'),\n            \'train_epochs\': 25,\n            \'epochs_per_eval\': 5,\n            \'lr_decay_factors\': \'1, 0.5, 0.1\',\n            \'decay_boundaries\': \'15, 20\',\n            \'model_scope\': \'skirt\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path.format(FLAGS.net_depth)) if FLAGS.run_on_cloud else FLAGS.checkpoint_path.format(FLAGS.net_depth),\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'skirt/additional_layer, skirt/feature_pyramid, skirt/global_net\',\n            \'ignore_missing_vars\': True,\n        },\n        \'trousers\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'trousers\'),\n            \'train_epochs\': 25,\n            \'epochs_per_eval\': 5,\n            \'lr_decay_factors\': \'1, 0.5, 0.1\',\n            \'decay_boundaries\': \'15, 20\',\n            \'model_scope\': \'trousers\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path.format(FLAGS.net_depth)) if FLAGS.run_on_cloud else FLAGS.checkpoint_path.format(FLAGS.net_depth),\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'trousers/additional_layer, trousers/feature_pyramid, trousers/global_net\',\n            \'ignore_missing_vars\': True,\n        },\n    }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'epochs_per_eval\'], detail_params[m][\'lr_decay_factors\'], detail_params[m][\'decay_boundaries\'], detail_params[m][\'checkpoint_path\'], detail_params[m][\'checkpoint_exclude_scopes\'], detail_params[m][\'checkpoint_model_scope\'], detail_params[m][\'ignore_missing_vars\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
train_senet_cpn_onebyone.py,148,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nfrom net import seresnet_cpn as cpn\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_sext_cpn/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_secs\', 3600,\n    \'The frequency with which the model is saved, in seconds.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 96,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_string(\n    \'backbone\', \'seresnext50\',#or seresnext50 seresnet50\n    \'The backbone network to use for feature pyramid.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'train_epochs\', 50,\n    \'The number of epochs to use for training.\')\ntf.app.flags.DEFINE_integer(\n    \'epochs_per_eval\', 20,\n    \'The number of training epochs to run between evaluations.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 10,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_integer(\n    \'xt_batch_size\', 10,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 1e-5, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 1e-4, \'Initial learning rate.\')#1e-3\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.00001,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', None,\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_boolean(\n    \'seq_train\', False,\n    \'Wether we will train a sequence model.\')\ntf.app.flags.DEFINE_string(#\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=FLAGS.epochs_per_eval):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, (FLAGS.xt_batch_size if \'seresnext50\' in FLAGS.backbone else FLAGS.batch_size), FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\ncpn_backbone = cpn.cascaded_pyramid_net\nif \'seresnext50\' in FLAGS.backbone:\n    cpn_backbone = cpn.xt_cascaded_pyramid_net\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = cpn_backbone(features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = [tf.transpose(pred_outputs[ind], [0, 3, 1, 2], name=\'outputs_trans_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n\n    score_map = pred_outputs[-1]\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n    blur_list = [1., 1.37, 1.73, 2.4, None]#[1., 1.5, 2., 3., None]\n    #blur_list = [None, None, None, None, None]\n\n    targets_list = []\n    for sigma in blur_list:\n        if sigma is None:\n            targets_list.append(targets)\n        else:\n            # always channels first foe targets\n            targets_list.append(gaussian_blur(targets, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], sigma, params[\'data_format\'], \'blur_{}\'.format(sigma)))\n\n    # print(key_v)\n    #targets = tf.reshape(255.*tf.one_hot(tf.ones_like(key_v,tf.int64)*(params[\'heatmap_size\']*params[\'heatmap_size\']//2+params[\'heatmap_size\']), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n    #norm_value = tf.ones_like(norm_value)\n    # score_map = tf.reshape(tf.one_hot(tf.ones_like(key_v,tf.int64)*(31*64+31), params[\'heatmap_size\']*params[\'heatmap_size\']), [cur_batch_size,-1,params[\'heatmap_size\'],params[\'heatmap_size\']])\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    # last_pred_mse = tf.metrics.mean_squared_error(score_map, targets,\n    #                             weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n    #                             name=\'last_pred_mse\')\n    # filter all invisible keypoint maybe better for this task\n    # all_visible = tf.logical_and(key_v>0, isvalid>0)\n    # targets_list = [tf.boolean_mask(targets_list[ind], all_visible) for ind in list(range(len(targets_list)))]\n    # pred_outputs = [tf.boolean_mask(pred_outputs[ind], all_visible, name=\'boolean_mask_{}\'.format(ind)) for ind in list(range(len(pred_outputs)))]\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets_list = [targets_list[ind] * all_visible for ind in list(range(len(targets_list)))]\n    pred_outputs = [pred_outputs[ind] * all_visible for ind in list(range(len(pred_outputs)))]\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs[-1]), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    base_learning_rate = params[\'learning_rate\']\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        base_learning_rate = 1. * base_learning_rate\n        for pred_ind in list(range(len(pred_outputs) - 1)):\n            mse_loss_list.append(0.5 * tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets_list[-1], pred_outputs[-1], weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets_list[-1], gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs[-1], gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(len(pred_outputs) - 1),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        for pred_ind in list(range(len(pred_outputs))):\n            mse_loss_list.append(tf.losses.mean_squared_error(targets_list[pred_ind], pred_outputs[pred_ind],\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss_{}\'.format(pred_ind),\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets_list[pred_ind]/255., name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [base_learning_rate * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold_(params[\'checkpoint_path\'], params[\'model_dir\'], params[\'checkpoint_exclude_scopes\'], params[\'model_scope\'], params[\'checkpoint_model_scope\'], params[\'ignore_missing_vars\'])))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, epochs_per_eval, lr_decay_factors, decay_boundaries, checkpoint_path=None, checkpoint_exclude_scopes=\'\', checkpoint_model_scope=\'\', ignore_missing_vars=True):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // (FLAGS.xt_batch_size if \'seresnext50\' in FLAGS.backbone else FLAGS.batch_size)\n    fashionAI = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, config=run_config,\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'checkpoint_exclude_scopes\': checkpoint_exclude_scopes,\n            \'model_scope\': model_scope,\n            \'checkpoint_model_scope\': checkpoint_model_scope,\n            \'ignore_missing_vars\': ignore_missing_vars,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'batch_size\': (FLAGS.xt_batch_size if \'seresnext50\' in FLAGS.backbone else FLAGS.batch_size),\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(lr_decay_factors),\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    for _ in range(train_epochs // epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, epochs_per_eval), hooks=[logging_hook], max_steps=(steps_per_epoch*train_epochs))\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, model_scope, 1))\n        tf.logging.info(eval_results)\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=FLAGS.save_checkpoints_secs).replace(\n                                        save_checkpoints_steps=None).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    if FLAGS.seq_train:\n        detail_params = {\n            \'all\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'all\'),\n                \'train_epochs\': 6,\n                \'epochs_per_eval\': 4,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'3, 4\',\n                \'model_scope\': \'all\',\n                \'checkpoint_path\': None,\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'\',\n                \'ignore_missing_vars\': True,\n            },\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'blouse/feature_pyramid/conv_heatmap, blouse/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'dress/feature_pyramid/conv_heatmap, dress/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'outwear/feature_pyramid/conv_heatmap, outwear/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'skirt/feature_pyramid/conv_heatmap, skirt/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 50,\n                \'epochs_per_eval\': 30,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'15, 30\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.model_dir, \'all\'),\n                \'checkpoint_model_scope\': \'all\',\n                \'checkpoint_exclude_scopes\': \'trousers/feature_pyramid/conv_heatmap, trousers/global_net/conv_heatmap\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    else:\n        detail_params = {\n            \'blouse\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'blouse\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'blouse\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.backbone) if FLAGS.run_on_cloud else os.path.join(FLAGS.checkpoint_path, FLAGS.backbone),\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'blouse/feature_pyramid, blouse/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'dress\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'dress\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'dress\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.backbone) if FLAGS.run_on_cloud else os.path.join(FLAGS.checkpoint_path, FLAGS.backbone),\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'dress/feature_pyramid, dress/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'outwear\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'outwear\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'outwear\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.backbone) if FLAGS.run_on_cloud else os.path.join(FLAGS.checkpoint_path, FLAGS.backbone),\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'outwear/feature_pyramid, outwear/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'skirt\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'skirt\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'skirt\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.backbone) if FLAGS.run_on_cloud else os.path.join(FLAGS.checkpoint_path, FLAGS.backbone),\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'skirt/feature_pyramid, skirt/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n            \'trousers\': {\n                \'model_dir\' : os.path.join(FLAGS.model_dir, \'trousers\'),\n                \'train_epochs\': 28,\n                \'epochs_per_eval\': 7,\n                \'lr_decay_factors\': \'1, 0.5, 0.1\',\n                \'decay_boundaries\': \'10, 20\',\n                \'model_scope\': \'trousers\',\n                \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.backbone) if FLAGS.run_on_cloud else os.path.join(FLAGS.checkpoint_path, FLAGS.backbone),\n                \'checkpoint_model_scope\': \'\',\n                \'checkpoint_exclude_scopes\': \'trousers/feature_pyramid, trousers/global_net\',\n                \'ignore_missing_vars\': True,\n            },\n        }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'epochs_per_eval\'], detail_params[m][\'lr_decay_factors\'], detail_params[m][\'decay_boundaries\'], detail_params[m][\'checkpoint_path\'], detail_params[m][\'checkpoint_exclude_scopes\'], detail_params[m][\'checkpoint_model_scope\'], detail_params[m][\'ignore_missing_vars\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n\n# 0.04473711425469029\n# blouse: 0.042138283111307795\n# dress: 0.04147867224643174\n# outwear: 0.04511445541161763\n# skirt: 0.05388678376709799\n# trousers: 0.04985801318493035\n\n'"
train_simplenet_onebyone.py,134,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\n#from scipy.misc import imread, imsave, imshow, imresize\nimport tensorflow as tf\n\nimport tf_replicate_model_fn\n\nfrom net import simple_xt\n\nfrom utility import train_helper\nfrom utility import mertric\n\nfrom preprocessing import preprocessing\nfrom preprocessing import dataset\nimport config\n\n# hardware related configuration\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 16,#16\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 48,#48\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_integer(\n    \'num_cpu_threads\', 0,\n    \'The number of cpu cores used to train.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 1., \'GPU memory fraction to use.\')\n# scaffold related configuration\ntf.app.flags.DEFINE_string(\n    \'data_dir\', \'../Datasets/tfrecords\',#\'/media/rs/0E06CD1706CD0127/Kapok/Chi/Datasets/tfrecords\',\n    \'The directory where the dataset input data is stored.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'{}_????\', \'The pattern of the dataset name to load.\')\ntf.app.flags.DEFINE_string(\n    \'model_dir\', \'./logs_simple_net/\',\n    \'The parent directory where the model will be stored.\')\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summary_steps\', 100,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_checkpoints_steps\', 8000,\n    \'The frequency with which the model is saved, in steps.\')\n# model related configuration\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', 384,\n    \'The size of the input image for the model to use.\')\ntf.app.flags.DEFINE_integer(\n    \'heatmap_size\', 192,\n    \'The size of the output heatmap of the model.\')\ntf.app.flags.DEFINE_float(\n    \'heatmap_sigma\', 1.,\n    \'The sigma of Gaussian which generate the target heatmap.\')\ntf.app.flags.DEFINE_float(\n    \'bbox_border\', 25.,\n    \'The nearest distance of the crop border to al keypoints.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 8,\n    \'Batch size for training and evaluation.\')\ntf.app.flags.DEFINE_boolean(\n    \'use_ohkm\', True,\n    \'Wether we will use the ohkm for hard keypoints.\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'channels_first\', # \'channels_first\' or \'channels_last\'\n    \'A flag to override the data format used in the model. channels_first \'\n    \'provides a performance boost on GPU but is not always compatible \'\n    \'with CPU. If left unspecified, the data format will be chosen \'\n    \'automatically based on whether TensorFlow was built for CPU or GPU.\')\n# optimizer related configuration\ntf.app.flags.DEFINE_integer(\n    \'tf_random_seed\', 20180417, \'Random seed for TensorFlow initializers.\')\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 1e-5, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'mse_weight\', 1., \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 8e-3, \'Initial learning rate.\')#1e-3\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0000001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'warmup_learning_rate\', 0.0002,\n    \'The start warm-up learning rate to avoid NAN.\')\ntf.app.flags.DEFINE_integer(\n    \'warmup_steps\', 100,\n    \'The total steps to warm-up.\')\n# for learning rate piecewise_constant decay\ntf.app.flags.DEFINE_string(\n    \'decay_boundaries\', \'2, 3\',\n    \'Learning rate decay boundaries by global_step (comma-separated list).\')\ntf.app.flags.DEFINE_string(\n    \'lr_decay_factors\', \'1, 0.5, 0.1\',\n    \'The values of learning_rate decay factor for each segment between boundaries (comma-separated list).\')\n# checkpoint related configuration\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'./model/seresnext101\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', \'\',\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    #\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\', \'all\'\n    \'model_scope\', None,\n    \'Model scope name used to replace the name_scope in checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', True,\n    \'When restoring a checkpoint would ignore missing variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'run_on_cloud\', True,\n    \'Wether we will train on cloud.\')\ntf.app.flags.DEFINE_boolean(\n    \'multi_gpu\', True,\n    \'Wether we will use multi-GPUs to train.\')\ntf.app.flags.DEFINE_string(\n    \'cloud_checkpoint_path\', \'seresnext101\',\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'model_to_train\', \'blouse, dress, outwear, skirt, trousers\', #\'all, blouse, dress, outwear, skirt, trousers\', \'skirt, dress, outwear, trousers\',\n    \'The sub-model to train (comma-separated list).\')\n\nFLAGS = tf.app.flags.FLAGS\n#--model_scope=blouse --checkpoint_path=./logs/all --data_format=channels_last --batch_size=1\n\ndef validate_batch_size_for_multi_gpu(batch_size):\n    """"""For multi-gpu, batch-size must be a multiple of the number of\n    available GPUs.\n\n    Note that this should eventually be handled by replicate_model_fn\n    directly. Multi-GPU support is currently experimental, however,\n    so doing the work here until that feature is in place.\n    """"""\n    if not FLAGS.multi_gpu:\n        return 0\n\n    from tensorflow.python.client import device_lib\n\n    local_device_protos = device_lib.list_local_devices()\n    num_gpus = sum([1 for d in local_device_protos if d.device_type == \'GPU\'])\n    if not num_gpus:\n        raise ValueError(\'Multi-GPU mode was specified, but no GPUs \'\n                        \'were found. To use CPU, run without --multi_gpu=False.\')\n\n    remainder = batch_size % num_gpus\n    if remainder:\n        err = (\'When running with multiple GPUs, batch size \'\n                \'must be a multiple of the number of available GPUs. \'\n                \'Found {} GPUs with a batch size of {}; try --batch_size={} instead.\'\n                ).format(num_gpus, batch_size, batch_size - remainder)\n        raise ValueError(err)\n    return num_gpus\n\ndef input_pipeline(is_training=True, model_scope=FLAGS.model_scope, num_epochs=None):\n    if \'all\' in model_scope:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, FLAGS.train_image_size, FLAGS.train_image_size, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=is_training, data_format=(\'NCHW\' if FLAGS.data_format==\'channels_first\' else \'NHWC\'), category=(model_scope if \'all\' not in model_scope else \'*\'), bbox_border=FLAGS.bbox_border, heatmap_sigma=FLAGS.heatmap_sigma, heatmap_size=FLAGS.heatmap_size)\n\n    images, shape, classid, targets, key_v, isvalid, norm_value = dataset.slim_get_split(FLAGS.data_dir, preprocessing_fn, FLAGS.batch_size, FLAGS.num_readers, FLAGS.num_preprocessing_threads, num_epochs=num_epochs, is_training=is_training, file_pattern=FLAGS.dataset_name, category=(model_scope if \'all\' not in model_scope else \'*\'), reader=None)\n\n    return images, {\'targets\': targets, \'key_v\': key_v, \'shape\': shape, \'classid\': classid, \'isvalid\': isvalid, \'norm_value\': norm_value}\n\nif config.PRED_DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, height, width, heatmap_size, targets, pred_heatmap, indR, indG, indB):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist()) + 128\n      #print(img_to_save.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n\n      heatmap0 = np.sum(targets[indR, ...], axis=0).astype(np.uint8)\n      heatmap1 = np.sum(targets[indG, ...], axis=0).astype(np.uint8)\n      heatmap2 = np.sum(targets[indB, ...], axis=0).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      img_to_save = imresize(img_to_save, (height, width), interp=\'lanczos\')\n      heatmap0 = imresize(heatmap0, (height, width), interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, (height, width), interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, (height, width), interp=\'lanczos\')\n\n      img_to_save = img_to_save/2\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'targets_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n\n      pred_heatmap = np.array(pred_heatmap.tolist())\n      #print(pred_heatmap.shape)\n      for ind in range(pred_heatmap.shape[0]):\n        img = pred_heatmap[ind]\n        img = img - img.min()\n        img *= 255.0/img.max()\n        file_name = \'heatmap_{}_{}.jpg\'.format(save_image_with_heatmap.counter, ind)\n        imsave(os.path.join(config.DEBUG_DIR, file_name), img.astype(np.uint8))\n      return save_image_with_heatmap.counter\n\ndef get_keypoint(image, targets, predictions, heatmap_size, height, width, category, clip_at_zero=True, data_format=\'channels_last\', name=None):\n    predictions = tf.reshape(predictions, [1, -1, heatmap_size*heatmap_size])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.cast(tf.floormod(pred_indices, heatmap_size), tf.float32), tf.cast(tf.floordiv(pred_indices, heatmap_size), tf.float32)\n\n    width, height = tf.cast(width, tf.float32), tf.cast(height, tf.float32)\n    pred_x, pred_y = pred_x * width / tf.cast(heatmap_size, tf.float32), pred_y * height / tf.cast(heatmap_size, tf.float32)\n\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n      pred_x = pred_x * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (width / 2.)\n      pred_y = pred_y * tf.cast(pred_max>0, tf.float32) + tf.cast(pred_max<=0, tf.float32) * (height / 2.)\n\n    if config.PRED_DEBUG:\n      pred_indices_ = tf.squeeze(pred_indices)\n      image_ = tf.squeeze(image) * 255.\n      pred_heatmap = tf.one_hot(pred_indices_, heatmap_size*heatmap_size, on_value=1., off_value=0., axis=-1, dtype=tf.float32)\n\n      pred_heatmap = tf.reshape(pred_heatmap, [-1, heatmap_size, heatmap_size])\n      if data_format == \'channels_first\':\n        image_ = tf.transpose(image_, perm=(1, 2, 0))\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image_, height, width,\n                                  heatmap_size,\n                                  tf.reshape(pred_heatmap * 255., [-1, heatmap_size, heatmap_size]),\n                                  tf.reshape(predictions, [-1, heatmap_size, heatmap_size]),\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2]],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        pred_x, pred_y = pred_x * 1., pred_y * 1.\n    return pred_x, pred_y\n\ndef gaussian_blur(inputs, inputs_filters, sigma, data_format, name=None):\n    with tf.name_scope(name, ""gaussian_blur"", [inputs]):\n        data_format_ = \'NHWC\' if data_format==\'channels_last\' else \'NCHW\'\n        if data_format_ == \'NHWC\':\n            inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        ksize = int(6 * sigma + 1.)\n        x = tf.expand_dims(tf.range(ksize, delta=1, dtype=tf.float32), axis=1)\n        y = tf.transpose(x, [1, 0])\n        kernel_matrix = tf.exp(- ((x - ksize/2.) ** 2 + (y - ksize/2.) ** 2) / (2 * sigma ** 2))\n        #print(kernel_matrix)\n        kernel_filter = tf.reshape(kernel_matrix, [ksize, ksize, 1, 1])\n        kernel_filter = tf.tile(kernel_filter, [1, 1, inputs_filters, 1])\n        #kernel_filter = tf.transpose(kernel_filter, [1, 0, 2, 3])\n        outputs = tf.nn.depthwise_conv2d(inputs, kernel_filter, strides=[1, 1, 1, 1], padding=\'SAME\', data_format=data_format_, name=\'blur\')\n        if data_format_ == \'NHWC\':\n            outputs = tf.transpose(outputs, [0, 3, 1, 2])\n        return outputs\n\ndef keypoint_model_fn(features, labels, mode, params):\n    targets = labels[\'targets\']\n    shape = labels[\'shape\']\n    classid = labels[\'classid\']\n    key_v = labels[\'key_v\']\n    isvalid = labels[\'isvalid\']\n    norm_value = labels[\'norm_value\']\n\n    cur_batch_size = tf.shape(features)[0]\n    #features= tf.ones_like(features)\n\n    with tf.variable_scope(params[\'model_scope\'], default_name=None, values=[features], reuse=tf.AUTO_REUSE):\n        pred_outputs = simple_xt.simple_net(features, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], params[\'heatmap_size\'], (mode == tf.estimator.ModeKeys.TRAIN), params[\'data_format\'])[0]\n\n    if params[\'data_format\'] == \'channels_last\':\n        pred_outputs = tf.transpose(pred_outputs, [0, 3, 1, 2], name=\'outputs_trans\')\n\n    score_map = pred_outputs\n\n    pred_x, pred_y = get_keypoint(features, targets, score_map, params[\'heatmap_size\'], params[\'train_image_size\'], params[\'train_image_size\'], (params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\'), clip_at_zero=True, data_format=params[\'data_format\'])\n\n    # this is important!!!\n    targets = 255. * targets\n\n    #with tf.control_dependencies([pred_x, pred_y]):\n    ne_mertric = mertric.normalized_error(targets, score_map, norm_value, key_v, isvalid,\n                             cur_batch_size,\n                             config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')],\n                             params[\'heatmap_size\'],\n                             params[\'train_image_size\'])\n\n    all_visible = tf.expand_dims(tf.expand_dims(tf.cast(tf.logical_and(key_v>0, isvalid>0), tf.float32), axis=-1), axis=-1)\n    targets = targets * all_visible\n    pred_outputs = pred_outputs * all_visible\n\n    sq_diff = tf.reduce_sum(tf.squared_difference(targets, pred_outputs), axis=-1)\n    last_pred_mse = tf.metrics.mean_absolute_error(sq_diff, tf.zeros_like(sq_diff), name=\'last_pred_mse\')\n\n    metrics = {\'normalized_error\': ne_mertric, \'last_pred_mse\':last_pred_mse}\n    predictions = {\'normalized_error\': ne_mertric[1]}\n    ne_mertric = tf.identity(ne_mertric[1], name=\'ne_mertric\')\n\n    base_learning_rate = params[\'learning_rate\']\n    mse_loss_list = []\n    if params[\'use_ohkm\']:\n        base_learning_rate = 1. * base_learning_rate\n        temp_loss = tf.reduce_mean(tf.reshape(tf.losses.mean_squared_error(targets, pred_outputs, weights=1.0, loss_collection=None, reduction=tf.losses.Reduction.NONE), [cur_batch_size, config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')], -1]), axis=-1)\n\n        num_topk = config.class_num_joints[(params[\'model_scope\'] if \'all\' not in params[\'model_scope\'] else \'*\')] // 2\n        gather_col = tf.nn.top_k(temp_loss, k=num_topk, sorted=True)[1]\n        gather_row = tf.reshape(tf.tile(tf.reshape(tf.range(cur_batch_size), [-1, 1]), [1, num_topk]), [-1, 1])\n        gather_indcies = tf.stop_gradient(tf.stack([gather_row, tf.reshape(gather_col, [-1, 1])], axis=-1))\n\n        select_targets = tf.gather_nd(targets, gather_indcies)\n        select_heatmap = tf.gather_nd(pred_outputs, gather_indcies)\n\n        mse_loss_list.append(tf.losses.mean_squared_error(select_targets, select_heatmap,\n                                weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                                scope=\'loss\',\n                                loss_collection=None,#tf.GraphKeys.LOSSES,\n                                # mean all elements of all pixels in all batch\n                                reduction=tf.losses.Reduction.MEAN))\n    else:\n        mse_loss_list.append(tf.losses.mean_squared_error(targets, pred_outputs,\n                            weights=1.0 / tf.cast(cur_batch_size, tf.float32),\n                            scope=\'loss\',\n                            loss_collection=None,#tf.GraphKeys.LOSSES,\n                            # mean all elements of all pixels in all batch\n                            reduction=tf.losses.Reduction.MEAN))# SUM, SUM_OVER_BATCH_SIZE, default mean by all elements\n\n    mse_loss = tf.multiply(params[\'mse_weight\'], tf.add_n(mse_loss_list), name=\'mse_loss\')\n    tf.summary.scalar(\'mse\', mse_loss)\n    tf.losses.add_loss(mse_loss)\n\n    # bce_loss_list = []\n    # for pred_ind in list(range(len(pred_outputs))):\n    #     bce_loss_list.append(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_outputs[pred_ind], labels=targets_list[pred_ind]/255., name=\'loss_{}\'.format(pred_ind)), name=\'loss_mean_{}\'.format(pred_ind)))\n\n    # mse_loss = tf.multiply(params[\'mse_weight\'] / params[\'num_stacks\'], tf.add_n(bce_loss_list), name=\'mse_loss\')\n    # tf.summary.scalar(\'mse\', mse_loss)\n    # tf.losses.add_loss(mse_loss)\n\n    # Add weight decay to the loss. We exclude the batch norm variables because\n    # doing so leads to a small improvement in accuracy.\n    loss = mse_loss + params[\'weight_decay\'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'batch_normalization\' not in v.name])\n    total_loss = tf.identity(loss, name=\'total_loss\')\n    tf.summary.scalar(\'loss\', total_loss)\n\n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, predictions=predictions, eval_metric_ops=metrics)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        global_step = tf.train.get_or_create_global_step()\n\n        lr_values = [params[\'warmup_learning_rate\']] + [base_learning_rate * decay for decay in params[\'lr_decay_factors\']]\n        learning_rate = tf.train.piecewise_constant(tf.cast(global_step, tf.int32),\n                                                    [params[\'warmup_steps\']] + [int(float(ep)*params[\'steps_per_epoch\']) for ep in params[\'decay_boundaries\']],\n                                                    lr_values)\n        truncated_learning_rate = tf.maximum(learning_rate, tf.constant(params[\'end_learning_rate\'], dtype=learning_rate.dtype), name=\'learning_rate\')\n        tf.summary.scalar(\'lr\', truncated_learning_rate)\n\n        optimizer = tf.train.MomentumOptimizer(learning_rate=truncated_learning_rate,\n                                                momentum=params[\'momentum\'])\n\n        optimizer = tf_replicate_model_fn.TowerOptimizer(optimizer)\n\n        # Batch norm requires update_ops to be added as a train_op dependency.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = optimizer.minimize(loss, global_step)\n    else:\n        train_op = None\n\n    return tf.estimator.EstimatorSpec(\n                          mode=mode,\n                          predictions=predictions,\n                          loss=loss,\n                          train_op=train_op,\n                          eval_metric_ops=metrics,\n                          scaffold=tf.train.Scaffold(init_fn=train_helper.get_init_fn_for_scaffold_(params[\'checkpoint_path\'], params[\'model_dir\'], params[\'checkpoint_exclude_scopes\'], params[\'model_scope\'], params[\'checkpoint_model_scope\'], params[\'ignore_missing_vars\'])))\n\ndef parse_comma_list(args):\n    return [float(s.strip()) for s in args.split(\',\')]\n\ndef sub_loop(model_fn, model_scope, model_dir, run_config, train_epochs, epochs_per_eval, lr_decay_factors, decay_boundaries, checkpoint_path=None, checkpoint_exclude_scopes=\'\', checkpoint_model_scope=\'\', ignore_missing_vars=True):\n    steps_per_epoch = config.split_size[(model_scope if \'all\' not in model_scope else \'*\')][\'train\'] // FLAGS.batch_size\n\n    _replicate_model_fn = tf_replicate_model_fn.replicate_model_fn(model_fn, loss_reduction=tf.losses.Reduction.MEAN)\n\n    fashionAI = tf.estimator.Estimator(\n        model_fn=_replicate_model_fn, model_dir=model_dir, config=run_config.replace(save_checkpoints_steps=2*steps_per_epoch),\n        params={\n            \'checkpoint_path\': checkpoint_path,\n            \'model_dir\': model_dir,\n            \'checkpoint_exclude_scopes\': checkpoint_exclude_scopes,\n            \'model_scope\': model_scope,\n            \'checkpoint_model_scope\': checkpoint_model_scope,\n            \'ignore_missing_vars\': ignore_missing_vars,\n            \'train_image_size\': FLAGS.train_image_size,\n            \'heatmap_size\': FLAGS.heatmap_size,\n            \'data_format\': FLAGS.data_format,\n            \'steps_per_epoch\': steps_per_epoch,\n            \'use_ohkm\': FLAGS.use_ohkm,\n            \'batch_size\': FLAGS.batch_size,\n            \'weight_decay\': FLAGS.weight_decay,\n            \'mse_weight\': FLAGS.mse_weight,\n            \'momentum\': FLAGS.momentum,\n            \'learning_rate\': FLAGS.learning_rate,\n            \'end_learning_rate\': FLAGS.end_learning_rate,\n            \'warmup_learning_rate\': FLAGS.warmup_learning_rate,\n            \'warmup_steps\': FLAGS.warmup_steps,\n            \'decay_boundaries\': parse_comma_list(decay_boundaries),\n            \'lr_decay_factors\': parse_comma_list(lr_decay_factors),\n        })\n\n    tf.gfile.MakeDirs(model_dir)\n    tf.logging.info(\'Starting to train model {}.\'.format(model_scope))\n    for _ in range(train_epochs // epochs_per_eval):\n        tensors_to_log = {\n            \'lr\': \'learning_rate\',\n            \'loss\': \'total_loss\',\n            \'mse\': \'mse_loss\',\n            \'ne\': \'ne_mertric\',\n        }\n\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=FLAGS.log_every_n_steps, formatter=lambda dicts: \'{}:\'.format(model_scope) + (\', \'.join([\'%s=%.6f\' % (k, v) for k, v in dicts.items()])))\n\n        tf.logging.info(\'Starting a training cycle.\')\n        fashionAI.train(input_fn=lambda : input_pipeline(True, model_scope, epochs_per_eval), hooks=[logging_hook], max_steps=(steps_per_epoch*train_epochs))\n\n        tf.logging.info(\'Starting to evaluate.\')\n        eval_results = fashionAI.evaluate(input_fn=lambda : input_pipeline(False, model_scope, 1))\n        tf.logging.info(eval_results)\n    tf.logging.info(\'Finished model {}.\'.format(model_scope))\n\ndef main(_):\n    # Using the Winograd non-fused algorithms provides a small performance boost.\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = FLAGS.gpu_memory_fraction)\n    sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False, intra_op_parallelism_threads = FLAGS.num_cpu_threads, inter_op_parallelism_threads = FLAGS.num_cpu_threads, gpu_options = gpu_options)\n\n    # Set up a RunConfig to only save checkpoints once per training cycle.\n    run_config = tf.estimator.RunConfig().replace(\n                                        save_checkpoints_secs=None).replace(\n                                        save_checkpoints_steps=FLAGS.save_checkpoints_steps).replace(\n                                        save_summary_steps=FLAGS.save_summary_steps).replace(\n                                        keep_checkpoint_max=5).replace(\n                                        tf_random_seed=FLAGS.tf_random_seed).replace(\n                                        log_step_count_steps=FLAGS.log_every_n_steps).replace(\n                                        session_config=sess_config)\n\n    num_gpus = validate_batch_size_for_multi_gpu(FLAGS.batch_size)\n\n    full_model_dir = FLAGS.model_dir if FLAGS.run_on_cloud else FLAGS.model_dir\n    detail_params = {\n        \'blouse\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'blouse\'),\n            \'train_epochs\': 30,\n            \'epochs_per_eval\': 30,\n            \'lr_decay_factors\': \'1, 0.5, 0.1, 0.01\',\n            \'decay_boundaries\': \'15, 20, 28\',\n            \'model_scope\': \'blouse\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'blouse/additional_layer\',\n            \'ignore_missing_vars\': True,\n        },\n        \'dress\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'dress\'),\n            \'train_epochs\': 30,\n            \'epochs_per_eval\': 30,\n            \'lr_decay_factors\': \'1, 0.5, 0.1, 0.01\',\n            \'decay_boundaries\': \'15, 20, 28\',\n            \'model_scope\': \'dress\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'dress/additional_layer\',\n            \'ignore_missing_vars\': True,\n        },\n        \'outwear\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'outwear\'),\n            \'train_epochs\': 30,\n            \'epochs_per_eval\': 30,\n            \'lr_decay_factors\': \'1, 0.5, 0.1, 0.01\',\n            \'decay_boundaries\': \'15, 20, 28\',\n            \'model_scope\': \'outwear\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'outwear/additional_layer\',\n            \'ignore_missing_vars\': True,\n        },\n        \'skirt\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'skirt\'),\n            \'train_epochs\': 30,\n            \'epochs_per_eval\': 30,\n            \'lr_decay_factors\': \'1, 0.5, 0.1, 0.01\',\n            \'decay_boundaries\': \'15, 20, 28\',\n            \'model_scope\': \'skirt\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'skirt/additional_layer\',\n            \'ignore_missing_vars\': True,\n        },\n        \'trousers\': {\n            \'model_dir\' : os.path.join(full_model_dir, \'trousers\'),\n            \'train_epochs\': 30,\n            \'epochs_per_eval\': 30,\n            \'lr_decay_factors\': \'1, 0.5, 0.1, 0.01\',\n            \'decay_boundaries\': \'15, 20, 28\',\n            \'model_scope\': \'trousers\',\n            \'checkpoint_path\': os.path.join(FLAGS.data_dir, FLAGS.cloud_checkpoint_path) if FLAGS.run_on_cloud else FLAGS.checkpoint_path,\n            \'checkpoint_model_scope\': \'\',\n            \'checkpoint_exclude_scopes\': \'trousers/additional_layer\',\n            \'ignore_missing_vars\': True,\n        },\n    }\n    model_to_train = [s.strip() for s in FLAGS.model_to_train.split(\',\')]\n\n    for m in model_to_train:\n        sub_loop(keypoint_model_fn, m, detail_params[m][\'model_dir\'], run_config, detail_params[m][\'train_epochs\'], detail_params[m][\'epochs_per_eval\'], detail_params[m][\'lr_decay_factors\'], detail_params[m][\'decay_boundaries\'], detail_params[m][\'checkpoint_path\'], detail_params[m][\'checkpoint_exclude_scopes\'], detail_params[m][\'checkpoint_model_scope\'], detail_params[m][\'ignore_missing_vars\'])\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
net/cpn.py,40,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\n_USE_FUSED_BN = True\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format, name=None):\n    """"""Performs a batch normalization using a standard set of parameters.""""""\n    # We set fused=True for a significant performance boost. See\n    # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n        scale=True, training=training, name=name, fused=_USE_FUSED_BN)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    """"""Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                   Should be a positive integer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      A tensor with the same format as the input with the data either intact\n      (if kernel_size == 1) or padded (if kernel_size > 1).\n    """"""\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == \'channels_first\':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                      [pad_beg, pad_end], [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                      [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=tf.glorot_uniform_initializer, name=None):\n    """"""Strided 2-D convolution with explicit padding.""""""\n    # The padding is consistent and is based only on `kernel_size`, not on the\n    # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n                inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n                padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n                kernel_initializer=kernel_initializer(),\n                data_format=data_format, name=name)\n\n\n################################################################################\n# ResNet block definitions.\n################################################################################\ndef _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n                         strides, data_format):\n    """"""A single block for ResNet v1, with a bottleneck.\n\n    Similar to _building_block_v1(), except using the ""bottleneck"" blocks\n    described in:\n      Convolution then batch normalization then ReLU as described by:\n        Deep Residual Learning for Image Recognition\n        https://arxiv.org/pdf/1512.03385.pdf\n        by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      filters: The number of filters for the convolutions.\n      training: A Boolean for whether the model is in training or inference\n        mode. Needed for batch normalization.\n      projection_shortcut: The function to use for projection shortcuts\n        (typically a 1x1 convolution when downsampling the input).\n      strides: The block\'s stride. If greater than 1, this block will ultimately\n        downsample the input.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      The output tensor of the block; shape should match inputs.\n    """"""\n    shortcut = inputs\n\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n        shortcut = batch_norm(inputs=shortcut, training=training,\n                              data_format=data_format)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs += shortcut\n    inputs = tf.nn.relu(inputs)\n\n    return inputs\n\ndef block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n                training, name, data_format):\n    """"""Creates one layer of blocks for the ResNet model.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      filters: The number of filters for the first convolution of the layer.\n      bottleneck: Is the block created a bottleneck block.\n      block_fn: The block to use within the model, either `building_block` or\n        `bottleneck_block`.\n      blocks: The number of blocks contained in the layer.\n      strides: The stride to use for the first convolution of the layer. If\n        greater than 1, this layer will ultimately downsample the input.\n      training: Either True or False, whether we are currently training the\n        model. Needed for batch norm.\n      name: A string name for the tensor output of the block layer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      The output tensor of the block layer.\n    """"""\n\n    # Bottleneck blocks end with 4x the number of filters as they start with\n    filters_out = filters * 4 if bottleneck else filters\n\n    def projection_shortcut(inputs):\n        return conv2d_fixed_padding(\n            inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n            data_format=data_format)\n\n    # Only the first block per block_layer uses projection_shortcut and strides\n    inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n                      data_format)\n\n    for _ in range(1, blocks):\n        inputs = block_fn(inputs, filters, training, None, 1, data_format)\n\n    return tf.identity(inputs, name)\n\ndef cpn_backbone(inputs, istraining, data_format):\n    block_strides = [1, 2, 2, 2]\n    inputs = conv2d_fixed_padding(inputs=inputs, filters=64, kernel_size=7, strides=2, data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer)\n    inputs = tf.identity(inputs, \'initial_conv\')\n\n    inputs = tf.layers.max_pooling2d(inputs=inputs, pool_size=3, strides=2, padding=\'SAME\', data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_max_pool\')\n\n    end_points = []\n    for i, num_blocks in enumerate([3, 4, 6, 3]):\n      num_filters = 64 * (2**i)\n      #with tf.variable_scope(\'block_{}\'.format(i), \'resnet50\', values=[inputs]):\n      inputs = block_layer(\n          inputs=inputs, filters=num_filters, bottleneck=True,\n          block_fn=_bottleneck_block_v1, blocks=num_blocks,\n          strides=block_strides[i], training=istraining,\n          name=\'block_layer{}\'.format(i + 1), data_format=data_format)\n      end_points.append(inputs)\n\n    return end_points\n\ndef global_net_bottleneck_block(inputs, filters, istraining, data_format, projection_shortcut=None, name=None):\n    with tf.variable_scope(name, \'global_net_bottleneck\', values=[inputs]):\n        shortcut = inputs\n        if projection_shortcut is not None:\n            shortcut = projection_shortcut(inputs)\n            shortcut = batch_norm(inputs=shortcut, training=istraining,\n                                  data_format=data_format, name=\'batch_normalization_shortcut\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_down\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_1\')\n        inputs = tf.nn.relu(inputs, name=\'relu1\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=3, strides=1,\n            data_format=data_format, name=\'3x3_conv\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_2\')\n        inputs = tf.nn.relu(inputs, name=\'relu2\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=2 * filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_up\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_3\')\n        inputs += shortcut\n        inputs = tf.nn.relu(inputs, name=\'relu3\')\n\n        return inputs\n\ndef cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format):\n    #with tf.variable_scope(\'resnet50\', \'resnet50\', values=[inputs]):\n    end_points = cpn_backbone(inputs, istraining, data_format)\n    pyramid_len = len(end_points)\n    up_sampling = None\n    pyramid_heatmaps = []\n    pyramid_laterals = []\n    with tf.variable_scope(\'feature_pyramid\', \'feature_pyramid\', values=end_points):\n        # top-down\n        for ind, pyramid in enumerate(reversed(end_points)):\n            inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv1_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(inputs, name=\'relu1_p{}\'.format(pyramid_len - ind))\n            if up_sampling is not None:\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name=\'trans_p{}\'.format(pyramid_len - ind))\n                up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name=\'upsample_p{}\'.format(pyramid_len - ind))\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name=\'trans_inv_p{}\'.format(pyramid_len - ind))\n                up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'up_conv_p{}\'.format(pyramid_len - ind))\n                up_sampling = lateral + up_sampling\n                lateral = up_sampling\n            else:\n                up_sampling = lateral\n\n            pyramid_laterals.append(lateral)\n\n            lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv2_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(lateral, name=\'relu2_p{}\'.format(pyramid_len - ind))\n\n            outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 2, 3, 1], name=\'output_trans_p{}\'.format(pyramid_len - ind))\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            pyramid_heatmaps.append(outputs)\n    with tf.variable_scope(\'global_net\', \'global_net\', values=pyramid_laterals):\n        global_pyramids = []\n        for ind, lateral in enumerate(pyramid_laterals):\n            inputs = lateral\n            for bottleneck_ind in range(pyramid_len - ind - 1):\n                inputs = global_net_bottleneck_block(inputs, 128, istraining, data_format, name=\'global_net_bottleneck_{}_p{}\'.format(bottleneck_ind, pyramid_len - ind))\n\n            #if ind < pyramid_len - 1:\n                # resize back to the output heatmap size\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(inputs, [0, 2, 3, 1], name=\'global_output_trans_p{}\'.format(pyramid_len - ind))\n            else:\n                outputs = inputs\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'global_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'global_heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            # else:\n            #     outputs = tf.identity(inputs, \'global_heatmap_p{}\'.format(pyramid_len - ind))\n\n            global_pyramids.append(outputs)\n\n        concat_pyramids = tf.concat(global_pyramids, 1 if data_format == \'channels_first\' else 3, name=\'concat\')\n\n        def projection_shortcut(inputs):\n            return conv2d_fixed_padding(inputs=inputs, filters=256, kernel_size=1, strides=1, data_format=data_format, name=\'shortcut\')\n\n        outputs = global_net_bottleneck_block(concat_pyramids, 128, istraining, data_format, projection_shortcut=projection_shortcut, name=\'global_concat_bottleneck\')\n        outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap\')\n\n\n    return pyramid_heatmaps + [outputs]\n\n\n\n\n\n'"
net/detnet_cpn.py,47,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\n_USE_FUSED_BN = True\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format, name=None):\n    """"""Performs a batch normalization using a standard set of parameters.""""""\n    # We set fused=True for a significant performance boost. See\n    # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n        scale=True, training=training, name=name, fused=_USE_FUSED_BN)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    """"""Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                   Should be a positive integer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      A tensor with the same format as the input with the data either intact\n      (if kernel_size == 1) or padded (if kernel_size > 1).\n    """"""\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == \'channels_first\':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                      [pad_beg, pad_end], [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                      [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=tf.glorot_uniform_initializer, name=None):\n    """"""Strided 2-D convolution with explicit padding.""""""\n    # The padding is consistent and is based only on `kernel_size`, not on the\n    # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n                inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n                padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n                kernel_initializer=kernel_initializer(),\n                data_format=data_format, name=name)\n\n\n################################################################################\n# ResNet block definitions.\n################################################################################\ndef _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n                         strides, data_format):\n    """"""A single block for ResNet v1, with a bottleneck.\n\n    Similar to _building_block_v1(), except using the ""bottleneck"" blocks\n    described in:\n      Convolution then batch normalization then ReLU as described by:\n        Deep Residual Learning for Image Recognition\n        https://arxiv.org/pdf/1512.03385.pdf\n        by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      filters: The number of filters for the convolutions.\n      training: A Boolean for whether the model is in training or inference\n        mode. Needed for batch normalization.\n      projection_shortcut: The function to use for projection shortcuts\n        (typically a 1x1 convolution when downsampling the input).\n      strides: The block\'s stride. If greater than 1, this block will ultimately\n        downsample the input.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      The output tensor of the block; shape should match inputs.\n    """"""\n    shortcut = inputs\n\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n        shortcut = batch_norm(inputs=shortcut, training=training,\n                              data_format=data_format)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs += shortcut\n    inputs = tf.nn.relu(inputs)\n\n    return inputs\n\ndef block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n                training, name, data_format):\n    """"""Creates one layer of blocks for the ResNet model.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      filters: The number of filters for the first convolution of the layer.\n      bottleneck: Is the block created a bottleneck block.\n      block_fn: The block to use within the model, either `building_block` or\n        `bottleneck_block`.\n      blocks: The number of blocks contained in the layer.\n      strides: The stride to use for the first convolution of the layer. If\n        greater than 1, this layer will ultimately downsample the input.\n      training: Either True or False, whether we are currently training the\n        model. Needed for batch norm.\n      name: A string name for the tensor output of the block layer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      The output tensor of the block layer.\n    """"""\n\n    # Bottleneck blocks end with 4x the number of filters as they start with\n    filters_out = filters * 4 if bottleneck else filters\n\n    def projection_shortcut(inputs):\n        return conv2d_fixed_padding(\n            inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n            data_format=data_format)\n\n    # Only the first block per block_layer uses projection_shortcut and strides\n    inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n                      data_format)\n\n    for _ in range(1, blocks):\n        inputs = block_fn(inputs, filters, training, None, 1, data_format)\n\n    return tf.identity(inputs, name)\n\ndef _dilated_bottleneck_block_v1(inputs, filters, training, projection_shortcut, data_format):\n    shortcut = inputs\n\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n        shortcut = batch_norm(inputs=shortcut, training=training,\n                              data_format=data_format)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=3, strides=1,\n                  dilation_rate=(2, 2), padding=\'SAME\', use_bias=False,\n                  kernel_initializer=tf.glorot_uniform_initializer(),\n                  data_format=data_format, name=None)\n\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    #print(inputs)\n    inputs += shortcut\n    inputs = tf.nn.relu(inputs)\n\n    return inputs\n\ndef dilated_block_layer(inputs, filters, bottleneck, block_fn, blocks,\n                training, name, data_format):\n    # Bottleneck blocks end with 4x the number of filters as they start with\n    filters_out = filters * 4 if bottleneck else filters\n\n    def projection_shortcut(inputs):\n        return conv2d_fixed_padding(inputs=inputs, filters=filters_out, kernel_size=1, strides=1, data_format=data_format)\n\n    # Only the first block per block_layer uses projection_shortcut and strides\n    inputs = block_fn(inputs, filters, training, projection_shortcut, data_format)\n\n    for _ in range(1, blocks):\n        inputs = block_fn(inputs, filters, training, None, data_format)\n\n    return tf.identity(inputs, name)\n\ndef detnet_cpn_backbone(inputs, istraining, data_format):\n    block_strides = [1, 2, 2]\n    inputs = conv2d_fixed_padding(inputs=inputs, filters=64, kernel_size=7, strides=2, data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer)\n    inputs = tf.identity(inputs, \'initial_conv\')\n\n    inputs = tf.layers.max_pooling2d(inputs=inputs, pool_size=3, strides=2, padding=\'SAME\', data_format=data_format)\n    inputs = tf.identity(inputs, \'initial_max_pool\')\n\n    end_points = []\n    for i, num_blocks in enumerate([3, 4, 6]):\n      num_filters = 64 * (2**i)\n      #with tf.variable_scope(\'block_{}\'.format(i), \'resnet50\', values=[inputs]):\n      inputs = block_layer(\n          inputs=inputs, filters=num_filters, bottleneck=True,\n          block_fn=_bottleneck_block_v1, blocks=num_blocks,\n          strides=block_strides[i], training=istraining,\n          name=\'block_layer{}\'.format(i + 1), data_format=data_format)\n      end_points.append(inputs)\n\n    #print(inputs)\n    with tf.variable_scope(\'additional_layer\', \'additional_layer\', values=[inputs]):\n      # conv5\n      inputs = dilated_block_layer(\n            inputs=inputs, filters=256, bottleneck=True,\n            block_fn=_dilated_bottleneck_block_v1, blocks=3, training=istraining,\n            name=\'block_layer{}\'.format(4), data_format=data_format)\n      end_points.append(inputs)\n      # conv6\n      inputs = dilated_block_layer(\n            inputs=inputs, filters=256, bottleneck=True,\n            block_fn=_dilated_bottleneck_block_v1, blocks=3, training=istraining,\n            name=\'block_layer{}\'.format(5), data_format=data_format)\n      end_points.append(inputs)\n\n    return end_points[1:]\n\ndef global_net_bottleneck_block(inputs, filters, istraining, data_format, projection_shortcut=None, name=None):\n    with tf.variable_scope(name, \'global_net_bottleneck\', values=[inputs]):\n        shortcut = inputs\n        if projection_shortcut is not None:\n            shortcut = projection_shortcut(inputs)\n            shortcut = batch_norm(inputs=shortcut, training=istraining,\n                                  data_format=data_format, name=\'batch_normalization_shortcut\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_down\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_1\')\n        inputs = tf.nn.relu(inputs, name=\'relu1\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=3, strides=1,\n            data_format=data_format, name=\'3x3_conv\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_2\')\n        inputs = tf.nn.relu(inputs, name=\'relu2\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=2 * filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_up\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_3\')\n        inputs += shortcut\n        inputs = tf.nn.relu(inputs, name=\'relu3\')\n\n        return inputs\n\ndef cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format):\n    #with tf.variable_scope(\'resnet50\', \'resnet50\', values=[inputs]):\n    end_points = detnet_cpn_backbone(inputs, istraining, data_format)\n    pyramid_len = len(end_points)\n    up_sampling = None\n    pyramid_heatmaps = []\n    pyramid_laterals = []\n    with tf.variable_scope(\'feature_pyramid\', \'feature_pyramid\', values=end_points):\n        # top-down\n        for ind, pyramid in enumerate(reversed(end_points)):\n            inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv1_p{}\'.format(pyramid_len - ind + 1))\n            lateral = tf.nn.relu(inputs, name=\'relu1_p{}\'.format(pyramid_len - ind + 1))\n            if up_sampling is not None:\n                if ind > pyramid_len - 2:\n                    if data_format == \'channels_first\':\n                        up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name=\'trans_p{}\'.format(pyramid_len - ind + 1))\n                    up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name=\'upsample_p{}\'.format(pyramid_len - ind + 1))\n                    if data_format == \'channels_first\':\n                        up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name=\'trans_inv_p{}\'.format(pyramid_len - ind + 1))\n                    up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                              data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'up_conv_p{}\'.format(pyramid_len - ind + 1))\n                up_sampling = lateral + up_sampling\n                lateral = up_sampling\n            else:\n                up_sampling = lateral\n\n            pyramid_laterals.append(lateral)\n\n            lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv2_p{}\'.format(pyramid_len - ind + 1))\n            lateral = tf.nn.relu(lateral, name=\'relu2_p{}\'.format(pyramid_len - ind + 1))\n\n            outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap_p{}\'.format(pyramid_len - ind + 1))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 2, 3, 1], name=\'output_trans_p{}\'.format(pyramid_len - ind + 1))\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'heatmap_p{}\'.format(pyramid_len - ind + 1))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'heatmap_trans_inv_p{}\'.format(pyramid_len - ind + 1))\n            pyramid_heatmaps.append(outputs)\n    with tf.variable_scope(\'global_net\', \'global_net\', values=pyramid_laterals):\n        global_pyramids = []\n        for ind, lateral in enumerate(pyramid_laterals):\n            inputs = lateral\n            for bottleneck_ind in range(pyramid_len - ind - 1):\n                inputs = global_net_bottleneck_block(inputs, 128, istraining, data_format, name=\'global_net_bottleneck_{}_p{}\'.format(bottleneck_ind, pyramid_len - ind))\n\n            #if ind < pyramid_len - 1:\n                # resize back to the output heatmap size\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(inputs, [0, 2, 3, 1], name=\'global_output_trans_p{}\'.format(pyramid_len - ind))\n            else:\n                outputs = inputs\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'global_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'global_heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            # else:\n            #     outputs = tf.identity(inputs, \'global_heatmap_p{}\'.format(pyramid_len - ind))\n\n            global_pyramids.append(outputs)\n\n        concat_pyramids = tf.concat(global_pyramids, 1 if data_format == \'channels_first\' else 3, name=\'concat\')\n\n        def projection_shortcut(inputs):\n            return conv2d_fixed_padding(inputs=inputs, filters=256, kernel_size=1, strides=1, data_format=data_format, name=\'shortcut\')\n\n        outputs = global_net_bottleneck_block(concat_pyramids, 128, istraining, data_format, projection_shortcut=projection_shortcut, name=\'global_concat_bottleneck\')\n        outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap\')\n\n\n    return pyramid_heatmaps + [outputs]\n'"
net/detxt_cpn.py,192,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport math\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\n_USE_FUSED_BN = True\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format, name=None):\n    """"""Performs a batch normalization using a standard set of parameters.""""""\n    # We set fused=True for a significant performance boost. See\n    # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n        scale=True, training=training, name=name, fused=_USE_FUSED_BN)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    """"""Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                   Should be a positive integer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      A tensor with the same format as the input with the data either intact\n      (if kernel_size == 1) or padded (if kernel_size > 1).\n    """"""\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == \'channels_first\':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                      [pad_beg, pad_end], [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                      [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=tf.glorot_uniform_initializer, name=None):\n    """"""Strided 2-D convolution with explicit padding.""""""\n    # The padding is consistent and is based only on `kernel_size`, not on the\n    # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n                inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n                padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n                kernel_initializer=kernel_initializer(),\n                data_format=data_format, name=name)\n\n# input image order: BGR, range [0-255]\n# mean_value: 104, 117, 123\n# only subtract mean is used\ndef constant_xavier_initializer(shape, group, dtype=tf.float32, uniform=True):\n    """"""Initializer function.""""""\n    if not dtype.is_floating:\n      raise TypeError(\'Cannot create initializer for non-floating point type.\')\n    # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n    # This is the right thing for matrix multiply and convolutions.\n    if shape:\n      fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n      fan_out = float(shape[-1])/group\n    else:\n      fan_in = 1.0\n      fan_out = 1.0\n    for dim in shape[:-2]:\n      fan_in *= float(dim)\n      fan_out *= float(dim)\n\n    # Average number of inputs and output connections.\n    n = (fan_in + fan_out) / 2.0\n    if uniform:\n      # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n      limit = math.sqrt(3.0 * 1.0 / n)\n      return tf.random_uniform(shape, -limit, limit, dtype, seed=None)\n    else:\n      # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n      trunc_stddev = math.sqrt(1.3 * 1.0 / n)\n      return tf.truncated_normal(shape, 0.0, trunc_stddev, dtype, seed=None)\n\ndef wrapper_initlizer(shape, dtype=None, partition_info=None):\n    return constant_xavier_initializer(shape, 32, dtype)\n# for root block, use dummy input_filters, e.g. 128 rather than 64 for the first block\ndef se_next_bottleneck_block(inputs, input_filters, name_prefix, is_training, group, data_format=\'channels_last\', need_reduce=True, is_root=False, reduced_scale=16):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    strides_to_use = 1\n    residuals = inputs\n    if need_reduce:\n        strides_to_use = 1 if is_root else 2\n        #print(strides_to_use)\n        proj_mapping = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_proj\', strides=(strides_to_use, strides_to_use),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        # print(proj_mapping)\n        residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    #print(strides_to_use)\n    reduced_inputs = tf.layers.conv2d(inputs, input_filters // 2, (1, 1), use_bias=False,\n                            name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                            padding=\'valid\', data_format=data_format, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer())\n    reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n    if data_format == \'channels_first\':\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n    else:\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n    convolved = [tf.nn.convolution(x, weight, padding=\'VALID\', strides=[strides_to_use, strides_to_use], name=name_prefix + \'_group_conv\',\n                    data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n    if data_format == \'channels_first\':\n        conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n    else:\n        conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n    conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n    increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n    down_inputs = tf.layers.conv2d(pooled_inputs, input_filters // reduced_scale, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n    up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n    rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n    pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n    return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n\ndef dilated_se_next_bottleneck_block(inputs, input_filters, name_prefix, is_training, group, data_format=\'channels_last\', need_reduce=True, reduced_scale=16):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    residuals = inputs\n    if need_reduce:\n        proj_mapping = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_proj\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        # print(proj_mapping)\n        residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    #print(strides_to_use)\n    reduced_inputs = tf.layers.conv2d(inputs, input_filters // 2, (1, 1), use_bias=False,\n                            name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                            padding=\'valid\', data_format=data_format, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer())\n    reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n    if data_format == \'channels_first\':\n        #reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n    else:\n        #reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n    # !!! before is VALID !!!\n    convolved = [tf.nn.convolution(x, weight, padding=\'SAME\', strides=[1, 1], dilation_rate=[2, 2], name=name_prefix + \'_group_conv\',\n                    data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n    if data_format == \'channels_first\':\n        conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n    else:\n        conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n    conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n    increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n    down_inputs = tf.layers.conv2d(pooled_inputs, input_filters // reduced_scale, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n    up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n    rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n    pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n    return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n\n# the input image should in BGR order, note that this is not the common case in Tensorflow\ndef sext_cpn_backbone(input_image, istraining, data_format, net_depth=50, group=32):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n\n    if data_format == \'channels_last\':\n        image_channels = tf.unstack(input_image, axis=-1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=-1)\n    else:\n        image_channels = tf.unstack(input_image, axis=1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=1)\n    #swaped_input_image = input_image\n\n    if net_depth not in [50, 101]:\n        raise TypeError(\'Only ResNeXt50 or ResNeXt101 is supprted now.\')\n\n    input_depth = [256, 512, 1024] # the input depth of the the first block is dummy input\n    num_units = [3, 4, 6] if net_depth==50 else [3, 4, 23]\n    block_name_prefix = [\'conv2_{}\', \'conv3_{}\', \'conv4_{}\']\n\n    if data_format == \'channels_first\':\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [0, 0], [3, 3], [3, 3]])\n    else:\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [3, 3], [3, 3], [0, 0]])\n\n    inputs_features = tf.layers.conv2d(swaped_input_image, input_depth[0]//4, (7, 7), use_bias=False,\n                                name=\'conv1/7x7_s2\', strides=(2, 2),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    #print(ee)\n\n    inputs_features = tf.layers.batch_normalization(inputs_features, momentum=_BATCH_NORM_DECAY,\n                                        name=\'conv1/7x7_s2/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=istraining, reuse=None, fused=_USE_FUSED_BN)\n    inputs_features = tf.nn.relu(inputs_features, name=\'conv1/relu_7x7_s2\')\n\n    inputs_features = tf.layers.max_pooling2d(inputs_features, [3, 3], [2, 2], padding=\'same\', data_format=data_format, name=\'pool1/3x3_s2\')\n\n    end_points = []\n    is_root = True\n    for ind, num_unit in enumerate(num_units):\n        need_reduce = True\n        for unit_index in range(1, num_unit+1):\n            inputs_features = se_next_bottleneck_block(inputs_features, input_depth[ind], block_name_prefix[ind].format(unit_index), is_training=istraining, group=group, data_format=data_format, need_reduce=need_reduce, is_root=is_root)\n            need_reduce = False\n            is_root = False\n        end_points.append(inputs_features)\n\n    #print(inputs)\n    with tf.variable_scope(\'additional_layer\', \'additional_layer\', values=[inputs_features], reuse=None):\n      # conv5\n      need_reduce = True\n      for unit_index in range(1, 4):\n          inputs_features = dilated_se_next_bottleneck_block(inputs_features, 1024, \'conv5_{}\'.format(unit_index), is_training=istraining, group=group, data_format=data_format, need_reduce=need_reduce)\n          need_reduce = False\n      end_points.append(inputs_features)\n      # conv6\n      need_reduce = True\n      for unit_index in range(1, 4):\n          inputs_features = dilated_se_next_bottleneck_block(inputs_features, 1024, \'conv6_{}\'.format(unit_index), is_training=istraining, group=group, data_format=data_format, need_reduce=need_reduce)\n          need_reduce = False\n      end_points.append(inputs_features)\n\n    return end_points[1:]\n\ndef global_net_bottleneck_block(inputs, filters, istraining, data_format, projection_shortcut=None, name=None):\n    with tf.variable_scope(name, \'global_net_bottleneck\', values=[inputs]):\n        shortcut = inputs\n        if projection_shortcut is not None:\n            shortcut = projection_shortcut(inputs)\n            shortcut = batch_norm(inputs=shortcut, training=istraining,\n                                  data_format=data_format, name=\'batch_normalization_shortcut\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_down\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_1\')\n        inputs = tf.nn.relu(inputs, name=\'relu1\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=3, strides=1,\n            data_format=data_format, name=\'3x3_conv\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_2\')\n        inputs = tf.nn.relu(inputs, name=\'relu2\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=2 * filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_up\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_3\')\n        inputs += shortcut\n        inputs = tf.nn.relu(inputs, name=\'relu3\')\n\n        return inputs\n\n\ndef global_net_sext_bottleneck_block(inputs, input_filters, is_training, data_format, need_reduce=False, name_prefix=None, group=32, reduced_scale=16):\n    with tf.variable_scope(name_prefix, \'global_net_sext_bottleneck_block\', values=[inputs]):\n        bn_axis = -1 if data_format == \'channels_last\' else 1\n        residuals = inputs\n        if need_reduce:\n            proj_mapping = tf.layers.conv2d(inputs, input_filters * 2, (1, 1), use_bias=False,\n                                    name=name_prefix + \'_1x1_proj\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n            # print(proj_mapping)\n            residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                    name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                    epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n        reduced_inputs = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                            name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                            epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n        reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n        if data_format == \'channels_first\':\n            reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n            weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters]\n            if is_training:\n                weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n            else:\n                weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n            weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n            xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n        else:\n            reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n            weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters]\n            if is_training:\n                weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n            else:\n                weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n            weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n            xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n        convolved = [tf.nn.convolution(x, weight, padding=\'VALID\', strides=[1, 1], name=name_prefix + \'_group_conv\',\n                        data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n        if data_format == \'channels_first\':\n            conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n        else:\n            conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n        conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                            axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n        conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n        increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters * 2, (1, 1), use_bias=False,\n                                    name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n        increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                            name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                            epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n        if data_format == \'channels_first\':\n            pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n        else:\n            pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n        down_inputs = tf.layers.conv2d(pooled_inputs, input_filters * 2 // reduced_scale, (1, 1), use_bias=True,\n                                    name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n        down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n        up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters * 2, (1, 1), use_bias=True,\n                                    name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n        prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n        #print(residuals, prob_outputs, increase_inputs_bn)\n        rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n        pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n        return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n\ndef cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=50):\n    #with tf.variable_scope(\'resnet50\', \'resnet50\', values=[inputs]):\n    end_points = sext_cpn_backbone(inputs, istraining, data_format, net_depth=net_depth)\n    pyramid_len = len(end_points)\n    up_sampling = None\n    pyramid_heatmaps = []\n    pyramid_laterals = []\n    with tf.variable_scope(\'feature_pyramid\', \'feature_pyramid\', values=end_points):\n        # top-down\n        for ind, pyramid in enumerate(reversed(end_points)):\n            inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv1_p{}\'.format(pyramid_len - ind + 1))\n            lateral = tf.nn.relu(inputs, name=\'relu1_p{}\'.format(pyramid_len - ind + 1))\n            if up_sampling is not None:\n                if ind > pyramid_len - 2:\n                    if data_format == \'channels_first\':\n                        up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name=\'trans_p{}\'.format(pyramid_len - ind + 1))\n                    up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name=\'upsample_p{}\'.format(pyramid_len - ind + 1))\n                    if data_format == \'channels_first\':\n                        up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name=\'trans_inv_p{}\'.format(pyramid_len - ind + 1))\n                    up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                              data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'up_conv_p{}\'.format(pyramid_len - ind + 1))\n                up_sampling = lateral + up_sampling\n                lateral = up_sampling\n            else:\n                up_sampling = lateral\n\n            pyramid_laterals.append(lateral)\n\n            lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv2_p{}\'.format(pyramid_len - ind + 1))\n            lateral = tf.nn.relu(lateral, name=\'relu2_p{}\'.format(pyramid_len - ind + 1))\n\n            outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap_p{}\'.format(pyramid_len - ind + 1))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 2, 3, 1], name=\'output_trans_p{}\'.format(pyramid_len - ind + 1))\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'heatmap_p{}\'.format(pyramid_len - ind + 1))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'heatmap_trans_inv_p{}\'.format(pyramid_len - ind + 1))\n            pyramid_heatmaps.append(outputs)\n    with tf.variable_scope(\'global_net\', \'global_net\', values=pyramid_laterals):\n        global_pyramids = []\n        for ind, lateral in enumerate(pyramid_laterals):\n            inputs = lateral\n            for bottleneck_ind in range(pyramid_len - ind - 1):\n                inputs = global_net_bottleneck_block(inputs, 128, istraining, data_format, name=\'global_net_bottleneck_{}_p{}\'.format(bottleneck_ind, pyramid_len - ind))\n\n            #if ind < pyramid_len - 1:\n                # resize back to the output heatmap size\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(inputs, [0, 2, 3, 1], name=\'global_output_trans_p{}\'.format(pyramid_len - ind))\n            else:\n                outputs = inputs\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'global_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'global_heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            # else:\n            #     outputs = tf.identity(inputs, \'global_heatmap_p{}\'.format(pyramid_len - ind))\n\n            global_pyramids.append(outputs)\n\n        concat_pyramids = tf.concat(global_pyramids, 1 if data_format == \'channels_first\' else 3, name=\'concat\')\n\n        def projection_shortcut(inputs):\n            return conv2d_fixed_padding(inputs=inputs, filters=256, kernel_size=1, strides=1, data_format=data_format, name=\'shortcut\')\n\n        outputs = global_net_bottleneck_block(concat_pyramids, 128, istraining, data_format, projection_shortcut=projection_shortcut, name=\'global_concat_bottleneck\')\n        outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap\')\n\n\n    return pyramid_heatmaps + [outputs]\n\ndef head_xt_cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format):\n    #with tf.variable_scope(\'resnet50\', \'resnet50\', values=[inputs]):\n    end_points = sext_cpn_backbone(inputs, istraining, data_format)\n    pyramid_len = len(end_points)\n    up_sampling = None\n    pyramid_heatmaps = []\n    pyramid_laterals = []\n    with tf.variable_scope(\'feature_pyramid\', \'feature_pyramid\', values=end_points):\n        # top-down\n        for ind, pyramid in enumerate(reversed(end_points)):\n            inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv1_p{}\'.format(pyramid_len - ind + 1))\n            lateral = tf.nn.relu(inputs, name=\'relu1_p{}\'.format(pyramid_len - ind + 1))\n            if up_sampling is not None:\n                if ind > pyramid_len - 2:\n                    if data_format == \'channels_first\':\n                        up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name=\'trans_p{}\'.format(pyramid_len - ind + 1))\n                    up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name=\'upsample_p{}\'.format(pyramid_len - ind + 1))\n                    if data_format == \'channels_first\':\n                        up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name=\'trans_inv_p{}\'.format(pyramid_len - ind + 1))\n                    up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                              data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'up_conv_p{}\'.format(pyramid_len - ind + 1))\n                up_sampling = lateral + up_sampling\n                lateral = up_sampling\n            else:\n                up_sampling = lateral\n\n            pyramid_laterals.append(lateral)\n\n            lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv2_p{}\'.format(pyramid_len - ind + 1))\n            lateral = tf.nn.relu(lateral, name=\'relu2_p{}\'.format(pyramid_len - ind + 1))\n\n            outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap_p{}\'.format(pyramid_len - ind + 1))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 2, 3, 1], name=\'output_trans_p{}\'.format(pyramid_len - ind + 1))\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'heatmap_p{}\'.format(pyramid_len - ind + 1))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'heatmap_trans_inv_p{}\'.format(pyramid_len - ind + 1))\n            pyramid_heatmaps.append(outputs)\n    with tf.variable_scope(\'global_net\', \'global_net\', values=pyramid_laterals):\n        global_pyramids = []\n        for ind, lateral in enumerate(pyramid_laterals):\n            inputs = lateral\n            for bottleneck_ind in range(pyramid_len - ind - 1):\n                inputs = global_net_sext_bottleneck_block(inputs, 128, istraining, data_format, name_prefix=\'global_net_bottleneck_{}_p{}\'.format(bottleneck_ind, pyramid_len - ind))\n\n            #if ind < pyramid_len - 1:\n                # resize back to the output heatmap size\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(inputs, [0, 2, 3, 1], name=\'global_output_trans_p{}\'.format(pyramid_len - ind))\n            else:\n                outputs = inputs\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'global_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'global_heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            # else:\n            #     outputs = tf.identity(inputs, \'global_heatmap_p{}\'.format(pyramid_len - ind))\n\n            global_pyramids.append(outputs)\n\n        concat_pyramids = tf.concat(global_pyramids, 1 if data_format == \'channels_first\' else 3, name=\'concat\')\n\n        outputs = global_net_sext_bottleneck_block(concat_pyramids, 128, istraining, data_format, need_reduce=True, name_prefix=\'global_concat_bottleneck\')\n\n        outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap\')\n\n\n    return pyramid_heatmaps + [outputs]\n'"
net/hourglass.py,35,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\n_USE_FUSED_BN = True\n#initializer_to_use = tf.glorot_uniform_initializer  glorot_normal_initializer\ninitializer_to_use = tf.glorot_uniform_initializer\nconv_bn_initializer_to_use = tf.glorot_uniform_initializer#lambda : tf.truncated_normal_initializer(mean=0.0, stddev=0.005)\n\ndef batch_norm_relu(inputs, is_training, data_format, name=None):\n  """"""Performs a batch normalization followed by a ReLU.""""""\n  # We set fused=True for a significant performance boost. See\n  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n  with tf.variable_scope(name, \'batch_norm_relu\', values=[inputs]):\n    inputs = tf.layers.batch_normalization(\n              inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n              momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n              scale=True, training=is_training, fused=_USE_FUSED_BN, name=\'batch_normalization\')\n    inputs = tf.nn.relu(inputs, name=\'relu\')\n    return inputs\n\ndef batch_norm(inputs, is_training, data_format, name=None):\n  """"""Performs a batch normalization followed by a ReLU.""""""\n  # We set fused=True for a significant performance boost. See\n  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n  with tf.variable_scope(name, \'batch_norm\', values=[inputs]):\n    inputs = tf.layers.batch_normalization(\n              inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n              momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n              scale=True, training=is_training, fused=_USE_FUSED_BN, name=\'batch_normalization\')\n    return inputs\n\ndef fixed_padding(inputs, kernel_size, data_format):\n  with tf.variable_scope(\'fixed_padding\', values=[inputs]):\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == \'channels_first\':\n      padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                      [pad_beg, pad_end], [pad_beg, pad_end]], name=\'padding\')\n    else:\n      padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                      [pad_beg, pad_end], [0, 0]], name=\'padding\')\n    return padded_inputs\n\n# this is only can be used before BN\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=conv_bn_initializer_to_use, name=None):\n  """"""Strided 2-D convolution with explicit padding.""""""\n  # The padding is consistent and is based only on `kernel_size`, not on the\n  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n  with tf.variable_scope(name, \'fix_padding_conv\', values=[inputs]):\n    if strides > 1:\n      inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n              inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n              padding=(\'same\' if strides == 1 else \'valid\'), use_bias=False,\n              kernel_initializer=kernel_initializer(),\n              data_format=data_format, name=\'conv2d\')\n\ndef bottleneck_block_v2(inputs, in_filters, out_filters, is_training, data_format, name=None):\n  with tf.variable_scope(name, \'bottleneck_block\', values=[inputs]):\n    shortcut = inputs\n    inputs = batch_norm_relu(inputs, is_training, data_format, name=\'bn_relu_1\')\n\n    # The projection shortcut should come after the first batch norm and ReLU\n    # since it performs a 1x1 convolution.\n    # different from original hourglass\n    if in_filters != out_filters:\n      shortcut = conv2d_fixed_padding(\n                  inputs=inputs, filters=out_filters, kernel_size=1, strides=1,\n                  data_format=data_format, name=\'skip\')\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=out_filters//2, kernel_size=1, strides=1,\n        data_format=data_format, name=\'1x1_down\')\n    inputs = batch_norm_relu(inputs, is_training, data_format, name=\'bn_relu_2\')\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=out_filters//2, kernel_size=3, strides=1,\n        data_format=data_format, name=\'3x3_conv\')\n    inputs = batch_norm_relu(inputs, is_training, data_format, name=\'bn_relu_3\')\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=out_filters, kernel_size=1, strides=1,\n        data_format=data_format, name=\'1x1_up\')\n\n    return tf.add(inputs, shortcut, name=\'elem_add\')\n\ndef bottleneck_block_v1(inputs, in_filters, out_filters, is_training, data_format, name=None):\n  with tf.variable_scope(name, \'bottleneck_block_v1\', values=[inputs]):\n    shortcut = inputs\n    if in_filters != out_filters:\n      shortcut = conv2d_fixed_padding(\n                  inputs=shortcut, filters=out_filters, kernel_size=1, strides=1,\n                  data_format=data_format, name=\'skip\')\n      shortcut = batch_norm(shortcut, is_training, data_format, name=\'skip_bn\')\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=out_filters//2, kernel_size=1, strides=1,\n        data_format=data_format, name=\'1x1_down\')\n    inputs = batch_norm_relu(inputs, is_training, data_format, name=\'bn_relu_1\')\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=out_filters//2, kernel_size=3, strides=1,\n        data_format=data_format, name=\'3x3_conv\')\n    inputs = batch_norm_relu(inputs, is_training, data_format, name=\'bn_relu_2\')\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs, filters=out_filters, kernel_size=1, strides=1,\n        data_format=data_format, name=\'1x1_up\')\n    inputs = batch_norm(inputs, is_training, data_format, name=\'up_bn\')\n\n    return tf.nn.relu(tf.add(inputs, shortcut, name=\'elem_add\'), name=\'relu\')\n\n\nbottleneck_block = bottleneck_block_v2\n\ndef dozen_bottleneck_blocks(inputs, in_filters, out_filters, num_modules, is_training, data_format, name=None):\n  for m in range(num_modules):\n    inputs = bottleneck_block(inputs, in_filters, out_filters, is_training, data_format, name=None if name is None else name.format(m))\n\n  return inputs\n\ndef hourglass(inputs, filters, is_training, data_format, deep_index=1, num_modules=1, name=None):\n  with tf.variable_scope(name, \'hourglass_unit\', values=[inputs]):\n    upchannal1 = dozen_bottleneck_blocks(inputs, filters, filters, num_modules, is_training, data_format, name=\'up_{}\')\n    downchannal1 = tf.layers.max_pooling2d(inputs=inputs, pool_size=2, strides=2, padding=\'valid\', data_format=data_format, name=\'down_pool\')\n    downchannal1 = dozen_bottleneck_blocks(downchannal1, filters, filters, num_modules, is_training, data_format, name=\'down1_{}\')\n\n    if deep_index > 1:\n      downchannal2 = hourglass(downchannal1, filters, is_training, data_format, deep_index=deep_index-1, num_modules=num_modules, name=\'inner_{}\'.format(deep_index))\n    else:\n      downchannal2 = dozen_bottleneck_blocks(downchannal1, filters, filters, num_modules, is_training, data_format, name=\'down2_{}\')\n\n    downchannal3 = dozen_bottleneck_blocks(downchannal2, filters, filters, num_modules, is_training, data_format, name=\'down3_{}\')\n\n    if data_format == \'channels_first\':\n        downchannal3 = tf.transpose(downchannal3, [0, 2, 3, 1], name=\'trans\')\n    input_shape = tf.shape(downchannal3)[-3:-1] * 2\n    upchannal2 = tf.image.resize_bilinear(downchannal3, input_shape, name=\'resize\')\n    if data_format == \'channels_first\':\n      upchannal2 = tf.transpose(upchannal2, [0, 3, 1, 2], name=\'trans_inv\')\n\n    return tf.add(upchannal1, upchannal2, name=\'elem_add\')\n\ndef create_model(inputs, num_stack, feat_channals, output_channals, num_modules, is_training, data_format):\n  with tf.variable_scope(\'precede\', values=[inputs]):\n    inputs = conv2d_fixed_padding(inputs=inputs, filters=64, kernel_size=7, strides=2,\n              data_format=data_format, kernel_initializer=conv_bn_initializer_to_use, name=\'conv_7x7\')\n    inputs = batch_norm_relu(inputs, is_training, data_format, name=\'inputs_bn\')\n\n    inputs = bottleneck_block(inputs, 64, 128, is_training, data_format, name=\'residual1\')\n    inputs = tf.layers.max_pooling2d(inputs=inputs, pool_size=2, strides=2, padding=\'valid\',\n                data_format=data_format, name=\'pool\')\n\n    inputs = bottleneck_block(inputs, 128, 128, is_training, data_format, name=\'residual2\')\n    inputs = bottleneck_block(inputs, 128, feat_channals, is_training, data_format, name=\'residual3\')\n\n  #return [inputs]\n  hg_inputs = inputs\n  outputs_list = []\n  for stack_index in range(num_stack):\n    hg = hourglass(hg_inputs, feat_channals, is_training, data_format, deep_index=4, num_modules=num_modules, name=\'stack_{}/hg\'.format(stack_index))\n\n    hg = dozen_bottleneck_blocks(hg, feat_channals, feat_channals, num_modules, is_training, data_format, name=\'stack_{}/\'.format(stack_index) + \'output_{}\')\n\n    # produce prediction\n    output_scores = conv2d_fixed_padding(inputs=hg, filters=feat_channals, kernel_size=1, strides=1, data_format=data_format, name=\'stack_{}/output_1x1\'.format(stack_index))\n    #outputs_list.append(output_scores)\n    output_scores = batch_norm_relu(output_scores, is_training, data_format, name=\'stack_{}/output_bn\'.format(stack_index))\n\n    # produce heatmap from prediction\n    # use variable_scope to help model resotre name filter\n    heatmap = tf.layers.conv2d(inputs=output_scores, filters=output_channals, kernel_size=1,\n                                strides=1, padding=\'same\', use_bias=True, activation=None,\n                                kernel_initializer=initializer_to_use(),\n                                bias_initializer=tf.zeros_initializer(),\n                                data_format=data_format,\n                                name=\'hg_heatmap/stack_{}/heatmap_1x1\'.format(stack_index))\n\n\n    outputs_list.append(heatmap)\n    # no remap conv for the last hourglass\n    if stack_index < num_stack - 1:\n      output_scores_ = tf.layers.conv2d(inputs=output_scores, filters=feat_channals, kernel_size=1,\n                          strides=1, padding=\'same\', use_bias=True, activation=None,\n                          kernel_initializer=initializer_to_use(),\n                          bias_initializer=tf.zeros_initializer(),\n                          data_format=data_format,\n                          name=\'stack_{}/remap_outputs\'.format(stack_index))\n      # use variable_scope to help model resotre name filter\n      heatmap_ = tf.layers.conv2d(inputs=heatmap, filters=feat_channals, kernel_size=1,\n                        strides=1, padding=\'same\', use_bias=True, activation=None,\n                        kernel_initializer=initializer_to_use(),\n                        bias_initializer=tf.zeros_initializer(),\n                        data_format=data_format,\n                        name=\'hg_heatmap/stack_{}/remap_heatmap\'.format(stack_index))\n\n      # next hourglass inputs\n      fused_heatmap = tf.add(output_scores_, heatmap_, \'stack_{}/fused_heatmap\'.format(stack_index))\n      hg_inputs = tf.add(hg_inputs, fused_heatmap, \'stack_{}/next_inputs\'.format(stack_index))\n      #hg_inputs = hg_inputs + output_scores_ + heatmap_\n\n  return outputs_list\n\n\n\n\n'"
net/seresnet_cpn.py,219,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport math\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\n_USE_FUSED_BN = True\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format, name=None):\n    """"""Performs a batch normalization using a standard set of parameters.""""""\n    # We set fused=True for a significant performance boost. See\n    # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n        scale=True, training=training, name=name, fused=_USE_FUSED_BN)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    """"""Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                   Should be a positive integer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      A tensor with the same format as the input with the data either intact\n      (if kernel_size == 1) or padded (if kernel_size > 1).\n    """"""\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == \'channels_first\':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                      [pad_beg, pad_end], [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                      [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=tf.glorot_uniform_initializer, name=None):\n    """"""Strided 2-D convolution with explicit padding.""""""\n    # The padding is consistent and is based only on `kernel_size`, not on the\n    # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n                inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n                padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n                kernel_initializer=kernel_initializer(),\n                data_format=data_format, name=name)\n\n\n################################################################################\n# ResNet block definitions.\n################################################################################\ndef _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n                         strides, data_format):\n    """"""A single block for ResNet v1, with a bottleneck.\n\n    Similar to _building_block_v1(), except using the ""bottleneck"" blocks\n    described in:\n      Convolution then batch normalization then ReLU as described by:\n        Deep Residual Learning for Image Recognition\n        https://arxiv.org/pdf/1512.03385.pdf\n        by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      filters: The number of filters for the convolutions.\n      training: A Boolean for whether the model is in training or inference\n        mode. Needed for batch normalization.\n      projection_shortcut: The function to use for projection shortcuts\n        (typically a 1x1 convolution when downsampling the input).\n      strides: The block\'s stride. If greater than 1, this block will ultimately\n        downsample the input.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      The output tensor of the block; shape should match inputs.\n    """"""\n    shortcut = inputs\n\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n        shortcut = batch_norm(inputs=shortcut, training=training,\n                              data_format=data_format)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n                inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n                data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs += shortcut\n    inputs = tf.nn.relu(inputs)\n\n    return inputs\n\ndef block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n                training, name, data_format):\n    """"""Creates one layer of blocks for the ResNet model.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      filters: The number of filters for the first convolution of the layer.\n      bottleneck: Is the block created a bottleneck block.\n      block_fn: The block to use within the model, either `building_block` or\n        `bottleneck_block`.\n      blocks: The number of blocks contained in the layer.\n      strides: The stride to use for the first convolution of the layer. If\n        greater than 1, this layer will ultimately downsample the input.\n      training: Either True or False, whether we are currently training the\n        model. Needed for batch norm.\n      name: A string name for the tensor output of the block layer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      The output tensor of the block layer.\n    """"""\n\n    # Bottleneck blocks end with 4x the number of filters as they start with\n    filters_out = filters * 4 if bottleneck else filters\n\n    def projection_shortcut(inputs):\n        return conv2d_fixed_padding(\n            inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n            data_format=data_format)\n\n    # Only the first block per block_layer uses projection_shortcut and strides\n    inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n                      data_format)\n\n    for _ in range(1, blocks):\n        inputs = block_fn(inputs, filters, training, None, 1, data_format)\n\n    return tf.identity(inputs, name)\n\n# input image order: BGR, range [0-255]\n# mean_value: 104, 117, 123\n# only subtract mean is used\ndef constant_xavier_initializer(shape, group, dtype=tf.float32, uniform=True):\n    """"""Initializer function.""""""\n    if not dtype.is_floating:\n      raise TypeError(\'Cannot create initializer for non-floating point type.\')\n    # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n    # This is the right thing for matrix multiply and convolutions.\n    if shape:\n      fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n      fan_out = float(shape[-1])/group\n    else:\n      fan_in = 1.0\n      fan_out = 1.0\n    for dim in shape[:-2]:\n      fan_in *= float(dim)\n      fan_out *= float(dim)\n\n    # Average number of inputs and output connections.\n    n = (fan_in + fan_out) / 2.0\n    if uniform:\n      # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n      limit = math.sqrt(3.0 * 1.0 / n)\n      return tf.random_uniform(shape, -limit, limit, dtype, seed=None)\n    else:\n      # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n      trunc_stddev = math.sqrt(1.3 * 1.0 / n)\n      return tf.truncated_normal(shape, 0.0, trunc_stddev, dtype, seed=None)\n\ndef wrapper_initlizer(shape, dtype=None, partition_info=None):\n    return constant_xavier_initializer(shape, 32, dtype)\n\n# for root block, use dummy input_filters, e.g. 128 rather than 64 for the first block\ndef se_next_bottleneck_block(inputs, input_filters, name_prefix, is_training, group, data_format=\'channels_last\', need_reduce=True, is_root=False, reduced_scale=16):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    strides_to_use = 1\n    residuals = inputs\n    if need_reduce:\n        strides_to_use = 1 if is_root else 2\n        #print(strides_to_use)\n        proj_mapping = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_proj\', strides=(strides_to_use, strides_to_use),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        # print(proj_mapping)\n        residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    #print(strides_to_use)\n    reduced_inputs = tf.layers.conv2d(inputs, input_filters // 2, (1, 1), use_bias=False,\n                            name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                            padding=\'valid\', data_format=data_format, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer())\n    reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n    if data_format == \'channels_first\':\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n    else:\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n    convolved = [tf.nn.convolution(x, weight, padding=\'VALID\', strides=[strides_to_use, strides_to_use], name=name_prefix + \'_group_conv\',\n                    data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n    if data_format == \'channels_first\':\n        conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n    else:\n        conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n    conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n    increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n    down_inputs = tf.layers.conv2d(pooled_inputs, input_filters // reduced_scale, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n    up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n    #print(residuals, prob_outputs, increase_inputs_bn)\n    rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n    pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n    return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n    #return tf.nn.relu(residuals + prob_outputs * increase_inputs_bn, name=name_prefix + \'/relu\')\n\n# the input image should in BGR order, note that this is not the common case in Tensorflow\ndef sext_cpn_backbone(input_image, istraining, data_format, net_depth=50, group=32):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n\n    if data_format == \'channels_last\':\n        image_channels = tf.unstack(input_image, axis=-1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=-1)\n    else:\n        image_channels = tf.unstack(input_image, axis=1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=1)\n    #swaped_input_image = input_image\n\n    if net_depth not in [50, 101]:\n        raise TypeError(\'Only ResNeXt50 or ResNeXt101 is supprted now.\')\n\n    input_depth = [256, 512, 1024, 2048] # the input depth of the the first block is dummy input\n    num_units = [3, 4, 6, 3] if net_depth==50 else [3, 4, 23, 3]\n    block_name_prefix = [\'conv2_{}\', \'conv3_{}\', \'conv4_{}\', \'conv5_{}\']\n\n    if data_format == \'channels_first\':\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [0, 0], [3, 3], [3, 3]])\n    else:\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [3, 3], [3, 3], [0, 0]])\n\n    inputs_features = tf.layers.conv2d(swaped_input_image, input_depth[0]//4, (7, 7), use_bias=False,\n                                name=\'conv1/7x7_s2\', strides=(2, 2),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    #print(ee)\n\n    inputs_features = tf.layers.batch_normalization(inputs_features, momentum=_BATCH_NORM_DECAY,\n                                        name=\'conv1/7x7_s2/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=istraining, reuse=None, fused=_USE_FUSED_BN)\n    inputs_features = tf.nn.relu(inputs_features, name=\'conv1/relu_7x7_s2\')\n\n    inputs_features = tf.layers.max_pooling2d(inputs_features, [3, 3], [2, 2], padding=\'same\', data_format=data_format, name=\'pool1/3x3_s2\')\n\n    end_points = []\n    is_root = True\n    for ind, num_unit in enumerate(num_units):\n        need_reduce = True\n        for unit_index in range(1, num_unit+1):\n            inputs_features = se_next_bottleneck_block(inputs_features, input_depth[ind], block_name_prefix[ind].format(unit_index), is_training=istraining, group=group, data_format=data_format, need_reduce=need_reduce, is_root=is_root)\n            need_reduce = False\n            is_root = False\n        end_points.append(inputs_features)\n\n    return end_points\n\n# input image order: BGR, range [0-255]\n# mean_value: 104, 117, 123\n# only subtract mean is used\n\n# for root block, use dummy input_filters, e.g. 128 rather than 64 for the first block\ndef se_bottleneck_block(inputs, input_filters, name_prefix, is_training, data_format=\'channels_last\', need_reduce=True, is_root=False, reduced_scale=16):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    strides_to_use = 1\n    residuals = inputs\n    if need_reduce:\n        strides_to_use = 1 if is_root else 2\n        proj_mapping = tf.layers.conv2d(inputs, input_filters * 2, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_proj\', strides=(strides_to_use, strides_to_use),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    #print(strides_to_use)\n    reduced_inputs = tf.layers.conv2d(inputs, input_filters / 2, (1, 1), use_bias=False,\n                            name=name_prefix + \'_1x1_reduce\', strides=(strides_to_use, strides_to_use),\n                            padding=\'valid\', data_format=data_format, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer())\n    reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n\n    conv3_inputs = tf.layers.conv2d(reduced_inputs_relu, input_filters / 2, (3, 3), use_bias=False,\n                                name=name_prefix + \'_3x3\', strides=(1, 1),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n    increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters * 2, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n    down_inputs = tf.layers.conv2d(pooled_inputs, (input_filters * 2) // reduced_scale, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n    up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters * 2, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n    rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n    pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n    return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n\ndef se_cpn_backbone(input_image, istraining, data_format):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n\n    if data_format == \'channels_last\':\n        image_channels = tf.unstack(input_image, axis=-1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=-1)\n    else:\n        image_channels = tf.unstack(input_image, axis=1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=1)\n\n    input_depth = [128, 256, 512, 1024] # the input depth of the the first block is dummy input\n    num_units = [3, 4, 6, 3]\n    block_name_prefix = [\'conv2_{}\', \'conv3_{}\', \'conv4_{}\', \'conv5_{}\']\n\n    if data_format == \'channels_first\':\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [0, 0], [3, 3], [3, 3]])\n    else:\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [3, 3], [3, 3], [0, 0]])\n\n    inputs_features = tf.layers.conv2d(swaped_input_image, input_depth[0]//2, (7, 7), use_bias=False,\n                                name=\'conv1/7x7_s2\', strides=(2, 2),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n\n    inputs_features = tf.layers.batch_normalization(inputs_features, momentum=_BATCH_NORM_DECAY,\n                                        name=\'conv1/7x7_s2/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=istraining, reuse=None, fused=_USE_FUSED_BN)\n    inputs_features = tf.nn.relu(inputs_features, name=\'conv1/relu_7x7_s2\')\n\n    inputs_features = tf.layers.max_pooling2d(inputs_features, [3, 3], [2, 2], padding=\'same\', data_format=data_format, name=\'pool1/3x3_s2\')\n\n    end_points = []\n    is_root = True\n    for ind, num_unit in enumerate(num_units):\n        need_reduce = True\n        for unit_index in range(1, num_unit+1):\n            inputs_features = se_bottleneck_block(inputs_features, input_depth[ind], block_name_prefix[ind].format(unit_index), is_training=istraining, data_format=data_format, need_reduce=need_reduce, is_root=is_root)\n            need_reduce = False\n            is_root = False\n        end_points.append(inputs_features)\n\n    return end_points\n\ndef global_net_bottleneck_block(inputs, filters, istraining, data_format, projection_shortcut=None, name=None):\n    with tf.variable_scope(name, \'global_net_bottleneck\', values=[inputs]):\n        shortcut = inputs\n        if projection_shortcut is not None:\n            shortcut = projection_shortcut(inputs)\n            shortcut = batch_norm(inputs=shortcut, training=istraining,\n                                  data_format=data_format, name=\'batch_normalization_shortcut\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_down\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_1\')\n        inputs = tf.nn.relu(inputs, name=\'relu1\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=filters, kernel_size=3, strides=1,\n            data_format=data_format, name=\'3x3_conv\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_2\')\n        inputs = tf.nn.relu(inputs, name=\'relu2\')\n\n        inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=2 * filters, kernel_size=1, strides=1,\n            data_format=data_format, name=\'1x1_up\')\n        inputs = batch_norm(inputs, istraining, data_format, name=\'batch_normalization_3\')\n        inputs += shortcut\n        inputs = tf.nn.relu(inputs, name=\'relu3\')\n\n        return inputs\n\ndef global_net_sext_bottleneck_block(inputs, input_filters, is_training, data_format, need_reduce=False, name_prefix=None, group=32, reduced_scale=16):\n    with tf.variable_scope(name_prefix, \'global_net_sext_bottleneck_block\', values=[inputs]):\n        bn_axis = -1 if data_format == \'channels_last\' else 1\n        residuals = inputs\n        if need_reduce:\n            proj_mapping = tf.layers.conv2d(inputs, input_filters * 2, (1, 1), use_bias=False,\n                                    name=name_prefix + \'_1x1_proj\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n            # print(proj_mapping)\n            residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                    name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                    epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n        reduced_inputs = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                            name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                            epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n        reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n        if data_format == \'channels_first\':\n            reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n            weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters]\n            if is_training:\n                weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n            else:\n                weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n            weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n            xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n        else:\n            reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n            weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters]\n            if is_training:\n                weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n            else:\n                weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n            weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n            xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n        convolved = [tf.nn.convolution(x, weight, padding=\'VALID\', strides=[1, 1], name=name_prefix + \'_group_conv\',\n                        data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n        if data_format == \'channels_first\':\n            conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n        else:\n            conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n        conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                            axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n        conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n        increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters * 2, (1, 1), use_bias=False,\n                                    name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n        increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                            name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                            epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n        if data_format == \'channels_first\':\n            pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n        else:\n            pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n        down_inputs = tf.layers.conv2d(pooled_inputs, input_filters * 2 // reduced_scale, (1, 1), use_bias=True,\n                                    name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n        down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n        up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters * 2, (1, 1), use_bias=True,\n                                    name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                    padding=\'valid\', data_format=data_format, activation=None,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                    bias_initializer=tf.zeros_initializer())\n        prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n        #print(residuals, prob_outputs, increase_inputs_bn)\n        rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n        pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n        return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n\ndef cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format):\n    #with tf.variable_scope(\'resnet50\', \'resnet50\', values=[inputs]):\n    end_points = se_cpn_backbone(inputs, istraining, data_format)\n    pyramid_len = len(end_points)\n    up_sampling = None\n    pyramid_heatmaps = []\n    pyramid_laterals = []\n    with tf.variable_scope(\'feature_pyramid\', \'feature_pyramid\', values=end_points):\n        # top-down\n        for ind, pyramid in enumerate(reversed(end_points)):\n            inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv1_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(inputs, name=\'relu1_p{}\'.format(pyramid_len - ind))\n            if up_sampling is not None:\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name=\'trans_p{}\'.format(pyramid_len - ind))\n                up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name=\'upsample_p{}\'.format(pyramid_len - ind))\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name=\'trans_inv_p{}\'.format(pyramid_len - ind))\n                up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'up_conv_p{}\'.format(pyramid_len - ind))\n                up_sampling = lateral + up_sampling\n                lateral = up_sampling\n            else:\n                up_sampling = lateral\n\n            pyramid_laterals.append(lateral)\n\n            lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv2_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(lateral, name=\'relu2_p{}\'.format(pyramid_len - ind))\n\n            outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 2, 3, 1], name=\'output_trans_p{}\'.format(pyramid_len - ind))\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            pyramid_heatmaps.append(outputs)\n    with tf.variable_scope(\'global_net\', \'global_net\', values=pyramid_laterals):\n        global_pyramids = []\n        for ind, lateral in enumerate(pyramid_laterals):\n            inputs = lateral\n            for bottleneck_ind in range(pyramid_len - ind - 1):\n                inputs = global_net_bottleneck_block(inputs, 128, istraining, data_format, name=\'global_net_bottleneck_{}_p{}\'.format(bottleneck_ind, pyramid_len - ind))\n\n            #if ind < pyramid_len - 1:\n                # resize back to the output heatmap size\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(inputs, [0, 2, 3, 1], name=\'global_output_trans_p{}\'.format(pyramid_len - ind))\n            else:\n                outputs = inputs\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'global_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'global_heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            # else:\n            #     outputs = tf.identity(inputs, \'global_heatmap_p{}\'.format(pyramid_len - ind))\n\n            global_pyramids.append(outputs)\n\n        concat_pyramids = tf.concat(global_pyramids, 1 if data_format == \'channels_first\' else 3, name=\'concat\')\n\n        def projection_shortcut(inputs):\n            return conv2d_fixed_padding(inputs=inputs, filters=256, kernel_size=1, strides=1, data_format=data_format, name=\'shortcut\')\n\n        outputs = global_net_bottleneck_block(concat_pyramids, 128, istraining, data_format, projection_shortcut=projection_shortcut, name=\'global_concat_bottleneck\')\n        outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap\')\n\n\n    return pyramid_heatmaps + [outputs]\n\ndef xt_cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=50):\n    #with tf.variable_scope(\'resnet50\', \'resnet50\', values=[inputs]):\n    end_points = sext_cpn_backbone(inputs, istraining, data_format, net_depth=net_depth)\n    pyramid_len = len(end_points)\n    up_sampling = None\n    pyramid_heatmaps = []\n    pyramid_laterals = []\n    with tf.variable_scope(\'feature_pyramid\', \'feature_pyramid\', values=end_points):\n        # top-down\n        for ind, pyramid in enumerate(reversed(end_points)):\n            inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv1_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(inputs, name=\'relu1_p{}\'.format(pyramid_len - ind))\n            if up_sampling is not None:\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name=\'trans_p{}\'.format(pyramid_len - ind))\n                up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name=\'upsample_p{}\'.format(pyramid_len - ind))\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name=\'trans_inv_p{}\'.format(pyramid_len - ind))\n                up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'up_conv_p{}\'.format(pyramid_len - ind))\n                up_sampling = lateral + up_sampling\n                lateral = up_sampling\n            else:\n                up_sampling = lateral\n\n            pyramid_laterals.append(lateral)\n\n            lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv2_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(lateral, name=\'relu2_p{}\'.format(pyramid_len - ind))\n\n            outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 2, 3, 1], name=\'output_trans_p{}\'.format(pyramid_len - ind))\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            pyramid_heatmaps.append(outputs)\n    with tf.variable_scope(\'global_net\', \'global_net\', values=pyramid_laterals):\n        global_pyramids = []\n        for ind, lateral in enumerate(pyramid_laterals):\n            inputs = lateral\n            for bottleneck_ind in range(pyramid_len - ind - 1):\n                inputs = global_net_bottleneck_block(inputs, 128, istraining, data_format, name=\'global_net_bottleneck_{}_p{}\'.format(bottleneck_ind, pyramid_len - ind))\n\n            #if ind < pyramid_len - 1:\n                # resize back to the output heatmap size\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(inputs, [0, 2, 3, 1], name=\'global_output_trans_p{}\'.format(pyramid_len - ind))\n            else:\n                outputs = inputs\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'global_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'global_heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            # else:\n            #     outputs = tf.identity(inputs, \'global_heatmap_p{}\'.format(pyramid_len - ind))\n\n            global_pyramids.append(outputs)\n\n        concat_pyramids = tf.concat(global_pyramids, 1 if data_format == \'channels_first\' else 3, name=\'concat\')\n\n        def projection_shortcut(inputs):\n            return conv2d_fixed_padding(inputs=inputs, filters=256, kernel_size=1, strides=1, data_format=data_format, name=\'shortcut\')\n\n        outputs = global_net_bottleneck_block(concat_pyramids, 128, istraining, data_format, projection_shortcut=projection_shortcut, name=\'global_concat_bottleneck\')\n        outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap\')\n\n\n    return pyramid_heatmaps + [outputs]\n\ndef head_xt_cascaded_pyramid_net(inputs, output_channals, heatmap_size, istraining, data_format):\n    #with tf.variable_scope(\'resnet50\', \'resnet50\', values=[inputs]):\n    end_points = sext_cpn_backbone(inputs, istraining, data_format)\n    pyramid_len = len(end_points)\n    up_sampling = None\n    pyramid_heatmaps = []\n    pyramid_laterals = []\n    with tf.variable_scope(\'feature_pyramid\', \'feature_pyramid\', values=end_points):\n        # top-down\n        for ind, pyramid in enumerate(reversed(end_points)):\n            inputs = conv2d_fixed_padding(inputs=pyramid, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv1_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(inputs, name=\'relu1_p{}\'.format(pyramid_len - ind))\n            if up_sampling is not None:\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 2, 3, 1], name=\'trans_p{}\'.format(pyramid_len - ind))\n                up_sampling = tf.image.resize_bilinear(up_sampling, tf.shape(up_sampling)[-3:-1] * 2, name=\'upsample_p{}\'.format(pyramid_len - ind))\n                if data_format == \'channels_first\':\n                    up_sampling = tf.transpose(up_sampling, [0, 3, 1, 2], name=\'trans_inv_p{}\'.format(pyramid_len - ind))\n                up_sampling = conv2d_fixed_padding(inputs=up_sampling, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'up_conv_p{}\'.format(pyramid_len - ind))\n                up_sampling = lateral + up_sampling\n                lateral = up_sampling\n            else:\n                up_sampling = lateral\n\n            pyramid_laterals.append(lateral)\n\n            lateral = conv2d_fixed_padding(inputs=lateral, filters=256, kernel_size=1, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'1x1_conv2_p{}\'.format(pyramid_len - ind))\n            lateral = tf.nn.relu(lateral, name=\'relu2_p{}\'.format(pyramid_len - ind))\n\n            outputs = conv2d_fixed_padding(inputs=lateral, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 2, 3, 1], name=\'output_trans_p{}\'.format(pyramid_len - ind))\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            pyramid_heatmaps.append(outputs)\n    with tf.variable_scope(\'global_net\', \'global_net\', values=pyramid_laterals):\n        global_pyramids = []\n        for ind, lateral in enumerate(pyramid_laterals):\n            inputs = lateral\n            for bottleneck_ind in range(pyramid_len - ind - 1):\n                inputs = global_net_sext_bottleneck_block(inputs, 128, istraining, data_format, name_prefix=\'global_net_bottleneck_{}_p{}\'.format(bottleneck_ind, pyramid_len - ind))\n\n            #if ind < pyramid_len - 1:\n                # resize back to the output heatmap size\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(inputs, [0, 2, 3, 1], name=\'global_output_trans_p{}\'.format(pyramid_len - ind))\n            else:\n                outputs = inputs\n            outputs = tf.image.resize_bilinear(outputs, [heatmap_size, heatmap_size], name=\'global_heatmap_p{}\'.format(pyramid_len - ind))\n            if data_format == \'channels_first\':\n                outputs = tf.transpose(outputs, [0, 3, 1, 2], name=\'global_heatmap_trans_inv_p{}\'.format(pyramid_len - ind))\n            # else:\n            #     outputs = tf.identity(inputs, \'global_heatmap_p{}\'.format(pyramid_len - ind))\n\n            global_pyramids.append(outputs)\n\n        concat_pyramids = tf.concat(global_pyramids, 1 if data_format == \'channels_first\' else 3, name=\'concat\')\n\n        outputs = global_net_sext_bottleneck_block(concat_pyramids, 128, istraining, data_format, need_reduce=True, name_prefix=\'global_concat_bottleneck\')\n        outputs = conv2d_fixed_padding(inputs=outputs, filters=output_channals, kernel_size=3, strides=1,\n                          data_format=data_format, kernel_initializer=tf.glorot_uniform_initializer, name=\'conv_heatmap\')\n\n\n    return pyramid_heatmaps + [outputs]\n'"
net/simple_xt.py,119,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport math\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\n_USE_FUSED_BN = True\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format, name=None):\n    """"""Performs a batch normalization using a standard set of parameters.""""""\n    # We set fused=True for a significant performance boost. See\n    # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n        scale=True, training=training, name=name, fused=_USE_FUSED_BN)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    """"""Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                   Should be a positive integer.\n      data_format: The input format (\'channels_last\' or \'channels_first\').\n\n    Returns:\n      A tensor with the same format as the input with the data either intact\n      (if kernel_size == 1) or padded (if kernel_size > 1).\n    """"""\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == \'channels_first\':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                      [pad_beg, pad_end], [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                      [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format, kernel_initializer=tf.glorot_uniform_initializer, name=None):\n    """"""Strided 2-D convolution with explicit padding.""""""\n    # The padding is consistent and is based only on `kernel_size`, not on the\n    # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n                inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n                padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n                kernel_initializer=kernel_initializer(),\n                data_format=data_format, name=name)\n\n# input image order: BGR, range [0-255]\n# mean_value: 104, 117, 123\n# only subtract mean is used\ndef constant_xavier_initializer(shape, group, dtype=tf.float32, uniform=True):\n    """"""Initializer function.""""""\n    if not dtype.is_floating:\n      raise TypeError(\'Cannot create initializer for non-floating point type.\')\n    # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n    # This is the right thing for matrix multiply and convolutions.\n    if shape:\n      fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n      fan_out = float(shape[-1])/group\n    else:\n      fan_in = 1.0\n      fan_out = 1.0\n    for dim in shape[:-2]:\n      fan_in *= float(dim)\n      fan_out *= float(dim)\n\n    # Average number of inputs and output connections.\n    n = (fan_in + fan_out) / 2.0\n    if uniform:\n      # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n      limit = math.sqrt(3.0 * 1.0 / n)\n      return tf.random_uniform(shape, -limit, limit, dtype, seed=None)\n    else:\n      # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n      trunc_stddev = math.sqrt(1.3 * 1.0 / n)\n      return tf.truncated_normal(shape, 0.0, trunc_stddev, dtype, seed=None)\n\ndef wrapper_initlizer(shape, dtype=None, partition_info=None):\n    return constant_xavier_initializer(shape, 32, dtype)\n# for root block, use dummy input_filters, e.g. 128 rather than 64 for the first block\ndef se_next_bottleneck_block(inputs, input_filters, name_prefix, is_training, group, data_format=\'channels_last\', need_reduce=True, is_root=False, reduced_scale=16):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    strides_to_use = 1\n    residuals = inputs\n    if need_reduce:\n        strides_to_use = 1 if is_root else 2\n        #print(strides_to_use)\n        proj_mapping = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_proj\', strides=(strides_to_use, strides_to_use),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        # print(proj_mapping)\n        residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    #print(strides_to_use)\n    reduced_inputs = tf.layers.conv2d(inputs, input_filters // 2, (1, 1), use_bias=False,\n                            name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                            padding=\'valid\', data_format=data_format, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer())\n    reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n    if data_format == \'channels_first\':\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n    else:\n        reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n    convolved = [tf.nn.convolution(x, weight, padding=\'VALID\', strides=[strides_to_use, strides_to_use], name=name_prefix + \'_group_conv\',\n                    data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n    if data_format == \'channels_first\':\n        conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n    else:\n        conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n    conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n    increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n    down_inputs = tf.layers.conv2d(pooled_inputs, input_filters // reduced_scale, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n    up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n    rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n    pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n    return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n\ndef dilated_se_next_bottleneck_block(inputs, input_filters, name_prefix, is_training, group, data_format=\'channels_last\', need_reduce=True, reduced_scale=16):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    residuals = inputs\n    if need_reduce:\n        proj_mapping = tf.layers.conv2d(inputs, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_proj\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n        # print(proj_mapping)\n        residuals = tf.layers.batch_normalization(proj_mapping, momentum=_BATCH_NORM_DECAY,\n                                name=name_prefix + \'_1x1_proj/bn\', axis=bn_axis,\n                                epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    #print(strides_to_use)\n    reduced_inputs = tf.layers.conv2d(inputs, input_filters // 2, (1, 1), use_bias=False,\n                            name=name_prefix + \'_1x1_reduce\', strides=(1, 1),\n                            padding=\'valid\', data_format=data_format, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer())\n    reduced_inputs_bn = tf.layers.batch_normalization(reduced_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_reduce/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    reduced_inputs_relu = tf.nn.relu(reduced_inputs_bn, name=name_prefix + \'_1x1_reduce/relu\')\n\n    if data_format == \'channels_first\':\n        #reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [0, 0], [1, 1], [1, 1]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=1, name=name_prefix + \'_inputs_split\')\n    else:\n        #reduced_inputs_relu = tf.pad(reduced_inputs_relu, paddings = [[0, 0], [1, 1], [1, 1], [0, 0]])\n        weight_shape = [3, 3, reduced_inputs_relu.get_shape().as_list()[-1]//group, input_filters // 2]\n        if is_training:\n            weight_ = tf.Variable(constant_xavier_initializer(weight_shape, group=group, dtype=tf.float32), trainable=is_training, name=name_prefix + \'_3x3/kernel\')\n        else:\n            weight_ = tf.get_variable(name_prefix + \'_3x3/kernel\', shape=weight_shape, initializer=wrapper_initlizer, trainable=is_training)\n        weight_groups = tf.split(weight_, num_or_size_splits=group, axis=-1, name=name_prefix + \'_weight_split\')\n        xs = tf.split(reduced_inputs_relu, num_or_size_splits=group, axis=-1, name=name_prefix + \'_inputs_split\')\n\n    # !!! before is VALID !!!\n    convolved = [tf.nn.convolution(x, weight, padding=\'SAME\', strides=[1, 1], dilation_rate=[2, 2], name=name_prefix + \'_group_conv\',\n                    data_format=(\'NCHW\' if data_format == \'channels_first\' else \'NHWC\')) for (x, weight) in zip(xs, weight_groups)]\n\n    if data_format == \'channels_first\':\n        conv3_inputs = tf.concat(convolved, axis=1, name=name_prefix + \'_concat\')\n    else:\n        conv3_inputs = tf.concat(convolved, axis=-1, name=name_prefix + \'_concat\')\n\n    conv3_inputs_bn = tf.layers.batch_normalization(conv3_inputs, momentum=_BATCH_NORM_DECAY, name=name_prefix + \'_3x3/bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n    conv3_inputs_relu = tf.nn.relu(conv3_inputs_bn, name=name_prefix + \'_3x3/relu\')\n\n\n    increase_inputs = tf.layers.conv2d(conv3_inputs_relu, input_filters, (1, 1), use_bias=False,\n                                name=name_prefix + \'_1x1_increase\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    increase_inputs_bn = tf.layers.batch_normalization(increase_inputs, momentum=_BATCH_NORM_DECAY,\n                                        name=name_prefix + \'_1x1_increase/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=is_training, reuse=None, fused=_USE_FUSED_BN)\n\n    if data_format == \'channels_first\':\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [2, 3], name=name_prefix + \'_global_pool\', keep_dims=True)\n    else:\n        pooled_inputs = tf.reduce_mean(increase_inputs_bn, [1, 2], name=name_prefix + \'_global_pool\', keep_dims=True)\n\n    down_inputs = tf.layers.conv2d(pooled_inputs, input_filters // reduced_scale, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_down\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    down_inputs_relu = tf.nn.relu(down_inputs, name=name_prefix + \'_1x1_down/relu\')\n\n\n    up_inputs = tf.layers.conv2d(down_inputs_relu, input_filters, (1, 1), use_bias=True,\n                                name=name_prefix + \'_1x1_up\', strides=(1, 1),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    prob_outputs = tf.nn.sigmoid(up_inputs, name=name_prefix + \'_prob\')\n\n    rescaled_feat = tf.multiply(prob_outputs, increase_inputs_bn, name=name_prefix + \'_mul\')\n    pre_act = tf.add(residuals, rescaled_feat, name=name_prefix + \'_add\')\n    return tf.nn.relu(pre_act, name=name_prefix + \'/relu\')\n\n# the input image should in BGR order, note that this is not the common case in Tensorflow\ndef sext_backbone(input_image, istraining, data_format, net_depth=101, group=32):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n\n    if data_format == \'channels_last\':\n        image_channels = tf.unstack(input_image, axis=-1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=-1)\n    else:\n        image_channels = tf.unstack(input_image, axis=1)\n        swaped_input_image = tf.stack([image_channels[2], image_channels[1], image_channels[0]], axis=1)\n    #swaped_input_image = input_image\n\n    if net_depth not in [50, 101]:\n        raise TypeError(\'Only ResNeXt50 or ResNeXt101 is supprted now.\')\n\n    input_depth = [256, 512, 1024] # the input depth of the the first block is dummy input\n    num_units = [3, 4, 6] if net_depth==50 else [3, 4, 23]\n    block_name_prefix = [\'conv2_{}\', \'conv3_{}\', \'conv4_{}\']\n\n    if data_format == \'channels_first\':\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [0, 0], [3, 3], [3, 3]])\n    else:\n        swaped_input_image = tf.pad(swaped_input_image, paddings = [[0, 0], [3, 3], [3, 3], [0, 0]])\n\n    inputs_features = tf.layers.conv2d(swaped_input_image, input_depth[0]//4, (7, 7), use_bias=False,\n                                name=\'conv1/7x7_s2\', strides=(2, 2),\n                                padding=\'valid\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    #print(ee)\n\n    inputs_features = tf.layers.batch_normalization(inputs_features, momentum=_BATCH_NORM_DECAY,\n                                        name=\'conv1/7x7_s2/bn\', axis=bn_axis,\n                                        epsilon=_BATCH_NORM_EPSILON, training=istraining, reuse=None, fused=_USE_FUSED_BN)\n    inputs_features = tf.nn.relu(inputs_features, name=\'conv1/relu_7x7_s2\')\n\n    inputs_features = tf.layers.max_pooling2d(inputs_features, [3, 3], [2, 2], padding=\'same\', data_format=data_format, name=\'pool1/3x3_s2\')\n\n    end_points = []\n    is_root = True\n    for ind, num_unit in enumerate(num_units):\n        need_reduce = True\n        for unit_index in range(1, num_unit+1):\n            inputs_features = se_next_bottleneck_block(inputs_features, input_depth[ind], block_name_prefix[ind].format(unit_index), is_training=istraining, group=group, data_format=data_format, need_reduce=need_reduce, is_root=is_root)\n            need_reduce = False\n            is_root = False\n        end_points.append(inputs_features)\n\n    # conv5\n    need_reduce = True\n    for unit_index in range(1, 4):\n        inputs_features = dilated_se_next_bottleneck_block(inputs_features, 2048, \'conv5_{}\'.format(unit_index), is_training=istraining, group=group, data_format=data_format, need_reduce=need_reduce)\n        need_reduce = False\n    end_points.append(inputs_features)\n\n    #print(inputs)\n    return end_points\n\n\ndef simple_net(inputs, output_channals, heatmap_size, istraining, data_format, net_depth=101):\n    end_points = sext_backbone(inputs, istraining, data_format, net_depth=net_depth)\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    with tf.variable_scope(\'additional_layer\', \'additional_layer\', values=end_points, reuse=None):\n        inputs_features = tf.layers.conv2d_transpose(end_points[-1], 256, 4, strides=(2, 2), padding=\'same\',\n                            data_format=data_format, activation=None, use_bias=False, kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=None,\n                            kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n                            kernel_constraint=None, bias_constraint=None,\n                            trainable=istraining, name=\'deconv_1\', reuse=None)\n        inputs_features = tf.layers.batch_normalization(inputs_features, momentum=_BATCH_NORM_DECAY, name=\'deconv_1_bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=istraining, reuse=None, fused=_USE_FUSED_BN)\n        inputs_features = tf.nn.relu(inputs_features, name=\'deconv_1_relu\')\n\n        inputs_features = tf.layers.conv2d_transpose(inputs_features, 256, 4, strides=(2, 2), padding=\'same\',\n                            data_format=data_format, activation=None, use_bias=False, kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=None,\n                            kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n                            kernel_constraint=None, bias_constraint=None,\n                            trainable=istraining, name=\'deconv_2\', reuse=None)\n        inputs_features = tf.layers.batch_normalization(inputs_features, momentum=_BATCH_NORM_DECAY, name=\'deconv_2_bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=istraining, reuse=None, fused=_USE_FUSED_BN)\n        inputs_features = tf.nn.relu(inputs_features, name=\'deconv_2_relu\')\n\n        inputs_features = tf.layers.conv2d_transpose(inputs_features, 256, 4, strides=(2, 2), padding=\'same\',\n                            data_format=data_format, activation=None, use_bias=False, kernel_initializer=tf.contrib.layers.xavier_initializer(), bias_initializer=None,\n                            kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n                            kernel_constraint=None, bias_constraint=None,\n                            trainable=istraining, name=\'deconv_3\', reuse=None)\n        inputs_features = tf.layers.batch_normalization(inputs_features, momentum=_BATCH_NORM_DECAY, name=\'deconv_3_bn\',\n                                        axis=bn_axis, epsilon=_BATCH_NORM_EPSILON, training=istraining, reuse=None, fused=_USE_FUSED_BN)\n        inputs_features = tf.nn.relu(inputs_features, name=\'deconv_3_relu\')\n\n        heatmap = tf.layers.conv2d(inputs=inputs_features, filters=output_channals, kernel_size=1,\n                            strides=1, padding=\'same\', use_bias=True, activation=None,\n                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                            bias_initializer=tf.zeros_initializer(),\n                            data_format=data_format,\n                            name=\'heatmap_1x1\')\n\n        return [heatmap]\n'"
preprocessing/dataset.py,44,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport os\n\nimport sys; sys.path.insert(0, ""."")\nimport config\n\nslim = tf.contrib.slim\n\n# blouse_0000.tfrecord\n# {}_????_val.tfrecord\n#category = *\ndef slim_get_split(dataset_dir, image_preprocessing_fn, batch_size, num_readers, num_preprocessing_threads, num_epochs=None, is_training=True, category=\'blouse\', file_pattern=\'{}_????\', reader=None, return_keypoints=False):\n    # Allowing None in the signature so that dataset_factory can use the default.\n    if reader is None:\n        reader = tf.TFRecordReader\n\n    num_joints = config.class_num_joints[category]\n\n    suffix = \'.tfrecord\' if is_training else \'_val.tfrecord\'\n    file_pattern = file_pattern.format(category) + suffix\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/filename\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/classid\': tf.FixedLenFeature([1], tf.int64),\n        \'image/keypoint/x\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/keypoint/y\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/keypoint/v\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/keypoint/id\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/keypoint/gid\': tf.VarLenFeature(dtype=tf.int64),\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'height\': slim.tfexample_decoder.Tensor(\'image/height\'),\n        \'width\': slim.tfexample_decoder.Tensor(\'image/width\'),\n        \'channels\': slim.tfexample_decoder.Tensor(\'image/channels\'),\n        \'classid\': slim.tfexample_decoder.Tensor(\'image/classid\'),\n        \'keypoint/x\': slim.tfexample_decoder.Tensor(\'image/keypoint/x\'),\n        \'keypoint/y\': slim.tfexample_decoder.Tensor(\'image/keypoint/y\'),\n        \'keypoint/v\': slim.tfexample_decoder.Tensor(\'image/keypoint/v\'),\n        \'keypoint/id\': slim.tfexample_decoder.Tensor(\'image/keypoint/id\'),\n        \'keypoint/gid\': slim.tfexample_decoder.Tensor(\'image/keypoint/gid\'),\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n    input_source = os.path.join(dataset_dir, file_pattern)\n    dataset = slim.dataset.Dataset(\n                data_sources=input_source,\n                reader=reader,\n                decoder=decoder,\n                num_samples=config.split_size[category][\'train\' if is_training else \'val\'],#dataset_inspect.count_split_examples(dataset_dir, file_prefix=\'sacw_\'),\n                items_to_descriptions=None,\n                num_classes=num_joints,\n                labels_to_names=None)\n\n    with tf.name_scope(\'dataset_data_provider\'):\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n                                                        dataset,\n                                                        num_readers=num_readers,\n                                                        common_queue_capacity=32 * batch_size,\n                                                        common_queue_min=8 * batch_size,\n                                                        shuffle=True,\n                                                        num_epochs=num_epochs)\n\n    [org_image, height, width, channels, classid, key_x, key_y, key_v, key_id, key_gid] = provider.get([\'image\', \'height\',\n                                                                                            \'width\', \'channels\',\n                                                                                            \'classid\', \'keypoint/x\',\n                                                                                            \'keypoint/y\', \'keypoint/v\',\n                                                                                            \'keypoint/id\', \'keypoint/gid\'])\n\n\n    gather_ind = config.class2global_ind_map[category]\n\n    key_x, key_y, key_v, key_id, key_gid = tf.gather(key_x, gather_ind), tf.gather(key_y, gather_ind), tf.gather(key_v, gather_ind), tf.gather(key_id, gather_ind), tf.gather(key_gid, gather_ind)\n\n    shape = tf.stack([height, width, channels], axis=0)\n\n    if not return_keypoints:\n        image, targets, new_key_v, isvalid, norm_value = image_preprocessing_fn(org_image, classid, shape, key_x, key_y, key_v)\n        batch_list = [image, shape, classid, targets, new_key_v, isvalid, norm_value]\n    else:\n        image, targets, new_key_x, new_key_y, new_key_v, isvalid, norm_value = image_preprocessing_fn(org_image, classid, shape, key_x, key_y, key_v)\n        batch_list = [image, shape, classid, targets, new_key_x, new_key_y, new_key_v, isvalid, norm_value]\n\n    batch_input = tf.train.batch(batch_list,\n                                #classid, key_x, key_y, key_v, key_id, key_gid],\n                                dynamic_pad=False,#(not is_training),\n                                batch_size = batch_size,\n                                allow_smaller_final_batch=True,\n                                num_threads = num_preprocessing_threads,\n                                capacity = 64 * batch_size)\n    return batch_input\n\n\ndef slim_test_get_split(dataset_dir, image_preprocessing_fn, num_readers, num_preprocessing_threads, category=\'blouse\', file_pattern=\'{}_*.tfrecord\', reader=None, dynamic_pad=False):\n    # Allowing None in the signature so that dataset_factory can use the default.\n    if reader is None:\n        reader = tf.TFRecordReader\n\n    num_joints = config.class_num_joints[category]\n    file_pattern = file_pattern.format(category)\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/filename\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/classid\': tf.FixedLenFeature([1], tf.int64)\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'height\': slim.tfexample_decoder.Tensor(\'image/height\'),\n        \'width\': slim.tfexample_decoder.Tensor(\'image/width\'),\n        \'channels\': slim.tfexample_decoder.Tensor(\'image/channels\'),\n        \'classid\': slim.tfexample_decoder.Tensor(\'image/classid\'),\n        \'filename\': slim.tfexample_decoder.Tensor(\'image/filename\')\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n\n    input_source = os.path.join(dataset_dir, file_pattern)\n    #print(config.split_size[category][\'test\'])\n    dataset = slim.dataset.Dataset(\n                data_sources=input_source,\n                reader=reader,\n                decoder=decoder,\n                num_samples=config.split_size[category][\'test\'],#dataset_inspect.count_split_examples(dataset_dir, file_prefix=\'sacw_\'),\n                items_to_descriptions=None,\n                num_classes=num_joints,\n                labels_to_names=None)\n\n    with tf.name_scope(\'dataset_data_provider\'):\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n                                                        dataset,\n                                                        num_readers=num_readers,\n                                                        common_queue_capacity=32,\n                                                        common_queue_min=8,\n                                                        shuffle=False,\n                                                        num_epochs=1)\n\n    [org_image, height, width, channels, classid, filename] = provider.get([\'image\', \'height\', \'width\', \'channels\', \'classid\', \'filename\'])\n\n    shape = tf.stack([height, width, channels], axis=0)\n    if image_preprocessing_fn is not None:\n        image, shape, offsets = image_preprocessing_fn(org_image, filename, shape)\n    else:\n        image = org_image\n        offsets = tf.constant([0, 0], tf.int64)\n\n    batch_input = tf.train.batch([image, shape, filename, classid, offsets],\n                                dynamic_pad = dynamic_pad,\n                                batch_size = 1,\n                                allow_smaller_final_batch=True,\n                                num_threads = num_preprocessing_threads,\n                                capacity = 64)\n    return batch_input\nif __name__ == \'__main__\':\n    import preprocessing\n\n    category=\'skirt\'\n    if \'*\' in category:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.global_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.global_norm_rvalues, dtype=tf.int64)), 1)\n    else:\n        lnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_lvalues, dtype=tf.int64)), 0)\n        rnorm_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(tf.constant(config.local_norm_key, dtype=tf.int64),\n                                                                tf.constant(config.local_norm_rvalues, dtype=tf.int64)), 1)\n    preprocessing_fn = lambda org_image, classid, shape, key_x, key_y, key_v: preprocessing.preprocess_image(org_image, classid, shape, 256, 256, key_x, key_y, key_v, (lnorm_table, rnorm_table), is_training=True, category=category)\n    #[\'blouse\', \'dress\', \'outwear\', \'skirt\', \'trousers\']\n    batch_input = slim_get_split(config.RECORDS_DATA_DIR, preprocessing_fn, 1, 2, 4, num_epochs=None, is_training=True, file_pattern=\'{}_????\', category=category, reader=None)\n\n    #preprocessing_fn = lambda org_image, classid, shape: preprocessing.preprocess_for_test(org_image, classid, shape, 256, 256)\n    #batch_input = slim_test_get_split(config.TEST_RECORDS_DATA_DIR, preprocessing_fn, 2, 4)\n    # Create the graph, etc.\n    init_op = tf.group([tf.local_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer()])\n\n    # Create a session for running operations in the Graph.\n    sess = tf.Session()\n    rotate_matrix = tf.contrib.image.angles_to_projective_transforms(1., 128,128)\n    # Initialize the variables (like the epoch counter).\n    sess.run(init_op)\n\n    # Start input enqueue threads.\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    try:\n        while not coord.should_stop():\n            # Run training steps or whatever\n            print(sess.run(batch_input)[-3:])\n            #print(sess.run(rotate_matrix))\n            #print(sess.run(batch_input)[-2][0].decode(\'utf8\'))\n\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # When done, ask the threads to stop.\n        coord.request_stop()\n\n    # Wait for threads to finish.\n    coord.join(threads)\n    sess.close()\n'"
preprocessing/dataset_inspect.py,2,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport os\n\nimport tensorflow as tf\nimport numpy as np\n\nimport config\n\ndef count_split_examples(split_path, category=\'tfrecord\', file_prefix=\'.tfrecord\'):\n    # Count the total number of examples in all of these shard\n    num_samples = 0\n    tfrecords_to_count = [os.path.join(split_path, file) for file in os.listdir(split_path) if file_prefix in file]\n    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    for tfrecord_file in tfrecords_to_count:\n        if category not in tfrecord_file: continue\n        for record in tf.python_io.tf_record_iterator(tfrecord_file):#, options = opts):\n            num_samples += 1\n    return num_samples\n\nif __name__ == \'__main__\':\n    # print(count_split_examples(config.RECORDS_DATA_DIR, \'*_val.tfrecord\'))\n    # print(count_split_examples(config.RECORDS_DATA_DIR, \'_????.tfrecord\'))\n    # print(count_split_examples(config.TEST_RECORDS_DATA_DIR, \'*.tfrecord\'))\n\n    print(count_split_examples(config.RECORDS_DATA_DIR, \'tfrecord\', \'val.tfrecord\'))\n    print(count_split_examples(config.RECORDS_DATA_DIR, \'tfrecord\', \'.tfrecord\') - count_split_examples(config.RECORDS_DATA_DIR, \'tfrecord\', \'val.tfrecord\'))\n    print(count_split_examples(config.TEST_RECORDS_DATA_DIR, \'tfrecord\', \'.tfrecord\'))\n    for cat in config.CATEGORIES:\n        print(\'count category: \', cat)\n        print(count_split_examples(config.RECORDS_DATA_DIR, cat, \'val.tfrecord\'))\n        print(count_split_examples(config.RECORDS_DATA_DIR, cat, \'.tfrecord\') - count_split_examples(config.RECORDS_DATA_DIR, cat, \'val.tfrecord\'))\n        print(count_split_examples(config.TEST_RECORDS_DATA_DIR, cat, \'.tfrecord\'))\n'"
preprocessing/get_dataset_mean_std.py,1,"b'import os\nfrom scipy.misc import imread, imsave, imshow, imresize\n\nimport numpy as np\nimport tensorflow as tf\n\nimport sys; sys.path.insert(0, ""."")\nimport config\n\ndef get_dataset_mean_std():\n    all_sub_dirs = []\n    for split in config.SPLITS:\n        if \'test\' not in split:\n            for cat in config.CATEGORIES:\n                all_sub_dirs.append(os.path.join(config.DATA_DIR, split, \'Images\', cat))\n    all_image_nums = 0\n    #print(all_sub_dirs)\n    means = [0., 0., 0.]\n    stds = [0., 0., 0.]\n    for dirs in all_sub_dirs:\n        all_images = tf.gfile.Glob(os.path.join(dirs, \'*.jpg\'))\n        for image in all_images:\n            np_image = imread(image, mode=\'RGB\')\n            if len(np_image.shape) < 3 or np_image.shape[-1] != 3:\n                continue\n            all_image_nums += 1\n\n            means[0] += np.mean(np_image[:, :, 0]) / 10000.\n            means[1] += np.mean(np_image[:, :, 1]) / 10000.\n            means[2] += np.mean(np_image[:, :, 2]) / 10000.\n\n            stds[0] += np.std(np_image[:, :, 0]) / 10000.\n            stds[1] += np.std(np_image[:, :, 1]) / 10000.\n            stds[2] += np.std(np_image[:, :, 2]) / 10000.\n\n        print([_*10000./all_image_nums for _ in means])\n        print([_*10000./all_image_nums for _ in stds])\n    print([_*10000./all_image_nums for _ in means])\n    print([_*10000./all_image_nums for _ in stds])\n    print(all_image_nums)\n\nget_dataset_mean_std()\n# [171.04052664596992, 162.98214001911154, 159.88648003318914]\n# [62.370313796103616, 64.64434475667025, 64.35966787914904]\n'"
preprocessing/imagenet_preprocessing.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nTraining images are sampled using the provided bounding boxes, and subsequently\ncropped to the sampled bounding box. Images are additionally flipped randomly,\nthen resized to the target output size (without aspect-ratio preservation).\n\nImages used during evaluation are resized (with aspect-ratio preservation) and\ncentrally cropped.\n\nAll images undergo mean color subtraction.\n\nNote that these steps are colloquially referred to as ""ResNet preprocessing,""\nand they differ from ""VGG preprocessing,"" which does not use bounding boxes\nand instead does an aspect-preserving resize followed by random crop during\ntraining. (These both differ from ""Inception preprocessing,"" which introduces\ncolor distortion steps.)\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n_CHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\n\n\ndef _decode_crop_and_flip(image_buffer, bbox, num_channels):\n  """"""Crops the given image to a random part of the image, and randomly flips.\n\n  We use the fused decode_and_crop op, which performs better than the two ops\n  used separately in series, but note that this requires that the image be\n  passed in as an un-decoded string Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    num_channels: Integer depth of the image buffer for decoding.\n\n  Returns:\n    3-D tensor with cropped image.\n\n  """"""\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      tf.image.extract_jpeg_shape(image_buffer),\n      bounding_boxes=bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=[0.75, 1.33],\n      area_range=[0.05, 1.0],\n      max_attempts=100,\n      use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n      image_buffer, crop_window, channels=num_channels)\n\n  # Flip to add a little more random distortion in.\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped\n\n\ndef _central_crop(image, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image: a 3-D image tensor\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  amount_to_be_cropped_h = (height - crop_height)\n  crop_top = amount_to_be_cropped_h // 2\n  amount_to_be_cropped_w = (width - crop_width)\n  crop_left = amount_to_be_cropped_w // 2\n  return tf.slice(\n      image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef _mean_image_subtraction(image, means, num_channels):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  means = tf.expand_dims(tf.expand_dims(means, 0), 0)\n\n  return image - means\n\n\ndef _smallest_size_at_least(height, width, resize_min):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: an int32 scalar tensor indicating the new width.\n  """"""\n  resize_min = tf.cast(resize_min, tf.float32)\n\n  # Convert to floats to make subsequent calculations go smoothly.\n  height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32)\n\n  smaller_dim = tf.minimum(height, width)\n  scale_ratio = resize_min / smaller_dim\n\n  # Convert back to ints to make heights and widths that TF ops will accept.\n  new_height = tf.cast(height * scale_ratio, tf.int32)\n  new_width = tf.cast(width * scale_ratio, tf.int32)\n\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, resize_min):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)\n\n\ndef _resize_image(image, height, width):\n  """"""Simple wrapper around tf.resize_images.\n\n  This is primarily to make sure we use the same `ResizeMethod` and other\n  details each time.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    height: The target height for the resized image.\n    width: The target width for the resized image.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image. The first two\n      dimensions have the shape [height, width].\n  """"""\n  return tf.image.resize_images(\n      image, [height, width], method=tf.image.ResizeMethod.BILINEAR,\n      align_corners=False)\n\n\ndef preprocess_image(image_buffer, bbox, output_height, output_width,\n                     num_channels, is_training=False):\n  """"""Preprocesses the given image.\n\n  Preprocessing includes decoding, cropping, and resizing for both training\n  and eval images. Training preprocessing, however, introduces some random\n  distortion of the image to improve accuracy.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    num_channels: Integer depth of the image buffer for decoding.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    # For training, we want to randomize some of the distortions.\n    image = _decode_crop_and_flip(image_buffer, bbox, num_channels)\n    image = _resize_image(image, output_height, output_width)\n  else:\n    # For validation, we want to decode, resize, then just crop the middle.\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels)\n    image = _aspect_preserving_resize(image, _RESIZE_MIN)\n    image = _central_crop(image, output_height, output_width)\n\n  image.set_shape([output_height, output_width, num_channels])\n\n  return _mean_image_subtraction(image, _CHANNEL_MEANS, num_channels)\n'"
preprocessing/preprocessing.py,300,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\nimport math\nimport os\n\nimport config\n\nslim = tf.contrib.slim\n\n# _R_MEAN = 123.68\n# _G_MEAN = 116.78\n# _B_MEAN = 103.94\n_R_MEAN = 171.04\n_G_MEAN = 162.98\n_B_MEAN = 159.89\n# std: 62.37, 64.64, 64.36\n\n# [171.04052664596992, 162.98214001911154, 159.88648003318914]\n# [62.370313796103616, 64.64434475667025, 64.35966787914904]\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\ndef distort_color_v0(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 255.0)\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\ndef unwhiten_image(image):\n  means=[_R_MEAN, _G_MEAN, _B_MEAN]\n  num_channels = image.get_shape().as_list()[-1]\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] += means[i]\n  return tf.concat(axis=2, values=channels)\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(tf.rint(height * scale))\n  new_width = tf.to_int32(tf.rint(width * scale))\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=1.0,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.45, 1.0),#(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\nif config.DEBUG:\n  from scipy.misc import imread, imsave, imshow, imresize\n  def save_image_with_heatmap(image, heatmap, indR, indG, indB, shape, heatmap_size):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist())\n      #print(img_to_save)\n\n      #mean = [_R_MEAN, _G_MEAN, _B_MEAN]\n      #img_to_save += np.array(mean, dtype=img_to_save.dtype)\n      #print(img_to_save.shape, heatmap.shape)\n\n      img_to_save = img_to_save.astype(np.uint8)\n      file_name = \'raw_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save)\n      #print(heatmap.shape)\n      heatmap_all = (np.sum(heatmap, axis=0) * 255.).astype(np.uint8)\n      file_name = \'heatmap_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), heatmap_all)\n\n      # heatmap0 = (np.sum(heatmap[np.arange(0, np.shape(heatmap)[0], 2), ...], axis=0) * 255.).astype(np.uint8)\n      # heatmap1 = (np.sum(heatmap[np.arange(1, np.shape(heatmap)[0], 2), ...], axis=0) * 255.).astype(np.uint8)\n\n      heatmap0 = (np.sum(heatmap[indR, ...], axis=0) * 255.).astype(np.uint8)\n      heatmap1 = (np.sum(heatmap[indG, ...], axis=0) * 255.).astype(np.uint8)\n      heatmap2 = (np.sum(heatmap[indB, ...], axis=0) * 255.).astype(np.uint8) if len(indB) > 0 else np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n\n      heatmap0 = imresize(heatmap0, shape, interp=\'lanczos\')\n      heatmap1 = imresize(heatmap1, shape, interp=\'lanczos\')\n      heatmap2 = imresize(heatmap2, shape, interp=\'lanczos\')\n\n      img_to_save = img_to_save/2.\n      img_to_save[:,:,0] = np.clip((img_to_save[:,:,0] + heatmap0 + heatmap2), 0, 255)\n      img_to_save[:,:,1] = np.clip((img_to_save[:,:,1] + heatmap1 + heatmap2), 0, 255)\n      #img_to_save[:,:,2] = np.clip((img_to_save[:,:,2]/4. + heatmap2), 0, 255)\n      file_name = \'with_heatmap_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save.astype(np.uint8))\n      # for num_pt in range(heatmap.shape[0]):\n      #   heatmap_to_save = np.array(heatmap[num_pt]) * 255.\n      #   heatmap_to_save = heatmap_to_save.astype(np.uint8)\n      #   file_name = \'{}_{}.jpg\'.format(save_image_with_heatmap.counter, num_pt)\n      #   imsave(os.path.join(config.DEBUG_DIR, file_name), heatmap_to_save)\n      return save_image_with_heatmap.counter\n  def _save_image(image):\n      if not hasattr(save_image_with_heatmap, ""counter""):\n          save_image_with_heatmap.counter = 0  # it doesn\'t exist yet, so initialize it\n      save_image_with_heatmap.counter += 1\n\n      img_to_save = np.array(image.tolist())\n\n      img_to_save = img_to_save.astype(np.uint8)\n      file_name = \'raw_{}.jpg\'.format(save_image_with_heatmap.counter)\n      imsave(os.path.join(config.DEBUG_DIR, file_name), img_to_save)\n\n      return save_image_with_heatmap.counter\n\ndef np_draw_labelmap(pt, heatmap_sigma, heatmap_size, type=\'Gaussian\'):\n    # Draw a 2D gaussian\n    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n    img = np.zeros((heatmap_size, heatmap_size), dtype=np.float32)\n    if pt[0] < 1 or pt[1] < 1:\n        return (img, 0)\n\n    # Check that any part of the gaussian is in-bounds\n    ul = [int(pt[0] - 3 * heatmap_sigma), int(pt[1] - 3 * heatmap_sigma)]\n    br = [int(pt[0] + 3 * heatmap_sigma + 1), int(pt[1] + 3 * heatmap_sigma + 1)]\n    if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or\n            br[0] < 0 or br[1] < 0):\n        # If not, just return the image as is\n        return (img, 0)\n\n    # Generate gaussian\n    size = 6 * heatmap_sigma + 1\n    x = np.arange(0, size, 1, float)\n    y = x[:, np.newaxis]\n    x0 = y0 = size // 2\n    # The gaussian is not normalized, we want the center value to equal 1\n    if type == \'Gaussian\':\n        g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * heatmap_sigma ** 2))\n    elif type == \'Cauchy\':\n        g = heatmap_sigma / (((x - x0) ** 2 + (y - y0) ** 2 + heatmap_sigma ** 2) ** 1.5)\n\n\n    # Usable gaussian range\n    g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n    g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n    # Image range\n    img_x = max(0, ul[0]), min(br[0], img.shape[1])\n    img_y = max(0, ul[1]), min(br[1], img.shape[0])\n\n    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n    return (img, 1)\n\ndef draw_labelmap(x, y, heatmap_sigma, heatmap_size):\n  heatmap, isvalid = tf.map_fn(lambda pt : tf.py_func(np_draw_labelmap, [pt, heatmap_sigma, heatmap_size], [tf.float32, tf.int64], stateful=True),\n                    tf.stack([x, y], axis=-1),\n                    dtype=[tf.float32, tf.int64], parallel_iterations=10,\n                    back_prop=False, swap_memory=False, infer_shape=True)\n  heatmap.set_shape([x.get_shape().as_list()[0], heatmap_size, heatmap_size])\n  isvalid.set_shape([x.get_shape().as_list()[0]])\n  return heatmap, isvalid\n\n# def get_suitable_scale(angles, image_height, image_width, x, y):\n#   rotate_matrix = tf.contrib.image.angles_to_projective_transforms(angles, image_height, image_width)\n\n#   flaten_rotate_matrix = tf.squeeze(rotate_matrix)\n#   a0, a1, a2, b0, b1, b2 = flaten_rotate_matrix[0], \\\n#                             flaten_rotate_matrix[1], \\\n#                             flaten_rotate_matrix[2], \\\n#                             flaten_rotate_matrix[3], \\\n#                             flaten_rotate_matrix[4], \\\n#                             flaten_rotate_matrix[5]\n\n#   normalizor = a1 * b0 - a0 * b1 + 1e-8\n\n#   new_x = -(b1 * x - a1 * y - b1 * a2 + a1 * b2)/normalizor\n#   new_y = (b0 * x - a0 * y - a2 * b0 + a0 * b2)/normalizor\n\n#   valid_x = tf.boolean_mask(new_x, x > 0.)\n#   valid_y = tf.boolean_mask(new_y, y > 0.)\n\n#   min_x = tf.reduce_min(valid_x, axis=-1)\n#   max_x = tf.reduce_max(valid_x, axis=-1)\n#   min_y = tf.reduce_min(valid_y, axis=-1)\n#   max_y = tf.reduce_max(valid_y, axis=-1)\n\n#   return tf.maximum(max_x - min_x, 0.)\n\ndef get_projective_transforms(angles, image_height, image_width, x, y, name=None):\n  """"""Returns projective transform(s) for the given angle(s).\n  Args:\n    angles: A scalar angle to rotate all images by, or (for batches of images)\n        a vector with an angle to rotate each image in the batch. The rank must\n        be statically known (the shape is not `TensorShape(None)`.\n    image_height: Height of the image(s) to be transformed.\n    image_width: Width of the image(s) to be transformed.\n  Returns:\n    A tensor of shape (num_images, 8). Projective transforms which can be given\n      to `tf.contrib.image.transform`.\n  """"""\n  with tf.name_scope(name, ""get_projective_transforms""):\n    angle_or_angles = tf.convert_to_tensor(angles, name=""angles"", dtype=tf.float32)\n    if len(angle_or_angles.get_shape()) == 0:  # pylint: disable=g-explicit-length-test\n      angles = angle_or_angles[None]\n    elif len(angle_or_angles.get_shape()) == 1:\n      angles = angle_or_angles\n    else:\n      raise TypeError(""Angles should have rank 0 or 1."")\n\n    valid_x = tf.boolean_mask(x, x > 0.)\n    valid_y = tf.boolean_mask(y, y > 0.)\n\n    min_x = tf.reduce_min(valid_x, axis=-1)\n    max_x = tf.reduce_max(valid_x, axis=-1)\n    min_y = tf.reduce_min(valid_y, axis=-1)\n    max_y = tf.reduce_max(valid_y, axis=-1)\n    center_x = (min_x + max_x)/2.\n    center_y = (min_y + max_y)/2.\n\n    # map the center of all keypoints to the center of the transformed image\n    x_offset = center_x - (tf.cos(angles) * image_width / 2. - tf.sin(angles) * image_height / 2.)\n    y_offset = center_y - (tf.sin(angles) * image_width / 2. + tf.cos(angles) * image_height / 2.)\n\n    # x_offset = ((image_width - 1) - (tf.cos(angles) *\n    #                                  (image_width - 1) - tf.sin(angles) *\n    #                                  (image_height - 1))) / 2.0\n    # y_offset = ((image_height - 1) - (tf.sin(angles) *\n    #                                   (image_width - 1) + tf.cos(angles) *\n    #                                   (image_height - 1))) / 2.0\n    num_angles = tf.shape(angles)[0]\n    return tf.concat(\n        values=[\n            tf.cos(angles)[:, None],\n            -tf.sin(angles)[:, None],\n            x_offset[:, None],\n            tf.sin(angles)[:, None],\n            tf.cos(angles)[:, None],\n            y_offset[:, None],\n            tf.zeros((num_angles, 2), tf.float32),\n        ],\n        axis=1)\n\n\n# single image only\ndef rotate_all(images, angles, x, y, interpolation=""NEAREST""):\n  """"""Rotate image(s) by the passed angle(s) in radians.\n  Args:\n    images: A tensor of shape (num_images, num_rows, num_columns, num_channels)\n       (NHWC), (num_rows, num_columns, num_channels) (HWC), or\n       (num_rows, num_columns) (HW).\n    angles: A scalar angle to rotate all images by, or (if images has rank 4)\n       a vector of length num_images, with an angle for each image in the batch.\n    interpolation: Interpolation mode. Supported values: ""NEAREST"", ""BILINEAR"".\n  Returns:\n    Image(s) with the same type and shape as `images`, rotated by the given\n    angle(s). Empty space due to the rotation will be filled with zeros.\n  Raises:\n    TypeError: If `image` is an invalid type.\n  """"""\n  image_or_images = tf.convert_to_tensor(images, name=""images"")\n  if len(image_or_images.get_shape()) == 2:\n    images = image_or_images[None, :, :, None]\n  elif len(image_or_images.get_shape()) == 3:\n    images = image_or_images[None, :, :, :]\n  elif len(image_or_images.get_shape()) == 4:\n    images = image_or_images\n  else:\n    raise TypeError(""Images should have rank between 2 and 4."")\n\n  image_height = tf.cast(tf.shape(images)[1], tf.float32)[None]\n  image_width = tf.cast(tf.shape(images)[2], tf.float32)[None]\n\n  rotate_matrix = get_projective_transforms(angles, image_height, image_width, x, y)\n\n  flaten_rotate_matrix = tf.squeeze(rotate_matrix)\n  a0, a1, a2, b0, b1, b2 = flaten_rotate_matrix[0], \\\n                            flaten_rotate_matrix[1], \\\n                            flaten_rotate_matrix[2], \\\n                            flaten_rotate_matrix[3], \\\n                            flaten_rotate_matrix[4], \\\n                            flaten_rotate_matrix[5]\n\n  normalizor = a1 * b0 - a0 * b1 + 1e-8\n\n  new_x = -(b1 * x - a1 * y - b1 * a2 + a1 * b2)/normalizor\n  new_y = (b0 * x - a0 * y - a2 * b0 + a0 * b2)/normalizor\n\n  #new_x, new_y = new_x/tf.cast(shape[1], tf.float32), new_y/tf.cast(shape[0], tf.float32)\n  output = tf.contrib.image.transform(images, rotate_matrix, interpolation=interpolation)\n  if len(image_or_images.get_shape()) == 2:\n    return output[0, :, :, 0], new_x, new_y\n  elif len(image_or_images.get_shape()) == 3:\n    return output[0, :, :, :], new_x, new_y\n  else:\n    return output, new_x, new_y\n\ndef rotate_augum(image, shape, fkey_x, fkey_y, bbox_border):\n  # only consider valid keypoint\n  x_mask = (fkey_x > 0.)\n  y_mask = (fkey_y > 0.)\n  # backup the input image, keypoint, recover when the transformed image contains no keypoint\n  bak_fkey_x, bak_fkey_y, bak_image = fkey_x/tf.cast(shape[1], tf.float32), fkey_y/tf.cast(shape[0], tf.float32), image\n  # do rotate for image and all point, and use these new point to crop later\n  # transform keypoint and image\n  image, fkey_x, fkey_y = tf.cond(tf.random_uniform([1], minval=0., maxval=1., dtype=tf.float32)[0] < 0.4, lambda: rotate_all(image, tf.random_uniform([1], minval=-3.14/6., maxval=3.14/6., dtype=tf.float32)[0], fkey_x, fkey_y), lambda: (image, fkey_x, fkey_y))\n  #image = tf.Print(image,[fkey_x, fkey_y])\n  # normalize keypoint coord\n  fkey_x, fkey_y = fkey_x/tf.cast(shape[1], tf.float32), fkey_y/tf.cast(shape[0], tf.float32)\n  # mask all invalid keypoints after rotate, get mask for range 0-1\n  x_mask_ = tf.logical_and(fkey_x > 0., fkey_x < 1.)\n  y_mask_ = tf.logical_and(fkey_y > 0., fkey_y < 1.)\n  # AND to get final valid mask\n  x_mask = tf.logical_and(x_mask, x_mask_)\n  y_mask = tf.logical_and(y_mask, y_mask_)\n  # make these point negtive\n  fkey_x = fkey_x * tf.cast(x_mask, tf.float32) + (tf.cast(x_mask, tf.float32) - 1.)\n  fkey_y = fkey_y * tf.cast(y_mask, tf.float32) + (tf.cast(y_mask, tf.float32) - 1.)\n  # no valid keypoint pair, then rollback\n  new_image, new_fkey_x, new_fkey_y = tf.cond(tf.count_nonzero(tf.logical_and(x_mask, y_mask)) > 0, lambda : (image, fkey_x, fkey_y), lambda : (bak_image, bak_fkey_x, bak_fkey_y))\n\n  valid_x = tf.boolean_mask(new_fkey_x, new_fkey_x > 0.)\n  valid_y = tf.boolean_mask(new_fkey_y, new_fkey_y > 0.)\n\n  # the region contains all keypoint\n  min_x = tf.maximum(tf.reduce_min(valid_x, axis=-1) - bbox_border / tf.cast(shape[0], tf.float32), 0.)\n  max_x = tf.minimum(tf.reduce_max(valid_x, axis=-1) + bbox_border / tf.cast(shape[0], tf.float32), 1.)\n  min_y = tf.maximum(tf.reduce_min(valid_y, axis=-1) - bbox_border / tf.cast(shape[1], tf.float32), 0.)\n  max_y = tf.minimum(tf.reduce_max(valid_y, axis=-1) + bbox_border / tf.cast(shape[1], tf.float32), 1.)\n\n  return new_image, new_fkey_x, new_fkey_y, tf.reshape(tf.stack([min_y, min_x, max_y, max_x], axis=-1), [1, 1, 4])\n\ndef preprocess_for_train(image,\n                         classid,\n                         shape,\n                         output_height,\n                         output_width,\n                         key_x, key_y, key_v, norm_table,\n                         data_format,\n                         category,\n                         bbox_border, heatmap_sigma, heatmap_size,\n                         return_keypoints=False,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX,\n                         fast_mode=False,\n                         scope=None,\n                         add_image_summaries=True):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'vgg_distort_image\', [image, output_height, output_width]):\n    orig_dtype = image.dtype\n    if orig_dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Randomly distort the colors. There are 1 or 4 ways to do it.\n    num_distort_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(image,\n                                              lambda x, ordering: distort_color(x, ordering, fast_mode),\n                                              num_cases=num_distort_cases)\n    distorted_image = tf.to_float(tf.image.convert_image_dtype(distorted_image, orig_dtype, saturate=True))\n    if add_image_summaries:\n      tf.summary.image(\'color_distorted_image\', tf.cast(tf.expand_dims(distorted_image, 0), tf.uint8))\n\n    normarlized_image = _mean_image_subtraction(distorted_image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n    fkey_x, fkey_y = tf.cast(key_x, tf.float32), tf.cast(key_y, tf.float32)\n    #print(fkey_x, fkey_y)\n    # rotate transform, with bbox contains the clothes region\n    image, fkey_x, fkey_y, bbox = rotate_augum(normarlized_image, shape, fkey_x, fkey_y, bbox_border)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    #distorted_image, distorted_bbox = image, tf.reshape(tf.stack([0., 0., 1., 1.], axis=-1), [1, 1, 4])\n\n    distorted_bbox = tf.squeeze(distorted_bbox)\n    fkey_x = fkey_x - distorted_bbox[1]# * tf.cast(x_mask, tf.float32)\n    fkey_y = fkey_y - distorted_bbox[0]# * tf.cast(y_mask, tf.float32)\n\n    outside_x = (fkey_x >= distorted_bbox[3])\n    outside_y = (fkey_y >= distorted_bbox[2])\n\n    fkey_x = fkey_x - tf.cast(outside_x, tf.float32)\n    fkey_y = fkey_y - tf.cast(outside_y, tf.float32)\n\n    fkey_x = fkey_x / (distorted_bbox[3] - distorted_bbox[1])\n    fkey_y = fkey_y / (distorted_bbox[2] - distorted_bbox[0])\n\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_image\', tf.expand_dims(distorted_image, 0))\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [output_height, output_width], method),\n        num_cases=num_resize_cases)\n    distorted_image.set_shape([output_height, output_width, 3])\n    #fkey_x = tf.Print(fkey_x,[fkey_x,fkey_y])\n    #print(heatmap_size)\n    #fkey_x = tf.Print(fkey_x,[fkey_x])\n    #fkey_y = tf.Print(fkey_y,[fkey_y])\n    ikey_x = tf.cast(tf.round(fkey_x * heatmap_size), tf.int64)\n    ikey_y = tf.cast(tf.round(fkey_y * heatmap_size), tf.int64)\n\n    gather_ind = config.left_right_remap[category]\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_resized_image\', tf.expand_dims(distorted_image, 0))\n\n    # when do flip_left_right we should also swap the left and right keypoint\n    distorted_image, new_key_x, new_key_y, new_key_v = tf.cond(tf.random_uniform([1], minval=0., maxval=1., dtype=tf.float32)[0] < 0.5, lambda: (tf.image.flip_left_right(distorted_image), heatmap_size - tf.gather(ikey_x, gather_ind), tf.gather(ikey_y, gather_ind), tf.gather(key_v, gather_ind)), lambda: (distorted_image, ikey_x, ikey_y, key_v))\n\n    # new_key_x = tf.Print(new_key_x,[new_key_x])\n    # new_key_y = tf.Print(new_key_y,[new_key_y])\n    #new_key_x = tf.Print(new_key_x,[tf.shape(new_key_x)])\n    targets, isvalid = draw_labelmap(new_key_x, new_key_y, heatmap_sigma, heatmap_size)\n    #norm_gather_ind_ = config.normalize_point_ind_by_id[classid]\n\n    norm_gather_ind = tf.stack([norm_table[0].lookup(classid), norm_table[1].lookup(classid)], axis=-1)\n\n    scale_x_ = tf.cast(output_width, tf.float32)/tf.cast(shape[1], tf.float32)\n    scale_y_ = tf.cast(output_height, tf.float32)/tf.cast(shape[0], tf.float32)\n    scale_x = tf.cast(output_width, tf.float32)/tf.cast(heatmap_size, tf.float32)\n    scale_y = tf.cast(output_height, tf.float32)/tf.cast(heatmap_size, tf.float32)\n    # if the two point used for calculate norm factor missing, then we use original point\n    norm_x, norm_y = tf.cond(tf.reduce_sum(tf.gather(isvalid, norm_gather_ind)) < 2,\n                        lambda: (tf.cast(tf.gather(key_x, norm_gather_ind), tf.float32) * scale_x_,\n                                tf.cast(tf.gather(key_y, norm_gather_ind), tf.float32) * scale_y_),\n                        lambda:(tf.cast(tf.gather(new_key_x, norm_gather_ind), tf.float32) * scale_x,\n                                tf.cast(tf.gather(new_key_y, norm_gather_ind), tf.float32) * scale_y))\n\n    norm_x, norm_y = tf.squeeze(norm_x), tf.squeeze(norm_y)\n\n    norm_value = tf.pow(tf.pow(norm_x[0] - norm_x[1], 2.) + tf.pow(norm_y[0] - norm_y[1], 2.), .5)\n    #targets = draw_labelmap(new_key_x, new_key_y) * tf.expand_dims(tf.expand_dims(tf.cast(tf.clip_by_value(new_key_v, 0, 1), tf.float32), -1), -1)\n\n    if config.DEBUG:\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [unwhiten_image(distorted_image), targets,\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2],\n                                  [output_height, output_width],\n                                  heatmap_size],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        distorted_image = distorted_image/255.\n    else:\n      distorted_image = distorted_image/255.\n    if data_format == \'NCHW\':\n      distorted_image = tf.transpose(distorted_image, perm=(2, 0, 1))\n\n    if not return_keypoints:\n      return distorted_image, targets, new_key_v, isvalid, norm_value\n    else:\n      return distorted_image, targets, new_key_x, new_key_y, new_key_v, isvalid, norm_value\n\n\ndef preprocess_for_train_v0(image,\n                           classid,\n                           shape,\n                           output_height,\n                           output_width,\n                           key_x, key_y, key_v, norm_table,\n                           data_format,\n                           category,\n                           bbox_border, heatmap_sigma, heatmap_size,\n                           return_keypoints=False,\n                           resize_side_min=_RESIZE_SIDE_MIN,\n                           resize_side_max=_RESIZE_SIDE_MAX,\n                           fast_mode=True,\n                           scope=None,\n                           add_image_summaries=True):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'vgg_distort_image\', [image, output_height, output_width]):\n    fkey_x, fkey_y = tf.cast(key_x, tf.float32), tf.cast(key_y, tf.float32)\n    #print(fkey_x, fkey_y)\n    # rotate transform, with bbox contains the clothes region\n    image, fkey_x, fkey_y, bbox = rotate_augum(image, shape, fkey_x, fkey_y, bbox_border)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    #distorted_image, distorted_bbox = image, tf.reshape(tf.stack([0., 0., 1., 1.], axis=-1), [1, 1, 4])\n\n    distorted_bbox = tf.squeeze(distorted_bbox)\n    fkey_x = fkey_x - distorted_bbox[1]# * tf.cast(x_mask, tf.float32)\n    fkey_y = fkey_y - distorted_bbox[0]# * tf.cast(y_mask, tf.float32)\n\n    outside_x = (fkey_x >= distorted_bbox[3])\n    outside_y = (fkey_y >= distorted_bbox[2])\n\n    fkey_x = fkey_x - tf.cast(outside_x, tf.float32)\n    fkey_y = fkey_y - tf.cast(outside_y, tf.float32)\n\n    fkey_x = fkey_x / (distorted_bbox[3] - distorted_bbox[1])\n    fkey_y = fkey_y / (distorted_bbox[2] - distorted_bbox[0])\n\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_image\', tf.expand_dims(distorted_image, 0))\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [output_height, output_width], method),\n        num_cases=num_resize_cases)\n    distorted_image.set_shape([output_height, output_width, 3])\n    #fkey_x = tf.Print(fkey_x,[fkey_x,fkey_y])\n    #print(heatmap_size)\n    #fkey_x = tf.Print(fkey_x,[fkey_x])\n    #fkey_y = tf.Print(fkey_y,[fkey_y])\n    ikey_x = tf.cast(tf.round(fkey_x * heatmap_size), tf.int64)\n    ikey_y = tf.cast(tf.round(fkey_y * heatmap_size), tf.int64)\n\n    gather_ind = config.left_right_remap[category]\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_resized_image\', tf.expand_dims(distorted_image, 0))\n\n    # when do flip_left_right we should also swap the left and right keypoint\n    distorted_image, new_key_x, new_key_y, new_key_v = tf.cond(tf.random_uniform([1], minval=0., maxval=1., dtype=tf.float32)[0] < 0.5, lambda: (tf.image.flip_left_right(distorted_image), heatmap_size - tf.gather(ikey_x, gather_ind), tf.gather(ikey_y, gather_ind), tf.gather(key_v, gather_ind)), lambda: (distorted_image, ikey_x, ikey_y, key_v))\n\n    distorted_image = tf.to_float(distorted_image)\n\n    # Randomly distort the colors. There are 1 or 4 ways to do it.\n    num_distort_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(distorted_image,\n                                              lambda x, ordering: distort_color_v0(x, ordering, fast_mode),\n                                              num_cases=num_distort_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'final_distorted_image\', tf.cast(tf.expand_dims(distorted_image, 0), tf.uint8))\n\n    # new_key_x = tf.Print(new_key_x,[new_key_x])\n    # new_key_y = tf.Print(new_key_y,[new_key_y])\n    #new_key_x = tf.Print(new_key_x,[tf.shape(new_key_x)])\n    targets, isvalid = draw_labelmap(new_key_x, new_key_y, heatmap_sigma, heatmap_size)\n    #norm_gather_ind_ = config.normalize_point_ind_by_id[classid]\n\n    norm_gather_ind = tf.stack([norm_table[0].lookup(classid), norm_table[1].lookup(classid)], axis=-1)\n\n    scale_x_ = tf.cast(output_width, tf.float32)/tf.cast(shape[1], tf.float32)\n    scale_y_ = tf.cast(output_height, tf.float32)/tf.cast(shape[0], tf.float32)\n    scale_x = tf.cast(output_width, tf.float32)/tf.cast(heatmap_size, tf.float32)\n    scale_y = tf.cast(output_height, tf.float32)/tf.cast(heatmap_size, tf.float32)\n    # if the two point used for calculate norm factor missing, then we use original point\n    norm_x, norm_y = tf.cond(tf.reduce_sum(tf.gather(isvalid, norm_gather_ind)) < 2,\n                        lambda: (tf.cast(tf.gather(key_x, norm_gather_ind), tf.float32) * scale_x_,\n                                tf.cast(tf.gather(key_y, norm_gather_ind), tf.float32) * scale_y_),\n                        lambda:(tf.cast(tf.gather(new_key_x, norm_gather_ind), tf.float32) * scale_x,\n                                tf.cast(tf.gather(new_key_y, norm_gather_ind), tf.float32) * scale_y))\n\n    norm_x, norm_y = tf.squeeze(norm_x), tf.squeeze(norm_y)\n\n    norm_value = tf.pow(tf.pow(norm_x[0] - norm_x[1], 2.) + tf.pow(norm_y[0] - norm_y[1], 2.), .5)\n    #targets = draw_labelmap(new_key_x, new_key_y) * tf.expand_dims(tf.expand_dims(tf.cast(tf.clip_by_value(new_key_v, 0, 1), tf.float32), -1), -1)\n\n    if config.DEBUG:\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [distorted_image, targets,\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2],\n                                  [output_height, output_width],\n                                  heatmap_size],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        normarlized_image = _mean_image_subtraction(distorted_image, [_R_MEAN, _G_MEAN, _B_MEAN])\n    else:\n      normarlized_image = _mean_image_subtraction(distorted_image, [_R_MEAN, _G_MEAN, _B_MEAN])\n    if data_format == \'NCHW\':\n      normarlized_image = tf.transpose(normarlized_image, perm=(2, 0, 1))\n\n    return normarlized_image/255., targets, new_key_v, isvalid, norm_value\n\ndef preprocess_for_eval(image, classid, shape, output_height, output_width, key_x, key_y, key_v, norm_table, data_format, category, bbox_border, heatmap_sigma, heatmap_size, resize_side, scope=None):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'vgg_eval_image\', [image, output_height, output_width]):\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    fkey_x, fkey_y = tf.cast(key_x, tf.float32)/tf.cast(shape[1], tf.float32), tf.cast(key_y, tf.float32)/tf.cast(shape[0], tf.float32)\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [output_height, output_width], align_corners=False)\n    image = tf.squeeze(image, [0])\n    image.set_shape([output_height, output_width, 3])\n    image = tf.to_float(image)\n\n    ikey_x = tf.cast(tf.round(fkey_x * heatmap_size), tf.int64)\n    ikey_y = tf.cast(tf.round(fkey_y * heatmap_size), tf.int64)\n\n    targets, isvalid = draw_labelmap(ikey_x, ikey_y, heatmap_sigma, heatmap_size)\n\n    norm_gather_ind = tf.stack([norm_table[0].lookup(classid), norm_table[1].lookup(classid)], axis=-1)\n\n    key_x = tf.cast(tf.round(fkey_x * output_width), tf.int64)\n    key_y = tf.cast(tf.round(fkey_y * output_height), tf.int64)\n\n    norm_x, norm_y = tf.cast(tf.gather(key_x, norm_gather_ind), tf.float32), tf.cast(tf.gather(key_y, norm_gather_ind), tf.float32)\n    norm_x, norm_y = tf.squeeze(norm_x), tf.squeeze(norm_y)\n    norm_value = tf.pow(tf.pow(norm_x[0] - norm_x[1], 2.) + tf.pow(norm_y[0] - norm_y[1], 2.), .5)\n\n    if config.DEBUG:\n      save_image_op = tf.py_func(save_image_with_heatmap,\n                                  [image, targets,\n                                  config.left_right_group_map[category][0],\n                                  config.left_right_group_map[category][1],\n                                  config.left_right_group_map[category][2],\n                                  [output_height, output_width],\n                                  heatmap_size],\n                                  tf.int64, stateful=True)\n      with tf.control_dependencies([save_image_op]):\n        normarlized_image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n    else:\n      normarlized_image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n    if data_format == \'NCHW\':\n      normarlized_image = tf.transpose(normarlized_image, perm=(2, 0, 1))\n    return normarlized_image/255., targets, key_v, isvalid, norm_value\n\n\ndef preprocess_for_test_v0(image, shape, output_height, output_width, data_format=\'NCHW\', bbox_border=25., heatmap_sigma=1., heatmap_size=64, scope=None):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'vgg_test_image\', [image, output_height, output_width]):\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [output_height, output_width], align_corners=False)\n    image = tf.squeeze(image, [0])\n    image.set_shape([output_height, output_width, 3])\n    image = tf.to_float(image)\n\n    normarlized_image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n    if data_format == \'NCHW\':\n      normarlized_image = tf.transpose(normarlized_image, perm=(2, 0, 1))\n    return normarlized_image/255.\n\ndef preprocess_for_test(image, file_name, shape, output_height, output_width, data_format=\'NCHW\', bbox_border=25., heatmap_sigma=1., heatmap_size=64, pred_df=None, scope=None):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'vgg_test_image\', [image, output_height, output_width]):\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n\n    if pred_df is not None:\n      xmin, ymin, xmax, ymax  = [table_.lookup(file_name) for table_ in pred_df]\n      #xmin, ymin, xmax, ymax = [tf.to_float(b) for b in bbox_cord]\n      #xmin = tf.Print(xmin, [file_name, xmin, ymin, xmax, ymax], summarize=500)\n      height, width, channals = tf.unstack(shape, axis=0)\n      xmin, ymin, xmax, ymax = xmin - 100, ymin - 80, xmax + 100, ymax + 80\n\n      xmin, ymin, xmax, ymax = tf.clip_by_value(xmin, 0, width[0]-1), tf.clip_by_value(ymin, 0, height[0]-1), \\\n                              tf.clip_by_value(xmax, 0, width[0]-1), tf.clip_by_value(ymax, 0, height[0]-1)\n\n      bbox_h = ymax - ymin\n      bbox_w = xmax - xmin\n      areas = bbox_h * bbox_w\n\n      offsets=tf.stack([xmin, ymin], axis=0)\n      crop_shape = tf.stack([bbox_h, bbox_w, channals[0]], axis=0)\n\n      ymin, xmin, bbox_h, bbox_w = tf.cast(ymin, tf.int32), tf.cast(xmin, tf.int32), tf.cast(bbox_h, tf.int32), tf.cast(bbox_w, tf.int32)\n      crop_image = tf.image.crop_to_bounding_box(image, ymin, xmin, bbox_h, bbox_w)\n\n      image, shape, offsets = tf.cond(areas > 0, lambda : (crop_image, crop_shape, offsets),\n                                      lambda : (image, shape, tf.constant([0, 0], tf.int64)))\n      offsets.set_shape([2])\n      shape.set_shape([3])\n    else:\n      offsets = tf.constant([0, 0], tf.int64)\n\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [output_height, output_width], align_corners=False)\n    image = tf.squeeze(image, [0])\n    image.set_shape([output_height, output_width, 3])\n\n    if config.DEBUG:\n      save_image_op = tf.py_func(_save_image,\n                                  [image],\n                                  tf.int64, stateful=True)\n      image = tf.Print(image, [save_image_op])\n\n    image = tf.to_float(image)\n    normarlized_image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n    if data_format == \'NCHW\':\n      normarlized_image = tf.transpose(normarlized_image, perm=(2, 0, 1))\n    return normarlized_image/255., shape, offsets\n\ndef preprocess_for_test_raw_output(image, output_height, output_width, data_format=\'NCHW\', scope=None):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  with tf.name_scope(scope, \'vgg_test_image_raw_output\', [image, output_height, output_width]):\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    image = tf.image.resize_bilinear(image, [output_height, output_width], align_corners=False)\n    image = tf.squeeze(image, [0])\n    image.set_shape([output_height, output_width, 3])\n\n    if config.DEBUG:\n      save_image_op = tf.py_func(_save_image,\n                                  [image],\n                                  tf.int64, stateful=True)\n      image = tf.Print(image, [save_image_op])\n\n    image = tf.to_float(image)\n    normarlized_image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n    if data_format == \'NCHW\':\n      normarlized_image = tf.transpose(normarlized_image, perm=(2, 0, 1))\n    return tf.expand_dims(normarlized_image/255., 0)\n\ndef preprocess_image(image, classid, shape, output_height, output_width,\n                    key_x, key_y, key_v, norm_table,\n                    is_training=False,\n                    data_format=\'NCHW\',\n                    category=\'*\',\n                    bbox_border=25., heatmap_sigma=1., heatmap_size=64,\n                    return_keypoints=False,\n                    resize_side_min=_RESIZE_SIDE_MIN,\n                    resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, classid, shape, output_height, output_width, key_x, key_y, key_v, norm_table, data_format,\n                              category, bbox_border, heatmap_sigma, heatmap_size, return_keypoints, resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, classid, shape, output_height, output_width, key_x, key_y, key_v, norm_table, data_format,\n                              category, bbox_border, heatmap_sigma, heatmap_size, min(output_height, output_width))\n'"
utility/__init__.py,0,b'\n'
utility/mertric.py,14,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\n\nimport tensorflow as tf\n\ndef metric_variable(shape, dtype, validate_shape=True, name=None):\n  """"""Create variable in `GraphKeys.(LOCAL|METRIC_VARIABLES`) collections.""""""\n\n  return variable_scope.variable(\n      lambda: array_ops.zeros(shape, dtype),\n      trainable=False,\n      collections=[\n          ops.GraphKeys.LOCAL_VARIABLES #, ops.GraphKeys.METRIC_VARIABLES\n      ],\n      validate_shape=validate_shape,\n      name=name)\n\ndef _safe_div(numerator, denominator, name):\n  """"""Divides two tensors element-wise, returning 0 if the denominator is <= 0.\n\n  Args:\n    numerator: A real `Tensor`.\n    denominator: A real `Tensor`, with dtype matching `numerator`.\n    name: Name for the returned op.\n\n  Returns:\n    0 if `denominator` <= 0, else `numerator` / `denominator`\n  """"""\n  t = math_ops.truediv(numerator, denominator)\n  zero = array_ops.zeros_like(t, dtype=denominator.dtype)\n  condition = math_ops.greater(denominator, zero)\n  zero = math_ops.cast(zero, t.dtype)\n  return array_ops.where(condition, t, zero, name=name)\n\ndef normalized_error(targets, predictions, norm_value, visible, isvalid,\n             bacth_size, num_keypoint, heatmap_size,\n             train_image_size, clip_at_zero=True, name=None):\n\n  with variable_scope.variable_scope(name, \'normalized_error\', (targets, predictions, norm_value, visible, isvalid, bacth_size, num_keypoint, train_image_size, heatmap_size)):\n\n    total = metric_variable([], dtypes.float32, name=\'total\')\n    count = metric_variable([], dtypes.float32, name=\'count\')\n\n    targets, predictions = tf.reshape(targets, [bacth_size, num_keypoint, -1]), tf.reshape(predictions, [bacth_size, num_keypoint, -1])\n\n    pred_max = tf.reduce_max(predictions, axis=-1)\n    pred_indices = tf.argmax(predictions, axis=-1)\n    pred_x, pred_y = tf.floormod(pred_indices, heatmap_size) * train_image_size / heatmap_size, tf.floordiv(pred_indices, heatmap_size) * train_image_size / heatmap_size\n    pred_x, pred_y = tf.cast(pred_x, tf.float32), tf.cast(pred_y, tf.float32)\n    if clip_at_zero:\n      pred_x, pred_y =  pred_x * tf.cast(pred_max>0, tf.float32), pred_y * tf.cast(pred_max>0, tf.float32)\n\n    gt_indices = tf.argmax(targets, axis=-1)\n    gt_x, gt_y = tf.floormod(gt_indices, heatmap_size) * train_image_size / heatmap_size, tf.floordiv(gt_indices, heatmap_size) * train_image_size / heatmap_size\n\n    gt_x, gt_y = tf.cast(gt_x, tf.float32), tf.cast(gt_y, tf.float32)\n    #print(gt_x,gt_y,pred_x,pred_y)\n    #print(norm_value)\n    #print(gt_x)\n    #print(pred_x)\n    dist = _safe_div(tf.pow(tf.pow(gt_x - pred_x, 2.) + tf.pow(gt_y - pred_y, 2.), .5), tf.expand_dims(norm_value, -1), \'norm_dist\')\n\n    #print(visible, isvalid)\n\n    #dist = tf.cond(tf.equal(tf.shape(visible)[-1], tf.shape(isvalid)[-1]), lambda : tf.boolean_mask(dist, tf.logical_and(visible>0, isvalid>0)), lambda : dist)\n    #print(dist)\n    dist = tf.boolean_mask(dist, tf.logical_and(visible>0, isvalid>0))\n    #dist = dist * tf.cast(tf.logical_and(visible>0, isvalid>0), tf.float32)\n\n    update_total_op = state_ops.assign(total, math_ops.reduce_sum(dist))#assign_add #assign\n    update_count_op = state_ops.assign(count, tf.cast(tf.shape(dist)[0], tf.float32))#assign_add #assign\n\n    mean_t = _safe_div(total, count, \'value\')\n    update_op = _safe_div(update_total_op, update_count_op, \'update_op\')\n\n    return mean_t, update_op\n\n\n'"
utility/train_helper.py,51,"b'# Copyright 2018 Changan Wang\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport os\n\nimport tensorflow as tf\n\ndef get_init_fn_for_scaffold(flags):\n    flags_checkpoint_path = flags.checkpoint_path\n    # Warn the user if a checkpoint exists in the model_dir. Then ignore.\n    if tf.train.latest_checkpoint(flags.model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s\' % flags.model_dir)\n        return None\n    if flags_checkpoint_path is None:\n        return None\n    exclusions = []\n    if flags.checkpoint_exclude_scopes:\n        exclusions = [scope.strip() for scope in flags.checkpoint_exclude_scopes.split(\',\')]\n    variables_to_restore = []\n    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n        excluded = False\n        #print(var.op.name)\n        for exclusion in exclusions:\n            if var.op.name.startswith(exclusion):\n                excluded = True\n                break\n        if not excluded:\n            variables_to_restore.append(var)\n    if flags.checkpoint_model_scope is not None:\n        if flags.checkpoint_model_scope.strip() == \'\':\n            variables_to_restore = {var.op.name.replace(flags.model_scope + \'/\', flags.checkpoint_model_scope): var for var in variables_to_restore}\n        else:\n            variables_to_restore = {var.op.name.replace(flags.model_scope, flags.checkpoint_model_scope): var for var in variables_to_restore}\n\n    if tf.gfile.IsDirectory(flags_checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags_checkpoint_path)\n    else:\n        checkpoint_path = flags_checkpoint_path\n\n    tf.logging.info(\'Fine-tuning from %s. Ignoring missing vars: %s\' % (checkpoint_path, flags.ignore_missing_vars))\n\n    # For DEBUG #\n    # reader = tf.train.NewCheckpointReader(checkpoint_path)\n    # variables = reader.get_variable_to_shape_map()\n    # print(\'######################## variables in checkpoint ########################\')\n    # for ele in variables:\n    #     print(ele)\n    # print(\'######################### variables_to_restore #########################\')\n    # for ele in variables_to_restore:\n    #     print(ele)\n    # For DEBUG #\n    if not variables_to_restore:\n        raise ValueError(\'variables_to_restore cannot be empty\')\n    if flags.ignore_missing_vars:\n        reader = tf.train.NewCheckpointReader(checkpoint_path)\n        if isinstance(variables_to_restore, dict):\n            var_dict = variables_to_restore\n        else:\n            var_dict = {var.op.name: var for var in variables_to_restore}\n        available_vars = {}\n        for var in var_dict:\n            if reader.has_tensor(var):\n                available_vars[var] = var_dict[var]\n            else:\n                tf.logging.warning(\'Variable %s missing in checkpoint %s\', var, checkpoint_path)\n        variables_to_restore = available_vars\n    if variables_to_restore:\n        saver = tf.train.Saver(variables_to_restore, reshape=False)\n        saver.build()\n        def callback(scaffold, session):\n            saver.restore(session, checkpoint_path)\n        return callback\n    else:\n        tf.logging.warning(\'No Variables to restore\')\n        return None\n\ndef get_latest_checkpoint_for_evaluate(flags):\n    flags_checkpoint_path = flags.checkpoint_path\n    # Warn the user if a checkpoint exists in the model_dir. Then ignore.\n    if tf.train.latest_checkpoint(flags.model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s\' % flags.model_dir)\n        return None\n\n    if tf.gfile.IsDirectory(flags_checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags_checkpoint_path)\n    else:\n        checkpoint_path = flags_checkpoint_path\n\n    tf.logging.info(\'Restore from %s.\' % (checkpoint_path))\n\n    return checkpoint_path\n\ndef get_init_fn_for_scaffold_(checkpoint_path, model_dir, checkpoint_exclude_scopes, model_scope, checkpoint_model_scope, ignore_missing_vars, use_v1=False):\n    flags_checkpoint_path = checkpoint_path\n    # Warn the user if a checkpoint exists in the model_dir. Then ignore.\n    if tf.train.latest_checkpoint(model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s\' % model_dir)\n        return None\n    if flags_checkpoint_path is None:\n        return None\n    exclusions = []\n    if checkpoint_exclude_scopes:\n        exclusions = [scope.strip() for scope in checkpoint_exclude_scopes.split(\',\')]\n    variables_to_restore = []\n    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n        excluded = False\n        #print(var.op.name)\n        for exclusion in exclusions:\n            if var.op.name.startswith(exclusion):\n                excluded = True\n                break\n        if not excluded:\n            variables_to_restore.append(var)\n    if checkpoint_model_scope is not None:\n        if checkpoint_model_scope.strip() == \'\':\n            variables_to_restore = {var.op.name.replace(model_scope + \'/\', checkpoint_model_scope): var for var in variables_to_restore}\n        else:\n            variables_to_restore = {var.op.name.replace(model_scope, checkpoint_model_scope): var for var in variables_to_restore}\n\n    if tf.gfile.IsDirectory(flags_checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags_checkpoint_path)\n    else:\n        checkpoint_path = flags_checkpoint_path\n\n    tf.logging.info(\'Fine-tuning from %s. Ignoring missing vars: %s\' % (checkpoint_path, ignore_missing_vars))\n\n    # For DEBUG #\n    # reader = tf.train.NewCheckpointReader(checkpoint_path)\n    # variables = reader.get_variable_to_shape_map()\n    # print(\'######################## variables in checkpoint ########################\')\n    # for ele in variables:\n    #     print(ele)\n    # print(\'######################### variables_to_restore #########################\')\n    # for ele in variables_to_restore:\n    #     print(ele)\n    # For DEBUG #\n    if not variables_to_restore:\n        raise ValueError(\'variables_to_restore cannot be empty\')\n    if ignore_missing_vars:\n        reader = tf.train.NewCheckpointReader(checkpoint_path)\n        if isinstance(variables_to_restore, dict):\n            var_dict = variables_to_restore\n        else:\n            var_dict = {var.op.name: var for var in variables_to_restore}\n        available_vars = {}\n        for var in var_dict:\n            if reader.has_tensor(var):\n                available_vars[var] = var_dict[var]\n            else:\n                tf.logging.warning(\'Variable %s missing in checkpoint %s\', var, checkpoint_path)\n        variables_to_restore = available_vars\n    if variables_to_restore:\n        saver = tf.train.Saver(variables_to_restore, reshape=False, write_version=tf.train.SaverDef.V1 if use_v1 else tf.train.SaverDef.V2)\n        saver.build()\n        def callback(scaffold, session):\n            saver.restore(session, checkpoint_path)\n        return callback\n    else:\n        tf.logging.warning(\'No Variables to restore\')\n        return None\n\n\ndef get_raw_init_fn_for_scaffold(checkpoint_path, model_dir, use_v1=False):\n    flags_checkpoint_path = checkpoint_path\n    # Warn the user if a checkpoint exists in the model_dir. Then ignore.\n    if tf.train.latest_checkpoint(model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s\' % model_dir)\n        return None\n    if flags_checkpoint_path is None:\n        return None\n    variables_to_restore = []\n    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n        variables_to_restore.append(var)\n\n    if tf.gfile.IsDirectory(flags_checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags_checkpoint_path)\n    else:\n        checkpoint_path = flags_checkpoint_path\n\n    tf.logging.info(\'Fine-tuning from %s. Ignoring missing vars: %s\' % (checkpoint_path, True))\n\n    if not variables_to_restore:\n        raise ValueError(\'variables_to_restore cannot be empty\')\n\n    reader = tf.train.NewCheckpointReader(checkpoint_path)\n    if isinstance(variables_to_restore, dict):\n        var_dict = variables_to_restore\n    else:\n        var_dict = {var.op.name: var for var in variables_to_restore}\n    available_vars = {}\n    for var in var_dict:\n        if reader.has_tensor(var):\n            available_vars[var] = var_dict[var]\n        else:\n            tf.logging.warning(\'Variable %s missing in checkpoint %s\', var, checkpoint_path)\n    variables_to_restore = available_vars\n    if variables_to_restore:\n        saver = tf.train.Saver(variables_to_restore, reshape=False, write_version=tf.train.SaverDef.V1 if use_v1 else tf.train.SaverDef.V2)\n        saver.build()\n        def callback(scaffold, session):\n            saver.restore(session, checkpoint_path)\n        return callback\n    else:\n        tf.logging.warning(\'No Variables to restore\')\n        return None\n\ndef swa_get_init_fn_for_scaffold(checkpoint_path, model_dir, variables_to_restore, ema, use_v1=False):\n    flags_checkpoint_path = checkpoint_path\n    # Warn the user if a checkpoint exists in the model_dir. Then ignore.\n    if tf.train.latest_checkpoint(model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s\' % model_dir)\n        return None\n    if flags_checkpoint_path is None:\n        return None\n\n    if tf.gfile.IsDirectory(flags_checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags_checkpoint_path)\n    else:\n        checkpoint_path = flags_checkpoint_path\n\n    tf.logging.info(\'Fine-tuning from %s. Ignoring missing vars: %s\' % (checkpoint_path, True))\n\n    if not variables_to_restore:\n        raise ValueError(\'variables_to_restore cannot be empty\')\n\n    reader = tf.train.NewCheckpointReader(checkpoint_path)\n    if isinstance(variables_to_restore, dict):\n        var_dict = variables_to_restore\n    else:\n        var_dict = {var.op.name: var for var in variables_to_restore}\n    available_vars = {}\n    for var in var_dict:\n        if reader.has_tensor(var):\n            available_vars[ema.average_name(var_dict[var])] = var_dict[var]\n        else:\n            tf.logging.warning(\'Variable %s missing in checkpoint %s\', var, checkpoint_path)\n    variables_to_restore = available_vars\n    if variables_to_restore:\n        saver = tf.train.Saver(variables_to_restore, reshape=False, write_version=tf.train.SaverDef.V1 if use_v1 else tf.train.SaverDef.V2)\n        saver.build()\n        def callback(scaffold, session):\n            saver.restore(session, checkpoint_path)\n        return callback\n    else:\n        tf.logging.warning(\'No Variables to restore\')\n        return None\n\ndef get_latest_checkpoint_for_evaluate_(checkpoint_path, model_dir):\n    flags_checkpoint_path = checkpoint_path\n    # Warn the user if a checkpoint exists in the model_dir. Then ignore.\n    if tf.train.latest_checkpoint(model_dir):\n        tf.logging.info(\'Ignoring --checkpoint_path because a checkpoint already exists in %s\' % model_dir)\n        return None\n\n    if tf.gfile.IsDirectory(flags_checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags_checkpoint_path)\n    else:\n        checkpoint_path = flags_checkpoint_path\n\n    tf.logging.info(\'Restore from %s.\' % (checkpoint_path))\n\n    return checkpoint_path\n'"
