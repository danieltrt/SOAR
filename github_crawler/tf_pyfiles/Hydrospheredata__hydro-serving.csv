file_path,api_count,code
docs/src/python/release_docs.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport os\nimport sys\nimport json\nimport logging\nimport http\nimport http.server\nimport re\nimport shutil\nimport itertools\nfrom collections import defaultdict, namedtuple\n\n\n###############################################################################\n# SEMVER HELPERS                                                              #\n###############################################################################\n\nVersion = namedtuple(""Version"", [""major"", ""minor"", ""patch"", ""prerelease"", ""build""])\n\n#: Regex for a semver version\n# Courtesy of https://github.com/python-semver/python-semver\nSEMVER_REGEX = re.compile(\n    r""""""\n        ^\n        (?P<major>0|[1-9]\\d*)\n        \\.\n        (?P<minor>0|[1-9]\\d*)\n        \\.\n        (?P<patch>0|[1-9]\\d*)\n        (?:-(?P<prerelease>\n            (?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*)\n            (?:\\.(?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*))*\n        ))?\n        (?:\\+(?P<build>\n            [0-9a-zA-Z-]+\n            (?:\\.[0-9a-zA-Z-]+)*\n        ))?\n        $\n    """""",\n    re.VERBOSE,\n)\n\ndef parse_version(release_name):\n    match = SEMVER_REGEX.match(release_name)\n    if match is None:\n        logging.warning(""{} is not a semantic version."".format(release_name))\n        return None\n    version_parts = match.groupdict()\n    version_parts[""major""] = int(version_parts[""major""])\n    version_parts[""minor""] = int(version_parts[""minor""])\n    version_parts[""patch""] = int(version_parts[""patch""])\n    return Version(version_parts[""major""], version_parts[""minor""], version_parts[""patch""], version_parts[""prerelease""], version_parts[""build""])\n\n\ndef version_to_string(version):\n    fmt_str = ""{}.{}.{}"".format(version.major, version.minor, version.patch)\n    if version.prerelease:\n      fmt_str =  ""{}-{}"".format(fmt_str, version[3])\n    if version.build:\n      fmt_str =  ""{}+{}"".format(fmt_str, version[4])\n    return fmt_str\n\n# Patch string representation for beautiful output\nVersion.__repr__ = version_to_string\n\ndef cleanup_versions(all_versions):\n    parsed = defaultdict(lambda: [])\n\n    versions = []\n    nonsemverions = []\n    to_delete = []\n    latest = None\n    \n    for x in all_versions:\n        p = parse_version(x)\n        if not p:\n            nonsemverions.append(x)\n            continue\n        parsed[p[:2]].append(p)\n\n    for v in parsed.values():\n        v.sort(reverse=True)\n        head, *tail = v\n        logging.debug(""leaving {}. deleting {}"".format(head, tail))\n        versions.append(head)\n        to_delete = to_delete + tail\n        if not latest:\n            latest = head\n        elif head.major > latest.major:\n            latest = head\n        elif head.minor > latest.minor:\n            latest = head\n    versions.sort(reverse=True)\n    versions = versions + nonsemverions\n    return (latest, versions, to_delete)\n\n###############################################################################\n# RELEASE PROCESS                                                             #\n###############################################################################\n\nclass DocReleaseManager:\n    VERSIONS_FILE = ""versions.json""\n    RELEASE_INFO_FILE = ""paradox.json""\n\n    def __init__(self, website_path, dry_run):\n        self.website_path = website_path\n        self.versions_path = os.path.join(self.website_path, self.VERSIONS_FILE)\n        self.paradox_path = os.path.join(self.website_path, self.RELEASE_INFO_FILE)\n\n        self.execute = not dry_run\n\n    def get_versions(self):\n        try:\n            with open(self.versions_path) as f:\n                ver_list = json.load(f)\n                ver_list = set(ver_list)\n                logging.info(""previous versions: {}"".format(ver_list))\n                return ver_list\n        except IOError:\n            logging.warning(""couldn\'t open {} file. assuming no previous versions."".format(self.versions_path))\n            return set()\n            \n    def update_paradox(self, latest_version):\n        root_latest_path = os.path.join(self.website_path, str(latest_version))\n        paradox_file = ""paradox.json""\n        paradox_path = os.path.join(root_latest_path, paradox_file)\n        root_paradox_path = os.path.join(self.website_path, paradox_file)\n        if self.execute:\n            if os.path.exists(root_paradox_path):\n                os.remove(root_paradox_path)\n            os.symlink(paradox_path, root_paradox_path)\n        logging.info(""linking paradox.json {} -> {}"".format(paradox_path, root_paradox_path))\n\n    def update_latest(self, latest_version):\n        latest_dir = ""latest""\n        fullpath = os.path.join(self.website_path, str(latest_version))\n        root_latest_path = os.path.join(self.website_path, latest_dir)\n        if self.execute:\n            if os.path.exists(root_latest_path):\n                os.remove(root_latest_path)\n            os.symlink(fullpath, root_latest_path, target_is_directory=True)\n        logging.info(""linking latest {} -> {}"".format(fullpath, root_latest_path))\n        self.update_paradox(latest_version)\n\n    def update_versions(self, versions):\n        str_versions = [str(x) for x in versions]\n        logging.info(""wrote versions to file: {}"".format(self.versions_path))\n        if self.execute:\n            with open(self.versions_path, ""w"") as f:\n                json.dump(str_versions, f)\n\n    def delete_version(self, version):\n        str_version = version_to_string(version)\n        fullpath = os.path.join(self.website_path, str_version)\n        if os.path.exists(fullpath):\n            logging.info(""deleted version {}: {}"".format(str_version, fullpath))\n            if self.execute:\n                shutil.rmtree(fullpath)\n        else:\n            logging.error(""version {} doesn\'t exist: {}"".format(str_version, fullpath))\n\n    def release(self, release_version, dry_run=False):\n        release_path = os.path.join(self.website_path, release_version)\n        if not os.path.isdir(release_path):\n            raise ValueError(""release folder is not found: {}"".format(release_path))\n        logging.info(""releasing {}"".format(release_path))\n\n        all_versions = self.get_versions()\n        all_versions.add(release_version)\n\n        if dry_run:\n            logging.warning(""dry run. no changes to the filesystem."")\n        (latest, versions, to_delete) = cleanup_versions(all_versions)\n        if not latest:\n            latest = release_version\n        logging.info(""versions: {}"".format(versions))\n        logging.info(""latest: {}"".format(latest))\n        self.update_latest(latest)\n        self.update_versions(versions)\n        \n        if to_delete:\n            for d in to_delete:\n                self.delete_version(d)\n\n        logging.info(""release is published to the site!"")\n\n###############################################################################\n# CLI                                                                         #\n###############################################################################\n\ndef main(website_path, release_version, dry_run=False, server=False):\n    if server:\n        handler_class = http.server.partial(http.server.SimpleHTTPRequestHandler,\n                                directory=website_path)\n        http.server.test(HandlerClass=handler_class, port=80, bind="""")\n        return\n    mngr = DocReleaseManager(website_path, dry_run)\n    mngr.release(release_version)\n\ndef dir_path(path):\n    if os.path.isdir(path):\n        return path\n    else:\n        raise NotADirectoryError(path)\n\nif __name__ == ""__main__"":\n    logging.basicConfig(level=logging.DEBUG, handlers=[logging.StreamHandler(sys.stdout)], format=""%(asctime)s [%(levelname)s] %(message)s"")\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--website-path"", type=dir_path, required=True)\n    parser.add_argument(""--release-version"", type=str, required=True)\n    parser.add_argument(""--dry-run"", action=""store_true"")\n    parser.add_argument(""--server"", action=""store_true"")\n    args = parser.parse_args()\n\n    main(**vars(args))'"
docs/src/main/paradox/getting-started/snippets/quickstart/client.py,0,"b'import grpc \nimport hydro_serving_grpc as hs  # pip install hydro-serving-grpc\n\n# connect to your ML Lamba instance\nchannel = grpc.insecure_channel(""localhost"")\nstub = hs.PredictionServiceStub(channel)\n\n# 1. define a model, that you\'ll use\nmodel_spec = hs.ModelSpec(name=""linear_regression"", signature_name=""infer"")\n# 2. define tensor_shape for Tensor instance\ntensor_shape = hs.TensorShapeProto(\n    dim=[\n        hs.TensorShapeProto.Dim(size=-1), \n        hs.TensorShapeProto.Dim(size=2)\n    ]\n)\n# 3. define tensor with needed data\ntensor = hs.TensorProto(dtype=hs.DT_DOUBLE, tensor_shape=tensor_shape, double_val=[1,1,1,1])\n# 4. create PredictRequest instance\nrequest = hs.PredictRequest(model_spec=model_spec, inputs={""x"": tensor})\n\n# call Predict method\nresult = stub.Predict(request)'"
docs/src/main/paradox/getting-started/snippets/quickstart/serve.py,1,"b'import numpy as np\nimport tensorflow as tf\nimport hydro_serving_grpc as hs\nfrom keras.models import load_model\n\n# 0. Load model once\nmodel = load_model(\'/model/files/model.h5\')\ngraph = tf.get_default_graph() \n\ndef infer(x):\n    # 1. Retrieve tensor\'s content and put it to numpy array\n    data = np.array(x.double_val)\n    data = data.reshape([dim.size for dim in x.tensor_shape.dim])\n\n    # 2. Make a prediction\n    with graph.as_default():\n        result = model.predict(data)\n    \n    # 3. Pack the answer\n    y_shape = hs.TensorShapeProto(dim=[hs.TensorShapeProto.Dim(size=-1)])\n    y_tensor = hs.TensorProto(\n        dtype=hs.DT_DOUBLE,\n        double_val=result.flatten(),\n        tensor_shape=y_shape)\n\n    # 4. Return the result\n    return hs.PredictResponse(outputs={""y"": y_tensor})'"
docs/src/main/paradox/getting-started/snippets/quickstart/train.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# initialize data\nn_samples = 1000\nX, y = make_regression(n_samples=n_samples, n_features=2, noise=0.5, random_state=112)\n\nscallar_x, scallar_y = MinMaxScaler(), MinMaxScaler()\nscallar_x.fit(X)\nscallar_y.fit(y.reshape(n_samples, 1))\nX = scallar_x.transform(X)\ny = scallar_y.transform(y.reshape(n_samples, 1))\n\n# create a model\nmodel = Sequential()\nmodel.add(Dense(4, input_dim=2, activation='relu'))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\n\nmodel.compile(loss='mse', optimizer='adam')\nmodel.fit(X, y, epochs=100)\n\n# save model\nmodel.save('model.h5')"""
docs/src/main/paradox/how-to/snippets/python/develop-runtime/main.py,0,"b'from runtime import RuntimeManager\n\nimport os\nimport time\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\nif __name__ == \'__main__\':\n    runtime = RuntimeManager(\'/model\', port=int(os.getenv(\'APP_PORT\', ""9090"")))\n    runtime.start()\n\n    try:\n        while True:\n            time.sleep(60 * 60 * 24)\n    except KeyboardInterrupt:\n        runtime.stop()'"
docs/src/main/paradox/how-to/snippets/python/develop-runtime/runtime.py,4,"b'from hydro_serving_grpc.tf.api.predict_pb2 import PredictRequest, PredictResponse\nfrom hydro_serving_grpc.tf.api.prediction_service_pb2_grpc import PredictionServiceServicer, add_PredictionServiceServicer_to_server\nfrom hydro_serving_grpc.tf.types_pb2 import *\nfrom hydro_serving_grpc.tf.tensor_pb2 import TensorProto\nfrom hydro_serving_grpc.contract.model_contract_pb2 import ModelContract\nfrom concurrent import futures\n\nimport os\nimport time\nimport grpc\nimport logging\nimport importlib\n\n\nclass RuntimeService(PredictionServiceServicer):\n    def __init__(self, model_path, contract):\n        self.contract = contract\n        self.model_path = model_path\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def Predict(self, request, context):\n        self.logger.info(f""Received inference request: {request}"")\n        \n        module = importlib.import_module(""func_main"")\n        executable = getattr(module, self.contract.predict.signature_name)\n        result = executable(**request.inputs)\n\n        if not isinstance(result, hs.PredictResponse):\n            self.logger.warning(f""Type of a result ({result}) is not `PredictResponse`"")\n            context.set_code(grpc.StatusCode.OUT_OF_RANGE)\n            context.set_details(f""Type of a result ({result}) is not `PredictResponse`"")\n            return PredictResponse()\n        return result\n\n\nclass RuntimeManager:\n    def __init__(self, model_path, port):\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.port = port\n        self.model_path = model_path\n        self.server = None\n        \n        with open(os.path.join(model_path, \'contract.protobin\')) as file:\n            contract = ModelContract.ParseFromString(file.read())\n        self.servicer = RuntimeService(os.path.join(self.model_path, \'files\'), contract)\n\n    def start(self):\n        self.logger.info(f""Starting PythonRuntime at {self.port}"")\n        self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n        add_PredictionServiceServicer_to_server(self.servicer, self.server)\n        self.server.add_insecure_port(f\'[::]:{self.port}\')\n        self.server.start()\n\n    def stop(self, code=0):\n        self.logger.info(f""Stopping PythonRuntime at {self.port}"")\n        self.server.stop(code)'"
docs/src/main/paradox/how-to/snippets/python/external-model/data-upload.py,0,"b'from argparse import ArgumentParser\nfrom urllib.parse import urljoin\n\nimport requests\n\n\ndef read_in_chunks(filename, chunk_size=1024):\n    """""" Generator to read a file peace by peace. """"""\n    with open(filename, ""rb"") as file:\n        while True:\n            data = file.read(chunk_size)\n            if not data:\n                break\n            yield data\n\n\nif __name__ == ""__main__"": \n    parser = ArgumentParser()\n    parser.add_argument(""--hydrosphere"", type=str, required=True)\n    parser.add_argument(""--model-version-id"", type=int, required=True)\n    parser.add_argument(""--filename"", required=True)\n    parser.add_argument(""--chunk-size"", default=1024)\n    args, unknown = parser.parse_known_args()\n    if unknown:\n        print(""Parsed unknown arguments: %s"", unknown)\n\n    endpoint_uri = ""/monitoring/profiles/batch/{}"".format(args.model_version_id)\n    endpoint_uri = urljoin(args.hydrosphere, endpoint_uri) \n    \n    gen = read_in_chunks(args.filename, chunk_size=args.chunk_size)\n    response = requests.post(endpoint_uri, data=gen, stream=True)\n    if response.status_code != 200:\n        print(""Got error:"", response.text)\n    else:\n        print(""Uploaded data:"", response.text)\n'"
docs/src/main/paradox/how-to/snippets/python/external-model/grpc.py,0,"b'import uuid\nimport grpc\nimport random\nimport hydro_serving_grpc as hs\n\nuse_ssl_connection = True\nif use_ssl_connection:\n    creds = grpc.ssl_channel_credentials()\n    channel = grpc.secure_channel(HYDROSPHERE_INSTANCE_GRPC_URI, credentials=creds)\nelse:\n    channel = grpc.insecure_channel(HYDROSPHERE_INSTANCE_GRPC_URI) \nmonitoring_stub = hs.MonitoringServiceStub(channel)\n\n# 1. Create an ExecutionMetadata message. ExecutionMetadata is used to define, \n# which model, registered within Hydrosphere platform, was used to process a \n# given request.\ntrace_id = str(uuid.uuid4())  # uuid used as an example\nexecution_metadata_proto = hs.ExecutionMetadata(\n    model_name=""external-model-example"",\n    modelVersion_id=2,\n    model_version=3,\n    signature_name=""predict"",\n    request_id=trace_id,\n    latency=0.014,\n)\n\n# 2. Create a PredictRequest message. PredictRequest is used to define the data \n# passed to the model for inference.\npredict_request_proto = hs.PredictRequest(\n    model_spec=hs.ModelSpec(\n        name=""external-model-example"",\n        signature_name=""predict"", \n    ),\n    inputs={\n        ""in"": hs.TensorProto(\n            dtype=hs.DT_DOUBLE, \n            double_val=[random.random()], \n            tensor_shape=hs.TensorShapeProto()\n        ),\n    }, \n)\n\n# 3. Create a PredictResponse message. PredictResponse is used to define the \n# outputs of the model inference.\npredict_response_proto = hs.PredictResponse(\n    outputs={\n        ""out"": hs.TensorProto(\n            dtype=hs.DT_DOUBLE, \n            double_val=[random.random()], \n            tensor_shape=hs.TensorShapeProto()\n        ),\n    },\n)\n\n# 4. Create an ExecutionInformation message. ExecutionInformation contains all \n# request data and all auxiliary information about request execution, required \n# to calculate metrics.\nexecution_information_proto = hs.ExecutionInformation(\n    request=predict_request_proto,\n    response=predict_response_proto,\n    metadata=execution_metadata_proto,\n)\n\n# 5. Use RPC method Analyse of the MonitoringService to calculate metrics\nmonitoring_stub.Analyze(execution_information_proto)\n'"
docs/src/main/paradox/how-to/snippets/python/invoke-applications/grpc.py,0,"b'import grpc \nimport hydro_serving_grpc as hs  # pip install hydro-serving-grpc\n\n# connect to your ML Lamba instance\nchannel = grpc.insecure_channel(""<host>"")\nstub = hs.PredictionServiceStub(channel)\n\n# 1. define a model, that you\'ll use\nmodel_spec = hs.ModelSpec(name=""model"")\n\n# 2. define tensor_shape for Tensor instance\ntensor_shape = hs.TensorShapeProto(\n    dim=[hs.TensorShapeProto.Dim(size=-1), hs.TensorShapeProto.Dim(size=2)])\n\n# 3. define tensor with needed data\ntensor = hs.TensorProto(dtype=hs.DT_DOUBLE, tensor_shape=tensor_shape, double_val=[1,1,1,1])\n\n# 4. create PredictRequest instance\nrequest = hs.PredictRequest(model_spec=model_spec, inputs={""x"": tensor})\n\n# call Predict method\nresult = stub.Predict(request)'"
docs/src/main/paradox/tutorials/monitoring/snippets/isolation_forest_anomaly_detection/serve.py,0,"b'import numpy as np\nimport hydro_serving_grpc as hs\nfrom joblib import dump, load\nimport collections\n\ninit_value = 1.0  # Default value, means that the sample is \'inlier\'\nwindow_len = 5  # Length of data sequence required for model.\n\nwindow = collections.deque(maxlen=window_len)\noutlier_detection_model = load(\'/model/files/iforest.joblib\')\n\n\ndef infer(pickups_last_hour, pickups_next_hour):\n    global window\n\n    # serving.yaml defines that the type of input is int, so we take int_val \n    # from input sample. The pickups_next_hour parameter is a prediction of \n    # the target monitored model.\n    input_value = int(pickups_last_hour.int_val[0])\n\n    if len(window) < window_len-1:\n        window.append(input_value)\n        return pack_predict(init_value)\n    else:\n        window.append(input_value)\n        prediction_vector = np.array(window)\n        # Make a prediction\n        result = outlier_detection_model.predict(prediction_vector.reshape(1, 5))\n        # Pack the answer\n        return pack_predict(result[0])\n\n\ndef pack_predict(result):\n    tensor = hs.TensorProto(\n        dtype=hs.DT_DOUBLE,\n        double_val=[result],\n        tensor_shape=hs.TensorShapeProto()\n    )\n    return hs.PredictResponse(outputs={""value"": tensor})'"
docs/src/main/paradox/tutorials/monitoring/snippets/isolation_forest_anomaly_detection/train.py,0,"b'import joblib\n\n# #main-section\ndf = pd.read_csv(""../data/taxi_pickups.csv"")\ndf.set_index(pd.to_datetime(df.pickup_datetime),inplace=True)\ndf.drop([""pickup_datetime""], axis=1, inplace=True)\n\ndata, _ = transform_to_sliding_windows(df)\niforest = IsolationForest(\n    n_jobs=-1, random_state=42,  behaviour=""new"", contamination=0.03)\nis_outlier = iforest.fit_predict(data)\n# Find outliers in training data \noutlier_indices = df.index[6:][is_outlier==-1]\n# #main-section\n\n\n# #plot-section\nplt.plot(df.index, df.pickups, label=""Training data"")\nplt.vlines(outlier_indices, 0, 600, colors=""red"", alpha=0.2, label=""Outliers"")\n\nplt.gcf().set_size_inches(25, 5)\nplt.legend()\n# #plot-section\n\n\n# #save-section\njoblib.dump(iforest, \'../monitoring_model/iforest.joblib\')\n# #save-section'"
docs/src/main/paradox/tutorials/monitoring/snippets/knn_anomaly_detection/serve.py,0,"b'import hydro_serving_grpc as hs\nimport numpy as np\nfrom joblib import load\n\nmonitoring_model = load(\'/model/files/monitoring_model.joblib\')\n\nfeatures = [\'age\',\n            \'workclass\',\n            \'education\',\n            \'marital_status\',\n            \'occupation\',\n            \'relationship\',\n            \'race\',\n            \'sex\',\n            \'capital_gain\',\n            \'capital_loss\',\n            \'hours_per_week\',\n            \'country\']\n\n\ndef extract_value(proto):\n    return np.array(proto.int64_val, dtype=\'int64\')[0]\n\n\ndef predict(**kwargs):\n    extracted = np.array([extract_value(kwargs[feature]) for feature in features])\n    transformed = np.dstack(extracted).reshape(1, len(features))\n    predicted = monitoring_model.decision_function(transformed)\n\n    response = hs.TensorProto(\n        double_val=[predicted.item()],\n        dtype=hs.DT_DOUBLE,\n        tensor_shape=hs.TensorShapeProto())\n\n    return hs.PredictResponse(outputs={""value"": response})\n'"
docs/src/main/paradox/tutorials/monitoring/snippets/knn_anomaly_detection/train.py,0,"b'import joblib\n\n\n# #main-section\ndf = pd.read_csv(""../data/adult.data"", header=None)\ntarget_labels = pd.Series(df.iloc[:, -1], index=df.index)\n\ndf = df.iloc[:, features_to_use]\ndf.dropna(inplace=True)\n\n# Run feature engineering and then transformations on all features.\nfor feature, func in transformations.items():\n    df[feature] = func(df[feature])\n\nX_train, X_test = train_test_split(np.array(df, dtype=""float""), test_size=0.2)\n\nmonitoring_model = KNN(contamination=0.05, n_neighbors=15, p = 5)\nmonitoring_model.fit(X_train)\n# #main-section\n\n\n# #plot-section\ny_train_pred = monitoring_model.labels_  # binary labels (0: inliers, 1: outliers)\ny_train_scores = monitoring_model.decision_scores_  # raw outlier scores\n\n# Get the prediction on the test data\ny_test_pred = monitoring_model.predict(X_test)  # outlier labels (0 or 1)\ny_test_scores = monitoring_model.decision_function(X_test)  # outlier scores\n\nplt.hist(\n    y_test_scores,\n    bins=30, \n    alpha=0.5, \n    density=True, \n    label=""Test data outlier scores""\n)\nplt.hist(\n    y_train_scores, \n    bins=30, \n    alpha=0.5, \n    density=True, \n    label=""Train data outlier scores""\n)\n\nplt.vlines(monitoring_model.threshold_, 0, 0.1, label = ""Threshold for marking outliers"")\nplt.gcf().set_size_inches(10, 5)\nplt.legend()\n# #plot-section\n\n\n# #save-section\njoblib.dump(monitoring_model, ""../monitoring_model/monitoring_model.joblib"")\n# #save-section\n'"
docs/src/main/paradox/tutorials/serving/snippets/python/serve.py,1,"b'import tensorflow as tf\nimport hydro_serving_grpc as hs  # this package is already present in the runtime\n\ndef increment(number):   # <- keep in mind the signature\n    request_number = tf.make_ndarray(number)\n    response_number = request_number + 1\n\n    response_tensor_shape = [\n        hs.TensorShapeProto.Dim(size=dim.size) for dim in number.tensor_shape.dim]\n    response_tensor = hs.TensorProto(\n        int_val=response_number.flatten(), \n        dtype=hs.DT_INT32,\n        tensor_shape=hs.TensorShapeProto(dim=response_tensor_shape)\n    )\n\n    return hs.PredictResponse(\n        outputs={""number"": response_tensor})'"
