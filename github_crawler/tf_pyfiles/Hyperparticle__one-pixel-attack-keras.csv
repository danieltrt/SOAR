file_path,api_count,code
attack.py,0,"b""#!/usr/bin/env python3\n\nimport argparse\n\nimport numpy as np\nimport pandas as pd\nfrom keras.datasets import cifar10\nimport pickle\n\n# Custom Networks\nfrom networks.lenet import LeNet\nfrom networks.pure_cnn import PureCnn\nfrom networks.network_in_network import NetworkInNetwork\nfrom networks.resnet import ResNet\nfrom networks.densenet import DenseNet\nfrom networks.wide_resnet import WideResNet\nfrom networks.capsnet import CapsNet\n\n# Helper functions\nfrom differential_evolution import differential_evolution\nimport helper\n\n\nclass PixelAttacker:\n    def __init__(self, models, data, class_names, dimensions=(32, 32)):\n        # Load data and model\n        self.models = models\n        self.x_test, self.y_test = data\n        self.class_names = class_names\n        self.dimensions = dimensions\n\n        network_stats, correct_imgs = helper.evaluate_models(self.models, self.x_test, self.y_test)\n        self.correct_imgs = pd.DataFrame(correct_imgs, columns=['name', 'img', 'label', 'confidence', 'pred'])\n        self.network_stats = pd.DataFrame(network_stats, columns=['name', 'accuracy', 'param_count'])\n\n    def predict_classes(self, xs, img, target_class, model, minimize=True):\n        # Perturb the image with the given pixel(s) x and get the prediction of the model\n        imgs_perturbed = helper.perturb_image(xs, img)\n        predictions = model.predict(imgs_perturbed)[:, target_class]\n        # This function should always be minimized, so return its complement if needed\n        return predictions if minimize else 1 - predictions\n\n    def attack_success(self, x, img, target_class, model, targeted_attack=False, verbose=False):\n        # Perturb the image with the given pixel(s) and get the prediction of the model\n        attack_image = helper.perturb_image(x, img)\n\n        confidence = model.predict(attack_image)[0]\n        predicted_class = np.argmax(confidence)\n\n        # If the prediction is what we want (misclassification or \n        # targeted classification), return True\n        if verbose:\n            print('Confidence:', confidence[target_class])\n        if ((targeted_attack and predicted_class == target_class) or\n                (not targeted_attack and predicted_class != target_class)):\n            return True\n\n    def attack(self, img_id, model, target=None, pixel_count=1,\n               maxiter=75, popsize=400, verbose=False, plot=False):\n        # Change the target class based on whether this is a targeted attack or not\n        targeted_attack = target is not None\n        target_class = target if targeted_attack else self.y_test[img_id, 0]\n\n        # Define bounds for a flat vector of x,y,r,g,b values\n        # For more pixels, repeat this layout\n        dim_x, dim_y = self.dimensions\n        bounds = [(0, dim_x), (0, dim_y), (0, 256), (0, 256), (0, 256)] * pixel_count\n\n        # Population multiplier, in terms of the size of the perturbation vector x\n        popmul = max(1, popsize // len(bounds))\n\n        # Format the predict/callback functions for the differential evolution algorithm\n        def predict_fn(xs):\n            return self.predict_classes(xs, self.x_test[img_id], target_class, model, target is None)\n\n        def callback_fn(x, convergence):\n            return self.attack_success(x, self.x_test[img_id], target_class, model, targeted_attack, verbose)\n\n        # Call Scipy's Implementation of Differential Evolution\n        attack_result = differential_evolution(\n            predict_fn, bounds, maxiter=maxiter, popsize=popmul,\n            recombination=1, atol=-1, callback=callback_fn, polish=False)\n\n        # Calculate some useful statistics to return from this function\n        attack_image = helper.perturb_image(attack_result.x, self.x_test[img_id])[0]\n        prior_probs = model.predict(np.array([self.x_test[img_id]]))[0]\n        predicted_probs = model.predict(np.array([attack_image]))[0]\n        predicted_class = np.argmax(predicted_probs)\n        actual_class = self.y_test[img_id, 0]\n        success = predicted_class != actual_class\n        cdiff = prior_probs[actual_class] - predicted_probs[actual_class]\n\n        # Show the best attempt at a solution (successful or not)\n        if plot:\n            helper.plot_image(attack_image, actual_class, self.class_names, predicted_class)\n\n        return [model.name, pixel_count, img_id, actual_class, predicted_class, success, cdiff, prior_probs,\n                predicted_probs, attack_result.x]\n\n    def attack_all(self, models, samples=500, pixels=(1, 3, 5), targeted=False,\n                   maxiter=75, popsize=400, verbose=False):\n        results = []\n        for model in models:\n            model_results = []\n            valid_imgs = self.correct_imgs[self.correct_imgs.name == model.name].img\n            img_samples = np.random.choice(valid_imgs, samples)\n\n            for pixel_count in pixels:\n                for i, img in enumerate(img_samples):\n                    print(model.name, '- image', img, '-', i + 1, '/', len(img_samples))\n                    targets = [None] if not targeted else range(10)\n\n                    for target in targets:\n                        if targeted:\n                            print('Attacking with target', self.class_names[target])\n                            if target == self.y_test[img, 0]:\n                                continue\n                        result = self.attack(img, model, target, pixel_count,\n                                             maxiter=maxiter, popsize=popsize,\n                                             verbose=verbose)\n                        model_results.append(result)\n\n            results += model_results\n            helper.checkpoint(results, targeted)\n        return results\n\n\nif __name__ == '__main__':\n    model_defs = {\n        'lenet': LeNet,\n        'pure_cnn': PureCnn,\n        'net_in_net': NetworkInNetwork,\n        'resnet': ResNet,\n        'densenet': DenseNet,\n        'wide_resnet': WideResNet,\n        'capsnet': CapsNet\n    }\n\n    parser = argparse.ArgumentParser(description='Attack models on Cifar10')\n    parser.add_argument('--model', nargs='+', choices=model_defs.keys(), default=model_defs.keys(),\n                        help='Specify one or more models by name to evaluate.')\n    parser.add_argument('--pixels', nargs='+', default=(1, 3, 5), type=int,\n                        help='The number of pixels that can be perturbed.')\n    parser.add_argument('--maxiter', default=75, type=int,\n                        help='The maximum number of iterations in the differential evolution algorithm before giving up and failing the attack.')\n    parser.add_argument('--popsize', default=400, type=int,\n                        help='The number of adversarial images generated each iteration in the differential evolution algorithm. Increasing this number requires more computation.')\n    parser.add_argument('--samples', default=500, type=int,\n                        help='The number of image samples to attack. Images are sampled randomly from the dataset.')\n    parser.add_argument('--targeted', action='store_true', help='Set this switch to test for targeted attacks.')\n    parser.add_argument('--save', default='networks/results/results.pkl', help='Save location for the results (pickle)')\n    parser.add_argument('--verbose', action='store_true', help='Print out additional information every iteration.')\n\n    args = parser.parse_args()\n\n    # Load data and model\n    _, test = cifar10.load_data()\n    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n    models = [model_defs[m](load_weights=True) for m in args.model]\n\n    attacker = PixelAttacker(models, test, class_names)\n\n    print('Starting attack')\n\n    results = attacker.attack_all(models, samples=args.samples, pixels=args.pixels, targeted=args.targeted,\n                                  maxiter=args.maxiter, popsize=args.popsize, verbose=args.verbose)\n\n    columns = ['model', 'pixels', 'image', 'true', 'predicted', 'success', 'cdiff', 'prior_probs', 'predicted_probs',\n               'perturbation']\n    results_table = pd.DataFrame(results, columns=columns)\n\n    print(results_table[['model', 'pixels', 'image', 'true', 'predicted', 'success']])\n\n    print('Saving to', args.save)\n    with open(args.save, 'wb') as file:\n        pickle.dump(results, file)\n"""
differential_evolution.py,0,"b'""""""\nA slight modification to Scipy\'s implementation of differential evolution. To speed up predictions, the entire parameters array is passed to `self.func`, where a neural network model can batch its computations and execute in parallel. Search for `CHANGES` to find all code changes.\n\nDan Kondratyuk 2018\n\nOriginal code adapted from\nhttps://github.com/scipy/scipy/blob/70e61dee181de23fdd8d893eaa9491100e2218d7/scipy/optimize/_differentialevolution.py\n----------\n\ndifferential_evolution: The differential evolution global optimization algorithm\nAdded by Andrew Nelson 2014\n""""""\nfrom __future__ import division, print_function, absolute_import\nimport numpy as np\nfrom scipy.optimize import OptimizeResult, minimize\nfrom scipy.optimize.optimize import _status_message\nfrom scipy._lib._util import check_random_state\nfrom scipy._lib.six import xrange, string_types\nimport warnings\n\n\n__all__ = [\'differential_evolution\']\n\n_MACHEPS = np.finfo(np.float64).eps\n\n\ndef differential_evolution(func, bounds, args=(), strategy=\'best1bin\',\n                           maxiter=1000, popsize=15, tol=0.01,\n                           mutation=(0.5, 1), recombination=0.7, seed=None,\n                           callback=None, disp=False, polish=True,\n                           init=\'latinhypercube\', atol=0):\n    """"""Finds the global minimum of a multivariate function.\n    Differential Evolution is stochastic in nature (does not use gradient\n    methods) to find the minimium, and can search large areas of candidate\n    space, but often requires larger numbers of function evaluations than\n    conventional gradient based techniques.\n    The algorithm is due to Storn and Price [1]_.\n    Parameters\n    ----------\n    func : callable\n        The objective function to be minimized.  Must be in the form\n        ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n        and ``args`` is a  tuple of any additional fixed parameters needed to\n        completely specify the function.\n    bounds : sequence\n        Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n        defining the lower and upper bounds for the optimizing argument of\n        `func`. It is required to have ``len(bounds) == len(x)``.\n        ``len(bounds)`` is used to determine the number of parameters in ``x``.\n    args : tuple, optional\n        Any additional fixed parameters needed to\n        completely specify the objective function.\n    strategy : str, optional\n        The differential evolution strategy to use. Should be one of:\n            - \'best1bin\'\n            - \'best1exp\'\n            - \'rand1exp\'\n            - \'randtobest1exp\'\n            - \'currenttobest1exp\'\n            - \'best2exp\'\n            - \'rand2exp\'\n            - \'randtobest1bin\'\n            - \'currenttobest1bin\'\n            - \'best2bin\'\n            - \'rand2bin\'\n            - \'rand1bin\'\n        The default is \'best1bin\'.\n    maxiter : int, optional\n        The maximum number of generations over which the entire population is\n        evolved. The maximum number of function evaluations (with no polishing)\n        is: ``(maxiter + 1) * popsize * len(x)``\n    popsize : int, optional\n        A multiplier for setting the total population size.  The population has\n        ``popsize * len(x)`` individuals (unless the initial population is\n        supplied via the `init` keyword).\n    tol : float, optional\n        Relative tolerance for convergence, the solving stops when\n        ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n        where and `atol` and `tol` are the absolute and relative tolerance\n        respectively.\n    mutation : float or tuple(float, float), optional\n        The mutation constant. In the literature this is also known as\n        differential weight, being denoted by F.\n        If specified as a float it should be in the range [0, 2].\n        If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n        randomly changes the mutation constant on a generation by generation\n        basis. The mutation constant for that generation is taken from\n        ``U[min, max)``. Dithering can help speed convergence significantly.\n        Increasing the mutation constant increases the search radius, but will\n        slow down convergence.\n    recombination : float, optional\n        The recombination constant, should be in the range [0, 1]. In the\n        literature this is also known as the crossover probability, being\n        denoted by CR. Increasing this value allows a larger number of mutants\n        to progress into the next generation, but at the risk of population\n        stability.\n    seed : int or `np.random.RandomState`, optional\n        If `seed` is not specified the `np.RandomState` singleton is used.\n        If `seed` is an int, a new `np.random.RandomState` instance is used,\n        seeded with seed.\n        If `seed` is already a `np.random.RandomState instance`, then that\n        `np.random.RandomState` instance is used.\n        Specify `seed` for repeatable minimizations.\n    disp : bool, optional\n        Display status messages\n    callback : callable, `callback(xk, convergence=val)`, optional\n        A function to follow the progress of the minimization. ``xk`` is\n        the current value of ``x0``. ``val`` represents the fractional\n        value of the population convergence.  When ``val`` is greater than one\n        the function halts. If callback returns `True`, then the minimization\n        is halted (any polishing is still carried out).\n    polish : bool, optional\n        If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n        method is used to polish the best population member at the end, which\n        can improve the minimization slightly.\n    init : str or array-like, optional\n        Specify which type of population initialization is performed. Should be\n        one of:\n            - \'latinhypercube\'\n            - \'random\'\n            - array specifying the initial population. The array should have\n              shape ``(M, len(x))``, where len(x) is the number of parameters.\n              `init` is clipped to `bounds` before use.\n        The default is \'latinhypercube\'. Latin Hypercube sampling tries to\n        maximize coverage of the available parameter space. \'random\'\n        initializes the population randomly - this has the drawback that\n        clustering can occur, preventing the whole of parameter space being\n        covered. Use of an array to specify a population subset could be used,\n        for example, to create a tight bunch of initial guesses in an location\n        where the solution is known to exist, thereby reducing time for\n        convergence.\n    atol : float, optional\n        Absolute tolerance for convergence, the solving stops when\n        ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n        where and `atol` and `tol` are the absolute and relative tolerance\n        respectively.\n    Returns\n    -------\n    res : OptimizeResult\n        The optimization result represented as a `OptimizeResult` object.\n        Important attributes are: ``x`` the solution array, ``success`` a\n        Boolean flag indicating if the optimizer exited successfully and\n        ``message`` which describes the cause of the termination. See\n        `OptimizeResult` for a description of other attributes.  If `polish`\n        was employed, and a lower minimum was obtained by the polishing, then\n        OptimizeResult also contains the ``jac`` attribute.\n    Notes\n    -----\n    Differential evolution is a stochastic population based method that is\n    useful for global optimization problems. At each pass through the population\n    the algorithm mutates each candidate solution by mixing with other candidate\n    solutions to create a trial candidate. There are several strategies [2]_ for\n    creating trial candidates, which suit some problems more than others. The\n    \'best1bin\' strategy is a good starting point for many systems. In this\n    strategy two members of the population are randomly chosen. Their difference\n    is used to mutate the best member (the `best` in `best1bin`), :math:`b_0`,\n    so far:\n    .. math::\n        b\' = b_0 + mutation * (population[rand0] - population[rand1])\n    A trial vector is then constructed. Starting with a randomly chosen \'i\'th\n    parameter the trial is sequentially filled (in modulo) with parameters from\n    `b\'` or the original candidate. The choice of whether to use `b\'` or the\n    original candidate is made with a binomial distribution (the \'bin\' in\n    \'best1bin\') - a random number in [0, 1) is generated.  If this number is\n    less than the `recombination` constant then the parameter is loaded from\n    `b\'`, otherwise it is loaded from the original candidate.  The final\n    parameter is always loaded from `b\'`.  Once the trial candidate is built\n    its fitness is assessed. If the trial is better than the original candidate\n    then it takes its place. If it is also better than the best overall\n    candidate it also replaces that.\n    To improve your chances of finding a global minimum use higher `popsize`\n    values, with higher `mutation` and (dithering), but lower `recombination`\n    values. This has the effect of widening the search radius, but slowing\n    convergence.\n    .. versionadded:: 0.15.0\n    Examples\n    --------\n    Let us consider the problem of minimizing the Rosenbrock function. This\n    function is implemented in `rosen` in `scipy.optimize`.\n    >>> from scipy.optimize import rosen, differential_evolution\n    >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n    >>> result = differential_evolution(rosen, bounds)\n    >>> result.x, result.fun\n    (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n    Next find the minimum of the Ackley function\n    (http://en.wikipedia.org/wiki/Test_functions_for_optimization).\n    >>> from scipy.optimize import differential_evolution\n    >>> import numpy as np\n    >>> def ackley(x):\n    ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n    ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n    ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n    >>> bounds = [(-5, 5), (-5, 5)]\n    >>> result = differential_evolution(ackley, bounds)\n    >>> result.x, result.fun\n    (array([ 0.,  0.]), 4.4408920985006262e-16)\n    References\n    ----------\n    .. [1] Storn, R and Price, K, Differential Evolution - a Simple and\n           Efficient Heuristic for Global Optimization over Continuous Spaces,\n           Journal of Global Optimization, 1997, 11, 341 - 359.\n    .. [2] http://www1.icsi.berkeley.edu/~storn/code.html\n    .. [3] http://en.wikipedia.org/wiki/Differential_evolution\n    """"""\n\n    solver = DifferentialEvolutionSolver(func, bounds, args=args,\n                                         strategy=strategy, maxiter=maxiter,\n                                         popsize=popsize, tol=tol,\n                                         mutation=mutation,\n                                         recombination=recombination,\n                                         seed=seed, polish=polish,\n                                         callback=callback,\n                                         disp=disp, init=init, atol=atol)\n    return solver.solve()\n\n\nclass DifferentialEvolutionSolver(object):\n\n    """"""This class implements the differential evolution solver\n    Parameters\n    ----------\n    func : callable\n        The objective function to be minimized.  Must be in the form\n        ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n        and ``args`` is a  tuple of any additional fixed parameters needed to\n        completely specify the function.\n    bounds : sequence\n        Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n        defining the lower and upper bounds for the optimizing argument of\n        `func`. It is required to have ``len(bounds) == len(x)``.\n        ``len(bounds)`` is used to determine the number of parameters in ``x``.\n    args : tuple, optional\n        Any additional fixed parameters needed to\n        completely specify the objective function.\n    strategy : str, optional\n        The differential evolution strategy to use. Should be one of:\n            - \'best1bin\'\n            - \'best1exp\'\n            - \'rand1exp\'\n            - \'randtobest1exp\'\n            - \'currenttobest1exp\'\n            - \'best2exp\'\n            - \'rand2exp\'\n            - \'randtobest1bin\'\n            - \'currenttobest1bin\'\n            - \'best2bin\'\n            - \'rand2bin\'\n            - \'rand1bin\'\n        The default is \'best1bin\'\n    maxiter : int, optional\n        The maximum number of generations over which the entire population is\n        evolved. The maximum number of function evaluations (with no polishing)\n        is: ``(maxiter + 1) * popsize * len(x)``\n    popsize : int, optional\n        A multiplier for setting the total population size.  The population has\n        ``popsize * len(x)`` individuals (unless the initial population is\n        supplied via the `init` keyword).\n    tol : float, optional\n        Relative tolerance for convergence, the solving stops when\n        ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n        where and `atol` and `tol` are the absolute and relative tolerance\n        respectively.\n    mutation : float or tuple(float, float), optional\n        The mutation constant. In the literature this is also known as\n        differential weight, being denoted by F.\n        If specified as a float it should be in the range [0, 2].\n        If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n        randomly changes the mutation constant on a generation by generation\n        basis. The mutation constant for that generation is taken from\n        U[min, max). Dithering can help speed convergence significantly.\n        Increasing the mutation constant increases the search radius, but will\n        slow down convergence.\n    recombination : float, optional\n        The recombination constant, should be in the range [0, 1]. In the\n        literature this is also known as the crossover probability, being\n        denoted by CR. Increasing this value allows a larger number of mutants\n        to progress into the next generation, but at the risk of population\n        stability.\n    seed : int or `np.random.RandomState`, optional\n        If `seed` is not specified the `np.random.RandomState` singleton is\n        used.\n        If `seed` is an int, a new `np.random.RandomState` instance is used,\n        seeded with `seed`.\n        If `seed` is already a `np.random.RandomState` instance, then that\n        `np.random.RandomState` instance is used.\n        Specify `seed` for repeatable minimizations.\n    disp : bool, optional\n        Display status messages\n    callback : callable, `callback(xk, convergence=val)`, optional\n        A function to follow the progress of the minimization. ``xk`` is\n        the current value of ``x0``. ``val`` represents the fractional\n        value of the population convergence.  When ``val`` is greater than one\n        the function halts. If callback returns `True`, then the minimization\n        is halted (any polishing is still carried out).\n    polish : bool, optional\n        If True, then `scipy.optimize.minimize` with the `L-BFGS-B` method\n        is used to polish the best population member at the end. This requires\n        a few more function evaluations.\n    maxfun : int, optional\n        Set the maximum number of function evaluations. However, it probably\n        makes more sense to set `maxiter` instead.\n    init : str or array-like, optional\n        Specify which type of population initialization is performed. Should be\n        one of:\n            - \'latinhypercube\'\n            - \'random\'\n            - array specifying the initial population. The array should have\n              shape ``(M, len(x))``, where len(x) is the number of parameters.\n              `init` is clipped to `bounds` before use.\n        The default is \'latinhypercube\'. Latin Hypercube sampling tries to\n        maximize coverage of the available parameter space. \'random\'\n        initializes the population randomly - this has the drawback that\n        clustering can occur, preventing the whole of parameter space being\n        covered. Use of an array to specify a population could be used, for\n        example, to create a tight bunch of initial guesses in an location\n        where the solution is known to exist, thereby reducing time for\n        convergence.\n    atol : float, optional\n        Absolute tolerance for convergence, the solving stops when\n        ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n        where and `atol` and `tol` are the absolute and relative tolerance\n        respectively.\n    """"""\n\n    # Dispatch of mutation strategy method (binomial or exponential).\n    _binomial = {\'best1bin\': \'_best1\',\n                 \'randtobest1bin\': \'_randtobest1\',\n                 \'currenttobest1bin\': \'_currenttobest1\',\n                 \'best2bin\': \'_best2\',\n                 \'rand2bin\': \'_rand2\',\n                 \'rand1bin\': \'_rand1\'}\n    _exponential = {\'best1exp\': \'_best1\',\n                    \'rand1exp\': \'_rand1\',\n                    \'randtobest1exp\': \'_randtobest1\',\n                    \'currenttobest1exp\': \'_currenttobest1\',\n                    \'best2exp\': \'_best2\',\n                    \'rand2exp\': \'_rand2\'}\n\n    __init_error_msg = (""The population initialization method must be one of ""\n                        ""\'latinhypercube\' or \'random\', or an array of shape ""\n                        ""(M, N) where N is the number of parameters and M>5"")\n\n    def __init__(self, func, bounds, args=(),\n                 strategy=\'best1bin\', maxiter=1000, popsize=15,\n                 tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None,\n                 maxfun=np.inf, callback=None, disp=False, polish=True,\n                 init=\'latinhypercube\', atol=0):\n\n        if strategy in self._binomial:\n            self.mutation_func = getattr(self, self._binomial[strategy])\n        elif strategy in self._exponential:\n            self.mutation_func = getattr(self, self._exponential[strategy])\n        else:\n            raise ValueError(""Please select a valid mutation strategy"")\n        self.strategy = strategy\n\n        self.callback = callback\n        self.polish = polish\n\n        # relative and absolute tolerances for convergence\n        self.tol, self.atol = tol, atol\n\n        # Mutation constant should be in [0, 2). If specified as a sequence\n        # then dithering is performed.\n        self.scale = mutation\n        if (not np.all(np.isfinite(mutation)) or\n                np.any(np.array(mutation) >= 2) or\n                np.any(np.array(mutation) < 0)):\n            raise ValueError(\'The mutation constant must be a float in \'\n                             \'U[0, 2), or specified as a tuple(min, max)\'\n                             \' where min < max and min, max are in U[0, 2).\')\n\n        self.dither = None\n        if hasattr(mutation, \'__iter__\') and len(mutation) > 1:\n            self.dither = [mutation[0], mutation[1]]\n            self.dither.sort()\n\n        self.cross_over_probability = recombination\n\n        self.func = func\n        self.args = args\n\n        # convert tuple of lower and upper bounds to limits\n        # [(low_0, high_0), ..., (low_n, high_n]\n        #     -> [[low_0, ..., low_n], [high_0, ..., high_n]]\n        self.limits = np.array(bounds, dtype=\'float\').T\n        if (np.size(self.limits, 0) != 2 or not\n                np.all(np.isfinite(self.limits))):\n            raise ValueError(\'bounds should be a sequence containing \'\n                             \'real valued (min, max) pairs for each value\'\n                             \' in x\')\n\n        if maxiter is None:  # the default used to be None\n            maxiter = 1000\n        self.maxiter = maxiter\n        if maxfun is None:  # the default used to be None\n            maxfun = np.inf\n        self.maxfun = maxfun\n\n        # population is scaled to between [0, 1].\n        # We have to scale between parameter <-> population\n        # save these arguments for _scale_parameter and\n        # _unscale_parameter. This is an optimization\n        self.__scale_arg1 = 0.5 * (self.limits[0] + self.limits[1])\n        self.__scale_arg2 = np.fabs(self.limits[0] - self.limits[1])\n\n        self.parameter_count = np.size(self.limits, 1)\n\n        self.random_number_generator = check_random_state(seed)\n\n        # default population initialization is a latin hypercube design, but\n        # there are other population initializations possible.\n        # the minimum is 5 because \'best2bin\' requires a population that\'s at\n        # least 5 long\n        self.num_population_members = max(5, popsize * self.parameter_count)\n\n        self.population_shape = (self.num_population_members,\n                                 self.parameter_count)\n\n        self._nfev = 0\n        if isinstance(init, string_types):\n            if init == \'latinhypercube\':\n                self.init_population_lhs()\n            elif init == \'random\':\n                self.init_population_random()\n            else:\n                raise ValueError(self.__init_error_msg)\n        else:\n            self.init_population_array(init)\n\n        self.disp = disp\n\n    def init_population_lhs(self):\n        """"""\n        Initializes the population with Latin Hypercube Sampling.\n        Latin Hypercube Sampling ensures that each parameter is uniformly\n        sampled over its range.\n        """"""\n        rng = self.random_number_generator\n\n        # Each parameter range needs to be sampled uniformly. The scaled\n        # parameter range ([0, 1)) needs to be split into\n        # `self.num_population_members` segments, each of which has the following\n        # size:\n        segsize = 1.0 / self.num_population_members\n\n        # Within each segment we sample from a uniform random distribution.\n        # We need to do this sampling for each parameter.\n        samples = (segsize * rng.random_sample(self.population_shape)\n\n        # Offset each segment to cover the entire parameter range [0, 1)\n                   + np.linspace(0., 1., self.num_population_members,\n                                 endpoint=False)[:, np.newaxis])\n\n        # Create an array for population of candidate solutions.\n        self.population = np.zeros_like(samples)\n\n        # Initialize population of candidate solutions by permutation of the\n        # random samples.\n        for j in range(self.parameter_count):\n            order = rng.permutation(range(self.num_population_members))\n            self.population[:, j] = samples[order, j]\n\n        # reset population energies\n        self.population_energies = (np.ones(self.num_population_members) *\n                                    np.inf)\n\n        # reset number of function evaluations counter\n        self._nfev = 0\n\n    def init_population_random(self):\n        """"""\n        Initialises the population at random.  This type of initialization\n        can possess clustering, Latin Hypercube sampling is generally better.\n        """"""\n        rng = self.random_number_generator\n        self.population = rng.random_sample(self.population_shape)\n\n        # reset population energies\n        self.population_energies = (np.ones(self.num_population_members) *\n                                    np.inf)\n\n        # reset number of function evaluations counter\n        self._nfev = 0\n\n    def init_population_array(self, init):\n        """"""\n        Initialises the population with a user specified population.\n        Parameters\n        ----------\n        init : np.ndarray\n            Array specifying subset of the initial population. The array should\n            have shape (M, len(x)), where len(x) is the number of parameters.\n            The population is clipped to the lower and upper `bounds`.\n        """"""\n        # make sure you\'re using a float array\n        popn = np.asfarray(init)\n\n        if (np.size(popn, 0) < 5 or\n                popn.shape[1] != self.parameter_count or\n                len(popn.shape) != 2):\n            raise ValueError(""The population supplied needs to have shape""\n                             "" (M, len(x)), where M > 4."")\n\n        # scale values and clip to bounds, assigning to population\n        self.population = np.clip(self._unscale_parameters(popn), 0, 1)\n\n        self.num_population_members = np.size(self.population, 0)\n\n        self.population_shape = (self.num_population_members,\n                                 self.parameter_count)\n\n        # reset population energies\n        self.population_energies = (np.ones(self.num_population_members) *\n                                    np.inf)\n\n        # reset number of function evaluations counter\n        self._nfev = 0\n\n    @property\n    def x(self):\n        """"""\n        The best solution from the solver\n        Returns\n        -------\n        x : ndarray\n            The best solution from the solver.\n        """"""\n        return self._scale_parameters(self.population[0])\n\n    @property\n    def convergence(self):\n        """"""\n        The standard deviation of the population energies divided by their\n        mean.\n        """"""\n        return (np.std(self.population_energies) /\n                np.abs(np.mean(self.population_energies) + _MACHEPS))\n\n    def solve(self):\n        """"""\n        Runs the DifferentialEvolutionSolver.\n        Returns\n        -------\n        res : OptimizeResult\n            The optimization result represented as a ``OptimizeResult`` object.\n            Important attributes are: ``x`` the solution array, ``success`` a\n            Boolean flag indicating if the optimizer exited successfully and\n            ``message`` which describes the cause of the termination. See\n            `OptimizeResult` for a description of other attributes.  If `polish`\n            was employed, and a lower minimum was obtained by the polishing,\n            then OptimizeResult also contains the ``jac`` attribute.\n        """"""\n        nit, warning_flag = 0, False\n        status_message = _status_message[\'success\']\n\n        # The population may have just been initialized (all entries are\n        # np.inf). If it has you have to calculate the initial energies.\n        # Although this is also done in the evolve generator it\'s possible\n        # that someone can set maxiter=0, at which point we still want the\n        # initial energies to be calculated (the following loop isn\'t run).\n        if np.all(np.isinf(self.population_energies)):\n            self._calculate_population_energies()\n\n        # do the optimisation.\n        for nit in xrange(1, self.maxiter + 1):\n            # evolve the population by a generation\n            try:\n                next(self)\n            except StopIteration:\n                warning_flag = True\n                status_message = _status_message[\'maxfev\']\n                break\n\n            if self.disp:\n                print(""differential_evolution step %d: f(x)= %g""\n                      % (nit,\n                         self.population_energies[0]))\n\n            # should the solver terminate?\n            convergence = self.convergence\n\n            if (self.callback and\n                    self.callback(self._scale_parameters(self.population[0]),\n                                  convergence=self.tol / convergence) is True):\n\n                warning_flag = True\n                status_message = (\'callback function requested stop early \'\n                                  \'by returning True\')\n                break\n\n            intol = (np.std(self.population_energies) <=\n                     self.atol +\n                     self.tol * np.abs(np.mean(self.population_energies)))\n            if warning_flag or intol:\n                break\n\n        else:\n            status_message = _status_message[\'maxiter\']\n            warning_flag = True\n\n        DE_result = OptimizeResult(\n            x=self.x,\n            fun=self.population_energies[0],\n            nfev=self._nfev,\n            nit=nit,\n            message=status_message,\n            success=(warning_flag is not True))\n\n        if self.polish:\n            result = minimize(self.func,\n                              np.copy(DE_result.x),\n                              method=\'L-BFGS-B\',\n                              bounds=self.limits.T,\n                              args=self.args)\n\n            self._nfev += result.nfev\n            DE_result.nfev = self._nfev\n\n            if result.fun < DE_result.fun:\n                DE_result.fun = result.fun\n                DE_result.x = result.x\n                DE_result.jac = result.jac\n                # to keep internal state consistent\n                self.population_energies[0] = result.fun\n                self.population[0] = self._unscale_parameters(result.x)\n\n        return DE_result\n\n    def _calculate_population_energies(self):\n        """"""\n        Calculate the energies of all the population members at the same time.\n        Puts the best member in first place. Useful if the population has just\n        been initialised.\n        """"""\n\n        ##############\n        ## CHANGES: self.func operates on the entire parameters array\n        ##############\n        itersize = max(0, min(len(self.population), self.maxfun - self._nfev + 1))\n        candidates = self.population[:itersize]\n        parameters = np.array([self._scale_parameters(c) for c in candidates]) # TODO: can be vectorized\n        energies = self.func(parameters, *self.args)\n        self.population_energies = energies\n        self._nfev += itersize\n\n        # for index, candidate in enumerate(self.population):\n        #     if self._nfev > self.maxfun:\n        #         break\n\n        #     parameters = self._scale_parameters(candidate)\n        #     self.population_energies[index] = self.func(parameters,\n        #                                                 *self.args)\n        #     self._nfev += 1\n\n        ##############\n        ##############\n\n        \n\n        minval = np.argmin(self.population_energies)\n\n        # put the lowest energy into the best solution position.\n        lowest_energy = self.population_energies[minval]\n        self.population_energies[minval] = self.population_energies[0]\n        self.population_energies[0] = lowest_energy\n\n        self.population[[0, minval], :] = self.population[[minval, 0], :]\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        """"""\n        Evolve the population by a single generation\n        Returns\n        -------\n        x : ndarray\n            The best solution from the solver.\n        fun : float\n            Value of objective function obtained from the best solution.\n        """"""\n        # the population may have just been initialized (all entries are\n        # np.inf). If it has you have to calculate the initial energies\n        if np.all(np.isinf(self.population_energies)):\n            self._calculate_population_energies()\n\n        if self.dither is not None:\n            self.scale = (self.random_number_generator.rand()\n                          * (self.dither[1] - self.dither[0]) + self.dither[0])\n\n        ##############\n        ## CHANGES: self.func operates on the entire parameters array\n        ##############\n\n        itersize = max(0, min(self.num_population_members, self.maxfun - self._nfev + 1))\n        trials = np.array([self._mutate(c) for c in range(itersize)]) # TODO: can be vectorized\n        for trial in trials: self._ensure_constraint(trial)\n        parameters = np.array([self._scale_parameters(trial) for trial in trials])\n        energies = self.func(parameters, *self.args)\n        self._nfev += itersize\n\n        for candidate,(energy,trial) in enumerate(zip(energies, trials)):\n            # if the energy of the trial candidate is lower than the\n            # original population member then replace it\n            if energy < self.population_energies[candidate]:\n                self.population[candidate] = trial\n                self.population_energies[candidate] = energy\n\n                # if the trial candidate also has a lower energy than the\n                # best solution then replace that as well\n                if energy < self.population_energies[0]:\n                    self.population_energies[0] = energy\n                    self.population[0] = trial\n\n        # for candidate in range(self.num_population_members):\n        #     if self._nfev > self.maxfun:\n        #         raise StopIteration\n\n        #     # create a trial solution\n        #     trial = self._mutate(candidate)\n\n        #     # ensuring that it\'s in the range [0, 1)\n        #     self._ensure_constraint(trial)\n\n        #     # scale from [0, 1) to the actual parameter value\n        #     parameters = self._scale_parameters(trial)\n\n        #     # determine the energy of the objective function\n        #     energy = self.func(parameters, *self.args)\n        #     self._nfev += 1\n\n        #     # if the energy of the trial candidate is lower than the\n        #     # original population member then replace it\n        #     if energy < self.population_energies[candidate]:\n        #         self.population[candidate] = trial\n        #         self.population_energies[candidate] = energy\n\n        #         # if the trial candidate also has a lower energy than the\n        #         # best solution then replace that as well\n        #         if energy < self.population_energies[0]:\n        #             self.population_energies[0] = energy\n        #             self.population[0] = trial\n\n        ##############\n        ##############\n\n        return self.x, self.population_energies[0]\n\n    def next(self):\n        """"""\n        Evolve the population by a single generation\n        Returns\n        -------\n        x : ndarray\n            The best solution from the solver.\n        fun : float\n            Value of objective function obtained from the best solution.\n        """"""\n        # next() is required for compatibility with Python2.7.\n        return self.__next__()\n\n    def _scale_parameters(self, trial):\n        """"""\n        scale from a number between 0 and 1 to parameters.\n        """"""\n        return self.__scale_arg1 + (trial - 0.5) * self.__scale_arg2\n\n    def _unscale_parameters(self, parameters):\n        """"""\n        scale from parameters to a number between 0 and 1.\n        """"""\n        return (parameters - self.__scale_arg1) / self.__scale_arg2 + 0.5\n\n    def _ensure_constraint(self, trial):\n        """"""\n        make sure the parameters lie between the limits\n        """"""\n        for index in np.where((trial < 0) | (trial > 1))[0]:\n            trial[index] = self.random_number_generator.rand()\n\n    def _mutate(self, candidate):\n        """"""\n        create a trial vector based on a mutation strategy\n        """"""\n        trial = np.copy(self.population[candidate])\n\n        rng = self.random_number_generator\n\n        fill_point = rng.randint(0, self.parameter_count)\n\n        if self.strategy in [\'currenttobest1exp\', \'currenttobest1bin\']:\n            bprime = self.mutation_func(candidate,\n                                        self._select_samples(candidate, 5))\n        else:\n            bprime = self.mutation_func(self._select_samples(candidate, 5))\n\n        if self.strategy in self._binomial:\n            crossovers = rng.rand(self.parameter_count)\n            crossovers = crossovers < self.cross_over_probability\n            # the last one is always from the bprime vector for binomial\n            # If you fill in modulo with a loop you have to set the last one to\n            # true. If you don\'t use a loop then you can have any random entry\n            # be True.\n            crossovers[fill_point] = True\n            trial = np.where(crossovers, bprime, trial)\n            return trial\n\n        elif self.strategy in self._exponential:\n            i = 0\n            while (i < self.parameter_count and\n                   rng.rand() < self.cross_over_probability):\n\n                trial[fill_point] = bprime[fill_point]\n                fill_point = (fill_point + 1) % self.parameter_count\n                i += 1\n\n            return trial\n\n    def _best1(self, samples):\n        """"""\n        best1bin, best1exp\n        """"""\n        r0, r1 = samples[:2]\n        return (self.population[0] + self.scale *\n                (self.population[r0] - self.population[r1]))\n\n    def _rand1(self, samples):\n        """"""\n        rand1bin, rand1exp\n        """"""\n        r0, r1, r2 = samples[:3]\n        return (self.population[r0] + self.scale *\n                (self.population[r1] - self.population[r2]))\n\n    def _randtobest1(self, samples):\n        """"""\n        randtobest1bin, randtobest1exp\n        """"""\n        r0, r1, r2 = samples[:3]\n        bprime = np.copy(self.population[r0])\n        bprime += self.scale * (self.population[0] - bprime)\n        bprime += self.scale * (self.population[r1] -\n                                self.population[r2])\n        return bprime\n\n    def _currenttobest1(self, candidate, samples):\n        """"""\n        currenttobest1bin, currenttobest1exp\n        """"""\n        r0, r1 = samples[:2]\n        bprime = (self.population[candidate] + self.scale * \n                  (self.population[0] - self.population[candidate] +\n                   self.population[r0] - self.population[r1]))\n        return bprime\n\n    def _best2(self, samples):\n        """"""\n        best2bin, best2exp\n        """"""\n        r0, r1, r2, r3 = samples[:4]\n        bprime = (self.population[0] + self.scale *\n                  (self.population[r0] + self.population[r1] -\n                   self.population[r2] - self.population[r3]))\n\n        return bprime\n\n    def _rand2(self, samples):\n        """"""\n        rand2bin, rand2exp\n        """"""\n        r0, r1, r2, r3, r4 = samples\n        bprime = (self.population[r0] + self.scale *\n                  (self.population[r1] + self.population[r2] -\n                   self.population[r3] - self.population[r4]))\n\n        return bprime\n\n    def _select_samples(self, candidate, number_samples):\n        """"""\n        obtain random integers from range(self.num_population_members),\n        without replacement.  You can\'t have the original candidate either.\n        """"""\n        idxs = list(range(self.num_population_members))\n        idxs.remove(candidate)\n        self.random_number_generator.shuffle(idxs)\n        idxs = idxs[:number_samples]\n        return idxs\n'"
helper.py,0,"b'# CIFAR - 10\n\nimport pickle\nimport numpy as np\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport requests\nfrom tqdm import tqdm\n\n\ndef perturb_image(xs, img):\n    # If this function is passed just one perturbation vector,\n    # pack it in a list to keep the computation the same\n    if xs.ndim < 2:\n        xs = np.array([xs])\n\n    # Copy the image n == len(xs) times so that we can \n    # create n new perturbed images\n    tile = [len(xs)] + [1] * (xs.ndim + 1)\n    imgs = np.tile(img, tile)\n\n    # Make sure to floor the members of xs as int types\n    xs = xs.astype(int)\n\n    for x, img in zip(xs, imgs):\n        # Split x into an array of 5-tuples (perturbation pixels)\n        # i.e., [[x,y,r,g,b], ...]\n        pixels = np.split(x, len(x) // 5)\n        for pixel in pixels:\n            # At each pixel\'s x,y position, assign its rgb value\n            x_pos, y_pos, *rgb = pixel\n            img[x_pos, y_pos] = rgb\n\n    return imgs\n\n\ndef plot_image(image, label_true=None, class_names=None, label_pred=None):\n    if image.ndim == 4 and image.shape[0] == 1:\n        image = image[0]\n\n    plt.grid()\n    plt.imshow(image.astype(np.uint8))\n\n    # Show true and predicted classes\n    if label_true is not None and class_names is not None:\n        labels_true_name = class_names[label_true]\n        if label_pred is None:\n            xlabel = ""True: "" + labels_true_name\n        else:\n            # Name of the predicted class\n            labels_pred_name = class_names[label_pred]\n\n            xlabel = ""True: "" + labels_true_name + ""\\nPredicted: "" + labels_pred_name\n\n        # Show the class on the x-axis\n        plt.xlabel(xlabel)\n\n    plt.xticks([])  # Remove ticks from the plot\n    plt.yticks([])\n    plt.show()  # Show the plot\n\n\ndef plot_images(images, labels_true, class_names, labels_pred=None,\n                confidence=None, titles=None):\n    assert len(images) == len(labels_true)\n\n    # Create a figure with sub-plots\n    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n\n    # Adjust the vertical spacing\n    hspace = 0.2\n    if labels_pred is not None:\n        hspace += 0.2\n    if titles is not None:\n        hspace += 0.2\n\n    fig.subplots_adjust(hspace=hspace, wspace=0.0)\n\n    for i, ax in enumerate(axes.flat):\n        # Fix crash when less than 9 images\n        if i < len(images):\n            # Plot the image\n            ax.imshow(images[i])\n\n            # Name of the true class\n            labels_true_name = class_names[labels_true[i]]\n\n            # Show true and predicted classes\n            if labels_pred is None:\n                xlabel = ""True: "" + labels_true_name\n            else:\n                # Name of the predicted class\n                labels_pred_name = class_names[labels_pred[i]]\n\n                xlabel = ""True: "" + labels_true_name + ""\\nPred: "" + labels_pred_name\n                if (confidence is not None):\n                    xlabel += "" ("" + ""{0:.1f}"".format(confidence[i] * 100) + ""%)""\n\n            # Show the class on the x-axis\n            ax.set_xlabel(xlabel)\n\n            if titles is not None:\n                ax.set_title(titles[i])\n\n        # Remove ticks from the plot\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    # Show the plot\n    plt.show()\n\n\ndef plot_model(model_details):\n    # Create sub-plots\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Summarize history for accuracy\n    axs[0].plot(range(1, len(model_details.history[\'acc\']) + 1), model_details.history[\'acc\'])\n    axs[0].plot(range(1, len(model_details.history[\'val_acc\']) + 1), model_details.history[\'val_acc\'])\n    axs[0].set_title(\'Model Accuracy\')\n    axs[0].set_ylabel(\'Accuracy\')\n    axs[0].set_xlabel(\'Epoch\')\n    axs[0].set_xticks(np.arange(1, len(model_details.history[\'acc\']) + 1), len(model_details.history[\'acc\']) / 10)\n    axs[0].legend([\'train\', \'val\'], loc=\'best\')\n\n    # Summarize history for loss\n    axs[1].plot(range(1, len(model_details.history[\'loss\']) + 1), model_details.history[\'loss\'])\n    axs[1].plot(range(1, len(model_details.history[\'val_loss\']) + 1), model_details.history[\'val_loss\'])\n    axs[1].set_title(\'Model Loss\')\n    axs[1].set_ylabel(\'Loss\')\n    axs[1].set_xlabel(\'Epoch\')\n    axs[1].set_xticks(np.arange(1, len(model_details.history[\'loss\']) + 1), len(model_details.history[\'loss\']) / 10)\n    axs[1].legend([\'train\', \'val\'], loc=\'best\')\n\n    # Show the plot\n    plt.show()\n\n\ndef visualize_attack(df, class_names):\n    _, (x_test, _) = cifar10.load_data()\n\n    results = df[df.success].sample(9)\n\n    z = zip(results.perturbation, x_test[results.image])\n    images = np.array([perturb_image(p, img)[0]\n                       for p, img in z])\n\n    labels_true = np.array(results.true)\n    labels_pred = np.array(results.predicted)\n    titles = np.array(results.model)\n\n    # Plot the first 9 images.\n    plot_images(images=images,\n                labels_true=labels_true,\n                class_names=class_names,\n                labels_pred=labels_pred,\n                titles=titles)\n\n\ndef attack_stats(df, models, network_stats):\n    stats = []\n    for model in models:\n        val_accuracy = np.array(network_stats[network_stats.name == model.name].accuracy)[0]\n        m_result = df[df.model == model.name]\n        pixels = list(set(m_result.pixels))\n\n        for pixel in pixels:\n            p_result = m_result[m_result.pixels == pixel]\n            success_rate = len(p_result[p_result.success]) / len(p_result)\n            stats.append([model.name, val_accuracy, pixel, success_rate])\n\n    return pd.DataFrame(stats, columns=[\'model\', \'accuracy\', \'pixels\', \'attack_success_rate\'])\n\n\ndef evaluate_models(models, x_test, y_test):\n    correct_imgs = []\n    network_stats = []\n    for model in models:\n        print(\'Evaluating\', model.name)\n\n        predictions = model.predict(x_test)\n\n        correct = [[model.name, i, label, np.max(pred), pred]\n                   for i, (label, pred)\n                   in enumerate(zip(y_test[:, 0], predictions))\n                   if label == np.argmax(pred)]\n        accuracy = len(correct) / len(x_test)\n\n        correct_imgs += correct\n        network_stats += [[model.name, accuracy, model.count_params()]]\n    return network_stats, correct_imgs\n\n\ndef load_results():\n    with open(\'networks/results/untargeted_results.pkl\', \'rb\') as file:\n        untargeted = pickle.load(file)\n    with open(\'networks/results/targeted_results.pkl\', \'rb\') as file:\n        targeted = pickle.load(file)\n    return untargeted, targeted\n\n\ndef checkpoint(results, targeted=False):\n    filename = \'targeted\' if targeted else \'untargeted\'\n\n    with open(\'networks/results/\' + filename + \'_results.pkl\', \'wb\') as file:\n        pickle.dump(results, file)\n\n\ndef download_from_url(url, dst):\n    """"""\n    @param: url to download file\n    @param: dst place to put the file\n    """"""\n    # Streaming, so we can iterate over the response.\n    r = requests.get(url, stream=True)\n\n    with open(dst, \'wb\') as f:\n        for data in tqdm(r.iter_content(), unit=\'B\', unit_scale=True):\n            f.write(data)\n\n# def load_imagenet():\n#     with open(\'data/imagenet_class_index.json\', \'r\') as f:\n#         class_names = json.load(f)\n#     class_names = pd.DataFrame([[i,wid,name] for i,(wid,name) in class_names.items()], columns=[\'id\', \'wid\', \'text\'])\n\n#     wid_to_id = {wid:int(i) for i,wid in class_names[[\'id\', \'wid\']].as_matrix()}\n\n#     imagenet_urls = pd.read_csv(\'data/imagenet_urls.txt\', delimiter=\'\\t\', names=[\'label\', \'url\'])\n#     imagenet_urls[\'label\'], imagenet_urls[\'id\'] = zip(*imagenet_urls.label.apply(lambda x: x.split(\'_\')))\n#     imagenet_urls.label = imagenet_urls.label.apply(lambda wid: wid_to_id[wid])\n'"
train.py,0,"b""#!/usr/bin/env python3\n\nimport os\nimport argparse\n\nfrom networks.lenet import LeNet\nfrom networks.pure_cnn import PureCnn\nfrom networks.network_in_network import NetworkInNetwork\nfrom networks.resnet import ResNet\nfrom networks.densenet import DenseNet\nfrom networks.wide_resnet import WideResNet\nfrom networks.capsnet import CapsNet\n\nif __name__ == '__main__':\n    models = {\n        'lenet': LeNet,\n        'pure_cnn': PureCnn,\n        'net_in_net': NetworkInNetwork,\n        'resnet': ResNet,\n        'densenet': DenseNet,\n        'wide_resnet': WideResNet,\n        'capsnet': CapsNet\n    }\n\n    parser = argparse.ArgumentParser(description='Train models on Cifar10')\n    parser.add_argument('--model', choices=models.keys(), required=True, help='Specify a model by name to train.')\n\n    parser.add_argument('--epochs', default=None, type=int)\n    parser.add_argument('--batch_size', default=None, type=int)\n\n    args = parser.parse_args()\n    model_name = args.model\n\n    args = {k: v for k, v in vars(args).items() if v != None}\n    del args['model']\n\n    model = models[model_name](**args, load_weights=False)\n\n    model.train()\n"""
networks/capsnet.py,0,"b""from keras.utils import to_categorical\nimport numpy as np\n\nfrom networks.capsulenet.capsule_net import CapsNetv1, train as train_net\n\n# Capsule Network taken from https://github.com/theblackcat102/dynamic-routing-capsule-cifar\nclass CapsNet:\n    def __init__(self, epochs=200, batch_size=128, load_weights=True):\n        self.name               = 'capsnet'\n        self.model_filename     = 'networks/models/capsnet.h5'\n        self.num_classes        = 10\n        self.input_shape        = 32, 32, 3\n        self.num_routes         = 3\n        self.batch_size         = batch_size\n        self.epochs             = epochs\n\n        self._model = CapsNetv1(input_shape=self.input_shape,\n                        n_class=self.num_classes,\n                        n_route=self.num_routes)\n\n        if load_weights:\n            try:\n                self._model.load_weights(self.model_filename)\n                print('Successfully loaded', self.name)\n            except (ImportError, ValueError, OSError):\n                print('Failed to load', self.name)\n\n    def count_params(self):\n        return self._model.count_params()\n\n    def train(self):\n        self._model = train_net()\n\n    def color_process(self, imgs):\n        if imgs.ndim < 4:\n            imgs = np.array([imgs])\n        imgs = imgs.astype('float32')\n        mean = [125.307, 122.95, 113.865]\n        std  = [62.9932, 62.0887, 66.7048]\n        for img in imgs:\n            for i in range(3):\n                img[:,:,i] = (img[:,:,i] - mean[i]) / std[i]\n        return imgs\n\n    def predict(self, img):\n        label = to_categorical(np.repeat([0], len(img)), self.num_classes) # Don't care what label it is, just needs to be fed into the network\n        processed = self.color_process(img)\n        input_ = [processed, label]\n        pred, _ = self._model.predict(input_, batch_size=self.batch_size)\n        return pred\n    \n    def predict_one(self, img):\n        return self.predict(img)[0]\n"""
networks/densenet.py,0,"b""import keras\r\nimport math\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, AveragePooling2D, GlobalAveragePooling2D\r\nfrom keras.layers import Lambda, concatenate\r\nfrom keras.initializers import he_normal\r\nfrom keras.layers.merge import Concatenate\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\nfrom keras.models import Model, load_model\r\nfrom keras import optimizers\r\nfrom keras import regularizers\r\nfrom keras.utils import plot_model\r\n\r\nfrom networks.train_plot import PlotLearning\r\n\r\n# Code taken from https://github.com/BIGBALLON/cifar-10-cnn\r\nclass DenseNet:\r\n    def __init__(self, epochs=250, batch_size=64, load_weights=True):\r\n        self.name               = 'densenet'\r\n        self.model_filename     = 'networks/models/densenet.h5'\r\n        self.growth_rate        = 12 \r\n        self.depth              = 100\r\n        self.compression        = 0.5\r\n        self.num_classes        = 10\r\n        self.img_rows, self.img_cols = 32, 32\r\n        self.img_channels       = 3\r\n        self.batch_size         = batch_size\r\n        self.epochs             = epochs\r\n        self.iterations         = 782\r\n        self.weight_decay       = 0.0001\r\n        self.log_filepath       = r'networks/models/densenet/'\r\n\r\n        if load_weights:\r\n            try:\r\n                self._model = load_model(self.model_filename)\r\n                print('Successfully loaded', self.name)\r\n            except (ImportError, ValueError, OSError):\r\n                print('Failed to load', self.name)\r\n    \r\n    def count_params(self):\r\n        return self._model.count_params()\r\n\r\n    def color_preprocessing(self, x_train,x_test):\r\n        x_train = x_train.astype('float32')\r\n        x_test = x_test.astype('float32')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for i in range(3):\r\n            x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n            x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n        return x_train, x_test\r\n\r\n    def scheduler(self, epoch):\r\n        if epoch <= 75:\r\n            return 0.1\r\n        if epoch <= 150:\r\n            return 0.01\r\n        if epoch <= 210:\r\n            return 0.001\r\n        return 0.0005\r\n\r\n    def densenet(self, img_input,classes_num):\r\n\r\n        def bn_relu(x):\r\n            x = BatchNormalization()(x)\r\n            x = Activation('relu')(x)\r\n            return x\r\n\r\n        def bottleneck(x):\r\n            channels = self.growth_rate * 4\r\n            x = bn_relu(x)\r\n            x = Conv2D(channels,kernel_size=(1,1),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay),use_bias=False)(x)\r\n            x = bn_relu(x)\r\n            x = Conv2D(self.growth_rate,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay),use_bias=False)(x)\r\n            return x\r\n\r\n        def single(x):\r\n            x = bn_relu(x)\r\n            x = Conv2D(self.growth_rate,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay),use_bias=False)(x)\r\n            return x\r\n\r\n        def transition(x, inchannels):\r\n            outchannels = int(inchannels * self.compression)\r\n            x = bn_relu(x)\r\n            x = Conv2D(outchannels,kernel_size=(1,1),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay),use_bias=False)(x)\r\n            x = AveragePooling2D((2,2), strides=(2, 2))(x)\r\n            return x, outchannels\r\n\r\n        def dense_block(x,blocks,nchannels):\r\n            concat = x\r\n            for i in range(blocks):\r\n                x = bottleneck(concat)\r\n                concat = concatenate([x,concat], axis=-1)\r\n                nchannels += self.growth_rate\r\n            return concat, nchannels\r\n\r\n        def dense_layer(x):\r\n            return Dense(classes_num,activation='softmax',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay))(x)\r\n\r\n\r\n        nblocks = (self.depth - 4) // 6 \r\n        nchannels = self.growth_rate * 2\r\n\r\n        x = Conv2D(nchannels,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay),use_bias=False)(img_input)\r\n\r\n        x, nchannels = dense_block(x,nblocks,nchannels)\r\n        x, nchannels = transition(x,nchannels)\r\n        x, nchannels = dense_block(x,nblocks,nchannels)\r\n        x, nchannels = transition(x,nchannels)\r\n        x, nchannels = dense_block(x,nblocks,nchannels)\r\n        x, nchannels = transition(x,nchannels)\r\n        x = bn_relu(x)\r\n        x = GlobalAveragePooling2D()(x)\r\n        x = dense_layer(x)\r\n        return x\r\n\r\n    def train(self):\r\n        # load data\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test  = keras.utils.to_categorical(y_test, self.num_classes)\r\n        x_train = x_train.astype('float32')\r\n        x_test  = x_test.astype('float32')\r\n        \r\n        # color preprocessing\r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        # build network\r\n        img_input = Input(shape=(self.img_rows,self.img_cols,self.img_channels))\r\n        output    = self.densenet(img_input,self.num_classes)\r\n        model     = Model(img_input, output)\r\n        model.summary()\r\n        \r\n        # plot_model(model, show_shapes=True, to_file='model.png')\r\n\r\n        # set optimizer\r\n        sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n\r\n        # set callback\r\n        tb_cb     = TensorBoard(log_dir=self.log_filepath, histogram_freq=0)\r\n        change_lr = LearningRateScheduler(self.scheduler)\r\n        ckpt      = ModelCheckpoint(self.model_filename, monitor='val_loss', verbose=0, save_best_only= True, mode='auto')\r\n        plot_callback = PlotLearning()\r\n        cbks      = [change_lr,tb_cb,ckpt, plot_callback]\r\n\r\n        # set data augmentation\r\n        print('Using real-time data augmentation.')\r\n        datagen   = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\r\n\r\n        datagen.fit(x_train)\r\n\r\n        # start training\r\n        model.fit_generator(datagen.flow(x_train, y_train,batch_size=self.batch_size), steps_per_epoch=self.iterations, epochs=self.epochs, callbacks=cbks,validation_data=(x_test, y_test))\r\n        model.save(self.model_filename)\r\n\r\n        self._model = model\r\n        self.param_count = self._model.count_params()\r\n\r\n    def color_process(self, imgs):\r\n        if imgs.ndim < 4:\r\n            imgs = np.array([imgs])\r\n        imgs = imgs.astype('float32')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for img in imgs:\r\n            for i in range(3):\r\n                img[:,:,i] = (img[:,:,i] - mean[i]) / std[i]\r\n        return imgs\r\n\r\n    def predict(self, img):\r\n        processed = self.color_process(img)\r\n        return self._model.predict(processed, batch_size=self.batch_size)\r\n    \r\n    def predict_one(self, img):\r\n        return self.predict(img)[0]\r\n\r\n    def accuracy(self):\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n        \r\n        # color preprocessing\r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        return self._model.evaluate(x_test, y_test, verbose=0)[1]"""
networks/lenet.py,0,"b""import keras\nimport numpy as np\nfrom keras import optimizers\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.regularizers import l2\n\nfrom networks.train_plot import PlotLearning\n\n# Code taken from https://github.com/BIGBALLON/cifar-10-cnn\nclass LeNet:\n    def __init__(self, epochs=200, batch_size=128, load_weights=True):\n        self.name               = 'lenet'\n        self.model_filename     = 'networks/models/lenet.h5'\n        self.num_classes        = 10\n        self.input_shape        = 32, 32, 3\n        self.batch_size         = batch_size\n        self.epochs             = epochs\n        self.iterations         = 391\n        self.weight_decay       = 0.0001\n        self.log_filepath       = r'networks/models/lenet/'\n\n        if load_weights:\n            try:\n                self._model = load_model(self.model_filename)\n                print('Successfully loaded', self.name)\n            except (ImportError, ValueError, OSError):\n                print('Failed to load', self.name)\n    \n    def count_params(self):\n        return self._model.count_params()\n\n    def color_preprocessing(self, x_train, x_test):\n        x_train = x_train.astype('float32')\n        x_test = x_test.astype('float32')\n        mean = [125.307, 122.95, 113.865]\n        std  = [62.9932, 62.0887, 66.7048]\n        for i in range(3):\n            x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n            x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n        return x_train, x_test\n\n    def build_model(self):\n        model = Sequential()\n        model.add(Conv2D(6, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(self.weight_decay), input_shape=self.input_shape))\n        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n        model.add(Conv2D(16, (5, 5), padding='valid', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(self.weight_decay)))\n        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n        model.add(Flatten())\n        model.add(Dense(120, activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(self.weight_decay) ))\n        model.add(Dense(84, activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=l2(self.weight_decay) ))\n        model.add(Dense(10, activation = 'softmax', kernel_initializer='he_normal', kernel_regularizer=l2(self.weight_decay) ))\n        sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n        return model\n\n    def scheduler(self, epoch):\n        if epoch <= 60:\n            return 0.05\n        if epoch <= 120:\n            return 0.01\n        if epoch <= 160:    \n            return 0.002\n        return 0.0004\n\n    def train(self):\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n        \n        # color preprocessing\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\n\n        # build network\n        model = self.build_model()\n        model.summary()\n\n        # Save the best model during each training checkpoint\n        checkpoint = ModelCheckpoint(self.model_filename,\n                                    monitor='val_loss', \n                                    verbose=0,\n                                    save_best_only= True,\n                                    mode='auto')\n        plot_callback = PlotLearning()\n        tb_cb = TensorBoard(log_dir=self.log_filepath, histogram_freq=0)\n\n        cbks = [checkpoint, plot_callback, tb_cb]\n\n        # using real-time data augmentation\n        print('Using real-time data augmentation.')\n        datagen = ImageDataGenerator(horizontal_flip=True,\n                width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n\n        datagen.fit(x_train)\n\n        # start traing \n        model.fit_generator(datagen.flow(x_train, y_train,batch_size=self.batch_size),\n                            steps_per_epoch=self.iterations,\n                            epochs=self.epochs,\n                            callbacks=cbks,\n                            validation_data=(x_test, y_test))\n        # save model\n        model.save(self.model_filename)\n\n        self._model = model\n\n    def color_process(self, imgs):\n        if imgs.ndim < 4:\n            imgs = np.array([imgs])\n        imgs = imgs.astype('float32')\n        mean = [125.307, 122.95, 113.865]\n        std  = [62.9932, 62.0887, 66.7048]\n        for img in imgs:\n            for i in range(3):\n                img[:,:,i] = (img[:,:,i] - mean[i]) / std[i]\n        return imgs\n\n    def predict(self, img):\n        processed = self.color_process(img)\n        return self._model.predict(processed, batch_size=self.batch_size)\n    \n    def predict_one(self, img):\n        return self.predict(img)[0]\n\n    def accuracy(self):\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n\n        # color preprocessing\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\n\n        return self._model.evaluate(x_test, y_test, verbose=0)[1]\n\n\n\n"""
networks/network_in_network.py,0,"b'import keras\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential, load_model\r\nfrom keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\r\nfrom keras.initializers import RandomNormal  \r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras import optimizers\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\n\r\nfrom networks.train_plot import PlotLearning\r\n\r\n# Code taken from https://github.com/BIGBALLON/cifar-10-cnn\r\nclass NetworkInNetwork:\r\n    def __init__(self, epochs=200, batch_size=128, load_weights=True):\r\n        self.name               = \'net_in_net\'\r\n        self.model_filename     = \'networks/models/net_in_net.h5\'\r\n        self.num_classes        = 10\r\n        self.input_shape        = 32, 32, 3\r\n        self.batch_size         = batch_size\r\n        self.epochs             = epochs\r\n        self.iterations         = 391\r\n        self.weight_decay       = 0.0001\r\n        self.dropout            = 0.5\r\n        self.log_filepath       = r\'networks/models/net_in_net/\'\r\n\r\n        if load_weights:\r\n            try:\r\n                self._model = load_model(self.model_filename)\r\n                print(\'Successfully loaded\', self.name)\r\n            except (ImportError, ValueError, OSError):\r\n                print(\'Failed to load\', self.name)\r\n    \r\n    def count_params(self):\r\n        return self._model.count_params()\r\n\r\n    def color_preprocessing(self, x_train,x_test):\r\n        x_train = x_train.astype(\'float32\')\r\n        x_test = x_test.astype(\'float32\')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for i in range(3):\r\n            x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n            x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n        return x_train, x_test\r\n\r\n    def scheduler(self, epoch):\r\n        if epoch <= 60:\r\n            return 0.05\r\n        if epoch <= 120:\r\n            return 0.01\r\n        if epoch <= 160:    \r\n            return 0.002\r\n        return 0.0004\r\n\r\n    def build_model(self):\r\n        model = Sequential()\r\n\r\n        model.add(Conv2D(192, (5, 5), padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal"", input_shape=self.input_shape))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(Conv2D(160, (1, 1), padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(Conv2D(96, (1, 1), padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = \'same\'))\r\n        \r\n        model.add(Dropout(self.dropout))\r\n        \r\n        model.add(Conv2D(192, (5, 5), padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(Conv2D(192, (1, 1),padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(Conv2D(192, (1, 1),padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = \'same\'))\r\n        \r\n        model.add(Dropout(self.dropout))\r\n        \r\n        model.add(Conv2D(192, (3, 3), padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(Conv2D(192, (1, 1), padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        model.add(Conv2D(10, (1, 1), padding=\'same\', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=""he_normal""))\r\n        model.add(BatchNormalization())\r\n        model.add(Activation(\'relu\'))\r\n        \r\n        model.add(GlobalAveragePooling2D())\r\n        model.add(Activation(\'softmax\'))\r\n        \r\n        sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n        model.compile(loss=\'categorical_crossentropy\', optimizer=sgd, metrics=[\'accuracy\'])\r\n        return model\r\n\r\n    def train(self):\r\n        # load data\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n        \r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        # build network\r\n        model = self.build_model()\r\n        model.summary()\r\n\r\n        # Save the best model during each training checkpoint\r\n        checkpoint = ModelCheckpoint(self.model_filename,\r\n                                    monitor=\'val_loss\', \r\n                                    verbose=0,\r\n                                    save_best_only= True,\r\n                                    mode=\'auto\')\r\n        plot_callback = PlotLearning()\r\n        tb_cb = TensorBoard(log_dir=self.log_filepath, histogram_freq=0)\r\n\r\n        cbks = [checkpoint, plot_callback, tb_cb]\r\n\r\n        # set data augmentation\r\n        print(\'Using real-time data augmentation.\')\r\n        datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode=\'constant\',cval=0.)\r\n        datagen.fit(x_train)\r\n\r\n        # start training\r\n        model.fit_generator(datagen.flow(x_train, y_train,batch_size=self.batch_size),steps_per_epoch=self.iterations,epochs=self.epochs,callbacks=cbks,validation_data=(x_test, y_test))\r\n        \r\n        model.save(self.model_filename)\r\n\r\n        self._model = model\r\n\r\n    def color_process(self, imgs):\r\n        if imgs.ndim < 4:\r\n            imgs = np.array([imgs])\r\n        imgs = imgs.astype(\'float32\')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for img in imgs:\r\n            for i in range(3):\r\n                img[:,:,i] = (img[:,:,i] - mean[i]) / std[i]\r\n        return imgs\r\n\r\n    def predict(self, img):\r\n        processed = self.color_process(img)\r\n        return self._model.predict(processed, batch_size=self.batch_size)\r\n    \r\n    def predict_one(self, img):\r\n        return self.predict(img)[0]\r\n\r\n    def accuracy(self):\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n\r\n        # color preprocessing\r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        return self._model.evaluate(x_test, y_test, verbose=0)[1]\r\n'"
networks/pure_cnn.py,0,"b""import numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport keras\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam, SGD\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\nfrom keras.constraints import maxnorm\nfrom keras.models import load_model\nfrom keras.layers import GlobalAveragePooling2D, Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.datasets import cifar10\n\nfrom networks.train_plot import PlotLearning\n\n# A pure CNN model from https://arxiv.org/pdf/1412.6806.pdf\n# Code taken from https://github.com/09rohanchopra/cifar10\nclass PureCnn:\n    def __init__(self, epochs=350, batch_size=128, load_weights=True):\n        self.name               = 'pure_cnn'\n        self.model_filename     = 'networks/models/pure_cnn.h5'\n        self.num_classes        = 10\n        self.input_shape        = 32, 32, 3\n        self.batch_size         = batch_size\n        self.epochs             = epochs\n        self.learn_rate         = 1.0e-4\n        self.log_filepath       = r'networks/models/pure_cnn/'\n\n        if load_weights:\n            try:\n                self._model = load_model(self.model_filename)\n                print('Successfully loaded', self.name)\n            except (ImportError, ValueError, OSError):\n                print('Failed to load', self.name)\n    \n    def count_params(self):\n        return self._model.count_params()\n        \n    def color_preprocessing(self, x_train, x_test):\n        x_train = x_train.astype('float32')\n        x_test = x_test.astype('float32')\n        mean = [125.307, 122.95, 113.865]\n        std  = [62.9932, 62.0887, 66.7048]\n        for i in range(3):\n            x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n            x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n        return x_train, x_test\n\n    def pure_cnn_network(self, input_shape):\n        model = Sequential()\n        \n        model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', input_shape=input_shape))    \n        model.add(Dropout(0.2))\n        \n        model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same'))  \n        model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2))    \n        model.add(Dropout(0.5))\n        \n        model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))    \n        model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n        model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2))    \n        model.add(Dropout(0.5))    \n        \n        model.add(Conv2D(192, (3, 3), padding = 'same'))\n        model.add(Activation('relu'))\n        model.add(Conv2D(192, (1, 1),padding='valid'))\n        model.add(Activation('relu'))\n        model.add(Conv2D(10, (1, 1), padding='valid'))\n\n        model.add(GlobalAveragePooling2D())\n        \n        model.add(Activation('softmax'))\n\n        return model\n    \n    def train(self):\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n        \n        # color preprocessing\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\n\n        model = self.pure_cnn_network(self.input_shape)\n        model.summary()\n\n        # Save the best model during each training checkpoint\n        checkpoint = ModelCheckpoint(self.model_filename,\n                                    monitor='val_loss', \n                                    verbose=0,\n                                    save_best_only= True,\n                                    mode='auto')\n        plot_callback = PlotLearning()\n        tb_cb = TensorBoard(log_dir=self.log_filepath, histogram_freq=0)\n\n        cbks = [checkpoint, plot_callback, tb_cb]\n\n        # set data augmentation\n        print('Using real-time data augmentation.')\n        datagen = ImageDataGenerator(horizontal_flip=True,\n                width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n\n        datagen.fit(x_train)\n\n        model.compile(loss='categorical_crossentropy', # Better loss function for neural networks\n                    optimizer=Adam(lr=self.learn_rate), # Adam optimizer with 1.0e-4 learning rate\n                    metrics = ['accuracy']) # Metrics to be evaluated by the model\n\n        model.fit_generator(datagen.flow(x_train, y_train, batch_size = self.batch_size),\n                            epochs = self.epochs,\n                            validation_data= (x_test, y_test),\n                            callbacks=cbks,\n                            verbose=1)\n\n        model.save(self.model_filename)\n\n        self._model = model\n\n    def color_process(self, imgs):\n        if imgs.ndim < 4:\n            imgs = np.array([imgs])\n        imgs = imgs.astype('float32')\n        mean = [125.307, 122.95, 113.865]\n        std  = [62.9932, 62.0887, 66.7048]\n        for img in imgs:\n            for i in range(3):\n                img[:,:,i] = (img[:,:,i] - mean[i]) / std[i]\n        return imgs\n\n    def predict(self, img):\n        processed = self.color_process(img)\n        return self._model.predict(processed, batch_size=self.batch_size)\n    \n    def predict_one(self, img):\n        return self.predict(img)[0]\n\n    def accuracy(self):\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n\n        # color preprocessing\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\n\n        return self._model.evaluate(x_test, y_test, verbose=0)[1]"""
networks/resnet.py,0,"b'import keras\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\nfrom keras.models import Model, load_model\r\nfrom keras import optimizers, regularizers\r\n\r\nfrom networks.train_plot import PlotLearning\r\n\r\n# Code taken from https://github.com/BIGBALLON/cifar-10-cnn\r\nclass ResNet:\r\n    def __init__(self, epochs=200, batch_size=128, load_weights=True):\r\n        self.name               = \'resnet\'\r\n        self.model_filename     = \'networks/models/resnet.h5\'\r\n        \r\n        self.stack_n            = 5    \r\n        self.num_classes        = 10\r\n        self.img_rows, self.img_cols = 32, 32\r\n        self.img_channels       = 3\r\n        self.batch_size         = batch_size\r\n        self.epochs             = epochs\r\n        self.iterations         = 50000 // self.batch_size\r\n        self.weight_decay       = 0.0001\r\n        self.log_filepath       = r\'networks/models/resnet/\'\r\n\r\n        if load_weights:\r\n            try:\r\n                self._model = load_model(self.model_filename)\r\n                print(\'Successfully loaded\', self.name)\r\n            except (ImportError, ValueError, OSError):\r\n                print(\'Failed to load\', self.name)\r\n    \r\n    def count_params(self):\r\n        return self._model.count_params()\r\n\r\n    def color_preprocessing(self, x_train,x_test):\r\n        x_train = x_train.astype(\'float32\')\r\n        x_test = x_test.astype(\'float32\')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for i in range(3):\r\n            x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n            x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n        return x_train, x_test\r\n\r\n    def scheduler(self, epoch):\r\n        if epoch < 80:\r\n            return 0.1\r\n        if epoch < 150:\r\n            return 0.01\r\n        return 0.001\r\n\r\n    def residual_network(self, img_input,classes_num=10,stack_n=5):\r\n        def residual_block(intput,out_channel,increase=False):\r\n            if increase:\r\n                stride = (2,2)\r\n            else:\r\n                stride = (1,1)\r\n\r\n            pre_bn   = BatchNormalization()(intput)\r\n            pre_relu = Activation(\'relu\')(pre_bn)\r\n\r\n            conv_1 = Conv2D(out_channel,kernel_size=(3,3),strides=stride,padding=\'same\',\r\n                            kernel_initializer=""he_normal"",\r\n                            kernel_regularizer=regularizers.l2(self.weight_decay))(pre_relu)\r\n            bn_1   = BatchNormalization()(conv_1)\r\n            relu1  = Activation(\'relu\')(bn_1)\r\n            conv_2 = Conv2D(out_channel,kernel_size=(3,3),strides=(1,1),padding=\'same\',\r\n                            kernel_initializer=""he_normal"",\r\n                            kernel_regularizer=regularizers.l2(self.weight_decay))(relu1)\r\n            if increase:\r\n                projection = Conv2D(out_channel,\r\n                                    kernel_size=(1,1),\r\n                                    strides=(2,2),\r\n                                    padding=\'same\',\r\n                                    kernel_initializer=""he_normal"",\r\n                                    kernel_regularizer=regularizers.l2(self.weight_decay))(intput)\r\n                block = add([conv_2, projection])\r\n            else:\r\n                block = add([intput,conv_2])\r\n            return block\r\n\r\n        # build model\r\n        # total layers = stack_n * 3 * 2 + 2\r\n        # stack_n = 5 by default, total layers = 32\r\n        # input: 32x32x3 output: 32x32x16\r\n        x = Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),padding=\'same\',\r\n                kernel_initializer=""he_normal"",\r\n                kernel_regularizer=regularizers.l2(self.weight_decay))(img_input)\r\n\r\n        # input: 32x32x16 output: 32x32x16\r\n        for _ in range(stack_n):\r\n            x = residual_block(x,16,False)\r\n\r\n        # input: 32x32x16 output: 16x16x32\r\n        x = residual_block(x,32,True)\r\n        for _ in range(1,stack_n):\r\n            x = residual_block(x,32,False)\r\n        \r\n        # input: 16x16x32 output: 8x8x64\r\n        x = residual_block(x,64,True)\r\n        for _ in range(1,stack_n):\r\n            x = residual_block(x,64,False)\r\n\r\n        x = BatchNormalization()(x)\r\n        x = Activation(\'relu\')(x)\r\n        x = GlobalAveragePooling2D()(x)\r\n\r\n        # input: 64 output: 10\r\n        x = Dense(classes_num,activation=\'softmax\',\r\n                kernel_initializer=""he_normal"",\r\n                kernel_regularizer=regularizers.l2(self.weight_decay))(x)\r\n        return x\r\n\r\n    def train(self):\r\n        # load data\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n        \r\n        # color preprocessing\r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        # build network\r\n        img_input = Input(shape=(self.img_rows,self.img_cols,self.img_channels))\r\n        output    = self.residual_network(img_input,self.num_classes,self.stack_n)\r\n        resnet    = Model(img_input, output)\r\n        resnet.summary()\r\n\r\n        # set optimizer\r\n        sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n        resnet.compile(loss=\'categorical_crossentropy\', optimizer=sgd, metrics=[\'accuracy\'])\r\n\r\n        # set callback\r\n        tb_cb = TensorBoard(log_dir=self.log_filepath, histogram_freq=0)\r\n        change_lr = LearningRateScheduler(self.scheduler)\r\n        checkpoint = ModelCheckpoint(self.model_filename, \r\n                monitor=\'val_loss\', verbose=0, save_best_only= True, mode=\'auto\')\r\n        plot_callback = PlotLearning()\r\n        cbks = [change_lr,tb_cb,checkpoint,plot_callback]\r\n\r\n        # set data augmentation\r\n        print(\'Using real-time data augmentation.\')\r\n        datagen = ImageDataGenerator(horizontal_flip=True,\r\n                                    width_shift_range=0.125,\r\n                                    height_shift_range=0.125,\r\n                                    fill_mode=\'constant\',cval=0.)\r\n\r\n        datagen.fit(x_train)\r\n\r\n        # start training\r\n        resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=self.batch_size),\r\n                            steps_per_epoch=self.iterations,\r\n                            epochs=self.epochs,\r\n                            callbacks=cbks,\r\n                            validation_data=(x_test, y_test))\r\n        resnet.save(self.model_filename)\r\n\r\n        self._model = resnet\r\n        self.param_count = self._model.count_params()\r\n\r\n    def color_process(self, imgs):\r\n        if imgs.ndim < 4:\r\n            imgs = np.array([imgs])\r\n        imgs = imgs.astype(\'float32\')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for img in imgs:\r\n            for i in range(3):\r\n                img[:,:,i] = (img[:,:,i] - mean[i]) / std[i]\r\n        return imgs\r\n\r\n    def predict(self, img):\r\n        processed = self.color_process(img)\r\n        return self._model.predict(processed, batch_size=self.batch_size)\r\n    \r\n    def predict_one(self, img):\r\n        return self.predict(img)[0]\r\n\r\n    def accuracy(self):\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n        \r\n        # color preprocessing\r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        return self._model.evaluate(x_test, y_test, verbose=0)[1]'"
networks/train_plot.py,0,"b'from matplotlib import pyplot as plt\nfrom IPython.display import clear_output\nimport keras\n\n# Live loss plot as the network trains\n# https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\nclass PlotLearning(keras.callbacks.Callback):\n    def __init__(self, clear_on_begin=False):\n        self.clear_on_begin = clear_on_begin\n        self.reset()\n    \n    def on_train_begin(self, logs={}):\n        if (self.clear_on_begin):\n            self.reset()\n    \n    def reset(self):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        self.acc = []\n        self.val_acc = []\n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get(\'loss\'))\n        self.val_losses.append(logs.get(\'val_loss\'))\n        self.acc.append(logs.get(\'acc\'))\n        self.val_acc.append(logs.get(\'val_acc\'))\n        self.i += 1\n        \n        if (self.i < 3):\n            return\n        \n        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n        \n        clear_output(wait=True)\n        \n        ax1.plot(self.x, self.losses, label=""loss"")\n        ax1.plot(self.x, self.val_losses, label=""val_loss"")\n        ax1.set_title(\'Model Loss\')\n        ax1.set_ylabel(\'Loss\')\n        ax1.set_xlabel(\'Epoch\')\n        ax1.legend([\'train\', \'val\'], loc=\'best\')\n        \n        ax2.plot(self.x, self.acc, label=""accuracy"")\n        ax2.plot(self.x, self.val_acc, label=""validation accuracy"")\n        ax2.set_title(\'Model Accuracy\')\n        ax2.set_ylabel(\'Accuracy\')\n        ax2.set_xlabel(\'Epoch\')\n        ax2.legend([\'train\', \'val\'], loc=\'best\')\n        \n        plt.show()'"
networks/vgg16.py,0,"b""# Very deep convolutional model (VGG16) from https://arxiv.org/pdf/1409.1556.pdf\n# Code taken from https://github.com/geifmany/cifar-vgg\n\ndef vgg16_model(input_shape=INPUT_SHAPE):\n    weight_decay = 0.0005\n    \n    model = Sequential()\n\n    model.add(Conv2D(64, (3, 3), padding='same',\n                     input_shape=input_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n\n    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n\n    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n\n    model.add(Flatten())\n    model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n    model.add(Activation('relu'))\n    model.add(BatchNormalization())\n\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n    \n#     model.summary()\n\n    return model"""
networks/wide_resnet.py,0,"b""import keras\r\nimport numpy as np\r\nfrom keras.datasets import cifar10\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D\r\nfrom keras.initializers import he_normal\r\nfrom keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\r\nfrom keras.models import Model, load_model\r\nfrom keras import optimizers\r\nfrom keras import regularizers\r\n\r\nfrom networks.train_plot import PlotLearning\r\n\r\n# Code taken from https://github.com/BIGBALLON/cifar-10-cnn\r\nclass WideResNet:\r\n    def __init__(self, epochs=200, batch_size=128, load_weights=True):\r\n        self.name               = 'wide_resnet'\r\n        self.model_filename     = 'networks/models/wide_resnet.h5'\r\n        \r\n        self.depth              = 16\r\n        self.wide               = 8\r\n        self.num_classes        = 10\r\n        self.img_rows, self.img_cols = 32, 32\r\n        self.img_channels       = 3\r\n        self.batch_size         = batch_size\r\n        self.epochs             = epochs\r\n        self.iterations         = 391\r\n        self.weight_decay       = 0.0005\r\n        self.log_filepath       = r'networks/models/wide_resnet/'\r\n\r\n        if load_weights:\r\n            try:\r\n                self._model = load_model(self.model_filename)\r\n                print('Successfully loaded', self.name)\r\n            except (ImportError, ValueError, OSError):\r\n                print('Failed to load', self.name)\r\n    \r\n    def count_params(self):\r\n        return self._model.count_params()\r\n\r\n    def scheduler(self, epoch):\r\n        if epoch <= 60:\r\n            return 0.1\r\n        if epoch <= 120:\r\n            return 0.02\r\n        if epoch <= 160:\r\n            return 0.004\r\n        return 0.0008\r\n\r\n    def color_preprocessing(self, x_train,x_test):\r\n        x_train = x_train.astype('float32')\r\n        x_test = x_test.astype('float32')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for i in range(3):\r\n            x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\r\n            x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\r\n        return x_train, x_test\r\n\r\n    def wide_residual_network(self, img_input,classes_num,depth,k):\r\n\r\n        print('Wide-Resnet %dx%d' %(depth, k))\r\n        n_filters  = [16, 16*k, 32*k, 64*k]\r\n        n_stack    = (depth - 4) / 6\r\n        in_filters = 16\r\n\r\n        def conv3x3(x,filters):\r\n            return Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1), padding='same',\r\n            kernel_initializer=he_normal(),\r\n            kernel_regularizer=regularizers.l2(self.weight_decay))(x)\r\n\r\n        def residual_block(x,out_filters,increase_filter=False):\r\n            if increase_filter:\r\n                first_stride = (2,2)\r\n            else:\r\n                first_stride = (1,1)\r\n            pre_bn   = BatchNormalization()(x)\r\n            pre_relu = Activation('relu')(pre_bn)\r\n            conv_1 = Conv2D(out_filters,kernel_size=(3,3),strides=first_stride,padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay))(pre_relu)\r\n            bn_1   = BatchNormalization()(conv_1)\r\n            relu1  = Activation('relu')(bn_1)\r\n            conv_2 = Conv2D(out_filters, kernel_size=(3,3), strides=(1,1), padding='same', kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay))(relu1)\r\n            if increase_filter or in_filters != out_filters:\r\n                projection = Conv2D(out_filters,kernel_size=(1,1),strides=first_stride,padding='same',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay))(x)\r\n                block = add([conv_2, projection])\r\n            else:\r\n                block = add([conv_2,x])\r\n            return block\r\n\r\n        def wide_residual_layer(x,out_filters,increase_filter=False):\r\n            x = residual_block(x,out_filters,increase_filter)\r\n            in_filters = out_filters\r\n            for _ in range(1,int(n_stack)):\r\n                x = residual_block(x,out_filters)\r\n            return x\r\n\r\n        x = conv3x3(img_input,n_filters[0])\r\n        x = wide_residual_layer(x,n_filters[1])\r\n        x = wide_residual_layer(x,n_filters[2],increase_filter=True)\r\n        x = wide_residual_layer(x,n_filters[3],increase_filter=True)\r\n        x = BatchNormalization()(x)\r\n        x = Activation('relu')(x)\r\n        x = GlobalAveragePooling2D()(x)\r\n        x = Dense(classes_num,activation='softmax',kernel_initializer=he_normal(),kernel_regularizer=regularizers.l2(self.weight_decay))(x)\r\n        return x\r\n\r\n    def train(self):\r\n        # load data\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n        \r\n        # color preprocessing\r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        # build network\r\n        img_input = Input(shape=(self.img_rows,self.img_cols,self.img_channels))\r\n        output = self.wide_residual_network(img_input,self.num_classes,self.depth,self.wide)\r\n        resnet = Model(img_input, output)\r\n        resnet.summary()\r\n        \r\n        # set optimizer\r\n        sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\r\n        resnet.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n\r\n        # set callback\r\n        tb_cb = TensorBoard(log_dir=self.log_filepath, histogram_freq=0)\r\n        change_lr = LearningRateScheduler(self.scheduler)\r\n        checkpoint = ModelCheckpoint(self.model_filename, \r\n                monitor='val_loss', verbose=0, save_best_only= True, mode='auto')\r\n        plot_callback = PlotLearning()\r\n        cbks = [change_lr,tb_cb,checkpoint,plot_callback]\r\n\r\n        # set data augmentation\r\n        print('Using real-time data augmentation.')\r\n        datagen = ImageDataGenerator(horizontal_flip=True,\r\n                width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\r\n\r\n        datagen.fit(x_train)\r\n\r\n        # start training\r\n        resnet.fit_generator(datagen.flow(x_train, y_train,batch_size=self.batch_size),\r\n                            steps_per_epoch=self.iterations,\r\n                            epochs=self.epochs,\r\n                            callbacks=cbks,\r\n                            validation_data=(x_test, y_test))\r\n        resnet.save(self.model_filename)\r\n\r\n        self._model = resnet\r\n        self.param_count = self._model.count_params()\r\n\r\n    def color_process(self, imgs):\r\n        if imgs.ndim < 4:\r\n            imgs = np.array([imgs])\r\n        imgs = imgs.astype('float32')\r\n        mean = [125.307, 122.95, 113.865]\r\n        std  = [62.9932, 62.0887, 66.7048]\r\n        for img in imgs:\r\n            for i in range(3):\r\n                img[:,:,i] = (img[:,:,i] - mean[i]) / std[i]\r\n        return imgs\r\n\r\n    def predict(self, img):\r\n        processed = self.color_process(img)\r\n        return self._model.predict(processed, batch_size=self.batch_size)\r\n    \r\n    def predict_one(self, img):\r\n        return self.predict(img)[0]\r\n\r\n    def accuracy(self):\r\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n        y_train = keras.utils.to_categorical(y_train, self.num_classes)\r\n        y_test = keras.utils.to_categorical(y_test, self.num_classes)\r\n        \r\n        # color preprocessing\r\n        x_train, x_test = self.color_preprocessing(x_train, x_test)\r\n\r\n        return self._model.evaluate(x_test, y_test, verbose=0)[1]"""
networks/capsulenet/capsule_layers.py,6,"b'""""""\n\nAdded minor tweak to allow user define clip value in Mask layer, other are same as XifengGuo repo code\n\nSome key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, \nnot just on MNIST.\n*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\nuncommenting them and commenting their counterparts.\nAuthor: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n""""""\n\nimport keras.backend as K\nimport tensorflow as tf\nfrom keras import initializers, layers\n\n\nclass Length(layers.Layer):\n    """"""\n    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n    output: shape=[dim_1, ..., dim_{n-1}]\n    """"""\n    def call(self, inputs, **kwargs):\n        return K.sqrt(K.sum(K.square(inputs), -1))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:-1]\n\n\nclass Mask(layers.Layer):\n    """"""\n        Mask Tensor layer by the max value in first axis\n        Input shape: [None,d1,d2]\n        Output shape: [None,d2]\n    """"""\n    clip_value = (0,1)\n\n    def Mask(self,clip_value=(0,1),**kwargs):\n        self.clip_value = clip_value # what if clip value is not 0 and 1?\n\n    def call(self,inputs,**kwargs):\n        if type(inputs) is list:\n            assert len(inputs) == 2\n            inputs,mask = inputs\n        else:\n            x = inputs\n            # enlarge range of values in x by mapping max(new_x) = 1, others \n            x = (x - K.max(x,1,True)) / K.epsilon() + 1\n            mask = K.clip(x,self.clip_value[0],self.clip_value[1]) # clip value beween 0 and 1\n        masked_input = K.batch_dot(inputs, mask, [1,1])\n        return masked_input\n\n    def compute_output_shape(self, input_shape):\n        if type(input_shape[0]) is tuple:\n            return tuple([None,input_shape[0][-1]])\n    \n        else:\n            return tuple([None, input_shape[-1]])\n\ndef squash(vector, axis=-1):\n    """"""\n    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n    :param vectors: some vectors to be squashed, N-dim tensor\n    :param axis: the axis to squash\n    :return: a Tensor with same shape as input vectors\n    """"""\n    s_squared_norm = K.sum(K.square(vector), axis, keepdims=True)\n    scale = s_squared_norm/(1+s_squared_norm)/K.sqrt(s_squared_norm)\n    return scale*vector\n\nclass CapsuleLayer(layers.Layer):\n    """"""\n    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n    \n    :param num_capsule: number of capsules in this layer\n    :param dim_vector: dimension of the output vectors of the capsules in this layer\n    :param num_routings: number of iterations for the routing algorithm\n    """"""\n    def __init__(self, num_capsule, dim_vector, num_routing=3,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 **kwargs):\n        super(CapsuleLayer, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_vector = dim_vector\n        self.num_routing = num_routing\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n    def build(self,input_shape):\n        assert len(input_shape) >= 3, ""Input tensor must have shape=[None, input_num_capsule,input_dim_vector]""\n        self.input_num_capsule = input_shape[1]\n        self.input_dim_vector = input_shape[2]\n\n        self.W = self.add_weight(shape=[self.input_num_capsule,self.num_capsule,self.input_dim_vector,self.dim_vector],\n                                initializer=self.kernel_initializer,\n                                name=\'W\')\n        self.bias = self.add_weight(shape=[1,self.input_num_capsule,self.num_capsule,1,1],\n                                initializer=self.bias_initializer,\n                                name=\'bias\',trainable=False)\n        self.built = True\n\n    def call(self,inputs,training=None):\n        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n\n        # Replicate num_capsule dimension to prepare being multiplied by W\n        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n\n        """"""  \n        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n        \n        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n        """"""\n        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n                             elems=inputs_tiled,\n                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n        """"""\n        # Routing algorithm V1. Use tf.while_loop in a dynamic way.\n        def body(i, b, outputs):\n            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n            b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n            return [i-1, b, outputs]\n        cond = lambda i, b, inputs_hat: i > 0\n        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n        _, _, outputs = tf.while_loop(cond, body, loop_vars)\n        """"""\n        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n\n        assert self.num_routing > 0, \'The num_routing should be > 0.\'\n        for i in range(self.num_routing):\n            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n\n            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n            if i != self.num_routing - 1:\n                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n            # tf.summary.histogram(\'BigBee\', self.bias)  # for debugging\n\n        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n\n    def compute_output_shape(self, input_shape):\n        return tuple([None, self.num_capsule, self.dim_vector])\n\ndef PrimaryCapsule(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n    """"""\n    Apply Conv2D `n_channels` times and concatenate all capsules\n    :param inputs: 4D tensor, shape=[None, width, height, channels]\n    :param dim_vector: the dim of the output vector of capsule\n    :param n_channels: the number of types of capsules\n    :return: output tensor, shape=[None, num_capsule, dim_vector]\n    """"""\n    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n    return layers.Lambda(squash)(outputs)'"
networks/capsulenet/capsule_net.py,0,"b'from keras.layers import (\n    Input,\n    Conv2D,\n    Activation,\n    Dense,\n    Flatten,\n    Reshape,\n    Dropout\n)\nfrom keras.layers.merge import add\nfrom keras.regularizers import l2\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nimport keras.backend as K\nfrom keras import optimizers\nimport numpy as np\n\nfrom networks.capsulenet.capsule_layers import CapsuleLayer, PrimaryCapsule, Length,Mask\nfrom networks.capsulenet.capsulenet import CapsNet as CapsNetv1\nfrom networks.capsulenet.helper_function import load_cifar_10,load_cifar_100\n\n\ndef convolution_block(input,kernel_size=8,filters=16,kernel_regularizer=l2(1.e-4)):\n    conv2 = Conv2D(filters=filters,kernel_size=kernel_size,kernel_regularizer=kernel_regularizer,\n                    kernel_initializer=""he_normal"",padding=""same"")(input)\n    norm = BatchNormalization(axis=3)(conv2)\n    activation = Activation(""relu"")(norm)\n    return activation    \n\ndef CapsNet(input_shape,n_class,n_route,n_prime_caps=32,dense_size = (512,1024)):\n    conv_filter = 256\n    n_kernel = 24\n    primary_channel =64\n    primary_vector = 9\n    vector_dim = 9\n\n    target_shape = input_shape\n\n    input = Input(shape=input_shape)\n\n    # TODO: try leaky relu next time\n    conv1 = Conv2D(filters=conv_filter,kernel_size=n_kernel, strides=1, padding=\'valid\', activation=\'relu\',name=\'conv1\',kernel_initializer=""he_normal"")(input)\n\n    primary_cap = PrimaryCapsule(conv1,dim_vector=8, n_channels=64,kernel_size=9,strides=2,padding=\'valid\')\n\n    routing_layer = CapsuleLayer(num_capsule=n_class, dim_vector=vector_dim, num_routing=n_route,name=\'routing_layer\')(primary_cap)\n\n    output = Length(name=\'output\')(routing_layer)\n\n    y = Input(shape=(n_class,))\n    masked = Mask()([routing_layer,y])\n    \n    x_recon = Dense(dense_size[0],activation=\'relu\')(masked)\n\n    for i in range(1,len(dense_size)):\n        x_recon = Dense(dense_size[i],activation=\'relu\')(x_recon)\n    # Is there any other way to do  \n    x_recon = Dense(target_shape[0]*target_shape[1]*target_shape[2],activation=\'relu\')(x_recon)\n    x_recon = Reshape(target_shape=target_shape,name=\'output_recon\')(x_recon)\n\n    return Model([input,y],[output,x_recon])\n\n# why using 512, 1024 Maybe to mimic original 10M params?\ndef CapsNetv2(input_shape,n_class,n_route,n_prime_caps=32,dense_size = (512,1024)):\n    conv_filter = 64\n    n_kernel = 16\n    primary_channel =64\n    primary_vector = 12\n    capsule_dim_size = 8\n\n    target_shape = input_shape\n\n    input = Input(shape=input_shape)\n\n    # TODO: try leaky relu next time\n    conv_block_1 = convolution_block(input,kernel_size=16,filters=64)\n    primary_cap = PrimaryCapsule(conv_block_1,dim_vector=capsule_dim_size,n_channels=primary_channel,kernel_size=9,strides=2,padding=\'valid\')    \n    # Suppose this act like a max pooling \n    routing_layer = CapsuleLayer(num_capsule=n_class,dim_vector=capsule_dim_size*2,num_routing=n_route,name=\'routing_layer_1\')(primary_cap)\n    output = Length(name=\'output\')(routing_layer)\n\n    y = Input(shape=(n_class,))\n    masked = Mask()([routing_layer,y])\n    \n    x_recon = Dense(dense_size[0],activation=\'relu\')(masked)\n\n    for i in range(1,len(dense_size)):\n        x_recon = Dense(dense_size[i],activation=\'relu\')(x_recon)\n    # Is there any other way to do  \n    x_recon = Dense(np.prod(target_shape),activation=\'relu\')(x_recon)\n    x_recon = Reshape(target_shape=target_shape,name=\'output_recon\')(x_recon)\n\n    # conv_block_2 = convolution_block(routing_layer)\n    # b12_sum = add([conv_block_2,conv_block_1])\n\n    return Model([input,y],[output,x_recon])\n\ndef margin_loss(y_true, y_pred):\n    """"""\n    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n    :param y_true: [None, n_classes]\n    :param y_pred: [None, num_capsule]\n    :return: a scalar loss value.\n    """"""\n    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n\n    return K.mean(K.sum(L, 1))\n\ndef train(epochs=50,batch_size=64,mode=1):\n    import numpy as np\n    import os\n    from keras import callbacks\n    from keras.utils.vis_utils import plot_model\n    if mode==1:\n        num_classes = 10\n        (x_train,y_train),(x_test,y_test) = load_cifar_10()\n    else:\n        num_classes = 100\n        (x_train,y_train),(x_test,y_test) = load_cifar_100()\n    model = CapsNetv1(input_shape=[32, 32, 3],\n                        n_class=num_classes,\n                        n_route=3)\n    print(\'x_train shape:\', x_train.shape)\n    print(x_train.shape[0], \'train samples\')\n    print(x_test.shape[0], \'test samples\')\n\n    model.summary()\n    log = callbacks.CSVLogger(\'networks/models/results/capsule-cifar-\'+str(num_classes)+\'-log.csv\')\n    tb = callbacks.TensorBoard(log_dir=\'networks/models/results/tensorboard-capsule-cifar-\'+str(num_classes)+\'-logs\',\n                               batch_size=batch_size, histogram_freq=True)\n    checkpoint = callbacks.ModelCheckpoint(\'networks/models/capsnet.h5\',\n                                           save_best_only=True, verbose=1)\n    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n\n    # plot_model(model, to_file=\'models/capsule-cifar-\'+str(num_classes)+\'.png\', show_shapes=True)\n\n    model.compile(optimizer=optimizers.Adam(lr=0.001),\n                  loss=[margin_loss, \'mse\'],\n                  loss_weights=[1., 0.1],\n                  metrics={\'output_recon\':\'accuracy\',\'output\':\'accuracy\'})\n    from networks.capsulenet.helper_function import data_generator\n\n    generator = data_generator(x_train,y_train,batch_size)\n    # Image generator significantly increase the accuracy and reduce validation loss\n    model.fit_generator(generator,\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        validation_data=([x_test, y_test], [y_test, x_test]),\n                        epochs=epochs, verbose=1, max_q_size=100,\n                        callbacks=[log,tb,checkpoint,lr_decay])\n    \n    return model\n\ndef test(epoch, mode=1):\n    import matplotlib.pyplot as plt\n    from PIL import Image\n    from networks.capsulenet.helper_function import combine_images\n\n    if mode == 1:\n        num_classes =10\n        _,(x_test,y_test) = load_cifar_10()\n    else:\n        num_classes = 100\n        _,(x_test,y_test) = load_cifar_100()\n    \n    model = CapsNetv2(input_shape=[32, 32, 3],\n                        n_class=num_classes,\n                        n_route=3)\n    model.load_weights(\'weights/capsule_weights/capsule-cifar-\'+str(num_classes)+\'weights-{:02d}.h5\'.format(epoch)) \n    print(""Weights loaded, start validation"")   \n    # model.load_weights(\'weights/capsule-weights-{:02d}.h5\'.format(epoch))    \n    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n    print(\'-\'*50)\n    # Test acc: 0.7307\n    print(\'Test acc:\', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n\n    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n    image = img*255\n    Image.fromarray(image.astype(np.uint8)).save(""results/real_and_recon.png"")\n    print(\'Reconstructed images are saved to ./results/real_and_recon.png\')\n    print(\'-\'*50)\n    plt.imshow(plt.imread(""results/real_and_recon.png"", ))\n    plt.show()\n'"
networks/capsulenet/capsulelayers.py,8,"b'""""""\nSome key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, \nnot just on MNIST.\n*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\nuncommenting them and commenting their counterparts.\n\nAuthor: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n""""""\n\nimport keras.backend as K\nimport tensorflow as tf\nfrom keras import initializers, layers\n\n\nclass Length(layers.Layer):\n    """"""\n    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n    output: shape=[dim_1, ..., dim_{n-1}]\n    """"""\n    def call(self, inputs, **kwargs):\n        return K.sqrt(K.sum(K.square(inputs), -1))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:-1]\n\n\nclass Mask(layers.Layer):\n    """"""\n    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n    Output shape: [None, d2]\n    """"""\n    def call(self, inputs, **kwargs):\n        # use true label to select target capsule, shape=[batch_size, num_capsule]\n        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n            assert len(inputs) == 2\n            inputs, mask = inputs\n        else:  # if no true label, mask by the max length of vectors of capsules\n            x = inputs\n            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n\n        # masked inputs, shape = [batch_size, dim_vector]\n        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n        return inputs_masked\n\n    def compute_output_shape(self, input_shape):\n        if type(input_shape[0]) is tuple:  # true label provided\n            return tuple([None, input_shape[0][-1]])\n        else:\n            return tuple([None, input_shape[-1]])\n\n\ndef squash(vectors, axis=-1):\n    """"""\n    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n    :param vectors: some vectors to be squashed, N-dim tensor\n    :param axis: the axis to squash\n    :return: a Tensor with same shape as input vectors\n    """"""\n    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n    return scale * vectors\n\n\nclass CapsuleLayer(layers.Layer):\n    """"""\n    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n    \n    :param num_capsule: number of capsules in this layer\n    :param dim_vector: dimension of the output vectors of the capsules in this layer\n    :param num_routings: number of iterations for the routing algorithm\n    """"""\n    def __init__(self, num_capsule, dim_vector, num_routing=3,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 **kwargs):\n        super(CapsuleLayer, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_vector = dim_vector\n        self.num_routing = num_routing\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 3, ""The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]""\n        self.input_num_capsule = input_shape[1]\n        self.input_dim_vector = input_shape[2]\n\n        # Transform matrix\n        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n                                 initializer=self.kernel_initializer,\n                                 name=\'W\')\n\n        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n                                    initializer=self.bias_initializer,\n                                    name=\'bias\',\n                                    trainable=False)\n        self.built = True\n\n    def call(self, inputs, training=None):\n        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n\n        # Replicate num_capsule dimension to prepare being multiplied by W\n        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n\n        """""" \n        # Begin: inputs_hat computation V1 ---------------------------------------------------------------------#\n        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n        # w_tiled.shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n        \n        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n        # End: inputs_hat computation V1 ---------------------------------------------------------------------#\n        """"""\n\n        # Begin: inputs_hat computation V2 ---------------------------------------------------------------------#\n        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n                             elems=inputs_tiled,\n                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n        # End: inputs_hat computation V2 ---------------------------------------------------------------------#\n        """"""\n        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n        def body(i, b, outputs):\n            c = tf.nn.softmax(b, dim=2)  # dim=2 is the num_capsule dimension\n            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n            if i != 1:\n                b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n            return [i-1, b, outputs]\n\n        cond = lambda i, b, inputs_hat: i > 0\n        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n        shape_invariants = [tf.TensorShape([]),\n                            tf.TensorShape([None, self.input_num_capsule, self.num_capsule, 1, 1]),\n                            tf.TensorShape([None, 1, self.num_capsule, 1, self.dim_vector])]\n        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n        """"""\n\n        # Begin: routing algorithm V2, static -----------------------------------------------------------#\n        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n        assert self.num_routing > 0, \'The num_routing should be > 0.\'\n        for i in range(self.num_routing):\n            c = tf.nn.softmax(self.bias, axis=2)  # dim=2 is the num_capsule dimension\n            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n\n            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n            if i != self.num_routing - 1:\n                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n            # tf.summary.histogram(\'BigBee\', self.bias)  # for debugging\n        # End: routing algorithm V2, static ------------------------------------------------------------#\n\n        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n\n    def compute_output_shape(self, input_shape):\n        return tuple([None, self.num_capsule, self.dim_vector])\n\n\ndef PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n    """"""\n    Apply Conv2D `n_channels` times and concatenate all capsules\n    :param inputs: 4D tensor, shape=[None, width, height, channels]\n    :param dim_vector: the dim of the output vector of capsule\n    :param n_channels: the number of types of capsules\n    :return: output tensor, shape=[None, num_capsule, dim_vector]\n    """"""\n    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n                           name=\'primarycap_conv2d\')(inputs)\n    outputs = layers.Reshape(target_shape=[-1, dim_vector], name=\'primarycap_reshape\')(output)\n    return layers.Lambda(squash, name=\'primarycap_squash\')(outputs)\n\n\n""""""\n# The following is another way to implement primary capsule layer. This is much slower.\n# Apply Conv2D `n_channels` times and concatenate all capsules\ndef PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n    outputs = []\n    for _ in range(n_channels):\n        output = layers.Conv2D(filters=dim_vector, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_vector])(output))\n    outputs = layers.Concatenate(axis=1)(outputs)\n    return layers.Lambda(squash)(outputs)\n""""""'"
networks/capsulenet/capsulenet.py,0,"b'""""""\nKeras implementation of CapsNet in Hinton\'s paper Dynamic Routing Between Capsules.\nThe current version maybe only works for TensorFlow backend. Actually it will be straightforward to re-write to TF code.\nAdopting to other backends should be easy, but I have not tested this. \n\nUsage:\n       python CapsNet.py\n       python CapsNet.py --epochs 100\n       python CapsNet.py --epochs 100 --num_routing 3\n       ... ...\n       \nResult:\n    Validation accuracy > 99.5% after 20 epochs. Still under-fitting.\n    About 110 seconds per epoch on a single GTX1070 GPU card\n    \nAuthor: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n""""""\n\nfrom keras import layers, models, optimizers\nfrom keras import backend as K\nfrom keras.utils import to_categorical\nimport numpy as np\n\nfrom networks.capsulenet.capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n\ndef CapsNet(input_shape, n_class, n_route):\n    """"""\n    A Capsule Network on MNIST.\n    :param input_shape: data shape, 3d, [width, height, channels]\n    :param n_class: number of classes\n    :param num_routing: number of routing iterations\n    :return: A Keras Model with 2 inputs and 2 outputs\n    """"""\n    x = layers.Input(shape=input_shape)\n\n    # Layer 1: Just a conventional Conv2D layer\n    conv1 = layers.Conv2D(filters=256, kernel_size=8, strides=1, padding=\'valid\', activation=\'relu\', name=\'conv1\')(x)\n\n    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding=\'valid\')\n\n    # Layer 3: Capsule layer. Routing algorithm works here.\n    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=n_route, name=\'digitcaps\')(primarycaps)\n\n    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label\'s shape.\n    # If using tensorflow, this will not be necessary. :)\n    out_caps = Length(name=\'output\')(digitcaps)\n\n    # Decoder network.\n    y = layers.Input(shape=(n_class,))\n    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n    x_recon = layers.Dense(512, activation=\'relu\')(masked)\n    x_recon = layers.Dense(1024, activation=\'relu\')(x_recon)\n    x_recon = layers.Dense(np.prod(input_shape), activation=\'sigmoid\')(x_recon)\n    x_recon = layers.Reshape(target_shape=input_shape, name=\'out_recon\')(x_recon)\n\n    # two-input-two-output keras Model\n    return models.Model([x, y], [out_caps, x_recon])\n\n\ndef margin_loss(y_true, y_pred):\n    """"""\n    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n    :param y_true: [None, n_classes]\n    :param y_pred: [None, num_capsule]\n    :return: a scalar loss value.\n    """"""\n    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n\n    return K.mean(K.sum(L, 1))\n\n\ndef train(model, data, args):\n    """"""\n    Training a CapsuleNet\n    :param model: the CapsuleNet model\n    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n    :param args: arguments\n    :return: The trained model\n    """"""\n    # unpacking the data\n    (x_train, y_train), (x_test, y_test) = data\n\n    # callbacks\n    log = callbacks.CSVLogger(args.save_dir + \'/log.csv\')\n    tb = callbacks.TensorBoard(log_dir=args.save_dir + \'/tensorboard-logs\',\n                               batch_size=args.batch_size, histogram_freq=args.debug)\n    checkpoint = callbacks.ModelCheckpoint(args.save_dir + \'/weights-{epoch:02d}.h5\',\n                                           save_best_only=True, save_weights_only=True, verbose=1)\n    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (0.9 ** epoch))\n\n    # compile the model\n    model.compile(optimizer=optimizers.Adam(lr=args.lr),\n                  loss=[margin_loss, \'mse\'],\n                  loss_weights=[1., args.lam_recon],\n                  metrics={\'out_caps\': \'accuracy\'})\n\n    """"""\n    # Training without data augmentation:\n    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n    """"""\n\n    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n    def train_generator(x, y, batch_size, shift_fraction=0.):\n        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n                                           height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n        generator = train_datagen.flow(x, y, batch_size=batch_size)\n        while 1:\n            x_batch, y_batch = generator.next()\n            yield ([x_batch, y_batch], [y_batch, x_batch])\n\n    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n    model.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n                        steps_per_epoch=int(y_train.shape[0] / args.batch_size),\n                        epochs=args.epochs,\n                        validation_data=[[x_test, y_test], [y_test, x_test]],\n                        callbacks=[log, tb, checkpoint, lr_decay])\n    # End: Training with data augmentation -----------------------------------------------------------------------#\n\n    model.save_weights(args.save_dir + \'/trained_model.h5\')\n    print(\'Trained model saved to \\\'%s/trained_model.h5\\\'\' % args.save_dir)\n\n    from networks.capsulenet.helper_function import plot_log\n    plot_log(args.save_dir + \'/log.csv\', show=True)\n\n    return model\n\n\ndef test(model, data):\n    x_test, y_test = data\n    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n    print(\'-\'*50)\n    print(\'Test acc:\', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n\n    import matplotlib.pyplot as plt\n    from networks.capsulenet.helper_function import combine_images\n    from PIL import Image\n\n    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n    image = img * 255\n    Image.fromarray(image.astype(np.uint8)).save(""real_and_recon.png"")\n    print()\n    print(\'Reconstructed images are saved to ./real_and_recon.png\')\n    print(\'-\'*50)\n    plt.imshow(plt.imread(""real_and_recon.png"", ))\n    plt.show()\n\n\ndef load_mnist():\n    # the data, shuffled and split between train and test sets\n    from keras.datasets import mnist\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    x_train = x_train.reshape(-1, 28, 28, 1).astype(\'float32\') / 255.\n    x_test = x_test.reshape(-1, 28, 28, 1).astype(\'float32\') / 255.\n    y_train = to_categorical(y_train.astype(\'float32\'))\n    y_test = to_categorical(y_test.astype(\'float32\'))\n    return (x_train, y_train), (x_test, y_test)\n\n\nif __name__ == ""__main__"":\n    import numpy as np\n    import os\n    from keras.preprocessing.image import ImageDataGenerator\n    from keras import callbacks\n    from keras.utils.vis_utils import plot_model\n\n    # setting the hyper parameters\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', default=100, type=int)\n    parser.add_argument(\'--epochs\', default=30, type=int)\n    parser.add_argument(\'--lam_recon\', default=0.392, type=float)  # 784 * 0.0005, paper uses sum of SE, here uses MSE\n    parser.add_argument(\'--num_routing\', default=3, type=int)  # num_routing should > 0\n    parser.add_argument(\'--shift_fraction\', default=0.1, type=float)\n    parser.add_argument(\'--debug\', default=0, type=int)  # debug>0 will save weights by TensorBoard\n    parser.add_argument(\'--save_dir\', default=\'./result\')\n    parser.add_argument(\'--is_training\', default=1, type=int)\n    parser.add_argument(\'--weights\', default=None)\n    parser.add_argument(\'--lr\', default=0.001, type=float)\n    args = parser.parse_args()\n    print(args)\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n\n    # load data\n    (x_train, y_train), (x_test, y_test) = load_mnist()\n\n    # define model\n    model = CapsNet(input_shape=[28, 28, 1],\n                    n_class=len(np.unique(np.argmax(y_train, 1))),\n                    num_routing=args.num_routing)\n    model.summary()\n    # plot_model(model, to_file=args.save_dir+\'/model.png\', show_shapes=True)\n\n    # train or test\n    if args.weights is not None:  # init the model weights with provided one\n        model.load_weights(args.weights)\n    if args.is_training:\n        train(model=model, data=((x_train, y_train), (x_test, y_test)), args=args)\n    else:  # as long as weights are given, will run testing\n        if args.weights is None:\n            print(\'No weights are provided. Will test using random initialized weights.\')\n        test(model=model, data=(x_test, y_test))'"
networks/capsulenet/helper_function.py,0,"b""import os, math, csv\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.utils import to_categorical\nfrom pandas import read_csv\n\ndef load_cifar_10():\n    from keras.datasets import cifar10\n    num_classes = 10\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train /= 255.0\n    x_test /= 255.0    \n    y_train = to_categorical(y_train, num_classes)\n    y_test = to_categorical(y_test, num_classes)\n\n    return (x_train,y_train),(x_test,y_test)\n\ndef load_cifar_100():\n    from keras.datasets import cifar100\n    num_classes = 100\n    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train /= 255.0\n    x_test /= 255.0    \n    y_train = to_categorical(y_train, num_classes)\n    y_test = to_categorical(y_test, num_classes)\n\n    return (x_train,y_train),(x_test,y_test)\n\ndef combine_images(generated_images):\n    num = generated_images.shape[0]\n    width = int(math.sqrt(num))\n    height = int(math.ceil(float(num)/width))\n    shape = generated_images.shape[1:3]\n    image = np.zeros((height*shape[0], width*shape[1]),\n                     dtype=generated_images.dtype)\n    for index, img in enumerate(generated_images):\n        i = int(index/width)\n        j = index % width\n        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n            img[:, :, 0]\n    return image\n\ndef initializer():\n    if not os.path.exists('results/'):\n        os.mkdir('results')\n    if not os.path.exists('weights/'):\n        os.mkdir('weights')\n\ndef plot_log(filename, show=True):\n    # load data\n    log_df = read_csv(filename)\n    # epoch_list = [i for i in range(len(values[:,0]))]\n    fig = plt.figure(figsize=(4,6))\n    fig.subplots_adjust(top=0.95, bottom=0.05, right=0.95)\n    fig.add_subplot(211)\n    for column in list(log_df):\n        if 'loss' in column and 'val' in column:\n            plt.plot(log_df['epoch'].tolist(),log_df[column].tolist(), label=column)\n    plt.legend()\n    plt.title('Training loss')\n\n    fig.add_subplot(212)\n    for column in list(log_df):\n        if 'acc' in column :\n            plt.plot(log_df['epoch'].tolist(),log_df[column].tolist(), label=column)\n    plt.legend()\n    plt.title('Training and validation accuracy')\n\n    # fig.savefig('result/log.png')\n    if show:\n        plt.show()\n\n\ndef data_generator(x,y,batch_size):\n    x_train,y_train = x,y\n    from keras.preprocessing.image import ImageDataGenerator\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n    generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n    while True:\n        x,y  = generator.next()\n        yield ([x,y],[y,x])"""
