file_path,api_count,code
__init__.py,0,b''
benchmarks/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/launch_benchmark.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os\nimport signal\nimport subprocess\nimport sys\nfrom argparse import ArgumentParser\nfrom common import base_benchmark_util\nfrom common import platform_util\nfrom common.utils.validators import check_no_spaces, check_volume_mount, check_shm_size\nfrom common.base_model_init import BaseModelInitializer\n\n\nclass LaunchBenchmark(base_benchmark_util.BaseBenchmarkUtil):\n    """"""Launches benchmarking job based on the specified args """"""\n\n    def __init__(self, *args, **kwargs):\n        super(LaunchBenchmark, self).__init__(*args, **kwargs)\n\n        self.args, self.unknown_args = self.parse_args()\n        try:\n            self.validate_args()\n        except (IOError, ValueError) as e:\n            sys.exit(""\\nError: {}"".format(e))\n\n    def main(self):\n        benchmark_scripts = os.path.dirname(os.path.realpath(__file__))\n        use_case = self.get_model_use_case(benchmark_scripts)\n        intelai_models = self.get_model_dir(benchmark_scripts, use_case)\n        env_var_dict = self.get_env_vars(benchmark_scripts, use_case, intelai_models)\n\n        if self.args.docker_image:\n            self.run_docker_container(benchmark_scripts, intelai_models, env_var_dict)\n        else:\n            self.run_bare_metal(benchmark_scripts, intelai_models, env_var_dict)\n\n    def parse_args(self):\n        # Additional args that are only used with the launch script\n        arg_parser = ArgumentParser(\n            parents=[self._common_arg_parser],\n            description=""Parse args for benchmark interface"")\n\n        arg_parser.add_argument(\n            ""--docker-image"",\n            help=""Specify the docker image/tag to use when running benchmarking within a container.""\n                 ""If no docker image is specified, then no docker container will be used."",\n            dest=""docker_image"", default=None, type=check_no_spaces)\n\n        arg_parser.add_argument(\n            ""--volume"",\n            help=""Specify a custom volume to mount in the container, which follows the same format as the ""\n                 ""docker --volume flag (https://docs.docker.com/storage/volumes/). ""\n                 ""This argument can only be used in conjunction with a --docker-image."",\n            action=""append"", dest=""custom_volumes"", type=check_volume_mount)\n\n        arg_parser.add_argument(\n            ""--shm-size"",\n            help=""Specify the size of docker /dev/shm. The format is <number><unit>. ""\n                 ""number must be greater than 0. Unit is optional and can be b (bytes), k (kilobytes), ""\n                 ""m (megabytes), or g (gigabytes)."",\n            dest=""shm_size"", default=""64m"", type=check_shm_size)\n\n        arg_parser.add_argument(\n            ""--debug"", help=""Launches debug mode which doesn\'t execute ""\n                            ""start.sh when running in a docker container."", action=""store_true"")\n\n        return arg_parser.parse_known_args()\n\n    def validate_args(self):\n        """"""validate the args""""""\n        # validate that we support this framework by checking folder names\n        benchmark_dir = os.path.dirname(os.path.realpath(__file__))\n        if glob.glob(""{}/*/{}"".format(benchmark_dir, self.args.framework)) == []:\n            raise ValueError(""The specified framework is not supported: {}"".\n                             format(self.args.framework))\n\n        # if neither benchmark_only or accuracy_only are specified, then enable\n        # benchmark_only as the default\n        if not self.args.benchmark_only and not self.args.accuracy_only:\n            self.args.benchmark_only = True\n\n        # default disable_tcmalloc=False for int8 and disable_tcmalloc=True for other precisions\n        if not self.args.disable_tcmalloc:\n            self.args.disable_tcmalloc = str(self.args.precision != ""int8"")\n\n        if self.args.custom_volumes and not self.args.docker_image:\n            raise ValueError(""Volume mounts can only be used when running in a docker container ""\n                             ""(a --docker-image must be specified when using --volume)."")\n\n        if self.args.mode == ""inference"" and self.args.checkpoint:\n            print(""Warning: The --checkpoint argument is being deprecated in favor of using frozen graphs."")\n\n    def get_model_use_case(self, benchmark_scripts):\n        """"""\n        Infers the use case based on the directory structure for the specified model.\n        """"""\n        args = self.args\n\n        # find the path to the model\'s benchmarks folder\n        search_path = os.path.join(\n            benchmark_scripts, ""*"", args.framework, args.model_name,\n            args.mode, args.precision)\n        matches = glob.glob(search_path)\n        error_str = """"\n        if len(matches) > 1:\n            error_str = ""Found multiple model locations for {} {} {}""\n        elif len(matches) == 0:\n            error_str = ""No model was found for {} {} {}""\n        if error_str:\n            raise ValueError(error_str.format(args.framework, args.model_name, args.precision))\n\n        # use the benchmarks directory path to find the use case\n        dir_list = matches[0].split(""/"")\n\n        # find the last occurrence of framework in the list, then return\n        # the element before it in the path, which is the use case\n        return next(dir_list[elem - 1] for elem in range(len(dir_list) - 1, -1, -1)\n                    if dir_list[elem] == args.framework)\n\n    def get_model_dir(self, benchmark_scripts, use_case):\n        """"""\n        Finds the path to the optimized model directory in this repo, if it exists.\n        """"""\n\n        # use the models directory as a default\n        intelai_models = os.path.join(benchmark_scripts, os.pardir, ""models"")\n\n        # find the intelai_optimized model directory\n        args = self.args\n        optimized_model_dir = os.path.join(\n            benchmark_scripts, os.pardir, ""models"", use_case,\n            args.framework, args.model_name)\n\n        # if we find an optimized model, then we will use that path\n        if os.path.isdir(optimized_model_dir):\n            intelai_models = optimized_model_dir\n\n        return intelai_models\n\n    def get_env_vars(self, benchmark_scripts, use_case, intelai_models):\n        """"""\n        Sets up dictionary of standard env vars that are used by start.sh\n        """"""\n        # Standard env vars\n        args = self.args\n        env_var_dict = {\n            ""DATASET_LOCATION_VOL"": args.data_location,\n            ""CHECKPOINT_DIRECTORY_VOL"": args.checkpoint,\n            ""EXTERNAL_MODELS_SOURCE_DIRECTORY"": args.model_source_dir,\n            ""INTELAI_MODELS"": intelai_models,\n            ""BENCHMARK_SCRIPTS"": benchmark_scripts,\n            ""SOCKET_ID"": args.socket_id,\n            ""MODEL_NAME"": args.model_name,\n            ""MODE"": args.mode,\n            ""PRECISION"": args.precision,\n            ""VERBOSE"": args.verbose,\n            ""BATCH_SIZE"": args.batch_size,\n            ""USE_CASE"": use_case,\n            ""FRAMEWORK"": args.framework,\n            ""NUM_CORES"": args.num_cores,\n            ""NUM_INTER_THREADS"": args.num_inter_threads,\n            ""NUM_INTRA_THREADS"": args.num_intra_threads,\n            ""DATA_NUM_INTER_THREADS"": args.data_num_inter_threads,\n            ""DATA_NUM_INTRA_THREADS"": args.data_num_intra_threads,\n            ""BENCHMARK_ONLY"": args.benchmark_only,\n            ""ACCURACY_ONLY"": args.accuracy_only,\n            ""OUTPUT_RESULTS"": args.output_results,\n            ""DISABLE_TCMALLOC"": args.disable_tcmalloc,\n            ""TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD"": args.tcmalloc_large_alloc_report_threshold,\n            ""DOCKER"": str(args.docker_image is not None),\n            ""PYTHON_EXE"": sys.executable if not args.docker_image else ""python"",\n            ""MPI_NUM_PROCESSES"": args.mpi,\n            ""MPI_NUM_PROCESSES_PER_SOCKET"": args.num_mpi\n        }\n\n        # Add custom model args as env vars)\n        for custom_arg in args.model_args:\n            if ""="" not in custom_arg:\n                raise ValueError(""Expected model args in the format ""\n                                 ""`name=value` but received: {}"".\n                                 format(custom_arg))\n            split_arg = custom_arg.split(""="")\n            split_arg[0] = split_arg[0].replace(""-"", ""_"")\n            env_var_dict[split_arg[0]] = split_arg[1]\n\n        # Set the default value for NOINSTALL, if it\'s not explicitly set by the user\n        if ""NOINSTALL"" not in env_var_dict:\n            env_var_dict[""NOINSTALL""] = ""False""\n\n        return env_var_dict\n\n    def run_bare_metal(self, benchmark_scripts, intelai_models, env_var_dict):\n        """"""\n        Runs the model without a container\n        """"""\n        # setup volume directories to be the local system directories, since we aren\'t\n        # mounting volumes when running bare metal, but start.sh expects these args\n        args = self.args\n        workspace = os.path.join(benchmark_scripts, ""common"", args.framework)\n        mount_benchmark = benchmark_scripts\n        in_graph_path = args.input_graph\n        checkpoint_path = args.checkpoint\n        dataset_path = args.data_location\n\n        mount_external_models_source = args.model_source_dir\n        mount_intelai_models = intelai_models\n\n        # Add env vars with bare metal settings\n        env_var_dict[""MOUNT_EXTERNAL_MODELS_SOURCE""] = mount_external_models_source\n        env_var_dict[""MOUNT_INTELAI_MODELS_SOURCE""] = mount_intelai_models\n\n        if in_graph_path:\n            env_var_dict[""IN_GRAPH""] = in_graph_path\n\n        if checkpoint_path:\n            env_var_dict[""CHECKPOINT_DIRECTORY""] = checkpoint_path\n\n        if dataset_path:\n            env_var_dict[""DATASET_LOCATION""] = dataset_path\n\n        # if using the default output directory, get the full path\n        if args.output_dir == ""/models/benchmarks/common/tensorflow/logs"":\n            args.output_dir = os.path.join(workspace, ""logs"")\n\n        # Add env vars with bare metal settings\n        env_var_dict[""WORKSPACE""] = workspace\n        env_var_dict[""MOUNT_BENCHMARK""] = mount_benchmark\n        env_var_dict[""OUTPUT_DIR""] = args.output_dir\n\n        # Set env vars for bare metal\n        for env_var_name in env_var_dict:\n            os.environ[env_var_name] = str(env_var_dict[env_var_name])\n\n        # Run the start script\n        start_script = os.path.join(workspace, ""start.sh"")\n        self._launch_command([""bash"", start_script])\n\n    def run_docker_container(self, benchmark_scripts, intelai_models, env_var_dict):\n        """"""\n        Runs a docker container with the specified image and environment\n        variables to start running the benchmarking job.\n        """"""\n        args = self.args\n        mount_benchmark = ""/workspace/benchmarks""\n        mount_external_models_source = ""/workspace/models""\n        mount_intelai_models = ""/workspace/intelai_models""\n        workspace = os.path.join(mount_benchmark, ""common"", args.framework)\n\n        mount_output_dir = False\n        output_dir = os.path.join(workspace, \'logs\')\n        if args.output_dir != ""/models/benchmarks/common/tensorflow/logs"":\n            # we don\'t need to mount log dir otherwise since default is workspace folder\n            mount_output_dir = True\n            output_dir = args.output_dir\n\n        in_graph_dir = os.path.dirname(args.input_graph) if args.input_graph \\\n            else """"\n        in_graph_filename = os.path.basename(args.input_graph) if \\\n            args.input_graph else """"\n\n        # env vars with docker settings\n        env_vars = [""--env"", ""WORKSPACE={}"".format(workspace),\n                    ""--env"", ""MOUNT_BENCHMARK={}"".format(mount_benchmark),\n                    ""--env"", ""MOUNT_EXTERNAL_MODELS_SOURCE={}"".format(mount_external_models_source),\n                    ""--env"", ""MOUNT_INTELAI_MODELS_SOURCE={}"".format(mount_intelai_models),\n                    ""--env"", ""OUTPUT_DIR={}"".format(output_dir)]\n\n        if args.input_graph:\n            env_vars += [""--env"", ""IN_GRAPH=/in_graph/{}"".format(in_graph_filename)]\n\n        if args.data_location:\n            env_vars += [""--env"", ""DATASET_LOCATION=/dataset""]\n\n        if args.checkpoint:\n            env_vars += [""--env"", ""CHECKPOINT_DIRECTORY=/checkpoints""]\n\n        # Add env vars with common settings\n        for env_var_name in env_var_dict:\n            env_vars += [""--env"", ""{}={}"".format(env_var_name, env_var_dict[env_var_name])]\n\n        # Add proxy to env variables if any set on host\n        for environment_proxy_setting in [\n            ""http_proxy"",\n            ""ftp_proxy"",\n            ""https_proxy"",\n            ""no_proxy"",\n        ]:\n            if not os.environ.get(environment_proxy_setting):\n                continue\n            env_vars.append(""--env"")\n            env_vars.append(""{}={}"".format(\n                environment_proxy_setting,\n                os.environ.get(environment_proxy_setting)\n            ))\n\n        volume_mounts = [""--volume"", ""{}:{}"".format(benchmark_scripts, mount_benchmark),\n                         ""--volume"", ""{}:{}"".format(args.model_source_dir, mount_external_models_source),\n                         ""--volume"", ""{}:{}"".format(intelai_models, mount_intelai_models)]\n\n        if mount_output_dir:\n            volume_mounts.extend([\n                ""--volume"", ""{}:{}"".format(output_dir, output_dir)])\n\n        if args.data_location:\n            volume_mounts.extend([\n                ""--volume"", ""{}:{}"".format(args.data_location, ""/dataset"")])\n\n        if args.checkpoint:\n            volume_mounts.extend([\n                ""--volume"", ""{}:{}"".format(args.checkpoint, ""/checkpoints"")])\n\n        if in_graph_dir:\n            volume_mounts.extend([\n                ""--volume"", ""{}:{}"".format(in_graph_dir, ""/in_graph"")])\n\n        if args.custom_volumes:\n            for custom_volume in args.custom_volumes:\n                volume_mounts.extend([""--volume"", custom_volume])\n\n        docker_run_cmd = [""docker"", ""run""]\n\n        # only use -it when debugging, otherwise we might get TTY error\n        if args.debug:\n            docker_run_cmd.append(""-it"")\n\n        docker_shm_size = ""--shm-size={}"".format(args.shm_size)\n        docker_run_cmd = docker_run_cmd + env_vars + volume_mounts + [\n            docker_shm_size, ""--privileged"", ""-u"", ""root:root"", ""-w"",\n            workspace, args.docker_image, ""/bin/bash""]\n\n        if not args.debug:\n            docker_run_cmd.append(""start.sh"")\n\n        if args.verbose:\n            print(""Docker run command:\\n{}"".format(docker_run_cmd))\n\n        self._launch_command(docker_run_cmd)\n\n    def _launch_command(self, run_cmd):\n        """"""runs command that runs the start script in a container or on bare metal and exits on ctrl c""""""\n        p = subprocess.Popen(run_cmd, preexec_fn=os.setsid)\n        try:\n            p.communicate()\n        except KeyboardInterrupt:\n            os.killpg(os.getpgid(p.pid), signal.SIGKILL)\n\n\nif __name__ == ""__main__"":\n    util = LaunchBenchmark()\n    util.main()\n'"
models/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
tests/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
tests/conftest.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\nimport inspect\nimport os\nimport pytest\nimport sys\nfrom mock import MagicMock\n\n\nMODULES = (\'common\',)\nMODULES_REPLACE = (\'tests.unit\', \'benchmarks\')\n\n\ndef patch_setattr(module_names, module_replace, monkeypatch, path, m):\n    """""" Credit for this goes mostly to @megawidget\n    do not call this directly -- assumes the fixture\'s caller is two stacks up\n    and will correspondingly guess the module path to patch\n    `path` can be:\n        1. an object, if it\'s defined in the module you\'re testing\n        2. a name, if it\'s imported in the module you\'re testing\n        3. a full path a la traditional monkeypatch\n    """"""\n    if hasattr(path, \'__module__\'):\n        monkeypatch.setattr(\'.\'.join((path.__module__, path.__name__)), m)\n        return\n    elif any(path.startswith(i + \'.\') for i in module_names):\n        # full path.  OK.\n        monkeypatch.setattr(path, m)\n    else:\n        # assume we\'re patching stuff in the file the test file is supposed to\n        # be testing\n        fn = inspect.getouterframes(inspect.currentframe())[2][1]\n        fn = os.path.splitext(os.path.relpath(fn))[0]\n        module = fn.replace(os.path.sep, \'.\').replace(\'test_\', \'\').replace(\n            *module_replace)\n        try:\n            monkeypatch.setattr(\'.\'.join((module, path)), m)\n        except AttributeError:\n            # handle mocking builtins like `open`\n            if sys.version_info.major == 3:\n                builtin = \'builtins\'\n            else:\n                builtin = \'__builtin__\'\n            # NOTE: `path` should be just the builtin, like `open`\n            # usage: patch(\'open\')\n            monkeypatch.setattr(""{}.{}"".format(builtin, path), m)\n\n\n@pytest.fixture\ndef patch(monkeypatch):\n    """"""allows us to add easy autouse fixtures by patching anything we want\n       Usage: return something like this in a @pytest.fixture\n       - patch(\'files.fetch_action_arg\', MagicMock(return_value=\'output\'))\n       Without the second arg, will default to just MagicMock()\n    """"""\n\n    def wrapper(path, mock=None):\n        m = mock if mock is not None else MagicMock()\n        patch_setattr(MODULES, MODULES_REPLACE, monkeypatch, path, m)\n        return m\n\n    return wrapper\n'"
benchmarks/common/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/common/base_benchmark_util.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018-2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom argparse import ArgumentParser\nfrom common import platform_util\nfrom common.utils.validators import (check_positive_number, check_valid_filename, check_valid_file_or_dir,\n                                     check_valid_folder, check_positive_number_or_equal_to_negative_one)\n\n\nclass BaseBenchmarkUtil(object):\n    """"""Base benchmark util class""""""\n    MODEL_INITIALIZER = ""model_init""\n\n    def __init__(self, platform_util_obj=None):\n        self._common_arg_parser = None\n        self._define_args()\n        self.args, _ = self._common_arg_parser.parse_known_args()\n        # currently used for testing, need to be able to pass in mocked values\n        # TODO: but also, why is this class not inheriting PlatformUtil?\n        self._platform_util = platform_util_obj or platform_util.PlatformUtil(self.args)\n        self._validate_args()\n\n    def _define_args(self):\n        """"""define args for the benchmark interface shared by FP32 and int8\n        models""""""\n\n        # only require the arg, if we aren\'t just printing out --help\n        required_arg = ""--help"" not in sys.argv\n\n        self._common_arg_parser = ArgumentParser(\n            add_help=False, description=""Parse args for base benchmark ""\n                                        ""interface"")\n\n        self._common_arg_parser.add_argument(\n            ""-f"", ""--framework"",\n            help=""Specify the name of the deep learning framework to use."",\n            dest=""framework"", default=None, required=required_arg)\n\n        self._common_arg_parser.add_argument(\n            ""-r"", ""--model-source-dir"",\n            help=""Specify the models source directory from your local machine"",\n            nargs=""?"", dest=""model_source_dir"", type=check_valid_folder)\n\n        self._common_arg_parser.add_argument(\n            ""-p"", ""--precision"",\n            help=""Specify the model precision to use: fp32, int8"",\n            required=required_arg, choices=[""fp32"", ""int8""],\n            dest=""precision"")\n\n        self._common_arg_parser.add_argument(\n            ""-mo"", ""--mode"", help=""Specify the type training or inference "",\n            required=required_arg, choices=[""training"", ""inference""], dest=""mode"")\n\n        self._common_arg_parser.add_argument(\n            ""-m"", ""--model-name"", required=required_arg,\n            help=""model name to run benchmarks for"", dest=""model_name"")\n\n        self._common_arg_parser.add_argument(\n            ""-b"", ""--batch-size"",\n            help=""Specify the batch size. If this parameter is not specified ""\n                 ""or is -1, the largest ideal batch size for the model will ""\n                 ""be used"",\n            dest=""batch_size"", default=-1,\n            type=check_positive_number_or_equal_to_negative_one)\n\n        self._common_arg_parser.add_argument(\n            ""--mpi_num_processes"", type=check_positive_number,\n            help=""The number of MPI processes"",\n            dest=""mpi"", default=None)\n\n        self._common_arg_parser.add_argument(\n            ""--mpi_num_processes_per_socket"", type=check_positive_number,\n            help=""Specify how many MPI processes to launch per socket"",\n            dest=""num_mpi"", default=1)\n\n        self._common_arg_parser.add_argument(\n            ""-d"", ""--data-location"",\n            help=""Specify the location of the data. If this parameter is not ""\n                 ""specified, the benchmark will use random/dummy data."",\n            dest=""data_location"", default=None, type=check_valid_file_or_dir)\n\n        self._common_arg_parser.add_argument(\n            ""-i"", ""--socket-id"",\n            help=""Specify which socket to use. Only one socket will be used ""\n                 ""when this value is set. If used in conjunction with ""\n                 ""--num-cores, all cores will be allocated on the single ""\n                 ""socket."",\n            dest=""socket_id"", type=int, default=-1)\n\n        self._common_arg_parser.add_argument(\n            ""-n"", ""--num-cores"",\n            help=""Specify the number of cores to use. If the parameter is not""\n                 "" specified or is -1, all cores will be used."",\n            dest=""num_cores"", type=int, default=-1)\n\n        self._common_arg_parser.add_argument(\n            ""--num-instances"", type=check_positive_number,\n            help=""Specify the number of instances to run."",\n            dest=""num_instances"", default=1)\n\n        self._common_arg_parser.add_argument(\n            ""-a"", ""--num-intra-threads"", type=check_positive_number,\n            help=""Specify the number of threads within the layer"",\n            dest=""num_intra_threads"", default=None)\n\n        self._common_arg_parser.add_argument(\n            ""-e"", ""--num-inter-threads"", type=check_positive_number,\n            help=""Specify the number threads between layers"",\n            dest=""num_inter_threads"", default=None)\n\n        self._common_arg_parser.add_argument(\n            ""--data-num-intra-threads"", type=check_positive_number,\n            help=""The number intra op threads for the data layer config"",\n            dest=""data_num_intra_threads"", default=None)\n\n        self._common_arg_parser.add_argument(\n            ""--data-num-inter-threads"", type=check_positive_number,\n            help=""The number inter op threads for the data layer config"",\n            dest=""data_num_inter_threads"", default=None)\n\n        self._common_arg_parser.add_argument(\n            ""-c"", ""--checkpoint"",\n            help=""Specify the location of trained model checkpoint directory. ""\n                 ""If mode=training model/weights will be written to this ""\n                 ""location. If mode=inference assumes that the location points""\n                 "" to a model that has already been trained. Note that using ""\n                 ""checkpoint files for inference is being deprecated, in favor ""\n                 ""of using frozen graphs."",\n            dest=""checkpoint"", default=None, type=check_valid_folder)\n\n        self._common_arg_parser.add_argument(\n            ""-g"", ""--in-graph"", help=""Full path to the input graph "",\n            dest=""input_graph"", default=None, type=check_valid_filename)\n\n\n        self._common_arg_parser.add_argument(\n            ""-k"", ""--benchmark-only"",\n            help=""For benchmark measurement only. If neither --benchmark-only ""\n                 ""or --accuracy-only are specified, it will default to run ""\n                 ""benchmarking."",\n            dest=""benchmark_only"", action=""store_true"")\n\n        self._common_arg_parser.add_argument(\n            ""--accuracy-only"",\n            help=""For accuracy measurement only.  If neither --benchmark-only ""\n                 ""or --accuracy-only are specified, it will default to run ""\n                 ""benchmarking."",\n            dest=""accuracy_only"", action=""store_true"")\n\n        self._common_arg_parser.add_argument(\n            ""--output-results"",\n            help=""Writes inference output to a file, when used in conjunction ""\n                 ""with --accuracy-only and --mode=inference."",\n            dest=""output_results"", action=""store_true"")\n\n        # Note this can\'t be a normal boolean flag, because we need to know when the user\n        # does not explicitly set the arg value so that we can apply the appropriate\n        # default value, depending on the the precision.\n        self._common_arg_parser.add_argument(\n            ""--disable-tcmalloc"",\n            help=""When TCMalloc is enabled, the google-perftools are installed (if running ""\n                 ""using docker) and the LD_PRELOAD environment variable is set to point to ""\n                 ""the TCMalloc library file. The TCMalloc memory allocator produces better ""\n                 ""performance results with smaller batch sizes. This flag disables the use of ""\n                 ""TCMalloc when set to True. For int8 benchmarking, TCMalloc is enabled by ""\n                 ""default (--disable-tcmalloc=False). For other precisions, the flag is ""\n                 ""--disable-tcmalloc=True by default."",\n            dest=""disable_tcmalloc"", choices=[""True"", ""False""],\n            default=None\n        )\n\n        self._common_arg_parser.add_argument(\n            ""--tcmalloc-large-alloc-report-threshold"",\n            help=""Sets the TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD environment variable to ""\n                 ""the specified value. The environment variable sets the threshold (in bytes) ""\n                 ""for when large memory allocation messages will be displayed."",\n            dest=""tcmalloc_large_alloc_report_threshold"", default=2147483648, type=int\n        )\n\n        self._common_arg_parser.add_argument(\n            ""-v"", ""--verbose"", help=""Print verbose information."",\n            dest=""verbose"", action=""store_true"")\n\n        self._common_arg_parser.add_argument(\n            ""--output-dir"",\n            help=""Folder to dump output into. The output directory will default to ""\n                 ""\'models/benchmarks/common/tensorflow/logs\' if no path is specified."",\n            default=""/models/benchmarks/common/tensorflow/logs"")\n\n        # Allow for additional command line args after --\n        self._common_arg_parser.add_argument(\n            ""model_args"", nargs=""*"",\n            help=""Additional command line arguments (prefix flag start with""\n                 "" \'--\')."")\n\n    def _validate_args(self):\n        """"""validate the args and initializes platform_util""""""\n        # check if socket id is in socket number range\n        num_sockets = self._platform_util.num_cpu_sockets\n        args = self.args\n        if not -1 <= args.socket_id < num_sockets:\n            raise ValueError(""Socket id must be within socket number range: ""\n                             ""[0, {}]."".format(num_sockets - 1))\n\n        # check number of cores\n        num_logical_cores_per_socket = \\\n            self._platform_util.num_cores_per_socket * \\\n            self._platform_util.num_threads_per_core\n        # if a socket_id is specified, only count cores from one socket\n        system_num_cores = num_logical_cores_per_socket if \\\n            args.socket_id != -1 else num_logical_cores_per_socket * \\\n            self._platform_util.num_cpu_sockets\n        num_cores = args.num_cores\n\n        if (num_cores <= 0) and (num_cores != -1):\n            raise ValueError(\n                ""Core number must be greater than 0 or -1. The default value ""\n                ""is -1 which means using all the cores in the sockets"")\n        elif num_cores > system_num_cores:\n            raise ValueError(""Number of cores exceeds system core number: {}"".\n                             format(system_num_cores))\n\n        if args.output_results and ((args.model_name != ""resnet50"" and\n                                    args.model_name != ""resnet50v1_5"") or args.precision != ""fp32""):\n            raise ValueError(""--output-results is currently only supported for resnet50 FP32 inference."")\n        elif args.output_results and (args.mode != ""inference"" or not args.data_location):\n            raise ValueError(""--output-results can only be used when running inference with a dataset."")\n\n    def initialize_model(self, args, unknown_args):\n        """"""Create model initializer for the specified model""""""\n        model_initializer = None\n        model_init_file = None\n        if args.model_name:  # not empty\n            current_path = os.path.dirname(\n                os.path.dirname(os.path.realpath(__file__)))\n\n            # find the path to the model_init.py file\n            filename = ""{}.py"".format(self.MODEL_INITIALIZER)\n            model_init_file = os.path.join(current_path, args.use_case,\n                                           args.framework, args.model_name,\n                                           args.mode, args.precision,\n                                           filename)\n            package = ""."".join([args.use_case, args.framework,\n                                args.model_name, args.mode, args.precision])\n            model_init_module = __import__(\n                package + ""."" + self.MODEL_INITIALIZER, fromlist=[""*""])\n            model_initializer = model_init_module.ModelInitializer(\n                args, unknown_args, self._platform_util)\n\n        if model_initializer is None:\n            raise ImportError(""Unable to locate {}."".format(model_init_file))\n\n        return model_initializer\n'"
benchmarks/common/base_model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport glob\nimport json\nimport os\n\n\ndef set_env_var(env_var, value, overwrite_existing=False):\n    """"""\n    Sets the specified environment variable.\n\n    If overwrite_existing is False, it will only set the new env var value\n    if the environment variable is not already set.\n\n    If overwrite_existing is True, the environment variable will always be\n    set to the specified value.\n    """"""\n    if overwrite_existing or not os.environ.get(env_var):\n        os.environ[env_var] = str(value)\n\n\nclass BaseModelInitializer(object):\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        self.args = args\n        self.custom_args = custom_args\n        self.platform_util = platform_util\n\n        # Set default values for TCMalloc and convert string value to a boolean\n        if self.args.disable_tcmalloc is None:\n            # Set to False for int8 and True for other precisions\n            self.args.disable_tcmalloc = self.args.precision != ""int8""\n        elif isinstance(self.args.disable_tcmalloc, str):\n            self.args.disable_tcmalloc = self.args.disable_tcmalloc == ""True""\n\n        # Ensure that we are using the proper version of python to run the benchmarking script\n        self.python_exe = os.environ[""PYTHON_EXE""]\n\n        if not platform_util:\n            raise ValueError(""Did not find any platform info."")\n\n        # Invoke mpirun if mpi_num_processes env is not None\n        if os.environ[""MPI_NUM_PROCESSES""] != ""None"":\n            if os.environ[""MPI_NUM_PROCESSES_PER_SOCKET""] == ""1"":\n              # Map by socket using OpenMPI by default (PPS=1).\n              self.python_exe = ""mpirun --allow-run-as-root -n "" + os.environ[""MPI_NUM_PROCESSES""] + "" --map-by socket "" + self.python_exe\n            else:\n              # number of processes per socket (pps)\n              pps = int(os.environ[""MPI_NUM_PROCESSES_PER_SOCKET""])\n              split_a_socket = str(platform_util.num_cores_per_socket // pps)\n              # Launch pps MPI processes over one socket\n              self.python_exe = ""mpirun --allow-run-as-root -n "" + os.environ[""MPI_NUM_PROCESSES""] + "" --map-by ppr:"" + str(pps) + "":socket:pe="" + split_a_socket + "" --cpus-per-proc "" + split_a_socket + "" "" + self.python_exe\n\n    def run_command(self, cmd):\n        """"""\n        Prints debug messages when verbose is enabled, and then runs the\n        specified command.\n        """"""\n        if self.args.verbose:\n            print(""Received these standard args: {}"".format(self.args))\n            print(""Received these custom args: {}"".format(self.custom_args))\n            print(""Current directory: {}"".format(os.getcwd()))\n            print(""Running: {}"".format(str(cmd)))\n\n        os.system(cmd)\n\n    def get_command_prefix(self, socket_id, numactl=True):\n        """"""\n        Returns the command prefix with:\n         - LD_PRELOAD for int8 models (if tcmalloc is not disabled)\n         - The numactl command with --cpunodebind and --membind set to the specified socket_id (if numactl=True)\n\n        Should be used only for single instance.\n        """"""\n        command = """"\n\n        if not self.args.disable_tcmalloc:\n            # Try to find the TCMalloc library file\n            matches = glob.glob(""/usr/lib/libtcmalloc.so*"")\n\n            if len(matches) == 0:\n                matches = glob.glob(""/usr/lib64/libtcmalloc.so*"")\n\n            if len(matches) > 0:\n                command += ""LD_PRELOAD={} "".format(matches[0])\n            else:\n                # Unable to find the TCMalloc library file\n                print(""Warning: Unable to find the TCMalloc library file (libtcmalloc.so) in /usr/lib or /usr/lib64, ""\n                      ""so the LD_PRELOAD environment variable will not be set."")\n\n        if socket_id != -1 and numactl:\n            command += ""numactl --cpunodebind={0} --membind={0} "".format(str(socket_id))\n\n        return command\n\n    def add_args_to_command(self, command, arg_list):\n        """"""\n        Add args that are specified in the arg list to the command.  batch_size\n        is a special case, where it\'s not added if it\'s set to -1 (undefined).\n        Returns the command string with args.\n        """"""\n        for arg in vars(self.args):\n            arg_value = getattr(self.args, arg)\n            if arg == ""batch_size"" and arg_value == -1:\n                continue\n            if arg_value and (arg in arg_list):\n                command = ""{cmd} --{param}={value}"".format(\n                    cmd=command, param=arg, value=arg_value)\n        return command\n\n    def set_num_inter_intra_threads(self, num_inter_threads=None, num_intra_threads=None):\n        """"""\n        Sets default values for self.args.num_inter_threads and\n        self.args.num_intra_threads, only if they are not already set.\n\n        If num_inter_threads and/or num_intra_threads are specified, then those\n        are the values that will be used. Otherwise, if they are None, then the\n        following criteria applies:\n\n        If a single socket is being used:\n         * num_inter_threads = 1\n         * num_intra_threads = The number of cores on a single socket, or\n           self.args.num_cores if a specific number of cores was defined.\n\n        If all sockets are being used:\n         * num_inter_threads = The number of sockets\n         * num_intra_threads = The total number of cores across all sockets, or\n           self.args.num_cores if a specific number of cores was defined.\n        """"""\n        # if num_inter_threads is specified, use that value as long as the arg isn\'t set\n        if num_inter_threads and not self.args.num_inter_threads:\n            self.args.num_inter_threads = num_inter_threads\n\n        # if num_intra_threads is specified, use that value as long as the arg isn\'t set\n        if num_intra_threads and not self.args.num_intra_threads:\n            self.args.num_intra_threads = num_intra_threads\n\n        if self.args.socket_id != -1:\n            if not self.args.num_inter_threads:\n                self.args.num_inter_threads = 1\n            if not self.args.num_intra_threads:\n                self.args.num_intra_threads = \\\n                    self.platform_util.num_cores_per_socket \\\n                    if self.args.num_cores == -1 else self.args.num_cores\n        else:\n            if not self.args.num_inter_threads:\n                self.args.num_inter_threads = self.platform_util.num_cpu_sockets\n            if not self.args.num_intra_threads:\n                if self.args.num_cores == -1:\n                    self.args.num_intra_threads = \\\n                        int(self.platform_util.num_cores_per_socket *\n                            self.platform_util.num_cpu_sockets)\n                else:\n                    self.args.num_intra_threads = self.args.num_cores\n\n        if self.args.verbose:\n            print(""num_inter_threads: {}\\nnum_intra_threads: {}"".format(\n                self.args.num_inter_threads, self.args.num_intra_threads))\n\n    def set_kmp_vars(self, config_file_path, kmp_settings=None, kmp_blocktime=None, kmp_affinity=None):\n        """"""\n        Sets KMP_* environment variables to the specified value, if the environment variable has not already been set.\n        The default values in the json file are the best known settings for the model.\n        """"""\n        if os.path.exists(config_file_path):\n            with open(config_file_path, \'r\') as config:\n                config_object = json.load(config)\n\n            # First sets default from config file\n            for param in config_object.keys():\n                for env in config_object[param].keys():\n                    set_env_var(env, config_object[param][env])\n\n        else:\n            print(""Warning: File {} does not exist and \\\n            cannot be used to set KMP environment variables"".format(config_file_path))\n\n        # Override user provided envs\n        if kmp_settings:\n            set_env_var(""KMP_SETTINGS"", kmp_settings, overwrite_existing=True)\n        if kmp_blocktime:\n            set_env_var(""KMP_BLOCKTIME"", kmp_blocktime, overwrite_existing=True)\n        if kmp_affinity:\n            set_env_var(""KMP_AFFINITY"", kmp_affinity, overwrite_existing=True)\n'"
benchmarks/common/platform_util.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\nimport platform as system_platform\nimport subprocess\nimport sys\n\nNUMA_NODES_STR_ = ""NUMA node(s)""\nCPU_SOCKETS_STR_ = ""Socket(s)""\nCORES_PER_SOCKET_STR_ = ""Core(s) per socket""\nTHREADS_PER_CORE_STR_ = ""Thread(s) per core""\nLOGICAL_CPUS_STR_ = ""CPU(s)""\n\n\nclass CPUInfo():\n    """"""CPU information class.""""""\n\n    def __init__(self):\n        """"""Initialize CPU information class.""""""\n        self._binding_data = CPUInfo._sort_membind_info(self._get_core_membind_info())\n\n    @staticmethod\n    def _get_core_membind_info():\n        """"""\n        Return sorted information about cores and memory binding.\n        E.g.\n        CPU ID, Socket ID, Node ID, HT CPU ID,\n        0  ,     0    ,    0   ,     0\n        1  ,     0    ,    0   ,     1\n        :return: list with cpu, sockets, ht core and memory binding information\n        :rtype: List[List[str, Any]]\n        """"""\n        args = [""lscpu"", ""--parse=CPU,Core,Socket,Node""]\n        process_lscpu = subprocess.check_output(args, universal_newlines=True).split(""\\n"")\n\n        # Get information about core, node, socket and cpu\n        bind_info = []\n        for line in process_lscpu:\n            pattern = r""^([\\d]+,[\\d]+,[\\d]+,[\\d]+)""\n            regex_out = re.search(pattern, line)\n            if regex_out:\n                bind_info.append(regex_out.group(1).strip().split("",""))\n\n        return bind_info\n\n    @staticmethod\n    def _sort_membind_info(membind_bind_info):\n        """"""\n        Sore membind info data.\n        :param membind_bind_info: raw membind info data\n        :type membind_bind_info: List[List[str]]\n        :return: sorted membind info\n        :rtype: List[List[Dict[str, int]]]\n        """"""\n        membind_cpu_list = []\n        nodes_count = int(max(element[2] for element in membind_bind_info)) + 1\n        # Sort list by Node id\n        for node_number in range(nodes_count):\n            node_core_list = []\n            core_info = {}\n            for entry in membind_bind_info:\n                cpu_id = int(entry[0])\n                core_id = int(entry[1])\n                node_id = int(entry[2])\n                socket_id = int(entry[3])\n\n                # Skip nodes other than current node number\n                if node_number != node_id:\n                    continue\n\n                # Add core info\n                if cpu_id == core_id:\n                    core_info.update({\n                        core_id: {\n                            ""cpu_id"": cpu_id,\n                            ""node_id"": node_id,\n                            ""socket_id"": socket_id,\n                        },\n                    })\n                else:\n                    # Add information about Hyper Threading\n                    core_info[core_id][""ht_cpu_id""] = cpu_id\n\n            # Change dict of dicts to list of dicts\n            for iterator in range(len(core_info)):\n                curr_core_id = len(core_info) * node_number + iterator\n                single_core_info = core_info.get(curr_core_id)\n                if single_core_info:\n                    node_core_list.append(single_core_info)\n\n            membind_cpu_list.append(node_core_list)\n\n        return membind_cpu_list\n\n    @property\n    def sockets(self):\n        """"""\n        Return count of sockets available on server.\n        :return: available cores\n        :rtype: int\n        """"""\n        available_sockets = len(self._binding_data)\n        return int(available_sockets)\n\n    @property\n    def cores(self):\n        """"""\n        Return amount of cores available on server.\n        :return: amount of cores\n        :rtype: int\n        """"""\n        available_cores = self.cores_per_socket * self.sockets\n        return int(available_cores)  # type: ignore\n\n    @property\n    def cores_per_socket(self):\n        """"""\n        Return amount of available cores per socket.\n        :return: amount of cores\n        :rtype: int\n        """"""\n        available_cores_per_socket = len(self._binding_data[0])\n        return available_cores_per_socket\n\n    @property\n    def binding_information(self):\n        """"""\n        Return information about cores and memory binding.\n        Format:\n        [\n            [ # socket 0\n                { # Core 0\n                    ""cpu_id"": 0,\n                    ""node_id"": 0,\n                    ""socket_id"": 0,\n                    ""ht_cpu_id"": 56\n                }\n            ],\n            [ # socket 1\n                { # Core 0\n                    ""cpu_id"": 28,\n                    ""node_id"": 1,\n                    ""socket_id"": 1,\n                    ""ht_cpu_id"": 84\n                }\n            ]\n        ]\n        :return: dict with cpu, sockets, ht core and memory binding information\n        :rtype: List[List[Dict[str, int]]]\n        """"""\n        return self._binding_data\n\n\nclass PlatformUtil:\n    \'\'\'\n    This module implements a platform utility that exposes functions that\n    detects platform information.\n    \'\'\'\n\n    def __init__(self, args):\n        self.args = args\n        self.num_cpu_sockets = 0\n        self.num_cores_per_socket = 0\n        self.num_threads_per_core = 0\n        self.num_logical_cpus = 0\n        self.num_numa_nodes = 0\n\n        os_type = system_platform.system()\n        if ""Windows"" == os_type:\n            self.windows_init()\n        elif ""Mac"" == os_type or ""Darwin"" == os_type:\n            self.mac_init()\n        elif ""Linux"" == os_type:\n            self.linux_init()\n        else:\n            raise ValueError(""Unable to determine Operating system type."")\n\n    def linux_init(self):\n        lscpu_cmd = ""lscpu""\n        try:\n            lscpu_output = subprocess.check_output([lscpu_cmd],\n                                                   stderr=subprocess.STDOUT)\n            # handle python2 vs 3 (bytes vs str type)\n            if isinstance(lscpu_output, bytes):\n                lscpu_output = lscpu_output.decode(\'utf-8\')\n\n            cpu_info = lscpu_output.split(\'\\n\')\n\n        except Exception as e:\n            print(""Problem getting CPU info: {}"".format(e))\n            sys.exit(1)\n\n        # parse it\n        for line in cpu_info:\n            #      NUMA_NODES_STR_       = ""NUMA node(s)""\n            if line.find(NUMA_NODES_STR_) == 0:\n                self.num_numa_nodes = int(line.split("":"")[1].strip())\n            #      CPU_SOCKETS_STR_      = ""Socket(s)""\n            elif line.find(CPU_SOCKETS_STR_) == 0:\n                self.num_cpu_sockets = int(line.split("":"")[1].strip())\n            #      CORES_PER_SOCKET_STR_ = ""Core(s) per socket""\n            elif line.find(CORES_PER_SOCKET_STR_) == 0:\n                self.num_cores_per_socket = int(line.split("":"")[1].strip())\n            #      THREADS_PER_CORE_STR_ = ""Thread(s) per core""\n            elif line.find(THREADS_PER_CORE_STR_) == 0:\n                self.num_threads_per_core = int(line.split("":"")[1].strip())\n            #      LOGICAL_CPUS_STR_     = ""CPU(s)""\n            elif line.find(LOGICAL_CPUS_STR_) == 0:\n                self.num_logical_cpus = int(line.split("":"")[1].strip())\n\n    def windows_init(self):\n        raise NotImplementedError(""Windows Support not yet implemented"")\n\n    def mac_init(self):\n        raise NotImplementedError(""Mac Support not yet implemented"")\n'"
benchmarks/image_recognition/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/language_translation/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/reinforcement/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
models/image_recognition/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/language_translation/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/object_detection/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/recommendation/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/reinforcement/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n'"
tests/test_utils/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
tests/test_utils/io.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport os\nimport json\n\n\ndef parse_json_files(json_dir_path):\n    """"""\n    Reads the JSON files in the specified directory.  Checks for a value number of columns in\n    each row. Returns the JSON files values as a list of tuples.\n    """"""\n    values = []\n    for model_file in os.listdir(json_dir_path):\n        file_path = os.path.join(json_dir_path, model_file)\n        with open(file_path) as f:\n            data = json.load(f)\n            for x in data:\n                values.append(\n                    tuple((x[\'input\'], x[\'output\'], model_file + "" :: "" + x[\'_comment\'])))\n    return values\n'"
tests/test_utils/platform_config.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Constants used for test mocks\nSYSTEM_TYPE = ""Linux""\nLSCPU_PATH = ""/usr/bin/lscpu""\nLSCPU_OUTPUT = (""Architecture:          x86_64\\n""\n                ""CPU(s):                112\\n""\n                ""Thread(s) per core:    2\\n""\n                ""Core(s) per socket:    28\\n""\n                ""Socket(s):             2\\n""\n                ""NUMA node(s):          2\\n"")\n\n\ndef set_mock_system_type(mock_platform):\n    """"""\n    Sets the system type return value to Linux, which is currently the only\n    supported system type.\n    """"""\n    mock_platform.system.return_value = SYSTEM_TYPE\n\n\ndef set_mock_os_access(mock_os):\n    """"""\n    Sets the os.access return value to True\n    """"""\n    mock_os.access.return_value = True\n\n\ndef set_mock_lscpu_subprocess_values(mock_subprocess):\n    """"""\n    Sets mock return values for two subprocess calls that are made in\n    platform_util, which returns the lscpu path and the lscpu output.\n    """"""\n    mock_subprocess.check_output.side_effect = [LSCPU_PATH,\n                                                LSCPU_OUTPUT]\n'"
tests/unit/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
tests/unit/test_launch_benchmark.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\nfrom __future__ import print_function\nfrom conditional import conditional\n\nimport os\nimport sys\n\nimport pytest\nfrom mock import MagicMock, patch as mock_patch\n\nfrom launch_benchmark import LaunchBenchmark\n\n\n# Example args and output strings for testing mocks\ntest_model_name = ""resnet50""\ntest_framework = ""tensorflow""\ntest_mode = ""inference""\ntest_precision = ""fp32""\ntest_docker_image = ""foo""\ntest_batch_size = ""100""\ntest_num_cores = ""1""\n# need a valid file for tests to work, see conftest.py for where this is managed\ntest_input_graph = ""test.pb""\ntest_tfserving_framework = ""tensorflow_serving""\n\n\n@pytest.fixture\ndef mock_platform_util(patch):\n    return patch(""common.base_benchmark_util.platform_util.PlatformUtil"",\n                 MagicMock(num_cpu_sockets=1, num_cores_per_socket=1, num_threads_per_core=1, num_local_cpus=1,\n                           num_numa_nodes=1))\n\n\n@pytest.fixture\ndef mock_os(patch):\n    return patch(""common.base_benchmark_util.platform_util.os"")\n\n\n@pytest.fixture\ndef mock_subprocess(patch):\n    return patch(""common.base_benchmark_util.platform_util.subprocess"")\n\n\n@pytest.fixture\ndef mock_popen(patch):\n    return patch(""subprocess.Popen"")\n\n\n@pytest.fixture\ndef mock_system_platform(patch):\n    return patch(""common.base_benchmark_util.platform_util.system_platform"")\n\n\n@pytest.fixture\ndef mock_path_exists(patch):\n    return patch(""os.path.exists"", MagicMock(return_value=True))\n\n\n@pytest.fixture\ndef mock_isfile(patch):\n    return patch(""os.path.isfile"", MagicMock(return_value=True))\n\n\n@pytest.fixture\ndef mock_isdir(patch):\n    return patch(""os.path.isdir"", MagicMock(return_value=True))\n\n\n@pytest.fixture\ndef mock_islink(patch):\n    return patch(""os.path.islink"", MagicMock(return_value=False))\n\n\n@pytest.fixture\ndef mock_stat(patch):\n    stat = MagicMock()\n    stat.return_value.st_nlink = 0\n    return patch(""os.stat"", stat)\n\n\n@pytest.fixture(autouse=True)\ndef launch_benchmark(mock_platform_util, request, mock_isdir, mock_isfile, mock_islink, mock_stat, mock_path_exists):\n    """"""sets up launch_benchmark obj for every test case and handles catching errors if we wanna test that\n       To catch errors called when running launch_benchmark, use something like:\n           [\'catch_error\', SystemExit, [{args}], {error_message}] in parametrize\n       where args are args to pass to the benchmark creation and error_message is an optional error message to check for\n       otherwise just pass in the req args you\'d like to run with via []\n       catch_error_override_all_params will not use any example_req_args when creating benchmark\n\n       Sample request.params:\n       [\'catch_error\', SystemExit, []]\n       [\'catch_error_override_all_params\', SystemExit, []]\n       [\'catch_error\', SystemExit, [\'--framework\', \'foo\'], ""The specified framework is not supported""]]\n       """"""\n    catch_error = False\n    error = None\n    error_message = \'\'\n\n    # deleting from this sometimes so need to redeclare it, probably can do that differently...\n    example_req_args = [""--model-name"", test_model_name,\n                        ""--framework"", test_framework,\n                        ""--mode"", test_mode,\n                        ""--precision"", test_precision,\n                        ""--docker-image"", test_docker_image,\n                        ""--batch-size"", test_batch_size,\n                        ""--num-cores"", test_num_cores]\n\n    if hasattr(request, \'param\'):\n        if \'catch_error\' in request.param[0]:\n            catch_error = True\n            error = request.param[1]\n            if request.param[0] != \'catch_error_override_all_params\':\n                # TODO: make more efficient! Want to get rid of any example_req_args that exist in request.param[2]\n                # using safe deletion from the back\n                for idx in range(len(example_req_args) - 1, -1, -1):\n                    arg = example_req_args[idx]\n                    if not arg.startswith(\'--\'):\n                        continue\n                    if arg in request.param[2]:\n                        # flags are always followed by their value in example_req_args, so delete both arg and its value\n                        del example_req_args[idx]\n                        del example_req_args[idx]\n                req_args = request.param[2] + example_req_args\n            else:\n                req_args = request.param[2]\n            error_message = request.param[3] if len(request.param) == 4 else \'\'\n        else:\n            # add extra arguments to the default ones when calling LaunchBenchmark\n            req_args = request.param + example_req_args\n    else:\n        # only use default arguments when calling LaunchBenchmark\n        req_args = example_req_args\n\n    with mock_patch.object(sys, ""argv"", [\'run_tf_benchmark.py\'] + req_args):\n        with conditional(catch_error, pytest.raises(error)) as e:\n            obj = LaunchBenchmark(mock_platform_util)\n            if error_message:\n                assert error_message in str(e.value)\n            return obj\n\n\ndef test_launch_benchmark_parse_args(launch_benchmark):\n    """"""\n    Verifies that that arg parsing gives us the expected results.\n    """"""\n    assert launch_benchmark.args.model_name == test_model_name\n    assert launch_benchmark.args.framework == test_framework\n    assert launch_benchmark.args.mode == test_mode\n    assert launch_benchmark.args.precision == test_precision\n    assert launch_benchmark.args.docker_image == test_docker_image\n    assert launch_benchmark.unknown_args == []\n\n\n@pytest.mark.parametrize(\'launch_benchmark\', [[""--test"", ""foo""]], indirect=True)\ndef test_launch_benchmark_parse_unknown_args(launch_benchmark):\n    """"""\n    Checks parsing of unknown args\n    """"""\n    assert launch_benchmark.unknown_args == [""--test""]\n\n\n@pytest.mark.parametrize(\'launch_benchmark\', [[\'catch_error_override_all_params\', SystemExit, []],\n                                              [\'catch_error\', SystemExit, [\'--framework\', \'foo\'],\n                                                  ""The specified framework is not supported""],\n                                              [\'catch_error\', SystemExit, [\'--docker-image\', \'test \'],\n                                                  ""docker image string should not have whitespace(s)""],\n                                              [\'catch_error\', ValueError, [""--model-name"", test_model_name,\n                                                                           ""--framework"", test_framework,\n                                                                           ""--mode"", ""training"",\n                                                                           ""--precision"", test_precision,\n                                                                           ""--docker-image"", test_docker_image,\n                                                                           ""--benchmark-only"",\n                                                                           ""--output-results""],\n                                                  ""--output-results can only be used when running ""\n                                                  ""inference with a dataset""],\n                                              [\'catch_error\', ValueError, [""--model-name"", test_model_name,\n                                                                           ""--framework"", test_framework,\n                                                                           ""--mode"", ""training"",\n                                                                           ""--precision"", test_precision,\n                                                                           ""--docker-image"", test_docker_image,\n                                                                           ""--accuracy-only"",\n                                                                           ""--output-results""],\n                                                  ""--output-results can only be used when running ""\n                                                  ""inference with a dataset""],\n                                              [\'catch_error_override_all_params\', SystemExit,\n                                               [""--model-name"", test_model_name,\n                                                ""--framework"", test_framework,\n                                                ""--mode"", test_mode,\n                                                ""--precision"", test_precision,\n                                                ""--volume"", ""~:test""],\n                                                  ""Volume mounts can only be used when running in a docker container""]\n                                              ], indirect=True)\ndef test_launch_benchmark_parse_bad_args(launch_benchmark):\n    """"""\n    Checks for failures with no args or bad args\n    """"""\n    pass\n\n\n@pytest.mark.parametrize(\'launch_benchmark\', [[""--model-name"", test_model_name,\n                                               ""--framework"", test_framework,\n                                               ""--mode"", ""training"",\n                                               ""--precision"", test_precision,\n                                               ""--docker-image"", test_docker_image,\n                                               ""--data-location"", ""."",\n                                               ""--benchmark-only"",\n                                               ""--output-results""]])\ndef test_output_results_with_accuracy(launch_benchmark, mock_system_platform, mock_os, mock_subprocess):\n    """"""\n    Tests that the launch script validation passes when running accuracy with output\n    """"""\n    pass\n\n\ndef test_launch_benchmark_validate_model(launch_benchmark, mock_popen):\n    """"""\n    Verifies that a valid model name passes validation and starts a docker container.\n    """"""\n    launch_benchmark.main()\n    assert mock_popen.called\n    args, kwargs = mock_popen.call_args\n    assert ""docker"" == args[0][0]\n    assert ""run"" == args[0][1]\n\n\ndef test_bare_metal(launch_benchmark, mock_popen):\n    """""" Tests the bare metal launch script function """"""\n    test_env_vars = {""TEST_ENV_VAR_1"": ""a"", ""TEST_ENV_VAR_2"": ""b""}\n    launch_benchmark.run_bare_metal(""/foo"", ""/bar"", test_env_vars)\n    assert mock_popen.called\n    args, kwargs = mock_popen.call_args\n\n    # make sure that the start script is run\n    assert ""bash"" == args[0][0]\n    assert ""start.sh"" in args[0][1]\n\n    # ensure env vars are set\n    assert os.environ[""TEST_ENV_VAR_1""] == test_env_vars[""TEST_ENV_VAR_1""]\n    assert os.environ[""TEST_ENV_VAR_2""] == test_env_vars[""TEST_ENV_VAR_2""]\n\n\n@pytest.mark.parametrize(\'launch_benchmark\', [[""--in-graph"", test_input_graph]], indirect=True)\ndef test_launch_benchmark_tensorflow_serving_framework(launch_benchmark, mock_popen):\n    """"""\n    Tests that the launch script works for tensorflow serving framework\n    """"""\n    test_env_vars = {""TEST_ENV_VAR_1"": ""a"", ""TEST_ENV_VAR_2"": ""b""}\n    # Override framework and docker image.\n    launch_benchmark.args.framework = test_tfserving_framework\n    launch_benchmark.args.docker_image = None\n    launch_benchmark.run_bare_metal(""/foo"", ""/bar"", test_env_vars)\n    assert mock_popen.called\n    args, kwargs = mock_popen.call_args\n\n    assert launch_benchmark.args.input_graph == test_input_graph\n    assert launch_benchmark.args.framework == test_tfserving_framework\n\n    # make sure that the start script is run\n    assert ""bash"" == args[0][0]\n    assert ""start.sh"" in args[0][1]\n\n    # ensure env vars are set\n    assert os.environ[""TEST_ENV_VAR_1""] == test_env_vars[""TEST_ENV_VAR_1""]\n    assert os.environ[""TEST_ENV_VAR_2""] == test_env_vars[""TEST_ENV_VAR_2""]\n\n\ndef test_help(mock_platform_util, capsys):\n    """""" Tests `launch_benchmark.py --help` output and ensures there is no error """"""\n    with mock_patch.object(sys, \'argv\', [""launch_benchmark.py"", ""--help""]):\n        with pytest.raises(SystemExit) as e:\n            LaunchBenchmark(mock_platform_util)\n        assert e.value.code == 0\n\n        # get the stdout and check the output\n        captured = capsys.readouterr()\n        assert ""usage: launch_benchmark.py [-h] "" in captured.out\n\n        # check for an arg that is only in launch_benchmark.py\n        assert ""--docker-image DOCKER_IMAGE"" in captured.out\n\n        # check for an arg that\'s in base_benchmark_util.py\n        assert ""-f FRAMEWORK, --framework FRAMEWORK"" in captured.out\n\n        # make sure there were no errors printed\n        assert ""error"" not in captured.out.lower()\n\n\ndef test_launch_benchmark_custom_volume(launch_benchmark, mock_popen):\n    """"""\n    Verifies the docker run command includes custom volumes\n    """"""\n    custom_volumes = [""~:/foo1"", ""~:/foo2""]\n    launch_benchmark.args.custom_volumes = custom_volumes\n    launch_benchmark.main()\n    assert mock_popen.called\n    args, _ = mock_popen.call_args\n    # convert the run command args to a string and then check for the custom volume mounts\n    docker_run_cmd = "" "".join(args[0])\n    for custom_volume in custom_volumes:\n        assert ""--volume {}"".format(custom_volume) in docker_run_cmd\n\n\n@pytest.mark.parametrize(""precision,expected_disable_tcmalloc"", [[""int8"", ""False""],\n                                                                 [""fp32"", ""True""]])\ndef test_disable_tcmalloc(launch_benchmark, mock_popen, precision, expected_disable_tcmalloc):\n    launch_benchmark.args.precision = precision\n    launch_benchmark.main()\n    assert mock_popen.called\n    args, _ = mock_popen.call_args\n    # convert the run command args to a string and then check for the custom volume mounts\n    docker_run_cmd = "" "".join(args[0])\n    assert ""--env DISABLE_TCMALLOC="".format(expected_disable_tcmalloc) in docker_run_cmd\n'"
benchmarks/common/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/common/tensorflow/run_tf_benchmark.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom argparse import ArgumentParser\nfrom common.base_benchmark_util import BaseBenchmarkUtil\n\n\nclass ModelBenchmarkUtil(BaseBenchmarkUtil):\n    """"""Benchmark util for int8 and fp32 models """"""\n\n    def main(self):\n        # Additional args that are used internally for the start script to call the model_init.py\n        arg_parser = ArgumentParser(parents=[self._common_arg_parser],\n                                    description=\'Parse args for benchmark \'\n                                                \'interface\')\n\n        arg_parser.add_argument(""--intelai-models"",\n                                help=""Local path to the intelai optimized ""\n                                     ""model scripts"",\n                                nargs=\'?\',\n                                dest=""intelai_models"")\n\n        arg_parser.add_argument(""--benchmark-dir"",\n                                help=""Local path intelai benchmark directory"",\n                                nargs=\'?\',\n                                dest=""benchmark_dir"")\n\n        arg_parser.add_argument(""--use-case"",\n                                help=""The corresponding use case of the given ""\n                                     ""model "",\n                                nargs=\'?\',\n                                dest=""use_case"")\n\n        args, unknown = arg_parser.parse_known_args()\n        mi = self.initialize_model(args, unknown)\n        if mi is not None:  # start model initializer if not None\n            mi.run()\n\n\nif __name__ == ""__main__"":\n    util = ModelBenchmarkUtil()\n    util.main()\n'"
benchmarks/common/utils/__init__.py,0,b''
benchmarks/common/utils/multi_instance.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\n""""""Multi instance utils module.""""""\n\nfrom common.platform_util import CPUInfo\n\n\ndef buckets(array, bucket_size):\n    """"""\n    Split array into multiple arrays with specified size.\n    :param array: array that will be splited\n    :type array: List[Any]\n    :param bucket_size_size: target arrays size\n    :type bucket_size_size: int\n    :return: list with parameters\n    :rtype: List[List[Any]]\n    """"""\n    bucket_size_list = []\n    for i in range(0, len(array), bucket_size):\n        bucket_size_list.append(array[i:i + bucket_size])\n\n    return bucket_size_list\n\n\nclass InferencePrefix:\n    """"""Multi instance class.""""""\n\n    def __init__(self, sockets=0, instances=0, cores_per_instance=0):\n        """"""\n        Initialize multi instance class.\n        :param sockets: sockets used for execution, defaults to 0\n        :type sockets: int, optional\n        :param instances: number of instances, defaults to 0\n        :type instances: int, optional\n        :param cores_per_instance: number of cores that will be used by one instance, defaults to 0\n        :type cores_per_instance: int, optional\n        """"""\n        self._cpu_information = CPUInfo()\n        self._sockets = sockets\n        self._instances = instances\n        self._cores_per_instance = cores_per_instance\n\n    @property\n    def is_basic_configuration(self):\n        """"""\n        Check if workload is multi instance or should use core/memory binding.\n        :return: True if basic configuration else False\n        :rtype: bool\n        """"""\n        # Expected single instance parameters\n        single_instance_params = self._platform_single_instance_args()\n\n        # Current workload parameters\n        default_cores_per_instance = self._cpu_information.cores_per_socket * self.sockets\n        workload_params = {\n            ""cores_per_instance"": self._cores_per_instance or default_cores_per_instance,\n            ""instance"": self._instances if self._instances != 0 else 1,\n            ""sockets"": self.sockets,\n        }\n\n        return single_instance_params == workload_params\n\n    @property\n    def sockets(self):\n        """"""\n        Return amount of sockets used for execution.\n        :return: amount of sockets\n        :rtype: int\n        """"""\n        if self._sockets == 0:\n            sockets = self._cpu_information.sockets\n        else:\n            sockets = self._sockets\n            if sockets > self._cpu_information.sockets:\n                raise Exception(""The specified number of sockets is greater ""\n                                ""than the number of server available sockets."")\n\n        return sockets\n\n    @property\n    def cores_per_socket(self):\n        """"""\n        Return amount of cores per socket used for execution.\n        :raises Exception: Cores assigned to one instance > cores available on one socket\n        :return: amount of cores\n        :rtype: int\n        """"""\n        if self._cores_per_instance > 0:\n            if self._cores_per_instance > self._cpu_information.cores_per_socket:\n                raise Exception(""Cores assigned to one instance is greater than amount of cores on one socket."")\n\n            cores_per_socket = self._cpu_information.cores_per_socket - \\\n                self._cpu_information.cores_per_socket % self._cores_per_instance\n        else:\n            cores_per_socket = self._cpu_information.cores_per_socket\n\n        return cores_per_socket\n\n    @property\n    def cores(self):\n        """"""\n        Return amount of cores used for execution.\n        :return: amount of cores used for execution\n        :rtype: int\n        """"""\n        cores = self.cores_per_socket * self.sockets\n        return cores\n\n    @property\n    def instances_per_socket(self):\n        """"""\n        Return number of instances.\n        :return: number of instances\n        :rtype: int\n        """"""\n        if self._instances > 0:\n            if self._instances % self.sockets != 0:\n                raise Exception(""Instances could not be distributed equally between sockets. ""\n                                ""Amount of instances should be divisible by socket amount. ""\n                                ""{} % {} != 0"".format(self._instances, self.sockets))\n\n            instances = int(self._instances / self.sockets)\n        elif self._cores_per_instance > 0:\n            instances = int(self.cores_per_socket / self._cores_per_instance)\n\n        else:\n            instances = 0\n\n        return instances\n\n    @property\n    def instances(self):\n        """"""\n        Return total number of instances.\n        :return: total number of instances\n        :rtype: int\n        """"""\n        # Set number of instances to 1 if instances_per_socket == 0\n        if self.is_basic_configuration:\n            return 1\n        else:\n            return (self.instances_per_socket * self.sockets) or 1\n\n    @property\n    def cores_per_instance(self):\n        """"""\n        Return cores per instance.\n        :return: amount of cores per instance\n        :rtype: int\n        """"""\n        if not self.is_basic_configuration:\n            if self._cores_per_instance > 0:\n                if self._cores_per_instance * self.instances_per_socket > self.cores_per_socket:\n                    raise Exception(""Total cores used on one socket > cores available on one socket. ""\n                                    ""{} * {} > {}"".format(\n                                        self._cores_per_instance,\n                                        self.instances_per_socket,\n                                        self.cores_per_socket,\n                                    ))\n\n                cores_per_instance = self._cores_per_instance\n            else:\n                instances_per_socket = self.instances_per_socket\n                if self.cores_per_socket % instances_per_socket != 0:\n                    raise Exception(""Amount of cores per socket should be divisible by amount of instances per socket."")\n\n                cores_per_instance = self.cores_per_socket // instances_per_socket\n\n        else:\n            cores_per_instance = self._cpu_information.cores\n\n        return int(cores_per_instance)  # type: ignore\n\n    @property\n    def sockets_per_instance(self):\n        """"""\n        Return amount of sockets per instance.\n        :return: amount of sockets per instance\n        :rtype: int\n        """"""\n        if self.is_basic_configuration:\n            sockets = self._cpu_information.sockets\n        else:\n            sockets = 1\n\n        return sockets\n\n    @staticmethod\n    def get_cores_range(cores, ht_cores, use_ht):\n        """"""\n        Return the range of cores.\n        :param cores: number of cores\n        :param ht_cores: number of cores with hyperthreading\n        :param use_ht: defines if hyperthreading should be used\n        :return: range of cores\n        """"""\n        if use_ht and ht_cores:\n            cores_range = ""{},{}"".format(cores, ht_cores)\n        else:\n            cores_range = cores\n\n        return cores_range\n\n    def split_cores(self):\n        """"""\n        Return cores in instance buckets.\n        :raises Exception: 1 instance on sockets > 1 not implemented\n        :return: instance buckets\n        :rtype: Dict[str, List[List[Dict[str, Any]]]]\n        """"""\n        membind_info = self._cpu_information.binding_information\n        cores_per_instance = self.cores_per_instance\n        if cores_per_instance == 0:\n            raise Exception(""1 instance on sockets > 1 not implemented."")\n\n        bucketed_cores = {}\n        for node_id in range(self.sockets):\n            socket_cores = membind_info[node_id][:self.cores_per_socket]\n            instance_buckets = buckets(socket_cores, cores_per_instance)\n            bucketed_cores.update({str(node_id): instance_buckets[0:self.instances_per_socket]})\n\n        return bucketed_cores\n\n    def generate_multi_instance_ranges(self, use_ht=False):\n        """"""\n        Create config for multi-instance execution.\n        :param use_ht: defines if hyperthreading should be used\n        :return: information about splitted cores\n        """"""\n        instance_binding = []\n        split_cores = self.split_cores()\n        for instance_buckets in split_cores.values():\n            for instance_config in instance_buckets:\n                if len(instance_config) == 1:\n                    cores = instance_config[0].get(""cpu_id"")\n                    ht_cores = instance_config[0].get(""ht_cpu_id"", None)\n                else:\n                    cores = ""{first}-{last}"".format(first=instance_config[0].get(""cpu_id""),\n                                                    last=instance_config[-1].get(""cpu_id""))\n\n                    first_ht = instance_config[0].get(""ht_cpu_id"", None)\n                    last_ht = instance_config[-1].get(""ht_cpu_id"", None)\n                    if first_ht is None or last_ht is None:\n                        ht_cores = None\n                    else:\n                        ht_cores = ""{first}-{last}"".format(first=first_ht, last=last_ht)\n\n                cores_range = self.get_cores_range(cores, ht_cores, use_ht)\n                instance_binding.append({""cores_range"": cores_range,\n                                         ""socket_id"": instance_config[0].get(""socket_id"")})\n\n        return instance_binding\n\n    def generate_multi_instance_prefix(self, command, use_ht=False):\n        """"""\n        Add \'numactl\' prefix for multi-instance execution.\n        :param command: command that will be run using numactl\n        :param use_ht: defines if hyperthreading should be used\n        :return: array of commands if multi-instance else command\n        """"""\n        if self.is_basic_configuration:\n            return [command]\n\n        commands_array = []\n        for instance in self.generate_multi_instance_ranges(use_ht):\n            numa_cmd = [""numactl"",\n                        ""--membind={}"".format(instance.get(""socket_id"")),\n                        ""--physcpubind={}"".format(instance.get(""cores_range""))]\n\n            commands_array.append(numa_cmd + command)\n\n        return commands_array\n\n    def _platform_single_instance_args(self):\n        """"""\n        Return single instance parameters for current platform.\n        :return: single instance parameters for current platform\n        :rtype: Dict[str, int]\n        """"""\n        return {\n            ""cores_per_instance"": self._cpu_information.cores,\n            ""instance"": 1,\n            ""sockets"": self._cpu_information.sockets,\n        }\n'"
benchmarks/common/utils/validators.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom argparse import ArgumentTypeError\nimport os\nimport re\n\n""""""\nFunctions used in `type=` arguments to ArgumentParser\nUnfortunately can only take 1 string as argument https://docs.python.org/3/library/argparse.html#type\n""""""\n\n\ndef check_for_link(value):\n    """"""\n    Throws an error if the specified path is a link. os.islink returns\n    True for sym links.  For files, we also look at the number of links in\n    os.stat() to determine if it\'s a hard link.\n    """"""\n    if os.path.islink(value) or \\\n            (os.path.isfile(value) and os.stat(value).st_nlink > 1):\n        raise ArgumentTypeError(""{} cannot be a link."".format(value))\n\n\ndef check_no_spaces(value):\n    """"""checks for spaces in string""""""\n    if \' \' in value:\n        raise ArgumentTypeError(""{} should not have whitespace(s)."")\n    return value\n\n\ndef check_positive_number(value):\n    if value:\n        value = int(value)\n        if value <= 0:\n            raise ArgumentTypeError(""{} should be a positive number."")\n    return value\n\n\ndef check_positive_number_or_equal_to_negative_one(value):\n    if value:\n        value = int(value)\n        if value == 0 or value < -1:\n            raise ArgumentTypeError(""{} is not valid."".format(value))\n    return value\n\n\ndef check_valid_filename(value):\n    """"""verifies filename exists and isn\'t a link""""""\n    if value is not None:\n        if not os.path.isfile(value):\n            raise ArgumentTypeError(""{} does not exist or is not a file."".\n                                    format(value))\n        check_for_link(value)\n    return value\n\n\ndef check_valid_folder(value):\n    """"""verifies filename exists and isn\'t a link""""""\n    if value is not None:\n        if not os.path.isdir(value):\n            raise ArgumentTypeError(""{} does not exist or is not a directory."".\n                                    format(value))\n        check_for_link(value)\n    return value\n\n\ndef check_valid_file_or_dir(value):\n    """"""verfies file/dir exists and isn\'t a link""""""\n    if value is not None:\n        if not os.path.exists(value):\n            raise ArgumentTypeError(""{} does not exist."".format(value))\n        check_for_link(value)\n    return value\n\n\ndef check_volume_mount(value):\n    """"""\n    Verifies that the value is a valid docker volume mount, where there should be\n    at least two fields separated by a : (for the local directory to mount and the\n    path to the where the directory will be mounted in the container. The third\n    optional field is for extra options like read only.\n    """"""\n    if value:\n        # Check that we have at least 2 fields and at most 3 fields\n        if not 3 > value.count("":"") > 0:\n            raise ArgumentTypeError(\n                ""{} is not a valid volume mount string where \':\' is used to separate the fields. ""\n                ""See https://docs.docker.com/storage/volumes for information on formatting the volume ""\n                ""mount string"".format(value))\n\n        # Check that the local directory specified is a valid folder and not a link\n        check_valid_folder(value.split(\':\')[0])\n    return value\n\n\ndef check_shm_size(value):\n    """"""verfies the format of docker shm-size """"""\n    if value is not None:\n        if not re.match(""([1-9][0-9]*)[\'b\',\'k\',\'m\',\'g\']"", value):\n            raise ArgumentTypeError(""{} does not follow the --shm-size format definition."".format(value))\n    return value\n'"
benchmarks/image_recognition/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/language_translation/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/reinforcement/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#'"
models/image_recognition/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/language_translation/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/object_detection/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/recommendation/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/reinforcement/tensorflow/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n'"
tests/unit/common/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
tests/unit/common/test_base_model_init.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\nfrom contextlib import contextmanager\nimport os\nimport pytest\nimport sys\nimport tempfile\n\ntry:\n    # python 2\n    from cStringIO import StringIO\nexcept ImportError:\n    # python 3\n    # only supports unicode so can\'t be used in python 2 for sys.stdout\n    # because (from `print` documentation)\n    # ""All non-keyword arguments are converted to strings like str() does""\n    from io import StringIO\n\n\nfrom mock import MagicMock, patch\n\nfrom benchmarks.common.base_model_init import BaseModelInitializer\nfrom benchmarks.common.base_model_init import set_env_var\n\n\n@contextmanager\ndef catch_stdout():\n    _stdout = sys.stdout\n    sys.stdout = caught_output = StringIO()\n    try:\n        yield caught_output\n    finally:\n        sys.stdout = _stdout\n        caught_output.close()\n\n\n@pytest.fixture\ndef mock_json(patch):\n    return patch(\'json\')\n\n\n@pytest.fixture\ndef mock_glob(patch):\n    return patch(\'glob.glob\')\n\n\n# Example args and output strings for testing mocks\ntest_model_name = ""resnet50""\ntest_framework = ""tensorflow""\ntest_mode = ""inference""\ntest_precision = ""fp32""\ntest_docker_image = ""foo""\nexample_req_args = [""--model-name"", test_model_name,\n                    ""--framework"", test_framework,\n                    ""--mode"", test_mode,\n                    ""--precision"", test_precision,\n                    ""--docker-image"", test_docker_image]\n\n\n@patch(""common.platform_util.os"")\n@patch(""common.platform_util.system_platform"")\n@patch(""common.platform_util.subprocess"")\n@patch(""os.system"")\ndef test_base_model_initializer(\n        mock_system, mock_subprocess, mock_platform, mock_os):\n    # Setup base model init with test settings\n    platform_util = MagicMock()\n    args = MagicMock(verbose=True, model_name=test_model_name)\n    os.environ[""PYTHON_EXE""] = ""python""\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n\n    # call run_command and then check the output\n    test_run_command = ""python foo.py""\n    base_model_init.run_command(test_run_command)\n    mock_system.assert_called_with(test_run_command)\n\n\ndef test_env_var_already_set():\n    """""" Tests setting and env var when it\'s already set """"""\n    env_var = ""model-zoo-test-env-var-name""\n    original_value = ""original""\n    modified_value = ""modified""\n\n    try:\n        # Set the env var to an initial value\n        os.environ[env_var] = original_value\n\n        # Try to modify that value but set overwrite flag to False\n        set_env_var(env_var, modified_value, overwrite_existing=False)\n\n        # Verify that we still have the original value\n        assert os.environ[env_var] == original_value\n\n        # Try to modify the value with the overwrite flag set to True\n        set_env_var(env_var, modified_value, overwrite_existing=True)\n\n        # Verify that we now have the modified value\n        assert os.environ[env_var] == modified_value\n    finally:\n        if os.environ.get(env_var):\n            del os.environ[env_var]\n\n\ndef test_env_var_not_already_set():\n    """""" Tests setting and env var when it\'s not already set """"""\n    env_var = ""model-zoo-test-env-var-name""\n    new_value = ""new_value""\n\n    try:\n        # Make sure that the env var is unset to start\n        if os.environ.get(env_var):\n            del os.environ[env_var]\n\n        # Try setting the value with the overwrite flag set to False\n        set_env_var(env_var, new_value, overwrite_existing=False)\n\n        # Verify that we now have a value\n        assert os.environ[env_var] == new_value\n\n        # Unset the env var and set it with the overwrite flag set to True\n        del os.environ[env_var]\n        new_value = ""another_new_value""\n        set_env_var(env_var, new_value, overwrite_existing=True)\n\n        # Verify that we have the new value set\n        assert os.environ[env_var] == new_value\n    finally:\n        if os.environ.get(env_var):\n            del os.environ[env_var]\n\n\ndef test_set_kmp_vars_config_json_does_not_exists():\n    """"""Test config.json does not exist""""""\n    # Setup base model init with test settings\n    platform_util = MagicMock()\n    args = MagicMock(verbose=True, model_name=test_model_name)\n    os.environ[""PYTHON_EXE""] = ""python""\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n\n    config_file_path = \'/test/foo/config.json\'\n\n    with catch_stdout() as caught_output:\n        base_model_init.set_kmp_vars(config_file_path)\n        output = caught_output.getvalue()\n\n    assert ""Warning: File {} does not exist and \\\n            cannot be used to set KMP environment variables"".format(config_file_path) == output.strip()\n\n\ndef test_set_kmp_vars_config_json_exists(mock_json):\n    """"""Test config.json when exists""""""\n    # Setup base model init with test settings\n    platform_util = MagicMock()\n    args = MagicMock(verbose=True, model_name=test_model_name)\n    os.environ[""PYTHON_EXE""] = ""python""\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n\n    file_descriptor, config_file_path = tempfile.mkstemp(suffix="".json"")\n\n    base_model_init.set_kmp_vars(config_file_path)\n\n\n@pytest.mark.parametrize(\'precision\', [\'int8\'])\ndef test_command_prefix_tcmalloc_int8(precision, mock_glob):\n    """""" For Int8 models, TCMalloc should be enabled by default and models should include\n     LD_PRELOAD in the command prefix, unless disable_tcmalloc=True is set """"""\n    platform_util = MagicMock()\n    args = MagicMock(verbose=True, model_name=test_model_name)\n    test_tcmalloc_lib = ""/usr/lib/libtcmalloc.so.4.2.6""\n    mock_glob.return_value = [test_tcmalloc_lib]\n    os.environ[""PYTHON_EXE""] = ""python""\n    args.socket_id = 0\n    args.precision = precision\n\n    # If tcmalloc is not disabled, we should have LD_PRELOAD in the prefix\n    args.disable_tcmalloc = False\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command_prefix = base_model_init.get_command_prefix(args.socket_id)\n    assert ""LD_PRELOAD={}"".format(test_tcmalloc_lib) in command_prefix\n    assert ""numactl --cpunodebind=0 --membind=0"" in command_prefix\n\n    # If tcmalloc is disabled, LD_PRELOAD shouild not be in the prefix\n    args.disable_tcmalloc = True\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command_prefix = base_model_init.get_command_prefix(args.socket_id)\n    assert ""LD_PRELOAD={}"".format(test_tcmalloc_lib) not in command_prefix\n    assert ""numactl --cpunodebind=0 --membind=0"" in command_prefix\n\n    # If numactl is set to false, we should not have numactl in the prefix\n    args.disable_tcmalloc = False\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command_prefix = base_model_init.get_command_prefix(args.socket_id, numactl=False)\n    assert ""LD_PRELOAD={}"".format(test_tcmalloc_lib) in command_prefix\n    assert ""numactl"" not in command_prefix\n\n\n@pytest.mark.parametrize(\'precision\', [\'fp32\'])\ndef test_command_prefix_tcmalloc_fp32(precision, mock_glob):\n    """""" FP32 models should have TC Malloc disabled by default, but models should\n    include LD_PRELOAD in the command prefix if disable_tcmalloc=False is explicitly set. """"""\n    platform_util = MagicMock()\n    args = MagicMock(verbose=True, model_name=test_model_name)\n    test_tcmalloc_lib = ""/usr/lib/libtcmalloc.so.4.2.6""\n    mock_glob.return_value = [test_tcmalloc_lib]\n    os.environ[""PYTHON_EXE""] = ""python""\n    args.socket_id = 0\n    args.precision = precision\n\n    # By default, TCMalloc should not be used\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command_prefix = base_model_init.get_command_prefix(args.socket_id)\n    assert ""LD_PRELOAD={}"".format(test_tcmalloc_lib) not in command_prefix\n    assert ""numactl --cpunodebind=0 --membind=0"" in command_prefix\n\n    # If tcmalloc is disabled, LD_PRELOAD shouild not be in the prefix\n    args.disable_tcmalloc = False\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command_prefix = base_model_init.get_command_prefix(args.socket_id)\n    assert ""LD_PRELOAD={}"".format(test_tcmalloc_lib) in command_prefix\n    assert ""numactl --cpunodebind=0 --membind=0"" in command_prefix\n\n    # If numactl is set to false, we should not have numactl in the prefix\n    args.disable_tcmalloc = True\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command_prefix = base_model_init.get_command_prefix(args.socket_id, numactl=False)\n    assert ""LD_PRELOAD={}"".format(test_tcmalloc_lib) not in command_prefix\n    assert ""numactl"" not in command_prefix\n\n\ndef test_multi_instance_train_prefix():\n    platform_util = MagicMock()\n    args = MagicMock(verbose=True, model_name=test_model_name)\n    args.num_processes = 2\n    args.num_processes_per_node = 1\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command = base_model_init.get_multi_instance_train_prefix(option_list=[""--genv:test""])\n    assert command == ""mpirun -n 2 -ppn 1 --genv test ""\n\n    args.num_processes = None\n    args.num_processes_per_node = None\n    base_model_init = BaseModelInitializer(args, [], platform_util)\n    command = base_model_init.get_multi_instance_train_prefix(option_list=[""--genv:test"", ""--genv:test2""])\n    assert command == ""mpirun --genv test --genv test2 ""\n'"
tests/unit/common/test_platform_util.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport pytest\nfrom mock import MagicMock\n\nfrom benchmarks.common.platform_util import PlatformUtil\nfrom test_utils import platform_config\n\n\ndef setup_mock_values(mock_platform, mock_os, mock_subprocess):\n    platform_config.set_mock_system_type(mock_platform)\n    platform_config.set_mock_os_access(mock_os)\n    platform_config.set_mock_lscpu_subprocess_values(mock_subprocess)\n\n\n@pytest.fixture\ndef os_mock(patch):\n    return patch(""os.access"")\n\n\n@pytest.fixture\ndef subprocess_mock(patch):\n    return patch(""subprocess.check_output"")\n\n\n@pytest.fixture\ndef platform_mock(patch):\n    return patch(""system_platform.system"")\n\n\ndef test_platform_util_lscpu_parsing(platform_mock, subprocess_mock, os_mock):\n    """"""\n    Verifies that platform_utils gives us the proper values that we expect\n    based on the lscpu_output string provided.\n    """"""\n    platform_mock.return_value = platform_config.SYSTEM_TYPE\n    os_mock.return_value = True\n    subprocess_mock.side_effect = [platform_config.LSCPU_PATH,\n                                   platform_config.LSCPU_OUTPUT]\n    platform_util = PlatformUtil(MagicMock(verbose=True))\n    assert platform_util.num_cpu_sockets == 2\n    assert platform_util.num_cores_per_socket == 28\n    assert platform_util.num_threads_per_core == 2\n    assert platform_util.num_logical_cpus == 112\n    assert platform_util.num_numa_nodes == 2\n\n\ndef test_platform_util_unsupported_os(platform_mock, subprocess_mock, os_mock):\n    """"""\n    Verifies that platform_utils gives us the proper values that we expect\n    based on the lscpu_output string provided.\n    """"""\n    os_mock.return_value = True\n    subprocess_mock.side_effect = [platform_config.LSCPU_PATH,\n                                   platform_config.LSCPU_OUTPUT]\n    # Mac is not supported yet\n    platform_mock.return_value = ""Mac""\n    with pytest.raises(NotImplementedError) as e:\n        PlatformUtil(MagicMock(verbose=True))\n    assert ""Mac Support not yet implemented"" in str(e)\n    # Windows is not supported yet\n    platform_mock.return_value = ""Windows""\n    with pytest.raises(NotImplementedError) as e:\n        PlatformUtil(MagicMock(verbose=False))\n    assert ""Windows Support not yet implemented"" in str(e)\n'"
benchmarks/image_recognition/tensorflow/densenet169/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv3/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv4/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/mobilenet_v1/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet101/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/language_translation/tensorflow/mlperf_gnmt/__init__.py,0,b''
benchmarks/language_translation/tensorflow/transformer_lt_official/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/rfcn/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-mobilenet/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-resnet34/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/reinforcement/tensorflow/minigo/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
models/image_recognition/tensorflow/resnet101/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/image_recognition/tensorflow/resnet50/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/language_translation/tensorflow/mlperf_gnmt/__init__.py,0,b''
models/object_detection/tensorflow/rfcn/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/object_detection/tensorflow/ssd-mobilenet/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/object_detection/tensorflow/ssd-resnet34/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/recommendation/tensorflow/wide_deep/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/recommendation/tensorflow/wide_deep_large_ds/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/reinforcement/tensorflow/minigo/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
tests/unit/common/tensorflow/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
tests/unit/common/tensorflow/test_run_tf_benchmarks.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport fnmatch\nimport os\nimport pytest\nimport re\nimport sys\n\nfrom mock import MagicMock, patch\n\nfrom benchmarks.common.tensorflow.run_tf_benchmark import ModelBenchmarkUtil\nfrom test_utils import platform_config\nfrom test_utils.io import parse_json_files\n\n\ndef parse_model_args_file():\n    """"""\n    Gets test args from the models files in the specified directory to use as parameters\n    for testing model benchmarking scripts.  The file has a\n    run_tf_benchmarks.py command with args with the corresponding run command\n    that should get called from model_init.py\n    """"""\n    current_dir = os.path.dirname(os.path.realpath(__file__))\n    models_args_path = os.path.join(current_dir, ""tf_model_args"")\n    return parse_json_files(models_args_path)\n\n\ndef delete_env_var(env_var):\n    if env_var in os.environ:\n        del os.environ[env_var]\n\n\ndef clear_kmp_env_vars():\n    """"""\n    Clear env vars to ensure that previously set values are not affecting the next test\n    """"""\n    delete_env_var(""KMP_SETTINGS"")\n    delete_env_var(""KMP_BLOCKTIME"")\n    delete_env_var(""KMP_AFFINITY"")\n    delete_env_var(""KMP_HW_SUBSET"")\n    delete_env_var(""OMP_NUM_THREADS"")\n\n\n# Get test args to use as parameters for test_run_benchmark\ntest_arg_values = parse_model_args_file()\n\n\n@pytest.mark.parametrize(""test_args,expected_cmd,comment"", test_arg_values)\n@patch(""os.mkdir"")\n@patch(""shutil.rmtree"")\n@patch(""os.listdir"")\n@patch(""os.path.isdir"")\n@patch(""os.path.isfile"")\n@patch(""os.path.exists"")\n@patch(""os.stat"")\n@patch(""os.chdir"")\n@patch(""os.remove"")\n@patch(""glob.glob"")\n@patch(""common.platform_util.os"")\n@patch(""common.platform_util.system_platform"")\n@patch(""common.platform_util.subprocess"")\n@patch(""common.base_model_init.BaseModelInitializer.run_command"")\ndef test_run_benchmark(mock_run_command, mock_subprocess, mock_platform, mock_os,\n                       mock_glob, mock_remove, mock_chdir, mock_stat, mock_path_exists,\n                       mock_is_file, mock_is_dir, mock_listdir, mock_rmtree, mock_mkdir,\n                       test_args, expected_cmd, comment):\n    """"""\n    Runs through executing the specified run_tf_benchmarks.py command from the\n    test_args and verifying that the model_init file calls run_command with\n    the expected_cmd string.\n    """"""\n    print(""****** Running The {} test ******"".format(comment))\n    os.environ[""PYTHON_EXE""] = ""python""\n    mock_path_exists.return_value = True\n    mock_is_dir.return_value = True\n    mock_is_file.return_value = True\n    mock_stat.return_value = MagicMock(st_nlink=0)\n    parse_model_args_file()\n    mock_listdir.return_value = True\n    mock_glob.return_value = [""/usr/lib/libtcmalloc.so.4.2.6""]\n    clear_kmp_env_vars()\n    platform_config.set_mock_system_type(mock_platform)\n    platform_config.set_mock_os_access(mock_os)\n    platform_config.set_mock_lscpu_subprocess_values(mock_subprocess)\n    test_args = re.sub("" +"", "" "", test_args)        # get rid of extra spaces in the test_args string\n    expected_cmd = re.sub("" +"", "" "", expected_cmd)  # get rid of extra spaces in the expected_cmd string\n    test_arg_list = test_args.split("" "")\n    with patch.object(sys, ""argv"", test_arg_list):\n        model_benchmark = ModelBenchmarkUtil()\n        model_benchmark.main()\n    assert len(mock_run_command.call_args_list) == 1\n    call_args = mock_run_command.call_args_list[0][0][0]\n    # python3 argparse parses things in different order than python2\n    # we\'ll check that the args are all there though\n    for actual_arg, expected_arg in zip(sorted(call_args.split()), sorted(expected_cmd.split())):\n        # use fnmatch in case we have file names with wildcards (like timestamps in output files)\n        assert fnmatch.fnmatch(actual_arg, expected_arg), \\\n            ""Expected: {}\\nActual: {}"".format(expected_cmd, call_args)\n'"
tests/unit/common/utils/test_validators.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\nimport os\nimport tempfile\nfrom argparse import ArgumentTypeError\n\nimport pytest\nfrom mock import MagicMock\n\nfrom common.utils.validators import (check_for_link, check_no_spaces, check_positive_number,\n                                     check_positive_number_or_equal_to_negative_one, check_valid_filename,\n                                     check_valid_folder, check_valid_file_or_dir, check_volume_mount,\n                                     check_shm_size)\n\n\n@pytest.fixture()\ndef mock_link(patch):\n    return patch(""common.utils.validators.check_for_link"")\n\n\n@pytest.fixture()\ndef mock_exists(patch):\n    return patch(""os.path.exists"")\n\n\n@pytest.fixture()\ndef mock_isfile(patch):\n    return patch(""os.path.isfile"", MagicMock(return_value=True))\n\n\ndef test_check_for_link_file():\n    """"""\n    Tests check_for_link to ensure that sym links and hard links\n    are not allowed. Creates a symlink and hard link of a temporary file and\n    verifies that things fail with the appropriate error message\n    """"""\n\n    with tempfile.NamedTemporaryFile() as temp_file:\n        # directory where the temp file is located\n        parent_dir = os.path.dirname(temp_file.name)\n\n        # create sym link to the temp file\n        symlink_file = os.path.join(parent_dir, ""temp_symlink_file"")\n        if os.path.exists(symlink_file):\n            os.remove(symlink_file)\n        os.symlink(temp_file.name, symlink_file)\n\n        # create hard link to the temp file\n        hardlink_file = os.path.join(parent_dir, ""temp_hardlink_file"")\n        if os.path.exists(hardlink_file):\n            os.remove(hardlink_file)\n        os.link(temp_file.name, hardlink_file)\n\n        try:\n            # Test that hard link errors\n            with pytest.raises(ArgumentTypeError) as e:\n                check_for_link(hardlink_file)\n            assert ""cannot be a link"" in str(e)\n\n            # Test that sym link errors\n            with pytest.raises(ArgumentTypeError) as e:\n                check_for_link(symlink_file)\n            assert ""cannot be a link"" in str(e)\n        finally:\n            if os.path.exists(symlink_file):\n                os.remove(symlink_file)\n            if os.path.exists(hardlink_file):\n                os.remove(hardlink_file)\n\n\ndef test_check_for_link_folder():\n    """"""\n    Tests creating a directory path and ensuring that symlinks are not\n    allowed. Creates a symlink of a temporary directory and verifies that things fail with proper error msg\n    """"""\n    # create temp directory\n    temp_dir = tempfile.mkdtemp()\n    parent_dir = os.path.dirname(temp_dir)\n\n    # create sym link to the temp directory\n    symlink_dir = os.path.join(parent_dir, ""temp_symlink_dir"")\n    if os.path.exists(symlink_dir):\n        os.remove(symlink_dir)\n    os.symlink(temp_dir, symlink_dir)\n\n    try:\n        with pytest.raises(ArgumentTypeError) as e:\n            check_for_link(symlink_dir)\n        assert ""cannot be a link"" in str(e)\n    finally:\n        if os.path.exists(symlink_dir):\n            os.remove(symlink_dir)\n        os.rmdir(temp_dir)\n\n\ndef test_check_no_spaces():\n    with pytest.raises(ArgumentTypeError):\n        check_no_spaces(\'foo bar\')\n\n\ndef test_check_positive_number():\n    with pytest.raises(ArgumentTypeError):\n        check_positive_number(-1)\n\n\ndef test_check_positive_number_or_equal_to_negative_one():\n    with pytest.raises(ArgumentTypeError):\n        check_positive_number_or_equal_to_negative_one(-2)\n\n\ndef test_check_valid_filename(mock_link, mock_isfile):\n    with tempfile.NamedTemporaryFile() as temp_file:\n        assert check_valid_filename(temp_file) == temp_file\n\n\ndef test_check_valid_filename_bad():\n    with pytest.raises(ArgumentTypeError):\n        check_valid_filename(\'3245jlnsdfnsfd234ofds\')\n\n\ndef test_check_valid_folder(mock_link):\n    temp_folder = tempfile.mkdtemp()\n    assert check_valid_folder(temp_folder) == temp_folder\n\n\ndef test_check_valid_folder_bad():\n    with pytest.raises(ArgumentTypeError):\n        check_valid_folder(\'3245jlnsdfnsfd234ofds\')\n\n\ndef test_check_valid_file_or_dir(mock_link, mock_exists):\n    with tempfile.NamedTemporaryFile() as temp_file:\n        assert check_valid_file_or_dir(temp_file) == temp_file\n\n\ndef test_check_valid_file_or_dir_bad():\n    with pytest.raises(ArgumentTypeError):\n        check_valid_file_or_dir(\'3245jlnsdfnsfd234ofds\')\n\n\ndef test_check_invalid_shm_size():\n    with pytest.raises(ArgumentTypeError):\n        check_shm_size(\'-g123ff\')\n\n\ndef test_check_valid_shm_size():\n    assert check_shm_size(\'500g\') == \'500g\'\n    assert check_shm_size(\'64m\') == \'64m\'\n    assert check_shm_size(\'1024k\') == \'1024k\'\n\n\n@pytest.mark.parametrize(""volume_mount_str"",\n                         [""foo"",\n                          ""foo:foo:foo:foo"",\n                          ""foo,foo""])\ndef test_bad_volume_mount_strings(volume_mount_str):\n    with pytest.raises(ArgumentTypeError):\n        check_volume_mount(volume_mount_str)\n\n\ndef test_valid_volume_mount():\n    # create temp directory\n    temp_dir = tempfile.mkdtemp()\n\n    try:\n        # test string that mounts local directory with mount path\n        volume_mount = temp_dir + "":/mount_path""\n        check_volume_mount(volume_mount)\n\n        # test string that mounts local directory with mount path and specifies read only\n        volume_mount = temp_dir + "":/mount_path:ro""\n        check_volume_mount(volume_mount)\n    finally:\n        os.rmdir(temp_dir)\n'"
benchmarks/image_recognition/tensorflow/densenet169/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv3/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv4/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv4/inference/inceptionv4_model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nfrom common.base_model_init import BaseModelInitializer, set_env_var\n\n\nclass InceptionV4ModelInitializer(BaseModelInitializer):\n    """"""Common model initializer for InceptionV4 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(InceptionV4ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # Environment variables\n        set_env_var(""OMP_NUM_THREADS"", platform_util.num_cores_per_socket\n                    if self.args.num_cores == -1 else self.args.num_cores)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        self.set_num_inter_intra_threads(num_inter_threads=platform_util.num_threads_per_core,\n                                         num_intra_threads=platform_util.num_cores_per_socket)\n\n    def parse_args(self):\n        if self.custom_args:\n            parser = argparse.ArgumentParser()\n            parser.add_argument(\n                ""--input-height"", default=None, dest=""input_height"",\n                type=int, help=""input height"")\n            parser.add_argument(\n                ""--input-width"", default=None,\n                dest=""input_width"", type=int, help=""input width"")\n            parser.add_argument(\n                ""--warmup-steps"", dest=""warmup_steps"",\n                help=""number of warmup steps"", type=int, default=10)\n            parser.add_argument(\n                ""--steps"", dest=""steps"", help=""number of steps"", type=int,\n                default=50)\n            parser.add_argument(\n                ""--input-layer"", dest=""input_layer"",\n                help=""name of input layer"", type=str, default=None)\n            parser.add_argument(\n                ""--output-layer"", dest=""output_layer"",\n                help=""name of output layer"", type=str, default=None)\n\n            self.args = parser.parse_args(self.custom_args,\n                                          namespace=self.args)\n\n    def add_command_prefix(self, script_path):\n        """""" Uses the specified script path and adds on the command prefix """"""\n        return self.get_command_prefix(self.args.socket_id) + self.python_exe + "" "" + \\\n            script_path\n\n    def run_benchmark(self):\n        """""" Setup the command string and run the benchmarking script """"""\n        benchmark_script = os.path.join(\n            self.args.intelai_models, self.args.mode, ""benchmark.py"")\n        script_args_list = [\n            ""input_graph"", ""input_height"", ""input_width"", ""batch_size"",\n            ""input_layer"", ""output_layer"", ""num_inter_threads"",\n            ""num_intra_threads"",\n            ""warmup_steps"", ""steps""]\n        cmd_prefix = self.add_command_prefix(benchmark_script)\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n\n        self.run_command(cmd)\n\n    def run_accuracy(self):\n        """""" Setup the command string and run the accuracy test """"""\n        accuracy_script = os.path.join(\n            self.args.intelai_models, self.args.mode, ""accuracy.py"")\n        script_args_list = [\n            ""input_graph"", ""data_location"", ""input_height"", ""input_width"",\n            ""batch_size"", ""input_layer"", ""output_layer"",\n            ""num_inter_threads"", ""num_intra_threads""]\n        cmd_prefix = self.add_command_prefix(accuracy_script)\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n\n        self.run_command(cmd)\n\n    def run(self):\n        self.parse_args()\n        if self.args.benchmark_only:\n            self.run_benchmark()\n        if self.args.accuracy_only:\n            self.run_accuracy()\n'"
benchmarks/image_recognition/tensorflow/mobilenet_v1/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet101/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/training/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
benchmarks/language_translation/tensorflow/mlperf_gnmt/inference/__init__.py,0,b''
benchmarks/language_translation/tensorflow/transformer_lt_official/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/rfcn/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-mobilenet/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-resnet34/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/reinforcement/tensorflow/minigo/training/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#'"
models/image_recognition/tensorflow/inceptionv3/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/image_recognition/tensorflow/inceptionv3/fp32/datasets.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nimport os\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nimport preprocessing\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, height=None, width=None, depth=None, data_dir=None,\n               queue_runner_required=False, num_classes=1000):\n    self.name = name\n    self.height = height\n    self.width = width\n    self.depth = depth or 3\n\n    self.data_dir = data_dir\n    self._queue_runner_required = queue_runner_required\n    self._num_classes = num_classes\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @num_classes.setter\n  def num_classes(self, val):\n    self._num_classes = val\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n  def get_image_preprocessor(self):\n    return None\n\n  def queue_runner_required(self):\n    return self._queue_runner_required\n\n  def use_synthetic_gpu_images(self):\n    return not self.data_dir\n\n\nclass ImagenetData(Dataset):\n  """"""Configuration for Imagenet dataset.""""""\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'imagenet\', 300, 300, data_dir=data_dir)\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    return preprocessing.RecordInputImagePreprocessor\n\n'"
models/image_recognition/tensorflow/inceptionv3/fp32/eval_image_classifier_inference.py,18,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport time\nfrom argparse import ArgumentParser\n\nimport tensorflow as tf\nfrom tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\nfrom tensorflow.python.framework import dtypes\n\nimport datasets\n\nINPUTS = \'input\'\nOUTPUTS = \'predict\'\n\nINCEPTION_V3_IMAGE_SIZE = 299\n\n\nclass eval_classifier_optimized_graph:\n  """"""Evaluate image classifier with optimized TensorFlow graph""""""\n\n  def __init__(self):\n\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--num-inter-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--num-intra-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph for the transform tool\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n\n    arg_parser.add_argument(""--warmup-steps"", type=int, default=10,\n                            help=""number of warmup steps"")\n    arg_parser.add_argument(""--steps"", type=int, default=50,\n                            help=""number of steps"")\n\n    arg_parser.add_argument(\n      \'--data-num-inter-threads\', dest=\'data_num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=16)\n    arg_parser.add_argument(\n      \'--data-num-intra-threads\', dest=\'data_num_intra_threads\',\n      help=\'number threads for data layer operator\',\n      type=int, default=14)\n    arg_parser.add_argument(\n      \'--num-cores\', dest=\'num_cores\',\n      help=\'number of cores\',\n      type=int, default=28)\n\n    self.args = arg_parser.parse_args()\n\n    # validate the arguments specific for InceptionV3\n    self.validate_args()\n\n  def run(self):\n    """"""run benchmark with optimized graph""""""\n\n    print(""Run inference"")\n\n    data_config = tf.compat.v1.ConfigProto()\n    data_config.intra_op_parallelism_threads = self.args.data_num_intra_threads\n    data_config.inter_op_parallelism_threads = self.args.data_num_inter_threads\n    data_config.use_per_session_threads = 1\n\n    infer_config = tf.compat.v1.ConfigProto()\n    infer_config.intra_op_parallelism_threads = self.args.num_intra_threads\n    infer_config.inter_op_parallelism_threads = self.args.num_inter_threads\n    infer_config.use_per_session_threads = 1\n\n    data_graph = tf.Graph()\n    with data_graph.as_default():\n      if (self.args.data_location):\n        print(""Inference with real data."")\n        dataset = datasets.ImagenetData(self.args.data_location)\n        preprocessor = dataset.get_image_preprocessor()(\n          INCEPTION_V3_IMAGE_SIZE, INCEPTION_V3_IMAGE_SIZE, self.args.batch_size,\n          num_cores=self.args.num_cores,\n          resize_method=\'bilinear\')\n        images, labels = preprocessor.minibatch(dataset, subset=\'validation\')\n      else:\n        print(""Inference with dummy data."")\n        input_shape = [self.args.batch_size, INCEPTION_V3_IMAGE_SIZE, INCEPTION_V3_IMAGE_SIZE, 3]\n        images = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name=\'synthetic_images\')\n\n    infer_graph = tf.Graph()\n    with infer_graph.as_default():\n      graph_def = tf.compat.v1.GraphDef()\n      with tf.compat.v1.gfile.FastGFile(self.args.input_graph, \'rb\') as input_file:\n        input_graph_content = input_file.read()\n        graph_def.ParseFromString(input_graph_content)\n\n      output_graph = optimize_for_inference(graph_def, [INPUTS], \n                              [OUTPUTS], dtypes.float32.as_datatype_enum, False)\n      tf.import_graph_def(output_graph, name=\'\')\n\n    # Definite input and output Tensors for detection_graph\n    input_tensor = infer_graph.get_tensor_by_name(\'input:0\')\n    output_tensor = infer_graph.get_tensor_by_name(\'predict:0\')\n\n    data_sess  = tf.compat.v1.Session(graph=data_graph,  config=data_config)\n    infer_sess = tf.compat.v1.Session(graph=infer_graph, config=infer_config)\n\n    num_processed_images = 0\n    num_remaining_images = datasets.IMAGENET_NUM_VAL_IMAGES\n\n    if (not self.args.accuracy_only):\n      iteration = 0\n      warm_up_iteration = self.args.warmup_steps\n      total_run = self.args.steps\n      total_time = 0\n\n      while num_remaining_images >= self.args.batch_size and iteration < total_run:\n        iteration += 1\n\n        data_load_start = time.time()\n        image_np = data_sess.run(images)\n        data_load_time = time.time() - data_load_start\n\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        infer_sess.run([output_tensor], feed_dict={input_tensor: image_np})\n        time_consume = time.time() - start_time\n\n        # only add data loading time for real data, not for dummy data\n        if self.args.data_location:\n          time_consume += data_load_time\n\n        print(\'Iteration %d: %.6f sec\' % (iteration, time_consume))\n        if iteration > warm_up_iteration:\n          total_time += time_consume\n\n      time_average = total_time / (iteration - warm_up_iteration)\n      print(\'Average time: %.6f sec\' % (time_average))\n\n      print(\'Batch size = %d\' % self.args.batch_size)\n      if (self.args.batch_size == 1):\n        print(\'Latency: %.3f ms\' % (time_average * 1000))\n\n      print(\'Throughput: %.3f images/sec\' % (self.args.batch_size / time_average))\n\n    else:  # accuracy check\n      total_accuracy1, total_accuracy5 = (0.0, 0.0)\n\n      while num_remaining_images >= self.args.batch_size:\n        # Reads and preprocess data\n        np_images, np_labels = data_sess.run([images, labels])\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        # Compute inference on the preprocessed data\n        predictions = infer_sess.run(output_tensor,\n                                     {input_tensor: np_images})\n        elapsed_time = time.time() - start_time\n\n        with tf.Graph().as_default() as accu_graph:\n          accuracy1 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                   targets=tf.constant(np_labels), k=1), tf.float32))\n\n          accuracy5 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                   targets=tf.constant(np_labels), k=5), tf.float32))\n          with tf.compat.v1.Session() as accu_sess:\n            np_accuracy1, np_accuracy5 = accu_sess.run([accuracy1, accuracy5])\n\n          total_accuracy1 += np_accuracy1\n          total_accuracy5 += np_accuracy5\n\n        print(""Iteration time: %0.4f ms"" % elapsed_time)\n        print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n              % (num_processed_images, total_accuracy1 / num_processed_images,\n                 total_accuracy5 / num_processed_images))\n\n  def validate_args(self):\n    """"""validate the arguments""""""\n\n    if not self.args.data_location:\n      if self.args.accuracy_only:\n        raise ValueError(""You must use real data for accuracy measurement."")\n\n\nif __name__ == ""__main__"":\n  evaluate_opt_graph = eval_classifier_optimized_graph()\n  evaluate_opt_graph.run()\n'"
models/image_recognition/tensorflow/inceptionv3/fp32/preprocessing.py,32,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.platform import gfile\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                        default_value=\'\'),\n    \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                            default_value=-1)\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  return features[\'image/encoded\'], label\n\n\ndef eval_image(image, height, width, resize_method,\n               central_fraction=0.875, scope=None):\n\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if resize_method == \'crop\':\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                      true_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256, 256 * shape[1] / shape[0]],\n                                                                          dtype=tf.int32)),\n                      false_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256 * shape[0] / shape[1], 256],\n                                                                          dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height,                                                     width)\n      distorted_image.set_shape([height, width, 3])\n      return distorted_image\n    else:  # bilinear\n      if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n      # Crop the central region of the image with an area containing 87.5% of\n      # the original image.\n      if central_fraction:\n        image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n      if height and width:\n        # Resize the image to the specified height and width.\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize(image, [height, width],\n                                         method=tf.image.ResizeMethod.BILINEAR)\n        image = tf.squeeze(image, [0])\n      image = tf.subtract(image, 0.5)\n      image = tf.multiply(image, 2.0)\n      return image\n\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               num_cores,\n               resize_method=""bilinear""):\n\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.num_cores = num_cores\n    self.resize_method = resize_method\n\n  def parse_and_preprocess(self, value):\n    # parse\n    image_buffer, label_index = parse_example_proto(value)\n    # preprocess\n    image = tf.image.decode_jpeg(\n      image_buffer, channels=3, fancy_upscaling=False, dct_method=\'INTEGER_FAST\')\n    image = eval_image(image, self.height, self.width, self.resize_method)\n\n    return (image, label_index)\n\n  def minibatch(self, dataset, subset, cache_data=False):\n\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n\n      glob_pattern = dataset.tf_record_pattern(subset)\n      file_names = gfile.Glob(glob_pattern)\n      if not file_names:\n        raise ValueError(\'Found no files in --data_dir matching: {}\'\n                         .format(glob_pattern))\n      ds = tf.data.TFRecordDataset.list_files(file_names)\n\n      ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=self.num_cores, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n\n      if cache_data:\n        ds = ds.take(1).cache().repeat()\n\n      ds = ds.prefetch(buffer_size=10000)\n\n      # num of parallel batches not greater than 56\n      max_num_parallel_batches = min(56, 2 * self.num_cores)\n      ds = ds.apply(\n        map_and_batch(\n          map_func=self.parse_and_preprocess,\n          batch_size=self.batch_size,\n          num_parallel_batches=max_num_parallel_batches,\n          num_parallel_calls=None))\n\n      ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # this number can be tuned\n\n      ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n      images, labels = ds_iterator.get_next()\n      # reshape\n      labels = tf.reshape(labels, [self.batch_size])\n\n      return images, labels\n'"
models/image_recognition/tensorflow/inceptionv3/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/image_recognition/tensorflow/inceptionv3/int8/accuracy.py,14,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=299,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=299,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""predict"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 299\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 299\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n\n  data_graph = tf.Graph()\n  with data_graph.as_default():\n    dataset = datasets.ImagenetData(data_location)\n    preprocessor = dataset.get_image_preprocessor()(\n        input_height, input_width, batch_size,\n        1, # device count\n        tf.float32, # data_type for input fed to the graph\n        train=False, # doing inference\n        resize_method=\'bilinear\')\n\n    images, labels = preprocessor.minibatch(dataset, subset=\'validation\',\n                      use_datasets=True, cache_data=False)\n\n  infer_graph = load_graph(model_file)\n\n  input_tensor = infer_graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = infer_graph.get_tensor_by_name(output_layer + "":0"")\n  \n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = dataset.num_examples_per_epoch(subset=\'validation\') \\\n                            - num_processed_images\n  with tf.compat.v1.Session(graph=data_graph) as sess:\n    sess_graph = tf.compat.v1.Session(graph=infer_graph, config=config)\n    while num_remaining_images >= batch_size:\n      # Reads and preprocess data\n      np_images, np_labels = sess.run([images[0], labels[0]])\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      start_time = time.time()\n      # Compute inference on the preprocessed data\n      predictions = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      elapsed_time = time.time() - start_time\n      accuracy1 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=1), tf.float32))\n\n      accuracy5 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=5), tf.float32))\n      np_accuracy1, np_accuracy5 =  sess.run([accuracy1, accuracy5])\n      total_accuracy1 += np_accuracy1\n      total_accuracy5 += np_accuracy5\n      print(""Iteration time: %0.4f ms"" % elapsed_time)\n      print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n          % (num_processed_images, total_accuracy1/num_processed_images,\n          total_accuracy5/num_processed_images))\n'"
models/image_recognition/tensorflow/inceptionv3/int8/benchmark.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport time\n\nimport datasets\nimport tensorflow as tf\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--input_height"", default=299,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=299,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""dataset location"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""predict"",\n                      help=""name of output layer"")\n  parser.add_argument(""--num_cores"", default=28,\n                      type=int, help=""number of physical cores"")\n  parser.add_argument(\n    \'--num_inter_threads\',\n    help=\'number threads across operators\',\n    type=int, default=1)\n  parser.add_argument(\n    \'--num_intra_threads\',\n    help=\'number threads for an operator\',\n    type=int, default=1)\n  parser.add_argument(\n    \'--data_num_inter_threads\',\n    help=\'number threads across data layer operators\',\n    type=int, default=16)\n  parser.add_argument(\n    \'--data_num_intra_threads\',\n    help=\'number threads for an data layer operator\',\n    type=int, default=14)\n  parser.add_argument(""--warmup_steps"", type=int, default=10,\n                      help=""number of warmup steps"")\n  parser.add_argument(""--steps"", type=int, default=50, help=""number of steps"")\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 299\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 299\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  warmup_steps = args.warmup_steps\n  steps = args.steps\n  assert steps > 10, ""Benchmark steps should be at least 10.""\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n\n  data_config = tf.compat.v1.ConfigProto()\n  data_config.intra_op_parallelism_threads = args.data_num_intra_threads\n  data_config.inter_op_parallelism_threads = args.data_num_inter_threads\n  data_config.use_per_session_threads = 1\n\n  infer_config = tf.compat.v1.ConfigProto()\n  infer_config.intra_op_parallelism_threads = num_intra_threads\n  infer_config.inter_op_parallelism_threads = num_inter_threads\n  infer_config.use_per_session_threads = 1\n\n  data_graph = tf.Graph()\n  with data_graph.as_default():\n    if args.data_location:\n      print(""inference with real data"")\n      # get the images from dataset\n      dataset = datasets.ImagenetData(args.data_location)\n      preprocessor = dataset.get_image_preprocessor(benchmark=True)(\n        input_height, input_width, batch_size,\n        num_cores=args.num_cores,\n        resize_method=\'bilinear\')\n      images = preprocessor.minibatch(dataset, subset=\'validation\')\n    else:\n      # synthetic images\n      print(""inference with dummy data"")\n      input_shape = [batch_size, input_height, input_width, 3]\n      images = tf.random.uniform(\n        input_shape, 0.0, 255.0, dtype=tf.float32, name=\'synthetic_images\')\n\n  infer_graph = tf.Graph()\n  with infer_graph.as_default():\n    graph_def = tf.compat.v1.GraphDef()\n    with open(model_file, ""rb"") as f:\n      graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name=\'\')\n\n  input_tensor = infer_graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = infer_graph.get_tensor_by_name(output_layer + "":0"")\n  tf.compat.v1.global_variables_initializer()\n\n  data_sess = tf.compat.v1.Session(graph=data_graph, config=data_config)\n  infer_sess = tf.compat.v1.Session(graph=infer_graph, config=infer_config)\n\n  print(""[Running warmup steps...]"")\n  for t in range(warmup_steps):\n    data_start_time = time.time()\n    image_data = data_sess.run(images)\n    data_load_time = time.time() - data_start_time\n\n    start_time = time.time()\n    infer_sess.run(output_tensor, {input_tensor: image_data})\n    elapsed_time = time.time() - start_time\n\n    # only count the data loading and processing time for real data\n    if args.data_location:\n      elapsed_time += data_load_time\n\n    if ((t + 1) % 10 == 0):\n      print(""steps = {0}, {1} images/sec""\n            """".format(t + 1, batch_size / elapsed_time))\n\n  print(""[Running benchmark steps...]"")\n  total_time = 0\n  total_images = 0\n\n  for t in range(steps):\n    try:\n      data_start_time = time.time()\n      image_data = data_sess.run(images)\n      data_load_time = time.time() - data_start_time\n\n      start_time = time.time()\n      infer_sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n\n      # only count the data loading and processing time for real data\n      if args.data_location:\n        elapsed_time += data_load_time\n\n      total_time += elapsed_time\n      total_images += batch_size\n      if ((t + 1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t + 1, batch_size / elapsed_time))\n    except tf.errors.OutOfRangeError:\n      print(""Running out of images from dataset."")\n      break\n\n  print(""Average throughput for batch size {0}: {1} images/sec"".format(batch_size, total_images / total_time))\n\n'"
models/image_recognition/tensorflow/inceptionv3/int8/calibration.py,13,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=299,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=299,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""predict"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 224\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 224\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n  dataset = datasets.ImagenetData(data_location)\n  preprocessor = dataset.get_image_preprocessor()(\n      input_height, input_width, batch_size,\n      1, # device count\n      tf.float32, # data_type for input fed to the graph\n      train=False, # doing inference\n      resize_method=\'bilinear\')\n\n  images, labels = preprocessor.minibatch(dataset, subset=\'train\',\n                    use_datasets=True, cache_data=False)\n  graph = load_graph(model_file)\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n  \n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = dataset.num_examples_per_epoch(subset=\'train\') \\\n                            - num_processed_images\n  with tf.compat.v1.Session() as sess:\n    sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n    while (num_remaining_images >= batch_size) and (num_processed_images < 10000):\n      # Reads and preprocess data\n      np_images, np_labels = sess.run([images[0], labels[0]])\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      # Compute inference on the preprocessed data\n      predictions = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      accuracy1 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=1), tf.float32))\n\n      accuracy5 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=5), tf.float32))\n      np_accuracy1, np_accuracy5 =  sess.run([accuracy1, accuracy5])\n      total_accuracy1 += np_accuracy1\n      total_accuracy5 += np_accuracy5\n      print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n          % (num_processed_images, total_accuracy1/num_processed_images,\n          total_accuracy5/num_processed_images))\n'"
models/image_recognition/tensorflow/inceptionv3/int8/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n'"
models/image_recognition/tensorflow/inceptionv3/int8/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, height=None, width=None, depth=None, data_dir=None,\n               queue_runner_required=False, num_classes=1000):\n    self.name = name\n    self.height = height\n    self.width = width\n    self.depth = depth or 3\n\n    self.data_dir = data_dir\n    self._queue_runner_required = queue_runner_required\n    self._num_classes = num_classes\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @num_classes.setter\n  def num_classes(self, val):\n    self._num_classes = val\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n  def get_image_preprocessor(self):\n    return None\n\n  def queue_runner_required(self):\n    return self._queue_runner_required\n\n  def use_synthetic_gpu_images(self):\n    return not self.data_dir\n\n\nclass ImagenetData(Dataset):\n  """"""Configuration for Imagenet dataset.""""""\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'imagenet\', 300, 300, data_dir=data_dir)\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    elif subset == \'calibrate\' or subset == \'calibration\':\n      return 100\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self, benchmark=False):\n    if benchmark:\n      import preprocessing_benchmark\n      return preprocessing_benchmark.RecordInputImagePreprocessor\n    else:\n      import preprocessing\n      return preprocessing.RecordInputImagePreprocessor\n'"
models/image_recognition/tensorflow/inceptionv3/int8/preprocessing.py,119,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport math\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.python.layers import utils\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.platform import gfile\nimport cnn_util\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef parse_example_proto(example_serialized):\n    """"""Parses an Example proto containing a training example of an image.\n  \n    The output of the build_image_data.py image preprocessing script is a dataset\n    containing serialized Example protocol buffers. Each Example proto contains\n    the following fields:\n  \n      image/height: 462\n      image/width: 581\n      image/colorspace: \'RGB\'\n      image/channels: 3\n      image/class/label: 615\n      image/class/synset: \'n03623198\'\n      image/class/text: \'knee pad\'\n      image/object/bbox/xmin: 0.1\n      image/object/bbox/xmax: 0.9\n      image/object/bbox/ymin: 0.2\n      image/object/bbox/ymax: 0.6\n      image/object/bbox/label: 615\n      image/format: \'JPEG\'\n      image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n      image/encoded: <JPEG encoded string>\n  \n    Args:\n      example_serialized: scalar Tensor tf.string containing a serialized\n        Example protocol buffer.\n  \n    Returns:\n      image_buffer: Tensor tf.string containing the contents of a JPEG file.\n      label: Tensor tf.int32 containing the label.\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged as\n        [ymin, xmin, ymax, xmax].\n      text: Tensor tf.string containing the human-readable label.\n    """"""\n    # Dense features in Example proto.\n    feature_map = {\n        \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                            default_value=\'\'),\n        \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                                default_value=-1),\n        \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                               default_value=\'\'),\n    }\n    sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n    # Sparse features in Example proto.\n    feature_map.update(\n        {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                     \'image/object/bbox/ymin\',\n                                     \'image/object/bbox/xmax\',\n                                     \'image/object/bbox/ymax\']})\n\n    features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n    label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n    xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n    ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n    xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n    ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n    # Note that we impose an ordering of (y, x) just to make life difficult.\n    bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n    # Force the variable number of bounding boxes into the shape\n    # [1, num_boxes, coords].\n    bbox = tf.expand_dims(bbox, 0)\n    bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n    return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef get_image_resize_method(resize_method, batch_position=0):\n    """"""Get tensorflow resize method.\n  \n    If resize_method is \'round_robin\', return different methods based on batch\n    position in a round-robin fashion. NOTE: If the batch size is not a multiple\n    of the number of methods, then the distribution of methods will not be\n    uniform.\n  \n    Args:\n      resize_method: (string) nearest, bilinear, bicubic, area, or round_robin.\n      batch_position: position of the image in a batch. NOTE: this argument can\n        be an integer or a tensor\n    Returns:\n      one of resize type defined in tf.image.ResizeMethod.\n    """"""\n    resize_methods_map = {\n        \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n        \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n        \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n        \'area\': tf.image.ResizeMethod.AREA\n    }\n\n    if resize_method != \'round_robin\':\n        return resize_methods_map[resize_method]\n\n    # return a resize method based on batch position in a round-robin fashion.\n    resize_methods = resize_methods_map.values()\n\n    def lookup(index):\n        return resize_methods[index]\n\n    def resize_method_0():\n        return utils.smart_cond(batch_position % len(resize_methods) == 0,\n                                lambda: lookup(0), resize_method_1)\n\n    def resize_method_1():\n        return utils.smart_cond(batch_position % len(resize_methods) == 1,\n                                lambda: lookup(1), resize_method_2)\n\n    def resize_method_2():\n        return utils.smart_cond(batch_position % len(resize_methods) == 2,\n                                lambda: lookup(2), lambda: lookup(3))\n\n    # NOTE(jsimsa): Unfortunately, we cannot use a single recursive function here\n    # because TF would not be able to construct a finite graph.\n\n    return resize_method_0()\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n    """"""Decode a JPEG string into one 3-D float image Tensor.\n  \n    Args:\n      image_buffer: scalar string Tensor.\n      scope: Optional scope for op_scope.\n    Returns:\n      3-D float Tensor with values ranging from [0, 1).\n    """"""\n    # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n    # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n    with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n        # Decode the string as an RGB JPEG.\n        # Note that the resulting image contains an unknown height and width\n        # that is set dynamically by decode_jpeg. In other words, the height\n        # and width of image is unknown at compile-time.\n        image = tf.image.decode_jpeg(image_buffer, channels=3)  # ,\n        #     fancy_upscaling=False,\n        #     dct_method=\'INTEGER_FAST\')\n\n        # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n        return image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n    """"""Prepare one image for evaluation.\n  \n    If height and width are specified it would output an image with that size by\n    applying resize_bilinear.\n  \n    If central_fraction is specified it would crop the central fraction of the\n    input image.\n  \n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      central_fraction: Optional Float, fraction of the image to crop.\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D float Tensor of prepared image.\n    """"""\n    with tf.compat.v1.name_scope(scope, \'eval_image\', [image, height, width]):\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Crop the central region of the image with an area containing 87.5% of\n        # the original image.\n        if central_fraction:\n            image = tf.image.central_crop(image,\n                                          central_fraction=central_fraction)\n\n        if height and width:\n            # Resize the image to the specified height and width.\n            image = tf.expand_dims(image, 0)\n            image = tf.image.resize(image, [height, width],\n                                             method=tf.image.ResizeMethod.BILINEAR)\n            image = tf.squeeze(image, [0])\n        image = tf.subtract(image, 0.5)\n        image = tf.multiply(image, 2.0)\n        return image\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n  \n    Args:\n      x: input Tensor.\n      func: Python function to apply.\n      num_cases: Python int32, number of cases to sample sel from.\n  \n    Returns:\n      The result of func(x, sel), where func receives the value of the\n      selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random.uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n        func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n        for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n  \n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n  \n    Args:\n      image: 3-D Tensor containing single image in [0, 1].\n      color_ordering: Python int, a type of distortion (valid values: 0-3).\n      fast_mode: Avoids slower ops (random_hue and random_contrast)\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n      ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.compat.v1.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n  \n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n  \n    Args:\n      image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n        image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding box\n        supplied.\n      aspect_ratio_range: An optional list of `floats`. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `floats`. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n      scope: Optional scope for name_scope.\n    Returns:\n      A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.compat.v1.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n        # A large fraction of image datasets contain a human-annotated bounding\n        # box delineating the region of the image containing the object of interest.\n        # We choose to create a new bounding box for the object which is a randomly\n        # distorted version of the human-annotated bounding box that obeys an\n        # allowed range of aspect ratios, sizes and overlap with the human-annotated\n        # bounding box. If no box is supplied, then we assume the bounding box is\n        # the entire image.\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            image_size=tf.shape(input=image),\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         batch_position,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n    """"""Distort one image for training a network.\n  \n    Distorting images provides a useful technique for augmenting the data\n    set during training in order to make the network invariant to aspects\n    of the image that do not effect the label.\n  \n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax].\n      batch_position: position of the image in a batch, which affects how images\n        are distorted and resized. NOTE: this argument can be an integer or a\n        tensor\n      scope: Optional scope for op_scope.\n      add_image_summaries: Enable image summaries.\n    Returns:\n      3-D float Tensor of distorted image used for training with range [-1, 1].\n    """"""\n\n    with tf.compat.v1.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n        if bbox is None:\n            bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                               dtype=tf.float32,\n                               shape=[1, 1, 4])\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                      bbox)\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n        distorted_image, distorted_bbox = distorted_bounding_box_crop(image,\n                                                                      bbox)\n        # Restore the shape since the dynamic slice based upon the bbox_size loses\n        # the third dimension.\n        distorted_image.set_shape([None, None, 3])\n        image_with_distorted_box = tf.image.draw_bounding_boxes(\n            tf.expand_dims(image, 0), distorted_bbox)\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'images_with_distorted_bounding_box\',\n                             image_with_distorted_box)\n\n        # This resizing operation may distort the images because the aspect\n        # ratio is not respected. We select a resize method in a round robin\n        # fashion based on the thread number.\n        # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n        # We select only 1 case for fast_mode bilinear.\n        num_resize_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n            distorted_image,\n            lambda x, method: tf.image.resize(x, [height, width],\n                                                     method),\n            num_cases=num_resize_cases)\n\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'cropped_resized_image\',\n                             tf.expand_dims(distorted_image, 0))\n\n        # Randomly flip the image horizontally.\n        distorted_image = tf.image.random_flip_left_right(distorted_image)\n        # Randomly distort the colors. There are 1 or 4 ways to do it.\n        num_distort_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n            distorted_image,\n            lambda x, ordering: distort_color(x, ordering, fast_mode),\n            num_cases=num_distort_cases)\n\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'final_distorted_image\',\n                             tf.expand_dims(distorted_image, 0))\n        distorted_image = tf.subtract(distorted_image, 0.5)\n        distorted_image = tf.multiply(distorted_image, 2.0)\n        return distorted_image\n\n\ndef distort_color(image, batch_position=0, distort_color_in_yiq=False,\n                  scope=None):\n    """"""Distort the color of the image.\n  \n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops based on the position of the image in a batch.\n  \n    Args:\n      image: float32 Tensor containing single image. Tensor values should be in\n        range [0, 1].\n      batch_position: the position of the image in a batch. NOTE: this argument\n        can be an integer or a tensor\n      distort_color_in_yiq: distort color of input images in YIQ space.\n      scope: Optional scope for op_scope.\n    Returns:\n      color-distorted image\n    """"""\n    with tf.compat.v1.name_scope(scope or \'distort_color\'):\n        def distort_fn_0(image=image):\n            """"""Variant 0 of distort function.""""""\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            # if distort_color_in_yiq:\n            #  image = distort_image_ops.random_hsv_in_yiq(\n            #      image, lower_saturation=0.5, upper_saturation=1.5,\n            #      max_delta_hue=0.2 * math.pi)\n            # else:\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            return image\n\n        def distort_fn_1(image=image):\n            """"""Variant 1 of distort function.""""""\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            # if distort_color_in_yiq:\n            #  image = distort_image_ops.random_hsv_in_yiq(\n            #      image, lower_saturation=0.5, upper_saturation=1.5,\n            #      max_delta_hue=0.2 * math.pi)\n            # else:\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            return image\n\n        image = utils.smart_cond(batch_position % 2 == 0, distort_fn_0,\n                                 distort_fn_1)\n        # The random_* ops do not necessarily clamp.\n        image = tf.clip_by_value(image, 0.0, 1.0)\n        return image\n\n\nclass RecordInputImagePreprocessor(object):\n    """"""Preprocessor for images with RecordInput format.""""""\n\n    def __init__(self,\n                 height,\n                 width,\n                 batch_size,\n                 num_splits,\n                 dtype,\n                 train,\n                 distortions=False,\n                 resize_method=""bilinear"",\n                 shift_ratio=0,\n                 summary_verbosity=1,\n                 distort_color_in_yiq=False,\n                 fuse_decode_and_crop=False):\n        self.height = height\n        self.width = width\n        self.batch_size = batch_size\n        self.num_splits = num_splits\n        self.dtype = dtype\n        self.train = train\n        self.resize_method = resize_method\n        self.shift_ratio = shift_ratio\n        self.distortions = distortions\n        self.distort_color_in_yiq = distort_color_in_yiq\n        self.fuse_decode_and_crop = fuse_decode_and_crop\n        if self.batch_size % self.num_splits != 0:\n            raise ValueError(\n                (\'batch_size must be a multiple of num_splits: \'\n                 \'batch_size %d, num_splits: %d\') %\n                (self.batch_size, self.num_splits))\n        self.batch_size_per_split = self.batch_size // self.num_splits\n        self.summary_verbosity = summary_verbosity\n\n    def image_preprocess(self, image_buffer, bbox, batch_position):\n        """"""Preprocessing image_buffer as a function of its batch position.""""""\n        if self.train:\n            image_buffer = tf.image.decode_jpeg(\n                image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n            image = preprocess_for_train(image_buffer, self.height, self.width,\n                                         bbox,\n                                         batch_position)\n        else:\n            image = tf.image.decode_jpeg(\n                image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n            image = preprocess_for_eval(image, self.height, self.width)\n        return image\n\n    def parse_and_preprocess(self, value, batch_position):\n        image_buffer, label_index, bbox, _ = parse_example_proto(value)\n        image = self.image_preprocess(image_buffer, bbox, batch_position)\n        return (label_index, image)\n\n    def minibatch(self, dataset, subset, use_datasets, cache_data,\n                  shift_ratio=-1):\n        if shift_ratio < 0:\n            shift_ratio = self.shift_ratio\n        with tf.compat.v1.name_scope(\'batch_processing\'):\n            # Build final results per split.\n            images = [[] for _ in range(self.num_splits)]\n            labels = [[] for _ in range(self.num_splits)]\n            if use_datasets:\n                glob_pattern = dataset.tf_record_pattern(subset)\n                file_names = gfile.Glob(glob_pattern)\n                if not file_names:\n                    raise ValueError(\n                        \'Found no files in --data_dir matching: {}\'\n                        .format(glob_pattern))\n                ds = tf.data.TFRecordDataset.list_files(file_names)\n                ds = ds.apply(\n                    tf.data.experimental.parallel_interleave(\n                        tf.data.TFRecordDataset, cycle_length=10))\n                if cache_data:\n                    ds = ds.take(1).cache().repeat()\n                counter = tf.data.Dataset.range(self.batch_size)\n                counter = counter.repeat()\n                ds = tf.data.Dataset.zip((ds, counter))\n                ds = ds.prefetch(buffer_size=self.batch_size)\n                ds = ds.shuffle(buffer_size=10000)\n                ds = ds.repeat()\n                ds = ds.apply(\n                    tf.compat.v1.data.experimental.map_and_batch(\n                        map_func=self.parse_and_preprocess,\n                        batch_size=self.batch_size_per_split,\n                        num_parallel_batches=self.num_splits))\n                ds = ds.prefetch(buffer_size=self.num_splits)\n                ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n                for d in xrange(self.num_splits):\n                    labels[d], images[d] = ds_iterator.get_next()\n\n            else:\n                record_input = data_flow_ops.RecordInput(\n                    file_pattern=dataset.tf_record_pattern(subset),\n                    seed=301,\n                    parallelism=64,\n                    buffer_size=10000,\n                    batch_size=self.batch_size,\n                    shift_ratio=shift_ratio,\n                    name=\'record_input\')\n                records = record_input.get_yield_op()\n                records = tf.split(records, self.batch_size, 0)\n                records = [tf.reshape(record, []) for record in records]\n                for idx in xrange(self.batch_size):\n                    value = records[idx]\n                    (label, image) = self.parse_and_preprocess(value, idx)\n                    split_index = idx % self.num_splits\n                    labels[split_index].append(label)\n                    images[split_index].append(image)\n\n            for split_index in xrange(self.num_splits):\n                if not use_datasets:\n                    images[split_index] = tf.parallel_stack(\n                        images[split_index])\n                    labels[split_index] = tf.concat(labels[split_index], 0)\n                images[split_index] = tf.cast(images[split_index], self.dtype)\n                depth = 3\n                images[split_index] = tf.reshape(\n                    images[split_index],\n                    shape=[self.batch_size_per_split, self.height, self.width,\n                           depth])\n                labels[split_index] = tf.reshape(labels[split_index],\n                                                 [self.batch_size_per_split])\n            return images, labels\n'"
models/image_recognition/tensorflow/inceptionv3/int8/preprocessing_benchmark.py,31,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.platform import gfile\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                        default_value=\'\'),\n    \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                            default_value=-1),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  return features[\'image/encoded\'], label\n\n\ndef eval_image(image, height, width, resize_method,\n               central_fraction=0.875, scope=None):\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if resize_method == \'crop\':\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                      true_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256, 256 * shape[1] / shape[0]],\n                                                                          dtype=tf.int32)),\n                      false_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256 * shape[0] / shape[1], 256],\n                                                                          dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height, width)\n      distorted_image.set_shape([height, width, 3])\n      return distorted_image\n    else:  # bilinear\n      if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n      # Crop the central region of the image with an area containing 87.5% of\n      # the original image.\n      if central_fraction:\n        image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n      if height and width:\n        # Resize the image to the specified height and width.\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize(image, [height, width],\n                                         method=tf.image.ResizeMethod.BILINEAR)\n        image = tf.squeeze(image, [0])\n      image = tf.subtract(image, 0.5)\n      image = tf.multiply(image, 2.0)\n      return image\n\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               num_cores,\n               resize_method):\n\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.num_cores = num_cores\n    self.resize_method = resize_method\n\n  def parse_and_preprocess(self, value):\n    # parse\n    image_buffer, label_index = parse_example_proto(value)\n    # preprocess\n    image = tf.image.decode_jpeg(\n      image_buffer, channels=3, fancy_upscaling=False, dct_method=\'INTEGER_FAST\')\n    image = eval_image(image, self.height, self.width, self.resize_method)\n\n    return (image, label_index)\n\n  def minibatch(self, dataset, subset, cache_data=False):\n\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n\n      glob_pattern = dataset.tf_record_pattern(subset)\n      file_names = gfile.Glob(glob_pattern)\n      if not file_names:\n        raise ValueError(\'Found no files in --data_dir matching: {}\'\n                         .format(glob_pattern))\n      ds = tf.data.TFRecordDataset.list_files(file_names)\n\n      ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=self.num_cores, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n\n      if cache_data:\n        ds = ds.take(1).cache().repeat()\n\n      ds = ds.prefetch(buffer_size=10000)\n\n      # num of parallel batches not greater than 56\n      max_num_parallel_batches = min(56, 2*self.num_cores)\n      ds = ds.apply(\n        map_and_batch(\n          map_func=self.parse_and_preprocess,\n          batch_size=self.batch_size,\n          num_parallel_batches=max_num_parallel_batches,\n          num_parallel_calls=None))  # this number should be tuned\n\n      ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # this number can be tuned\n\n      ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n      images, _ = ds_iterator.get_next()\n\n      return images\n'"
models/image_recognition/tensorflow/inceptionv4/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/image_recognition/tensorflow/inceptionv4/inference/accuracy.py,14,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\n\ndef load_graph(model_file):\n    graph = tf.Graph()\n    graph_def = tf.compat.v1.GraphDef()\n\n    import os\n    file_ext = os.path.splitext(model_file)[1]\n\n    with open(model_file, ""rb"") as f:\n        if file_ext == \'.pbtxt\':\n            text_format.Merge(f.read(), graph_def)\n        else:\n            graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def, name=\'\')\n\n    return graph\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--input_graph"", default=None,\n                        help=""graph/model to be executed"")\n    parser.add_argument(""--data_location"", default=None,\n                        help=""full path to the validation data"")\n    parser.add_argument(""--input_height"", default=None,\n                        type=int, help=""input height"")\n    parser.add_argument(""--input_width"", default=None,\n                        type=int, help=""input width"")\n    parser.add_argument(""--batch_size"", default=32,\n                        type=int, help=""batch size"")\n    parser.add_argument(""--input_layer"", default=""input"",\n                        help=""name of input layer"")\n    parser.add_argument(""--output_layer"",\n                        default=""InceptionV4/Logits/Predictions"",\n                        help=""name of output layer"")\n    parser.add_argument(\n        \'--num_inter_threads\',\n        help=\'number threads across operators\',\n        type=int, default=1)\n    parser.add_argument(\n        \'--num_intra_threads\',\n        help=\'number threads for an operator\',\n        type=int, default=1)\n    args = parser.parse_args()\n\n    if args.input_graph:\n        model_file = args.input_graph\n    else:\n        sys.exit(""Please provide a graph file."")\n    if args.input_height:\n        input_height = args.input_height\n    else:\n        input_height = 299\n    if args.input_width:\n        input_width = args.input_width\n    else:\n        input_width = 299\n    batch_size = args.batch_size\n    input_layer = args.input_layer\n    output_layer = args.output_layer\n    num_inter_threads = args.num_inter_threads\n    num_intra_threads = args.num_intra_threads\n    data_location = args.data_location\n\n\n    data_graph = tf.Graph() ###\n    with data_graph.as_default(): ###\n      dataset = datasets.ImagenetData(data_location)\n      preprocessor = dataset.get_image_preprocessor()(\n          input_height, input_width, batch_size,\n          1,  # device count\n          tf.float32,  # data_type for input fed to the graph\n          train=False,  # doing inference\n          resize_method=\'bilinear\')\n      images, labels = preprocessor.minibatch(dataset, subset=\'validation\',\n                                            use_datasets=True,\n                                            cache_data=False)\n    graph = load_graph(model_file)\n    input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n    output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n\n    config = tf.compat.v1.ConfigProto()\n    config.inter_op_parallelism_threads = num_inter_threads\n    config.intra_op_parallelism_threads = num_intra_threads\n\n    total_accuracy1, total_accuracy5 = (0.0, 0.0)\n    num_processed_images = 0\n    num_remaining_images = dataset.num_examples_per_epoch(subset=\'validation\') \\\n                           - num_processed_images\n\n    with tf.compat.v1.Session(graph=data_graph) as sess: ###\n        sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n        while num_remaining_images >= batch_size:\n            # Reads and preprocess data\n            np_images, np_labels = sess.run([images[0], labels[0]])\n            num_processed_images += batch_size\n            num_remaining_images -= batch_size\n            start_time = time.time()\n            # Compute inference on the preprocessed data\n            predictions = sess_graph.run(output_tensor,\n                                         {input_tensor: np_images})\n            elapsed_time = time.time() - start_time\n            accuracy1 = tf.reduce_sum(\n                input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                       targets=tf.constant(np_labels), k=1), tf.float32))\n\n            accuracy5 = tf.reduce_sum(\n                input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                       targets=tf.constant(np_labels), k=5), tf.float32))\n            np_accuracy1, np_accuracy5 = sess.run([accuracy1, accuracy5])\n            total_accuracy1 += np_accuracy1\n            total_accuracy5 += np_accuracy5\n            print(""Iteration time: %0.4f ms"" % elapsed_time)\n            print(\n                ""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n                % (\n                num_processed_images, total_accuracy1 / num_processed_images,\n                total_accuracy5 / num_processed_images))\n'"
models/image_recognition/tensorflow/inceptionv4/inference/benchmark.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--input_height"", default=None,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=None,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""InceptionV4/Logits/Predictions"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  parser.add_argument(""--warmup_steps"", type=int, default=10,\n                      help=""number of warmup steps"")\n  parser.add_argument(""--steps"", type=int, default=50, help=""number of steps"")\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 299\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 299\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  warmup_steps = args.warmup_steps\n  steps = args.steps\n  assert steps > 10, ""Benchmark steps should be at least 10.""\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n\n  data_graph = tf.Graph() ##\n  with data_graph.as_default():##\n    input_shape = [batch_size, input_height, input_width, 3]\n    images = tf.random.truncated_normal(\n          input_shape,\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\')\n\n  #image_data = None\n  #with tf.compat.v1.Session() as sess:\n  #  image_data = sess.run(images)\n\n  graph = load_graph(model_file)\n\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"");\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"");\n  tf.compat.v1.global_variables_initializer()###\n\n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  data_config = tf.compat.v1.ConfigProto()###\n  data_config.inter_op_parallelism_threads = num_inter_threads ###\n  data_config.intra_op_parallelism_threads = num_intra_threads ###\n  \n  data_sess = tf.compat.v1.Session(graph=data_graph, config=data_config) ###\n\n  with tf.compat.v1.Session(graph=graph, config=config) as sess:\n    sys.stdout.flush()\n    print(""[Running warmup steps...]"")\n    image_data = data_sess.run(images) ###\n    for t in range(warmup_steps):\n      start_time = time.time()\n      sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t+1, batch_size/elapsed_time))\n\n    print(""[Running benchmark steps...]"")\n    total_time   = 0;\n    total_images = 0;\n    for t in range(steps):\n      start_time = time.time()\n      results = sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t+1, batch_size/elapsed_time));\n      total_time += elapsed_time\n      total_images += batch_size\n    average_time = total_time / total_images\n    if batch_size == 1:\n      print(\'Latency: %.3f ms\' % (average_time * 1000))\n'"
models/image_recognition/tensorflow/inceptionv4/inference/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n'"
models/image_recognition/tensorflow/inceptionv4/inference/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport numpy as np\nfrom six.moves import cPickle\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nimport preprocessing\n\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\n\n\ndef create_dataset(data_dir, data_name):\n  """"""Create a Dataset instance based on data_dir and data_name.""""""\n  supported_datasets = {\n      \'imagenet\': ImagenetData,\n      \'cifar10\': Cifar10Data,\n  }\n  if not data_dir and not data_name:\n    # When using synthetic data, use synthetic imagenet images by default.\n    data_name = \'imagenet\'\n\n  if data_name is None:\n    for supported_name in supported_datasets:\n      if supported_name in data_dir:\n        data_name = supported_name\n        break\n\n  if data_name is None:\n    raise ValueError(\'Could not identify name of dataset. \'\n                     \'Please specify with --data_name option.\')\n\n  if data_name not in supported_datasets:\n    raise ValueError(\'Unknown dataset. Must be one of %s\', \', \'.join(\n        [key for key in sorted(supported_datasets.keys())]))\n\n  return supported_datasets[data_name](data_dir)\n\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, height=None, width=None, depth=None, data_dir=None,\n               queue_runner_required=False, num_classes=1000):\n    self.name = name\n    self.height = height\n    self.width = width\n    self.depth = depth or 3\n\n    self.data_dir = data_dir\n    self._queue_runner_required = queue_runner_required\n    self._num_classes = num_classes\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @num_classes.setter\n  def num_classes(self, val):\n    self._num_classes = val\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n  def get_image_preprocessor(self):\n    return None\n\n  def queue_runner_required(self):\n    return self._queue_runner_required\n\n  def use_synthetic_gpu_images(self):\n    return not self.data_dir\n\n\nclass ImagenetData(Dataset):\n  """"""Configuration for Imagenet dataset.""""""\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'imagenet\', 300, 300, data_dir=data_dir)\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    if self.use_synthetic_gpu_images():\n      return preprocessing.SyntheticImagePreprocessor\n    else:\n      return preprocessing.RecordInputImagePreprocessor\n\n\nclass Cifar10Data(Dataset):\n  """"""Configuration for cifar 10 dataset.\n\n  It will mount all the input images to memory.\n  """"""\n\n  def __init__(self, data_dir=None):\n    super(Cifar10Data, self).__init__(\'cifar10\', 32, 32, data_dir=data_dir,\n                                      queue_runner_required=True,\n                                      num_classes=10)\n\n  def read_data_files(self, subset=\'train\'):\n    """"""Reads from data file and returns images and labels in a numpy array.""""""\n    assert self.data_dir, (\'Cannot call `read_data_files` when using synthetic \'\n                           \'data\')\n    if subset == \'train\':\n      filenames = [os.path.join(self.data_dir, \'data_batch_%d\' % i)\n                   for i in xrange(1, 6)]\n    elif subset == \'validation\':\n      filenames = [os.path.join(self.data_dir, \'test_batch\')]\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n    inputs = []\n    for filename in filenames:\n      with gfile.Open(filename, \'r\') as f:\n        inputs.append(cPickle.load(f))\n    # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n    # input format.\n    all_images = np.concatenate(\n        [each_input[\'data\'] for each_input in inputs]).astype(np.float32)\n    all_labels = np.concatenate(\n        [each_input[\'labels\'] for each_input in inputs])\n    return all_images, all_labels\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return 50000\n    elif subset == \'validation\':\n      return 10000\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    if self.use_synthetic_gpu_images():\n      return preprocessing.SyntheticImagePreprocessor\n    else:\n      return preprocessing.Cifar10ImagePreprocessor\n'"
models/image_recognition/tensorflow/inceptionv4/inference/preprocessing.py,117,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport math\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n#from tensorflow.contrib.data.python.ops import batching\n#from tensorflow.contrib.data.python.ops import interleave_ops\n#from tensorflow.contrib.image.python.ops import distort_image_ops\n\nfrom tensorflow.python.data.experimental import parallel_interleave###\nfrom tensorflow.python.data.experimental import map_and_batch###\n\nfrom tensorflow.python.layers import utils\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.platform import gfile\nimport cnn_util\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef parse_example_proto(example_serialized):\n    """"""Parses an Example proto containing a training example of an image.\n  \n    The output of the build_image_data.py image preprocessing script is a dataset\n    containing serialized Example protocol buffers. Each Example proto contains\n    the following fields:\n  \n      image/height: 462\n      image/width: 581\n      image/colorspace: \'RGB\'\n      image/channels: 3\n      image/class/label: 615\n      image/class/synset: \'n03623198\'\n      image/class/text: \'knee pad\'\n      image/object/bbox/xmin: 0.1\n      image/object/bbox/xmax: 0.9\n      image/object/bbox/ymin: 0.2\n      image/object/bbox/ymax: 0.6\n      image/object/bbox/label: 615\n      image/format: \'JPEG\'\n      image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n      image/encoded: <JPEG encoded string>\n  \n    Args:\n      example_serialized: scalar Tensor tf.string containing a serialized\n        Example protocol buffer.\n  \n    Returns:\n      image_buffer: Tensor tf.string containing the contents of a JPEG file.\n      label: Tensor tf.int32 containing the label.\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged as\n        [ymin, xmin, ymax, xmax].\n      text: Tensor tf.string containing the human-readable label.\n    """"""\n    # Dense features in Example proto.\n    feature_map = {\n        \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                            default_value=\'\'),\n        \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                                default_value=-1),\n        \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                               default_value=\'\'),\n    }\n    sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n    # Sparse features in Example proto.\n    feature_map.update(\n        {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                     \'image/object/bbox/ymin\',\n                                     \'image/object/bbox/xmax\',\n                                     \'image/object/bbox/ymax\']})\n\n    features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n    label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n    xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n    ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n    xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n    ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n    # Note that we impose an ordering of (y, x) just to make life difficult.\n    bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n    # Force the variable number of bounding boxes into the shape\n    # [1, num_boxes, coords].\n    bbox = tf.expand_dims(bbox, 0)\n    bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n    return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef get_image_resize_method(resize_method, batch_position=0):\n    """"""Get tensorflow resize method.\n  \n    If resize_method is \'round_robin\', return different methods based on batch\n    position in a round-robin fashion. NOTE: If the batch size is not a multiple\n    of the number of methods, then the distribution of methods will not be\n    uniform.\n  \n    Args:\n      resize_method: (string) nearest, bilinear, bicubic, area, or round_robin.\n      batch_position: position of the image in a batch. NOTE: this argument can\n        be an integer or a tensor\n    Returns:\n      one of resize type defined in tf.image.ResizeMethod.\n    """"""\n    resize_methods_map = {\n        \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n        \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n        \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n        \'area\': tf.image.ResizeMethod.AREA\n    }\n\n    if resize_method != \'round_robin\':\n        return resize_methods_map[resize_method]\n\n    # return a resize method based on batch position in a round-robin fashion.\n    resize_methods = resize_methods_map.values()\n\n    def lookup(index):\n        return resize_methods[index]\n\n    def resize_method_0():\n        return utils.smart_cond(batch_position % len(resize_methods) == 0,\n                                lambda: lookup(0), resize_method_1)\n\n    def resize_method_1():\n        return utils.smart_cond(batch_position % len(resize_methods) == 1,\n                                lambda: lookup(1), resize_method_2)\n\n    def resize_method_2():\n        return utils.smart_cond(batch_position % len(resize_methods) == 2,\n                                lambda: lookup(2), lambda: lookup(3))\n\n    # NOTE(jsimsa): Unfortunately, we cannot use a single recursive function here\n    # because TF would not be able to construct a finite graph.\n\n    return resize_method_0()\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n    """"""Decode a JPEG string into one 3-D float image Tensor.\n  \n    Args:\n      image_buffer: scalar string Tensor.\n      scope: Optional scope for op_scope.\n    Returns:\n      3-D float Tensor with values ranging from [0, 1).\n    """"""\n    # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n    # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n    with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n        # Decode the string as an RGB JPEG.\n        # Note that the resulting image contains an unknown height and width\n        # that is set dynamically by decode_jpeg. In other words, the height\n        # and width of image is unknown at compile-time.\n        image = tf.image.decode_jpeg(image_buffer, channels=3)  # ,\n        #     fancy_upscaling=False,\n        #     dct_method=\'INTEGER_FAST\')\n\n        # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n        return image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n    """"""Prepare one image for evaluation.\n  \n    If height and width are specified it would output an image with that size by\n    applying resize_bilinear.\n  \n    If central_fraction is specified it would crop the central fraction of the\n    input image.\n  \n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      central_fraction: Optional Float, fraction of the image to crop.\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D float Tensor of prepared image.\n    """"""\n    with tf.compat.v1.name_scope(scope, \'eval_image\', [image, height, width]):\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Crop the central region of the image with an area containing 87.5% of\n        # the original image.\n        if central_fraction:\n            image = tf.image.central_crop(image,\n                                          central_fraction=central_fraction)\n\n        if height and width:\n            # Resize the image to the specified height and width.\n            image = tf.expand_dims(image, 0)\n            image = tf.image.resize(image, [height, width],\n                                             method=tf.image.ResizeMethod.BILINEAR)\n            image = tf.squeeze(image, [0])\n        image = tf.subtract(image, 0.5)\n        image = tf.multiply(image, 2.0)\n        return image\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n  \n    Args:\n      x: input Tensor.\n      func: Python function to apply.\n      num_cases: Python int32, number of cases to sample sel from.\n  \n    Returns:\n      The result of func(x, sel), where func receives the value of the\n      selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random.uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n        func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n        for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n  \n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n  \n    Args:\n      image: 3-D Tensor containing single image in [0, 1].\n      color_ordering: Python int, a type of distortion (valid values: 0-3).\n      fast_mode: Avoids slower ops (random_hue and random_contrast)\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n      ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.compat.v1.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n  \n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n  \n    Args:\n      image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n        image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding box\n        supplied.\n      aspect_ratio_range: An optional list of `floats`. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `floats`. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n      scope: Optional scope for name_scope.\n    Returns:\n      A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.compat.v1.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n        # A large fraction of image datasets contain a human-annotated bounding\n        # box delineating the region of the image containing the object of interest.\n        # We choose to create a new bounding box for the object which is a randomly\n        # distorted version of the human-annotated bounding box that obeys an\n        # allowed range of aspect ratios, sizes and overlap with the human-annotated\n        # bounding box. If no box is supplied, then we assume the bounding box is\n        # the entire image.\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            image_size=tf.shape(input=image),\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         batch_position,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n    """"""Distort one image for training a network.\n  \n    Distorting images provides a useful technique for augmenting the data\n    set during training in order to make the network invariant to aspects\n    of the image that do not effect the label.\n  \n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax].\n      batch_position: position of the image in a batch, which affects how images\n        are distorted and resized. NOTE: this argument can be an integer or a\n        tensor\n      scope: Optional scope for op_scope.\n      add_image_summaries: Enable image summaries.\n    Returns:\n      3-D float Tensor of distorted image used for training with range [-1, 1].\n    """"""\n\n    with tf.compat.v1.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n        if bbox is None:\n            bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                               dtype=tf.float32,\n                               shape=[1, 1, 4])\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                      bbox)\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n        distorted_image, distorted_bbox = distorted_bounding_box_crop(image,\n                                                                      bbox)\n        # Restore the shape since the dynamic slice based upon the bbox_size loses\n        # the third dimension.\n        distorted_image.set_shape([None, None, 3])\n        image_with_distorted_box = tf.image.draw_bounding_boxes(\n            tf.expand_dims(image, 0), distorted_bbox)\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'images_with_distorted_bounding_box\',\n                             image_with_distorted_box)\n\n        # This resizing operation may distort the images because the aspect\n        # ratio is not respected. We select a resize method in a round robin\n        # fashion based on the thread number.\n        # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n        # We select only 1 case for fast_mode bilinear.\n        num_resize_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n            distorted_image,\n            lambda x, method: tf.image.resize(x, [height, width],\n                                                     method),\n            num_cases=num_resize_cases)\n\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'cropped_resized_image\',\n                             tf.expand_dims(distorted_image, 0))\n\n        # Randomly flip the image horizontally.\n        distorted_image = tf.image.random_flip_left_right(distorted_image)\n        # Randomly distort the colors. There are 1 or 4 ways to do it.\n        num_distort_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n            distorted_image,\n            lambda x, ordering: distort_color(x, ordering, fast_mode),\n            num_cases=num_distort_cases)\n\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'final_distorted_image\',\n                             tf.expand_dims(distorted_image, 0))\n        distorted_image = tf.subtract(distorted_image, 0.5)\n        distorted_image = tf.multiply(distorted_image, 2.0)\n        return distorted_image\n\n\ndef distort_color(image, batch_position=0, distort_color_in_yiq=False,\n                  scope=None):\n    """"""Distort the color of the image.\n  \n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops based on the position of the image in a batch.\n  \n    Args:\n      image: float32 Tensor containing single image. Tensor values should be in\n        range [0, 1].\n      batch_position: the position of the image in a batch. NOTE: this argument\n        can be an integer or a tensor\n      distort_color_in_yiq: distort color of input images in YIQ space.\n      scope: Optional scope for op_scope.\n    Returns:\n      color-distorted image\n    """"""\n    with tf.compat.v1.name_scope(scope or \'distort_color\'):\n        def distort_fn_0(image=image):\n            """"""Variant 0 of distort function.""""""\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            # if distort_color_in_yiq:\n            #  image = distort_image_ops.random_hsv_in_yiq(\n            #      image, lower_saturation=0.5, upper_saturation=1.5,\n            #      max_delta_hue=0.2 * math.pi)\n            # else:\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            return image\n\n        def distort_fn_1(image=image):\n            """"""Variant 1 of distort function.""""""\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            # if distort_color_in_yiq:\n            #  image = distort_image_ops.random_hsv_in_yiq(\n            #      image, lower_saturation=0.5, upper_saturation=1.5,\n            #      max_delta_hue=0.2 * math.pi)\n            # else:\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            return image\n\n        image = utils.smart_cond(batch_position % 2 == 0, distort_fn_0,\n                                 distort_fn_1)\n        # The random_* ops do not necessarily clamp.\n        image = tf.clip_by_value(image, 0.0, 1.0)\n        return image\n\n\nclass RecordInputImagePreprocessor(object):\n    """"""Preprocessor for images with RecordInput format.""""""\n\n    def __init__(self,\n                 height,\n                 width,\n                 batch_size,\n                 num_splits,\n                 dtype,\n                 train,\n                 distortions=False,\n                 resize_method=""bilinear"",\n                 shift_ratio=0,\n                 summary_verbosity=1,\n                 distort_color_in_yiq=False,\n                 fuse_decode_and_crop=False):\n        self.height = height\n        self.width = width\n        self.batch_size = batch_size\n        self.num_splits = num_splits\n        self.dtype = dtype\n        self.train = train\n        self.resize_method = resize_method\n        self.shift_ratio = shift_ratio\n        self.distortions = distortions\n        self.distort_color_in_yiq = distort_color_in_yiq\n        self.fuse_decode_and_crop = fuse_decode_and_crop\n        if self.batch_size % self.num_splits != 0:\n            raise ValueError(\n                (\'batch_size must be a multiple of num_splits: \'\n                 \'batch_size %d, num_splits: %d\') %\n                (self.batch_size, self.num_splits))\n        self.batch_size_per_split = self.batch_size // self.num_splits\n        self.summary_verbosity = summary_verbosity\n\n    def image_preprocess(self, image_buffer, bbox, batch_position):\n        """"""Preprocessing image_buffer as a function of its batch position.""""""\n        if self.train:\n            image_buffer = tf.image.decode_jpeg(\n                image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n            image = preprocess_for_train(image_buffer, self.height, self.width,\n                                         bbox,\n                                         batch_position)\n        else:\n            image = tf.image.decode_jpeg(\n                image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n            image = preprocess_for_eval(image, self.height, self.width)\n        return image\n\n    def parse_and_preprocess(self, value, batch_position):\n        image_buffer, label_index, bbox, _ = parse_example_proto(value)\n        image = self.image_preprocess(image_buffer, bbox, batch_position)\n        return (label_index, image)\n\n    def minibatch(self, dataset, subset, use_datasets, cache_data,\n                  shift_ratio=-1):\n        if shift_ratio < 0:\n            shift_ratio = self.shift_ratio\n        with tf.compat.v1.name_scope(\'batch_processing\'):\n            # Build final results per split.\n            images = [[] for _ in range(self.num_splits)]\n            labels = [[] for _ in range(self.num_splits)]\n            if use_datasets:\n                glob_pattern = dataset.tf_record_pattern(subset)\n                file_names = gfile.Glob(glob_pattern)\n                if not file_names:\n                    raise ValueError(\n                        \'Found no files in --data_dir matching: {}\'\n                        .format(glob_pattern))\n                ds = tf.data.TFRecordDataset.list_files(file_names)\n                ds = ds.apply(\n                    #interleave_ops.parallel_interleave(\n                    parallel_interleave( #\n                        tf.data.TFRecordDataset, cycle_length=10))\n                if cache_data:\n                    ds = ds.take(1).cache().repeat()\n                counter = tf.data.Dataset.range(self.batch_size)\n                counter = counter.repeat()\n                ds = tf.data.Dataset.zip((ds, counter))\n                ds = ds.prefetch(buffer_size=self.batch_size)\n                ds = ds.shuffle(buffer_size=10000)\n                ds = ds.repeat()\n                ds = ds.apply(\n                    #batching.map_and_batch(\n                    map_and_batch( ###\n                        map_func=self.parse_and_preprocess,\n                        batch_size=self.batch_size_per_split,\n                        num_parallel_batches=self.num_splits))\n                ds = ds.prefetch(buffer_size=self.num_splits)\n                ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n                for d in xrange(self.num_splits):\n                    labels[d], images[d] = ds_iterator.get_next()\n\n            else:\n                record_input = data_flow_ops.RecordInput(\n                    file_pattern=dataset.tf_record_pattern(subset),\n                    seed=301,\n                    parallelism=64,\n                    buffer_size=10000,\n                    batch_size=self.batch_size,\n                    shift_ratio=shift_ratio,\n                    name=\'record_input\')\n                records = record_input.get_yield_op()\n                records = tf.split(records, self.batch_size, 0)\n                records = [tf.reshape(record, []) for record in records]\n                for idx in xrange(self.batch_size):\n                    value = records[idx]\n                    (label, image) = self.parse_and_preprocess(value, idx)\n                    split_index = idx % self.num_splits\n                    labels[split_index].append(label)\n                    images[split_index].append(image)\n\n            for split_index in xrange(self.num_splits):\n                if not use_datasets:\n                    images[split_index] = tf.parallel_stack(\n                        images[split_index])\n                    labels[split_index] = tf.concat(labels[split_index], 0)\n                images[split_index] = tf.cast(images[split_index], self.dtype)\n                depth = 3\n                images[split_index] = tf.reshape(\n                    images[split_index],\n                    shape=[self.batch_size_per_split, self.height, self.width,\n                           depth])\n                labels[split_index] = tf.reshape(labels[split_index],\n                                                 [self.batch_size_per_split])\n            return images, labels\n'"
models/image_recognition/tensorflow/resnet101/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/image_recognition/tensorflow/resnet101/inference/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport tensorflow as tf\n\nimport preprocessing\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\nIMAGENET_NUM_CLASSES = 1000\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, data_dir=None):\n    self.name = name\n    if data_dir is None:\n      raise ValueError(\'Data directory not specified\')\n    self.data_dir = data_dir\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @abstractmethod\n  def num_classes(self):\n    pass\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n\nclass ImagenetData(Dataset):\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'ImageNet\', data_dir)\n\n  def num_classes(self):\n    return IMAGENET_NUM_CLASSES\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    return preprocessing.RecordInputImagePreprocessor\n'"
models/image_recognition/tensorflow/resnet101/inference/eval_image_classifier_inference.py,21,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport time\nfrom argparse import ArgumentParser\n\nimport tensorflow as tf\nfrom tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\nfrom tensorflow.python.framework import dtypes\n\nimport datasets\n\nINPUTS = \'input\'\nOUTPUTS = \'resnet_v1_101/predictions/Reshape_1\'\n\nRESNET_IMAGE_SIZE = 224\nIMAGENET_VALIDATION_IMAGES = 50000\n\n\nclass eval_classifier_optimized_graph:\n  """"""Evaluate image classifier with optimized TensorFlow graph""""""\n\n  def __init__(self):\n\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--num-inter-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--num-intra-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-m\', ""--model-name"",\n                            help=\'Specify the model name to run benchmark for\',\n                            dest=\'model_name\')\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph for the transform tool\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n\n    arg_parser.add_argument(""--warmup-steps"", type=int, default=10,\n                            help=""number of warmup steps"")\n    arg_parser.add_argument(""--steps"", type=int, default=50,\n                            help=""number of steps"")\n    arg_parser.add_argument(\n        \'--data-num-inter-threads\', dest=\'data_num_inter_threads\',\n        help=\'number threads across operators\',\n        type=int, default=16)\n    arg_parser.add_argument(\n        \'--data-num-intra-threads\', dest=\'data_num_intra_threads\',\n        help=\'number threads for data layer operator\',\n        type=int, default=14)\n    # parse the arguments\n    self.args = arg_parser.parse_args()\n    # validate the arguements\n    self.validate_args()\n\n  def run(self):\n    """"""run benchmark with optimized graph""""""\n\n    print(""Run inference"")\n\n    data_config = tf.compat.v1.ConfigProto()\n    data_config.intra_op_parallelism_threads = self.args.data_num_intra_threads\n    data_config.inter_op_parallelism_threads = self.args.data_num_inter_threads\n    data_config.use_per_session_threads = 1\n\n    infer_config = tf.compat.v1.ConfigProto()\n    infer_config.intra_op_parallelism_threads = self.args.num_intra_threads\n    infer_config.inter_op_parallelism_threads = self.args.num_inter_threads\n    infer_config.use_per_session_threads = 1\n\n    data_graph = tf.Graph()\n    with data_graph.as_default():\n      if (self.args.data_location):\n        print(""Inference with real data."")\n        dataset = datasets.ImagenetData(self.args.data_location)\n        preprocessor = dataset.get_image_preprocessor()(\n            RESNET_IMAGE_SIZE, RESNET_IMAGE_SIZE, self.args.batch_size,\n            intra_threads=self.args.num_intra_threads,\n            resize_method=\'crop\')\n        images, labels = preprocessor.minibatch(dataset, subset=\'validation\')\n      else:\n        print(""Inference with dummy data."")\n        input_shape = [self.args.batch_size, RESNET_IMAGE_SIZE, RESNET_IMAGE_SIZE, 3]\n        images = tf.random.uniform(input_shape, 0.0, 255.0,dtype=tf.float32,name=\'synthetic_images\')\n\n    infer_graph = tf.Graph()\n    with infer_graph.as_default():\n      # convert the freezed graph to optimized graph\n      graph_def = tf.compat.v1.GraphDef()\n      with tf.compat.v1.gfile.FastGFile(self.args.input_graph, \'rb\') as input_file:\n        input_graph_content = input_file.read()\n        graph_def.ParseFromString(input_graph_content)\n\n      output_graph = optimize_for_inference(graph_def, [INPUTS], \n                              [OUTPUTS], dtypes.float32.as_datatype_enum, False)\n      tf.import_graph_def(output_graph, name=\'\')\n\n    # Definite input and output Tensors for detection_graph\n    input_tensor = infer_graph.get_tensor_by_name(\'input:0\')\n    #output_tensor = infer_graph.get_tensor_by_name(\'resnet_v1_101/SpatialSqueeze:0\')\n    output_tensor = infer_graph.get_tensor_by_name(\'resnet_v1_101/predictions/Reshape_1:0\')\n      \n    #tf.global_variables_initializer()\n    data_sess  = tf.compat.v1.Session(graph=data_graph,  config=data_config)\n    infer_sess = tf.compat.v1.Session(graph=infer_graph, config=infer_config)\n\n    num_processed_images = 0\n    num_remaining_images = IMAGENET_VALIDATION_IMAGES\n\n    if (not self.args.accuracy_only):  # performance check\n      iteration = 0\n      warm_up_iteration = self.args.warmup_steps\n      total_run = self.args.steps\n      total_time = 0\n      #options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n      #run_metadata = tf.RunMetadata()\n\n      while num_remaining_images >= self.args.batch_size and iteration < total_run:\n        iteration += 1\n\n        # Reads and preprocess data\n        data_load_start = time.time()\n        image_np = data_sess.run(images)\n        data_load_time = time.time() - data_load_start\n\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        infer_sess.run([output_tensor], feed_dict={input_tensor: image_np})\n        time_consume = time.time() - start_time\n\n        # only add data loading time for real data, not for dummy data\n        if self.args.data_location:\n          time_consume += data_load_time\n\n        #trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n        #with gfile.Open(\'resnet101_fp32_int8_master\', \'w\') as trace_file:\n        #    trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\n\n        print(\'Iteration %d: %.3f sec\' % (iteration, time_consume))\n        if iteration > warm_up_iteration:\n          total_time += time_consume\n\n      time_average = total_time / (iteration - warm_up_iteration)\n      print(\'Average time: %.3f sec\' % (time_average))\n\n      print(\'Batch size = %d\' % self.args.batch_size)\n      if (self.args.batch_size == 1):\n        print(\'Latency: %.3f ms\' % (time_average * 1000))\n      # print throughput for both batch size 1 and 128\n      print(\'Throughput: %.3f images/sec\' % (self.args.batch_size / time_average))\n\n    else:  # accuracy check\n      total_accuracy1, total_accuracy5 = (0.0, 0.0)\n\n\n      while num_remaining_images >= self.args.batch_size:\n        # Reads and preprocess data\n        np_images, np_labels = data_sess.run([images, labels])\n        np_labels -= 1\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        # Compute inference on the preprocessed data\n        predictions = infer_sess.run(output_tensor,\n                               {input_tensor: np_images})\n        elapsed_time = time.time() - start_time\n        with tf.Graph().as_default() as accu_graph:\n          # Putting all code within this make things faster.\n          accuracy1 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                 targets=tf.constant(np_labels), k=1), tf.float32))\n\n          accuracy5 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                 targets=tf.constant(np_labels), k=5), tf.float32))\n          with tf.compat.v1.Session() as accu_sess:\n            np_accuracy1, np_accuracy5 = accu_sess.run([accuracy1, accuracy5])\n          total_accuracy1 += np_accuracy1\n          total_accuracy5 += np_accuracy5\n          print(""Iteration time: %0.4f ms"" % elapsed_time)\n          print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n              % (num_processed_images, total_accuracy1 / num_processed_images,\n                 total_accuracy5 / num_processed_images))\n\n  def validate_args(self):\n    """"""validate the arguments""""""\n\n    if not self.args.data_location:\n      if self.args.accuracy_only:\n        raise ValueError(""You must use real data for accuracy measurement."")\n\n\nif __name__ == ""__main__"":\n  evaluate_opt_graph = eval_classifier_optimized_graph()\n  evaluate_opt_graph.run()\n'"
models/image_recognition/tensorflow/resnet101/inference/preprocessing.py,32,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.platform import gfile\nimport vgg_preprocessing\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                        default_value=\'\'),\n    \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                            default_value=-1)\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  return features[\'image/encoded\'], label\n\n\ndef eval_image(image, height, width, resize_method,\n               central_fraction=0.875, scope=None):\n\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if resize_method == \'crop\':\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                      true_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256, 256 * shape[1] / shape[0]],\n                                                                          dtype=tf.int32)),\n                      false_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256 * shape[0] / shape[1], 256],\n                                                                          dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height,                                                     width)\n      distorted_image.set_shape([height, width, 3])\n      return distorted_image\n    else:  # bilinear\n      if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n      # Crop the central region of the image with an area containing 87.5% of\n      # the original image.\n      if central_fraction:\n        image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n      if height and width:\n        # Resize the image to the specified height and width.\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize(image, [height, width],\n                                         method=tf.image.ResizeMethod.BILINEAR)\n        image = tf.squeeze(image, [0])\n      image = tf.subtract(image, 0.5)\n      image = tf.multiply(image, 2.0)\n      return image\n\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               intra_threads,\n               resize_method=""bilinear""):\n\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.intra_threads = intra_threads\n    self.resize_method = resize_method\n    # parallel number of files and tfrecords\n    # file_dict   = {1: 8,   2: 8,   4: 8,   8: 8,   16: 20, 32: 20, 64: 28}\n    # record_dict = {1: 150, 2: 150, 4: 150, 8: 150, 16: 10, 32: 10, 64: 5}\n    # self.num_files = file_dict.get(self.batch_size, 10)  # default is 10\n    # self.num_records = record_dict.get(self.batch_size, 2)  # default is 2\n\n  def parse_and_preprocess(self, value):\n    # parse\n    image_buffer, label_index = parse_example_proto(value)\n    # preprocess\n    image = tf.image.decode_jpeg(\n      image_buffer, channels=3, fancy_upscaling=False, dct_method=\'INTEGER_FAST\')\n    image = vgg_preprocessing.preprocess_image(image,224,224,False)\n\n    return (image, label_index)\n\n  def minibatch(self, dataset, subset, cache_data=False):\n\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n\n      glob_pattern = dataset.tf_record_pattern(subset)\n      file_names = gfile.Glob(glob_pattern)\n      if not file_names:\n        raise ValueError(\'Found no files in --data_dir matching: {}\'\n                         .format(glob_pattern))\n      ds = tf.data.TFRecordDataset.list_files(file_names)\n\n      # number of parallel open files and tfrecords should be tuned according to\n      # different batch size\n      ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=28, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n\n      if cache_data:\n        ds = ds.take(1).cache().repeat()\n\n      ds = ds.prefetch(buffer_size=10000)\n      #ds = ds.prefetch(buffer_size=self.batch_size)\n\n      ds = ds.apply(\n        map_and_batch(\n          map_func=self.parse_and_preprocess,\n          batch_size=self.batch_size,\n          num_parallel_batches=56,\n          num_parallel_calls=None))  # this number should be tuned\n\n      ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # this number can be tuned\n\n      ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n      images, labels = ds_iterator.get_next()\n      # reshape\n      labels = tf.reshape(labels, [self.batch_size])\n\n      return images, labels\n'"
models/image_recognition/tensorflow/resnet101/inference/vgg_preprocessing.py,60,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(input=image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.cast(tf.stack([offset_height, offset_width, 0]), dtype=tf.int32)\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(input=image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(input=image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random.uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random.uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(input=image)[0]\n    image_width = tf.shape(input=image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(value=smallest_side, dtype=tf.int32)\n\n  height = tf.cast(height, dtype=tf.float32)\n  width = tf.cast(width, dtype=tf.float32)\n  smallest_side = tf.cast(smallest_side, dtype=tf.float32)\n\n  scale = tf.cond(pred=tf.greater(height, width),\n                  true_fn=lambda: smallest_side / width,\n                  false_fn=lambda: smallest_side / height)\n  new_height = tf.cast(tf.math.rint(height * scale), dtype=tf.int32)\n  new_width = tf.cast(tf.math.rint(width * scale), dtype=tf.int32)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(value=smallest_side, dtype=tf.int32)\n  #import pdb\n  #pdb.set_trace()\n  shape = tf.shape(input=image)\n\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize(image, [new_height, new_width],\n                                           method=tf.image.ResizeMethod.BILINEAR)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random.uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.cast(image, dtype=tf.float32)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.cast(image, dtype=tf.float32)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
models/image_recognition/tensorflow/resnet101/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/image_recognition/tensorflow/resnet101/int8/calibration.py,18,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n    tf.io.write_graph(graph_def, \'/tmp/\', \'optimized_graph.pb\',as_text=False)\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=None,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=None,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""resnet_v1_101/SpatialSqueeze"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 224\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 224\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n  dataset = datasets.ImagenetData(data_location)\n  preprocessor = preprocessing.ImagePreprocessor(\n      input_height, input_width, batch_size,\n      1, # device count\n      tf.float32, # data_type for input fed to the graph\n      train=False, # doing inference\n      resize_method=\'crop\')\n  images, labels = preprocessor.minibatch(dataset, subset=\'train\')\n  graph = load_graph(model_file)\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n  \n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = 5000\n  top1 = 0\n  with tf.compat.v1.Session() as sess:\n    sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n    \n    while num_remaining_images >= batch_size:\n      # Reads and preprocess data\n      np_images, np_labels = sess.run([images[0], labels[0]])\n      np_labels -= 1\n      #print(np_labels.shape)\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      # Compute inference on the preprocessed data\n      predictions1 = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      #predictions = predictions +1 \n      #print(predictions1)\n      predictions2 = tf.argmax(input=predictions1, axis=1)\n      predictions = sess.run(predictions2)\n      top1 += batch_size - (np.count_nonzero(predictions - np_labels))\n      #print(top1/num_processed_images)\n      #print(num_processed_images) \n      #print(predictions)\n      #accuracy1 = tf.reduce_sum(\n      #                         tf.nn.in_top_k(tf.cast(tf.Variable(predictions2), tf.float32),\n      #                         tf.cast((tf.constant(np_labels), 1), tf.float32)))\n      accuracy1 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions1),\n              targets=tf.constant(np_labels), k=1), tf.float32))\n\n      accuracy5 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions1),\n              targets=tf.constant(np_labels), k=5), tf.float32))\n      np_accuracy1, np_accuracy5 =  sess.run([accuracy1, accuracy5])\n\n      ##print(labels)\n      total_accuracy1 += np_accuracy1\n      total_accuracy5 += np_accuracy5\n      print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n          % (num_processed_images, total_accuracy1/num_processed_images,\n          total_accuracy5/num_processed_images))\n'"
models/image_recognition/tensorflow/resnet101/int8/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n\n'"
models/image_recognition/tensorflow/resnet101/int8/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport tensorflow as tf\n\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, data_dir=None):\n    self.name = name\n    if data_dir is None:\n      raise ValueError(\'Data directory not specified\')\n    self.data_dir = data_dir\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @abstractmethod\n  def num_classes(self):\n    pass\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n\nclass FlowersData(Dataset):\n\n  def __init__(self, data_dir=None):\n    super(FlowersData, self).__init__(\'Flowers\', data_dir)\n\n  def num_classes(self):\n    return 5\n\n  def num_examples_per_epoch(self, subset):\n    if subset == \'train\':\n      return 3170\n    elif subset == \'validation\':\n      return 500\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n\nclass ImagenetData(Dataset):\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'ImageNet\', data_dir)\n\n  def num_classes(self):\n    return 1000\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return 1281167\n    elif subset == \'validation\':\n      return 50000\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n'"
models/image_recognition/tensorflow/resnet101/int8/preprocessing.py,90,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom random import randint\nimport vgg_preprocessing\nfrom tensorflow.python.ops import data_flow_ops\nimport cnn_util\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields:\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    text: Tensor tf.string containing the human-readable label.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n  """"""Decode a JPEG string into one 3-D float image Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n  # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n  with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 fancy_upscaling=False,\n                                 dct_method=\'INTEGER_FAST\')\n\n    # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n\n    return image\n\n\ndef eval_image(image, height, width, bbox, thread_id, resize):\n  """"""Get the image for model evaluation.""""""\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'original_image\', tf.expand_dims(image, 0))\n\n    if resize == \'crop\':\n      # Note: This is much slower than crop_to_bounding_box\n      #         It seems that the redundant pad step has huge overhead\n      # distorted_image = tf.image.resize_image_with_crop_or_pad(image,\n      #                                                         height, width)\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                        true_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256, 256*shape[1]/shape[0]], dtype=tf.int32)),\n                        false_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256*shape[0]/shape[1], 256], dtype=tf.int32)))\n      shape = tf.shape(input=image)\n\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      #y0=tf.random_uniform([],minval=0,maxval=(shape[0] - height + 1), dtype=tf.int32)\n      #x0=tf.random_uniform([],minval=0,maxval=(shape[1] - width + 1), dtype=tf.int32)\n      ## distorted_image = tf.slice(image, [y0,x0,0], [height,width,3])\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height,\n                                                      width)\n    else:\n      sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n          image_size=tf.shape(input=image),\n          bounding_boxes=bbox,\n          min_object_covered=0.5,\n          aspect_ratio_range=[0.90, 1.10],\n          area_range=[0.10, 1.0],\n          max_attempts=100,\n          use_image_if_no_bounding_boxes=True)\n      bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n      # Crop the image to the specified bounding box.\n      distorted_image = tf.slice(image, bbox_begin, bbox_size)\n      resize_method = {\n          \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n          \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n          \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n          \'area\': tf.image.ResizeMethod.AREA\n      }[resize]\n      # This resizing operation may distort the images because the aspect\n      # ratio is not respected.\n      if cnn_util.tensorflow_version() >= 11:\n        distorted_image = tf.image.resize(\n            distorted_image, [height, width],\n            resize_method)\n      else:\n        distorted_image = tf.image.resize(\n            distorted_image, height, width, resize_method)\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\', tf.expand_dims(distorted_image, 0))\n    image = distorted_image\n  return image\n\n\ndef distort_image(image, height, width, bbox, thread_id=0, scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    image: 3-D float Tensor of image\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    thread_id: integer indicating the preprocessing thread.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training.\n  """"""\n  # with tf.op_scope([image, height, width, bbox], scope, \'distort_image\'):\n  # with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n  with tf.compat.v1.name_scope(scope or \'distort_image\'):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they\'re rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Display the bounding box in the first thread only.\n    if not thread_id:\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.compat.v1.summary.image(\n          \'image_with_bounding_boxes\', image_with_box)\n\n  # A large fraction of image datasets contain a human-annotated bounding\n  # box delineating the region of the image containing the object of interest.\n  # We choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an allowed\n  # range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        image_size=tf.shape(input=image),\n        bounding_boxes=bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=[0.99, 1.01],\n        area_range=[0.05, 1.0],\n        max_attempts=100,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n    if not thread_id:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distort_bbox)\n      tf.compat.v1.summary.image(\n          \'images_with_distorted_bounding_box\',\n          image_with_distorted_box)\n\n    # Crop the image to the specified bounding box.\n    distorted_image = tf.slice(image, bbox_begin, bbox_size)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n    resize_method = thread_id % 4\n    if cnn_util.tensorflow_version() >= 11:\n      distorted_image = tf.image.resize(\n          distorted_image, [height, width], resize_method)\n    else:\n      distorted_image = tf.image.resize(\n          distorted_image, height, width, resize_method)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\',\n          tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors.\n    distorted_image = distort_color(distorted_image, thread_id)\n\n    # Note: This ensures the scaling matches the output of eval_image\n    distorted_image *= 256\n\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'final_distorted_image\',\n          tf.expand_dims(distorted_image, 0))\n    return distorted_image\n\n\ndef distort_color(image, thread_id=0, scope=None):\n  """"""Distort the color of the image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: Tensor containing single image.\n    thread_id: preprocessing thread ID.\n    scope: Optional scope for op_scope.\n  Returns:\n    color-distorted image\n  """"""\n  # with tf.op_scope([image], scope, \'distort_color\'):\n  # with tf.name_scope(scope, \'distort_color\', [image]):\n  with tf.compat.v1.name_scope(scope or \'distort_color\'):\n    color_ordering = thread_id % 2\n\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n\nclass ImagePreprocessor(object):\n  """"""Preprocessor for input images.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               device_count,\n               dtype=tf.float32,\n               train=True,\n               distortions=None,\n               resize_method=None):\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.device_count = device_count\n    self.dtype = dtype\n    self.train = train\n    self.resize_method = resize_method\n    if distortions is None:\n      distortions = False\n    self.distortions = distortions\n    if self.batch_size % self.device_count != 0:\n      raise ValueError(\n          (\'batch_size must be a multiple of device_count: \'\n           \'batch_size %d, device_count: %d\') %\n          (self.batch_size, self.device_count))\n    self.batch_size_per_device = self.batch_size // self.device_count\n\n  def preprocess(self, image_buffer, bbox, thread_id):\n    """"""Preprocessing image_buffer using thread_id.""""""\n    # Note: Width and height of image is known only at runtime.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 dct_method=\'INTEGER_FAST\')\n    if self.train and self.distortions:\n      image = distort_image(image, self.height, self.width, bbox, thread_id)\n    else:\n      #image = eval_image(image, self.height, self.width, bbox, thread_id,\n      #                   self.resize_method)\n      image = vgg_preprocessing.preprocess_image(image,224,224,False)\n    # Note: image is now float32 [height,width,3] with range [0, 255]\n\n    # image = tf.cast(image, tf.uint8) # HACK TESTING\n\n    return image\n\n  def minibatch(self, dataset, subset):\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n      images = [[] for i in range(self.device_count)]\n      labels = [[] for i in range(self.device_count)]\n      record_input = data_flow_ops.RecordInput(\n          file_pattern=dataset.tf_record_pattern(subset),\n          seed=randint(0, 9000),\n          parallelism=64,\n          buffer_size=10000,\n          batch_size=self.batch_size,\n          name=\'record_input\')\n      records = record_input.get_yield_op()\n      records = tf.split(records, self.batch_size, 0)\n      records = [tf.reshape(record, []) for record in records]\n      for i in xrange(self.batch_size):\n        value = records[i]\n        image_buffer, label_index, bbox, _ = parse_example_proto(value)\n        image = self.preprocess(image_buffer, bbox, i % 4)\n\n        device_index = i % self.device_count\n        images[device_index].append(image)\n        labels[device_index].append(label_index)\n      label_index_batch = [None] * self.device_count\n      for device_index in xrange(self.device_count):\n        images[device_index] = tf.parallel_stack(images[device_index])\n        label_index_batch[device_index] = tf.concat(labels[device_index], 0)\n\n        # dynamic_pad=True) # HACK TESTING dynamic_pad=True\n        images[device_index] = tf.cast(images[device_index], self.dtype)\n        depth = 3\n        images[device_index] = tf.reshape(\n            images[device_index],\n            shape=[self.batch_size_per_device, self.height, self.width, depth])\n        label_index_batch[device_index] = tf.reshape(\n            label_index_batch[device_index], [self.batch_size_per_device])\n        # Display the training images in the visualizer.\n        # tf.summary.image(\'images\', images)\n\n      return images, label_index_batch\n'"
models/image_recognition/tensorflow/resnet101/int8/vgg_preprocessing.py,60,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(input=image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.cast(tf.stack([offset_height, offset_width, 0]), dtype=tf.int32)\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(input=image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(input=image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random.uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random.uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(input=image)[0]\n    image_width = tf.shape(input=image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(value=smallest_side, dtype=tf.int32)\n\n  height = tf.cast(height, dtype=tf.float32)\n  width = tf.cast(width, dtype=tf.float32)\n  smallest_side = tf.cast(smallest_side, dtype=tf.float32)\n\n  scale = tf.cond(pred=tf.greater(height, width),\n                  true_fn=lambda: smallest_side / width,\n                  false_fn=lambda: smallest_side / height)\n  new_height = tf.cast(tf.math.rint(height * scale), dtype=tf.int32)\n  new_width = tf.cast(tf.math.rint(width * scale), dtype=tf.int32)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(value=smallest_side, dtype=tf.int32)\n  #import pdb\n  #pdb.set_trace()\n  shape = tf.shape(input=image)\n\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize(image, [new_height, new_width],\n                                           method=tf.image.ResizeMethod.BILINEAR)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random.uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.cast(image, dtype=tf.float32)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.cast(image, dtype=tf.float32)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
models/image_recognition/tensorflow/resnet50/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/image_recognition/tensorflow/resnet50/inference/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport tensorflow as tf\n\nimport preprocessing\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\nIMAGENET_NUM_CLASSES = 1000\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, data_dir=None):\n    self.name = name\n    if data_dir is None:\n      raise ValueError(\'Data directory not specified\')\n    self.data_dir = data_dir\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @abstractmethod\n  def num_classes(self):\n    pass\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n\nclass ImagenetData(Dataset):\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'ImageNet\', data_dir)\n\n  def num_classes(self):\n    return IMAGENET_NUM_CLASSES\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    elif subset == \'calibrate\' or subset == \'calibration\':\n      return 100\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    return preprocessing.RecordInputImagePreprocessor\n'"
models/image_recognition/tensorflow/resnet50/inference/eval_image_classifier_inference.py,18,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport time\nfrom argparse import ArgumentParser\n\nimport tensorflow as tf\nfrom tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\nfrom tensorflow.python.framework import dtypes\n\nimport datasets\nimport numpy as np\n\nINPUTS = \'input\'\nOUTPUTS = \'predict\'\n\nRESNET_IMAGE_SIZE = 224\n\n\nclass eval_classifier_optimized_graph:\n  """"""Evaluate image classifier with optimized TensorFlow graph""""""\n\n  def __init__(self):\n\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--num-inter-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--num-intra-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-m\', ""--model-name"",\n                            help=\'Specify the model name to run benchmark for\',\n                            dest=\'model_name\')\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph for the transform tool\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n    arg_parser.add_argument(\'--calibrate\', dest=\'calibrate\',\n                            help=\'Run accuracy with calibration data,\'\n                                 \'to generate min_max ranges, calibrate=[True/False]\',\n                            type=bool, default=False)\n    arg_parser.add_argument(""--results-file-path"",\n                            help=""File path for the inference results"",\n                            dest=""results_file_path"", default=None)\n    arg_parser.add_argument(""--warmup-steps"", type=int, default=10,\n                            help=""number of warmup steps"")\n    arg_parser.add_argument(""--steps"", type=int, default=50,\n                            help=""number of steps"")\n\n    arg_parser.add_argument(\n      \'--data-num-inter-threads\', dest=\'data_num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=32)\n    arg_parser.add_argument(\n      \'--data-num-intra-threads\', dest=\'data_num_intra_threads\',\n      help=\'number threads for data layer operator\',\n      type=int, default=14)\n    arg_parser.add_argument(\n      \'--num-cores\', dest=\'num_cores\',\n      help=\'number of cores\',\n      type=int, default=28)\n\n    self.args = arg_parser.parse_args()\n    # validate the arguements\n    self.validate_args()\n\n  def write_results_output(self, predictions, filenames, labels):\n    # If a results_file_path is provided, write the predictions to the file\n    if self.args.results_file_path:\n      top_predictions = np.argmax(predictions, 1)\n      with open(self.args.results_file_path, ""a"") as fp:\n        for filename, expected_label, top_prediction in zip(filenames, labels, top_predictions):\n          fp.write(""{},{},{}\\n"".format(filename, expected_label, top_prediction))\n\n  def run(self):\n    """"""run benchmark with optimized graph""""""\n\n    print(""Run inference"")\n\n    data_config = tf.compat.v1.ConfigProto()\n    data_config.intra_op_parallelism_threads = self.args.data_num_intra_threads\n    data_config.inter_op_parallelism_threads = self.args.data_num_inter_threads\n    data_config.use_per_session_threads = 1\n\n    infer_config = tf.compat.v1.ConfigProto()\n    infer_config.intra_op_parallelism_threads = self.args.num_intra_threads\n    infer_config.inter_op_parallelism_threads = self.args.num_inter_threads\n    infer_config.use_per_session_threads = 1\n\n    data_graph = tf.Graph()\n    with data_graph.as_default():\n      if (self.args.data_location):\n        print(""Inference with real data."")\n        if self.args.calibrate:\n            subset = \'calibration\'\n        else:\n            subset = \'validation\'\n        dataset = datasets.ImagenetData(self.args.data_location)\n        preprocessor = dataset.get_image_preprocessor()(\n            RESNET_IMAGE_SIZE, RESNET_IMAGE_SIZE, self.args.batch_size,\n            num_cores=self.args.num_cores,\n            resize_method=\'crop\')\n\n        images, labels, filenames = preprocessor.minibatch(dataset, subset=subset)\n\n        # If a results file path is provided, then start the prediction output file\n        if self.args.results_file_path:\n          with open(self.args.results_file_path, ""w+"") as fp:\n            fp.write(""filename,actual,prediction\\n"")\n      else:\n        print(""Inference with dummy data."")\n        input_shape = [self.args.batch_size, RESNET_IMAGE_SIZE, RESNET_IMAGE_SIZE, 3]\n        images = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name=\'synthetic_images\')\n\n    infer_graph = tf.Graph()\n    with infer_graph.as_default():\n      graph_def = tf.compat.v1.GraphDef()\n      with tf.compat.v1.gfile.FastGFile(self.args.input_graph, \'rb\') as input_file:\n        input_graph_content = input_file.read()\n        graph_def.ParseFromString(input_graph_content)\n\n      output_graph = optimize_for_inference(graph_def, [INPUTS], \n                              [OUTPUTS], dtypes.float32.as_datatype_enum, False)\n      tf.import_graph_def(output_graph, name=\'\')\n\n    # Definite input and output Tensors for detection_graph\n    input_tensor = infer_graph.get_tensor_by_name(\'input:0\')\n    output_tensor = infer_graph.get_tensor_by_name(\'predict:0\')\n\n    data_sess = tf.compat.v1.Session(graph=data_graph,  config=data_config)\n    infer_sess = tf.compat.v1.Session(graph=infer_graph, config=infer_config)\n\n    num_processed_images = 0\n    num_remaining_images = dataset.num_examples_per_epoch(subset=subset) - num_processed_images \\\n        if self.args.data_location else (self.args.batch_size * self.args.steps)\n\n    if (not self.args.accuracy_only):\n      iteration = 0\n      warm_up_iteration = self.args.warmup_steps\n      total_run = self.args.steps\n      total_time = 0\n\n      while num_remaining_images >= self.args.batch_size and iteration < total_run:\n        iteration += 1\n        tf_filenames = None\n        np_labels = None\n        data_load_start = time.time()\n        if self.args.results_file_path:\n          image_np, np_labels, tf_filenames = data_sess.run([images, labels, filenames])\n        else:\n          image_np = data_sess.run(images)\n\n        data_load_time = time.time() - data_load_start\n\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        predictions = infer_sess.run(output_tensor, feed_dict={input_tensor: image_np})\n        time_consume = time.time() - start_time\n\n        # Write out the file name, expected label, and top prediction\n        self.write_results_output(predictions, tf_filenames, np_labels)\n\n        # only add data loading time for real data, not for dummy data\n        if self.args.data_location:\n          time_consume += data_load_time\n\n        print(\'Iteration %d: %.6f sec\' % (iteration, time_consume))\n        if iteration > warm_up_iteration:\n          total_time += time_consume\n\n      time_average = total_time / (iteration - warm_up_iteration)\n      print(\'Average time: %.6f sec\' % (time_average))\n\n      print(\'Batch size = %d\' % self.args.batch_size)\n      if (self.args.batch_size == 1):\n        print(\'Latency: %.3f ms\' % (time_average * 1000))\n      # print throughput for both batch size 1 and 128\n      print(\'Throughput: %.3f images/sec\' % (self.args.batch_size / time_average))\n\n    else: # accuracy check\n      total_accuracy1, total_accuracy5 = (0.0, 0.0)\n\n      while num_remaining_images >= self.args.batch_size:\n        # Reads and preprocess data\n        tf_filenames = None\n        if self.args.results_file_path:\n          np_images, np_labels, tf_filenames = data_sess.run([images, labels, filenames])\n        else:\n          np_images, np_labels = data_sess.run([images, labels])\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        # Compute inference on the preprocessed data\n        predictions = infer_sess.run(output_tensor,\n                               {input_tensor: np_images})\n        elapsed_time = time.time() - start_time\n\n        # Write out the file name, expected label, and top prediction\n        self.write_results_output(predictions, tf_filenames, np_labels)\n\n        with tf.Graph().as_default() as accu_graph:\n          accuracy1 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                   targets=tf.constant(np_labels), k=1), tf.float32))\n\n          accuracy5 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                   targets=tf.constant(np_labels), k=5), tf.float32))\n          with tf.compat.v1.Session() as accu_sess:\n            np_accuracy1, np_accuracy5 = accu_sess.run([accuracy1, accuracy5])\n\n          total_accuracy1 += np_accuracy1\n          total_accuracy5 += np_accuracy5\n\n        print(""Iteration time: %0.4f ms"" % elapsed_time)\n        print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n                  % (num_processed_images, total_accuracy1 / num_processed_images,\n                     total_accuracy5 / num_processed_images))\n\n  def validate_args(self):\n    """"""validate the arguments""""""\n\n    if not self.args.data_location:\n      if self.args.accuracy_only:\n        raise ValueError(""You must use real data for accuracy measurement."")\n\n\nif __name__ == ""__main__"":\n  evaluate_opt_graph = eval_classifier_optimized_graph()\n  evaluate_opt_graph.run()\n'"
models/image_recognition/tensorflow/resnet50/inference/preprocessing.py,35,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.platform import gfile\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                        default_value=\'\'),\n    \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                            default_value=-1),\n    \'image/filename\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                         default_value="""")\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n  filename = tf.cast(features[\'image/filename\'], dtype=tf.string)\n\n  return features[\'image/encoded\'], label, filename\n\n\ndef eval_image(image, height, width, resize_method,\n               central_fraction=0.875, scope=None):\n\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if resize_method == \'crop\':\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                      true_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256, 256 * shape[1] / shape[0]],\n                                                                          dtype=tf.int32)),\n                      false_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256 * shape[0] / shape[1], 256],\n                                                                          dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height,                                                     width)\n      distorted_image.set_shape([height, width, 3])\n      return distorted_image\n    else:  # bilinear\n      if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n      # Crop the central region of the image with an area containing 87.5% of\n      # the original image.\n      if central_fraction:\n        image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n      if height and width:\n        # Resize the image to the specified height and width.\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize(image, [height, width],\n                                         method=tf.image.ResizeMethod.BILINEAR)\n        image = tf.squeeze(image, [0])\n      image = tf.subtract(image, 0.5)\n      image = tf.multiply(image, 2.0)\n      return image\n\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               num_cores,\n               resize_method=""bilinear""):\n\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.num_cores = num_cores\n    self.resize_method = resize_method\n\n  def parse_and_preprocess(self, value):\n    # parse\n    image_buffer, label_index, filename = parse_example_proto(value)\n    # preprocess\n    image = tf.image.decode_jpeg(\n      image_buffer, channels=3, fancy_upscaling=False, dct_method=\'INTEGER_FAST\')\n    image = eval_image(image, self.height, self.width, self.resize_method)\n\n    return (image, label_index, filename)\n\n  def minibatch(self, dataset, subset, cache_data=False):\n\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n\n      glob_pattern = dataset.tf_record_pattern(subset)\n      file_names = gfile.Glob(glob_pattern)\n      if not file_names:\n        raise ValueError(\'Found no files in --data_dir matching: {}\'\n                         .format(glob_pattern))\n      ds = tf.data.TFRecordDataset.list_files(file_names)\n\n      ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=self.num_cores, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n\n      if cache_data:\n        ds = ds.take(1).cache().repeat()\n\n      ds = ds.prefetch(buffer_size=10000)\n      #ds = ds.prefetch(buffer_size=self.batch_size)\n\n      # num of parallel batches not greater than 56\n      max_num_parallel_batches = min(56, 2 * self.num_cores)\n      ds = ds.apply(\n        map_and_batch(\n          map_func=self.parse_and_preprocess,\n          batch_size=self.batch_size,\n          num_parallel_batches=max_num_parallel_batches,\n          num_parallel_calls=None))\n\n      ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n      ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n      images, labels, filename = ds_iterator.get_next()\n      # reshape\n      labels = tf.reshape(labels, [self.batch_size])\n      filename = tf.reshape(filename, [self.batch_size])\n\n      return images, labels, filename\n'"
models/image_recognition/tensorflow/resnet50/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/image_recognition/tensorflow/resnet50/int8/benchmark.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport time\n\nimport datasets\nimport tensorflow as tf\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""dataset location"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""predict"",\n                      help=""name of output layer"")\n  parser.add_argument(""--num_cores"", default=28,\n                      type=int, help=""number of physical cores"")\n  parser.add_argument(\n    \'--num_inter_threads\',\n    help=\'number threads across operators\',\n    type=int, default=1)\n  parser.add_argument(\n    \'--num_intra_threads\',\n    help=\'number threads for an operator\',\n    type=int, default=1)\n  parser.add_argument(\n    \'--data_num_inter_threads\',\n    help=\'number threads across data layer operators\',\n    type=int, default=16)\n  parser.add_argument(\n    \'--data_num_intra_threads\',\n    help=\'number threads for an data layer operator\',\n    type=int, default=14)\n  parser.add_argument(""--warmup_steps"", type=int, default=10,\n                      help=""number of warmup steps"")\n  parser.add_argument(""--steps"", type=int, default=50, help=""number of steps"")\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 224\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 224\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  warmup_steps = args.warmup_steps\n  steps = args.steps\n  assert steps > 10, ""Benchmark steps should be at least 10.""\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n\n  data_config = tf.compat.v1.ConfigProto()\n  data_config.intra_op_parallelism_threads = args.data_num_intra_threads\n  data_config.inter_op_parallelism_threads = args.data_num_inter_threads\n  data_config.use_per_session_threads = 1\n\n  infer_config = tf.compat.v1.ConfigProto()\n  infer_config.intra_op_parallelism_threads = num_intra_threads\n  infer_config.inter_op_parallelism_threads = num_inter_threads\n  infer_config.use_per_session_threads = 1\n\n  data_graph = tf.Graph()\n  with data_graph.as_default():\n    if args.data_location:\n      print(""inference with real data"")\n      # get the images from dataset\n      dataset = datasets.ImagenetData(args.data_location)\n      preprocessor = dataset.get_image_preprocessor(benchmark=True)(\n        input_height, input_width, batch_size,\n        num_cores=args.num_cores,\n        resize_method=\'crop\')\n      images = preprocessor.minibatch(dataset, subset=\'validation\')\n    else:\n      # synthetic images\n      print(""inference with dummy data"")\n      input_shape = [batch_size, input_height, input_width, 3]\n      images = tf.random.uniform(\n        input_shape, 0.0, 255.0, dtype=tf.float32, name=\'synthetic_images\')\n\n  infer_graph = tf.Graph()\n  with infer_graph.as_default():\n    graph_def = tf.compat.v1.GraphDef()\n    with open(model_file, ""rb"") as f:\n      graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name=\'\')\n\n  input_tensor = infer_graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = infer_graph.get_tensor_by_name(output_layer + "":0"")\n  tf.compat.v1.global_variables_initializer()\n\n  data_sess = tf.compat.v1.Session(graph=data_graph, config=data_config)\n  infer_sess = tf.compat.v1.Session(graph=infer_graph, config=infer_config)\n\n  print(""[Running warmup steps...]"")\n  step_total_time = 0\n  step_total_images = 0\n\n  for t in range(warmup_steps):\n    data_start_time = time.time()\n    image_data = data_sess.run(images)\n    data_load_time = time.time() - data_start_time\n\n    start_time = time.time()\n    infer_sess.run(output_tensor, {input_tensor: image_data})\n    elapsed_time = time.time() - start_time\n\n    # only count the data loading and processing time for real data\n    if args.data_location:\n      elapsed_time += data_load_time\n\n    step_total_time += elapsed_time\n    step_total_images += batch_size\n\n    if ((t + 1) % 10 == 0):\n      print(""steps = {0}, {1} images/sec""\n            """".format(t + 1, step_total_images / step_total_time))\n      step_total_time = 0\n      step_total_images = 0\n\n  print(""[Running benchmark steps...]"")\n  total_time = 0\n  total_images = 0\n\n  step_total_time = 0\n  step_total_images = 0\n\n  for t in range(steps):\n    try:\n      data_start_time = time.time()\n      image_data = data_sess.run(images)\n      data_load_time = time.time() - data_start_time\n\n      start_time = time.time()\n      infer_sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n\n      # only count the data loading and processing time for real data\n      if args.data_location:\n        elapsed_time += data_load_time\n\n      total_time += elapsed_time\n      total_images += batch_size\n\n      step_total_time += elapsed_time\n      step_total_images += batch_size\n\n      if ((t + 1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t + 1, step_total_images / step_total_time))\n        step_total_time = 0\n        step_total_images = 0\n\n    except tf.errors.OutOfRangeError:\n      print(""Running out of images from dataset."")\n      break\n\n  print(""Average throughput for batch size {0}: {1} images/sec"".format(batch_size, total_images / total_time))\n'"
models/image_recognition/tensorflow/resnet50/int8/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n\n'"
models/image_recognition/tensorflow/resnet50/int8/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, height=None, width=None, depth=None, data_dir=None,\n               queue_runner_required=False, num_classes=1000):\n    self.name = name\n    self.height = height\n    self.width = width\n    self.depth = depth or 3\n\n    self.data_dir = data_dir\n    self._queue_runner_required = queue_runner_required\n    self._num_classes = num_classes\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @num_classes.setter\n  def num_classes(self, val):\n    self._num_classes = val\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n  def get_image_preprocessor(self):\n    return None\n\n  def queue_runner_required(self):\n    return self._queue_runner_required\n\n  def use_synthetic_gpu_images(self):\n    return not self.data_dir\n\n\nclass ImagenetData(Dataset):\n  """"""Configuration for Imagenet dataset.""""""\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'imagenet\', 300, 300, data_dir=data_dir)\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    elif subset == \'calibrate\' or subset == \'calibration\':\n      return 100\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self, benchmark=False):\n    if benchmark:\n      import preprocessing_benchmark\n      return preprocessing_benchmark.RecordInputImagePreprocessor\n    else:\n      import preprocessing\n      return preprocessing.RecordInputImagePreprocessor\n\n'"
models/image_recognition/tensorflow/resnet50/int8/generate_calibration_data.py,14,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\nfrom collections import namedtuple\nfrom operator import attrgetter\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""predict"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 224\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 224\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n  dataset = datasets.ImagenetData(data_location)\n  preprocessor = preprocessing.ImagePreprocessor(\n      input_height, input_width, batch_size,\n      1, # device count\n      tf.float32, # data_type for input fed to the graph\n      train=False, # doing inference\n      resize_method=\'crop\')\n  images, labels, tf_records = preprocessor.minibatch(dataset, subset=\'train\')\n  graph = load_graph(model_file)\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n  \n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = dataset.num_examples_per_epoch(subset=\'train\') \\\n                            - num_processed_images\n\n  CALIBRATION_POOL_SIZE = 1000\n  CALIBRATION_SET_SIZE = 100\n  calibration_pool = []\n  ImageWithConfidence = namedtuple(\'ImageWithConfidence\',\n                                   [\'tf_record\', \'confidence\'])\n  current_pool_size = 0\n  with tf.compat.v1.Session() as sess:\n    sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n    while num_remaining_images >= batch_size:\n      # Reads and preprocess data\n      np_images, np_labels, serialized_images = sess.run(\n          [images[0], labels[0], tf_records])\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      # Compute inference on the preprocessed data\n      predictions = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      selected_img_indices = np.where(\n          predictions.argmax(axis=1) == np_labels)[0].tolist()\n      current_pool_size += len(selected_img_indices)\n      for indx in selected_img_indices:\n        calibration_pool.append(ImageWithConfidence(\n            serialized_images[indx], predictions[indx].max()))  \n\n      accuracy1 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=1), tf.float32))\n\n      accuracy5 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=5), tf.float32))\n      np_accuracy1, np_accuracy5 =  sess.run([accuracy1, accuracy5])\n      total_accuracy1 += np_accuracy1\n      total_accuracy5 += np_accuracy5\n      print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n          % (num_processed_images, total_accuracy1/num_processed_images,\n          total_accuracy5/num_processed_images))\n      if current_pool_size >= CALIBRATION_POOL_SIZE:\n        break\n\n  writer = tf.io.TFRecordWriter(\'calibration-1-of-1\')\n  calibration_pool = sorted(calibration_pool, \n                            key=attrgetter(\'confidence\'), reverse=True)\n  for i in range(CALIBRATION_SET_SIZE):\n    writer.write(calibration_pool[i].tf_record)\n  writer.close()\n'"
models/image_recognition/tensorflow/resnet50/int8/preprocessing.py,90,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom random import randint\n\nfrom tensorflow.python.ops import data_flow_ops\nimport cnn_util\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields:\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    text: Tensor tf.string containing the human-readable label.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n  """"""Decode a JPEG string into one 3-D float image Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n  # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n  with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 fancy_upscaling=False,\n                                 dct_method=\'INTEGER_FAST\')\n\n    # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n\n    return image\n\n\ndef eval_image(image, height, width, bbox, thread_id, resize):\n  """"""Get the image for model evaluation.""""""\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'original_image\', tf.expand_dims(image, 0))\n\n    if resize == \'crop\':\n      # Note: This is much slower than crop_to_bounding_box\n      #         It seems that the redundant pad step has huge overhead\n      # distorted_image = tf.image.resize_image_with_crop_or_pad(image,\n      #                                                         height, width)\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                        true_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256, 256*shape[1]/shape[0]], dtype=tf.int32)),\n                        false_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256*shape[0]/shape[1], 256], dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      \n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      #y0=tf.random_uniform([],minval=0,maxval=(shape[0] - height + 1), dtype=tf.int32)\n      #x0=tf.random_uniform([],minval=0,maxval=(shape[1] - width + 1), dtype=tf.int32)\n      ## distorted_image = tf.slice(image, [y0,x0,0], [height,width,3])\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height,\n                                                      width)\n    else:\n      sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n          image_size=tf.shape(input=image),\n          bounding_boxes=bbox,\n          min_object_covered=0.5,\n          aspect_ratio_range=[0.90, 1.10],\n          area_range=[0.10, 1.0],\n          max_attempts=100,\n          use_image_if_no_bounding_boxes=True)\n      bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n      # Crop the image to the specified bounding box.\n      distorted_image = tf.slice(image, bbox_begin, bbox_size)\n      resize_method = {\n          \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n          \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n          \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n          \'area\': tf.image.ResizeMethod.AREA\n      }[resize]\n      # This resizing operation may distort the images because the aspect\n      # ratio is not respected.\n      if cnn_util.tensorflow_version() >= 11:\n        distorted_image = tf.image.resize(\n            distorted_image, [height, width],\n            resize_method)\n      else:\n        distorted_image = tf.image.resize(\n            distorted_image, height, width, resize_method)\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\', tf.expand_dims(distorted_image, 0))\n    image = distorted_image\n  return image\n\n\ndef distort_image(image, height, width, bbox, thread_id=0, scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    image: 3-D float Tensor of image\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    thread_id: integer indicating the preprocessing thread.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training.\n  """"""\n  # with tf.op_scope([image, height, width, bbox], scope, \'distort_image\'):\n  # with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n  with tf.compat.v1.name_scope(scope or \'distort_image\'):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they\'re rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Display the bounding box in the first thread only.\n    if not thread_id:\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.compat.v1.summary.image(\n          \'image_with_bounding_boxes\', image_with_box)\n\n  # A large fraction of image datasets contain a human-annotated bounding\n  # box delineating the region of the image containing the object of interest.\n  # We choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an allowed\n  # range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        image_size=tf.shape(input=image),\n        bounding_boxes=bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=[0.99, 1.01],\n        area_range=[0.05, 1.0],\n        max_attempts=100,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n    if not thread_id:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distort_bbox)\n      tf.compat.v1.summary.image(\n          \'images_with_distorted_bounding_box\',\n          image_with_distorted_box)\n\n    # Crop the image to the specified bounding box.\n    distorted_image = tf.slice(image, bbox_begin, bbox_size)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n    resize_method = thread_id % 4\n    if cnn_util.tensorflow_version() >= 11:\n      distorted_image = tf.image.resize(\n          distorted_image, [height, width], resize_method)\n    else:\n      distorted_image = tf.image.resize(\n          distorted_image, height, width, resize_method)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\',\n          tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors.\n    distorted_image = distort_color(distorted_image, thread_id)\n\n    # Note: This ensures the scaling matches the output of eval_image\n    distorted_image *= 256\n\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'final_distorted_image\',\n          tf.expand_dims(distorted_image, 0))\n    return distorted_image\n\n\ndef distort_color(image, thread_id=0, scope=None):\n  """"""Distort the color of the image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: Tensor containing single image.\n    thread_id: preprocessing thread ID.\n    scope: Optional scope for op_scope.\n  Returns:\n    color-distorted image\n  """"""\n  # with tf.op_scope([image], scope, \'distort_color\'):\n  # with tf.name_scope(scope, \'distort_color\', [image]):\n  with tf.compat.v1.name_scope(scope or \'distort_color\'):\n    color_ordering = thread_id % 2\n\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n\nclass ImagePreprocessor(object):\n  """"""Preprocessor for input images.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               device_count,\n               dtype=tf.float32,\n               train=True,\n               distortions=None,\n               resize_method=None):\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.device_count = device_count\n    self.dtype = dtype\n    self.train = train\n    self.resize_method = resize_method\n    if distortions is None:\n      distortions = False\n    self.distortions = distortions\n    if self.batch_size % self.device_count != 0:\n      raise ValueError(\n          (\'batch_size must be a multiple of device_count: \'\n           \'batch_size %d, device_count: %d\') %\n          (self.batch_size, self.device_count))\n    self.batch_size_per_device = self.batch_size // self.device_count\n\n  def preprocess(self, image_buffer, bbox, thread_id):\n    """"""Preprocessing image_buffer using thread_id.""""""\n    # Note: Width and height of image is known only at runtime.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 dct_method=\'INTEGER_FAST\')\n    if self.train and self.distortions:\n      image = distort_image(image, self.height, self.width, bbox, thread_id)\n    else:\n      image = eval_image(image, self.height, self.width, bbox, thread_id,\n                         self.resize_method)\n    # Note: image is now float32 [height,width,3] with range [0, 255]\n\n    # image = tf.cast(image, tf.uint8) # HACK TESTING\n\n    return image\n\n  def minibatch(self, dataset, subset):\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n      images = [[] for i in range(self.device_count)]\n      labels = [[] for i in range(self.device_count)]\n      record_input = data_flow_ops.RecordInput(\n          file_pattern=dataset.tf_record_pattern(subset),\n          seed=randint(0, 9000),\n          parallelism=64,\n          buffer_size=10000,\n          batch_size=self.batch_size,\n          name=\'record_input\')\n      records = record_input.get_yield_op()\n      records = tf.split(records, self.batch_size, 0)\n      records = [tf.reshape(record, []) for record in records]\n      for i in xrange(self.batch_size):\n        value = records[i]\n        image_buffer, label_index, bbox, _ = parse_example_proto(value)\n        image = self.preprocess(image_buffer, bbox, i % 4)\n        device_index = i % self.device_count\n        images[device_index].append(image)\n        labels[device_index].append(label_index)\n      label_index_batch = [None] * self.device_count\n      for device_index in xrange(self.device_count):\n        images[device_index] = tf.parallel_stack(images[device_index])\n        label_index_batch[device_index] = tf.concat(labels[device_index], 0)\n\n        # dynamic_pad=True) # HACK TESTING dynamic_pad=True\n        images[device_index] = tf.cast(images[device_index], self.dtype)\n        depth = 3\n        images[device_index] = tf.reshape(\n            images[device_index],\n            shape=[self.batch_size_per_device, self.height, self.width, depth])\n        label_index_batch[device_index] = tf.reshape(\n            label_index_batch[device_index], [self.batch_size_per_device])\n        # Display the training images in the visualizer.\n        # tf.summary.image(\'images\', images)\n\n      return images, label_index_batch, records\n'"
models/image_recognition/tensorflow/resnet50/int8/preprocessing_benchmark.py,31,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.platform import gfile\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                        default_value=\'\'),\n    \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                            default_value=-1),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  return features[\'image/encoded\'], label\n\n\ndef eval_image(image, height, width, resize_method,\n               central_fraction=0.875, scope=None):\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if resize_method == \'crop\':\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                      true_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256, 256 * shape[1] / shape[0]],\n                                                                          dtype=tf.int32)),\n                      false_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256 * shape[0] / shape[1], 256],\n                                                                          dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height, width)\n      distorted_image.set_shape([height, width, 3])\n      return distorted_image\n    else:  # bilinear\n      if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n      # Crop the central region of the image with an area containing 87.5% of\n      # the original image.\n      if central_fraction:\n        image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n      if height and width:\n        # Resize the image to the specified height and width.\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize(image, [height, width],\n                                         method=tf.image.ResizeMethod.BILINEAR)\n        image = tf.squeeze(image, [0])\n      image = tf.subtract(image, 0.5)\n      image = tf.multiply(image, 2.0)\n      return image\n\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               num_cores,\n               resize_method):\n\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.num_cores = num_cores\n    self.resize_method = resize_method\n\n  def parse_and_preprocess(self, value):\n    # parse\n    image_buffer, label_index = parse_example_proto(value)\n    # preprocess\n    image = tf.image.decode_jpeg(\n      image_buffer, channels=3, fancy_upscaling=False, dct_method=\'INTEGER_FAST\')\n    image = eval_image(image, self.height, self.width, self.resize_method)\n\n    return (image, label_index)\n\n  def minibatch(self, dataset, subset, cache_data=False):\n\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n\n      glob_pattern = dataset.tf_record_pattern(subset)\n      file_names = gfile.Glob(glob_pattern)\n      if not file_names:\n        raise ValueError(\'Found no files in --data_dir matching: {}\'\n                         .format(glob_pattern))\n      ds = tf.data.TFRecordDataset.list_files(file_names)\n\n      ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=self.num_cores, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n\n      if cache_data:\n        ds = ds.take(1).cache().repeat()\n\n      ds = ds.prefetch(buffer_size=10000)\n      # ds = ds.prefetch(buffer_size=self.batch_size)\n\n      # num of parallel batches not greater than 56\n      max_num_parallel_batches = min(56, 2*self.num_cores)\n      ds = ds.apply(\n        map_and_batch(\n          map_func=self.parse_and_preprocess,\n          batch_size=self.batch_size,\n          num_parallel_batches=max_num_parallel_batches,\n          num_parallel_calls=None))  # this number should be tuned\n\n      ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # this number can be tuned\n\n      ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n      images, _ = ds_iterator.get_next()\n\n      return images\n'"
models/image_recognition/tensorflow/resnet50v1_5/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/inference/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport tensorflow as tf\n\nimport preprocessing\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\nIMAGENET_NUM_CLASSES = 1000\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, data_dir=None):\n    self.name = name\n    if data_dir is None:\n      raise ValueError(\'Data directory not specified\')\n    self.data_dir = data_dir\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @abstractmethod\n  def num_classes(self):\n    pass\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n\nclass ImagenetData(Dataset):\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'ImageNet\', data_dir)\n\n  def num_classes(self):\n    return IMAGENET_NUM_CLASSES\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    elif subset == \'calibrate\' or subset == \'calibration\':\n      return 100\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    return preprocessing.RecordInputImagePreprocessor\n'"
models/image_recognition/tensorflow/resnet50v1_5/inference/eval_image_classifier_inference.py,18,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport time\nfrom argparse import ArgumentParser\n\nimport tensorflow as tf\nfrom tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\nfrom tensorflow.python.framework import dtypes\n\nimport datasets\nimport numpy as np\n\nINPUTS = \'input_tensor\'\nOUTPUTS = \'softmax_tensor\'\n\nRESNET_IMAGE_SIZE = 224\n\n\nclass eval_classifier_optimized_graph:\n  """"""Evaluate image classifier with optimized TensorFlow graph""""""\n\n  def __init__(self):\n\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--num-inter-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--num-intra-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-m\', ""--model-name"",\n                            help=\'Specify the model name to run benchmark for\',\n                            dest=\'model_name\')\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph for the transform tool\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n    arg_parser.add_argument(\'--calibrate\', dest=\'calibrate\',\n                            help=\'Run accuracy with calibration data,\'\n                                 \'to generate min_max ranges, calibrate=[True/False]\',\n                            type=bool, default=False)\n    arg_parser.add_argument(""--results-file-path"",\n                            help=""File path for the inference results"",\n                            dest=""results_file_path"", default=None)\n    arg_parser.add_argument(""--warmup-steps"", type=int, default=10,\n                            help=""number of warmup steps"")\n    arg_parser.add_argument(""--steps"", type=int, default=50,\n                            help=""number of steps"")\n\n    arg_parser.add_argument(\n      \'--data-num-inter-threads\', dest=\'data_num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=32)\n    arg_parser.add_argument(\n      \'--data-num-intra-threads\', dest=\'data_num_intra_threads\',\n      help=\'number threads for data layer operator\',\n      type=int, default=14)\n    arg_parser.add_argument(\n      \'--num-cores\', dest=\'num_cores\',\n      help=\'number of cores\',\n      type=int, default=28)\n\n    self.args = arg_parser.parse_args()\n    # validate the arguements\n    self.validate_args()\n\n  def write_results_output(self, predictions, filenames, labels):\n    # If a results_file_path is provided, write the predictions to the file\n    if self.args.results_file_path:\n      top_predictions = np.argmax(predictions, 1)\n      with open(self.args.results_file_path, ""a"") as fp:\n        for filename, expected_label, top_prediction in zip(filenames, labels, top_predictions):\n          fp.write(""{},{},{}\\n"".format(filename, expected_label, top_prediction))\n\n  def run(self):\n    """"""run benchmark with optimized graph""""""\n\n    print(""Run inference"")\n\n    data_config = tf.compat.v1.ConfigProto()\n    data_config.intra_op_parallelism_threads = self.args.data_num_intra_threads\n    data_config.inter_op_parallelism_threads = self.args.data_num_inter_threads\n    data_config.use_per_session_threads = 1\n\n    infer_config = tf.compat.v1.ConfigProto()\n    infer_config.intra_op_parallelism_threads = self.args.num_intra_threads\n    infer_config.inter_op_parallelism_threads = self.args.num_inter_threads\n    infer_config.use_per_session_threads = 1\n\n    data_graph = tf.Graph()\n    with data_graph.as_default():\n      if (self.args.data_location):\n        print(""Inference with real data."")\n        if self.args.calibrate:\n            subset = \'calibration\'\n        else:\n            subset = \'validation\'\n        dataset = datasets.ImagenetData(self.args.data_location)\n        preprocessor = dataset.get_image_preprocessor()(\n            RESNET_IMAGE_SIZE, RESNET_IMAGE_SIZE, self.args.batch_size,\n            num_cores=self.args.num_cores,\n            resize_method=\'crop\')\n\n        images, labels, filenames = preprocessor.minibatch(dataset, subset=subset)\n\n        # If a results file path is provided, then start the prediction output file\n        if self.args.results_file_path:\n          with open(self.args.results_file_path, ""w+"") as fp:\n            fp.write(""filename,actual,prediction\\n"")\n      else:\n        print(""Inference with dummy data."")\n        input_shape = [self.args.batch_size, RESNET_IMAGE_SIZE, RESNET_IMAGE_SIZE, 3]\n        images = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name=\'synthetic_images\')\n\n    infer_graph = tf.Graph()\n    with infer_graph.as_default():\n      graph_def = tf.compat.v1.GraphDef()\n      with tf.compat.v1.gfile.FastGFile(self.args.input_graph, \'rb\') as input_file:\n        input_graph_content = input_file.read()\n        graph_def.ParseFromString(input_graph_content)\n\n      output_graph = optimize_for_inference(graph_def, [INPUTS], \n                              [OUTPUTS], dtypes.float32.as_datatype_enum, False)\n      tf.import_graph_def(output_graph, name=\'\')\n\n    # Definite input and output Tensors for detection_graph\n    input_tensor = infer_graph.get_tensor_by_name(\'input_tensor:0\')\n    output_tensor = infer_graph.get_tensor_by_name(\'softmax_tensor:0\')\n\n    data_sess  = tf.compat.v1.Session(graph=data_graph,  config=data_config)\n    infer_sess = tf.compat.v1.Session(graph=infer_graph, config=infer_config)\n\n    num_processed_images = 0\n    num_remaining_images = dataset.num_examples_per_epoch(subset=subset) - num_processed_images \\\n        if self.args.data_location else datasets.IMAGENET_NUM_VAL_IMAGES\n\n    if (not self.args.accuracy_only):\n      iteration = 0\n      warm_up_iteration = self.args.warmup_steps\n      total_run = self.args.steps\n      total_time = 0\n\n      while num_remaining_images >= self.args.batch_size and iteration < total_run:\n        iteration += 1\n        tf_filenames = None\n        np_labels = None\n        data_load_start = time.time()\n        if self.args.results_file_path:\n          image_np, np_labels, tf_filenames = data_sess.run([images, labels, filenames])\n        else:\n          image_np = data_sess.run(images)\n\n        data_load_time = time.time() - data_load_start\n\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        predictions = infer_sess.run(output_tensor, feed_dict={input_tensor: image_np})\n        time_consume = time.time() - start_time\n\n        # Write out the file name, expected label, and top prediction\n        self.write_results_output(predictions, tf_filenames, np_labels)\n\n        # only add data loading time for real data, not for dummy data\n        if self.args.data_location:\n          time_consume += data_load_time\n\n        print(\'Iteration %d: %.6f sec\' % (iteration, time_consume))\n        if iteration > warm_up_iteration:\n          total_time += time_consume\n\n      time_average = total_time / (iteration - warm_up_iteration)\n      print(\'Average time: %.6f sec\' % (time_average))\n\n      print(\'Batch size = %d\' % self.args.batch_size)\n      if (self.args.batch_size == 1):\n        print(\'Latency: %.3f ms\' % (time_average * 1000))\n      # print throughput for both batch size 1 and 128\n      print(\'Throughput: %.3f images/sec\' % (self.args.batch_size / time_average))\n\n    else: # accuracy check\n      total_accuracy1, total_accuracy5 = (0.0, 0.0)\n\n      while num_remaining_images >= self.args.batch_size:\n        # Reads and preprocess data\n        tf_filenames = None\n        if self.args.results_file_path:\n          np_images, np_labels, tf_filenames = data_sess.run([images, labels, filenames])\n        else:\n          np_images, np_labels = data_sess.run([images, labels])\n        num_processed_images += self.args.batch_size\n        num_remaining_images -= self.args.batch_size\n\n        start_time = time.time()\n        # Compute inference on the preprocessed data\n        predictions = infer_sess.run(output_tensor,\n                               {input_tensor: np_images})\n        elapsed_time = time.time() - start_time\n\n        # Write out the file name, expected label, and top prediction\n        self.write_results_output(predictions, tf_filenames, np_labels)\n\n        with tf.Graph().as_default() as accu_graph:\n          accuracy1 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                   targets=tf.constant(np_labels), k=1), tf.float32))\n\n          accuracy5 = tf.reduce_sum(\n            input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                   targets=tf.constant(np_labels), k=5), tf.float32))\n          with tf.compat.v1.Session() as accu_sess:\n            np_accuracy1, np_accuracy5 = accu_sess.run([accuracy1, accuracy5])\n\n          total_accuracy1 += np_accuracy1\n          total_accuracy5 += np_accuracy5\n\n        print(""Iteration time: %0.4f ms"" % elapsed_time)\n        print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n                  % (num_processed_images, total_accuracy1 / num_processed_images,\n                     total_accuracy5 / num_processed_images))\n\n  def validate_args(self):\n    """"""validate the arguments""""""\n\n    if not self.args.data_location:\n      if self.args.accuracy_only:\n        raise ValueError(""You must use real data for accuracy measurement."")\n\n\nif __name__ == ""__main__"":\n  evaluate_opt_graph = eval_classifier_optimized_graph()\n  evaluate_opt_graph.run()\n'"
models/image_recognition/tensorflow/resnet50v1_5/inference/preprocessing.py,36,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.platform import gfile\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                        default_value=\'\'),\n    \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                            default_value=-1),\n    \'image/filename\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                         default_value="""")\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n  filename = tf.cast(features[\'image/filename\'], dtype=tf.string)\n\n  return features[\'image/encoded\'], label, filename\n\n\ndef eval_image(image, height, width, resize_method,\n               central_fraction=0.875, scope=None):\n\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if resize_method == \'crop\':\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                      true_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256, 256 * shape[1] / shape[0]],\n                                                                          dtype=tf.int32)),\n                      false_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256 * shape[0] / shape[1], 256],\n                                                                          dtype=tf.int32)))\n\n      shape = tf.shape(input=image)\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height, width)\n      distorted_image.set_shape([height, width, 3])\n      means = tf.broadcast_to([123.68, 116.78, 103.94], tf.shape(input=distorted_image))\n      return distorted_image - means\n    else:  # bilinear\n      if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n      # Crop the central region of the image with an area containing 87.5% of\n      # the original image.\n      if central_fraction:\n        image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n      if height and width:\n        # Resize the image to the specified height and width.\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize(image, [height, width],\n                                         method=tf.image.ResizeMethod.BILINEAR)\n        image = tf.squeeze(image, [0])\n      image = tf.subtract(image, 0.5)\n      image = tf.multiply(image, 2.0)\n      return image\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               num_cores,\n               resize_method=""bilinear""):\n\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.num_cores = num_cores\n    self.resize_method = resize_method\n\n  def parse_and_preprocess(self, value):\n    # parse\n    image_buffer, label_index, filename = parse_example_proto(value)\n    # preprocess\n    image = tf.image.decode_jpeg(\n      image_buffer, channels=3, fancy_upscaling=False, dct_method=\'INTEGER_FAST\')\n    image = eval_image(image, self.height, self.width, self.resize_method)\n    return (image, label_index, filename)\n\n  def minibatch(self, dataset, subset, cache_data=False):\n\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n\n      glob_pattern = dataset.tf_record_pattern(subset)\n      file_names = gfile.Glob(glob_pattern)\n      if not file_names:\n        raise ValueError(\'Found no files in --data_dir matching: {}\'\n                         .format(glob_pattern))\n      ds = tf.data.TFRecordDataset.list_files(file_names)\n\n      ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=self.num_cores, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n\n      if cache_data:\n        ds = ds.take(1).cache().repeat()\n\n      ds = ds.prefetch(buffer_size=10000)\n      #ds = ds.prefetch(buffer_size=self.batch_size)\n\n      # num of parallel batches not greater than 56\n      max_num_parallel_batches = min(56, 2 * self.num_cores)\n      ds = ds.apply(\n        map_and_batch(\n          map_func=self.parse_and_preprocess,\n          batch_size=self.batch_size,\n          num_parallel_batches=max_num_parallel_batches,\n          num_parallel_calls=None))\n\n      ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n      ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n      images, labels, filename = ds_iterator.get_next()\n      # reshape\n      labels = tf.reshape(labels, [self.batch_size])\n      filename = tf.reshape(filename, [self.batch_size])\n\n      return images, labels, filename\n'"
models/image_recognition/tensorflow/resnet50v1_5/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/int8/benchmark.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport time\n\nimport datasets\nimport tensorflow as tf\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""dataset location"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""predict"",\n                      help=""name of output layer"")\n  parser.add_argument(""--num_cores"", default=28,\n                      type=int, help=""number of physical cores"")\n  parser.add_argument(\n    \'--num_inter_threads\',\n    help=\'number threads across operators\',\n    type=int, default=1)\n  parser.add_argument(\n    \'--num_intra_threads\',\n    help=\'number threads for an operator\',\n    type=int, default=1)\n  parser.add_argument(\n    \'--data_num_inter_threads\',\n    help=\'number threads across data layer operators\',\n    type=int, default=16)\n  parser.add_argument(\n    \'--data_num_intra_threads\',\n    help=\'number threads for an data layer operator\',\n    type=int, default=14)\n  parser.add_argument(""--warmup_steps"", type=int, default=10,\n                      help=""number of warmup steps"")\n  parser.add_argument(""--steps"", type=int, default=50, help=""number of steps"")\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 224\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 224\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  warmup_steps = args.warmup_steps\n  steps = args.steps\n  assert steps > 10, ""Benchmark steps should be at least 10.""\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n\n  data_config = tf.compat.v1.ConfigProto()\n  data_config.intra_op_parallelism_threads = args.data_num_intra_threads\n  data_config.inter_op_parallelism_threads = args.data_num_inter_threads\n  data_config.use_per_session_threads = 1\n\n  infer_config = tf.compat.v1.ConfigProto()\n  infer_config.intra_op_parallelism_threads = num_intra_threads\n  infer_config.inter_op_parallelism_threads = num_inter_threads\n  infer_config.use_per_session_threads = 1\n\n  data_graph = tf.Graph()\n  with data_graph.as_default():\n    if args.data_location:\n      print(""inference with real data"")\n      # get the images from dataset\n      dataset = datasets.ImagenetData(args.data_location)\n      preprocessor = dataset.get_image_preprocessor(benchmark=True)(\n        input_height, input_width, batch_size,\n        num_cores=args.num_cores,\n        resize_method=\'crop\')\n      images = preprocessor.minibatch(dataset, subset=\'validation\')\n    else:\n      # synthetic images\n      print(""inference with dummy data"")\n      input_shape = [batch_size, input_height, input_width, 3]\n      images = tf.random.uniform(\n        input_shape, 0.0, 255.0, dtype=tf.float32, name=\'synthetic_images\')\n\n  infer_graph = tf.Graph()\n  with infer_graph.as_default():\n    graph_def = tf.compat.v1.GraphDef()\n    with open(model_file, ""rb"") as f:\n      graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name=\'\')\n\n  input_tensor = infer_graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = infer_graph.get_tensor_by_name(output_layer + "":0"")\n  tf.compat.v1.global_variables_initializer()\n\n  data_sess = tf.compat.v1.Session(graph=data_graph, config=data_config)\n  infer_sess = tf.compat.v1.Session(graph=infer_graph, config=infer_config)\n\n  print(""[Running warmup steps...]"")\n  step_total_time = 0\n  step_total_images = 0\n\n  for t in range(warmup_steps):\n    data_start_time = time.time()\n    image_data = data_sess.run(images)\n    data_load_time = time.time() - data_start_time\n\n    start_time = time.time()\n    infer_sess.run(output_tensor, {input_tensor: image_data})\n    elapsed_time = time.time() - start_time\n\n    # only count the data loading and processing time for real data\n    if args.data_location:\n      elapsed_time += data_load_time\n\n    step_total_time += elapsed_time\n    step_total_images += batch_size\n\n    if ((t + 1) % 10 == 0):\n      print(""steps = {0}, {1} images/sec""\n            """".format(t + 1, step_total_images / step_total_time))\n      step_total_time = 0\n      step_total_images = 0\n\n  print(""[Running benchmark steps...]"")\n  total_time = 0\n  total_images = 0\n\n  step_total_time = 0\n  step_total_images = 0\n\n  for t in range(steps):\n    try:\n      data_start_time = time.time()\n      image_data = data_sess.run(images)\n      data_load_time = time.time() - data_start_time\n\n      start_time = time.time()\n      infer_sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n\n      # only count the data loading and processing time for real data\n      if args.data_location:\n        elapsed_time += data_load_time\n\n      total_time += elapsed_time\n      total_images += batch_size\n\n      step_total_time += elapsed_time\n      step_total_images += batch_size\n\n      if ((t + 1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t + 1, step_total_images / step_total_time))\n        step_total_time = 0\n        step_total_images = 0\n\n    except tf.errors.OutOfRangeError:\n      print(""Running out of images from dataset."")\n      break\n\n  print(""Average throughput for batch size {0}: {1} images/sec"".format(batch_size, total_images / total_time))\n'"
models/image_recognition/tensorflow/resnet50v1_5/int8/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/int8/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, height=None, width=None, depth=None, data_dir=None,\n               queue_runner_required=False, num_classes=1000):\n    self.name = name\n    self.height = height\n    self.width = width\n    self.depth = depth or 3\n\n    self.data_dir = data_dir\n    self._queue_runner_required = queue_runner_required\n    self._num_classes = num_classes\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @num_classes.setter\n  def num_classes(self, val):\n    self._num_classes = val\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n  def get_image_preprocessor(self):\n    return None\n\n  def queue_runner_required(self):\n    return self._queue_runner_required\n\n  def use_synthetic_gpu_images(self):\n    return not self.data_dir\n\n\nclass ImagenetData(Dataset):\n  """"""Configuration for Imagenet dataset.""""""\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'imagenet\', 300, 300, data_dir=data_dir)\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    elif subset == \'calibrate\' or subset == \'calibration\':\n      return 100\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self, benchmark=False):\n    if benchmark:\n      import preprocessing_benchmark\n      return preprocessing_benchmark.RecordInputImagePreprocessor\n    else:\n      import preprocessing\n      return preprocessing.RecordInputImagePreprocessor\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/int8/generate_calibration_data.py,14,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\nfrom collections import namedtuple\nfrom operator import attrgetter\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""predict"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  if args.input_height:\n    input_height = args.input_height\n  else:\n    input_height = 224\n  if args.input_width:\n    input_width = args.input_width\n  else:\n    input_width = 224\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n  dataset = datasets.ImagenetData(data_location)\n  preprocessor = preprocessing.ImagePreprocessor(\n      input_height, input_width, batch_size,\n      1, # device count\n      tf.float32, # data_type for input fed to the graph\n      train=False, # doing inference\n      resize_method=\'crop\')\n  images, labels, tf_records = preprocessor.minibatch(dataset, subset=\'train\')\n  graph = load_graph(model_file)\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n  \n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = dataset.num_examples_per_epoch(subset=\'train\') \\\n                            - num_processed_images\n\n  CALIBRATION_POOL_SIZE = 1000\n  CALIBRATION_SET_SIZE = 100\n  calibration_pool = []\n  ImageWithConfidence = namedtuple(\'ImageWithConfidence\',\n                                   [\'tf_record\', \'confidence\'])\n  current_pool_size = 0\n  with tf.compat.v1.Session() as sess:\n    sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n    while num_remaining_images >= batch_size:\n      # Reads and preprocess data\n      np_images, np_labels, serialized_images = sess.run(\n          [images[0], labels[0], tf_records])\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      # Compute inference on the preprocessed data\n      predictions = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      selected_img_indices = np.where(\n          predictions.argmax(axis=1) == np_labels)[0].tolist()\n      current_pool_size += len(selected_img_indices)\n      for indx in selected_img_indices:\n        calibration_pool.append(ImageWithConfidence(\n            serialized_images[indx], predictions[indx].max()))  \n\n      accuracy1 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=1), tf.float32))\n\n      accuracy5 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=5), tf.float32))\n      np_accuracy1, np_accuracy5 =  sess.run([accuracy1, accuracy5])\n      total_accuracy1 += np_accuracy1\n      total_accuracy5 += np_accuracy5\n      print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n          % (num_processed_images, total_accuracy1/num_processed_images,\n          total_accuracy5/num_processed_images))\n      if current_pool_size >= CALIBRATION_POOL_SIZE:\n        break\n\n  writer = tf.io.TFRecordWriter(\'calibration-1-of-1\')\n  calibration_pool = sorted(calibration_pool, \n                            key=attrgetter(\'confidence\'), reverse=True)\n  for i in range(CALIBRATION_SET_SIZE):\n    writer.write(calibration_pool[i].tf_record)\n  writer.close()\n'"
models/image_recognition/tensorflow/resnet50v1_5/int8/preprocessing.py,90,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom random import randint\n\nfrom tensorflow.python.ops import data_flow_ops\nimport cnn_util\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields:\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    text: Tensor tf.string containing the human-readable label.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n  """"""Decode a JPEG string into one 3-D float image Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n  # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n  with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 fancy_upscaling=False,\n                                 dct_method=\'INTEGER_FAST\')\n\n    # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n\n    return image\n\n\ndef eval_image(image, height, width, bbox, thread_id, resize):\n  """"""Get the image for model evaluation.""""""\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'original_image\', tf.expand_dims(image, 0))\n\n    if resize == \'crop\':\n      # Note: This is much slower than crop_to_bounding_box\n      #         It seems that the redundant pad step has huge overhead\n      # distorted_image = tf.image.resize_image_with_crop_or_pad(image,\n      #                                                         height, width)\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                        true_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256, 256*shape[1]/shape[0]], dtype=tf.int32)),\n                        false_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256*shape[0]/shape[1], 256], dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      \n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      #y0=tf.random_uniform([],minval=0,maxval=(shape[0] - height + 1), dtype=tf.int32)\n      #x0=tf.random_uniform([],minval=0,maxval=(shape[1] - width + 1), dtype=tf.int32)\n      ## distorted_image = tf.slice(image, [y0,x0,0], [height,width,3])\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height,\n                                                      width)\n    else:\n      sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n          image_size=tf.shape(input=image),\n          bounding_boxes=bbox,\n          min_object_covered=0.5,\n          aspect_ratio_range=[0.90, 1.10],\n          area_range=[0.10, 1.0],\n          max_attempts=100,\n          use_image_if_no_bounding_boxes=True)\n      bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n      # Crop the image to the specified bounding box.\n      distorted_image = tf.slice(image, bbox_begin, bbox_size)\n      resize_method = {\n          \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n          \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n          \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n          \'area\': tf.image.ResizeMethod.AREA\n      }[resize]\n      # This resizing operation may distort the images because the aspect\n      # ratio is not respected.\n      if cnn_util.tensorflow_version() >= 11:\n        distorted_image = tf.image.resize(\n            distorted_image, [height, width],\n            resize_method)\n      else:\n        distorted_image = tf.image.resize(\n            distorted_image, height, width, resize_method)\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\', tf.expand_dims(distorted_image, 0))\n    image = distorted_image\n  return image\n\n\ndef distort_image(image, height, width, bbox, thread_id=0, scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    image: 3-D float Tensor of image\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    thread_id: integer indicating the preprocessing thread.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training.\n  """"""\n  # with tf.op_scope([image, height, width, bbox], scope, \'distort_image\'):\n  # with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n  with tf.compat.v1.name_scope(scope or \'distort_image\'):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they\'re rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Display the bounding box in the first thread only.\n    if not thread_id:\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.compat.v1.summary.image(\n          \'image_with_bounding_boxes\', image_with_box)\n\n  # A large fraction of image datasets contain a human-annotated bounding\n  # box delineating the region of the image containing the object of interest.\n  # We choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an allowed\n  # range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        image_size=tf.shape(input=image),\n        bounding_boxes=bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=[0.99, 1.01],\n        area_range=[0.05, 1.0],\n        max_attempts=100,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n    if not thread_id:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distort_bbox)\n      tf.compat.v1.summary.image(\n          \'images_with_distorted_bounding_box\',\n          image_with_distorted_box)\n\n    # Crop the image to the specified bounding box.\n    distorted_image = tf.slice(image, bbox_begin, bbox_size)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n    resize_method = thread_id % 4\n    if cnn_util.tensorflow_version() >= 11:\n      distorted_image = tf.image.resize(\n          distorted_image, [height, width], resize_method)\n    else:\n      distorted_image = tf.image.resize(\n          distorted_image, height, width, resize_method)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\',\n          tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors.\n    distorted_image = distort_color(distorted_image, thread_id)\n\n    # Note: This ensures the scaling matches the output of eval_image\n    distorted_image *= 256\n\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'final_distorted_image\',\n          tf.expand_dims(distorted_image, 0))\n    return distorted_image\n\n\ndef distort_color(image, thread_id=0, scope=None):\n  """"""Distort the color of the image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: Tensor containing single image.\n    thread_id: preprocessing thread ID.\n    scope: Optional scope for op_scope.\n  Returns:\n    color-distorted image\n  """"""\n  # with tf.op_scope([image], scope, \'distort_color\'):\n  # with tf.name_scope(scope, \'distort_color\', [image]):\n  with tf.compat.v1.name_scope(scope or \'distort_color\'):\n    color_ordering = thread_id % 2\n\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n\nclass ImagePreprocessor(object):\n  """"""Preprocessor for input images.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               device_count,\n               dtype=tf.float32,\n               train=True,\n               distortions=None,\n               resize_method=None):\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.device_count = device_count\n    self.dtype = dtype\n    self.train = train\n    self.resize_method = resize_method\n    if distortions is None:\n      distortions = False\n    self.distortions = distortions\n    if self.batch_size % self.device_count != 0:\n      raise ValueError(\n          (\'batch_size must be a multiple of device_count: \'\n           \'batch_size %d, device_count: %d\') %\n          (self.batch_size, self.device_count))\n    self.batch_size_per_device = self.batch_size // self.device_count\n\n  def preprocess(self, image_buffer, bbox, thread_id):\n    """"""Preprocessing image_buffer using thread_id.""""""\n    # Note: Width and height of image is known only at runtime.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 dct_method=\'INTEGER_FAST\')\n    if self.train and self.distortions:\n      image = distort_image(image, self.height, self.width, bbox, thread_id)\n    else:\n      image = eval_image(image, self.height, self.width, bbox, thread_id,\n                         self.resize_method)\n    # Note: image is now float32 [height,width,3] with range [0, 255]\n\n    # image = tf.cast(image, tf.uint8) # HACK TESTING\n\n    return image\n\n  def minibatch(self, dataset, subset):\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n      images = [[] for i in range(self.device_count)]\n      labels = [[] for i in range(self.device_count)]\n      record_input = data_flow_ops.RecordInput(\n          file_pattern=dataset.tf_record_pattern(subset),\n          seed=randint(0, 9000),\n          parallelism=64,\n          buffer_size=10000,\n          batch_size=self.batch_size,\n          name=\'record_input\')\n      records = record_input.get_yield_op()\n      records = tf.split(records, self.batch_size, 0)\n      records = [tf.reshape(record, []) for record in records]\n      for i in xrange(self.batch_size):\n        value = records[i]\n        image_buffer, label_index, bbox, _ = parse_example_proto(value)\n        image = self.preprocess(image_buffer, bbox, i % 4)\n        device_index = i % self.device_count\n        images[device_index].append(image)\n        labels[device_index].append(label_index)\n      label_index_batch = [None] * self.device_count\n      for device_index in xrange(self.device_count):\n        images[device_index] = tf.parallel_stack(images[device_index])\n        label_index_batch[device_index] = tf.concat(labels[device_index], 0)\n\n        # dynamic_pad=True) # HACK TESTING dynamic_pad=True\n        images[device_index] = tf.cast(images[device_index], self.dtype)\n        depth = 3\n        images[device_index] = tf.reshape(\n            images[device_index],\n            shape=[self.batch_size_per_device, self.height, self.width, depth])\n        label_index_batch[device_index] = tf.reshape(\n            label_index_batch[device_index], [self.batch_size_per_device])\n        # Display the training images in the visualizer.\n        # tf.summary.image(\'images\', images)\n\n      return images, label_index_batch, records\n'"
models/image_recognition/tensorflow/resnet50v1_5/int8/preprocessing_benchmark.py,32,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.platform import gfile\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                        default_value=\'\'),\n    \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                            default_value=-1),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  return features[\'image/encoded\'], label\n\n\ndef eval_image(image, height, width, resize_method,\n               central_fraction=0.875, scope=None):\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if resize_method == \'crop\':\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                      true_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256, 256 * shape[1] / shape[0]],\n                                                                          dtype=tf.int32)),\n                      false_fn=lambda: tf.image.resize(image,\n                                                     tf.convert_to_tensor(value=[256 * shape[0] / shape[1], 256],\n                                                                          dtype=tf.int32)))\n      shape = tf.shape(input=image)\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height, width)\n      distorted_image.set_shape([height, width, 3])\n      means = tf.broadcast_to([123.68, 116.78, 103.94], tf.shape(input=distorted_image))\n      return distorted_image - means\n    else:  # bilinear\n      if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n      # Crop the central region of the image with an area containing 87.5% of\n      # the original image.\n      if central_fraction:\n        image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n      if height and width:\n        # Resize the image to the specified height and width.\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize(image, [height, width],\n                                         method=tf.image.ResizeMethod.BILINEAR)\n        image = tf.squeeze(image, [0])\n      image = tf.subtract(image, 0.5)\n      image = tf.multiply(image, 2.0)\n      return image\n\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               num_cores,\n               resize_method):\n\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.num_cores = num_cores\n    self.resize_method = resize_method\n\n  def parse_and_preprocess(self, value):\n    # parse\n    image_buffer, label_index = parse_example_proto(value)\n    # preprocess\n    image = tf.image.decode_jpeg(\n      image_buffer, channels=3, fancy_upscaling=False, dct_method=\'INTEGER_FAST\')\n    image = eval_image(image, self.height, self.width, self.resize_method)\n\n    return (image, label_index)\n\n  def minibatch(self, dataset, subset, cache_data=False):\n\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n\n      glob_pattern = dataset.tf_record_pattern(subset)\n      file_names = gfile.Glob(glob_pattern)\n      if not file_names:\n        raise ValueError(\'Found no files in --data_dir matching: {}\'\n                         .format(glob_pattern))\n      ds = tf.data.TFRecordDataset.list_files(file_names)\n\n      ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=self.num_cores, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n\n      if cache_data:\n        ds = ds.take(1).cache().repeat()\n\n      ds = ds.prefetch(buffer_size=10000)\n      # ds = ds.prefetch(buffer_size=self.batch_size)\n\n      # num of parallel batches not greater than 56\n      max_num_parallel_batches = min(56, 2*self.num_cores)\n      ds = ds.apply(\n        map_and_batch(\n          map_func=self.parse_and_preprocess,\n          batch_size=self.batch_size,\n          num_parallel_batches=max_num_parallel_batches,\n          num_parallel_calls=None))  # this number should be tuned\n\n      ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # this number can be tuned\n\n      ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n      images, _ = ds_iterator.get_next()\n\n      return images\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/__init__.py,0,b''
models/language_translation/tensorflow/mlperf_gnmt/fp32/__init__.py,0,b''
models/language_translation/tensorflow/mlperf_gnmt/fp32/bleu.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n# Modifications copyright (C) 2019 MLPerf Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Python implementation of BLEU, smooth-BLEU and Running BLEU.\n\n@note The most common usage case is to invoke the function compute_bleu\n\nThis module provides a Python implementation of BLEU and smooth-BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\nChin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\nevaluation metrics for machine translation. COLING 2004.\n""""""\n\nimport collections\nimport math\n\n##\n# @brief Class to compute running BLEU scores\n# @detail BLEU scores can be computed in a non-linear way,\n# or without having access to the full translated corpus in time.\nclass RunningBLEUScorer:\n\n  def __init__(self, max_order=4, smooth=False):\n    self.max_order = max_order\n    self.smooth = smooth\n    self.reset()\n\n  ##\n  # @brief Reset all variables (none of the previus sentences will be taken into account)\n  def reset(self):\n    self.matches_by_order = [0] * self.max_order\n    self.possible_matches_by_order = [0] * self.max_order\n    self.reference_length = 0\n    self.translation_length = 0\n\n  ##\n  # @brief Add a single sentence\n  # @param reference list of words for a reference sentence\n  # @param translation list of words for its corresponding translated sentence\n  # @post Updates internal structures to take this sentence\'s translation\n  # result into account in final BLEU score\n  def add_sentence(self, reference, translation):\n    self.add_sentence_with_multiple_refs([reference], translation)\n\n  ##\n  # @brief Add a single reference, with potentially multiple references\n  # @param reference list of list of words for a reference sentence\n  # @note That we could have multiple sentences serving as a reference\n  # @param translation (single) list of words for its corresponding translated sentence\n  # @post Updates internal structures to take this sentence\'s translation\n  # result into account in final BLEU score\n  def add_sentence_with_multiple_refs(self, references, translation):\n    self.reference_length += min(len(r) for r in references)\n    self.translation_length += len(translation)\n\n    merged_ref_ngram_counts = collections.Counter()\n    for reference in references:\n      merged_ref_ngram_counts |= self._get_ngrams(reference)\n\n    translation_ngram_counts = self._get_ngrams(translation)\n\n    new_matches_by_order, new_possible_matches_by_order = self._get_ngram_match_values(merged_ref_ngram_counts, translation_ngram_counts, len(translation))\n\n    for i in range(self.max_order):\n      self.matches_by_order[i] += new_matches_by_order[i]\n      self.possible_matches_by_order[i] += new_possible_matches_by_order[i]\n\n  ##\n  # @brief Calculate final BLEU score\n  def calc_BLEU_score(self):\n    precisions = [0] * self.max_order\n    for i in range(0, self.max_order):\n      if self.smooth:\n        precisions[i] = ((self.matches_by_order[i] + 1.) /\n                         (self.possible_matches_by_order[i] + 1.))\n      else:\n        if self.possible_matches_by_order[i] > 0:\n          precisions[i] = (float(self.matches_by_order[i]) /\n                           self.possible_matches_by_order[i])\n        else:\n          precisions[i] = 0.0\n\n    if min(precisions) > 0:\n      p_log_sum = sum((1. / self.max_order) * math.log(p) for p in precisions)\n      geo_mean = math.exp(p_log_sum)\n    else:\n      geo_mean = 0\n\n    ratio = float(self.translation_length) / self.reference_length\n\n    if ratio > 1.0:\n      bp = 1.\n    else:\n      bp = math.exp(1 - 1. / ratio)\n\n    bleu = geo_mean * bp\n\n    return (bleu, precisions, bp, ratio, self.translation_length, self.reference_length)\n\n  ##\n  # @brief Internal function to compute matching percentages for different order ngrams\n  def _get_ngram_match_values(self, ref_ngram_counts, translation_ngram_counts, translation_length):\n    new_matches_by_order = [0] * self.max_order\n    new_possible_matches_by_order = [0] * self.max_order\n\n    overlap = translation_ngram_counts & ref_ngram_counts\n    for ngram in overlap:\n      new_matches_by_order[len(ngram)-1] += overlap[ngram]\n    for order in range(1, self.max_order+1):\n      possible_matches = translation_length - order + 1\n      new_possible_matches_by_order[order-1] = max(0, possible_matches)\n\n    return (new_matches_by_order, new_possible_matches_by_order)\n\n  def _get_ngrams(self, segment):\n    """"""Internal function to extract all n-grams upto a given maximum order from an input segment.\n\n    Args:\n      segment: text segment from which n-grams will be extracted.\n\n    Returns:\n      The Counter containing all n-grams upto max_order in segment\n      with a count of how many times each n-gram occurred.\n    """"""\n    ngram_counts = collections.Counter()\n    for order in range(1, self.max_order + 1):\n      for i in range(0, len(segment) - order + 1):\n        ngram = tuple(segment[i:i+order])\n        ngram_counts[ngram] += 1\n    return ngram_counts\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n  """"""Computes BLEU score of translated segments against one or more references.\n    This is the most common usage when calculating BLEU scores.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n        reference_corpus[i][j][k] represents the k\'th word of the i\'th sentence\n        for the j\'th reference text\n    translation_corpus: list of translated sentences to score. Each sentence\n        should be tokenized into a list of tokens.\n        translation_corpus[i][j] represents the j\'th word for the i\'th sentence\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  """"""\n  runningBLEU = RunningBLEUScorer(max_order=max_order, smooth=smooth)\n\n\n  for (references, translation) in zip(reference_corpus,\n                                       translation_corpus):\n    runningBLEU.add_sentence_with_multiple_refs(references, translation)\n\n  return runningBLEU.calc_BLEU_score()'"
models/language_translation/tensorflow/mlperf_gnmt/fp32/evaluation_utils.py,8,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility for evaluating various tasks, e.g., translation & summarization.""""""\nimport codecs\nimport os\nimport re\nimport subprocess\nimport shlex\n\nimport tensorflow as tf\n\nimport bleu\nimport rouge\n\n\n__all__ = [""evaluate""]\n\n\ndef evaluate(ref_file, trans_file, metric, subword_option=None):\n  """"""Pick a metric and evaluate depending on task.""""""\n  # BLEU scores for translation task\n  if metric.lower() == ""bleu"":\n    evaluation_score = _bleu(ref_file, trans_file,\n                             subword_option=subword_option)\n  # ROUGE scores for summarization tasks\n  elif metric.lower() == ""rouge"":\n    evaluation_score = _rouge(ref_file, trans_file,\n                              subword_option=subword_option)\n  elif metric.lower() == ""accuracy"":\n    evaluation_score = _accuracy(ref_file, trans_file)\n  elif metric.lower() == ""word_accuracy"":\n    evaluation_score = _word_accuracy(ref_file, trans_file)\n  else:\n    raise ValueError(""Unknown metric %s"" % metric)\n\n  return evaluation_score\n\n\ndef _clean(sentence, subword_option):\n  """"""Clean and handle BPE or SPM outputs.""""""\n  sentence = sentence.strip()\n\n  # BPE\n  if subword_option == ""bpe"":\n    sentence = re.sub(""@@ "", """", sentence)\n\n  # SPM\n  elif subword_option == ""spm"":\n    sentence = u"""".join(sentence.split()).replace(u""\\u2581"", u"" "").lstrip()\n\n  return sentence\n\n\n# Follow //transconsole/localization/machine_translation/metrics/bleu_calc.py\ndef _bleu(ref_file, trans_file, subword_option=None):\n  """"""Compute BLEU scores and handling BPE.""""""\n  max_order = 4\n  smooth = False\n\n  ref_files = [ref_file]\n  reference_text = []\n  for reference_filename in ref_files:\n    with codecs.getreader(""utf-8"")(\n        tf.io.gfile.GFile(reference_filename, ""rb"")) as fh:\n      reference_text.append(fh.readlines())\n\n  per_segment_references = []\n  for references in zip(*reference_text):\n    reference_list = []\n    for reference in references:\n      reference = _clean(reference, subword_option)\n      reference_list.append(reference.split("" ""))\n    per_segment_references.append(reference_list)\n\n  translations = []\n  with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(trans_file, ""rb"")) as fh:\n    for line in fh:\n      line = _clean(line, subword_option=None)\n      translations.append(line.split("" ""))\n\n  # bleu_score, precisions, bp, ratio, translation_length, reference_length\n  bleu_score, _, _, _, _, _ = bleu.compute_bleu(\n      per_segment_references, translations, max_order, smooth)\n  return 100 * bleu_score\n\n\ndef _rouge(ref_file, summarization_file, subword_option=None):\n  """"""Compute ROUGE scores and handling BPE.""""""\n\n  references = []\n  with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(ref_file, ""rb"")) as fh:\n    for line in fh:\n      references.append(_clean(line, subword_option))\n\n  hypotheses = []\n  with codecs.getreader(""utf-8"")(\n      tf.io.gfile.GFile(summarization_file, ""rb"")) as fh:\n    for line in fh:\n      hypotheses.append(_clean(line, subword_option=None))\n\n  rouge_score_map = rouge.rouge(hypotheses, references)\n  return 100 * rouge_score_map[""rouge_l/f_score""]\n\n\ndef _accuracy(label_file, pred_file):\n  """"""Compute accuracy, each line contains a label.""""""\n\n  with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(label_file, ""rb"")) as label_fh:\n    with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(pred_file, ""rb"")) as pred_fh:\n      count = 0.0\n      match = 0.0\n      for label in label_fh:\n        label = label.strip()\n        pred = pred_fh.readline().strip()\n        if label == pred:\n          match += 1\n        count += 1\n  return 100 * match / count\n\n\ndef _word_accuracy(label_file, pred_file):\n  """"""Compute accuracy on per word basis.""""""\n\n  with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(label_file, ""r"")) as label_fh:\n    with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(pred_file, ""r"")) as pred_fh:\n      total_acc, total_count = 0., 0.\n      for sentence in label_fh:\n        labels = sentence.strip().split("" "")\n        preds = pred_fh.readline().strip().split("" "")\n        match = 0.0\n        for pos in range(min(len(labels), len(preds))):\n          label = labels[pos]\n          pred = preds[pos]\n          if label == pred:\n            match += 1\n        total_acc += 100 * match / max(len(labels), len(preds))\n        total_count += 1\n  return total_acc / total_count\n\n\ndef _moses_bleu(multi_bleu_script, tgt_test, trans_file, subword_option=None):\n  """"""Compute BLEU scores using Moses multi-bleu.perl script.""""""\n\n  # TODO(thangluong): perform rewrite using python\n  # BPE\n  if subword_option == ""bpe"":\n    debpe_tgt_test = tgt_test + "".debpe""\n    if not os.path.exists(debpe_tgt_test):\n      subprocess.call(shlex.split(""cp %s %s"" % (tgt_test, debpe_tgt_test)), shell=False)\n      subprocess.call(shlex.split(""sed s/@@ //g %s"" % (debpe_tgt_test)),\n                      shell=False)\n    tgt_test = debpe_tgt_test\n  elif subword_option == ""spm"":\n    despm_tgt_test = tgt_test + "".despm""\n    if not os.path.exists(despm_tgt_test):\n      subprocess.call(""cp %s %s"" % (tgt_test, despm_tgt_test))\n      subprocess.call(""sed s/ //g %s"" % (despm_tgt_test))\n      subprocess.call(u""sed s/^\\u2581/g %s"" % (despm_tgt_test))\n      subprocess.call(u""sed s/\\u2581/ /g %s"" % (despm_tgt_test))\n    tgt_test = despm_tgt_test\n  cmd = ""%s %s < %s"" % (multi_bleu_script, tgt_test, trans_file)\n\n  # subprocess\n  bleu_output = subprocess.check_output(shlex.split(cmd), shell=False)\n\n  # extract BLEU score\n  m = re.search(""BLEU = (.+?),"", bleu_output)\n  bleu_score = float(m.group(1))\n\n  return bleu_score\n'"
models/language_translation/tensorflow/mlperf_gnmt/fp32/hparam.py,0,"b'import json\nimport numbers\nimport six\n\nfrom tensorflow.python.util import compat\n\ndef _parse_fail(name, var_type, value, values):\n  """"""Helper function for raising a value error for bad assignment.""""""\n  raise ValueError(\n      \'Could not parse hparam \\\'%s\\\' of type \\\'%s\\\' with value \\\'%s\\\' in %s\' %\n      (name, var_type.__name__, value, values))\n\n\ndef _reuse_fail(name, values):\n  """"""Helper function for raising a value error for reuse of name.""""""\n  raise ValueError(\'Multiple assignments to variable \\\'%s\\\' in %s\' % (name,\n                                                                      values))\n\n\ndef _process_scalar_value(name, parse_fn, var_type, m_dict, values,\n                          results_dictionary):\n  """"""Update results_dictionary with a scalar value.\n  Used to update the results_dictionary to be returned by parse_values when\n  encountering a clause with a scalar RHS (e.g.  ""s=5"" or ""arr[0]=5"".)\n  Mutates results_dictionary.\n  Args:\n    name: Name of variable in assignment (""s"" or ""arr"").\n    parse_fn: Function for parsing the actual value.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n      m_dict[\'index\']: List index value (or None)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n  Raises:\n    ValueError: If the name has already been used.\n  """"""\n  try:\n    parsed_value = parse_fn(m_dict[\'val\'])\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'val\'], values)\n\n  # If no index is provided\n  if not m_dict[\'index\']:\n    if name in results_dictionary:\n      _reuse_fail(name, values)\n    results_dictionary[name] = parsed_value\n  else:\n    if name in results_dictionary:\n      # The name has already been used as a scalar, then it\n      # will be in this dictionary and map to a non-dictionary.\n      if not isinstance(results_dictionary.get(name), dict):\n        _reuse_fail(name, values)\n    else:\n      results_dictionary[name] = {}\n\n    index = int(m_dict[\'index\'])\n    # Make sure the index position hasn\'t already been assigned a value.\n    if index in results_dictionary[name]:\n      _reuse_fail(\'{}[{}]\'.format(name, index), values)\n    results_dictionary[name][index] = parsed_value\n\n\ndef _process_list_value(name, parse_fn, var_type, m_dict, values,\n                        results_dictionary):\n  """"""Update results_dictionary from a list of values.\n  Used to update results_dictionary to be returned by parse_values when\n  encountering a clause with a list RHS (e.g.  ""arr=[1,2,3]"".)\n  Mutates results_dictionary.\n  Args:\n    name: Name of variable in assignment (""arr"").\n    parse_fn: Function for parsing individual values.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n  Raises:\n    ValueError: If the name has an index or the values cannot be parsed.\n  """"""\n  if m_dict[\'index\'] is not None:\n    raise ValueError(\'Assignment of a list to a list index.\')\n  elements = filter(None, re.split(\'[ ,]\', m_dict[\'vals\']))\n  # Make sure the name hasn\'t already been assigned a value\n  if name in results_dictionary:\n    raise _reuse_fail(name, values)\n  try:\n    results_dictionary[name] = [parse_fn(e) for e in elements]\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'vals\'], values)\n\n\ndef _cast_to_type_if_compatible(name, param_type, value):\n  """"""Cast hparam to the provided type, if compatible.\n  Args:\n    name: Name of the hparam to be cast.\n    param_type: The type of the hparam.\n    value: The value to be cast, if compatible.\n  Returns:\n    The result of casting `value` to `param_type`.\n  Raises:\n    ValueError: If the type of `value` is not compatible with param_type.\n      * If `param_type` is a string type, but `value` is not.\n      * If `param_type` is a boolean, but `value` is not, or vice versa.\n      * If `param_type` is an integer type, but `value` is not.\n      * If `param_type` is a float type, but `value` is not a numeric type.\n  """"""\n  fail_msg = (\n      ""Could not cast hparam \'%s\' of type \'%s\' from value %r"" %\n      (name, param_type, value))\n\n  # If `value` is already of type `param_type`, return it directly.\n  # `isinstance` is too weak (e.g. isinstance(True, int) == True).\n  if type(value) == param_type:  # pylint: disable=unidiomatic-typecheck\n    return value\n\n  # Some callers use None, for which we can\'t do any casting/checking. :(\n  if issubclass(param_type, type(None)):\n    return value\n\n  # Avoid converting a non-string type to a string.\n  if (issubclass(param_type, (six.string_types, six.binary_type)) and\n      not isinstance(value, (six.string_types, six.binary_type))):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a number or string type to a boolean or vice versa.\n  if issubclass(param_type, bool) != isinstance(value, bool):\n    raise ValueError(fail_msg)\n\n  # Avoid converting float to an integer (the reverse is fine).\n  if (issubclass(param_type, numbers.Integral) and\n      not isinstance(value, numbers.Integral)):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a non-numeric type to a numeric type.\n  if (issubclass(param_type, numbers.Number) and\n      not isinstance(value, numbers.Number)):\n    raise ValueError(fail_msg)\n\n  return param_type(value)\n\n\ndef parse_values(values, type_map, ignore_unknown=False):\n  """"""Parses hyperparameter values from a string into a python map.\n  `values` is a string containing comma-separated `name=value` pairs.\n  For each pair, the value of the hyperparameter named `name` is set to\n  `value`.\n  If a hyperparameter name appears multiple times in `values`, a ValueError\n  is raised (e.g. \'a=1,a=2\', \'a[1]=1,a[1]=2\').\n  If a hyperparameter name in both an index assignment and scalar assignment,\n  a ValueError is raised.  (e.g. \'a=[1,2,3],a[0] = 1\').\n  The hyperparameter name may contain \'.\' symbols, which will result in an\n  attribute name that is only accessible through the getattr and setattr\n  functions.  (And must be first explicit added through add_hparam.)\n  WARNING: Use of \'.\' in your variable names is allowed, but is not well\n  supported and not recommended.\n  The `value` in `name=value` must follows the syntax according to the\n  type of the parameter:\n  *  Scalar integer: A Python-parsable integer point value.  E.g.: 1,\n     100, -12.\n  *  Scalar float: A Python-parsable floating point value.  E.g.: 1.0,\n     -.54e89.\n  *  Boolean: Either true or false.\n  *  Scalar string: A non-empty sequence of characters, excluding comma,\n     spaces, and square brackets.  E.g.: foo, bar_1.\n  *  List: A comma separated list of scalar values of the parameter type\n     enclosed in square brackets.  E.g.: [1,2,3], [1.0,1e-12], [high,low].\n  When index assignment is used, the corresponding type_map key should be the\n  list name.  E.g. for ""arr[1]=0"" the type_map must have the key ""arr"" (not\n  ""arr[1]"").\n  Args:\n    values: String.  Comma separated list of `name=value` pairs where\n      \'value\' must follow the syntax described above.\n    type_map: A dictionary mapping hyperparameter names to types.  Note every\n      parameter name in values must be a key in type_map.  The values must\n      conform to the types indicated, where a value V is said to conform to a\n      type T if either V has type T, or V is a list of elements of type T.\n      Hence, for a multidimensional parameter \'x\' taking float values,\n      \'x=[0.1,0.2]\' will parse successfully if type_map[\'x\'] = float.\n    ignore_unknown: Bool. Whether values that are missing a type in type_map\n      should be ignored. If set to True, a ValueError will not be raised for\n      unknown hyperparameter type.\n  Returns:\n    A python map mapping each name to either:\n    * A scalar value.\n    * A list of scalar values.\n    * A dictionary mapping index numbers to scalar values.\n    (e.g. ""x=5,L=[1,2],arr[1]=3"" results in {\'x\':5,\'L\':[1,2],\'arr\':{1:3}}"")\n  Raises:\n    ValueError: If there is a problem with input.\n    * If `values` cannot be parsed.\n    * If a list is assigned to a list index (e.g. \'a[1] = [1,2,3]\').\n    * If the same rvalue is assigned two different values (e.g. \'a=1,a=2\',\n      \'a[1]=1,a[1]=2\', or \'a=1,a=[1]\')\n  """"""\n  results_dictionary = {}\n  pos = 0\n  while pos < len(values):\n    m = PARAM_RE.match(values, pos)\n    if not m:\n      raise ValueError(\'Malformed hyperparameter value: %s\' % values[pos:])\n    # Check that there is a comma between parameters and move past it.\n    pos = m.end()\n    # Parse the values.\n    m_dict = m.groupdict()\n    name = m_dict[\'name\']\n    if name not in type_map:\n      if ignore_unknown:\n        continue\n      raise ValueError(\'Unknown hyperparameter type for %s\' % name)\n    type_ = type_map[name]\n\n    # Set up correct parsing function (depending on whether type_ is a bool)\n    if type_ == bool:\n\n      def parse_bool(value):\n        if value in [\'true\', \'True\']:\n          return True\n        elif value in [\'false\', \'False\']:\n          return False\n        else:\n          try:\n            return bool(int(value))\n          except ValueError:\n            _parse_fail(name, type_, value, values)\n\n      parse = parse_bool\n    else:\n      parse = type_\n\n    # If a singe value is provided\n    if m_dict[\'val\'] is not None:\n      _process_scalar_value(name, parse, type_, m_dict, values,\n                            results_dictionary)\n\n    # If the assigned value is a list:\n    elif m_dict[\'vals\'] is not None:\n      _process_list_value(name, parse, type_, m_dict, values,\n                          results_dictionary)\n\n    else:  # Not assigned a list or value\n      _parse_fail(name, type_, \'\', values)\n\n  return results_dictionary\n\n\nclass HParams(object):\n\n  _HAS_DYNAMIC_ATTRIBUTES = True  # Required for pytype checks.\n\n  def __init__(self, hparam_def=None, model_structure=None, **kwargs):\n    self._hparam_types = {}\n    self._model_structure = model_structure\n    if hparam_def:\n      self._init_from_proto(hparam_def)\n      if kwargs:\n        raise ValueError(\'hparam_def and initialization values are \'\n                         \'mutually exclusive\')\n    else:\n      for name, value in six.iteritems(kwargs):\n        self.add_hparam(name, value)\n\n  def _init_from_proto(self, hparam_def):\n    """"""Creates a new HParams from `HParamDef` protocol buffer.\n    Args:\n      hparam_def: `HParamDef` protocol buffer.\n    """"""\n    assert isinstance(hparam_def, hparam_pb2.HParamDef)\n    for name, value in hparam_def.hparam.items():\n      kind = value.WhichOneof(\'kind\')\n      if kind.endswith(\'_value\'):\n        # Single value.\n        if kind.startswith(\'int64\'):\n          # Setting attribute value to be \'int\' to ensure the type is compatible\n          # with both Python2 and Python3.\n          self.add_hparam(name, int(getattr(value, kind)))\n        elif kind.startswith(\'bytes\'):\n          # Setting attribute value to be \'str\' to ensure the type is compatible\n          # with both Python2 and Python3. UTF-8 encoding is assumed.\n          self.add_hparam(name, compat.as_str(getattr(value, kind)))\n        else:\n          self.add_hparam(name, getattr(value, kind))\n      else:\n        # List of values.\n        if kind.startswith(\'int64\'):\n          # Setting attribute value to be \'int\' to ensure the type is compatible\n          # with both Python2 and Python3.\n          self.add_hparam(name, [int(v) for v in getattr(value, kind).value])\n        elif kind.startswith(\'bytes\'):\n          # Setting attribute value to be \'str\' to ensure the type is compatible\n          # with both Python2 and Python3. UTF-8 encoding is assumed.\n          self.add_hparam(\n              name, [compat.as_str(v) for v in getattr(value, kind).value])\n        else:\n          self.add_hparam(name, [v for v in getattr(value, kind).value])\n\n  def add_hparam(self, name, value):\n    """"""Adds {name, value} pair to hyperparameters.\n    Args:\n      name: Name of the hyperparameter.\n      value: Value of the hyperparameter. Can be one of the following types:\n        int, float, string, int list, float list, or string list.\n    Raises:\n      ValueError: if one of the arguments is invalid.\n    """"""\n    # Keys in kwargs are unique, but \'name\' could the name of a pre-existing\n    # attribute of this object.  In that case we refuse to use it as a\n    # hyperparameter name.\n    if getattr(self, name, None) is not None:\n      raise ValueError(\'Hyperparameter name is reserved: %s\' % name)\n    if isinstance(value, (list, tuple)):\n      if not value:\n        raise ValueError(\n            \'Multi-valued hyperparameters cannot be empty: %s\' % name)\n      self._hparam_types[name] = (type(value[0]), True)\n    else:\n      self._hparam_types[name] = (type(value), False)\n    setattr(self, name, value)\n\n  def set_hparam(self, name, value):\n    """"""Set the value of an existing hyperparameter.\n    This function verifies that the type of the value matches the type of the\n    existing hyperparameter.\n    Args:\n      name: Name of the hyperparameter.\n      value: New value of the hyperparameter.\n    Raises:\n      KeyError: If the hyperparameter doesn\'t exist.\n      ValueError: If there is a type mismatch.\n    """"""\n    param_type, is_list = self._hparam_types[name]\n    if isinstance(value, list):\n      if not is_list:\n        raise ValueError(\n            \'Must not pass a list for single-valued parameter: %s\' % name)\n      setattr(self, name, [\n          _cast_to_type_if_compatible(name, param_type, v) for v in value])\n    else:\n      if is_list:\n        raise ValueError(\n            \'Must pass a list for multi-valued parameter: %s.\' % name)\n      setattr(self, name, _cast_to_type_if_compatible(name, param_type, value))\n\n  def del_hparam(self, name):\n    """"""Removes the hyperparameter with key \'name\'.\n    Does nothing if it isn\'t present.\n    Args:\n      name: Name of the hyperparameter.\n    """"""\n    if hasattr(self, name):\n      delattr(self, name)\n      del self._hparam_types[name]\n\n  def parse(self, values):\n    """"""Override existing hyperparameter values, parsing new values from a string.\n    See parse_values for more detail on the allowed format for values.\n    Args:\n      values: String.  Comma separated list of `name=value` pairs where \'value\'\n        must follow the syntax described above.\n    Returns:\n      The `HParams` instance.\n    Raises:\n      ValueError: If `values` cannot be parsed or a hyperparameter in `values`\n      doesn\'t exist.\n    """"""\n    type_map = {}\n    for name, t in self._hparam_types.items():\n      param_type, _ = t\n      type_map[name] = param_type\n\n    values_map = parse_values(values, type_map)\n    return self.override_from_dict(values_map)\n\n  def override_from_dict(self, values_dict):\n    """"""Override existing hyperparameter values, parsing new values from a dictionary.\n    Args:\n      values_dict: Dictionary of name:value pairs.\n    Returns:\n      The `HParams` instance.\n    Raises:\n      KeyError: If a hyperparameter in `values_dict` doesn\'t exist.\n      ValueError: If `values_dict` cannot be parsed.\n    """"""\n    for name, value in values_dict.items():\n      self.set_hparam(name, value)\n    return self\n\n  def set_model_structure(self, model_structure):\n    self._model_structure = model_structure\n\n  def get_model_structure(self):\n    return self._model_structure\n\n  def to_json(self, indent=None, separators=None, sort_keys=False):\n    """"""Serializes the hyperparameters into JSON.\n    Args:\n      indent: If a non-negative integer, JSON array elements and object members\n        will be pretty-printed with that indent level. An indent level of 0, or\n        negative, will only insert newlines. `None` (the default) selects the\n        most compact representation.\n      separators: Optional `(item_separator, key_separator)` tuple. Default is\n        `(\', \', \': \')`.\n      sort_keys: If `True`, the output dictionaries will be sorted by key.\n    Returns:\n      A JSON string.\n    """"""\n    return json.dumps(\n        self.values(),\n        indent=indent,\n        separators=separators,\n        sort_keys=sort_keys)\n\n  def parse_json(self, values_json):\n    """"""Override existing hyperparameter values, parsing new values from a json object.\n    Args:\n      values_json: String containing a json object of name:value pairs.\n    Returns:\n      The `HParams` instance.\n    Raises:\n      KeyError: If a hyperparameter in `values_json` doesn\'t exist.\n      ValueError: If `values_json` cannot be parsed.\n    """"""\n    values_map = json.loads(values_json)\n    return self.override_from_dict(values_map)\n\n  def values(self):\n    """"""Return the hyperparameter values as a Python dictionary.\n    Returns:\n      A dictionary with hyperparameter names as keys.  The values are the\n      hyperparameter values.\n    """"""\n    return {n: getattr(self, n) for n in self._hparam_types.keys()}\n\n  def get(self, key, default=None):\n    """"""Returns the value of `key` if it exists, else `default`.""""""\n    if key in self._hparam_types:\n      # Ensure that default is compatible with the parameter type.\n      if default is not None:\n        param_type, is_param_list = self._hparam_types[key]\n        type_str = \'list<%s>\' % param_type if is_param_list else str(param_type)\n        fail_msg = (""Hparam \'%s\' of type \'%s\' is incompatible with ""\n                    \'default=%s\' % (key, type_str, default))\n\n        is_default_list = isinstance(default, list)\n        if is_param_list != is_default_list:\n          raise ValueError(fail_msg)\n\n        try:\n          if is_default_list:\n            for value in default:\n              _cast_to_type_if_compatible(key, param_type, value)\n          else:\n            _cast_to_type_if_compatible(key, param_type, default)\n        except ValueError as e:\n          raise ValueError(\'%s. %s\' % (fail_msg, e))\n\n      return getattr(self, key)\n\n    return default\n\n  def __contains__(self, key):\n    return key in self._hparam_types\n\n  def __str__(self):\n    hpdict = self.values()\n    output_list = [\'{}={}\'.format(key, hpdict[key]) for key in hpdict]\n    return \',\'.join(output_list)\n\n  def __repr__(self):\n    strval = str(sorted(self.values().items()))\n    return \'%s(%s)\' % (type(self).__name__, strval)\n\n  @staticmethod\n  def _get_kind_name(param_type, is_list):\n    """"""Returns the field name given parameter type and is_list.\n    Args:\n      param_type: Data type of the hparam.\n      is_list: Whether this is a list.\n    Returns:\n      A string representation of the field name.\n    Raises:\n      ValueError: If parameter type is not recognized.\n    """"""\n    if issubclass(param_type, bool):\n      # This check must happen before issubclass(param_type, six.integer_types),\n      # since Python considers bool to be a subclass of int.\n      typename = \'bool\'\n    elif issubclass(param_type, six.integer_types):\n      # Setting \'int\' and \'long\' types to be \'int64\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'int64\'\n    elif issubclass(param_type, (six.string_types, six.binary_type)):\n      # Setting \'string\' and \'bytes\' types to be \'bytes\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'bytes\'\n    elif issubclass(param_type, float):\n      typename = \'float\'\n    else:\n      raise ValueError(\'Unsupported parameter type: %s\' % str(param_type))\n\n    suffix = \'list\' if is_list else \'value\'\n    return \'_\'.join([typename, suffix])\n\n  def to_proto(self, export_scope=None):  # pylint: disable=unused-argument\n    """"""Converts a `HParams` object to a `HParamDef` protocol buffer.\n    Args:\n      export_scope: Optional `string`. Name scope to remove.\n    Returns:\n      A `HParamDef` protocol buffer.\n    """"""\n    hparam_proto = hparam_pb2.HParamDef()\n    for name in self._hparam_types:\n      # Parse the values.\n      param_type, is_list = self._hparam_types.get(name, (None, None))\n      kind = HParams._get_kind_name(param_type, is_list)\n\n      if is_list:\n        if kind.startswith(\'bytes\'):\n          v_list = [compat.as_bytes(v) for v in getattr(self, name)]\n        else:\n          v_list = [v for v in getattr(self, name)]\n        getattr(hparam_proto.hparam[name], kind).value.extend(v_list)\n      else:\n        v = getattr(self, name)\n        if kind.startswith(\'bytes\'):\n          v = compat.as_bytes(getattr(self, name))\n        setattr(hparam_proto.hparam[name], kind, v)\n\n    return hparam_proto\n\n  @staticmethod\n  def from_proto(hparam_def, import_scope=None):  # pylint: disable=unused-argument\n    return HParams(hparam_def=hparam_def)\n\n'"
models/language_translation/tensorflow/mlperf_gnmt/fp32/misc_utils.py,9,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Generally useful utility functions.""""""\nfrom __future__ import print_function\n\nimport codecs\nimport collections\nimport json\nimport math\nimport os\nimport sys\nimport time\nfrom distutils import version\n\nimport numpy as np\nimport tensorflow as tf\n\nimport hparam\n\ndef check_tensorflow_version():\n  min_tf_version = ""1.4.0-dev20171024""\n  if (version.LooseVersion(tf.__version__) <\n      version.LooseVersion(min_tf_version)):\n    raise EnvironmentError(""Tensorflow version must >= %s"" % min_tf_version)\n\n\ndef safe_exp(value):\n  """"""Exponentiation with catching of overflow error.""""""\n  try:\n    ans = math.exp(value)\n  except OverflowError:\n    ans = float(""inf"")\n  return ans\n\n\ndef print_time(s, start_time):\n  """"""Take a start time, print elapsed duration, and return a new time.""""""\n  print(""%s, time %ds, %s."" % (s, (time.time() - start_time), time.ctime()))\n  sys.stdout.flush()\n  return time.time()\n\n\ndef print_out(s, f=None, new_line=True):\n  """"""Similar to print but with support to flush and output to a file.""""""\n  if isinstance(s, bytes):\n    s = s.decode(""utf-8"")\n\n  if f:\n    f.write(s.encode(""utf-8""))\n    if new_line:\n      f.write(b""\\n"")\n\n  # stdout\n  out_s = s.encode(""utf-8"")\n  if not isinstance(out_s, str):\n    out_s = out_s.decode(""utf-8"")\n  print(out_s, end="""", file=sys.stdout)\n\n  if new_line:\n    sys.stdout.write(""\\n"")\n  sys.stdout.flush()\n\n\ndef print_hparams(hparams, skip_patterns=None, header=None):\n  """"""Print hparams, can skip keys based on pattern.""""""\n  if header: print_out(""%s"" % header)\n  values = hparams.values()\n  for key in sorted(values.keys()):\n    if not skip_patterns or all(\n        [skip_pattern not in key for skip_pattern in skip_patterns]):\n      print_out(""  %s=%s"" % (key, str(values[key])))\n\n\ndef load_hparams(model_dir):\n  """"""Load hparams from an existing model directory.""""""\n  hparams_file = os.path.join(model_dir, ""hparams"")\n  if tf.io.gfile.exists(hparams_file):\n    print_out(""# Loading hparams from %s"" % hparams_file)\n    with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(hparams_file, ""rb"")) as f:\n      try:\n        hparams_values = json.load(f)\n        hparams = hparams.HParams(**hparams_values)\n      except ValueError:\n        print_out(""  can\'t load hparams file"")\n        return None\n    return hparams\n  else:\n    return None\n\n\ndef maybe_parse_standard_hparams(hparams, hparams_path):\n  """"""Override hparams values with existing standard hparams config.""""""\n  if hparams_path and tf.io.gfile.exists(hparams_path):\n    print_out(""# Loading standard hparams from %s"" % hparams_path)\n    with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(hparams_path, ""rb"")) as f:\n      hparams.parse_json(f.read())\n  return hparams\n\n\ndef save_hparams(out_dir, hparams):\n  """"""Save hparams.""""""\n  hparams_file = os.path.join(out_dir, ""hparams"")\n  print_out(""  saving hparams to %s"" % hparams_file)\n  with codecs.getwriter(""utf-8"")(tf.io.gfile.GFile(hparams_file, ""wb"")) as f:\n    f.write(hparams.to_json(indent=4, sort_keys=True))\n\n\ndef debug_tensor(s, msg=None, summarize=10):\n  """"""Print the shape and value of a tensor at test time. Return a new tensor.""""""\n  if not msg:\n    msg = s.name\n  return tf.compat.v1.Print(s, [tf.shape(input=s), s], msg + "" "", summarize=summarize)\n\n\ndef add_summary(summary_writer, global_step, tag, value):\n  """"""Add a new summary to the current summary_writer.\n  Useful to log things that are not part of the training graph, e.g., tag=BLEU.\n  """"""\n  summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=tag, simple_value=value)])\n  summary_writer.add_summary(summary, global_step)\n\n\ndef get_config_proto(log_device_placement=False, allow_soft_placement=True,\n                     num_intra_threads=0, num_inter_threads=0):\n  # GPU options:\n  # https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html\n  config_proto = tf.compat.v1.ConfigProto(\n      log_device_placement=log_device_placement,\n      allow_soft_placement=allow_soft_placement)\n  config_proto.gpu_options.allow_growth = True\n\n  # CPU threads options\n  if num_intra_threads:\n    config_proto.intra_op_parallelism_threads = num_intra_threads\n  if num_inter_threads:\n    config_proto.inter_op_parallelism_threads = num_inter_threads\n\n  return config_proto\n\n\ndef format_text(words):\n  """"""Convert a sequence words into sentence.""""""\n  if (not hasattr(words, ""__len__"") and  # for numpy array\n      not isinstance(words, collections.Iterable)):\n    words = [words]\n  return b"" "".join(words)\n\n\ndef format_bpe_text(symbols, delimiter=b""@@""):\n  """"""Convert a sequence of bpe words into sentence.""""""\n  words = []\n  word = b""""\n  if isinstance(symbols, str):\n    symbols = symbols.encode()\n  delimiter_len = len(delimiter)\n  for symbol in symbols:\n    if len(symbol) >= delimiter_len and symbol[-delimiter_len:] == delimiter:\n      word += symbol[:-delimiter_len]\n    else:  # end of a word\n      word += symbol\n      words.append(word)\n      word = b""""\n  return b"" "".join(words)\n\n\ndef format_spm_text(symbols):\n  """"""Decode a text in SPM (https://github.com/google/sentencepiece) format.""""""\n  return u"""".join(format_text(symbols).decode(""utf-8"").split()).replace(\n      u""\\u2581"", u"" "").strip().encode(""utf-8"")\n'"
models/language_translation/tensorflow/mlperf_gnmt/fp32/nmt_utils.py,3,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility functions specifically for NMT.""""""\nfrom __future__ import print_function\n\nimport codecs\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport logging as log\nimport evaluation_utils\nimport misc_utils as utils\n\n\n__all__ = [""decode_and_evaluate"", ""get_translation""]\n\n\ndef decode_and_evaluate(mode, sess, out_tensor, trans_file, ref_file,\n                        metric=\'bleu\', beam_width=10,\n                        num_translations_per_input=1, iterations=1):\n  """"""Decode a test set and compute a score according to the evaluation task.""""""\n  utils.print_out(""  Decoding to output %s"" % trans_file)\n\n  with codecs.getwriter(""utf-8"")(\n      tf.io.gfile.GFile(trans_file, mode=""wb"")) as trans_f:\n    trans_f.write("""")  # Write empty string to ensure file is created.\n    num_translations_per_input = min(num_translations_per_input, beam_width)\n\n    print(""  Running inference with beam_width %g, num translations per input %d. "" \\\n          % (beam_width, num_translations_per_input))\n    print(""  Total iterations count %d."" % iterations)\n\n    # Warmup for the first batch to take out the very first runtime\n    # session overhead.\n    nmt_outputs = sess.run(out_tensor) # time x batch_size x beam_width\n    nmt_outputs = nmt_outputs.transpose() # beam_width x batch_size x time\n    batch_size = nmt_outputs.shape[1]\n    for sent_id in range(batch_size):\n      translation = get_translation(nmt_outputs[0], sent_id,\n                                    tgt_eos=\'</s>\')\n      if mode == \'accuracy\':\n        trans_f.write((translation + b""\\n"").decode(""utf-8""))\n\n    # prediction time is the time for the model prediction only\n    # overall time is the time for data pre-processing and data post-processing\n    prediction_times = list()\n    overall_start = time.time()\n    num_sentences = 0\n    n = 0\n    while n < iterations:\n      n += 1\n      while True:\n        try:\n          start = time.time()\n          nmt_outputs = sess.run(out_tensor) # time x batch_size x beam_width\n          nmt_outputs = nmt_outputs.transpose() # beam_width x batch_size x time\n          prediction_times.append(time.time() - start)\n          batch_size = nmt_outputs.shape[1]\n          num_sentences += batch_size\n          for sent_id in range(batch_size):\n            for beam_id in range(num_translations_per_input):\n              translation = get_translation(nmt_outputs[beam_id], sent_id,\n                                            tgt_eos=\'</s>\')\n              if mode == \'accuracy\':\n                trans_f.write((translation + b""\\n"").decode(""utf-8""))\n\n        except tf.errors.OutOfRangeError:\n          utils.print_time(\n              ""  Done, num sentences %d, num translations per input %d"" %\n              (num_sentences, num_translations_per_input), overall_start)\n          break\n\n  overall_time = (time.time() - overall_start)\n  print(""\\nAverage Prediction Latency: {:.5f} sec per batch."".format(\n    sum(prediction_times)/float(len(prediction_times))))\n  print(""Overall Latency: {:.5f} sec for the entire test ""\n        ""dataset."".format(overall_time/float(iterations)))\n  print(""Overall Throughput : {:.3f} sentences per sec."".format(\n        num_sentences/float(overall_time)))\n\n  # Evaluation\n  if mode == \'accuracy\':\n    if ref_file and tf.io.gfile.exists(trans_file):\n      score = evaluation_utils.evaluate(ref_file, trans_file, metric)\n      utils.print_out(""  Accuracy metric %s: %.1f"" % (metric, score))\n\n\ndef get_translation(nmt_outputs, sent_id, tgt_eos):\n  """"""Given batch decoding outputs, select a sentence and turn to text.""""""\n  if tgt_eos: tgt_eos = tgt_eos.encode(""utf-8"")\n  # Select a sentence\n  output = nmt_outputs[sent_id, :].tolist()\n\n  # If there is an eos symbol in outputs, cut them at that point.\n  if tgt_eos and tgt_eos in output:\n    output = output[:output.index(tgt_eos)]\n    translation = utils.format_text(output)\n\n  return translation\n'"
models/language_translation/tensorflow/mlperf_gnmt/fp32/rouge.py,0,"b'""""""ROUGE metric implementation.\n\nCopy from tf_seq2seq/seq2seq/metrics/rouge.py.\nThis is a modified and slightly extended verison of\nhttps://github.com/miso-belica/sumy/blob/dev/sumy/evaluation/rouge.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport itertools\nimport numpy as np\n\n#pylint: disable=C0103\n\n\ndef _get_ngrams(n, text):\n  """"""Calcualtes n-grams.\n\n  Args:\n    n: which n-grams to calculate\n    text: An array of tokens\n\n  Returns:\n    A set of n-grams\n  """"""\n  ngram_set = set()\n  text_length = len(text)\n  max_index_ngram_start = text_length - n\n  for i in range(max_index_ngram_start + 1):\n    ngram_set.add(tuple(text[i:i + n]))\n  return ngram_set\n\n\ndef _split_into_words(sentences):\n  """"""Splits multiple sentences into words and flattens the result""""""\n  return list(itertools.chain(*[_.split("" "") for _ in sentences]))\n\n\ndef _get_word_ngrams(n, sentences):\n  """"""Calculates word n-grams for multiple sentences.\n  """"""\n  assert len(sentences) > 0\n  assert n > 0\n\n  words = _split_into_words(sentences)\n  return _get_ngrams(n, words)\n\n\ndef _len_lcs(x, y):\n  """"""\n  Returns the length of the Longest Common Subsequence between sequences x\n  and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns\n    integer: Length of LCS between x and y\n  """"""\n  table = _lcs(x, y)\n  n, m = len(x), len(y)\n  return table[n, m]\n\n\ndef _lcs(x, y):\n  """"""\n  Computes the length of the longest common subsequence (lcs) between two\n  strings. The implementation below uses a DP programming algorithm and runs\n  in O(nm) time where n = len(x) and m = len(y).\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: collection of words\n    y: collection of words\n\n  Returns:\n    Table of dictionary of coord and len lcs\n  """"""\n  n, m = len(x), len(y)\n  table = dict()\n  for i in range(n + 1):\n    for j in range(m + 1):\n      if i == 0 or j == 0:\n        table[i, j] = 0\n      elif x[i - 1] == y[j - 1]:\n        table[i, j] = table[i - 1, j - 1] + 1\n      else:\n        table[i, j] = max(table[i - 1, j], table[i, j - 1])\n  return table\n\n\ndef _recon_lcs(x, y):\n  """"""\n  Returns the Longest Subsequence between x and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns:\n    sequence: LCS of x and y\n  """"""\n  i, j = len(x), len(y)\n  table = _lcs(x, y)\n\n  def _recon(i, j):\n    """"""private recon calculation""""""\n    if i == 0 or j == 0:\n      return []\n    elif x[i - 1] == y[j - 1]:\n      return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n    elif table[i - 1, j] > table[i, j - 1]:\n      return _recon(i - 1, j)\n    else:\n      return _recon(i, j - 1)\n\n  recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n  return recon_tuple\n\n\ndef rouge_n(evaluated_sentences, reference_sentences, n=2):\n  """"""\n  Computes ROUGE-N of two text collections of sentences.\n  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n  papers/rouge-working-note-v1.3.1.pdf\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n    n: Size of ngram.  Defaults to 2.\n\n  Returns:\n    A tuple (f1, precision, recall) for ROUGE-N\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  """"""\n  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n    raise ValueError(""Collections must contain at least 1 sentence."")\n\n  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n  reference_count = len(reference_ngrams)\n  evaluated_count = len(evaluated_ngrams)\n\n  # Gets the overlapping ngrams between evaluated and reference\n  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n  overlapping_count = len(overlapping_ngrams)\n\n  # Handle edge case. This isn\'t mathematically correct, but it\'s good enough\n  if evaluated_count == 0:\n    precision = 0.0\n  else:\n    precision = overlapping_count / evaluated_count\n\n  if reference_count == 0:\n    recall = 0.0\n  else:\n    recall = overlapping_count / reference_count\n\n  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n\n  # return overlapping_count / reference_count\n  return f1_score, precision, recall\n\n\ndef _f_p_r_lcs(llcs, m, n):\n  """"""\n  Computes the LCS-based F-measure score\n  Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Args:\n    llcs: Length of LCS\n    m: number of words in reference summary\n    n: number of words in candidate summary\n\n  Returns:\n    Float. LCS-based F-measure score\n  """"""\n  r_lcs = llcs / m\n  p_lcs = llcs / n\n  beta = p_lcs / (r_lcs + 1e-12)\n  num = (1 + (beta**2)) * r_lcs * p_lcs\n  denom = r_lcs + ((beta**2) * p_lcs)\n  f_lcs = num / (denom + 1e-12)\n  return f_lcs, p_lcs, r_lcs\n\n\ndef rouge_l_sentence_level(evaluated_sentences, reference_sentences):\n  """"""\n  Computes ROUGE-L (sentence level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = LCS(X,Y)/m\n  P_lcs = LCS(X,Y)/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  X = reference summary\n  Y = Candidate summary\n  m = length of reference summary\n  n = length of candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  """"""\n  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n    raise ValueError(""Collections must contain at least 1 sentence."")\n  reference_words = _split_into_words(reference_sentences)\n  evaluated_words = _split_into_words(evaluated_sentences)\n  m = len(reference_words)\n  n = len(evaluated_words)\n  lcs = _len_lcs(evaluated_words, reference_words)\n  return _f_p_r_lcs(lcs, m, n)\n\n\ndef _union_lcs(evaluated_sentences, reference_sentence):\n  """"""\n  Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n  subsequence between reference sentence ri and candidate summary C. For example\n  if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and\n  c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is\n  ""w1 w2"" and the longest common subsequence of r_i and c2 is ""w1 w3 w5"". The\n  union longest common subsequence of r_i, c1, and c2 is ""w1 w2 w3 w5"" and\n  LCS_u(r_i, C) = 4/5.\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    float: LCS_u(r_i, C)\n\n  ValueError:\n    Raises exception if a param has len <= 0\n  """"""\n  if len(evaluated_sentences) <= 0:\n    raise ValueError(""Collections must contain at least 1 sentence."")\n\n  lcs_union = set()\n  reference_words = _split_into_words([reference_sentence])\n  combined_lcs_length = 0\n  for eval_s in evaluated_sentences:\n    evaluated_words = _split_into_words([eval_s])\n    lcs = set(_recon_lcs(reference_words, evaluated_words))\n    combined_lcs_length += len(lcs)\n    lcs_union = lcs_union.union(lcs)\n\n  union_lcs_count = len(lcs_union)\n  union_lcs_value = union_lcs_count / combined_lcs_length\n  return union_lcs_value\n\n\ndef rouge_l_summary_level(evaluated_sentences, reference_sentences):\n  """"""\n  Computes ROUGE-L (summary level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n  P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  SUM(i,u) = SUM from i through u\n  u = number of sentences in reference summary\n  C = Candidate summary made up of v sentences\n  m = number of words in reference summary\n  n = number of words in candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  """"""\n  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n    raise ValueError(""Collections must contain at least 1 sentence."")\n\n  # total number of words in reference sentences\n  m = len(_split_into_words(reference_sentences))\n\n  # total number of words in evaluated sentences\n  n = len(_split_into_words(evaluated_sentences))\n\n  union_lcs_sum_across_all_references = 0\n  for ref_s in reference_sentences:\n    union_lcs_sum_across_all_references += _union_lcs(evaluated_sentences,\n                                                      ref_s)\n  return _f_p_r_lcs(union_lcs_sum_across_all_references, m, n)\n\n\ndef rouge(hypotheses, references):\n  """"""Calculates average rouge scores for a list of hypotheses and\n  references""""""\n\n  # Filter out hyps that are of 0 length\n  # hyps_and_refs = zip(hypotheses, references)\n  # hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]\n  # hypotheses, references = zip(*hyps_and_refs)\n\n  # Calculate ROUGE-1 F1, precision, recall scores\n  rouge_1 = [\n      rouge_n([hyp], [ref], 1) for hyp, ref in zip(hypotheses, references)\n  ]\n  rouge_1_f, rouge_1_p, rouge_1_r = map(np.mean, zip(*rouge_1))\n\n  # Calculate ROUGE-2 F1, precision, recall scores\n  rouge_2 = [\n      rouge_n([hyp], [ref], 2) for hyp, ref in zip(hypotheses, references)\n  ]\n  rouge_2_f, rouge_2_p, rouge_2_r = map(np.mean, zip(*rouge_2))\n\n  # Calculate ROUGE-L F1, precision, recall scores\n  rouge_l = [\n      rouge_l_sentence_level([hyp], [ref])\n      for hyp, ref in zip(hypotheses, references)\n  ]\n  rouge_l_f, rouge_l_p, rouge_l_r = map(np.mean, zip(*rouge_l))\n\n  return {\n      ""rouge_1/f_score"": rouge_1_f,\n      ""rouge_1/r_score"": rouge_1_r,\n      ""rouge_1/p_score"": rouge_1_p,\n      ""rouge_2/f_score"": rouge_2_f,\n      ""rouge_2/r_score"": rouge_2_r,\n      ""rouge_2/p_score"": rouge_2_p,\n      ""rouge_l/f_score"": rouge_l_f,\n      ""rouge_l/r_score"": rouge_l_r,\n      ""rouge_l/p_score"": rouge_l_p,\n  }\n'"
models/language_translation/tensorflow/mlperf_gnmt/fp32/run_inference.py,9,"b'import codecs\nimport argparse\nimport os\nimport tensorflow as tf\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.framework import importer\n\nimport misc_utils as utils\nfrom nmt_utils import decode_and_evaluate\n\nfrom tensorflow_addons import seq2seq\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--in_graph"", type=str, required=True,\n                    help=""Specify the frozen inference graph in pb format."")\nparser.add_argument(""--batch_size"", type=int, default=32,\n                    help=""Specify inference batch size."")\nparser.add_argument(""--num_inter_threads"", type=int, default=0,\n                   help=""Specify number of inter-op threads."")\nparser.add_argument(""--num_intra_threads"", type=int, default=0,\n                   help=""Specify number of intra-op threads."")\nparser.add_argument(""--src_vocab_file"", type=str, required=True,\n                    help=""Specify source vocab file."")\nparser.add_argument(""--tgt_vocab_file"", type=str, required=True,\n                    help=""Specify target vocabulary file."")\nparser.add_argument(""--inference_input_file"", type=str, required=True,\n                    help=""Specify input file to be translated."")\nparser.add_argument(""--inference_output_file"", type=str, default=None,\n                    help=""Specify output file for resulting translation."")\nparser.add_argument(""--inference_ref_file"", type=str, required=True,\n                    help=""Specify reference output file."")\nparser.add_argument(""--run"", type=str, default=""accuracy"",\n                    help=""Specify either \'accuracy\' for BLEU metric or ""\n                         ""\'performance\' for latency and throughput."")\nargs = parser.parse_args()\n\nout_dir = os.path.join(os.getcwd(), \'output\')\ntf.io.gfile.makedirs(out_dir)\n\nif args.inference_output_file:\n  inference_output_file = args.inference_output_file\nelse:\n  inference_output_file = os.path.join(out_dir, \'gnmt-out\')\n\ndef read_source_sentences(inference_input_file):\n  """"""Load inference data.""""""\n  with codecs.getreader(""utf-8"")(\n      tf.io.gfile.GFile(inference_input_file, mode=""rb"")) as f:\n    inference_data = f.read().splitlines()\n  return inference_data\n\ndef create_new_vocab_file(vocab_file):\n  """"""Creates a new vocabulary file prepending three new tokens:\n  (1) <unk> for unknown tag, (2) <s> for start of sentence tag, and (3) </s> for end of\n  sentence tag.""""""\n  vocab = []\n  with codecs.getreader(""utf-8"")(tf.io.gfile.GFile(vocab_file, ""rb"")) as f:\n    vocab_size = 0\n    for word in f:\n      vocab_size += 1\n      vocab.append(word.strip())\n\n  if tf.io.gfile.exists(vocab_file):\n    utils.print_out(""# Vocab file %s exists"" % vocab_file)\n    assert len(vocab) >= 3\n    (unk, sos, eos) = (""<unk>"", ""<s>"", ""</s>"")\n    if vocab[0] != unk or vocab[1] != sos or vocab[2] != eos:\n      utils.print_out(""The first 3 vocab words [%s, %s, %s]""\n                      "" are not [%s, %s, %s]"" %\n                      (vocab[0], vocab[1], vocab[2], unk, sos, eos))\n      vocab = [unk, sos, eos] + vocab\n      vocab_size += 3\n      new_vocab_file = os.path.join(out_dir, os.path.basename(vocab_file))\n      with codecs.getwriter(""utf-8"")(\n          tf.io.gfile.GFile(new_vocab_file, ""wb"")) as f:\n        for word in vocab:\n          f.write(""%s\\n"" % word)\n      vocab_file = new_vocab_file\n  else:\n    raise ValueError(""vocab_file \'%s\' does not exist."" % vocab_file)\n  return vocab_file\n\nif __name__ == ""__main__"":\n  graph_def = graph_pb2.GraphDef()\n  with tf.io.gfile.GFile(args.in_graph, ""rb"") as f:\n    data = f.read()\n  graph_def.ParseFromString(data)\n  graph = tf.Graph()\n  with graph.as_default():\n    importer.import_graph_def(graph_def, input_map={}, name="""")\n    # Get input and output and tensors/ops for inference.\n    src_vocab_placeholder = graph.get_tensor_by_name(\'source_vocab_file:0\')\n    tgt_vocab_placeholder = graph.get_tensor_by_name(\'target_vocab_file:0\')\n    src_data_placeholder = graph.get_tensor_by_name(\'source_data:0\')\n    batch_size_placeholder = graph.get_tensor_by_name(\'batch_size:0\')\n\n    tables_initializer = graph.get_operation_by_name(\'init_all_tables\')\n    iterator_initilizer = graph.get_operation_by_name(\'MakeIterator\')\n    sample_words_tensor = graph.get_tensor_by_name(\'hash_table_Lookup_1/LookupTableFindV2:0\')\n\n  # Create a session with imported graph.\n  config_proto = tf.compat.v1.ConfigProto(allow_soft_placement=True,\n      intra_op_parallelism_threads = args.num_intra_threads,\n      inter_op_parallelism_threads = args.num_inter_threads)\n  sess = tf.compat.v1.Session(graph=graph, config=config_proto)\n\n  # Read source data.\n  src_data = read_source_sentences(args.inference_input_file)\n\n  # Initialize vocabulary tables and source data iterator.\n  sess.run(tables_initializer, feed_dict={\n      src_vocab_placeholder: create_new_vocab_file(args.src_vocab_file),\n      tgt_vocab_placeholder: create_new_vocab_file(args.tgt_vocab_file)})\n  sess.run(iterator_initilizer, feed_dict={\n      src_data_placeholder: src_data,\n      batch_size_placeholder: args.batch_size})\n\n  # Decode\n  decode_and_evaluate(args.run, sess, sample_words_tensor, inference_output_file,\n                      args.inference_ref_file)\n'"
models/object_detection/tensorflow/rfcn/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/coco_detection_evaluator.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nfrom inference import coco_tools\nfrom inference import coco_label_map\n\nclass CocoDetectionEvaluator:\n    """"""Class to evaluate COCO detection metrics.""""""\n\n    def __init__(self):\n        self._image_ids = {}\n        self._groundtruth_list = []\n        self._detection_boxes_list = []\n        self._annotation_id = 1\n        self._category_id_set = set([cat for cat in coco_label_map.category_map])\n        self._groundtruth_list = []\n        self._detection_boxes_list = []\n\n    def add_single_ground_truth_image_info(self,\n                                           image_id,\n                                           groundtruth_dict):\n        if image_id in self._image_ids:\n            return\n        \n        self._groundtruth_list.extend(\n        coco_tools.ExportSingleImageGroundtruthToCoco(\n            image_id=image_id,\n            next_annotation_id=self._annotation_id,\n            category_id_set=self._category_id_set,\n            groundtruth_boxes=groundtruth_dict[\'boxes\'],\n            groundtruth_classes=groundtruth_dict[\'classes\']))\n        self._annotation_id += groundtruth_dict[\'boxes\'].shape[0]\n        \n        self._image_ids[image_id] = False\n        is_debug = False\n        if image_id == \'000000059386.jpg\':\n            is_debug = True\n        if is_debug:\n            is_debug = False\n            print(groundtruth_dict[\'boxes\'])\n            print(groundtruth_dict[\'classes\'])\n            print(image_id)\n    \n    def add_single_detected_image_info(self,\n                                       image_id,\n                                       detections_dict):\n        assert (image_id in self._image_ids)\n        \n        if self._image_ids[image_id]:\n            return\n        \n        self._detection_boxes_list.extend(\n            coco_tools.ExportSingleImageDetectionBoxesToCoco(\n                image_id=image_id,\n                category_id_set=self._category_id_set,\n                detection_boxes=detections_dict[\'boxes\'],\n                detection_scores=detections_dict[\'scores\'],\n                detection_classes=detections_dict[\'classes\']))\n\n        self._image_ids[image_id] = True\n        is_debug = False\n        if image_id == \'000000059386.jpg\':\n            is_debug = True\n        if is_debug:\n            is_debug = False\n            print(detections_dict[\'boxes\'])\n            print(detections_dict[\'classes\'])\n            print(detections_dict[\'classes\'])\n            print(image_id)\n\n    def evaluate(self):\n        groundtruth_dict = {\n        \'annotations\': self._groundtruth_list,\n        \'images\': [{\'id\': image_id} for image_id in self._image_ids],\n        \'categories\': [{\'id\': k, \'name\': v} for k, v in coco_label_map.category_map.items()]\n        }\n        coco_wrapped_groundtruth = coco_tools.COCOWrapper(groundtruth_dict)\n        coco_wrapped_detections = coco_wrapped_groundtruth.LoadAnnotations(\n            self._detection_boxes_list)\n        box_evaluator = coco_tools.COCOEvalWrapper(\n            coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n        box_metrics, box_per_category_ap = box_evaluator.ComputeMetrics(\n            include_metrics_per_category=False,\n            all_metrics_per_category=False)\n        box_metrics.update(box_per_category_ap)\n        box_metrics = {\'DetectionBoxes_\'+ key: value\n                       for key, value in iter(box_metrics.items())}\n        return box_metrics\n    \n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/coco_label_map.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\ncategory_map = {\n    1: \'person\',\n    2: \'bicycle\',\n    3: \'car\',\n    4: \'motorcycle\',\n    5: \'airplane\',\n    6: \'bus\',\n    7: \'train\',\n    8: \'truck\',\n    9: \'boat\',\n    10: \'traffic light\',\n    11: \'fire hydrant\',\n    13: \'stop sign\',\n    14: \'parking meter\',\n    15: \'bench\',\n    16: \'bird\',\n    17: \'cat\',\n    18: \'dog\',\n    19: \'horse\',\n    20: \'sheep\',\n    21: \'cow\',\n    22: \'elephant\',\n    23: \'bear\',\n    24: \'zebra\',\n    25: \'giraffe\',\n    27: \'backpack\',\n    28: \'umbrella\',\n    31: \'handbag\',\n    32: \'tie\',\n    33: \'suitcase\',\n    34: \'frisbee\',\n    35: \'skis\',\n    36: \'snowboard\',\n    37: \'sports ball\',\n    38: \'kite\',\n    39: \'baseball bat\',\n    40: \'baseball glove\',\n    41: \'skateboard\',\n    42: \'surfboard\',\n    43: \'tennis racket\',\n    44: \'bottle\',\n    46: \'wine glass\',\n    47: \'cup\',\n    48: \'fork\',\n    49: \'knife\',\n    50: \'spoon\',\n    51: \'bowl\',\n    52: \'banana\',\n    53: \'apple\',\n    54: \'sandwich\',\n    55: \'orange\',\n    56: \'broccoli\',\n    57: \'carrot\',\n    58: \'hot dog\',\n    59: \'pizza\',\n    60: \'donut\',\n    61: \'cake\',\n    62: \'chair\',\n    63: \'couch\',\n    64: \'potted plant\',\n    65: \'bed\',\n    67: \'dining table\',\n    70: \'toilet\',\n    72: \'tv\',\n    73: \'laptop\',\n    74: \'mouse\',\n    75: \'remote\',\n    76: \'keyboard\',\n    77: \'cell phone\',\n    78: \'microwave\',\n    79: \'oven\',\n    80: \'toaster\',\n    81: \'sink\',\n    82: \'refrigerator\',\n    84: \'book\',\n    85: \'clock\',\n    86: \'vase\',\n    87: \'scissors\',\n    88: \'teddy bear\',\n    89: \'hair drier\',\n    90: \'toothbrush\'\n}\n\n\n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/coco_tools.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Wrappers for third party pycocotools to be used within object_detection.\n\nNote that nothing in this file is tensorflow related and thus cannot\nbe called directly as a slim metric, for example.\n\nTODO(jonathanhuang): wrap as a slim metric in metrics.py\n\n\nUsage example: given a set of images with ids in the list image_ids\nand corresponding lists of numpy arrays encoding groundtruth (boxes and classes)\nand detections (boxes, scores and classes), where elements of each list\ncorrespond to detections/annotations of a single image,\nthen evaluation (in multi-class mode) can be invoked as follows:\n\n  groundtruth_dict = coco_tools.ExportGroundtruthToCOCO(\n      image_ids, groundtruth_boxes_list, groundtruth_classes_list,\n      max_num_classes, output_path=None)\n  detections_list = coco_tools.ExportDetectionsToCOCO(\n      image_ids, detection_boxes_list, detection_scores_list,\n      detection_classes_list, output_path=None)\n  groundtruth = coco_tools.COCOWrapper(groundtruth_dict)\n  detections = groundtruth.LoadAnnotations(detections_list)\n  evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections,\n                                         agnostic_mode=False)\n  metrics = evaluator.ComputeMetrics()\n\n""""""\nfrom collections import OrderedDict\nimport copy\nimport time\nimport numpy as np\n\nfrom pycocotools import coco\nfrom pycocotools import cocoeval\nfrom pycocotools import mask\n\nimport tensorflow as tf\n\n\nclass COCOWrapper(coco.COCO):\n  """"""Wrapper for the pycocotools COCO class.""""""\n\n  def __init__(self, dataset, detection_type=\'bbox\'):\n    """"""COCOWrapper constructor.\n\n    See http://mscoco.org/dataset/#format for a description of the format.\n    By default, the coco.COCO class constructor reads from a JSON file.\n    This function duplicates the same behavior but loads from a dictionary,\n    allowing us to perform evaluation without writing to external storage.\n\n    Args:\n      dataset: a dictionary holding bounding box annotations in the COCO format.\n      detection_type: type of detections being wrapped. Can be one of [\'bbox\',\n        \'segmentation\']\n\n    Raises:\n      ValueError: if detection_type is unsupported.\n    """"""\n    supported_detection_types = [\'bbox\', \'segmentation\']\n    if detection_type not in supported_detection_types:\n      raise ValueError(\'Unsupported detection type: {}. \'\n                       \'Supported values are: {}\'.format(\n                           detection_type, supported_detection_types))\n    self._detection_type = detection_type\n    coco.COCO.__init__(self)\n    self.dataset = dataset\n    self.createIndex()\n\n  def LoadAnnotations(self, annotations):\n    """"""Load annotations dictionary into COCO datastructure.\n\n    See http://mscoco.org/dataset/#format for a description of the annotations\n    format.  As above, this function replicates the default behavior of the API\n    but does not require writing to external storage.\n\n    Args:\n      annotations: python list holding object detection results where each\n        detection is encoded as a dict with required keys [\'image_id\',\n        \'category_id\', \'score\'] and one of [\'bbox\', \'segmentation\'] based on\n        `detection_type`.\n\n    Returns:\n      a coco.COCO datastructure holding object detection annotations results\n\n    Raises:\n      ValueError: if annotations is not a list\n      ValueError: if annotations do not correspond to the images contained\n        in self.\n    """"""\n    results = coco.COCO()\n    results.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n    tf.compat.v1.logging.info(\'Loading and preparing annotation results...\')\n    tic = time.time()\n\n    if not isinstance(annotations, list):\n      raise ValueError(\'annotations is not a list of objects\')\n    annotation_img_ids = [ann[\'image_id\'] for ann in annotations]\n    if (set(annotation_img_ids) != (set(annotation_img_ids)\n                                    & set(self.getImgIds()))):\n      raise ValueError(\'Results do not correspond to current coco set\')\n    results.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n    if self._detection_type == \'bbox\':\n      for idx, ann in enumerate(annotations):\n        bb = ann[\'bbox\']\n        ann[\'area\'] = bb[2] * bb[3]\n        ann[\'id\'] = idx + 1\n        ann[\'iscrowd\'] = 0\n    elif self._detection_type == \'segmentation\':\n      for idx, ann in enumerate(annotations):\n        ann[\'area\'] = mask.area(ann[\'segmentation\'])\n        ann[\'bbox\'] = mask.toBbox(ann[\'segmentation\'])\n        ann[\'id\'] = idx + 1\n        ann[\'iscrowd\'] = 0\n    tf.compat.v1.logging.info(\'DONE (t=%0.2fs)\', (time.time() - tic))\n\n    results.dataset[\'annotations\'] = annotations\n    results.createIndex()\n    return results\n\n\nclass COCOEvalWrapper(cocoeval.COCOeval):\n  """"""Wrapper for the pycocotools COCOeval class.\n\n  To evaluate, create two objects (groundtruth_dict and detections_list)\n  using the conventions listed at http://mscoco.org/dataset/#format.\n  Then call evaluation as follows:\n\n    groundtruth = coco_tools.COCOWrapper(groundtruth_dict)\n    detections = groundtruth.LoadAnnotations(detections_list)\n    evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections,\n                                           agnostic_mode=False)\n\n    metrics = evaluator.ComputeMetrics()\n  """"""\n\n  def __init__(self, groundtruth=None, detections=None, agnostic_mode=False,\n               iou_type=\'bbox\'):\n    """"""COCOEvalWrapper constructor.\n\n    Note that for the area-based metrics to be meaningful, detection and\n    groundtruth boxes must be in image coordinates measured in pixels.\n\n    Args:\n      groundtruth: a coco.COCO (or coco_tools.COCOWrapper) object holding\n        groundtruth annotations\n      detections: a coco.COCO (or coco_tools.COCOWrapper) object holding\n        detections\n      agnostic_mode: boolean (default: False).  If True, evaluation ignores\n        class labels, treating all detections as proposals.\n      iou_type: IOU type to use for evaluation. Supports `bbox` or `segm`.\n    """"""\n    cocoeval.COCOeval.__init__(self, groundtruth, detections,\n                               iouType=iou_type)\n    if agnostic_mode:\n      self.params.useCats = 0\n\n  def GetCategory(self, category_id):\n    """"""Fetches dictionary holding category information given category id.\n\n    Args:\n      category_id: integer id\n    Returns:\n      dictionary holding \'id\', \'name\'.\n    """"""\n    return self.cocoGt.cats[category_id]\n\n  def GetAgnosticMode(self):\n    """"""Returns true if COCO Eval is configured to evaluate in agnostic mode.""""""\n    return self.params.useCats == 0\n\n  def GetCategoryIdList(self):\n    """"""Returns list of valid category ids.""""""\n    return self.params.catIds\n\n  def ComputeMetrics(self,\n                     include_metrics_per_category=False,\n                     all_metrics_per_category=False):\n    """"""Computes detection metrics.\n\n    Args:\n      include_metrics_per_category: If True, will include metrics per category.\n      all_metrics_per_category: If true, include all the summery metrics for\n        each category in per_category_ap. Be careful with setting it to true if\n        you have more than handful of categories, because it will pollute\n        your mldash.\n\n    Returns:\n      1. summary_metrics: a dictionary holding:\n        \'Precision/mAP\': mean average precision over classes averaged over IOU\n          thresholds ranging from .5 to .95 with .05 increments\n        \'Precision/mAP@.50IOU\': mean average precision at 50% IOU\n        \'Precision/mAP@.75IOU\': mean average precision at 75% IOU\n        \'Precision/mAP (small)\': mean average precision for small objects\n                        (area < 32^2 pixels)\n        \'Precision/mAP (medium)\': mean average precision for medium sized\n                        objects (32^2 pixels < area < 96^2 pixels)\n        \'Precision/mAP (large)\': mean average precision for large objects\n                        (96^2 pixels < area < 10000^2 pixels)\n        \'Recall/AR@1\': average recall with 1 detection\n        \'Recall/AR@10\': average recall with 10 detections\n        \'Recall/AR@100\': average recall with 100 detections\n        \'Recall/AR@100 (small)\': average recall for small objects with 100\n          detections\n        \'Recall/AR@100 (medium)\': average recall for medium objects with 100\n          detections\n        \'Recall/AR@100 (large)\': average recall for large objects with 100\n          detections\n      2. per_category_ap: a dictionary holding category specific results with\n        keys of the form: \'Precision mAP ByCategory/category\'\n        (without the supercategory part if no supercategories exist).\n        For backward compatibility \'PerformanceByCategory\' is included in the\n        output regardless of all_metrics_per_category.\n        If evaluating class-agnostic mode, per_category_ap is an empty\n        dictionary.\n\n    Raises:\n      ValueError: If category_stats does not exist.\n    """"""\n    self.evaluate()\n    self.accumulate()\n    self.summarize()\n\n    summary_metrics = OrderedDict([\n        (\'Precision/mAP\', self.stats[0]),\n        (\'Precision/mAP@.50IOU\', self.stats[1]),\n        (\'Precision/mAP@.75IOU\', self.stats[2]),\n        (\'Precision/mAP (small)\', self.stats[3]),\n        (\'Precision/mAP (medium)\', self.stats[4]),\n        (\'Precision/mAP (large)\', self.stats[5]),\n        (\'Recall/AR@1\', self.stats[6]),\n        (\'Recall/AR@10\', self.stats[7]),\n        (\'Recall/AR@100\', self.stats[8]),\n        (\'Recall/AR@100 (small)\', self.stats[9]),\n        (\'Recall/AR@100 (medium)\', self.stats[10]),\n        (\'Recall/AR@100 (large)\', self.stats[11])\n    ])\n    if not include_metrics_per_category:\n      return summary_metrics, {}\n    if not hasattr(self, \'category_stats\'):\n      raise ValueError(\'Category stats do not exist\')\n    per_category_ap = OrderedDict([])\n    if self.GetAgnosticMode():\n      return summary_metrics, per_category_ap\n    for category_index, category_id in enumerate(self.GetCategoryIdList()):\n      category = self.GetCategory(category_id)[\'name\']\n      # Kept for backward compatilbility\n      per_category_ap[\'PerformanceByCategory/mAP/{}\'.format(\n          category)] = self.category_stats[0][category_index]\n      if all_metrics_per_category:\n        per_category_ap[\'Precision mAP ByCategory/{}\'.format(\n            category)] = self.category_stats[0][category_index]\n        per_category_ap[\'Precision mAP@.50IOU ByCategory/{}\'.format(\n            category)] = self.category_stats[1][category_index]\n        per_category_ap[\'Precision mAP@.75IOU ByCategory/{}\'.format(\n            category)] = self.category_stats[2][category_index]\n        per_category_ap[\'Precision mAP (small) ByCategory/{}\'.format(\n            category)] = self.category_stats[3][category_index]\n        per_category_ap[\'Precision mAP (medium) ByCategory/{}\'.format(\n            category)] = self.category_stats[4][category_index]\n        per_category_ap[\'Precision mAP (large) ByCategory/{}\'.format(\n            category)] = self.category_stats[5][category_index]\n        per_category_ap[\'Recall AR@1 ByCategory/{}\'.format(\n            category)] = self.category_stats[6][category_index]\n        per_category_ap[\'Recall AR@10 ByCategory/{}\'.format(\n            category)] = self.category_stats[7][category_index]\n        per_category_ap[\'Recall AR@100 ByCategory/{}\'.format(\n            category)] = self.category_stats[8][category_index]\n        per_category_ap[\'Recall AR@100 (small) ByCategory/{}\'.format(\n            category)] = self.category_stats[9][category_index]\n        per_category_ap[\'Recall AR@100 (medium) ByCategory/{}\'.format(\n            category)] = self.category_stats[10][category_index]\n        per_category_ap[\'Recall AR@100 (large) ByCategory/{}\'.format(\n            category)] = self.category_stats[11][category_index]\n\n    return summary_metrics, per_category_ap\n\n\ndef _ConvertBoxToCOCOFormat(box):\n  """"""Converts a box in [ymin, xmin, ymax, xmax] format to COCO format.\n\n  This is a utility function for converting from our internal\n  [ymin, xmin, ymax, xmax] convention to the convention used by the COCO API\n  i.e., [xmin, ymin, width, height].\n\n  Args:\n    box: a [ymin, xmin, ymax, xmax] numpy array\n\n  Returns:\n    a list of floats representing [xmin, ymin, width, height]\n  """"""\n  return [float(box[1]), float(box[0]), float(box[3] - box[1]),\n          float(box[2] - box[0])]\n\n\ndef _RleCompress(masks):\n  """"""Compresses mask using Run-length encoding provided by pycocotools.\n\n  Args:\n    masks: uint8 numpy array of shape [mask_height, mask_width] with values in\n    {0, 1}.\n\n  Returns:\n    A pycocotools Run-length encoding of the mask.\n  """"""\n  return mask.encode(np.asfortranarray(masks))\n\n\ndef ExportSingleImageGroundtruthToCoco(image_id,\n                                       next_annotation_id,\n                                       category_id_set,\n                                       groundtruth_boxes,\n                                       groundtruth_classes,\n                                       groundtruth_masks=None,\n                                       groundtruth_is_crowd=None):\n  """"""Export groundtruth of a single image to COCO format.\n\n  This function converts groundtruth detection annotations represented as numpy\n  arrays to dictionaries that can be ingested by the COCO evaluation API. Note\n  that the image_ids provided here must match the ones given to\n  ExportSingleImageDetectionsToCoco. We assume that boxes and classes are in\n  correspondence - that is: groundtruth_boxes[i, :], and\n  groundtruth_classes[i] are associated with the same groundtruth annotation.\n\n  In the exported result, ""area"" fields are always set to the area of the\n  groundtruth bounding box.\n\n  Args:\n    image_id: a unique image identifier either of type integer or string.\n    next_annotation_id: integer specifying the first id to use for the\n      groundtruth annotations. All annotations are assigned a continuous integer\n      id starting from this value.\n    category_id_set: A set of valid class ids. Groundtruth with classes not in\n      category_id_set are dropped.\n    groundtruth_boxes: numpy array (float32) with shape [num_gt_boxes, 4]\n    groundtruth_classes: numpy array (int) with shape [num_gt_boxes]\n    groundtruth_masks: optional uint8 numpy array of shape [num_detections,\n      image_height, image_width] containing detection_masks.\n    groundtruth_is_crowd: optional numpy array (int) with shape [num_gt_boxes]\n      indicating whether groundtruth boxes are crowd.\n\n  Returns:\n    a list of groundtruth annotations for a single image in the COCO format.\n\n  Raises:\n    ValueError: if (1) groundtruth_boxes and groundtruth_classes do not have the\n      right lengths or (2) if each of the elements inside these lists do not\n      have the correct shapes or (3) if image_ids are not integers\n  """"""\n\n  if len(groundtruth_classes.shape) != 1:\n    raise ValueError(\'groundtruth_classes is \'\n                     \'expected to be of rank 1.\')\n  if len(groundtruth_boxes.shape) != 2:\n    raise ValueError(\'groundtruth_boxes is expected to be of \'\n                     \'rank 2.\')\n  if groundtruth_boxes.shape[1] != 4:\n    raise ValueError(\'groundtruth_boxes should have \'\n                     \'shape[1] == 4.\')\n  num_boxes = groundtruth_classes.shape[0]\n  if num_boxes != groundtruth_boxes.shape[0]:\n    raise ValueError(\'Corresponding entries in groundtruth_classes, \'\n                     \'and groundtruth_boxes should have \'\n                     \'compatible shapes (i.e., agree on the 0th dimension).\'\n                     \'Classes shape: %d. Boxes shape: %d. Image ID: %s\' % (\n                         groundtruth_classes.shape[0],\n                         groundtruth_boxes.shape[0], image_id))\n  has_is_crowd = groundtruth_is_crowd is not None\n  if has_is_crowd and len(groundtruth_is_crowd.shape) != 1:\n    raise ValueError(\'groundtruth_is_crowd is expected to be of rank 1.\')\n  groundtruth_list = []\n  for i in range(num_boxes):\n    if groundtruth_classes[i] in category_id_set:\n      iscrowd = groundtruth_is_crowd[i] if has_is_crowd else 0\n      export_dict = {\n          \'id\':\n              next_annotation_id + i,\n          \'image_id\':\n              image_id,\n          \'category_id\':\n              int(groundtruth_classes[i]),\n          \'bbox\':\n              list(_ConvertBoxToCOCOFormat(groundtruth_boxes[i, :])),\n          \'area\':\n              float((groundtruth_boxes[i, 2] - groundtruth_boxes[i, 0]) *\n                    (groundtruth_boxes[i, 3] - groundtruth_boxes[i, 1])),\n          \'iscrowd\':\n              iscrowd\n      }\n      if groundtruth_masks is not None:\n        export_dict[\'segmentation\'] = _RleCompress(groundtruth_masks[i])\n      groundtruth_list.append(export_dict)\n  return groundtruth_list\n\n\ndef ExportSingleImageDetectionBoxesToCoco(image_id,\n                                          category_id_set,\n                                          detection_boxes,\n                                          detection_scores,\n                                          detection_classes):\n  """"""Export detections of a single image to COCO format.\n\n  This function converts detections represented as numpy arrays to dictionaries\n  that can be ingested by the COCO evaluation API. Note that the image_ids\n  provided here must match the ones given to the\n  ExporSingleImageDetectionBoxesToCoco. We assume that boxes, and classes are in\n  correspondence - that is: boxes[i, :], and classes[i]\n  are associated with the same groundtruth annotation.\n\n  Args:\n    image_id: unique image identifier either of type integer or string.\n    category_id_set: A set of valid class ids. Detections with classes not in\n      category_id_set are dropped.\n    detection_boxes: float numpy array of shape [num_detections, 4] containing\n      detection boxes.\n    detection_scores: float numpy array of shape [num_detections] containing\n      scored for the detection boxes.\n    detection_classes: integer numpy array of shape [num_detections] containing\n      the classes for detection boxes.\n\n  Returns:\n    a list of detection annotations for a single image in the COCO format.\n\n  Raises:\n    ValueError: if (1) detection_boxes, detection_scores and detection_classes\n      do not have the right lengths or (2) if each of the elements inside these\n      lists do not have the correct shapes or (3) if image_ids are not integers.\n  """"""\n\n  if len(detection_classes.shape) != 1 or len(detection_scores.shape) != 1:\n    raise ValueError(\'All entries in detection_classes and detection_scores\'\n                     \'expected to be of rank 1.\')\n  if len(detection_boxes.shape) != 2:\n    raise ValueError(\'All entries in detection_boxes expected to be of \'\n                     \'rank 2.\')\n  if detection_boxes.shape[1] != 4:\n    raise ValueError(\'All entries in detection_boxes should have \'\n                     \'shape[1] == 4.\')\n  num_boxes = detection_classes.shape[0]\n  if not num_boxes == detection_boxes.shape[0] == detection_scores.shape[0]:\n    raise ValueError(\'Corresponding entries in detection_classes, \'\n                     \'detection_scores and detection_boxes should have \'\n                     \'compatible shapes (i.e., agree on the 0th dimension). \'\n                     \'Classes shape: %d. Boxes shape: %d. \'\n                     \'Scores shape: %d\' % (\n                         detection_classes.shape[0], detection_boxes.shape[0],\n                         detection_scores.shape[0]\n                     ))\n  detections_list = []\n  for i in range(num_boxes):\n    if detection_classes[i] in category_id_set:\n      detections_list.append({\n          \'image_id\': image_id,\n          \'category_id\': int(detection_classes[i]),\n          \'bbox\': list(_ConvertBoxToCOCOFormat(detection_boxes[i, :])),\n          \'score\': float(detection_scores[i])\n      })\n  return detections_list\n\n\ndef ExportSingleImageDetectionMasksToCoco(image_id,\n                                          category_id_set,\n                                          detection_masks,\n                                          detection_scores,\n                                          detection_classes):\n  """"""Export detection masks of a single image to COCO format.\n\n  This function converts detections represented as numpy arrays to dictionaries\n  that can be ingested by the COCO evaluation API. We assume that\n  detection_masks, detection_scores, and detection_classes are in correspondence\n  - that is: detection_masks[i, :], detection_classes[i] and detection_scores[i]\n    are associated with the same annotation.\n\n  Args:\n    image_id: unique image identifier either of type integer or string.\n    category_id_set: A set of valid class ids. Detections with classes not in\n      category_id_set are dropped.\n    detection_masks: uint8 numpy array of shape [num_detections, image_height,\n      image_width] containing detection_masks.\n    detection_scores: float numpy array of shape [num_detections] containing\n      scores for detection masks.\n    detection_classes: integer numpy array of shape [num_detections] containing\n      the classes for detection masks.\n\n  Returns:\n    a list of detection mask annotations for a single image in the COCO format.\n\n  Raises:\n    ValueError: if (1) detection_masks, detection_scores and detection_classes\n      do not have the right lengths or (2) if each of the elements inside these\n      lists do not have the correct shapes or (3) if image_ids are not integers.\n  """"""\n\n  if len(detection_classes.shape) != 1 or len(detection_scores.shape) != 1:\n    raise ValueError(\'All entries in detection_classes and detection_scores\'\n                     \'expected to be of rank 1.\')\n  num_boxes = detection_classes.shape[0]\n  if not num_boxes == len(detection_masks) == detection_scores.shape[0]:\n    raise ValueError(\'Corresponding entries in detection_classes, \'\n                     \'detection_scores and detection_masks should have \'\n                     \'compatible lengths and shapes \'\n                     \'Classes length: %d.  Masks length: %d. \'\n                     \'Scores length: %d\' % (\n                         detection_classes.shape[0], len(detection_masks),\n                         detection_scores.shape[0]\n                     ))\n  detections_list = []\n  for i in range(num_boxes):\n    if detection_classes[i] in category_id_set:\n      detections_list.append({\n          \'image_id\': image_id,\n          \'category_id\': int(detection_classes[i]),\n          \'segmentation\': _RleCompress(detection_masks[i]),\n          \'score\': float(detection_scores[i])\n      })\n  return detections_list\n'"
models/object_detection/tensorflow/ssd-resnet34/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/recommendation/tensorflow/wide_deep/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/recommendation/tensorflow/wide_deep_large_ds/dataset/featurecolumn_graph_optimization.py,19,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\'\'\'This script optimizes feature columns in the model by removing error handling\nand redundant nodes. Flag wide_and_deep_large_ds should be enabled for the additional\noptimization for wide_and_deep_large_ds_model which involves fusion of categorical \nand numeric columns\'\'\'\n\nfrom __future__ import division\nimport os\nimport sys\nimport tensorflow as tf\nimport argparse\nimport numpy as np\nfrom google.protobuf import text_format\nfrom tensorflow.python.framework import graph_util, ops, graph_io\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.client import session\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--input-graph\', type=str,\n                    help=\'full path of graph to be optimized\',\n                    dest=\'input_graph\', required=True)\nparser.add_argument(\'--output-graph\', type=str,\n                    help=\'name of optimized graph\',\n                    dest=\'output_graph\', required=True)\nparser.add_argument(\'--output-nodes\', type=str,\n                    help=\'Comma seperated list of ouput nodes: head/predictions/logistic,   \\\n                    init_all_tables\', dest=\'output_nodes\', required=True)\nparser.add_argument(\'--wide_and_deep_large_ds\', type=bool,\n                    help=\'Enable this flag when optimizing wide_and_deep_large_ds model,    \\\n                    to fuse categorical,numeric columns\',\n                    dest=\'enable_column_fusion\', default=False)\nargs = parser.parse_args()\noutput_nodes = args.output_nodes.split("","")\noutput_nodes = [""import/""+str(i) for i in output_nodes]\ngraph = ops.Graph()\ngraph_def = graph_pb2.GraphDef()\nold_graph_def = graph_pb2.GraphDef()\nold_graph_def1 = graph_pb2.GraphDef()\nfile_ext = os.path.splitext(args.input_graph)[1]\nwith open(args.input_graph, ""rb"") as f:\n    if file_ext == "".pbtxt"":\n        text_format.Merge(f.read(), graph_def)\n    else:\n        graph_def.ParseFromString(f.read())\ncategorical_column_weights_list, embedding_column_weights_list = [], []\nwith graph.as_default():\n    tf.import_graph_def(graph_def)\n    old_graph_def = graph.as_graph_def()\n    name_node_dict = dict()\n    #This method optimizes tf.embedding_column and tf.categorical_column_with_hash_bucket\n    def optimize_categorical_embedding_with_hash_bucket(nodename, gatherfound):\n        if \':\' in nodename:\n            nodename = nodename.split(\':\')[0]\n        node = name_node_dict[nodename]\n        if gatherfound[0] == 1 and node.op == ""StringToHashBucketFast"":\n            return node.name\n        if node.op == ""GatherV2"" and ""Unique"" in node.input[1]:\n            gatherfound[0] = 1\n            res = optimize_categorical_embedding_with_hash_bucket(node.input[1], gatherfound)\n            if res:\n                node.input[1] = res\n                if ""embedding"" in node.input[0]:\n                    embedding_column_weights_list.append(node.input[0]+"":0"")\n                else:\n                    categorical_column_weights_list.append(node.input[0]+"":0"")\n                return node.name\n        for inputname in node.input:\n            res = optimize_categorical_embedding_with_hash_bucket(inputname, gatherfound)\n            if res:\n                return res\n        return None\n\n    #This method optimizes tf.feature_column.bucketized_column\n    def optimize_bucketized_column(nodename, gatherfound):\n        if \':\' in nodename:\n            nodename = nodename.split(\':\')[0]\n        node = name_node_dict[nodename]\n        if gatherfound[0] == 1 and node.op == ""Reshape"" and ""Bucketize"" in node.input[0]:\n            return node.name\n        if node.op == ""GatherV2"" and ""Unique"" in node.input[1]:\n            gatherfound[0] = 1\n            res = optimize_bucketized_column(node.input[1], gatherfound)\n            if res:\n                node.input[1] = res\n                return node.name\n        for inputname in node.input:\n            res = optimize_bucketized_column(inputname, gatherfound)\n            if res:\n                return res\n        return None\n\n    #This method optimizes tf.feature_column.crossed_column\n    def optimize_crossed_column(nodename, gatherfound):\n        if \':\' in nodename:\n            nodename = nodename.split(\':\')[0]\n        node = name_node_dict[nodename]\n        if gatherfound[0] == 1 and node.op == ""Identity"" and ""SparseCross"" in node.input[0]:\n            return node.name\n        elif gatherfound[0] == 1 and node.op == ""Identity"" and \\\n                (""hash_table_Lookup"" in node.input[0] or ""lookup"" in node.input[0]):\n            return node.name\n        elif gatherfound[0] == 2 and node.op == ""SparseFillEmptyRows"" and \\\n                ""GatherV2"" in node.input[0]:\n            return node.name\n        elif gatherfound[0] == 2 and node.op == ""GatherV2"" and ""Unique"" in node.input[1] and \\\n                 ""Identity"" not in node.input[0]:\n            res = optimize_crossed_column(node.input[1], gatherfound)\n            if res:\n                node.input[1] = res+"":1""\n                return node.name\n        if  gatherfound[0] != 2 and node.op == ""GatherV2"" and ""Unique"" in node.input[1]:\n            gatherfound[0] = 1\n            res = optimize_crossed_column(node.input[1], gatherfound)\n            if res:\n                node.input[1] = res\n                return node.name\n        elif  gatherfound[0] == 2 and node.op == ""Mul"" and ""GatherV2"" in node.input[0]:\n            res = optimize_crossed_column(node.input[0], gatherfound)\n            if res:\n                node.input[0] = res\n                return node.name\n        elif node.op == ""SegmentSum"" and ""mul"" in node.input[0]:\n            gatherfound[0] = 2\n            res = optimize_crossed_column(node.input[0], gatherfound)\n            return node.name\n        for inputname in node.input:\n            res = optimize_crossed_column(inputname, gatherfound)\n            if res:\n                return res\n        return None\n\n    # This method optimizes tf.feature_column.categorical_column_with_identity\n    def optimize_categorical_column_with_identity(nodename, gatherfound):\n        if \':\' in nodename:\n            nodename = nodename.split(\':\')[0]\n        node = name_node_dict[nodename]\n        if gatherfound[0] == 1 and node.op == ""LookupTableFindV2"":\n            return node.name\n        if node.op == ""GatherV2"" and ""Unique"" in node.input[1]:\n            gatherfound[0] = 1\n            res = optimize_categorical_column_with_identity(node.input[1], gatherfound)\n            if res:\n                node.input[1] = res\n                return node.name\n        for inputname in node.input:\n            res = optimize_categorical_column_with_identity(inputname, gatherfound)\n            if res:\n                return res\n        return None\n\n    # This method optimizes tf.feature_column.categorical_column_with_vocabulary_list\n    def optimize_categorical_with_voc_list(nodename, gatherfound):\n        if \':\' in nodename:\n            nodename = nodename.split(\':\')[0]\n        node = name_node_dict[nodename]\n        if gatherfound[0] == 1 and node.op == ""Select"" and ""Add"" in node.input[2] and \\\n                ""hash_table_Lookup"" in node.input[1]:\n            return node.name\n        if node.op == ""GatherV2"" and ""Unique"" in node.input[1]:\n            gatherfound[0] = 1\n            res = optimize_categorical_with_voc_list(node.input[1], gatherfound)\n            if res:\n                node.input[1] = res\n                return node.name\n        for inputname in node.input:\n            res = optimize_categorical_with_voc_list(inputname, gatherfound)\n            if res:\n                return res\n        return None\n\n    #This method optimizes tf.feature_column.numeric_column\n    def optimize_numeric(nodename):\n        if \':\' in nodename:\n            nodename = nodename.split(\':\')[0]\n        node = name_node_dict[nodename]\n        if node.op == ""Reshape"" and (""ParseExample"" in node.input[0] or ""div"" in node.input[0]):\n            return node.input[0]\n        elif node.op == ""Reshape"" and ""Maximum"" in node.input[0]:\n            return node.input[0]\n\n    \'\'\'This method does model specific optimization(wide_deep_large_ds). It fuses 26 categorical,\n        embedding weights to one constant and expects fused normalized inputs to the \n        numeric and hashed inputs to categorical placeholders. It also replaces gatherv2 \n        with gathernd to gather weights from fused weights constant\'\'\'\n    def fuse_categorical_numeric_columns():\n        new_categorical_placeholder = tf.compat.v1.placeholder(tf.int64, shape=(None, None),\n                                                     name=\'new_categorical_placeholder\')\n        new_numeric_placeholder = tf.compat.v1.placeholder(tf.float32,\n                                                 shape=(None, None),\n                                                 name=\'new_numeric_placeholder\')\n        categorical_column_weights_list.sort()\n        embedding_column_weights_list.sort()\n        sess = session.Session()\n        categorical_weights_constant, embedding_weights_constant = [], []\n        list_of_indices = [i for i in range(1, 11)]+[0] + \\\n                          [i for i in range(12, 19)]+[11] + \\\n                          [i for i in range(19, 26)]\n        with sess.as_default():\n            for i in list_of_indices:\n                weight = graph.get_tensor_by_name(categorical_column_weights_list[i])\n                categorical_weights_constant.append(weight.eval())\n            for i in embedding_column_weights_list:\n                weight = graph.get_tensor_by_name(i)\n                embedding_weights_constant.append(weight.eval())\n        fused_categorical_weights_const = np.stack(categorical_weights_constant)\n        full_weights_constant_categorical = tf.constant(fused_categorical_weights_const,\n                                                        name=\'full_weights_constant_categorical\')\n        batch_gather_op = tf.gather_nd(full_weights_constant_categorical,\n                                       new_categorical_placeholder,\n                                       name=\'gather_categorical_weights\')\n        reshape_result = tf.reshape(batch_gather_op, shape=[-1, 26])\n        reduce_sum_op = tf.reduce_sum(reshape_result, 1, keepdims=True)\n        fused_embedding_weights_const = np.stack(embedding_weights_constant)\n        full_weights_constant_embedding = tf.constant(fused_embedding_weights_const,\n                                                      name=\'full_weights_constant_embedding\')\n        batch_gather_op_embedding = tf.gather_nd(full_weights_constant_embedding,\n                                                 new_categorical_placeholder,\n                                                 name=\'gather_embedding_weights\')\n        embedding_reshape = tf.reshape(batch_gather_op_embedding,\n                                       shape=[-1, 32*26],\n                                       name=\'embedding_reshape\')\n        real_div_input_tens_list = [embedding_reshape, new_numeric_placeholder]\n        new_concat_node = tf.concat(real_div_input_tens_list, name=\'new_concat_node\', axis=1)\n        concat_tensor = graph.get_tensor_by_name(""new_concat_node:0"")\n\n\n    \'\'\'Parsing all the nodes of graph and identifying feature columns to optimize \'\'\'\n    for node in old_graph_def.node:\n        nodename = node.name\n        if node.op == ""ConcatV2"" and ""dnn/input_from_feature_columns"" in nodename and \\\n                       ""input_layer/concat"" in nodename:\n            dnn_concat_node = node\n        elif node.op == ""AddN"" and ""weighted_sum_no_bias"" in nodename:\n            weightsumnobias_node = node\n        name_node_dict[nodename] = node\n    gatherfound = [0]\n    try:\n        for i, inputname in enumerate(weightsumnobias_node.input):\n            if  \'weighted_by\' not in inputname and \'_X_\' not in inputname:\n                gatherfound[0] = 0\n                res = optimize_categorical_with_voc_list(weightsumnobias_node.input[i], gatherfound)\n                if res:\n                    weightsumnobias_node.input[i] = res\n                else:\n                    gatherfound[0] = 0\n                    res = optimize_categorical_column_with_identity(weightsumnobias_node.input[i],\n                                                                    gatherfound)\n                    if res:\n                        weightsumnobias_node.input[i] = res\n                    else:\n                        gatherfound[0] = 0\n                        res = optimize_categorical_embedding_with_hash_bucket(\n                            weightsumnobias_node.input[i], gatherfound)\n                        if res:\n                            weightsumnobias_node.input[i] = res\n                        else:\n                            gatherfound[0] = 0\n                            res = optimize_bucketized_column(weightsumnobias_node.input[i], gatherfound)\n                            if res:\n                                weightsumnobias_node.input[i] = res\n            elif \'_X_\' in inputname or \'weighted_by\' in inputname:\n                gatherfound[0] = 0\n                res = optimize_crossed_column(weightsumnobias_node.input[i], gatherfound)\n                if res:\n                    weightsumnobias_node.input[i] = res\n\n        for i, inputname in enumerate(dnn_concat_node.input):\n            if \'_embedding\' in inputname and \'shared_embedding\' not in inputname \\\n                and \'weighted_by\' not in inputname and \'_X_\' not in inputname:\n                gatherfound[0] = 0\n                res = optimize_categorical_with_voc_list(dnn_concat_node.input[i], gatherfound)\n                if res:\n                    dnn_concat_node.input[i] = res\n                else:\n                    gatherfound[0] = 0\n                    res = optimize_categorical_column_with_identity(\n                        dnn_concat_node.input[i], gatherfound)\n                    if res:\n                        dnn_concat_node.input[i] = res\n                    else:\n                        gatherfound[0] = 0\n                        res = optimize_categorical_embedding_with_hash_bucket(\n                            dnn_concat_node.input[i], gatherfound)\n                        if res:\n                            dnn_concat_node.input[i] = res\n\n            elif \'shared_embedding\' not in inputname:\n                res2 = optimize_numeric(dnn_concat_node.input[i])\n                if res2:\n                    dnn_concat_node.input[i] = res2\n            else:\n                gatherfound[0] = 0\n                #shared_embedding\n                res = optimize_crossed_column(dnn_concat_node.input[i], gatherfound)\n                if res:\n                    dnn_concat_node.input[i] = res\n        if args.enable_column_fusion:\n            fuse_categorical_numeric_columns()\n            old_graph_def = graph.as_graph_def()\n            for node in old_graph_def.node:\n                if node.name == ""new_concat_node"":\n                    node.input[1] = ""new_numeric_placeholder:0""\n                elif node.op == ""BiasAdd"" and ""linear_model/weighted_sum"" in node.name:\n                    node.input[0] = ""Sum:0""\n                elif  node.op == ""MatMul"" and ""hiddenlayer_0/MatMul"" in node.name:\n                    node.input[0] = ""new_concat_node:0""\n    except Exception as e:\n        print(e)\n        print(\'--------------------------------------------------------------------------\')\n        print(""Cannot optimize the given graph. The given graph might be an optimized one"")\n        print(\'--------------------------------------------------------------------------\')\n        sys.exit()             \n\nnew_graph_def = tf.compat.v1.GraphDef()\nnew_graph_def = tf.compat.v1.graph_util.extract_sub_graph(\n    old_graph_def,\n    output_nodes\n)\n\nfilename = args.output_graph\ngraph_io.write_graph(new_graph_def,\n                     os.path.dirname(filename),\n                     os.path.basename(filename),\n                     as_text=False)\nprint(\'Optimized graph created\')\n'"
models/recommendation/tensorflow/wide_deep_large_ds/dataset/preprocess_csv_tfrecords.py,4,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport sys\nimport pandas\nimport argparse\nimport numpy as np\nimport tensorflow as tf\ntf.enable_eager_execution()\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--inputcsv-datafile\', type=str,\n                    help=\'full path of data file e.g. eval.csv\',\n                    dest=\'evaldatafile_path\',\n                    required=True)\nparser.add_argument(\'--calibrationcsv-datafile\', type=str,\n                    help=\'full path of data file of calibration/train dataset to get normalization ranges\',\n                    dest=\'traindatafile_path\',\n                    default=\'NULL\',\n                    required=False)\n\nparser.add_argument(\'--outputfile-name\', type=str,\n                    help=\'output tfrecord file name e.g. processed_eval.[tfrecords]\',\n                    dest=\'outputfile_path\',\n                    default=""processed_data.tfrecords"",\n                    required=False)\n\nargs = parser.parse_args()\n\neval_csv_file = args.evaldatafile_path\ntrain_csv_file = args.traindatafile_path\noutput_file = args.outputfile_path\n\nif not os.path.isfile(eval_csv_file):\n    print(""Please input a valid csv file"")\n    sys.exit(1)\n\nfilename, file_ext = os.path.splitext(output_file)\nin_filename, _ = os.path.splitext(os.path.basename(eval_csv_file))\n\nif file_ext != "".tfrecords"":\n    output_file = output_file + "".tfrecords""\n\noutput_file = ""{}_{}"".format(in_filename,output_file)\ncsv = pandas.read_csv(eval_csv_file, header=None)\nif len(csv.columns)==39:\n    dataset_type = \'test\'\nelse:\n    dataset_type = \'eval\'\nfill_na_dict  = {}\nif dataset_type==\'test\':\n    for i in range(0,13):\n        fill_na_dict[i]=0.0\n    for i in range(13,39):\n        fill_na_dict[i]=""""\nelse:\n    for i in range(1,14):\n        fill_na_dict[i]=0.0\n    for i in range(14,40):\n        fill_na_dict[i]=""""\ncsv=csv.fillna(value=fill_na_dict).values\nnumeric_feature_names = [""numeric_1""]\nstring_feature_names = [""string_1""]\nLABEL_COLUMN =[""clicked""]\nCATEGORICAL_COLUMNS1 = [""C""+str(i)+""_embedding"" for i in range(1, 27)]\nNUMERIC_COLUMNS1 = [""I""+str(i) for i in range(1, 14)]\nif dataset_type==\'eval\':\n    DATA_COLUMNS = LABEL_COLUMN + NUMERIC_COLUMNS1 + CATEGORICAL_COLUMNS1\nelse:\n    DATA_COLUMNS = NUMERIC_COLUMNS1 + CATEGORICAL_COLUMNS1\nCATEGORICAL_COLUMNS2 = [""C""+str(i)+""_embedding"" for i in range(1, 27)]\nNUMERIC_COLUMNS2 = [""I""+str(i) for i in range(1, 14)]\n\nCATEGORICAL_COLUMNS1.sort()\nNUMERIC_COLUMNS1.sort()\nno_of_rows = 0\nwith open(eval_csv_file, \'r\') as f:\n    if not os.path.isfile(train_csv_file):\n        nums=[line.strip(\'\\n\\r\').split(\',\') for line in f.readlines()]\n    else: \n        f1 = open(train_csv_file, \'r\')\n        nums=[line.strip(\'\\n\\r\').split(\',\') for line in f.readlines(\n        )]+[line.strip(\'\\n\\t\').split(\',\') for line in f1.readlines()]\n    numpy_arr = np.array(nums)\n    numpy_arr[numpy_arr==\'\']=\'0\'\n    min_list,max_list,range_list = [],[],[]\n    for i in range(len(DATA_COLUMNS)):\n        if DATA_COLUMNS[i] in NUMERIC_COLUMNS1:\n            col_min = numpy_arr[:,i].astype(np.float32).min()\n            col_max = numpy_arr[:,i].astype(np.float32).max()\n            min_list.append(col_min)\n            max_list.append(col_max)\n            range_list.append(col_max-col_min)\n    if os.path.isfile(train_csv_file):\n        f1.close()\n    print(\'min list\',min_list)\n    print(\'max list\',max_list)\n    print(\'range list\',range_list)\n\n\nwith tf.python_io.TFRecordWriter(output_file) as writer:\n    print(\'*****Processing data******\')\n    for row in csv:\n        no_of_rows = no_of_rows+1\n        if dataset_type == \'eval\':\n            unnormalized_vals = np.array(row[1:14])\n        else:\n            unnormalized_vals = np.array(row[0:13])\n        normalized_vals = (unnormalized_vals-min_list)/range_list\n        if dataset_type == \'eval\':\n            new_categorical_dict = dict(zip(CATEGORICAL_COLUMNS2, row[14:40]))\n        else:\n            new_categorical_dict = dict(zip(CATEGORICAL_COLUMNS2, row[13:39]))\n        new_categorical_list = []\n        for i in CATEGORICAL_COLUMNS1:\n            if pandas.isnull(new_categorical_dict[i]):\n                new_categorical_list.append("""")\n            else:\n                new_categorical_list.append(new_categorical_dict[i])\n        hash_values = tf.string_to_hash_bucket_fast(\n            new_categorical_list, 1000).numpy()\n        new_numerical_dict = dict(zip(NUMERIC_COLUMNS2, normalized_vals))\n        example = tf.train.Example()\n        for i in NUMERIC_COLUMNS1:\n            example.features.feature[numeric_feature_names[0]].float_list.value.extend([new_numerical_dict[i]])\n        for i in range(0, 26):\n            example.features.feature[string_feature_names[0]].int64_list.value.extend([i])\n            example.features.feature[string_feature_names[0]].int64_list.value.extend([hash_values[i]])\n        if dataset_type == \'eval\':\n            example.features.feature[""label""].int64_list.value.append(row[0])\n        writer.write(example.SerializeToString())\n\nprint(\'Total number of rows \', no_of_rows)\nprint(\'Generated output file name :\'+output_file)\n'"
models/recommendation/tensorflow/wide_deep_large_ds/inference/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/recommendation/tensorflow/wide_deep_large_ds/inference/inference.py,15,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nimport os\nimport numpy as np\nimport argparse\nimport collections\nimport time\nimport math\nimport json\nimport datetime\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import ops\nfrom tensorflow.core.framework import graph_pb2\nfrom google.protobuf import text_format\n\n\ndef str2bool(v):\n    if v.lower() in (\'true\'):\n        return True\n    else:\n        return False\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--input_graph\', type=str,\n                    help=\'file name for graph\',\n                    dest=\'input_graph\',\n                    required=True)\nparser.add_argument(\'--data_location\', type=str,\n                    help=\'full path of data file\',\n                    dest=\'data_location\',\n                    required=True)\nparser.add_argument(\'--batch_size\', type=int,\n                    help=\'batch size for inference.Default is 512\',\n                    default=512,\n                    dest=\'batch_size\')\nparser.add_argument(\'--num_intra_threads\', type=int,\n                    help=\'number of threads for an operator\',\n                    required=False,\n                    default=28,\n                    dest=\'num_intra_threads\')\nparser.add_argument(\'--num_inter_threads\', type=int,\n                    help=\'number of threads across operators\',\n                    required=False,\n                    default=2,\n                    dest=\'num_inter_threads\')\nparser.add_argument(\'--num_omp_threads\', type=str,\n                    help=\'number of threads to use\',\n                    required=False,\n                    default=None,\n                    dest=\'num_omp_threads\')\nparser.add_argument(""--accuracy_only"", type=str2bool,\n                    nargs=\'?\', const=True, default=False,\n                    dest=\'compute_accuracy\', required=False,\n                    help=""Enable accuracy calculation"")\n\nargs = parser.parse_args()\nif args.num_omp_threads:\n    os.environ[""OMP_NUM_THREADS""] = args.num_omp_threads\n\noutput_probabilities_node = \'import/import/head/predictions/probabilities\'\nprobabilities_node = \'import/\'+output_probabilities_node+\':0\'\nplaceholder_name = \'import/new_numeric_placeholder\'\ncategorical_placeholder = \'import/new_categorical_placeholder\'\n\nconfig = tf.compat.v1.ConfigProto(log_device_placement=False,\n                        inter_op_parallelism_threads=args.num_inter_threads,\n                        intra_op_parallelism_threads=args.num_intra_threads)\ngraph = ops.Graph()\ngraph_def = graph_pb2.GraphDef()\n\nfilename, file_ext = os.path.splitext(args.input_graph)\n\nbatch_size = args.batch_size\nwith open(args.input_graph, ""rb"") as f:\n    if file_ext == "".pbtxt"":\n        text_format.Merge(f.read(), graph_def)\n    else:\n        graph_def.ParseFromString(f.read())\nwith graph.as_default():\n  tf.import_graph_def(graph_def)\nnumeric_feature_names = [""numeric_1""]\nstring_feature_names = [""string_1""]\nif args.compute_accuracy:\n    full_features_names = numeric_feature_names + string_feature_names + [""label""]\n    feature_datatypes = [tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0, allow_missing=True)]+[tf.io.FixedLenSequenceFeature(\n            [], tf.int64, default_value=0, allow_missing=True)]+[tf.io.FixedLenSequenceFeature([], tf.int64, default_value=0, allow_missing=True)]\nelse:\n    full_features_names = numeric_feature_names + string_feature_names\n    feature_datatypes = [tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0, allow_missing=True)]+[tf.io.FixedLenSequenceFeature(\n            [], tf.int64, default_value=0, allow_missing=True)]\n\n\n\ndef input_fn(data_file, num_epochs, shuffle, batch_size):\n    """"""Generate an input function for the Estimator.""""""\n    def _parse_function(proto):\n        f = collections.OrderedDict(\n            zip(full_features_names, feature_datatypes))\n        parsed_features = tf.io.parse_example(proto, f)\n        parsed_feature_vals_num = [tf.reshape(\n            parsed_features[""numeric_1""], shape=[-1, 13])]\n        parsed_feature_vals_str = [tf.reshape(\n            parsed_features[""string_1""], shape=[-1, 2]) for i in string_feature_names]\n        parsed_feature_vals = parsed_feature_vals_num + parsed_feature_vals_str\n        if args.compute_accuracy:\n            parsed_feature_vals_label = [tf.reshape(parsed_features[i], shape=[-1]) for i in [""label""]]\n            parsed_feature_vals = parsed_feature_vals + parsed_feature_vals_label\n        return parsed_feature_vals\n\n    # Extract lines from input files using the Dataset API.\n    dataset = tf.data.TFRecordDataset([data_file])\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=20000)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(_parse_function, num_parallel_calls=28)\n    dataset = dataset.prefetch(batch_size*10)\n    return dataset\n\n\ndata_file = args.data_location\nno_of_test_samples = sum(1 for _ in tf.compat.v1.python_io.tf_record_iterator(data_file))\nno_of_batches = math.ceil(float(no_of_test_samples)/batch_size)\nplaceholder_list = [\'import/new_numeric_placeholder:0\',\'import/new_categorical_placeholder:0\']\ninput_tensor = [graph.get_tensor_by_name(name) for name in placeholder_list]\noutput_name = ""import/head/predictions/probabilities""\noutput_tensor = graph.get_tensor_by_name(""import/"" + output_name + "":0"" )\ncorrectly_predicted = 0\ntotal_infer_consume = 0.0\nwarm_iter = 100\nfeatures_list = []\nwith tf.compat.v1.Session(config=config, graph=graph) as sess:\n  res_dataset = input_fn(data_file, 1, False, batch_size)\n  iterator = tf.compat.v1.data.make_one_shot_iterator(res_dataset)\n  next_element = iterator.get_next()\n  for i in range(int(no_of_batches)):\n    batch=sess.run(next_element)\n    features=batch[0:3]\n    features_list.append(features)\n\nwith tf.compat.v1.Session(config=config, graph=graph) as sess1:    \n  i=0\n  while True:\n    if i >= no_of_batches:\n        break\n    if i > warm_iter:\n        inference_start = time.time()\n    logistic = sess1.run(output_tensor, dict(zip(input_tensor, features_list[i][0:2])))\n    if i > warm_iter:\n        infer_time = time.time() - inference_start\n        total_infer_consume += infer_time\n    if args.compute_accuracy:\n        predicted_labels = np.argmax(logistic,1)\n        correctly_predicted=correctly_predicted+np.sum(features_list[i][2] == predicted_labels)\n    \n    i=i+1\n  inference_end = time.time()\nif args.compute_accuracy:\n    accuracy = (\n        float(correctly_predicted)/float(no_of_test_samples))\nevaluate_duration = total_infer_consume\nlatency = (1000 * batch_size* float(evaluate_duration)/float(no_of_test_samples - warm_iter*batch_size))\nthroughput = (no_of_test_samples - warm_iter * batch_size)/evaluate_duration\n\nprint(\'--------------------------------------------------\')\nprint(\'Total test records           : \', no_of_test_samples)\nprint(\'Batch size is                : \', batch_size)\nprint(\'Number of batches            : \', int(no_of_batches))\nif args.compute_accuracy:\n    print(\'Classification accuracy (%)  : \', round((accuracy * 100), 4))\n    print(\'No of correct predictions    : \', int(correctly_predicted))\nprint(\'Inference duration (seconds) : \', round(evaluate_duration, 4))\nprint(\'Average Latency (ms/batch)   : \', round(latency,4))\nprint(\'Throughput is (records/sec)  : \', round(throughput, 3))\nprint(\'--------------------------------------------------\')\n'"
models/recommendation/tensorflow/wide_deep_large_ds/inference/parallel_inference.py,29,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: EPL-2.0\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nimport os\nimport numpy as np\nimport argparse\nimport collections\nimport time\nimport math\nimport json\nimport datetime\n\nimport tensorflow as tf\ntf.compat.v1.disable_v2_behavior()\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import ops\nfrom tensorflow.core.framework import graph_pb2\nfrom google.protobuf import text_format\n\n\ndef str2bool(v):\n    if v.lower() in (\'true\'):\n        return True\n    else:\n        return False\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--input_graph\', type=str,\n                    help=\'file name for graph\',\n                    dest=\'input_graph\',\n                    required=True)\nparser.add_argument(\'--data_location\', type=str,\n                    help=\'full path of data file\',\n                    dest=\'data_location\',\n                    required=True)\nparser.add_argument(\'--batch_size\', type=int,\n                    help=\'batch size for inference.Default is 512\',\n                    default=512,\n                    dest=\'batch_size\')\nparser.add_argument(\'--num_intra_threads\', type=int,\n                    help=\'number of threads for an operator\',\n                    required=False,\n                    default=28,\n                    dest=\'num_intra_threads\')\nparser.add_argument(\'--num_inter_threads\', type=int,\n                    help=\'number of threads across operators\',\n                    required=False,\n                    default=2,\n                    dest=\'num_inter_threads\')\nparser.add_argument(\'--num_omp_threads\', type=str,\n                    help=\'number of threads to use\',\n                    required=False,\n                    default=None,\n                    dest=\'num_omp_threads\')\nparser.add_argument(\'--num_parallel_batches\', type=int,\n                    help=\'number of parallel batches\',\n                    required=False,\n                    default=1,\n                    dest=\'num_parallel_batches\')\nparser.add_argument(""--accuracy_only"", type=str2bool,\n                    nargs=\'?\', const=True, default=False,\n                    dest=\'compute_accuracy\', required=False,\n                    help=""Enable accuracy calculation"")\n\nargs = parser.parse_args()\nif args.num_omp_threads:\n    os.environ[""OMP_NUM_THREADS""] = args.num_omp_threads\nnum_parallel_batches = args.num_parallel_batches\noutput_probabilities_node = \'import/import/head/predictions/probabilities\'\nwhile_probabilities_node = \'while/import/\'+output_probabilities_node+\':0\'\nwhile_softmax_operation = \'while/import/\'+output_probabilities_node\nplaceholder_name = \'import/new_numeric_placeholder\'\ncategorical_placeholder = \'import/new_categorical_placeholder\'\n\nconfig = tf.compat.v1.ConfigProto(log_device_placement=False,\n                        inter_op_parallelism_threads=args.num_inter_threads,\n                        intra_op_parallelism_threads=args.num_intra_threads)\ngraph = ops.Graph()\ngraph_def = graph_pb2.GraphDef()\n\nfilename, file_ext = os.path.splitext(args.input_graph)\n\nbatch_size = args.batch_size\nwith open(args.input_graph, ""rb"") as f:\n    if file_ext == "".pbtxt"":\n        text_format.Merge(f.read(), graph_def)\n    else:\n        graph_def.ParseFromString(f.read())\n\nnumeric_feature_names = [""numeric_1""]\nstring_feature_names = [""string_1""]\nif args.compute_accuracy:\n    full_features_names = numeric_feature_names + string_feature_names + [""label""]\n    feature_datatypes = [tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0, allow_missing=True)]+[tf.io.FixedLenSequenceFeature(\n            [], tf.int64, default_value=0, allow_missing=True)]+[tf.io.FixedLenSequenceFeature([], tf.int64, default_value=0, allow_missing=True)]\nelse:\n    full_features_names = numeric_feature_names + string_feature_names\n    feature_datatypes = [tf.io.FixedLenSequenceFeature([], tf.float32, default_value=0.0, allow_missing=True)]+[tf.io.FixedLenSequenceFeature(\n            [], tf.int64, default_value=0, allow_missing=True)]\n\n\n\ndef input_fn(data_file, num_epochs, shuffle, batch_size):\n    """"""Generate an input function for the Estimator.""""""\n    def _parse_function(proto):\n        f = collections.OrderedDict(\n            zip(full_features_names, feature_datatypes))\n        parsed_features = tf.io.parse_example(proto, f)\n        parsed_feature_vals_num = [tf.reshape(\n            parsed_features[""numeric_1""], shape=[-1, 13])]\n        parsed_feature_vals_str = [tf.reshape(\n            parsed_features[""string_1""], shape=[-1, 2]) for i in string_feature_names]\n        parsed_feature_vals = parsed_feature_vals_num + parsed_feature_vals_str\n        if args.compute_accuracy:\n            parsed_feature_vals_label = [tf.reshape(parsed_features[i], shape=[-1]) for i in [""label""]]\n            parsed_feature_vals = parsed_feature_vals + parsed_feature_vals_label\n        return parsed_feature_vals\n\n    # Extract lines from input files using the Dataset API.\n    dataset = tf.data.TFRecordDataset([data_file])\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=20000)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(_parse_function, num_parallel_calls=28)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(1)\n    return dataset\n\n\ndata_file = args.data_location\nno_of_test_samples = sum(1 for _ in tf.compat.v1.python_io.tf_record_iterator(data_file))\nno_of_batches = math.ceil(float(no_of_test_samples)/batch_size)\nwith graph.as_default():\n    tf.import_graph_def(graph_def)\n    res_dataset = input_fn(data_file, 1, False, batch_size)\n    iterator = tf.compat.v1.data.make_one_shot_iterator(res_dataset)\n    next_element = iterator.get_next()\n    iterator_names = [i.name.split(\':\')[1] for i in next_element]\n    placeholder_expandims = {}\n    full_nodes = []\n    old_graph_def = graph.as_graph_def()\n    for node in old_graph_def.node:\n        k = node.name\n        if k == ""IteratorGetNext"":\n            iterator_node = node\n        elif (node.op == ""GatherNd"" or node.op == \'ConcatV2\') and (placeholder_name in node.input[1] or categorical_placeholder in node.input[1]):\n            if node.op == \'GatherNd\' and node.name == \'import/gather_categorical_weights\':\n                gather_categorical_node = node\n            elif node.op == \'GatherNd\' and node.name == \'import/gather_embedding_weights\':\n                gather_embedding_node = node\n            elif node.op == \'ConcatV2\':\n                concat_node = node\n\n    gather_categorical_node.input[1] = iterator_node.name+"":1""\n    gather_embedding_node.input[1] = iterator_node.name+"":1""\n    concat_node.input[1] = iterator_node.name+"":0""\n\n\nnew_graph_def = tf.compat.v1.GraphDef()\nnew_graph_def = tf.compat.v1.graph_util.extract_sub_graph(\n    old_graph_def,\n    [output_probabilities_node]\n)\ntf.compat.v1.reset_default_graph()\ngraph = ops.Graph()\n\nwith graph.as_default():\n    i = tf.constant(0)\n    arr = tf.TensorArray(dtype=tf.int32, size=2000, dynamic_size=True)\n    def _body(i, arr):\n        tf.import_graph_def(new_graph_def)\n        output_tensor = graph.get_tensor_by_name(while_probabilities_node)\n        if args.compute_accuracy:\n            labels_tensor = graph.get_tensor_by_name(""while/import/IteratorGetNext:2"")\n            predicted_labels = tf.argmax(output_tensor,1,output_type=tf.int64)\n            correctly_predicted_bool = tf.equal(predicted_labels, labels_tensor)\n            num_correct_predictions_batch = tf.reduce_sum(tf.cast(correctly_predicted_bool, tf.int32))\n        else:\n            predicted_labels = tf.argmax(output_tensor,1,output_type=tf.int32)\n            num_correct_predictions_batch = tf.reduce_sum(predicted_labels)\n        arr = arr.write(i, num_correct_predictions_batch)\n        i = tf.add(i, 1)\n        return i, arr\n    i, arr = tf.compat.v2.while_loop(cond=lambda i, x: i < int(no_of_batches), body=_body, loop_vars=[i, arr], parallel_iterations=num_parallel_batches)\n    array_gather = arr.gather(tf.range(0, int(no_of_batches), delta=1, dtype=None, name=\'range\'))\n\nwith tf.compat.v1.Session(config=config, graph=graph) as sess:\n    inference_start = time.time()\n    try:\n        num_correct_predictions_batch = sess.run(array_gather)\n    except Exception as e:\n        print(\'--------------------------------------------------\')\n        print(""Exception during execution of model:  "",e)\n        print(\'--------------------------------------------------\')\n        sys.exit()\n    total_num_correct_predictions = num_correct_predictions_batch.sum(axis=0)\n    inference_end = time.time()\nif args.compute_accuracy:\n    accuracy = (float(total_num_correct_predictions)/float(no_of_test_samples))\nevaluate_duration = inference_end - inference_start\nlatency = (1000 * float(batch_size * num_parallel_batches) * float(evaluate_duration) / float(no_of_test_samples))\n\nthroughput = no_of_test_samples/evaluate_duration\nprint(\'--------------------------------------------------\')\nprint(\'Total test records           : \', no_of_test_samples)\nprint(\'Batch size is                : \', batch_size)\nprint(\'Number of batches            : \', int(no_of_batches))\nif args.compute_accuracy:\n    print(\'Classification accuracy (%)  : \', round((accuracy * 100), 4))\n    print(\'No of correct predictions    : \', int(total_num_correct_predictions))\nprint(\'Inference duration (seconds) : \', round(evaluate_duration, 4))\nprint(\'Average Latency (ms/batch)   : \', round(latency,4))\nprint(\'Throughput is (records/sec)  : \', round(throughput, 3))\nprint(\'--------------------------------------------------\')\n'"
models/recommendation/tensorflow/wide_deep_large_ds/training/train.py,17,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\'\'\'This script used to create and train wide and deep model on Kaggle\'s Criteo Dataset\'\'\'\n\nimport time\nimport argparse\nimport tensorflow as tf\nimport math\nimport collections\nimport numpy as np\nimport os.path\nfrom os import path\nimport sys\n# Set to INFO for tracking training, default is WARN. ERROR for least messages\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\nprint(""Using TensorFlow version %s"" % (tf.__version__))\nCONTINUOUS_COLUMNS = [""I""+str(i) for i in range(1, 14)]  # 1-13 inclusive\nCATEGORICAL_COLUMNS = [""C""+str(i) for i in range(1, 27)]  # 1-26 inclusive\nLABEL_COLUMN = [""clicked""]\nTRAIN_DATA_COLUMNS = LABEL_COLUMN + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\nFEATURE_COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n\n\ndef generate_input_fn(filename, batch_size, num_epochs):\n    def parse_csv(value):\n        tf.compat.v1.logging.info(\'Parsing {}\'.format(filename))\n        cont_defaults = [[0.0] for i in range(1, 14)]\n        cate_defaults = [["" ""] for i in range(1, 27)]\n        label_defaults = [[0]]\n        column_headers = TRAIN_DATA_COLUMNS\n        record_defaults = label_defaults + cont_defaults + cate_defaults\n        columns = tf.io.decode_csv(value, record_defaults=record_defaults)\n        all_columns = collections.OrderedDict(zip(column_headers, columns))\n        labels = all_columns.pop(LABEL_COLUMN[0])\n        features = all_columns\n        return features, labels\n\n    # Extract lines from input files using the Dataset API.\n    dataset = tf.data.TextLineDataset(filename)\n    dataset = dataset.shuffle(buffer_size=20000)\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.prefetch(batch_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(parse_csv, num_parallel_calls=28)\n    dataset = dataset.prefetch(1)\n    return dataset\n\n\ndef build_feature_cols(train_file_path,test_file_path):\n    # compute statistics(min,max,range) of train dataset\n    print(\'****Computing statistics of train dataset*****\')\n    with open(train_file_path, \'r\') as f, open(test_file_path, \'r\') as f1:\n        nums = [line.strip(\'\\n\').split(\',\') for line in f.readlines(\n        )]+[line.strip(\'\\n\').split(\',\') for line in f1.readlines()]\n        numpy_arr = np.array(nums)\n        mins_list, max_list, range_list = [], [], []\n        for i in range(len(TRAIN_DATA_COLUMNS)):\n            if TRAIN_DATA_COLUMNS[i] in CONTINUOUS_COLUMNS:\n                col_min = numpy_arr[:, i].astype(np.float32).min()\n                col_max = numpy_arr[:, i].astype(np.float32).max()\n                mins_list.append(col_min)\n                max_list.append(col_max)\n                range_list.append(col_max-col_min)\n\n    def numeric_column_normalized(column_name, normalizer_fn):\n        return tf.feature_column.numeric_column(column_name, normalizer_fn=normalizer_fn)\n\n    def make_minmaxscaler(min, range):\n        def minmaxscaler(col):\n            return (col - min)/range\n        return minmaxscaler\n    deep_columns = []\n    for i in range(len(CONTINUOUS_COLUMNS)):\n        normalizer_fn = None\n        col_min = mins_list[i]\n        col_range = range_list[i]\n        normalizer_fn = make_minmaxscaler(col_min, col_range)\n        deep_columns.append(numeric_column_normalized(\n            CONTINUOUS_COLUMNS[i], normalizer_fn))\n    wide_columns = []\n    for name in CATEGORICAL_COLUMNS:\n        wide_columns.append(tf.feature_column.categorical_column_with_hash_bucket(\n            name, hash_bucket_size=1000))\n    for col in wide_columns:\n        deep_columns.append(\n            tf.feature_column.embedding_column(col, dimension=32))\n    return wide_columns, deep_columns\n\n\ndef build_model(model_type, model_dir, wide_columns, deep_columns):\n    runconfig = tf.estimator.RunConfig(\n        save_checkpoints_steps=500\n    )\n    m = None\n    # Linear Classifier\n    if model_type == \'WIDE\':\n        m = tf.estimator.LinearClassifier(\n            config=runconfig,\n            model_dir=model_dir,\n            feature_columns=wide_columns)\n\n    # Deep Neural Net Classifier\n    elif model_type == \'DEEP\':\n        m = tf.estimator.DNNClassifier(\n            config=runconfig,\n            model_dir=model_dir,\n            feature_columns=deep_columns,\n            hidden_units=[1024, 512, 256])\n\n    # Combined Linear and Deep Classifier\n    elif model_type == \'WIDE_AND_DEEP\':\n        m = tf.estimator.DNNLinearCombinedClassifier(\n            config=runconfig,\n            model_dir=model_dir,\n            linear_feature_columns=wide_columns,\n            dnn_feature_columns=deep_columns,\n            dnn_hidden_units=[1024, 512, 256])\n\n    return m\n\n\ndef build_estimator(model_type=\'WIDE_AND_DEEP\', model_dir=None, train_file_path=None,test_file_path=None):\n    if model_dir is None:\n        model_dir = \'models/model_\' + model_type + \'_\' + str(int(time.time()))\n        print(""Model directory = %s"" % model_dir)\n\n    wide_columns, deep_columns = build_feature_cols(train_file_path,test_file_path)\n    m = build_model(model_type, model_dir, wide_columns, deep_columns)\n    print(\'estimator built\')\n    return m\n\n\n# All categorical columns are strings for this dataset\ndef column_to_dtype(column):\n    if column in CATEGORICAL_COLUMNS:\n        return tf.string\n    else:\n        return tf.float32\n\n\n""""""\n  This function maps input columns (feature_placeholders) to \n  tensors that can be inputted into the graph \n  (similar in purpose to the output of our input functions)\n  In this particular case, we need to accomodate the sparse fields (strings)\n  so we have to do a slight modification to expand their dimensions, \n  just like in the input functions\n""""""\n\n\ndef serving_input_fn():\n    feature_placeholders = {\n        column: tf.compat.v1.placeholder(column_to_dtype(column), [None])\n        for column in FEATURE_COLUMNS\n    }\n    # DNNCombinedLinearClassifier expects rank 2 Tensors,\n    # but inputs should be rank 1, so that we can provide\n    # scalars to the server\n    features = {\n        key: tf.expand_dims(tensor, -1)\n        for key, tensor in feature_placeholders.items()\n    }\n    return tf.estimator.export.ServingInputReceiver(\n        features,  # input into graph\n        feature_placeholders  # tensor input converted from request\n    )\n\n\ndef train_and_eval():\n    print(""Begin training and evaluation"")\n    train_file = args.data_location+\'/train.csv\'\n    test_file = args.data_location+\'/eval.csv\'\n    if (not path.exists(train_file)) or (not path.exists(test_file)):\n        print(\'------------------------------------------------------------------------------------------\')\n        print(""train.csv or eval.csv does not exist in the given data_location. Please provide valid path"")\n        print(\'------------------------------------------------------------------------------------------\')\n        sys.exit() \n    no_of_training_examples = sum(1 for line in open(train_file))\n    no_of_test_examples = sum(1 for line in open(test_file))\n    batch_size = args.batch_size\n    if args.steps == 0:\n        no_of_epochs = 10\n        train_steps = math.ceil(\n        (float(no_of_epochs)*no_of_training_examples)/batch_size)\n    else:\n        no_of_epochs = math.ceil(\n        (float(batch_size)*args.steps)/no_of_training_examples)\n        train_steps = args.steps\n    test_steps = math.ceil(float(no_of_test_examples)/batch_size)\n    model_type = \'WIDE_AND_DEEP\'\n    model_dir = \'model_\' + model_type + \'_\' + str(int(time.time()))\n    print(""Saving model checkpoints to "" + model_dir)\n    export_dir = model_dir + \'/exports\'\n    m = build_estimator(model_type, model_dir, train_file, test_file)\n    m.train(input_fn=lambda: generate_input_fn(\n        train_file, batch_size, int(no_of_epochs)),steps=int(train_steps))\n    print(\'fit done\')\n    results = m.evaluate(input_fn=lambda: generate_input_fn(\n        test_file, batch_size, 1), steps=test_steps)\n    print(\'evaluate done\')\n\n    export_folder = m.export_saved_model(\n        export_dir,\n        serving_input_fn\n    )\n    print(\'Model exported to \' + export_dir)\n    print(\'Accuracy: %s\' % results[\'accuracy\'])\n\n\ndef get_arg_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--data_location\',\n        help=\'Full path of train data\',\n        required=True\n    )\n    parser.add_argument(\n        \'--steps\',\n        help=\'set the number of steps on train dataset.Default is will set to 1 epoch\',\n        type=int,\n        default=0\n    )\n    parser.add_argument(\n        \'--batch_size\',\n        help=\'Batch size to train. Default is 512\',\n        type=int,\n        default=512\n    )\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_arg_parser()\n    args = parser.parse_args()\n    main_start = time.time()\n    train_and_eval()\n    main_end = time.time()\n    print(""Total time:"", main_end-main_start)\n'"
models/reinforcement/tensorflow/minigo/training/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
benchmarks/image_recognition/tensorflow/densenet169/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/densenet169/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for Densenet169 FP32 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n        self.cmd = self.get_command_prefix(self.args.socket_id) + ""{} "".format(self.python_exe)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        if self.args.batch_size == -1:\n            self.args.batch_size = 100\n\n        # set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        self.parse_args()\n\n        if self.args.benchmark_only:\n            run_script = os.path.join(self.args.intelai_models,\n                                      self.args.mode, self.args.precision,\n                                      ""benchmark.py"")\n\n            script_args_list = [\n                ""input_graph"", ""input_height"", ""input_width"", ""batch_size"",\n                ""input_layer"", ""output_layer"", ""num_inter_threads"",\n                ""num_intra_threads"", ""warmup_steps"", ""steps""]\n\n        elif self.args.accuracy_only:\n            run_script = os.path.join(self.args.intelai_models,\n                                      self.args.mode, self.args.precision,\n                                      ""accuracy.py"")\n\n            script_args_list = [\n                ""input_graph"", ""data_location"", ""input_height"", ""input_width"",\n                ""batch_size"", ""input_layer"", ""output_layer"",\n                ""num_inter_threads"", ""num_intra_threads""]\n\n        self.cmd = self.add_args_to_command(self.cmd + run_script,\n                                            script_args_list)\n\n    def parse_args(self):\n        if self.custom_args:\n            parser = argparse.ArgumentParser()\n            parser.add_argument(\n                ""--input_height"", default=224,\n                dest=\'input_height\', type=int, help=""input height"")\n            parser.add_argument(\n                ""--input_width"", default=224,\n                dest=\'input_width\', type=int, help=""input width"")\n            parser.add_argument(\n                \'--warmup_steps\', dest=\'warmup_steps\',\n                help=\'number of warmup steps\',\n                type=int, default=20)\n            parser.add_argument(\n                \'--steps\', dest=\'steps\',\n                help=\'number of steps\',\n                type=int, default=100)\n            parser.add_argument(\n                \'--input_layer\', dest=\'input_layer\',\n                help=\'name of input layer\',\n                type=str, default=""input"")\n            parser.add_argument(\n                \'--output_layer\', dest=\'output_layer\',\n                help=\'name of output layer\',\n                type=str, default=""densenet169/predictions/Reshape_1"")\n\n            self.args = parser.parse_args(self.custom_args,\n                                          namespace=self.args)\n\n    def run(self):\n        if self.cmd:\n            self.run_command(self.cmd)\n'"
benchmarks/image_recognition/tensorflow/inceptionv3/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv3/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport os\nfrom argparse import ArgumentParser\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""initialize mode and run benchmark""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.benchmark_command = """"\n        if not platform_util:\n            raise ValueError(""Did not find any platform info."")\n\n        # use default batch size if -1\n        if self.args.batch_size == -1:\n            self.args.batch_size = 128\n\n        # set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        arg_parser = ArgumentParser(description=\'Parse args\')\n\n        arg_parser.add_argument(""--warmup-steps"", dest=\'warmup_steps\',\n                                type=int, default=10,\n                                help=""number of warmup steps"")\n        arg_parser.add_argument(""--steps"", dest=\'steps\',\n                                type=int, default=50,\n                                help=""number of steps"")\n        arg_parser.add_argument(\n            \'--kmp-blocktime\', dest=\'kmp_blocktime\',\n            help=\'number of kmp block time\',\n            type=int, default=1)\n\n        self.args = arg_parser.parse_args(self.custom_args, namespace=self.args)\n\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path, kmp_blocktime=str(self.args.kmp_blocktime))\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        benchmark_script = os.path.join(\n            self.args.intelai_models, self.args.precision,\n            ""eval_image_classifier_inference.py"")\n\n        self.benchmark_command = self.get_command_prefix(args.socket_id) + \\\n            self.python_exe + "" "" + benchmark_script\n\n        num_cores = self.platform_util.num_cores_per_socket if self.args.num_cores == -1 \\\n            else self.args.num_cores\n\n        self.benchmark_command = \\\n            self.benchmark_command + \\\n            "" --input-graph="" + self.args.input_graph + \\\n            "" --num-inter-threads="" + str(self.args.num_inter_threads) + \\\n            "" --num-intra-threads="" + str(self.args.num_intra_threads) + \\\n            "" --num-cores="" + str(num_cores) + \\\n            "" --batch-size="" + str(self.args.batch_size) + \\\n            "" --warmup-steps="" + str(self.args.warmup_steps) + \\\n            "" --steps="" + str(self.args.steps)\n\n        if self.args.data_num_inter_threads:\n            self.benchmark_command += "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads)\n        if self.args.data_num_intra_threads:\n            self.benchmark_command += "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads)\n\n        # if the data location directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            self.benchmark_command += "" --data-location="" + \\\n                                      self.args.data_location\n        if self.args.accuracy_only:\n            self.benchmark_command += "" --accuracy-only""\n\n    def run(self):\n        if self.benchmark_command:\n            self.run_command(self.benchmark_command)\n'"
benchmarks/image_recognition/tensorflow/inceptionv3/inference/int8/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv3/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport argparse\nimport os\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for inception v3 int8 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # Set the num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n        # Set env vars, if they haven\'t already been set\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads, overwrite_existing=True)\n\n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            ""--warmup-steps"", dest=""warmup_steps"",\n            help=""number of warmup steps"",\n            type=int, default=10)\n        parser.add_argument(\n            ""--steps"", dest=""steps"",\n            help=""number of steps"",\n            type=int, default=50)\n        parser.add_argument(\n            \'--kmp-blocktime\', dest=\'kmp_blocktime\',\n            help=\'number of kmp block time\',\n            type=int, default=1)\n        parser.add_argument(\n            ""--calibration-only"",\n            help=""Calibrate the accuracy."",\n            dest=""calibration_only"", action=""store_true"")\n\n        self.args = parser.parse_args(self.custom_args,\n                                      namespace=self.args)\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path, kmp_blocktime=str(self.args.kmp_blocktime))\n\n    def run_benchmark(self):\n        benchmark_script = os.path.join(self.args.intelai_models,\n                                        self.args.precision, ""benchmark.py"")\n        script_args_list = [\n            ""input_graph"", ""batch_size"",\n            # ""data_location"",    # comment it out for now since start.sh added data-location=/dataset\n            ""num_inter_threads"", ""num_intra_threads"",\n            ""data_num_inter_threads"", ""data_num_intra_threads"",\n            ""warmup_steps"", ""steps""]\n\n        cmd_prefix = self.get_command_prefix(self.args.socket_id) + \\\n            self.python_exe + "" "" + benchmark_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        # add num_cores\n        num_cores = self.platform_util.num_cores_per_socket if self.args.num_cores == -1 \\\n            else self.args.num_cores\n        cmd += "" --num_cores="" + str(num_cores)\n        # workaround the --data-location problem\n        if self.args.data_location and os.listdir(self.args.data_location):\n            cmd += "" --data_location="" + self.args.data_location\n        self.run_command(cmd)\n\n    def run_accuracy(self):\n        accuracy_script = os.path.join(self.args.intelai_models,\n                                       self.args.precision, ""accuracy.py"")\n        script_args_list = [\n            ""input_graph"", ""data_location"",\n            ""batch_size"",\n            ""num_inter_threads"", ""num_intra_threads""]\n\n        cmd_prefix = self.get_command_prefix(self.args.socket_id) + \\\n            self.python_exe + "" "" + accuracy_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run_calibration(self):\n        calibration_script = os.path.join(self.args.intelai_models,\n                                          self.args.precision, ""calibration.py"")\n        script_args_list = [\n            ""input_graph"", ""data_location"",\n            ""batch_size"",\n            ""num_inter_threads"", ""num_intra_threads""]\n        cmd_prefix = self.get_command_prefix(self.args.socket_id) + \\\n            self.python_exe + "" "" + calibration_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run(self):\n        # Parse custom arguments and append to self.args\n        self.parse_args()\n        if self.args.benchmark_only:\n            self.run_benchmark()\n        if self.args.accuracy_only:\n            if not self.args.calibration_only:\n                self.run_accuracy()\n            else:\n                self.run_calibration()\n'"
benchmarks/image_recognition/tensorflow/inceptionv4/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv4/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom image_recognition.tensorflow.inceptionv4.inference.inceptionv4_model_init import InceptionV4ModelInitializer\n\n\nclass ModelInitializer(InceptionV4ModelInitializer):\n    """"""Model initializer for InceptionV4 FP32 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n'"
benchmarks/image_recognition/tensorflow/inceptionv4/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/inceptionv4/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom image_recognition.tensorflow.inceptionv4.inference.inceptionv4_model_init import InceptionV4ModelInitializer\n\n\nclass ModelInitializer(InceptionV4ModelInitializer):\n    """"""Model initializer for InceptionV4 FP32 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n'"
benchmarks/image_recognition/tensorflow/mobilenet_v1/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/mobilenet_v1/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport argparse\nimport os\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """""" Model initializer for MobileNet V1 FP32 inference """"""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # use default batch size if -1\n        if self.args.batch_size == -1:\n            self.args.batch_size = 128\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        # set num_inter_threads and num_intra_threads (override inter threads to 2)\n        self.set_num_inter_intra_threads(num_inter_threads=2)\n\n        script_name = ""accuracy.py"" if self.args.accuracy_only \\\n            else ""benchmark.py""\n        script_path = os.path.join(\n            self.args.intelai_models, self.args.mode, self.args.precision,\n            script_name)\n        self.command_prefix = ""{} {}"".format(self.python_exe, script_path)\n\n        if self.args.socket_id != -1:\n            self.command_prefix = ""numactl --cpunodebind={} -l {}"".format(\n                str(self.args.socket_id), self.command_prefix)\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        self.parse_args()\n\n        if not self.args.accuracy_only:\n            # add args for the benchmark script\n            script_args_list = [\n                ""input_graph"", ""input_height"", ""input_width"", ""batch_size"",\n                ""input_layer"", ""output_layer"", ""num_inter_threads"",\n                ""num_intra_threads"", ""warmup_steps"", ""steps""]\n            self.command_prefix = self.add_args_to_command(\n                self.command_prefix, script_args_list)\n        else:\n            # add args for the accuracy script\n            script_args_list = [\n                ""input_graph"", ""data_location"", ""input_height"", ""input_width"",\n                ""batch_size"", ""input_layer"", ""output_layer"",\n                ""num_inter_threads"", ""num_intra_threads""]\n            self.command_prefix = self.add_args_to_command(\n                self.command_prefix, script_args_list)\n\n    def parse_args(self):\n        if self.custom_args == None:\n            return\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            ""--input_height"", default=224,\n            dest=\'input_height\', type=int, help=""input height"")\n        parser.add_argument(\n            ""--input_width"", default=224,\n            dest=\'input_width\', type=int, help=""input width"")\n        parser.add_argument(\n            ""--warmup_steps"", dest=""warmup_steps"",\n            help=""number of warmup steps"",\n            type=int, default=10)\n        parser.add_argument(\n            ""--steps"", dest=""steps"",\n            help=""number of steps"",\n            type=int, default=50)\n        parser.add_argument(\n            ""--input_layer"", dest=""input_layer"",\n            help=""name of input layer"",\n            type=str, default=""input"")\n        parser.add_argument(\n            ""--output_layer"", dest=""output_layer"",\n            help=""name of output layer"",\n            type=str, default=""MobilenetV1/Predictions/Reshape_1"")\n\n        self.args = parser.parse_args(self.custom_args,\n            namespace=self.args)\n\n    def run(self):\n        self.run_command(self.command_prefix)\n'"
benchmarks/image_recognition/tensorflow/mobilenet_v1/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/mobilenet_v1/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for Mobilenet INT8 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n        self.cmd = self.get_command_prefix(self.args.socket_id) + ""python ""\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        # Set the num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n        # Set env vars, if they haven\'t already been set\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        self.parse_args()\n\n        if self.args.benchmark_only:\n            run_script = os.path.join(\n                self.args.intelai_models, self.args.mode,\n                self.args.precision, ""benchmark.py"")\n            script_args_list = [\n                ""input_graph"", ""input_height"", ""input_width"", ""batch_size"",\n                ""input_layer"", ""output_layer"", ""num_inter_threads"",\n                ""num_intra_threads"", ""warmup_steps"", ""steps""]\n\n        if self.args.calibration_only:\n            run_script = os.path.join(\n                self.args.intelai_models, self.args.mode,\n                self.args.precision, ""calibration.py"")\n            script_args_list = [\n                ""input_graph"", ""data_location"", ""input_height"", ""input_width"",\n                ""batch_size"", ""input_layer"", ""output_layer"",\n                ""num_inter_threads"", ""num_intra_threads""]\n        elif self.args.accuracy_only:\n            run_script = os.path.join(\n                self.args.intelai_models, self.args.mode,\n                self.args.precision, ""accuracy.py"")\n            script_args_list = [\n                ""input_graph"", ""data_location"", ""input_height"", ""input_width"",\n                ""batch_size"", ""input_layer"", ""output_layer"",\n                ""num_inter_threads"", ""num_intra_threads""]\n\n        self.cmd = self.add_args_to_command(self.cmd + run_script, script_args_list)\n\n    def parse_args(self):\n        if self.custom_args:\n            parser = argparse.ArgumentParser()\n            parser.add_argument(\n                ""--input_height"", default=224,\n                dest=\'input_height\', type=int, help=""input height"")\n            parser.add_argument(\n                ""--input_width"", default=224,\n                dest=\'input_width\', type=int, help=""input width"")\n            parser.add_argument(\n                ""--warmup_steps"", dest=""warmup_steps"",\n                help=""number of warmup steps"",\n                type=int, default=10)\n            parser.add_argument(\n                ""--steps"", dest=""steps"",\n                help=""number of steps"",\n                type=int, default=50)\n            parser.add_argument(\n                ""--input_layer"", dest=""input_layer"",\n                help=""name of input layer"",\n                type=str, default=""input"")\n            parser.add_argument(\n                ""--output_layer"", dest=""output_layer"",\n                help=""name of output layer"",\n                type=str, default=""MobilenetV1/Predictions/Reshape_1"")\n            parser.add_argument(\n                ""--calibration-only"", dest=""calibration_only"",\n                help=""calibrate the accuracy"",\n                action=""store_true"")\n\n            self.args = parser.parse_args(self.custom_args,\n                                          namespace=self.args)\n\n    def run(self):\n        if self.cmd:\n            self.run_command(self.cmd)\n'"
benchmarks/image_recognition/tensorflow/resnet101/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet101/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport os\nfrom argparse import ArgumentParser\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""initialize mode and run benchmark""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.benchmark_command = """"\n        if not platform_util:\n            raise ValueError(""Did not find any platform info."")\n\n        # use default batch size if -1\n        if self.args.batch_size == -1:\n            self.args.batch_size = 128\n\n        # set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        arg_parser = ArgumentParser(description=\'Parse args\')\n\n        arg_parser.add_argument(""--warmup-steps"", dest=\'warmup_steps\',\n                                type=int, default=10,\n                                help=""number of warmup steps"")\n        arg_parser.add_argument(""--steps"", dest=\'steps\',\n                                type=int, default=50,\n                                help=""number of steps"")\n        arg_parser.add_argument(\n            \'--kmp-blocktime\', dest=\'kmp_blocktime\',\n            help=\'number of kmp block time\',\n            type=int, default=1)\n\n        self.args = arg_parser.parse_args(self.custom_args, namespace=self.args)\n\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path, kmp_blocktime=str(self.args.kmp_blocktime))\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        benchmark_script = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            ""eval_image_classifier_inference.py"")\n\n        self.benchmark_command = self.get_command_prefix(args.socket_id) + \\\n            self.python_exe + "" "" + benchmark_script\n\n        self.benchmark_command = \\\n            self.benchmark_command + \\\n            "" --input-graph="" + self.args.input_graph + \\\n            "" --num-inter-threads="" + str(self.args.num_inter_threads) + \\\n            "" --num-intra-threads="" + str(self.args.num_intra_threads) + \\\n            "" --batch-size="" + str(self.args.batch_size) + \\\n            "" --warmup-steps="" + str(self.args.warmup_steps) + \\\n            "" --steps="" + str(self.args.steps)\n\n        if self.args.data_num_inter_threads:\n            self.benchmark_command += "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads)\n        if self.args.data_num_intra_threads:\n            self.benchmark_command += "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads)\n\n        # need to add data-num-inter-threads, data-num-intra-thread to the args list once\n        # they are ready from common interface\n        # "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads) + \\\n        # "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads) + \\\n\n        # if the data location directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            self.benchmark_command += "" --data-location="" + \\\n                                      self.args.data_location\n        if self.args.accuracy_only:\n            self.benchmark_command += "" --accuracy-only""\n\n    def run(self):\n        if self.benchmark_command:\n            self.run_command(self.benchmark_command)\n'"
benchmarks/image_recognition/tensorflow/resnet101/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet101/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport os\nimport argparse\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for resnet101 int8 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # Set the num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        # Set env vars, if they haven\'t already been set\n        set_env_var(""OMP_NUM_THREADS"",\n                    platform_util.num_cores_per_socket if args.num_cores == -1 else args.num_cores)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(""--input-height"", default=None,\n                            dest=\'input_height\', type=int,\n                            help=""input height"")\n        parser.add_argument(""--input-width"", default=None,\n                            dest=\'input_width\', type=int,\n                            help=""input width"")\n        parser.add_argument(\'--warmup-steps\', dest=\'warmup_steps\',\n                            help=\'number of warmup steps\', type=int,\n                            default=40)\n        parser.add_argument(\'--steps\', dest=\'steps\',\n                            help=\'number of steps\', type=int,\n                            default=100)\n        parser.add_argument(\'--input-layer\', dest=\'input_layer\',\n                            help=\'name of input layer\', type=str,\n                            default=None)\n        parser.add_argument(\'--output-layer\', dest=\'output_layer\',\n                            help=\'name of output layer\', type=str,\n                            default=None)\n        parser.add_argument(\n            ""--calibration-only"",\n            help=""Calibrate the accuracy."",\n            dest=""calibration_only"", action=""store_true"")\n\n        self.args = parser.parse_args(self.custom_args,\n                                      namespace=self.args)\n\n    def run_benchmark_or_accuracy(self):\n        cmd = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            ""eval_image_classifier_inference.py"")\n\n        cmd = self.get_command_prefix(self.args.socket_id) + self.python_exe + "" "" + cmd\n\n        cmd += "" --input-graph="" + self.args.input_graph + \\\n               "" --num-inter-threads="" + str(self.args.num_inter_threads) + \\\n               "" --num-intra-threads="" + str(self.args.num_intra_threads) + \\\n               "" --batch-size="" + str(self.args.batch_size) + \\\n               "" --warmup-steps="" + str(self.args.warmup_steps) + \\\n               "" --steps="" + str(self.args.steps)\n\n        if self.args.data_num_inter_threads:\n            cmd += "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads)\n        if self.args.data_num_intra_threads:\n            cmd += "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads)\n\n        # if the data location directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            cmd += "" --data-location="" + self.args.data_location\n        if self.args.accuracy_only:\n            cmd += "" --accuracy-only""\n\n        self.run_command(cmd)\n\n    def run_calibration(self):\n        calibration_script = os.path.join(self.args.intelai_models, self.args.mode,\n                                          self.args.precision, ""calibration.py"")\n        script_args_list = [\n            ""input_graph"", ""data_location"",\n            ""batch_size"",\n            ""num_inter_threads"", ""num_intra_threads""]\n        cmd_prefix = self.get_command_prefix(self.args.socket_id) + \\\n            self.python_exe + "" "" + calibration_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run(self):\n        # Parse custom arguments and append to self.args\n        self.parse_args()\n        if self.args.accuracy_only and self.args.calibration_only:\n            self.run_calibration()\n        else:\n            self.run_benchmark_or_accuracy()\n'"
benchmarks/image_recognition/tensorflow/resnet50/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport os\nfrom argparse import ArgumentParser\nimport time\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""initialize mode and run benchmark""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.benchmark_command = """"\n        if not platform_util:\n            raise ValueError(""Did not find any platform info."")\n\n        # use default batch size if -1\n        if self.args.batch_size == -1:\n            self.args.batch_size = 128\n\n        # set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        arg_parser = ArgumentParser(description=\'Parse args\')\n\n        arg_parser.add_argument(""--warmup-steps"", dest=\'warmup_steps\',\n                                type=int, default=10,\n                                help=""number of warmup steps"")\n        arg_parser.add_argument(""--steps"", dest=\'steps\',\n                                type=int, default=50,\n                                help=""number of steps"")\n        arg_parser.add_argument(\n            \'--kmp-blocktime\', dest=\'kmp_blocktime\',\n            help=\'number of kmp block time\',\n            type=int, default=1)\n\n        self.args = arg_parser.parse_args(self.custom_args, namespace=self.args)\n\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path, kmp_blocktime=str(self.args.kmp_blocktime))\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        benchmark_script = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            ""eval_image_classifier_inference.py"")\n\n        self.benchmark_command = self.get_command_prefix(args.socket_id) + \\\n            self.python_exe + "" "" + benchmark_script\n\n        num_cores = self.platform_util.num_cores_per_socket if self.args.num_cores == -1 \\\n            else self.args.num_cores\n\n        self.benchmark_command = \\\n            self.benchmark_command + \\\n            "" --input-graph="" + self.args.input_graph + \\\n            "" --num-inter-threads="" + str(self.args.num_inter_threads) + \\\n            "" --num-intra-threads="" + str(self.args.num_intra_threads) + \\\n            "" --num-cores="" + str(num_cores) + \\\n            "" --batch-size="" + str(self.args.batch_size) + \\\n            "" --warmup-steps="" + str(self.args.warmup_steps) + \\\n            "" --steps="" + str(self.args.steps)\n\n        if self.args.data_num_inter_threads:\n            self.benchmark_command += "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads)\n        if self.args.data_num_intra_threads:\n            self.benchmark_command += "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads)\n\n        # if the data location directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            self.benchmark_command += "" --data-location="" + \\\n                                      self.args.data_location\n        if self.args.accuracy_only:\n            self.benchmark_command += "" --accuracy-only""\n\n        # if output results is enabled, generate a results file name and pass it to the inference script\n        if self.args.output_results:\n            self.results_filename = ""{}_{}_{}_results_{}.txt"".format(\n                self.args.model_name, self.args.precision, self.args.mode,\n                time.strftime(""%Y%m%d_%H%M%S"", time.gmtime()))\n            self.results_file_path = os.path.join(self.args.output_dir, self.results_filename)\n            self.benchmark_command += "" --results-file-path {}"".format(self.results_file_path)\n\n    def run(self):\n        if self.benchmark_command:\n            self.run_command(self.benchmark_command)\n\n            if self.args.output_results:\n                print(""Inference results file in the output directory: {}"".format(self.results_filename))\n'"
benchmarks/image_recognition/tensorflow/resnet50/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport argparse\nimport os\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for resnet50 int8 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # Set the num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n        # Set env vars, if they haven\'t already been set\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads, overwrite_existing=True)\n\n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            ""--warmup-steps"", dest=""warmup_steps"",\n            help=""number of warmup steps"",\n            type=int, default=10)\n        parser.add_argument(\n            ""--steps"", dest=""steps"",\n            help=""number of steps"",\n            type=int, default=50)\n        parser.add_argument(\n            \'--kmp-blocktime\', dest=\'kmp_blocktime\',\n            help=\'number of kmp block time\',\n            type=int, default=1)\n        parser.add_argument(\n            ""--calibration-only"",\n            help=""Calibrate the accuracy."",\n            dest=""calibration_only"", action=""store_true"")\n        parser.add_argument(\n            ""--calibrate"", dest=""calibrate"",\n            help="" run accuracy with calibration data, ""\n                 ""to generate min_max ranges, calibrate=[True/False]"",\n            type=bool, default=False)\n\n        self.args = parser.parse_args(self.custom_args,\n                                      namespace=self.args)\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path, kmp_blocktime=str(self.args.kmp_blocktime))\n\n    def run_benchmark_or_accuracy(self):\n        cmd = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            ""eval_image_classifier_inference.py"")\n\n        cmd = self.get_command_prefix(self.args.socket_id) + self.python_exe + "" "" + cmd\n\n        cmd += "" --input-graph="" + self.args.input_graph + \\\n               "" --num-inter-threads="" + str(self.args.num_inter_threads) + \\\n               "" --num-intra-threads="" + str(self.args.num_intra_threads) + \\\n               "" --batch-size="" + str(self.args.batch_size) + \\\n               "" --warmup-steps="" + str(self.args.warmup_steps) + \\\n               "" --steps="" + str(self.args.steps)\n\n        if self.args.calibrate:\n            cmd += "" --calibrate="" + str(self.args.calibrate)\n        if self.args.data_num_inter_threads:\n            cmd += "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads)\n        if self.args.data_num_intra_threads:\n            cmd += "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads)\n\n        # if the data location directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            cmd += "" --data-location="" + self.args.data_location\n        if self.args.accuracy_only:\n            cmd += "" --accuracy-only""\n\n        self.run_command(cmd)\n\n    def run_calibration(self):\n        calibration_script = os.path.join(self.args.intelai_models,\n                                          self.args.precision,\n                                          ""generate_calibration_data.py"")\n        script_args_list = [\n            ""input_graph"", ""data_location"",\n            ""batch_size"",\n            ""num_inter_threads"", ""num_intra_threads""]\n        cmd_prefix = self.get_command_prefix(self.args.socket_id) + \\\n            self.python_exe + "" "" + calibration_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run(self):\n        # Parse custom arguments and append to self.args\n        self.parse_args()\n        if self.args.accuracy_only and self.args.calibration_only:\n            self.run_calibration()\n        else:\n            self.run_benchmark_or_accuracy()\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport os\nfrom argparse import ArgumentParser\nimport time\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""initialize mode and run benchmark""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.benchmark_command = """"\n        if not platform_util:\n            raise ValueError(""Did not find any platform info."")\n\n        # use default batch size if -1\n        if self.args.batch_size == -1:\n            self.args.batch_size = 128\n\n        # set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        arg_parser = ArgumentParser(description=\'Parse args\')\n\n        arg_parser.add_argument(""--warmup-steps"", dest=\'warmup_steps\',\n                                type=int, default=10,\n                                help=""number of warmup steps"")\n        arg_parser.add_argument(""--steps"", dest=\'steps\',\n                                type=int, default=50,\n                                help=""number of steps"")\n        arg_parser.add_argument(\n            \'--kmp-blocktime\', dest=\'kmp_blocktime\',\n            help=\'number of kmp block time\',\n            type=int, default=1)\n\n        self.args = arg_parser.parse_args(self.custom_args, namespace=self.args)\n\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path, kmp_blocktime=str(self.args.kmp_blocktime))\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        benchmark_script = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            ""eval_image_classifier_inference.py"")\n\n        self.benchmark_command = self.get_command_prefix(args.socket_id) + \\\n            self.python_exe + "" "" + benchmark_script\n\n        num_cores = self.platform_util.num_cores_per_socket if self.args.num_cores == -1 \\\n            else self.args.num_cores\n\n        self.benchmark_command = \\\n            self.benchmark_command + \\\n            "" --input-graph="" + self.args.input_graph + \\\n            "" --num-inter-threads="" + str(self.args.num_inter_threads) + \\\n            "" --num-intra-threads="" + str(self.args.num_intra_threads) + \\\n            "" --num-cores="" + str(num_cores) + \\\n            "" --batch-size="" + str(self.args.batch_size) + \\\n            "" --warmup-steps="" + str(self.args.warmup_steps) + \\\n            "" --steps="" + str(self.args.steps)\n\n        if self.args.data_num_inter_threads:\n            self.benchmark_command += "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads)\n        if self.args.data_num_intra_threads:\n            self.benchmark_command += "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads)\n\n        # if the data location directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            self.benchmark_command += "" --data-location="" + \\\n                                      self.args.data_location\n        if self.args.accuracy_only:\n            self.benchmark_command += "" --accuracy-only""\n\n        # if output results is enabled, generate a results file name and pass it to the inference script\n        if self.args.output_results:\n            self.results_filename = ""{}_{}_{}_results_{}.txt"".format(\n                self.args.model_name, self.args.precision, self.args.mode,\n                time.strftime(""%Y%m%d_%H%M%S"", time.gmtime()))\n            self.results_file_path = os.path.join(self.args.output_dir, self.results_filename)\n            self.benchmark_command += "" --results-file-path {}"".format(self.results_file_path)\n\n    def run(self):\n        if self.benchmark_command:\n            self.run_command(self.benchmark_command)\n\n            if self.args.output_results:\n                print(""Inference results file in the output directory: {}"".format(self.results_filename))\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport argparse\nimport os\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for resnet50 int8 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # Set the num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n        # Set env vars, if they haven\'t already been set\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads, overwrite_existing=True)\n\n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            ""--warmup-steps"", dest=""warmup_steps"",\n            help=""number of warmup steps"",\n            type=int, default=10)\n        parser.add_argument(\n            ""--steps"", dest=""steps"",\n            help=""number of steps"",\n            type=int, default=50)\n        parser.add_argument(\n            \'--kmp-blocktime\', dest=\'kmp_blocktime\',\n            help=\'number of kmp block time\',\n            type=int, default=1)\n        parser.add_argument(\n            ""--calibration-only"",\n            help=""Calibrate the accuracy."",\n            dest=""calibration_only"", action=""store_true"")\n        parser.add_argument(\n            ""--calibrate"", dest=""calibrate"",\n            help="" run accuracy with calibration data, ""\n                 ""to generate min_max ranges, calibrate=[True/False]"",\n            type=bool, default=False)\n\n        self.args = parser.parse_args(self.custom_args,\n                                      namespace=self.args)\n\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path, kmp_blocktime=str(self.args.kmp_blocktime))\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n    def run_benchmark_or_accuracy(self):\n        cmd = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            ""eval_image_classifier_inference.py"")\n\n        cmd = self.get_command_prefix(self.args.socket_id) + self.python_exe + "" "" + cmd\n\n        cmd += "" --input-graph="" + self.args.input_graph + \\\n               "" --num-inter-threads="" + str(self.args.num_inter_threads) + \\\n               "" --num-intra-threads="" + str(self.args.num_intra_threads) + \\\n               "" --batch-size="" + str(self.args.batch_size) + \\\n               "" --warmup-steps="" + str(self.args.warmup_steps) + \\\n               "" --steps="" + str(self.args.steps)\n\n        if self.args.calibrate:\n            cmd += "" --calibrate="" + str(self.args.calibrate)\n        if self.args.data_num_inter_threads:\n            cmd += "" --data-num-inter-threads="" + str(self.args.data_num_inter_threads)\n        if self.args.data_num_intra_threads:\n            cmd += "" --data-num-intra-threads="" + str(self.args.data_num_intra_threads)\n\n        # if the data location directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            cmd += "" --data-location="" + self.args.data_location\n        if self.args.accuracy_only:\n            cmd += "" --accuracy-only""\n\n        self.run_command(cmd)\n\n    def run_calibration(self):\n        calibration_script = os.path.join(self.args.intelai_models,\n                                          self.args.precision,\n                                          ""generate_calibration_data.py"")\n        script_args_list = [\n            ""input_graph"", ""data_location"",\n            ""batch_size"",\n            ""num_inter_threads"", ""num_intra_threads""]\n        cmd_prefix = self.get_command_prefix(self.args.socket_id) + \\\n            self.python_exe + "" "" + calibration_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run(self):\n        # Parse custom arguments and append to self.args\n        self.parse_args()\n        if self.args.accuracy_only and self.args.calibration_only:\n            self.run_calibration()\n        else:\n            self.run_benchmark_or_accuracy()\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/training/common_resnet50/__init__.py,0,b''
benchmarks/image_recognition/tensorflow/resnet50v1_5/training/common_resnet50/resnet50_model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\nimport os\nfrom argparse import ArgumentParser\nimport time\n\n\nclass ResNet50ModelInitializer(BaseModelInitializer):\n    """"""initialize mode and run benchmark""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ResNet50ModelInitializer, self).__init__(\n            args, custom_args, platform_util)\n\n        self.benchmark_command = """"\n        if not platform_util:\n            raise ValueError(""Did not find any platform info."")\n\n        # use default batch size if -1\n        if self.args.batch_size == -1:\n            self.args.batch_size = 64\n\n        # set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        arg_parser = ArgumentParser(description=\'Parse args\')\n\n        arg_parser.add_argument(""--steps"", dest=\'steps\',\n                                type=int, default=112590,\n                                help=""number of steps"")\n        arg_parser.add_argument(""--train_epochs"", dest=\'trainepochs\',\n                                type=int, default=72,\n                                help=""number of epochs"")\n        arg_parser.add_argument(""--epochs_between_evals"", dest=\'epochsbtwevals\',\n                                type=int, default=1,\n                                help=""number of epochs between eval"")\n\n        self.args = arg_parser.parse_args(self.custom_args, namespace=self.args)\n\n        # Set KMP env vars, if they haven\'t already been set, but override the default KMP_BLOCKTIME value\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        benchmark_script = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            ""mlperf_resnet/imagenet_main.py"")\n\n        # We need to change directory to model source to avoid python\n        # module name conflicts.\n        #self.benchmark_command = ""cd "" + self.args.model_source_dir + \\\n        #    ""/models && "" + self.get_command_prefix(args.socket_id) + \\\n        #    self.python_exe + "" "" + benchmark_script\n\n        self.benchmark_command = ""PYTHONPATH=$PYTHONPATH:"" + \\\n            os.path.join(self.args.intelai_models, self.args.mode) + \\\n            "" "" + self.get_command_prefix(args.socket_id) + \\\n            self.python_exe + "" "" + benchmark_script\n\n        # Model requires random_seed. Just setting it to a random value.\n        random_seed = 2\n        self.benchmark_command = \\\n            self.benchmark_command + \\\n            "" "" + str(random_seed) + \\\n            "" --batch_size="" + str(self.args.batch_size) + \\\n            "" --max_train_steps="" + str(self.args.steps) + \\\n            "" --train_epochs="" + str(self.args.trainepochs) + \\\n            "" --epochs_between_evals="" + str(self.args.epochsbtwevals) + \\\n            "" --inter_op_parallelism_threads "" + str(self.args.num_inter_threads) + \\\n            "" --intra_op_parallelism_threads "" + str(self.args.num_intra_threads) + \\\n            "" --version 1 --resnet_size 50""\n\n        # if the data location and checkpoint directory is not empty, then include the arg\n        if self.args.data_location and os.listdir(self.args.data_location):\n            self.benchmark_command += "" --data_dir="" + \\\n                                      self.args.data_location\n        if self.args.checkpoint:\n            self.benchmark_command += "" --model_dir="" + \\\n                                      self.args.checkpoint\n\n    def run(self):\n        if self.benchmark_command:\n            self.run_command(self.benchmark_command)\n'"
benchmarks/image_recognition/tensorflow/resnet50v1_5/training/fp32/__init__.py,0,b''
benchmarks/image_recognition/tensorflow/resnet50v1_5/training/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom ..common_resnet50.resnet50_model_init import ResNet50ModelInitializer\n\n\nclass ModelInitializer(ResNet50ModelInitializer):\n    """"""Initialize FP32 model and run benchmark""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n'"
benchmarks/language_translation/tensorflow/mlperf_gnmt/inference/fp32/__init__.py,0,b''
benchmarks/language_translation/tensorflow/mlperf_gnmt/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nimport os\nfrom argparse import ArgumentParser\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for GNMT FP32 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n        self.cmd = self.get_command_prefix(self.args.socket_id)\n\n        if self.args.socket_id != -1 and self.args.num_cores != -1:\n            self.cmd += ""--physcpubind=0-"" + \\\n                        (str(self.args.num_cores - 1)) + "" ""\n        self.cmd += ""{} "".format(self.python_exe)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        # use default batch size if -1\n        if self.args.batch_size == -1:\n            self.args.batch_size = 32\n\n        # set num_inter_threads and num_intra_threads (override inter threads to 2)\n        self.set_num_inter_intra_threads()\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n        arg_parser = ArgumentParser(description=""process custom_args"")\n        self.args = arg_parser.parse_args(self.custom_args, namespace=self.args)\n        src_vocab_file = os.path.join(self.args.data_location, ""vocab.bpe.32000.en"")\n        tgt_vocab_file = os.path.join(self.args.data_location, ""vocab.bpe.32000.de"")\n        inference_input_file = os.path.join(self.args.data_location, ""newstest2014.tok.bpe.32000.en"")\n        inference_ref_file = os.path.join(self.args.data_location, ""newstest2014.tok.bpe.32000.de"")\n\n        cmd_args = "" --in_graph="" + self.args.input_graph + \\\n                   "" --batch_size="" + str(self.args.batch_size) + \\\n                   "" --num_inter_threads="" + str(self.args.num_inter_threads) + \\\n                   "" --num_intra_threads="" + str(self.args.num_intra_threads) + \\\n                   "" --src_vocab_file="" + src_vocab_file + \\\n                   "" --tgt_vocab_file="" + tgt_vocab_file + \\\n                   "" --inference_input_file="" + inference_input_file + \\\n                   "" --inference_ref_file="" + inference_ref_file\n\n        run_script = os.path.join(self.args.intelai_models,\n                                  self.args.precision, ""run_inference.py"")\n\n        self.cmd = self.cmd + run_script + cmd_args\n\n    def run(self):\n        if self.cmd:\n            self.run_command(self.cmd)\n'"
benchmarks/language_translation/tensorflow/transformer_lt_official/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/language_translation/tensorflow/transformer_lt_official/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nimport os\nfrom argparse import ArgumentParser\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for Transformer LT FP32 inference""""""\n\n    def __init__(self, args, custom_args, platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.cmd = self.get_command_prefix(self.args.socket_id)\n        self.bleu_params = """"\n\n        self.set_num_inter_intra_threads()\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        MODEL_EXEC_DIR = os.path.join(self.args.intelai_models, self.args.mode, self.args.precision)\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        if self.args.socket_id != -1:\n            if self.args.num_cores != -1:\n                self.cmd += ""--physcpubind=0-"" + \\\n                            (str(self.args.num_cores - 1)) + "" ""\n        self.cmd += self.python_exe\n\n        run_script = os.path.join(MODEL_EXEC_DIR, ""infer_ab.py"")\n\n        # Model args\n        arg_parser = ArgumentParser(description=\'process custom_args\')\n        arg_parser.add_argument(\'--param_set\',\n                                help=\'hparameter setting\',\n                                dest=""param_set"",\n                                default=""big"")\n        arg_parser.add_argument(\'--vocab_file\',\n                                help=\'input vocable file for translation\',\n                                dest=""vocab_file"",\n                                default=""vocab.txt"")\n        arg_parser.add_argument(\'--in_graph\',\n                                help=\'input fp32 frozen graph file for inference\',\n                                dest=""fp32_graph"",\n                                default=""fp32_graphdef.pb"")\n        arg_parser.add_argument(\'--file\',\n                                help=\'decode input file with path\',\n                                dest=""decode_from_file"",\n                                default="""")\n        arg_parser.add_argument(\'--file_out\',\n                                help=\'inference output file name\',\n                                dest=""decode_to_file"",\n                                default=""translate.txt"")\n        arg_parser.add_argument(\'--reference\',\n                                help=\'inference ref file with path\',\n                                dest=""reference"",\n                                default="""")\n\n        self.args = arg_parser.parse_args(self.custom_args,\n                                          namespace=self.args)\n\n        # Model parameter control\n        translate_file = os.path.join(self.args.output_dir,\n                                      self.args.decode_to_file)\n        cmd_args = "" --param_set="" + self.args.param_set + \\\n                   "" --in_graph="" + self.args.fp32_graph + \\\n                   "" --batch_size="" + \\\n                   (str(self.args.batch_size)\n                    if self.args.batch_size != -1 else ""1"") + \\\n                   "" --file="" + self.args.decode_from_file + \\\n                   "" --file_out="" + translate_file + \\\n                   "" --vocab_file="" + self.args.vocab_file +\\\n                   "" --num_inter="" + str(self.args.num_inter_threads) +\\\n                   "" --num_intra="" + str(self.args.num_intra_threads)\n\n        self.bleu_params += "" --translation="" + translate_file + \\\n                            "" --reference="" + self.args.reference\n\n        self.cmd += "" "" + run_script + cmd_args\n        compute_bleu_script = os.path.join(MODEL_EXEC_DIR, ""compute_bleu.py"")\n        self.bleucmd = self.python_exe + "" "" + compute_bleu_script \\\n            + self.bleu_params\n\n    def run(self):\n        original_dir = os.getcwd()\n        #os.chdir(self.args.model_source_dir)\n        self.run_command(self.cmd)\n\n        # calculate the bleu number after inference is done\n        os.system(self.bleucmd)\n        os.chdir(original_dir)\n'"
benchmarks/object_detection/tensorflow/rfcn/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/rfcn/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport argparse\nimport os\nimport sys\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    accuracy_script = ""coco_mAP.sh""\n    accuracy_script_path = """"\n\n    def run_inference_sanity_checks(self, args, custom_args):\n        if args.batch_size != -1 and args.batch_size != 1:\n            sys.exit(""R-FCN inference supports \'batch-size=1\' "" +\n                     ""only, please modify via the \'--batch_size\' flag."")\n\n    def __init__(self, args, custom_args, platform_util):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.accuracy_script_path = os.path.join(\n            self.args.intelai_models, self.args.mode, self.args.precision,\n            self.accuracy_script)\n        self.benchmark_script = os.path.join(\n            self.args.intelai_models, self.args.mode,\n            self.args.precision, ""run_rfcn_inference.py"")\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        # Set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n        self.run_inference_sanity_checks(self.args, self.custom_args)\n        self.parse_custom_args()\n        self.research_dir = os.path.join(self.args.model_source_dir,\n                                         ""research"")\n\n    def parse_custom_args(self):\n        if self.custom_args:\n            parser = argparse.ArgumentParser()\n            mutex_group = parser.add_mutually_exclusive_group()\n            mutex_group.add_argument(""-x"", ""--number_of_steps"",\n                                     help=""Run for n number of steps"",\n                                     type=int, default=None)\n            mutex_group.add_argument(\n                ""-v"", ""--visualize"",\n                help=""Whether to visualize the output image"",\n                action=""store_true"")\n            parser.add_argument(""-q"", ""--split"",\n                                help=""Location of accuracy data"",\n                                type=str, default=None)\n            self.args = parser.parse_args(self.custom_args, namespace=self.args)\n        else:\n            raise ValueError(""Custom parameters are missing..."")\n\n    def run_perf_command(self):\n        # Get the command previx, but numactl is added later in run_perf_command()\n        command = []\n        num_cores = str(self.platform_util.num_cores_per_socket)\n        if self.args.num_cores != -1:\n            num_cores = str(self.args.num_cores)\n\n        set_env_var(""OMP_NUM_THREADS"", num_cores)\n\n        if self.args.socket_id != -1:\n            command.append(""numactl"")\n            if self.args.socket_id:\n                socket_id = self.args.socket_id\n            else:\n                socket_id = ""0""\n\n            if self.args.num_cores != -1:\n                command.append(""-C"")\n                command.append(""+0"")\n                i = 1\n                while i < self.args.num_cores:\n                    command.append("",{}"".format(i))\n                    i += i\n\n            command.append(""-N"")\n            command.append(""{}"".format(socket_id))\n            command.append(""-m"")\n            command.append(""{}"".format(socket_id))\n\n        command += (self.python_exe, self.benchmark_script)\n        command += (""-m"", self.args.model_source_dir)\n        command += (""-g"", self.args.input_graph)\n        command += (""--num-intra-threads"", str(self.args.num_intra_threads))\n        command += (""--num-inter-threads"", str(self.args.num_inter_threads))\n        if self.args.number_of_steps:\n            command += (""-x"", ""{}"".format(self.args.number_of_steps))\n        if self.args.visualize:\n            command += (""-v"")\n        if self.args.data_location:\n            command += (""-d"", self.args.data_location)\n        self.run_command("" "".join(command))\n\n    def run_accuracy_command(self):\n        if not os.path.exists(self.accuracy_script_path):\n            raise ValueError(""Unable to locate the R-FCN accuracy script: ""\n                             ""{}"".format(self.accuracy_script_path))\n        command = ""FROZEN_GRAPH="" + self.args.input_graph\n\n        if self.args.data_location and os.path.exists(\n                self.args.data_location):\n            command += "" TF_RECORD_FILE="" + self.args.data_location\n        else:\n            raise ValueError(\n                ""Unable to locate the coco data record file at {}"".format(\n                    self.args.tf_record_file))\n\n        if self.args.split:\n            command += "" SPLIT="" + self.args.split\n        else:\n            raise ValueError(""Must specify SPLIT parameter"")\n\n        command += "" TF_MODELS_ROOT={}"".format(\n            self.args.model_source_dir)\n\n        command += "" "" + self.accuracy_script_path\n        self.run_command(command)\n\n    def run(self):\n        original_dir = os.getcwd()\n        os.chdir(self.research_dir)\n        if self.args.accuracy_only:\n            self.run_accuracy_command()\n        else:\n            self.run_perf_command()\n        os.chdir(original_dir)\n'"
benchmarks/object_detection/tensorflow/rfcn/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/rfcn/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport argparse\n\nfrom common.base_model_init import BaseModelInitializer, set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    command = []\n    RFCN_PERF_SCRIPT = ""run_rfcn_inference.py""\n    RFCN_ACCURACY_SCRIPT = ""coco_mAP.sh""\n    perf_script_path = """"\n    accuracy_script_path = """"\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n        self.perf_script_path = os.path.join(\n            self.args.intelai_models, self.args.mode, self.args.precision,\n            self.RFCN_PERF_SCRIPT)\n        self.accuracy_script_path = os.path.join(\n            self.args.intelai_models, self.args.mode, self.args.precision,\n            self.RFCN_ACCURACY_SCRIPT)\n\n        # remove intelai models path, so that imports don\'t conflict\n        if ""MOUNT_BENCHMARK"" in os.environ and \\\n                os.environ[""MOUNT_BENCHMARK""] in sys.path:\n            sys.path.remove(os.environ[""MOUNT_BENCHMARK""])\n        if self.args.intelai_models in sys.path:\n            sys.path.remove(self.args.intelai_models)\n\n        self.parse_args()\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        # Set num_inter_threads and num_intra_threads\n        self.set_num_inter_intra_threads()\n\n    def parse_args(self):\n        if self.custom_args:\n            parser = argparse.ArgumentParser()\n            mutex_group = parser.add_mutually_exclusive_group()\n            mutex_group.add_argument(""-x"", ""--number_of_steps"",\n                                     help=""Run for n number of steps"",\n                                     type=int, default=None)\n            mutex_group.add_argument(\n                ""-v"", ""--visualize"",\n                help=""Whether to visualize the output image"",\n                action=""store_true"")\n            parser.add_argument(\n                ""-t"", ""--timeline"",\n                help=""Output file name for TF timeline"",\n                type=str, default=None)\n            parser.add_argument(""-e"", ""--evaluate_tensor"",\n                                help=""Full tensor name to evaluate"",\n                                type=str, default=None)\n            parser.add_argument(""-p"", ""--print_accuracy"",\n                                help=""Print accuracy results"",\n                                action=""store_true"")\n            parser.add_argument(""-q"", ""--split"",\n                                help=""Location of accuracy data"",\n                                type=str, default=None)\n            self.args = parser.parse_args(self.custom_args,\n                                          namespace=self.args)\n            self.validate_args()\n        else:\n            raise ValueError(""Custom parameters are missing..."")\n\n    def validate_args(self):\n        if not (self.args.batch_size == -1 or self.args.batch_size == 1):\n            raise ValueError(\n                ""Batch Size specified: {}. R-FCN inference only supports ""\n                ""batch size = 1"".format(self.args.batch_size))\n\n        if not os.path.exists(self.perf_script_path):\n            raise ValueError(""Unable to locate the R-FCN perf script: {}"".\n                             format(self.perf_script_path))\n\n        if not os.path.exists(self.accuracy_script_path):\n            raise ValueError(""Unable to locate the R-FCN accuracy script: ""\n                             ""{}"".format(self.accuracy_script_path))\n\n        if not self.args.model_source_dir or not os.path.isdir(\n                self.args.model_source_dir):\n            raise ValueError(""Unable to locate TensorFlow models at {}"".\n                             format(self.args.model_source_dir))\n\n    def run_perf_command(self):\n        # Get the command previx, but numactl is added later in run_perf_command()\n        self.command.append(self.get_command_prefix(self.args.socket_id, numactl=False))\n        num_cores = str(self.platform_util.num_cores_per_socket)\n        if self.args.num_cores != -1:\n            num_cores = str(self.args.num_cores)\n\n        set_env_var(""OMP_NUM_THREADS"", num_cores)\n\n        if self.args.socket_id != -1:\n            self.command.append(""numactl"")\n            if self.args.socket_id:\n                socket_id = self.args.socket_id\n            else:\n                socket_id = ""0""\n\n            if self.args.num_cores != -1:\n                self.command.append(""-C"")\n                self.command.append(""+0"")\n                i = 1\n                while i < self.args.num_cores:\n                    self.command.append("",{}"".format(i))\n                    i += i\n\n            self.command.append(""-N"")\n            self.command.append(""{}"".format(socket_id))\n            self.command.append(""-m"")\n            self.command.append(""{}"".format(socket_id))\n\n        self.command += (self.python_exe, self.perf_script_path)\n        self.command += (""-m"", self.args.model_source_dir)\n        self.command += (""-g"", self.args.input_graph)\n        self.command += (""--num-intra-threads"", str(self.args.num_intra_threads))\n        self.command += (""--num-inter-threads"", str(self.args.num_inter_threads))\n        if self.args.number_of_steps:\n            self.command += (""-x"", ""{}"".format(self.args.number_of_steps))\n        if self.args.visualize:\n            self.command += (""-v"")\n        if self.args.timeline:\n            self.command += (""-t"", self.args.timeline)\n        if self.args.data_location:\n            self.command += (""-d"", self.args.data_location)\n        if self.args.evaluate_tensor:\n            self.command += (""-e"", self.args.evaluate_tensor)\n        if self.args.print_accuracy:\n            self.command += (""-p"")\n        self.run_command("" "".join(self.command))\n\n    def run_accuracy_command(self):\n        # already validated by parent\n        self.command = self.get_command_prefix(self.args.socket_id, numactl=False)\n        self.command += ""FROZEN_GRAPH="" + self.args.input_graph\n\n        if self.args.data_location and os.path.exists(\n                self.args.data_location):\n            self.command += "" TF_RECORD_FILE="" + self.args.data_location\n        else:\n            raise ValueError(\n                ""Unable to locate the coco data record file at {}"".format(\n                    self.args.tf_record_file))\n\n        if self.args.split:\n            self.command += "" SPLIT="" + self.args.split\n        else:\n            raise ValueError(""Must specify SPLIT parameter"")\n\n        self.command += "" TF_MODELS_ROOT={}"".format(\n            self.args.model_source_dir)\n\n        self.command += "" "" + self.accuracy_script_path\n        self.run_command(self.command)\n\n    def run(self):\n        # Run script from the tensorflow models research directory\n        original_dir = os.getcwd()\n        os.chdir(os.path.join(self.args.model_source_dir, ""research""))\n        if self.args.accuracy_only:\n            self.run_accuracy_command()\n        else:\n            self.run_perf_command()\n        os.chdir(original_dir)\n'"
benchmarks/object_detection/tensorflow/ssd-mobilenet/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-mobilenet/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport os\nimport sys\n\nfrom common.base_model_init import BaseModelInitializer, set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    # SSD-MobileNet Int8 inference model initialization\n    args = None\n    custom_args = []\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        benchmark_script = os.path.join(self.args.intelai_models, self.args.mode,\n                                        self.args.precision, ""infer_detections.py"")\n        self.command_prefix = self.get_command_prefix(self.args.socket_id) + \\\n                ""{} {}"".format(self.python_exe, benchmark_script)                                        \n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        self.command_prefix += "" -g {0}"".format(self.args.input_graph)\n        self.command_prefix += "" -i 1000""\n        self.command_prefix += "" -w 200""\n        self.command_prefix += "" -a {0}"".format(self.args.num_intra_threads)\n        self.command_prefix += "" -e {0}"".format(self.args.num_inter_threads)\n        if self.args.data_location:\n            self.command_prefix += "" -d {0}"".format(self.args.data_location)\n\n        if self.args.accuracy_only:\n            self.command_prefix += "" -r""\n            assert self.args.data_location, ""accuracy must provide the data.""\n        else:\n            # Did not support multi-batch accuracy check.\n            self.command_prefix += "" -b {0}"".format(self.args.batch_size)\n\n    def run(self):\n        # Run script from the tensorflow models research directory\n        self.run_command(self.command_prefix)'"
benchmarks/object_detection/tensorflow/ssd-mobilenet/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-mobilenet/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport os\nimport sys\n\nfrom common.base_model_init import BaseModelInitializer, set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    # SSD-MobileNet Int8 inference model initialization\n    args = None\n    custom_args = []\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        benchmark_script = os.path.join(self.args.intelai_models, self.args.mode,\n                                        self.args.precision, ""infer_detections.py"")\n        self.command_prefix = self.get_command_prefix(self.args.socket_id) + \\\n                ""{} {}"".format(self.python_exe, benchmark_script)                                        \n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        self.command_prefix += "" -g {0}"".format(self.args.input_graph)\n        self.command_prefix += "" -i 1000""\n        self.command_prefix += "" -w 200""\n        self.command_prefix += "" -a {0}"".format(self.args.num_intra_threads)\n        self.command_prefix += "" -e {0}"".format(self.args.num_inter_threads)\n        if self.args.data_location:\n            self.command_prefix += "" -d {0}"".format(self.args.data_location)\n\n        if self.args.accuracy_only:\n            self.command_prefix += "" -r""\n            assert self.args.data_location, ""accuracy must provide the data.""\n        else:\n            # Did not support multi-batch accuracy check.\n            self.command_prefix += "" -b {0}"".format(self.args.batch_size)\n\n    def run(self):\n        # Run script from the tensorflow models research directory\n        self.run_command(self.command_prefix)'"
benchmarks/object_detection/tensorflow/ssd-resnet34/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-resnet34/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport os\nimport sys\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    def run_inference_sanity_checks(self, args, custom_args):\n        if not args.input_graph:\n            sys.exit(""Please provide a path to the frozen graph directory""\n                     "" via the \'--in-graph\' flag."")\n        if not args.data_location and self.args.accuracy_only:\n            sys.exit(""Please provide a path to the data directory via the ""\n                     ""\'--data-location\' flag."")\n        if args.socket_id == -1 and args.num_cores == -1:\n            print(""***Warning***: Running inference on all cores could degrade""\n                  "" performance. Pass a \'--socket-id\' to specify running on a""\n                  "" single socket instead.\\n"")\n\n    def __init__(self, args, custom_args, platform_util):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.run_inference_sanity_checks(self.args, self.custom_args)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        self.set_num_inter_intra_threads()\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        self.model_dir = os.path.join(self.args.intelai_models, self.args.mode, self.args.precision)\n\n        # get benchmark command\n        benchmark_script = os.path.join(self.model_dir, ""infer_detections.py"")\n\n        # get command with numactl\n        self.run_cmd = self.get_command_prefix(self.args.socket_id)\n        self.run_cmd += ""{0} {1}"".format(self.python_exe, benchmark_script)\n        self.run_cmd += "" --input-graph {0}"".format(self.args.input_graph)\n        self.run_cmd += "" --batch-size {0}"".format(args.batch_size)\n        self.run_cmd += "" --inter-op-parallelism-threads {0}"".format(self.args.num_inter_threads)\n        self.run_cmd += "" --intra-op-parallelism-threads {0}"".format(self.args.num_intra_threads)\n\n        if self.args.accuracy_only:\n            self.run_cmd += "" --accuracy-only ""\n            self.run_cmd += "" --data-location {0}"".format(self.args.data_location)\n\n    def run(self):\n        old_python_path = os.environ[""PYTHONPATH""]\n        os.environ[""PYTHONPATH""] = os.path.join(self.args.model_source_dir, ""research"")\n        os.environ[""PYTHONPATH""] += "":/tmp/benchmarks/scripts/tf_cnn_benchmarks/""\n        self.run_command(self.run_cmd)\n        os.environ[""PYTHONPATH""] = old_python_path\n'"
benchmarks/object_detection/tensorflow/ssd-resnet34/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/object_detection/tensorflow/ssd-resnet34/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport os\nimport sys\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    def run_inference_sanity_checks(self, args, custom_args):\n        if not args.input_graph:\n            sys.exit(""Please provide a path to the frozen graph directory""\n                     "" via the \'--in-graph\' flag."")\n        if not args.data_location and self.args.accuracy_only:\n            sys.exit(""Please provide a path to the data directory via the ""\n                     ""\'--data-location\' flag."")\n        if args.socket_id == -1 and args.num_cores == -1:\n            print(""***Warning***: Running inference on all cores could degrade""\n                  "" performance. Pass a \'--socket-id\' to specify running on a""\n                  "" single socket instead.\\n"")\n\n    def __init__(self, args, custom_args, platform_util):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        self.run_inference_sanity_checks(self.args, self.custom_args)\n\n        # Set KMP env vars, if they haven\'t already been set\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path)\n\n        self.set_num_inter_intra_threads()\n\n        set_env_var(""OMP_NUM_THREADS"", self.args.num_intra_threads)\n\n        self.model_dir = os.path.join(self.args.intelai_models, self.args.mode, self.args.precision)\n\n        # get benchmark command\n        benchmark_script = os.path.join(self.model_dir, ""infer_detections.py"")\n\n        # get command with numactl\n        self.run_cmd = self.get_command_prefix(self.args.socket_id)\n        self.run_cmd += ""{0} {1}"".format(self.python_exe, benchmark_script)\n        self.run_cmd += "" --input-graph {0}"".format(self.args.input_graph)\n        self.run_cmd += "" --batch-size {0}"".format(args.batch_size)\n        self.run_cmd += "" --inter-op-parallelism-threads {0}"".format(self.args.num_inter_threads)\n        self.run_cmd += "" --intra-op-parallelism-threads {0}"".format(self.args.num_intra_threads)\n\n        if self.args.accuracy_only:\n            self.run_cmd += "" --accuracy-only ""\n            self.run_cmd += "" --data-location {0}"".format(self.args.data_location)\n\n    def run(self):\n        old_python_path = os.environ[""PYTHONPATH""]\n        os.environ[""PYTHONPATH""] = os.path.join(self.args.model_source_dir, ""research"")\n        os.environ[""PYTHONPATH""] += "":/tmp/benchmarks/scripts/tf_cnn_benchmarks/""\n        self.run_command(self.run_cmd)\n        os.environ[""PYTHONPATH""] = old_python_path\n'"
benchmarks/recommendation/tensorflow/wide_deep/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep/inference/fp32/data_download.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Download and clean the Census Income Dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\nimport requests\n\nDATA_URL = \'https://archive.ics.uci.edu/ml/machine-learning-databases/adult\'\nTRAINING_FILE = \'adult.data\'\nTRAINING_URL = \'%s/%s\' % (DATA_URL, TRAINING_FILE)\nEVAL_FILE = \'adult.test\'\nEVAL_URL = \'%s/%s\' % (DATA_URL, EVAL_FILE)\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--data_dir\', type=str, default=\'/tmp/census_data\',\n                    help=\'Directory to download census data\')\nparser.add_argument(\'--http_proxy\', type=str, default=None)\nparser.add_argument(\'--https_proxy\', type=str, default=None)\n\n\ndef download_and_clean_file(filename, url):\n    """"""Downloads data from url, and makes changes to match the CSV format.""""""\n    proxies = {}\n    print(filename)\n    if ARGS.http_proxy:\n        proxies[\'http\'] = ARGS.http_proxy\n    if ARGS.https_proxy:\n        proxies[\'https\'] = ARGS.https_proxy\n    try:\n        request = requests.get(url, stream=True, proxies=proxies)\n        request.raise_for_status()\n        with open(filename, \'wb\') as eval_file:\n            for line in request.iter_lines():\n                line = line.strip()\n                line = line.replace(b\', \', b\',\')\n                if not line or b\',\' not in line:\n                    continue\n                if line[-1] == \'.\':\n                    line = line[:-1]\n                line += b\'\\n\'\n                eval_file.write(line)\n    except requests.exceptions.HTTPError as err:\n        print(err)\n        sys.exit(1)\n\n\ndef main():\n    if not os.path.exists(ARGS.data_dir):\n        os.mkdir(ARGS.data_dir)\n\n    training_file_path = os.path.join(ARGS.data_dir, TRAINING_FILE)\n    download_and_clean_file(training_file_path, TRAINING_URL)\n\n    eval_file_path = os.path.join(ARGS.data_dir, EVAL_FILE)\n    download_and_clean_file(eval_file_path, EVAL_URL)\n\n\nif __name__ == \'__main__\':\n    ARGS = parser.parse_args()\n    main()\n    print(""Wide & Deep dataset is downloaded at {}"".format(ARGS.data_dir))\n'"
benchmarks/recommendation/tensorflow/wide_deep/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport os\n\nfrom common.base_model_init import BaseModelInitializer\nfrom common.base_model_init import set_env_var\n\n\nclass ModelInitializer(BaseModelInitializer):\n    \'\'\'Add code here to detect the environment and set necessary variables\n    before launching the model\'\'\'\n\n    def __init__(self, args, custom_args, platform_util):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        set_env_var(""OMP_NUM_THREADS"", ""1"")\n\n        if args.batch_size == -1:\n            args.batch_size = 1\n            if args.verbose:\n                print(""Setting batch_size to 1 since it is not supplied."")\n\n        if args.batch_size == 1:\n            if args.verbose:\n                print(""Running Wide_Deep model Inference in Latency mode"")\n        else:\n            if args.verbose:\n                print(""Running Wide_Deep model Inference in Throughput mode"")\n\n        executable = os.path.join(args.mode, args.precision,\n                                  ""wide_deep_inference.py"")\n\n        self.run_cmd = "" OMP_NUM_THREADS=1"" + \\\n                       "" numactl --cpunodebind=0 --membind=0 "" + \\\n                       self.python_exe + "" "" + executable + \\\n                       "" --data_dir="" + self.args.data_location + \\\n                       "" --model_dir="" + self.args.checkpoint + \\\n                       "" --batch_size="" + str(self.args.batch_size)\n\n    def run(self):\n        original_dir = os.getcwd()\n        os.chdir(self.args.intelai_models)\n        self.run_command(self.run_cmd)\n        os.chdir(original_dir)\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/inference/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport argparse\nfrom common.base_model_init import BaseModelInitializer\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for Wide and deep large dataset FP32 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n        \n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(""--num_omp_threads"", dest=\'num_omp_threads\',\n                            type=str, default=None,\n                            help=""number of omp threads"")\n        parser.add_argument(""--use_parallel_batches"", dest=\'use_parallel_batches\',\n                            type=str, default=""False"",\n                            help=""Enable to batches in parallel"")\n        parser.add_argument(""--num_parallel_batches"",dest=\'num_parallel_batches\', default=""1"",\n                            type=str, help=""num of parallel batches.Default is 1"")\n        parser.add_argument(\'--kmp_block_time\', dest=\'kmp_block_time\',\n                            help=\'number of kmp block time.\',\n                            type=str, default=None)\n        parser.add_argument(\'--kmp_affinity\', dest=\'kmp_affinity\',\n                            help=\'kmp affinity value\',\n                            type=str, default=None)\n        parser.add_argument(\'--kmp_settings\', dest=\'kmp_settings\',\n                            help=\'kmp settings\',\n                            type=str, default=None)\n        self.args = parser.parse_args(self.custom_args,\n                                      namespace=self.args)\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path,kmp_settings=str(self.args.kmp_settings),kmp_blocktime=str(self.args.kmp_block_time),kmp_affinity=str(self.args.kmp_affinity))\n\n    def run_benchmark(self):\n        enable_parallel_batches = getattr(self.args, \'use_parallel_batches\')\n        script_args_list = [""input_graph"", ""batch_size"",\n                            ""num_inter_threads"", ""num_intra_threads"",\n                            ""accuracy_only"", ""data_location"", ""num_omp_threads""]\n        if enable_parallel_batches==\'True\':\n            benchmark_script = os.path.join(self.args.intelai_models,\n                                        self.args.mode, ""parallel_inference.py"") \n            script_args_list.append(""num_parallel_batches"")   \n        else:\n            benchmark_script = os.path.join(self.args.intelai_models,\n                                        self.args.mode, ""inference.py"")    \n        command_prefix = self.get_command_prefix(-1)\n        if self.args.socket_id != -1 and self.args.num_cores != -1:\n            command_prefix = command_prefix + "" numactl --physcpubind=0-{} --membind={} "".\\\n                format(str(int(self.args.num_cores) - 1), self.args.socket_id)\n        cmd_prefix = command_prefix + self.python_exe + "" "" + benchmark_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run(self):\n        # Parse custom arguments and append to self.args\n        self.parse_args()\n        self.run_benchmark()\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/inference/int8/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport argparse\nfrom common.base_model_init import BaseModelInitializer\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for Wide and deep large dataset INT8 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(""--num_omp_threads"", dest=\'num_omp_threads\',\n                            type=str, default=None,\n                            help=""number of omp threads"")\n        parser.add_argument(""--use_parallel_batches"", dest=\'use_parallel_batches\',\n                            type=str, default=""False"",\n                            help=""Enable to run batches in parallel"")\n        parser.add_argument(""--num_parallel_batches"",dest=\'num_parallel_batches\', default=""1"",\n                            type=str, help=""num of parallel batches.Default is 1"")\n        parser.add_argument(\'--kmp_block_time\', dest=\'kmp_block_time\',\n                            help=\'number of kmp block time.\',\n                            type=str, default=None)\n        parser.add_argument(\'--kmp_affinity\', dest=\'kmp_affinity\',\n                            help=\'kmp affinity value\',\n                            type=str, default=None)\n        parser.add_argument(\'--kmp_settings\', dest=\'kmp_settings\',\n                            help=\'kmp settings\',\n                            type=str, default=None)\n        self.args = parser.parse_args(self.custom_args,\n                                      namespace=self.args)\n        config_file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""config.json"")\n        self.set_kmp_vars(config_file_path,kmp_settings=str(self.args.kmp_settings),kmp_blocktime=str(self.args.kmp_block_time),kmp_affinity=str(self.args.kmp_affinity))\n\n    def run_benchmark(self):\n        enable_parallel_batches = getattr(self.args, \'use_parallel_batches\')\n        script_args_list = [""input_graph"", ""batch_size"",\n                            ""num_inter_threads"", ""num_intra_threads"",\n                            ""accuracy_only"", ""data_location"", ""num_omp_threads""]\n        if enable_parallel_batches==\'True\':\n            benchmark_script = os.path.join(self.args.intelai_models,\n                                        self.args.mode, ""parallel_inference.py"") \n            script_args_list.append(""num_parallel_batches"")   \n        else:\n            benchmark_script = os.path.join(self.args.intelai_models,\n                                        self.args.mode, ""inference.py"")    \n        command_prefix = self.get_command_prefix(-1)\n        if self.args.socket_id != -1 and self.args.num_cores != -1:\n            command_prefix = command_prefix + "" numactl --physcpubind=0-{} --membind={} "".\\\n                format(str(int(self.args.num_cores) - 1), self.args.socket_id)\n        cmd_prefix = command_prefix + self.python_exe + "" "" + benchmark_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run(self):\n        # Parse custom arguments and append to self.args\n        self.parse_args()\n        self.run_benchmark()\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/training/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
benchmarks/recommendation/tensorflow/wide_deep_large_ds/training/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport argparse\nfrom common.base_model_init import BaseModelInitializer\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for Wide and deep large dataset FP32 inference""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(""--steps"", dest=\'steps\',\n                            type=int, default=0,\n                            help=""number of train steps"")\n        self.args = parser.parse_args(self.custom_args,\n                                      namespace=self.args)\n    def run_benchmark(self):\n        benchmark_script = os.path.join(self.args.intelai_models,\n                                        self.args.mode, ""train.py"")\n        script_args_list = [""batch_size"", ""data_location"", ""steps""]\n        command_prefix = self.get_command_prefix(-1)\n        cmd_prefix = command_prefix + self.python_exe + "" "" + benchmark_script\n        cmd = self.add_args_to_command(cmd_prefix, script_args_list)\n        self.run_command(cmd)\n\n    def run(self):\n        # Parse custom arguments and append to self.args\n        self.parse_args()\n        self.run_benchmark()\n'"
benchmarks/reinforcement/tensorflow/minigo/training/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
benchmarks/reinforcement/tensorflow/minigo/training/fp32/model_init.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018-2020 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom common.base_model_init import BaseModelInitializer\n\nimport os\n\n\nimport argparse\n\n\n\nclass ModelInitializer(BaseModelInitializer):\n    """"""Model initializer for minigo""""""\n\n    def __init__(self, args, custom_args=[], platform_util=None):\n        super(ModelInitializer, self).__init__(args, custom_args, platform_util)\n\n        arg_parser = argparse.ArgumentParser(description=\'Parse additional args\')\n        \n        arg_parser.add_argument(\n            ""--quantization"", help=""quantization flag"",\n            dest=""quantization"", default=""False"")     \n        arg_parser.add_argument(\n            ""--large-scale"", help=""train on large scale"",\n            dest=""large_scale"", default=""False"") \n        arg_parser.add_argument(\n            ""--num-train-nodes"", help=""number of train nodes"",\n            dest=""num_train_nodes"", default=0, type=int) \n        arg_parser.add_argument(\n            ""--num-eval-nodes"", help=""number of evaluation nodes"",\n            dest=""num_eval_nodes"", default=0, type=int)  \n        arg_parser.add_argument(\n            ""--multi-node"", help=""train on large scale"",\n            dest=""multi_node"", default=""False"")    \n        \n        self.additional_args, unknown_args = arg_parser.parse_known_args(custom_args)\n        \n        if self.additional_args.large_scale == ""True"" and self.additional_args.multi_node == ""True"":\n            # multi-node training mode with large scale\n            self.cmd = ""./run_mn.sh ""\n            self.cmd += "" {0}"".format(self.additional_args.num_train_nodes)\n            self.cmd += "" {0}"".format(self.additional_args.num_eval_nodes)\n            self.cmd += "" {0}"".format(self.additional_args.quantization)\n        elif self.additional_args.large_scale == ""False"" and self.additional_args.multi_node == ""True"":\n            # multi-node training mode\n            self.cmd = ""./run_mn.sh ""\n            self.cmd += "" {0}"".format(self.additional_args.num_train_nodes)\n            self.cmd += "" {0}"".format(self.additional_args.quantization)\n        else:\n            # single-node training mode\n            self.cmd = ""./run.sh ""\n            self.cmd += "" {0}"".format(self.additional_args.quantization)\n            \n    def run(self):\n        org_path = os.getcwd()\n        os.chdir(self.args.model_source_dir)\n        self.run_command(self.cmd)\n        os.chdir(org_path)\n'"
models/image_recognition/tensorflow/densenet169/inference/fp32/accuracy.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport image_preprocessing\nimport dataset\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""densenet169/predictions/Reshape_1"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  input_height = args.input_height\n  input_width = args.input_width\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n\n  data_graph = tf.Graph() ###\n  with data_graph.as_default(): ###\n    dataset = dataset.ImagenetData(data_location)\n    preprocessor = image_preprocessing.ImagePreprocessor(\n        input_height, input_width, batch_size,\n        1, # device count\n        tf.float32, # data_type for input fed to the graph\n        train=False, # doing inference\n        resize_method=\'crop\')\n    images, labels = preprocessor.minibatch(dataset, subset=\'validation\')\n\n  graph = load_graph(model_file)\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n\n  rewrite_options = rewriter_config_pb2.RewriterConfig(\n          layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)\n\n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  config.graph_options.rewrite_options.remapping = (\n          rewriter_config_pb2.RewriterConfig.OFF)\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = dataset.num_examples_per_epoch(subset=\'validation\') \\\n                            - num_processed_images\n  top1 = 0\n  with tf.compat.v1.Session(graph=data_graph) as sess: ###\n    sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n\n    while num_remaining_images >= batch_size:\n      # Reads and preprocess data\n      #import pdb\n      #pdb.set_trace()\n      np_images, np_labels = sess.run([images[0], labels[0]])\n      np_labels -= 1\n      #print(np_labels.shape)\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      start_time = time.time()\n      # Compute inference on the preprocessed data\n      predictions1 = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      elapsed_time = time.time() - start_time\n      if(batch_size !=1):\n         predictions1 = sess.run(tf.squeeze(predictions1))\n      else :\n         predictions1 = sess.run(tf.reshape(predictions1,[1,1000]))\n      predictions2 = tf.argmax(input=predictions1, axis=1)\n      predictions = sess.run(predictions2)\n      top1 += batch_size - (np.count_nonzero(predictions - np_labels))\n      print(""Iteration time: %0.4f ms"" % elapsed_time)\n      print(top1/num_processed_images)\n'"
models/image_recognition/tensorflow/densenet169/inference/fp32/benchmark.py,11,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.  # You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom google.protobuf import text_format\nimport tensorflow as tf\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""densenet169/predictions/Reshape_1"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  parser.add_argument(""-gpu"", ""--gpu"",\n      default = -1,\n      type=int, help=""Run on gpu, other wise cpu"",\n      required=False)\n\n  parser.add_argument(""--warmup_steps"", type=int, default=40,\n                      help=""number of warmup steps"")\n  parser.add_argument(""--steps"", type=int, default=100, help=""number of steps"")\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  input_height = args.input_height\n  input_width = args.input_width\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  warmup_steps = args.warmup_steps\n  steps = args.steps\n  print(steps)\n  assert steps > 10, ""Benchmark steps should be at least 10.""\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n\n  data_graph = tf.Graph() ##\n  with data_graph.as_default():##\n    input_shape = [batch_size, input_height, input_width, 3]\n    images = tf.random.truncated_normal(\n          input_shape,\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\')\n\n  #image_data = None\n  \n  graph = load_graph(model_file)\n\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"");\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"");\n\n  rewrite_options = rewriter_config_pb2.RewriterConfig(\n          layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)\n  config = tf.compat.v1.ConfigProto()\n  if (args.gpu < 0):\n    config.inter_op_parallelism_threads = num_inter_threads\n    config.intra_op_parallelism_threads = num_intra_threads\n  config.graph_options.rewrite_options.remapping = (\n          rewriter_config_pb2.RewriterConfig.OFF)\n  #os.environ[""OMP_NUM_THREADS""] = ""14""\n  #with tf.compat.v1.Session(config=config) as sess:\n  #  image_data = sess.run(images)\n\n  data_config = tf.compat.v1.ConfigProto()###\n  data_config.inter_op_parallelism_threads = num_inter_threads ###\n  data_config.intra_op_parallelism_threads = num_intra_threads ###\n  \n  data_sess = tf.compat.v1.Session(graph=data_graph, config=data_config) ###\n  with tf.compat.v1.Session(graph=graph, config=config) as sess:\n    sys.stdout.flush()\n    print(""[Running warmup steps...]"")\n    image_data = data_sess.run(images) ###\n    for t in range(warmup_steps):\n      start_time = time.time()\n      sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t+1, batch_size/elapsed_time))\n    avg = 0\n    print(""[Running benchmark steps...]"")\n    total_time   = 0;\n    total_images = 0;\n    for t in range(steps):\n      start_time = time.time()\n      results = sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      avg += elapsed_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n    \t\t"""".format(t+1, batch_size*(t+1)/avg));\n        print(""       Latency: {0} ms""\n                """".format(avg*1000. /(t+1)))\n'"
models/image_recognition/tensorflow/densenet169/inference/fp32/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n\n'"
models/image_recognition/tensorflow/densenet169/inference/fp32/dataset.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport tensorflow as tf\n\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, data_dir=None):\n    self.name = name\n    if data_dir is None:\n      raise ValueError(\'Data directory not specified\')\n    self.data_dir = data_dir\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.TFRecordReader()\n\n  @abstractmethod\n  def num_classes(self):\n    pass\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n\nclass FlowersData(Dataset):\n\n  def __init__(self, data_dir=None):\n    super(FlowersData, self).__init__(\'Flowers\', data_dir)\n\n  def num_classes(self):\n    return 5\n\n  def num_examples_per_epoch(self, subset):\n    if subset == \'train\':\n      return 3170\n    elif subset == \'validation\':\n      return 500\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n\nclass ImagenetData(Dataset):\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'ImageNet\', data_dir)\n\n  def num_classes(self):\n    return 1000\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return 1281167\n    elif subset == \'validation\':\n      return 50000\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n'"
models/image_recognition/tensorflow/densenet169/inference/fp32/densenet_preprocessing.py,61,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n#slim = tf.contrib.slim ###\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_SCALE_FACTOR = 0.017\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(input=image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.cast(tf.stack([offset_height, offset_width, 0]), dtype=tf.int32)\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  with tf.control_dependencies([rank_assertions[0]]):\n    image_shape = tf.shape(input=image_list[0])\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    with tf.control_dependencies([rank_assertions[i]]):\n      shape = tf.shape(input=image)\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  with tf.control_dependencies(asserts):\n    max_offset_height = tf.reshape(image_height - crop_height + 1, [])\n  with tf.control_dependencies(asserts):\n    max_offset_width = tf.reshape(image_width - crop_width + 1, [])\n  offset_height = tf.random.uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random.uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(input=image)[0]\n    image_width = tf.shape(input=image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(value=smallest_side, dtype=tf.int32)\n\n  height = tf.cast(height, dtype=tf.float32)\n  width = tf.cast(width, dtype=tf.float32)\n  smallest_side = tf.cast(smallest_side, dtype=tf.float32)\n\n  scale = tf.cond(pred=tf.greater(height, width),\n                  true_fn=lambda: smallest_side / width,\n                  false_fn=lambda: smallest_side / height)\n  new_height = tf.cast(height * scale, dtype=tf.int32)\n  new_width = tf.cast(width * scale, dtype=tf.int32)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(value=smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(input=image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize(image, [new_height, new_width],\n                                           method=tf.image.ResizeMethod.BILINEAR)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random.uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.cast(image, dtype=tf.float32)\n  image = tf.image.random_flip_left_right(image)\n\n  image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n  return image * _SCALE_FACTOR\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.cast(image, dtype=tf.float32)\n\n  image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n  return image * _SCALE_FACTOR\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
models/image_recognition/tensorflow/densenet169/inference/fp32/image_preprocessing.py,90,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom random import randint\nimport densenet_preprocessing\nfrom tensorflow.python.ops import data_flow_ops\nimport cnn_util\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields:\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    text: Tensor tf.string containing the human-readable label.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n  """"""Decode a JPEG string into one 3-D float image Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n  # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n  with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 fancy_upscaling=False,\n                                 dct_method=\'INTEGER_FAST\')\n\n    # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n\n    return image\n\n\ndef eval_image(image, height, width, bbox, thread_id, resize):\n  """"""Get the image for model evaluation.""""""\n  with tf.compat.v1.name_scope(\'eval_image\'):\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'original_image\', tf.expand_dims(image, 0))\n\n    if resize == \'crop\':\n      # Note: This is much slower than crop_to_bounding_box\n      #         It seems that the redundant pad step has huge overhead\n      # distorted_image = tf.image.resize_image_with_crop_or_pad(image,\n      #                                                         height, width)\n      shape = tf.shape(input=image)\n      image = tf.cond(pred=tf.less(shape[0], shape[1]),\n                        true_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256, 256*shape[1]/shape[0]], dtype=tf.int32)),\n                        false_fn=lambda: tf.image.resize(image, tf.convert_to_tensor(value=[256*shape[0]/shape[1], 256], dtype=tf.int32)))\n      shape = tf.shape(input=image)\n\n      y0 = (shape[0] - height) // 2\n      x0 = (shape[1] - width) // 2\n      #y0=tf.random_uniform([],minval=0,maxval=(shape[0] - height + 1), dtype=tf.int32)\n      #x0=tf.random_uniform([],minval=0,maxval=(shape[1] - width + 1), dtype=tf.int32)\n      ## distorted_image = tf.slice(image, [y0,x0,0], [height,width,3])\n      distorted_image = tf.image.crop_to_bounding_box(image, y0, x0, height,\n                                                      width)\n    else:\n      sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n          image_size=tf.shape(input=image),\n          bounding_boxes=bbox,\n          min_object_covered=0.5,\n          aspect_ratio_range=[0.90, 1.10],\n          area_range=[0.10, 1.0],\n          max_attempts=100,\n          use_image_if_no_bounding_boxes=True)\n      bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n      # Crop the image to the specified bounding box.\n      distorted_image = tf.slice(image, bbox_begin, bbox_size)\n      resize_method = {\n          \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n          \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n          \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n          \'area\': tf.image.ResizeMethod.AREA\n      }[resize]\n      # This resizing operation may distort the images because the aspect\n      # ratio is not respected.\n      if cnn_util.tensorflow_version() >= 11:\n        distorted_image = tf.image.resize(\n            distorted_image, [height, width],\n            resize_method)\n      else:\n        distorted_image = tf.image.resize(\n            distorted_image, height, width, resize_method)\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\', tf.expand_dims(distorted_image, 0))\n    image = distorted_image\n  return image\n\n\ndef distort_image(image, height, width, bbox, thread_id=0, scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    image: 3-D float Tensor of image\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    thread_id: integer indicating the preprocessing thread.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training.\n  """"""\n  # with tf.op_scope([image, height, width, bbox], scope, \'distort_image\'):\n  # with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n  with tf.compat.v1.name_scope(scope or \'distort_image\'):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they\'re rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Display the bounding box in the first thread only.\n    if not thread_id:\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.compat.v1.summary.image(\n          \'image_with_bounding_boxes\', image_with_box)\n\n  # A large fraction of image datasets contain a human-annotated bounding\n  # box delineating the region of the image containing the object of interest.\n  # We choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an allowed\n  # range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        image_size=tf.shape(input=image),\n        bounding_boxes=bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=[0.99, 1.01],\n        area_range=[0.05, 1.0],\n        max_attempts=100,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n    if not thread_id:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distort_bbox)\n      tf.compat.v1.summary.image(\n          \'images_with_distorted_bounding_box\',\n          image_with_distorted_box)\n\n    # Crop the image to the specified bounding box.\n    distorted_image = tf.slice(image, bbox_begin, bbox_size)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n    resize_method = thread_id % 4\n    if cnn_util.tensorflow_version() >= 11:\n      distorted_image = tf.image.resize(\n          distorted_image, [height, width], resize_method)\n    else:\n      distorted_image = tf.image.resize(\n          distorted_image, height, width, resize_method)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'cropped_resized_image\',\n          tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors.\n    distorted_image = distort_color(distorted_image, thread_id)\n\n    # Note: This ensures the scaling matches the output of eval_image\n    distorted_image *= 256\n\n    if not thread_id:\n      tf.compat.v1.summary.image(\n          \'final_distorted_image\',\n          tf.expand_dims(distorted_image, 0))\n    return distorted_image\n\n\ndef distort_color(image, thread_id=0, scope=None):\n  """"""Distort the color of the image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: Tensor containing single image.\n    thread_id: preprocessing thread ID.\n    scope: Optional scope for op_scope.\n  Returns:\n    color-distorted image\n  """"""\n  # with tf.op_scope([image], scope, \'distort_color\'):\n  # with tf.name_scope(scope, \'distort_color\', [image]):\n  with tf.compat.v1.name_scope(scope or \'distort_color\'):\n    color_ordering = thread_id % 2\n\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n\nclass ImagePreprocessor(object):\n  """"""Preprocessor for input images.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               device_count,\n               dtype=tf.float32,\n               train=True,\n               distortions=None,\n               resize_method=None):\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.device_count = device_count\n    self.dtype = dtype\n    self.train = train\n    self.resize_method = resize_method\n    if distortions is None:\n      distortions = False\n    self.distortions = distortions\n    if self.batch_size % self.device_count != 0:\n      raise ValueError(\n          (\'batch_size must be a multiple of device_count: \'\n           \'batch_size %d, device_count: %d\') %\n          (self.batch_size, self.device_count))\n    self.batch_size_per_device = self.batch_size // self.device_count\n\n  def preprocess(self, image_buffer, bbox, thread_id):\n    """"""Preprocessing image_buffer using thread_id.""""""\n    # Note: Width and height of image is known only at runtime.\n    image = tf.image.decode_jpeg(image_buffer, channels=3,\n                                 dct_method=\'INTEGER_FAST\')\n    if self.train and self.distortions:\n      image = distort_image(image, self.height, self.width, bbox, thread_id)\n    else:\n      #image = eval_image(image, self.height, self.width, bbox, thread_id,\n      #                   self.resize_method)\n      image = densenet_preprocessing.preprocess_image(image,224,224,False)\n    # Note: image is now float32 [height,width,3] with range [0, 255]\n\n    # image = tf.cast(image, tf.uint8) # HACK TESTING\n\n    return image\n\n  def minibatch(self, dataset, subset):\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n      images = [[] for i in range(self.device_count)]\n      labels = [[] for i in range(self.device_count)]\n      record_input = data_flow_ops.RecordInput(\n          file_pattern=dataset.tf_record_pattern(subset),\n          seed=randint(0, 9000),\n          parallelism=64,\n          buffer_size=10000,\n          batch_size=self.batch_size,\n          name=\'record_input\')\n      records = record_input.get_yield_op()\n      records = tf.split(records, self.batch_size, 0)\n      records = [tf.reshape(record, []) for record in records]\n      for i in xrange(self.batch_size):\n        value = records[i]\n        image_buffer, label_index, bbox, _ = parse_example_proto(value)\n        image = self.preprocess(image_buffer, bbox, i % 4)\n\n        device_index = i % self.device_count\n        images[device_index].append(image)\n        labels[device_index].append(label_index)\n      label_index_batch = [None] * self.device_count\n      for device_index in xrange(self.device_count):\n        images[device_index] = tf.parallel_stack(images[device_index])\n        label_index_batch[device_index] = tf.concat(labels[device_index], 0)\n\n        # dynamic_pad=True) # HACK TESTING dynamic_pad=True\n        images[device_index] = tf.cast(images[device_index], self.dtype)\n        depth = 3\n        images[device_index] = tf.reshape(\n            images[device_index],\n            shape=[self.batch_size_per_device, self.height, self.width, depth])\n        label_index_batch[device_index] = tf.reshape(\n            label_index_batch[device_index], [self.batch_size_per_device])\n        # Display the training images in the visualizer.\n        # tf.summary.image(\'images\', images)\n\n      return images, label_index_batch\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/fp32/accuracy.py,14,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport accuracy_preprocessing as preprocessing\nimport accuracy_datasets as datasets\n\nNUM_TEST_IMAGES = 50000\n\n\ndef load_graph(model_file):\n    graph = tf.Graph()\n    graph_def = tf.compat.v1.GraphDef()\n\n    import os\n    file_ext = os.path.splitext(model_file)[1]\n\n    with open(model_file, ""rb"") as f:\n        if file_ext == \'.pbtxt\':\n            text_format.Merge(f.read(), graph_def)\n        else:\n            graph_def.ParseFromString(f.read())\n    with graph.as_default():\n        tf.import_graph_def(graph_def, name=\'\')\n\n    return graph\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--input_graph"", default=None,\n                        help=""graph/model to be executed"")\n    parser.add_argument(""--data_location"", default=None,\n                        help=""full path to the validation data"")\n    parser.add_argument(""--input_height"", default=None,\n                        type=int, help=""input height"")\n    parser.add_argument(""--input_width"", default=None,\n                        type=int, help=""input width"")\n    parser.add_argument(""--batch_size"", default=32,\n                        type=int, help=""batch size"")\n    parser.add_argument(""--input_layer"", default=""input"",\n                        help=""name of input layer"")\n    parser.add_argument(""--output_layer"",\n                        default=""MobilenetV1/Predictions/Reshape_1"",\n                        help=""name of output layer"")\n    parser.add_argument(\n        \'--num_inter_threads\',\n        help=\'number threads across operators\',\n        type=int, default=1)\n    parser.add_argument(\n        \'--num_intra_threads\',\n        help=\'number threads for an operator\',\n        type=int, default=1)\n    args = parser.parse_args()\n\n    if args.input_graph:\n        model_file = args.input_graph\n    else:\n        sys.exit(""Please provide a graph file."")\n    if args.input_height:\n        input_height = args.input_height\n    else:\n        input_height = 224\n    if args.input_width:\n        input_width = args.input_width\n    else:\n        input_width = 224\n    batch_size = args.batch_size\n    input_layer = args.input_layer\n    output_layer = args.output_layer\n    num_inter_threads = args.num_inter_threads\n    num_intra_threads = args.num_intra_threads\n    data_location = args.data_location\n    dataset = datasets.ImagenetData(data_location)\n    preprocessor = dataset.get_image_preprocessor()(\n        input_height, input_width, batch_size,\n        1,  # device count\n        tf.float32,  # data_type for input fed to the graph\n        train=False,  # doing inference\n        resize_method=\'bilinear\')\n\n    with tf.compat.v1.get_default_graph().as_default():\n        images, labels = preprocessor.minibatch(dataset, subset=\'validation\',\n                         use_datasets=True, cache_data=False)\n    graph = load_graph(model_file)\n    input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n    output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n\n    config = tf.compat.v1.ConfigProto()\n    config.inter_op_parallelism_threads = num_inter_threads\n    config.intra_op_parallelism_threads = num_intra_threads\n\n    total_accuracy1, total_accuracy5 = (0.0, 0.0)\n    num_processed_images = 0\n    num_remaining_images = dataset.num_examples_per_epoch(subset=\'validation\') \\\n                           - num_processed_images\n    with tf.compat.v1.Session() as sess:\n        sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n        while num_remaining_images >= batch_size:\n            # Reads and preprocess data\n            np_images, np_labels = sess.run([images[0], labels[0]])\n            num_processed_images += batch_size\n            num_remaining_images -= batch_size\n            start_time = time.time()\n            # Compute inference on the preprocessed data\n            predictions = sess_graph.run(output_tensor,\n                                         {input_tensor: np_images})\n            elapsed_time = time.time() - start_time\n            accuracy1 = tf.reduce_sum(\n                input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                       targets=tf.constant(np_labels), k=1), tf.float32))\n\n            accuracy5 = tf.reduce_sum(\n                input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n                                       targets=tf.constant(np_labels), k=5), tf.float32))\n            np_accuracy1, np_accuracy5 = sess.run([accuracy1, accuracy5])\n            total_accuracy1 += np_accuracy1\n            total_accuracy5 += np_accuracy5\n            print(""Iteration time: %0.4f ms"" % elapsed_time)\n            print(\n                ""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n                % (\n                num_processed_images, total_accuracy1 / num_processed_images,\n                total_accuracy5 / num_processed_images))\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/fp32/accuracy_datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport numpy as np\nfrom six.moves import cPickle\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nimport accuracy_preprocessing as preprocessing\n\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\n\n\ndef create_dataset(data_dir, data_name):\n  """"""Create a Dataset instance based on data_dir and data_name.""""""\n  supported_datasets = {\n      \'imagenet\': ImagenetData,\n      \'cifar10\': Cifar10Data,\n  }\n  if not data_dir and not data_name:\n    # When using synthetic data, use synthetic imagenet images by default.\n    data_name = \'imagenet\'\n\n  if data_name is None:\n    for supported_name in supported_datasets:\n      if supported_name in data_dir:\n        data_name = supported_name\n        break\n\n  if data_name is None:\n    raise ValueError(\'Could not identify name of dataset. \'\n                     \'Please specify with --data_name option.\')\n\n  if data_name not in supported_datasets:\n    raise ValueError(\'Unknown dataset. Must be one of %s\', \', \'.join(\n        [key for key in sorted(supported_datasets.keys())]))\n\n  return supported_datasets[data_name](data_dir)\n\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, height=None, width=None, depth=None, data_dir=None,\n               queue_runner_required=False, num_classes=1000):\n    self.name = name\n    self.height = height\n    self.width = width\n    self.depth = depth or 3\n\n    self.data_dir = data_dir\n    self._queue_runner_required = queue_runner_required\n    self._num_classes = num_classes\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.TFRecordReader()\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @num_classes.setter\n  def num_classes(self, val):\n    self._num_classes = val\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n  def get_image_preprocessor(self):\n    return None\n\n  def queue_runner_required(self):\n    return self._queue_runner_required\n\n  def use_synthetic_gpu_images(self):\n    return not self.data_dir\n\n\nclass ImagenetData(Dataset):\n  """"""Configuration for Imagenet dataset.""""""\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'imagenet\', 300, 300, data_dir=data_dir)\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    if self.use_synthetic_gpu_images():\n      return preprocessing.SyntheticImagePreprocessor\n    else:\n      return preprocessing.RecordInputImagePreprocessor\n\n\nclass Cifar10Data(Dataset):\n  """"""Configuration for cifar 10 dataset.\n\n  It will mount all the input images to memory.\n  """"""\n\n  def __init__(self, data_dir=None):\n    super(Cifar10Data, self).__init__(\'cifar10\', 32, 32, data_dir=data_dir,\n                                      queue_runner_required=True,\n                                      num_classes=10)\n\n  def read_data_files(self, subset=\'train\'):\n    """"""Reads from data file and returns images and labels in a numpy array.""""""\n    assert self.data_dir, (\'Cannot call `read_data_files` when using synthetic \'\n                           \'data\')\n    if subset == \'train\':\n      filenames = [os.path.join(self.data_dir, \'data_batch_%d\' % i)\n                   for i in xrange(1, 6)]\n    elif subset == \'validation\':\n      filenames = [os.path.join(self.data_dir, \'test_batch\')]\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n    inputs = []\n    for filename in filenames:\n      with gfile.Open(filename, \'r\') as f:\n        inputs.append(cPickle.load(f))\n    # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n    # input format.\n    all_images = np.concatenate(\n        [each_input[\'data\'] for each_input in inputs]).astype(np.float32)\n    all_labels = np.concatenate(\n        [each_input[\'labels\'] for each_input in inputs])\n    return all_images, all_labels\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return 50000\n    elif subset == \'validation\':\n      return 10000\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    if self.use_synthetic_gpu_images():\n      return preprocessing.SyntheticImagePreprocessor\n    else:\n      return preprocessing.Cifar10ImagePreprocessor\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/fp32/accuracy_preprocessing.py,116,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport math\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.python.data.experimental.ops import batching\nfrom tensorflow.python.data.experimental.ops import interleave_ops\nfrom tensorflow.python.layers import utils\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.platform import gfile\nimport cnn_util\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef parse_example_proto(example_serialized):\n    """"""Parses an Example proto containing a training example of an image.\n  \n    The output of the build_image_data.py image preprocessing script is a dataset\n    containing serialized Example protocol buffers. Each Example proto contains\n    the following fields:\n  \n      image/height: 462\n      image/width: 581\n      image/colorspace: \'RGB\'\n      image/channels: 3\n      image/class/label: 615\n      image/class/synset: \'n03623198\'\n      image/class/text: \'knee pad\'\n      image/object/bbox/xmin: 0.1\n      image/object/bbox/xmax: 0.9\n      image/object/bbox/ymin: 0.2\n      image/object/bbox/ymax: 0.6\n      image/object/bbox/label: 615\n      image/format: \'JPEG\'\n      image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n      image/encoded: <JPEG encoded string>\n  \n    Args:\n      example_serialized: scalar Tensor tf.string containing a serialized\n        Example protocol buffer.\n  \n    Returns:\n      image_buffer: Tensor tf.string containing the contents of a JPEG file.\n      label: Tensor tf.int32 containing the label.\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged as\n        [ymin, xmin, ymax, xmax].\n      text: Tensor tf.string containing the human-readable label.\n    """"""\n    # Dense features in Example proto.\n    feature_map = {\n        \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                            default_value=\'\'),\n        \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                                default_value=-1),\n        \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                               default_value=\'\'),\n    }\n    sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n    # Sparse features in Example proto.\n    feature_map.update(\n        {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                     \'image/object/bbox/ymin\',\n                                     \'image/object/bbox/xmax\',\n                                     \'image/object/bbox/ymax\']})\n\n    features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n    label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n    xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n    ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n    xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n    ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n    # Note that we impose an ordering of (y, x) just to make life difficult.\n    bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n    # Force the variable number of bounding boxes into the shape\n    # [1, num_boxes, coords].\n    bbox = tf.expand_dims(bbox, 0)\n    bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n    return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef get_image_resize_method(resize_method, batch_position=0):\n    """"""Get tensorflow resize method.\n  \n    If resize_method is \'round_robin\', return different methods based on batch\n    position in a round-robin fashion. NOTE: If the batch size is not a multiple\n    of the number of methods, then the distribution of methods will not be\n    uniform.\n  \n    Args:\n      resize_method: (string) nearest, bilinear, bicubic, area, or round_robin.\n      batch_position: position of the image in a batch. NOTE: this argument can\n        be an integer or a tensor\n    Returns:\n      one of resize type defined in tf.image.ResizeMethod.\n    """"""\n    resize_methods_map = {\n        \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n        \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n        \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n        \'area\': tf.image.ResizeMethod.AREA\n    }\n\n    if resize_method != \'round_robin\':\n        return resize_methods_map[resize_method]\n\n    # return a resize method based on batch position in a round-robin fashion.\n    resize_methods = resize_methods_map.values()\n\n    def lookup(index):\n        return resize_methods[index]\n\n    def resize_method_0():\n        return utils.smart_cond(batch_position % len(resize_methods) == 0,\n                                lambda: lookup(0), resize_method_1)\n\n    def resize_method_1():\n        return utils.smart_cond(batch_position % len(resize_methods) == 1,\n                                lambda: lookup(1), resize_method_2)\n\n    def resize_method_2():\n        return utils.smart_cond(batch_position % len(resize_methods) == 2,\n                                lambda: lookup(2), lambda: lookup(3))\n\n    # NOTE(jsimsa): Unfortunately, we cannot use a single recursive function here\n    # because TF would not be able to construct a finite graph.\n\n    return resize_method_0()\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n    """"""Decode a JPEG string into one 3-D float image Tensor.\n  \n    Args:\n      image_buffer: scalar string Tensor.\n      scope: Optional scope for op_scope.\n    Returns:\n      3-D float Tensor with values ranging from [0, 1).\n    """"""\n    # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n    # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n    with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n        # Decode the string as an RGB JPEG.\n        # Note that the resulting image contains an unknown height and width\n        # that is set dynamically by decode_jpeg. In other words, the height\n        # and width of image is unknown at compile-time.\n        image = tf.image.decode_jpeg(image_buffer, channels=3)  # ,\n        #     fancy_upscaling=False,\n        #     dct_method=\'INTEGER_FAST\')\n\n        # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n        return image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n    """"""Prepare one image for evaluation.\n  \n    If height and width are specified it would output an image with that size by\n    applying resize_bilinear.\n  \n    If central_fraction is specified it would crop the central fraction of the\n    input image.\n  \n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      central_fraction: Optional Float, fraction of the image to crop.\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D float Tensor of prepared image.\n    """"""\n    with tf.compat.v1.name_scope(scope, \'eval_image\', [image, height, width]):\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Crop the central region of the image with an area containing 87.5% of\n        # the original image.\n        if central_fraction:\n            image = tf.image.central_crop(image,\n                                          central_fraction=central_fraction)\n\n        if height and width:\n            # Resize the image to the specified height and width.\n            image = tf.expand_dims(image, 0)\n            image = tf.compat.v1.image.resize_bilinear(image, [height, width],\n                                                       align_corners=False)\n            image = tf.squeeze(image, [0])\n        image = tf.subtract(image, 0.5)\n        image = tf.multiply(image, 2.0)\n        return image\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n  \n    Args:\n      x: input Tensor.\n      func: Python function to apply.\n      num_cases: Python int32, number of cases to sample sel from.\n  \n    Returns:\n      The result of func(x, sel), where func receives the value of the\n      selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random.uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n        func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n        for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n  \n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n  \n    Args:\n      image: 3-D Tensor containing single image in [0, 1].\n      color_ordering: Python int, a type of distortion (valid values: 0-3).\n      fast_mode: Avoids slower ops (random_hue and random_contrast)\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n      ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.compat.v1.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n  \n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n  \n    Args:\n      image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n        image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding box\n        supplied.\n      aspect_ratio_range: An optional list of `floats`. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `floats`. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n      scope: Optional scope for name_scope.\n    Returns:\n      A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.compat.v1.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n        # A large fraction of image datasets contain a human-annotated bounding\n        # box delineating the region of the image containing the object of interest.\n        # We choose to create a new bounding box for the object which is a randomly\n        # distorted version of the human-annotated bounding box that obeys an\n        # allowed range of aspect ratios, sizes and overlap with the human-annotated\n        # bounding box. If no box is supplied, then we assume the bounding box is\n        # the entire image.\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            image_size=tf.shape(input=image),\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         batch_position,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n    """"""Distort one image for training a network.\n  \n    Distorting images provides a useful technique for augmenting the data\n    set during training in order to make the network invariant to aspects\n    of the image that do not effect the label.\n  \n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax].\n      batch_position: position of the image in a batch, which affects how images\n        are distorted and resized. NOTE: this argument can be an integer or a\n        tensor\n      scope: Optional scope for op_scope.\n      add_image_summaries: Enable image summaries.\n    Returns:\n      3-D float Tensor of distorted image used for training with range [-1, 1].\n    """"""\n\n    with tf.compat.v1.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n        if bbox is None:\n            bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                               dtype=tf.float32,\n                               shape=[1, 1, 4])\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                      bbox)\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n        distorted_image, distorted_bbox = distorted_bounding_box_crop(image,\n                                                                      bbox)\n        # Restore the shape since the dynamic slice based upon the bbox_size loses\n        # the third dimension.\n        distorted_image.set_shape([None, None, 3])\n        image_with_distorted_box = tf.image.draw_bounding_boxes(\n            tf.expand_dims(image, 0), distorted_bbox)\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'images_with_distorted_bounding_box\',\n                             image_with_distorted_box)\n\n        # This resizing operation may distort the images because the aspect\n        # ratio is not respected. We select a resize method in a round robin\n        # fashion based on the thread number.\n        # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n        # We select only 1 case for fast_mode bilinear.\n        num_resize_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n            distorted_image,\n            lambda x, method: tf.image.resize(x, [height, width],\n                                                     method),\n            num_cases=num_resize_cases)\n\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'cropped_resized_image\',\n                             tf.expand_dims(distorted_image, 0))\n\n        # Randomly flip the image horizontally.\n        distorted_image = tf.image.random_flip_left_right(distorted_image)\n        # Randomly distort the colors. There are 1 or 4 ways to do it.\n        num_distort_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n            distorted_image,\n            lambda x, ordering: distort_color(x, ordering, fast_mode),\n            num_cases=num_distort_cases)\n\n        if add_image_summaries:\n            tf.compat.v1.summary.image(\'final_distorted_image\',\n                             tf.expand_dims(distorted_image, 0))\n        distorted_image = tf.subtract(distorted_image, 0.5)\n        distorted_image = tf.multiply(distorted_image, 2.0)\n        return distorted_image\n\n\ndef distort_color(image, batch_position=0, distort_color_in_yiq=False,\n                  scope=None):\n    """"""Distort the color of the image.\n  \n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops based on the position of the image in a batch.\n  \n    Args:\n      image: float32 Tensor containing single image. Tensor values should be in\n        range [0, 1].\n      batch_position: the position of the image in a batch. NOTE: this argument\n        can be an integer or a tensor\n      distort_color_in_yiq: distort color of input images in YIQ space.\n      scope: Optional scope for op_scope.\n    Returns:\n      color-distorted image\n    """"""\n    with tf.compat.v1.name_scope(scope or \'distort_color\'):\n        def distort_fn_0(image=image):\n            """"""Variant 0 of distort function.""""""\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            # if distort_color_in_yiq:\n            #  image = distort_image_ops.random_hsv_in_yiq(\n            #      image, lower_saturation=0.5, upper_saturation=1.5,\n            #      max_delta_hue=0.2 * math.pi)\n            # else:\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            return image\n\n        def distort_fn_1(image=image):\n            """"""Variant 1 of distort function.""""""\n            image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            # if distort_color_in_yiq:\n            #  image = distort_image_ops.random_hsv_in_yiq(\n            #      image, lower_saturation=0.5, upper_saturation=1.5,\n            #      max_delta_hue=0.2 * math.pi)\n            # else:\n            image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            image = tf.image.random_hue(image, max_delta=0.2)\n            return image\n\n        image = utils.smart_cond(batch_position % 2 == 0, distort_fn_0,\n                                 distort_fn_1)\n        # The random_* ops do not necessarily clamp.\n        image = tf.clip_by_value(image, 0.0, 1.0)\n        return image\n\n\nclass RecordInputImagePreprocessor(object):\n    """"""Preprocessor for images with RecordInput format.""""""\n\n    def __init__(self,\n                 height,\n                 width,\n                 batch_size,\n                 num_splits,\n                 dtype,\n                 train,\n                 distortions=False,\n                 resize_method=""bilinear"",\n                 shift_ratio=0,\n                 summary_verbosity=1,\n                 distort_color_in_yiq=False,\n                 fuse_decode_and_crop=False):\n        self.height = height\n        self.width = width\n        self.batch_size = batch_size\n        self.num_splits = num_splits\n        self.dtype = dtype\n        self.train = train\n        self.resize_method = resize_method\n        self.shift_ratio = shift_ratio\n        self.distortions = distortions\n        self.distort_color_in_yiq = distort_color_in_yiq\n        self.fuse_decode_and_crop = fuse_decode_and_crop\n        if self.batch_size % self.num_splits != 0:\n            raise ValueError(\n                (\'batch_size must be a multiple of num_splits: \'\n                 \'batch_size %d, num_splits: %d\') %\n                (self.batch_size, self.num_splits))\n        self.batch_size_per_split = self.batch_size // self.num_splits\n        self.summary_verbosity = summary_verbosity\n\n    def center_crop(self, img, init_h, init_w):\n        height, width, _ = img.shape\n\n        left = int((width - init_w) // 2)\n        right = int((width + init_w) // 2)\n        top = int((height - init_h) // 2)\n        bottom = int((height + init_h) // 2)\n\n        img = img[top: bottom, left: right]\n\n        return img\n\n    def image_preprocess(self, image_buffer, bbox, batch_position):\n        """"""Preprocessing image_buffer as a function of its batch position.""""""\n        if self.train:\n            image_buffer = tf.image.decode_jpeg(\n                image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n            image = preprocess_for_train(image_buffer, self.height, self.width,\n                                         bbox,\n                                         batch_position)\n        else:\n            image = tf.image.decode_jpeg(\n                image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n\n            new_height = int(100. * self.height / 87.5)\n            new_width = int(100. * self.width / 87.5)\n\n            if(self.height > self.width):\n                w = new_width\n                h = int(new_height * self.height / self.width)\n            else:\n                h = new_height\n                w = int(new_width * self.width / self.height)\n\n            image = preprocess_for_eval(image, h, w)\n            image = self.center_crop(image, self.height, self.width)\n\n        return image\n\n    def parse_and_preprocess(self, value, batch_position):\n        image_buffer, label_index, bbox, _ = parse_example_proto(value)\n        image = self.image_preprocess(image_buffer, bbox, batch_position)\n        return (label_index, image)\n\n    def minibatch(self, dataset, subset, use_datasets, cache_data,\n                  shift_ratio=-1):\n        if shift_ratio < 0:\n            shift_ratio = self.shift_ratio\n        with tf.compat.v1.name_scope(\'batch_processing\'):\n            # Build final results per split.\n            images = [[] for _ in range(self.num_splits)]\n            labels = [[] for _ in range(self.num_splits)]\n            if use_datasets:\n                glob_pattern = dataset.tf_record_pattern(subset)\n                file_names = gfile.Glob(glob_pattern)\n                if not file_names:\n                    raise ValueError(\n                        \'Found no files in --data_dir matching: {}\'\n                        .format(glob_pattern))\n                ds = tf.data.TFRecordDataset.list_files(file_names, shuffle=False)\n                ds = ds.apply(\n                    interleave_ops.parallel_interleave(\n                        tf.data.TFRecordDataset, cycle_length=10))\n                if cache_data:\n                    ds = ds.take(1).cache().repeat()\n                counter = tf.data.Dataset.range(self.batch_size)\n                counter = counter.repeat()\n                ds = tf.data.Dataset.zip((ds, counter))\n                ds = ds.prefetch(buffer_size=self.batch_size)\n                ds = ds.repeat()\n                ds = ds.apply(\n                    batching.map_and_batch(\n                        map_func=self.parse_and_preprocess,\n                        batch_size=self.batch_size_per_split,\n                        num_parallel_batches=self.num_splits))\n                ds = ds.prefetch(buffer_size=self.num_splits)\n                ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n                for d in xrange(self.num_splits):\n                    labels[d], images[d] = ds_iterator.get_next()\n\n            else:\n                record_input = data_flow_ops.RecordInput(\n                    file_pattern=dataset.tf_record_pattern(subset),\n                    seed=301,\n                    parallelism=64,\n                    buffer_size=10000,\n                    batch_size=self.batch_size,\n                    shift_ratio=shift_ratio,\n                    name=\'record_input\')\n                records = record_input.get_yield_op()\n                records = tf.split(records, self.batch_size, 0)\n                records = [tf.reshape(record, []) for record in records]\n                for idx in xrange(self.batch_size):\n                    value = records[idx]\n                    (label, image) = self.parse_and_preprocess(value, idx)\n                    split_index = idx % self.num_splits\n                    labels[split_index].append(label)\n                    images[split_index].append(image)\n\n            for split_index in xrange(self.num_splits):\n                if not use_datasets:\n                    images[split_index] = tf.parallel_stack(\n                        images[split_index])\n                    labels[split_index] = tf.concat(labels[split_index], 0)\n                images[split_index] = tf.cast(images[split_index], self.dtype)\n                depth = 3\n                images[split_index] = tf.reshape(\n                    images[split_index],\n                    shape=[self.batch_size_per_split, self.height, self.width,\n                           depth])\n                labels[split_index] = tf.reshape(labels[split_index],\n                                                 [self.batch_size_per_split])\n            return images, labels\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/fp32/benchmark.py,7,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""MobilenetV1/Predictions/Reshape_1"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  parser.add_argument(""--warmup_steps"", type=int, default=10,\n                      help=""number of warmup steps"")\n  parser.add_argument(""--steps"", type=int, default=50, help=""number of steps"")\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  input_height = args.input_height\n  input_width = args.input_width\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  warmup_steps = args.warmup_steps\n  steps = args.steps\n  assert steps > 10, ""Benchmark steps should be at least 10.""\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n\n  graph = load_graph(model_file)\n\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"");\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"");\n\n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  with tf.compat.v1.Session(graph=graph, config=config) as sess:\n    input_shape = [batch_size, input_height, input_width, 3]\n    images = tf.random.truncated_normal(\n          input_shape,\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\')\n    image_data = sess.run(images)\n\n    sys.stdout.flush()\n    print(""[Running warmup steps...]"")\n    for t in range(warmup_steps):\n      start_time = time.time()\n      sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t+1, batch_size/elapsed_time))\n\n    print(""[Running benchmark steps...]"")\n    total_time   = 0;\n    total_images = 0;\n    for t in range(steps):\n      start_time = time.time()\n      results = sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t+1, batch_size/elapsed_time));\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/fp32/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/fp32/eval_image_classifier.py,36,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\nimport os\nimport time\nfrom datetime import datetime\n\nfrom datasets import dataset_factory\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 100, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'max_num_batches\', 1,\n    \'Max number of batches to evaluate by default use all.\')\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'/tmp/tfmodel/\',\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\n\ntf.app.flags.DEFINE_string(\n    \'eval_dir\', \'/tmp/tfmodel/\', \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'test\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to evaluate.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_image_size\', None, \'Eval image size\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_log_frequency\', 10, \'Number of eval steps to run between displaying \'\n     \'eval metrics.\')\n\ntf.app.flags.DEFINE_integer(\n    \'inter_op_parallelism_threads\', 1, \'The number of inter-thread.\')\n\ntf.app.flags.DEFINE_integer(\n    \'intra_op_parallelism_threads\', 28, \'The number of intra-thread.\')\n\n\nFLAGS = tf.app.flags.FLAGS\n\nclass _LoggerHook(tf.train.SessionRunHook):\n  """""" Logs loss and runtime.""""""\n\n  def begin(self):\n    self._step = -1\n    self._displayed_steps = 0\n    self._total_images_per_sec = 0\n\n  def before_run(self, run_context):\n    self._step += 1\n    self._start_time = time.time()\n\n  def after_run(self, run_context, run_values):\n    duration = time.time() - self._start_time\n    if (self._step + 1) % FLAGS.eval_log_frequency == 0:\n      images_per_sec = FLAGS.batch_size / duration\n      self._displayed_steps += 1\n      self._total_images_per_sec += images_per_sec\n\n      format_str = (\'%s: step %d, %.1f images/sec\')\n      print (format_str % (datetime.now(), (self._step+1), images_per_sec))\n\n  def end(self, run_context):\n    print(\'self._total_images_per_sec = %.1f\' % self._total_images_per_sec)\n    print(\'self._displayed_steps = %d\' % self._displayed_steps)\n    images_per_sec = self._total_images_per_sec / self._displayed_steps\n    print(\'Total images/sec = %.1f\' %(images_per_sec))\n    if FLAGS.batch_size == 1:\n      latency = 1000 / images_per_sec\n      print(\'Latency ms/step = %.1f\' % (latency))\n\ndef main(_):\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  with tf.Graph().as_default():\n    tf_global_step = slim.get_or_create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    if FLAGS.dataset_dir:\n        print(""Inference using real data"")\n        dataset = dataset_factory.get_dataset(\n            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n        num_classes = dataset.num_classes - FLAGS.labels_offset\n    else:\n        print(""Inference using synthetic data"")\n        num_classes = 1000\n\n    ####################\n    # Select the model #\n    ####################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=num_classes,\n        is_training=False)\n\n    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\n\n    if FLAGS.dataset_dir:\n        ##############################################################\n        # Create a dataset provider that loads data from the dataset #\n        ##############################################################\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n            dataset,\n            shuffle=False,\n            common_queue_capacity=2 * FLAGS.batch_size,\n            common_queue_min=FLAGS.batch_size)\n        [image, label] = provider.get([\'image\', \'label\'])\n        label -= FLAGS.labels_offset\n\n        #####################################\n        # Select the preprocessing function #\n        #####################################\n        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            preprocessing_name,\n            is_training=False)\n\n        image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\n\n        images, labels = tf.train.batch(\n            [image, label],\n            batch_size=FLAGS.batch_size,\n            num_threads=FLAGS.num_preprocessing_threads,\n            capacity=5 * FLAGS.batch_size)\n    else:\n        # Generate random images and labels with constant 0 when no dataset is used\n        input_shape = [FLAGS.batch_size, eval_image_size, eval_image_size, 3]\n        label_shape = [FLAGS.batch_size]\n        images = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name=\'synthetic_images\')\n        labels = tf.constant(0, shape=label_shape, dtype=tf.int64)\n\n    ####################\n    # Define the model #\n    ####################\n    logits, _ = network_fn(images)\n\n    if FLAGS.moving_average_decay:\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, tf_global_step)\n      variables_to_restore = variable_averages.variables_to_restore(\n          slim.get_model_variables())\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\n    else:\n      variables_to_restore = slim.get_variables_to_restore()\n\n    predictions = tf.argmax(logits, 1)\n    #labels = tf.squeeze(labels)\n\n    # Define the metrics:\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n        \'Accuracy\': slim.metrics.streaming_accuracy(predictions, labels),\n        \'Recall_5\': slim.metrics.streaming_recall_at_k(\n            logits, labels, 5),\n    })\n\n    # Print the summaries to screen.\n    for name, value in names_to_values.items():\n      summary_name = \'eval/%s\' % name\n      op = tf.summary.scalar(summary_name, value, collections=[])\n      op = tf.Print(op, [value], summary_name)\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n    # TODO(sguada) use num_epochs=1\n    if FLAGS.max_num_batches:\n      num_batches = FLAGS.max_num_batches\n    else:\n      # This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\n\n    num_batches = 100\n\n    config = tf.ConfigProto(inter_op_parallelism_threads=FLAGS.inter_op_parallelism_threads, intra_op_parallelism_threads=FLAGS.intra_op_parallelism_threads)\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n      checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n    slim.evaluation.evaluate_once(\n        master=FLAGS.master,\n        checkpoint_path=checkpoint_path,\n        logdir=FLAGS.eval_dir,\n        num_evals=num_batches,\n        eval_op=list(names_to_updates.values()),\n        variables_to_restore=variables_to_restore,\n        hooks=[_LoggerHook()],\n        session_config=config)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/int8/accuracy.py,14,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""MobilenetV1/Predictions/Reshape_1"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  input_height = args.input_height\n  input_width = args.input_width\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n  dataset = datasets.ImagenetData(data_location)\n  preprocessor = dataset.get_image_preprocessor()(\n      input_height, input_width, batch_size,\n      1, # device count\n      tf.float32, # data_type for input fed to the graph\n      train=False, # doing inference\n      resize_method=\'bilinear\')\n  with tf.compat.v1.get_default_graph().as_default():\n    images, labels = preprocessor.minibatch(dataset, subset=\'validation\',\n                     use_datasets=True, cache_data=False)\n  graph = load_graph(model_file)\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n\n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = dataset.num_examples_per_epoch(subset=\'validation\') \\\n                            - num_processed_images\n  with tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph()) as sess:\n    sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n    while num_remaining_images >= batch_size:\n      # Reads and preprocess data\n      np_images, np_labels = sess.run([images[0], labels[0]])\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      start_time = time.time()\n      # Compute inference on the preprocessed data\n      predictions = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      elapsed_time = time.time() - start_time\n      accuracy1 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=1), tf.float32))\n\n      accuracy5 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=5), tf.float32))\n      np_accuracy1, np_accuracy5 =  sess.run([accuracy1, accuracy5])\n      total_accuracy1 += np_accuracy1\n      total_accuracy5 += np_accuracy5\n      print(""Iteration time: %0.4f ms"" % elapsed_time)\n      print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n          % (num_processed_images, total_accuracy1/num_processed_images,\n          total_accuracy5/num_processed_images))\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/int8/benchmark.py,7,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""MobilenetV1/Predictions/Reshape_1"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  parser.add_argument(""--warmup_steps"", type=int, default=10,\n                      help=""number of warmup steps"")\n  parser.add_argument(""--steps"", type=int, default=50, help=""number of steps"")\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  input_height = args.input_height\n  input_width = args.input_width\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  warmup_steps = args.warmup_steps\n  steps = args.steps\n  assert steps > 10, ""Benchmark steps should be at least 10.""\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n\n  graph = load_graph(model_file)\n\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"");\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"");\n\n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  with tf.compat.v1.Session(graph=graph, config=config) as sess:\n    input_shape = [batch_size, input_height, input_width, 3]\n    images = tf.random.truncated_normal(\n          input_shape,\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\')\n    image_data = sess.run(images)\n\n    sys.stdout.flush()\n    print(""[Running warmup steps...]"")\n    for t in range(warmup_steps):\n      start_time = time.time()\n      sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t+1, batch_size/elapsed_time))\n\n    print(""[Running benchmark steps...]"")\n    total_time   = 0;\n    total_images = 0;\n    for t in range(steps):\n      start_time = time.time()\n      results = sess.run(output_tensor, {input_tensor: image_data})\n      elapsed_time = time.time() - start_time\n      if((t+1) % 10 == 0):\n        print(""steps = {0}, {1} images/sec""\n              """".format(t+1, batch_size/elapsed_time));\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/int8/calibration.py,14,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport os\nimport time\nimport numpy as np\n\nfrom google.protobuf import text_format\nimport tensorflow as tf\nimport preprocessing\nimport datasets\n\nNUM_TEST_IMAGES = 50000\n\ndef load_graph(model_file):\n  graph = tf.Graph()\n  graph_def = tf.compat.v1.GraphDef()\n\n  import os\n  file_ext = os.path.splitext(model_file)[1]\n\n  with open(model_file, ""rb"") as f:\n    if file_ext == \'.pbtxt\':\n      text_format.Merge(f.read(), graph_def)\n    else:\n      graph_def.ParseFromString(f.read())\n  with graph.as_default():\n    tf.import_graph_def(graph_def, name=\'\')\n\n  return graph\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--input_graph"", default=None,\n                      help=""graph/model to be executed"")\n  parser.add_argument(""--data_location"", default=None,\n                      help=""full path to the validation data"")\n  parser.add_argument(""--input_height"", default=224,\n                      type=int, help=""input height"")\n  parser.add_argument(""--input_width"", default=224,\n                      type=int, help=""input width"")\n  parser.add_argument(""--batch_size"", default=32,\n                      type=int, help=""batch size"")\n  parser.add_argument(""--input_layer"", default=""input"",\n                      help=""name of input layer"")\n  parser.add_argument(""--output_layer"", default=""MobilenetV1/Predictions/Reshape_1"",\n                      help=""name of output layer"")\n  parser.add_argument(\n      \'--num_inter_threads\',\n      help=\'number threads across operators\',\n      type=int, default=1)\n  parser.add_argument(\n      \'--num_intra_threads\',\n      help=\'number threads for an operator\',\n      type=int, default=1)\n  args = parser.parse_args()\n\n  if args.input_graph:\n    model_file = args.input_graph\n  else:\n    sys.exit(""Please provide a graph file."")\n  input_height = args.input_height\n  input_width = args.input_width\n  batch_size = args.batch_size\n  input_layer = args.input_layer\n  output_layer = args.output_layer\n  num_inter_threads = args.num_inter_threads\n  num_intra_threads = args.num_intra_threads\n  data_location = args.data_location\n  dataset = datasets.ImagenetData(data_location)\n  preprocessor = dataset.get_image_preprocessor()(\n      input_height, input_width, batch_size,\n      1, # device count\n      tf.float32, # data_type for input fed to the graph\n      train=False, # doing inference\n      resize_method=\'bilinear\')\n  with tf.compat.v1.get_default_graph().as_default():\n    images, labels = preprocessor.minibatch(dataset, subset=\'calibration\',\n                     use_datasets=True, cache_data=False)\n  graph = load_graph(model_file)\n  input_tensor = graph.get_tensor_by_name(input_layer + "":0"")\n  output_tensor = graph.get_tensor_by_name(output_layer + "":0"")\n\n  config = tf.compat.v1.ConfigProto()\n  config.inter_op_parallelism_threads = num_inter_threads\n  config.intra_op_parallelism_threads = num_intra_threads\n\n  total_accuracy1, total_accuracy5 = (0.0, 0.0)\n  num_processed_images = 0\n  num_remaining_images = dataset.num_examples_per_epoch(subset=\'calibration\') \\\n                            - num_processed_images\n  with tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph()) as sess:\n    sess_graph = tf.compat.v1.Session(graph=graph, config=config)\n    while num_remaining_images >= batch_size:\n      # Reads and preprocess data\n      np_images, np_labels = sess.run([images[0], labels[0]])\n      num_processed_images += batch_size\n      num_remaining_images -= batch_size\n      start_time = time.time()\n      # Compute inference on the preprocessed data\n      predictions = sess_graph.run(output_tensor,\n                             {input_tensor: np_images})\n      elapsed_time = time.time() - start_time\n      accuracy1 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=1), tf.float32))\n\n      accuracy5 = tf.reduce_sum(\n          input_tensor=tf.cast(tf.nn.in_top_k(predictions=tf.constant(predictions),\n              targets=tf.constant(np_labels), k=5), tf.float32))\n      np_accuracy1, np_accuracy5 =  sess.run([accuracy1, accuracy5])\n      total_accuracy1 += np_accuracy1\n      total_accuracy5 += np_accuracy5\n      print(""Iteration time: %0.4f ms"" % elapsed_time)\n      print(""Processed %d images. (Top1 accuracy, Top5 accuracy) = (%0.4f, %0.4f)"" \\\n          % (num_processed_images, total_accuracy1/num_processed_images,\n          total_accuracy5/num_processed_images))\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/int8/cnn_util.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for CNN benchmarks.""""""\n\nimport tensorflow as tf\n\n\ndef tensorflow_version_tuple():\n  v = tf.__version__\n  major, minor, patch = v.split(\'.\')\n  return (int(major), int(minor), patch)\n\n\ndef tensorflow_version():\n  vt = tensorflow_version_tuple()\n  return vt[0] * 1000 + vt[1]\n\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/int8/datasets.py,1,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Benchmark dataset utilities.\n""""""\n\nfrom abc import abstractmethod\nimport os\n\nimport numpy as np\nfrom six.moves import cPickle\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nimport preprocessing\n\n\nIMAGENET_NUM_TRAIN_IMAGES = 1281167\nIMAGENET_NUM_VAL_IMAGES = 50000\nIMAGENET_NUM_CALIB_IMAGES = 500\n\n\ndef create_dataset(data_dir, data_name):\n  """"""Create a Dataset instance based on data_dir and data_name.""""""\n  supported_datasets = {\n      \'imagenet\': ImagenetData,\n      \'cifar10\': Cifar10Data,\n  }\n  if not data_dir and not data_name:\n    # When using synthetic data, use synthetic imagenet images by default.\n    data_name = \'imagenet\'\n\n  if data_name is None:\n    for supported_name in supported_datasets:\n      if supported_name in data_dir:\n        data_name = supported_name\n        break\n\n  if data_name is None:\n    raise ValueError(\'Could not identify name of dataset. \'\n                     \'Please specify with --data_name option.\')\n\n  if data_name not in supported_datasets:\n    raise ValueError(\'Unknown dataset. Must be one of %s\', \', \'.join(\n        [key for key in sorted(supported_datasets.keys())]))\n\n  return supported_datasets[data_name](data_dir)\n\n\nclass Dataset(object):\n  """"""Abstract class for cnn benchmarks dataset.""""""\n\n  def __init__(self, name, height=None, width=None, depth=None, data_dir=None,\n               queue_runner_required=False, num_classes=1000):\n    self.name = name\n    self.height = height\n    self.width = width\n    self.depth = depth or 3\n\n    self.data_dir = data_dir\n    self._queue_runner_required = queue_runner_required\n    self._num_classes = num_classes\n\n  def tf_record_pattern(self, subset):\n    return os.path.join(self.data_dir, \'%s-*-of-*\' % subset)\n\n  def reader(self):\n    return tf.compat.v1.TFRecordReader()\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @num_classes.setter\n  def num_classes(self, val):\n    self._num_classes = val\n\n  @abstractmethod\n  def num_examples_per_epoch(self, subset):\n    pass\n\n  def __str__(self):\n    return self.name\n\n  def get_image_preprocessor(self):\n    return None\n\n  def queue_runner_required(self):\n    return self._queue_runner_required\n\n  def use_synthetic_gpu_images(self):\n    return not self.data_dir\n\n\nclass ImagenetData(Dataset):\n  """"""Configuration for Imagenet dataset.""""""\n\n  def __init__(self, data_dir=None):\n    super(ImagenetData, self).__init__(\'imagenet\', 300, 300, data_dir=data_dir)\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return IMAGENET_NUM_TRAIN_IMAGES\n    elif subset == \'validation\':\n      return IMAGENET_NUM_VAL_IMAGES\n    elif subset == \'calibration\':\n      return IMAGENET_NUM_CALIB_IMAGES\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    if self.use_synthetic_gpu_images():\n      return preprocessing.SyntheticImagePreprocessor\n    else:\n      return preprocessing.RecordInputImagePreprocessor\n\n\nclass Cifar10Data(Dataset):\n  """"""Configuration for cifar 10 dataset.\n\n  It will mount all the input images to memory.\n  """"""\n\n  def __init__(self, data_dir=None):\n    super(Cifar10Data, self).__init__(\'cifar10\', 32, 32, data_dir=data_dir,\n                                      queue_runner_required=True,\n                                      num_classes=10)\n\n  def read_data_files(self, subset=\'train\'):\n    """"""Reads from data file and returns images and labels in a numpy array.""""""\n    assert self.data_dir, (\'Cannot call `read_data_files` when using synthetic \'\n                           \'data\')\n    if subset == \'train\':\n      filenames = [os.path.join(self.data_dir, \'data_batch_%d\' % i)\n                   for i in xrange(1, 6)]\n    elif subset == \'validation\':\n      filenames = [os.path.join(self.data_dir, \'test_batch\')]\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n    inputs = []\n    for filename in filenames:\n      with gfile.Open(filename, \'r\') as f:\n        inputs.append(cPickle.load(f))\n    # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n    # input format.\n    all_images = np.concatenate(\n        [each_input[\'data\'] for each_input in inputs]).astype(np.float32)\n    all_labels = np.concatenate(\n        [each_input[\'labels\'] for each_input in inputs])\n    return all_images, all_labels\n\n  def num_examples_per_epoch(self, subset=\'train\'):\n    if subset == \'train\':\n      return 50000\n    elif subset == \'validation\':\n      return 10000\n    else:\n      raise ValueError(\'Invalid data subset ""%s""\' % subset)\n\n  def get_image_preprocessor(self):\n    if self.use_synthetic_gpu_images():\n      return preprocessing.SyntheticImagePreprocessor\n    else:\n      return preprocessing.Cifar10ImagePreprocessor\n'"
models/image_recognition/tensorflow/mobilenet_v1/inference/int8/preprocessing.py,116,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image pre-processing utilities.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport math\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom tensorflow.python.data.experimental.ops import batching\nfrom tensorflow.python.data.experimental.ops import interleave_ops\nfrom tensorflow.python.layers import utils\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.platform import gfile\nimport cnn_util\n\nfrom tensorflow.python.ops import control_flow_ops\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields:\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    text: Tensor tf.string containing the human-readable label.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef get_image_resize_method(resize_method, batch_position=0):\n  """"""Get tensorflow resize method.\n\n  If resize_method is \'round_robin\', return different methods based on batch\n  position in a round-robin fashion. NOTE: If the batch size is not a multiple\n  of the number of methods, then the distribution of methods will not be\n  uniform.\n\n  Args:\n    resize_method: (string) nearest, bilinear, bicubic, area, or round_robin.\n    batch_position: position of the image in a batch. NOTE: this argument can\n      be an integer or a tensor\n  Returns:\n    one of resize type defined in tf.image.ResizeMethod.\n  """"""\n  resize_methods_map = {\n      \'nearest\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n      \'bilinear\': tf.image.ResizeMethod.BILINEAR,\n      \'bicubic\': tf.image.ResizeMethod.BICUBIC,\n      \'area\': tf.image.ResizeMethod.AREA\n  }\n\n  if resize_method != \'round_robin\':\n    return resize_methods_map[resize_method]\n\n  # return a resize method based on batch position in a round-robin fashion.\n  resize_methods = resize_methods_map.values()\n  def lookup(index):\n    return resize_methods[index]\n\n  def resize_method_0():\n    return utils.smart_cond(batch_position % len(resize_methods) == 0,\n                            lambda: lookup(0), resize_method_1)\n\n  def resize_method_1():\n    return utils.smart_cond(batch_position % len(resize_methods) == 1,\n                            lambda: lookup(1), resize_method_2)\n\n  def resize_method_2():\n    return utils.smart_cond(batch_position % len(resize_methods) == 2,\n                            lambda: lookup(2), lambda: lookup(3))\n\n  # NOTE(jsimsa): Unfortunately, we cannot use a single recursive function here\n  # because TF would not be able to construct a finite graph.\n\n  return resize_method_0()\n\n\ndef decode_jpeg(image_buffer, scope=None):  # , dtype=tf.float32):\n  """"""Decode a JPEG string into one 3-D float image Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  # with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n  # with tf.name_scope(scope, \'decode_jpeg\', [image_buffer]):\n  with tf.compat.v1.name_scope(scope or \'decode_jpeg\'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3) #,\n                            #     fancy_upscaling=False,\n                            #     dct_method=\'INTEGER_FAST\')\n\n    # image = tf.Print(image, [tf.shape(image)], \'Image shape: \')\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    return image\n\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.compat.v1.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.compat.v1.image.resize_bilinear(image, [height, width], align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random.uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.compat.v1.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.compat.v1.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        image_size=tf.shape(input=image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\n\ndef preprocess_for_train(image, height,width, bbox,\n                batch_position,\n                fast_mode=True,\n                scope=None,\n                add_image_summaries=True):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    batch_position: position of the image in a batch, which affects how images\n      are distorted and resized. NOTE: this argument can be an integer or a\n      tensor\n    scope: Optional scope for op_scope.\n    add_image_summaries: Enable image summaries.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n\n  with tf.compat.v1.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    if add_image_summaries:\n      tf.compat.v1.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    if add_image_summaries:\n      tf.compat.v1.summary.image(\'images_with_distorted_bounding_box\',\n                       image_with_distorted_box)\n\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize(x, [height, width], method),\n        num_cases=num_resize_cases)\n\n    if add_image_summaries:\n      tf.compat.v1.summary.image(\'cropped_resized_image\',\n                       tf.expand_dims(distorted_image, 0))\n\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n    # Randomly distort the colors. There are 1 or 4 ways to do it.\n    num_distort_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=num_distort_cases)\n\n    if add_image_summaries:\n      tf.compat.v1.summary.image(\'final_distorted_image\',\n                       tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef distort_color(image, batch_position=0, distort_color_in_yiq=False,\n                  scope=None):\n  """"""Distort the color of the image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops based on the position of the image in a batch.\n\n  Args:\n    image: float32 Tensor containing single image. Tensor values should be in\n      range [0, 1].\n    batch_position: the position of the image in a batch. NOTE: this argument\n      can be an integer or a tensor\n    distort_color_in_yiq: distort color of input images in YIQ space.\n    scope: Optional scope for op_scope.\n  Returns:\n    color-distorted image\n  """"""\n  with tf.compat.v1.name_scope(scope or \'distort_color\'):\n\n    def distort_fn_0(image=image):\n      """"""Variant 0 of distort function.""""""\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      return image\n\n    def distort_fn_1(image=image):\n      """"""Variant 1 of distort function.""""""\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      return image\n\n    image = utils.smart_cond(batch_position % 2 == 0, distort_fn_0,\n                             distort_fn_1)\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n\nclass RecordInputImagePreprocessor(object):\n  """"""Preprocessor for images with RecordInput format.""""""\n\n  def __init__(self,\n               height,\n               width,\n               batch_size,\n               num_splits,\n               dtype,\n               train,\n               distortions=False,\n               resize_method=""bilinear"",\n               shift_ratio=0,\n               summary_verbosity=1,\n               distort_color_in_yiq=False,\n               fuse_decode_and_crop=False):\n    self.height = height\n    self.width = width\n    self.batch_size = batch_size\n    self.num_splits = num_splits\n    self.dtype = dtype\n    self.train = train\n    self.resize_method = resize_method\n    self.shift_ratio = shift_ratio\n    self.distortions = distortions\n    self.distort_color_in_yiq = distort_color_in_yiq\n    self.fuse_decode_and_crop = fuse_decode_and_crop\n    if self.batch_size % self.num_splits != 0:\n      raise ValueError(\n          (\'batch_size must be a multiple of num_splits: \'\n           \'batch_size %d, num_splits: %d\') %\n          (self.batch_size, self.num_splits))\n    self.batch_size_per_split = self.batch_size // self.num_splits\n    self.summary_verbosity = summary_verbosity\n\n  def center_crop(self, img, init_h, init_w):\n    height, width, _ = img.shape\n\n    left = int((width - init_w) // 2)\n    right = int((width + init_w) // 2)\n    top = int((height - init_h) // 2)\n    bottom = int((height + init_h) // 2)\n\n    img = img[top: bottom, left: right]\n\n    return img\n\n  def image_preprocess(self, image_buffer, bbox, batch_position):\n    """"""Preprocessing image_buffer as a function of its batch position.""""""\n    if self.train:\n      image_buffer = tf.image.decode_jpeg(\n          image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n      image = preprocess_for_train(image_buffer, self.height, self.width, bbox,\n                          batch_position)\n    else:\n      image = tf.image.decode_jpeg(\n          image_buffer, channels=3, dct_method=\'INTEGER_FAST\')\n\n      new_height = int(100. * self.height / 87.5)\n      new_width = int(100. * self.width / 87.5)\n\n      if(self.height > self.width):\n        w = new_width\n        h = int(new_height * self.height / self.width)\n      else:\n        h = new_height\n        w = int(new_width * self.width / self.height)\n\n      image = preprocess_for_eval(image, h, w)\n      image = self.center_crop(image, self.height, self.width)\n\n    return image\n\n  def parse_and_preprocess(self, value, batch_position):\n    image_buffer, label_index, bbox, _ = parse_example_proto(value)\n    image = self.image_preprocess(image_buffer, bbox, batch_position)\n    return (label_index, image)\n\n  def minibatch(self, dataset, subset, use_datasets, cache_data,\n                shift_ratio=-1):\n    if shift_ratio < 0:\n      shift_ratio = self.shift_ratio\n    with tf.compat.v1.name_scope(\'batch_processing\'):\n      # Build final results per split.\n      images = [[] for _ in range(self.num_splits)]\n      labels = [[] for _ in range(self.num_splits)]\n      if use_datasets:\n        glob_pattern = dataset.tf_record_pattern(subset)\n        file_names = gfile.Glob(glob_pattern)\n        if not file_names:\n          raise ValueError(\'Found no files in --data_dir matching: {}\'\n                           .format(glob_pattern))\n        ds = tf.data.TFRecordDataset.list_files(file_names, shuffle=False)\n        ds = ds.apply(\n            interleave_ops.parallel_interleave(\n                tf.data.TFRecordDataset, cycle_length=10))\n        if cache_data:\n          ds = ds.take(1).cache().repeat()\n        counter = tf.data.Dataset.range(self.batch_size)\n        counter = counter.repeat()\n        ds = tf.data.Dataset.zip((ds, counter))\n        ds = ds.prefetch(buffer_size=self.batch_size)\n        ds = ds.repeat()\n        ds = ds.apply(\n            batching.map_and_batch(\n                map_func=self.parse_and_preprocess,\n                batch_size=self.batch_size_per_split,\n                num_parallel_batches=self.num_splits))\n        ds = ds.prefetch(buffer_size=self.num_splits)\n        ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n        for d in xrange(self.num_splits):\n          labels[d], images[d] = ds_iterator.get_next()\n\n      else:\n        record_input = data_flow_ops.RecordInput(\n            file_pattern=dataset.tf_record_pattern(subset),\n            seed=301,\n            parallelism=64,\n            buffer_size=10000,\n            batch_size=self.batch_size,\n            shift_ratio=shift_ratio,\n            name=\'record_input\')\n        records = record_input.get_yield_op()\n        records = tf.split(records, self.batch_size, 0)\n        records = [tf.reshape(record, []) for record in records]\n        for idx in xrange(self.batch_size):\n          value = records[idx]\n          (label, image) = self.parse_and_preprocess(value, idx)\n          split_index = idx % self.num_splits\n          labels[split_index].append(label)\n          images[split_index].append(image)\n\n      for split_index in xrange(self.num_splits):\n        if not use_datasets:\n          images[split_index] = tf.parallel_stack(images[split_index])\n          labels[split_index] = tf.concat(labels[split_index], 0)\n        images[split_index] = tf.cast(images[split_index], self.dtype)\n        depth = 3\n        images[split_index] = tf.reshape(\n            images[split_index],\n            shape=[self.batch_size_per_split, self.height, self.width, depth])\n        labels[split_index] = tf.reshape(labels[split_index],\n                                         [self.batch_size_per_split])\n      return images, labels\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/__init__.py,0,b'from . import mlperf_log\n'
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/_gnmt_tags.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Keys which only appear in GNMT RNN Translation.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n# Loss smoothing factor\nMODEL_HP_LOSS_SMOOTHING = ""model_hp_loss_smoothing""\n\n# Number of layers in encoder and in decoder\nMODEL_HP_NUM_LAYERS = ""model_hp_num_layers""\n\n# RNN hidden size\nMODEL_HP_HIDDEN_SIZE = ""model_hp_hidden_size""\n\n# Dropout\nMODEL_HP_DROPOUT = ""model_hp_dropout""\n\n# Beam size for beam search\nEVAL_HP_BEAM_SIZE = ""eval_hp_beam_size""\n\n# Maximum sequence length for training\nTRAIN_HP_MAX_SEQ_LEN = ""train_hp_max_sequence_length""\n\n# Maximum sequence length for evaluation\nEVAL_HP_MAX_SEQ_LEN = ""eval_hp_max_sequence_length""\n\n# Length normalization constant for beam search\nEVAL_HP_LEN_NORM_CONST = ""eval_hp_length_normalization_constant""\n\n# Length normalization factor for beam search\nEVAL_HP_LEN_NORM_FACTOR = ""eval_hp_length_normalization_factor""\n\n# Coverage penalty factor for beam search\nEVAL_HP_COV_PENALTY_FACTOR = ""eval_hp_coverage_penalty_factor""\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/_maskrcnn_tags.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Keys which only appear in MASKRCNN.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n# Anchor overlap threshop\nFG_IOU_THRESHOLD = ""foreground_iou_threshold""\nBG_IOU_THRESHOLD = ""background_iou_threshold""\n\n# Top ROIs to be selected before and after NMS\nRPN_PRE_NMS_TOP_N_TRAIN = ""rpn_pre_nms_top_n_train""\nRPN_PRE_NMS_TOP_N_TEST = ""rpn_pre_nms_top_n_test""\nRPN_POST_NMS_TOP_N_TRAIN = ""rpn_post_nms_top_n_train""\nRPN_POST_NMS_TOP_N_TEST = ""rpn_post_nms_top_n_test""\n\n#Global batch size during training\nGLOBAL_BATCH_SIZE = ""global_batch_size""\n\n# Batch size during eval\nBATCH_SIZE_TEST = ""batch_size_test""\n\n\n# Pretrained classifer model\nBACKBONE = ""backbone""\n\n# Anchor aspect ratio\nASPECT_RATIOS = ""aspect_ratios""\n\n# Overlap threshold for NMS\nNMS_THRESHOLD = ""nms_threshold""\n\n# data pipeline\nMIN_IMAGE_SIZE = ""min_image_size""\nMAX_IMAGE_SIZE = ""max_image_size""\nRANDOM_FLIP_PROBABILITY = ""random_flip_probability""\nINPUT_NORMALIZATION_STD = ""input_normalization_std""\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/_ncf_tags.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Keys which only appear in NCF Recommendation.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# The minimum number of ratings for a user to be included.\nPREPROC_HP_MIN_RATINGS = ""preproc_hp_min_ratings""\n\n# The number of false negatives to use during evaluation.\nPREPROC_HP_NUM_EVAL = ""preproc_hp_num_eval""\n\n# Are evaluation negatives sampled with replacement?\nPREPROC_HP_SAMPLE_EVAL_REPLACEMENT = ""preproc_hp_sample_eval_replacement""\n\n\n# The number of false negatives per postive generated during training.\nINPUT_HP_NUM_NEG = ""input_hp_num_neg""\n\n# Are training negatives sampled with replacement?\nINPUT_HP_SAMPLE_TRAIN_REPLACEMENT = ""input_hp_sample_train_replacement""\n\n# This tag should be emitted each time the submission begins construction of the\n# false negatives for a trainging epoch.\nINPUT_STEP_TRAIN_NEG_GEN = ""input_step_train_neg_gen""\n\n# This tag should be emitted when the evaluation negatives are selected. This\n# should occur only once.\nINPUT_STEP_EVAL_NEG_GEN = ""input_step_eval_neg_gen""\n\n# The number of users in the evaluation set. This should be the same as the\n# number of users in the training set.\nEVAL_HP_NUM_USERS = ""eval_hp_num_users""\n\n# The number of false negatives per positive which actually appear during\n# evaluation. This should match PREPROC_HP_NUM_EVAL.\nEVAL_HP_NUM_NEG = ""eval_hp_num_neg""\n\n\n# The dimensionality of the matrix factorization portion of the model.\nMODEL_HP_MF_DIM = ""model_hp_mf_dim""\n\n# The sizes of the fully connected layers in the dense section of the model.\nMODEL_HP_MLP_LAYER_SIZES = ""model_hp_mlp_layer_sizes""\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/_resnet_tags.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Keys which only appear in ResNet.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nBOTTLENECK_BLOCK = ""bottleneck_block""\n\n# The ResNet reference specifies that evaluation occurs once every four epochs.\n# This can result in a quantization penalty for batch sizes which converge on\n# certain epochs. For instance a batch size which tends to converge on epoch 81\n# or 82 would be unduly punished by evaluating at epochs 80 and 84. In order to\n# address this, submissions may select an offset between 0 and 3 for the first\n# evaluation. So in the example above, the submitter could select an offset of\n# 1. In that case the first evaluation would occur on epoch 2, with later\n# evaluations correspondingly offset. Because this would trigger an eval on\n# epoch 82, the submission in this example can exit at a natural time.\nEVAL_EPOCH_OFFSET = ""eval_offset""\n\n# ==============================================================================\n# == Topology ==================================================================\n# ==============================================================================\n\nMODEL_HP_INITIAL_MAX_POOL = ""model_hp_initial_max_pool""\nMODEL_HP_BEGIN_BLOCK = ""model_hp_begin_block""\nMODEL_HP_END_BLOCK = ""model_hp_end_block""\nMODEL_HP_BLOCK_TYPE = ""model_hp_block_type""\nMODEL_HP_PROJECTION_SHORTCUT = ""model_hp_projection_shortcut""\nMODEL_HP_SHORTCUT_ADD = ""model_hp_shorcut_add""\n\nMODEL_HP_RESNET_TOPOLOGY = ""model_hp_resnet_topology""\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/_ssd_tags.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Keys which only appear in SSD.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n# Pretrained classifer model\nBACKBONE = ""backbone""\n\nFEATURE_SIZES = ""feature_sizes""\nSTEPS = ""steps""\nSCALES = ""scales""\nASPECT_RATIOS = ""aspect_ratios""\nNUM_DEFAULTS_PER_CELL = ""num_defaults_per_cell""\nLOC_CONF_OUT_CHANNELS = ""loc_conf_out_channels""\nNUM_DEFAULTS = ""num_default_boxes""\n\n# Overlap threshold for NMS\nNMS_THRESHOLD = ""nms_threshold""\nNMS_MAX_DETECTIONS = ""nms_max_detections""\n\n# data pipeline\nNUM_CROPPING_ITERATIONS = ""num_cropping_iterations""\nRANDOM_FLIP_PROBABILITY = ""random_flip_probability""\nDATA_NORMALIZATION_MEAN = ""data_normalization_mean""\nDATA_NORMALIZATION_STD = ""data_normalization_std""\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/_transformer_tags.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Keys which only appear in transformer.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nINPUT_MAX_LENGTH = ""input_max_length""\n\nMODEL_HP_INITIALIZER_GAIN = ""model_hp_initializer_gain""\nMODEL_HP_VOCAB_SIZE = ""model_hp_vocab_size""\nMODEL_HP_NUM_HIDDEN_LAYERS = ""model_hp_hidden_layers""\nMODEL_HP_EMBEDDING_SHARED_WEIGHTS = ""model_hp_embedding_shared_weights""\nMODEL_HP_ATTENTION_DENSE = ""model_hp_attention_dense""\nMODEL_HP_ATTENTION_DROPOUT = ""model_hp_attention_dropout""\nMODEL_HP_FFN_OUTPUT_DENSE = ""model_hp_ffn_output_dense""\nMODEL_HP_FFN_FILTER_DENSE = ""model_hp_ffn_filter_dense""\nMODEL_HP_RELU_DROPOUT = ""model_hp_relu_dropout""\nMODEL_HP_LAYER_POSTPROCESS_DROPOUT = ""model_hp_layer_postprocess_dropout""\nMODEL_HP_NORM = ""model_hp_norm""\nMODEL_HP_SEQ_BEAM_SEARCH = ""model_hp_sequence_beam_search""\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/mlperf_log.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convenience function for logging compliance tags to stdout.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\nimport logging\nimport json\nimport os\nimport re\nimport sys\nimport time\nimport uuid\n\nfrom mlperf_compliance.tags import *\n\nROOT_DIR_GNMT = None\nROOT_DIR_MASKRCNN = None\nROOT_DIR_MINIGO = None\nROOT_DIR_NCF = None\n\n# Set by imagenet_main.py\nROOT_DIR_RESNET = None\n\nROOT_DIR_SSD = None\n\n# Set by transformer_main.py and process_data.py\nROOT_DIR_TRANSFORMER = None\n\n\nPATTERN = re.compile(\'[a-zA-Z0-9]+\')\n\nLOG_FILE = os.getenv(""COMPLIANCE_FILE"")\n# create logger with \'spam_application\'\nLOGGER = logging.getLogger(\'mlperf_compliance\')\nLOGGER.setLevel(logging.DEBUG)\n\n_STREAM_HANDLER = logging.StreamHandler(stream=sys.stdout)\n_STREAM_HANDLER.setLevel(logging.INFO)\nLOGGER.addHandler(_STREAM_HANDLER)\n\nif LOG_FILE:\n  _FILE_HANDLER = logging.FileHandler(LOG_FILE)\n  _FILE_HANDLER.setLevel(logging.DEBUG)\n  LOGGER.addHandler(_FILE_HANDLER)\nelse:\n  _STREAM_HANDLER.setLevel(logging.DEBUG)\n\n\n\ndef get_caller(stack_index=2, root_dir=None):\n  \'\'\' Returns file.py:lineno of your caller. A stack_index of 2 will provide\n      the caller of the function calling this function. Notice that stack_index\n      of 2 or more will fail if called from global scope. \'\'\'\n  caller = inspect.getframeinfo(inspect.stack()[stack_index][0])\n\n  # Trim the filenames for readability.\n  filename = caller.filename\n  if root_dir is not None:\n    filename = re.sub(""^"" + root_dir + ""/"", """", filename)\n  return ""%s:%d"" % (filename, caller.lineno)\n\n\ndef _mlperf_print(key, value=None, benchmark=None, stack_offset=0,\n                  tag_set=None, deferred=False, root_dir=None,\n                  extra_print=False, prefix=""""):\n  \'\'\' Prints out an MLPerf Log Line.\n\n  key: The MLPerf log key such as \'CLOCK\' or \'QUALITY\'. See the list of log keys in the spec.\n  value: The value which contains no newlines.\n  benchmark: The short code for the benchmark being run, see the MLPerf log spec.\n  stack_offset: Increase the value to go deeper into the stack to find the callsite. For example, if this\n                is being called by a wraper/helper you may want to set stack_offset=1 to use the callsite\n                of the wraper/helper itself.\n  tag_set: The set of tags in which key must belong.\n  deferred: The value is not presently known. In that case, a unique ID will\n            be assigned as the value of this call and will be returned. The\n            caller can then include said unique ID when the value is known\n            later.\n  root_dir: Directory prefix which will be trimmed when reporting calling file\n            for compliance logging.\n  extra_print: Print a blank line before logging to clear any text in the line.\n  prefix: String with which to prefix the log message. Useful for\n          differentiating raw lines if stitching will be required.\n\n  Example output:\n    :::MLP-1537375353 MINGO[17] (eval.py:42) QUALITY: 43.7\n  \'\'\'\n\n  return_value = None\n\n  if (tag_set is None and not PATTERN.match(key)) or key not in tag_set:\n    raise ValueError(\'Invalid key for MLPerf print: \' + str(key))\n\n  if value is not None and deferred:\n    raise ValueError(""deferred is set to True, but a value was provided"")\n\n  if deferred:\n    return_value = str(uuid.uuid4())\n    value = ""DEFERRED: {}"".format(return_value)\n\n  if value is None:\n    tag = key\n  else:\n    str_json = json.dumps(value)\n    tag = \'{key}: {value}\'.format(key=key, value=str_json)\n\n  callsite = get_caller(2 + stack_offset, root_dir=root_dir)\n  now = time.time()\n\n  message = \'{prefix}:::MLPv0.5.0 {benchmark} {secs:.9f} ({callsite}) {tag}\'.format(\n      prefix=prefix, secs=now, benchmark=benchmark, callsite=callsite, tag=tag)\n\n  if extra_print:\n    print() # There could be prior text on a line\n\n  if tag in STDOUT_TAG_SET:\n    LOGGER.info(message)\n  else:\n    LOGGER.debug(message)\n\n  return return_value\n\n\nGNMT_TAG_SET = set(GNMT_TAGS)\ndef gnmt_print(key, value=None, stack_offset=1, deferred=False, prefix=""""):\n  return _mlperf_print(key=key, value=value, benchmark=GNMT,\n                       stack_offset=stack_offset, tag_set=GNMT_TAG_SET,\n                       deferred=deferred, root_dir=ROOT_DIR_GNMT)\n\n\nMASKRCNN_TAG_SET = set(MASKRCNN_TAGS)\ndef maskrcnn_print(key, value=None, stack_offset=1, deferred=False,\n    extra_print=True, prefix=""""):\n  return _mlperf_print(key=key, value=value, benchmark=MASKRCNN,\n                       stack_offset=stack_offset, tag_set=MASKRCNN_TAG_SET,\n                       deferred=deferred, extra_print=extra_print,\n                       root_dir=ROOT_DIR_MASKRCNN, prefix=prefix)\n\n\nMINIGO_TAG_SET = set(MINIGO_TAGS)\ndef minigo_print(key, value=None, stack_offset=1, deferred=False, prefix=""""):\n  return _mlperf_print(key=key, value=value, benchmark=MINIGO,\n                       stack_offset=stack_offset, tag_set=MINIGO_TAG_SET,\n                       deferred=deferred, root_dir=ROOT_DIR_MINIGO,\n                       prefix=prefix)\n\n\nNCF_TAG_SET = set(NCF_TAGS)\ndef ncf_print(key, value=None, stack_offset=1, deferred=False,\n              extra_print=True, prefix=""""):\n  # Extra print is needed for the reference NCF because of tqdm.\n  return _mlperf_print(key=key, value=value, benchmark=NCF,\n                       stack_offset=stack_offset, tag_set=NCF_TAG_SET,\n                       deferred=deferred, extra_print=extra_print,\n                       root_dir=ROOT_DIR_NCF, prefix=prefix)\n\n\nRESNET_TAG_SET = set(RESNET_TAGS)\ndef resnet_print(key, value=None, stack_offset=1, deferred=False, prefix=""""):\n  return _mlperf_print(key=key, value=value, benchmark=RESNET,\n                       stack_offset=stack_offset, tag_set=RESNET_TAG_SET,\n                       deferred=deferred, root_dir=ROOT_DIR_RESNET,\n                       prefix=prefix)\n\n\nSSD_TAG_SET = set(SSD_TAGS)\ndef ssd_print(key, value=None, stack_offset=1, deferred=False,\n              extra_print=True, prefix=""""):\n  return _mlperf_print(key=key, value=value, benchmark=SSD,\n                       stack_offset=stack_offset, tag_set=SSD_TAG_SET,\n                       deferred=deferred, extra_print=extra_print,\n                       root_dir=ROOT_DIR_SSD, prefix=prefix)\n\n\nTRANSFORMER_TAG_SET = set(TRANSFORMER_TAGS)\ndef transformer_print(key, value=None, stack_offset=1, deferred=False, prefix=""""):\n  return _mlperf_print(key=key, value=value, benchmark=TRANSFORMER,\n                       stack_offset=stack_offset, tag_set=TRANSFORMER_TAG_SET,\n                       deferred=deferred, root_dir=ROOT_DIR_TRANSFORMER,\n                       prefix=prefix)\n\n\nif __name__ == \'__main__\':\n  ncf_print(EVAL_ACCURACY, {\'epoch\': 7, \'accuracy\': 43.7})\n  ncf_print(INPUT_SIZE, 1024)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/resnet_log_helper.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convenience functions for logging ResNet topology.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom mlperf_compliance import mlperf_log\n\n_STACK_OFFSET = 2\n\ndef _get_shape(input_tensor):\n  return ""({})"".format("", "".join(\n      [str(i) for i in input_tensor.shape.as_list()[1:]]))\n\n\ndef _in_out_shape(input_tensor, output_tensor):\n  return ""{} -> {}"".format( _get_shape(input_tensor), _get_shape(output_tensor))\n\n\ndef log_max_pool(input_tensor, output_tensor):\n  mlperf_log.resnet_print(\n      key=mlperf_log.MODEL_HP_INITIAL_MAX_POOL, value=_in_out_shape(\n      input_tensor=input_tensor, output_tensor=output_tensor),\n      stack_offset=_STACK_OFFSET)\n\n\ndef log_begin_block(input_tensor, block_type):\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_BEGIN_BLOCK,\n                          value={""block_type"": block_type},\n                          stack_offset=_STACK_OFFSET)\n  mlperf_log.resnet_print(\n      key=mlperf_log.MODEL_HP_RESNET_TOPOLOGY,\n      value="" Block Input: {}"".format(_get_shape(input_tensor)),\n      stack_offset=_STACK_OFFSET)\n\n\ndef log_end_block(output_tensor):\n  mlperf_log.resnet_print(\n      key=mlperf_log.MODEL_HP_END_BLOCK,\n      value="" Block Output: {}"".format(_get_shape(output_tensor)),\n      stack_offset=_STACK_OFFSET)\n\n\ndef log_projection(input_tensor, output_tensor):\n  mlperf_log.resnet_print(\n      key=mlperf_log.MODEL_HP_PROJECTION_SHORTCUT,\n      value=_in_out_shape(input_tensor, output_tensor),\n      stack_offset=_STACK_OFFSET)\n\n\ndef log_conv2d(input_tensor, output_tensor, stride, filters, initializer,\n               use_bias):\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_CONV2D_FIXED_PADDING,\n                          value=_in_out_shape(input_tensor, output_tensor),\n                          stack_offset=_STACK_OFFSET)\n  mlperf_log.resnet_print(\n      key=mlperf_log.MODEL_HP_CONV2D_FIXED_PADDING,\n      value={""stride"": stride, ""filters"": filters, ""initializer"": initializer,\n             ""use_bias"": use_bias},\n      stack_offset=_STACK_OFFSET)\n\n\ndef log_batch_norm(input_tensor, output_tensor, momentum, epsilon, center,\n                   scale, training):\n  assert _get_shape(input_tensor) == _get_shape(output_tensor)\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_BATCH_NORM, value={\n    ""shape"": _get_shape(input_tensor), ""momentum"": momentum, ""epsilon"": epsilon,\n    ""center"": center, ""scale"": scale, ""training"": training},\n                          stack_offset=_STACK_OFFSET)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/tags.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Master list of MLPerf tags to be logged for benchmark submissions.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom mlperf_compliance._gnmt_tags import *\nfrom mlperf_compliance._ncf_tags import *\nfrom mlperf_compliance._resnet_tags import *\nfrom mlperf_compliance._ssd_tags import *\nfrom mlperf_compliance._transformer_tags import *\nfrom mlperf_compliance._maskrcnn_tags import *\n\n# ==============================================================================\n# == Benchmarks ================================================================\n# ==============================================================================\n\n# rnn_translator\nGNMT = ""gnmt""\n\n# reinforcement/\nMINIGO = ""minigo""\n\n# recommendation/\nNCF = ""ncf""\n\n# image_classification/\nRESNET = ""resnet""\n\n# single_stage_detector/\nSSD = ""ssd""\n\n# object_detection/\nMASKRCNN = ""maskrcnn""\n\n# translation/\nTRANSFORMER = ""transformer""\n\n# ==============================================================================\n# == Tags ======================================================================\n# ==============================================================================\n""""""\nTags may be used by all models, a subset of models, or only one model. A\nspecification for which models require which tags can be found below the tag\ndefinitions.\n""""""\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n# All models: Tags which should appear in absolutely every MLPerf model.\n# //////////////////////////////////////////////////////////////////////////////\n\n# This tag signals to start the timer. Emission of this tag need not be (and\n# generally will not be) the first part of a submission script. Rather, this\n# tag must be emitted prior to performing any work which the MLPerf rules\n# state must be timed. This tag is generally emitted directly before the first\n# step which invokes random number generation or the first step which must be\n# performed on the system under test. (Whichever comes first.) If clarification\n# is needed, please file an issue under:\n#   https://github.com/mlperf/policies\nRUN_START = ""run_start""\n\n# This tag signals that a submission has reached the relevant stopping criteria,\n# and has completed all tasks which are performed in the reference. The wall\n# time for a submission will be computed as the difference between the time\n# when this tag is emitted and the time whe the RUN_START is emitted.\nRUN_STOP = ""run_stop""\n\n# This tag should be emitted immediately before ending a run, and should be the\n# last tag emitted. This tag should indicate the completion of untimed post\n# processing work such as system specific cleanup.\nRUN_FINAL = ""run_final""\n\n\n# Emit this tag in the place(s) where random seeds are set.\nRUN_SET_RANDOM_SEED = ""run_set_random_seed""\n\n# Emit this tag when training data has been purged from volatile caches prior\n# to run start.\nRUN_CLEAR_CACHES = ""run_clear_caches""\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n# Common Values: Constants which are expected to be reported across many models.\n#                These values are included for convenience.\n# //////////////////////////////////////////////////////////////////////////////\nBCE = ""binary_cross_entropy""\nCCE = ""categorical_cross_entropy""\n\nSGD = ""stochastic_gradient_descent""\n\n# Some conventions distinguish between ""vanilla"" SGD and SGD with momentum\n# (where vanilla SGD would be the specific case of momentum=0)\nSGD_WITH_MOMENTUM = ""stochastic_gradient_descent_with_momentum""\n\nADAM = ""adam""\nLAZY_ADAM = ""lazy_adam""\n\nTRUNCATED_NORMAL = ""truncated_normal""\n\nRELU = ""relu""\n\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n# Preprocessing: Tags for generic preprocessing steps\n# //////////////////////////////////////////////////////////////////////////////\n\n# The number of training examples in a single epoch\nPREPROC_NUM_TRAIN_EXAMPLES = ""preproc_num_train_examples""\n\n# The number of evaluation examples in a single epoch\nPREPROC_NUM_EVAL_EXAMPLES = ""preproc_num_eval_examples""\n\n# This tag is used to declare what part of code tokenizes the training data.\nPREPROC_TOKENIZE_TRAINING = ""preproc_tokenize_training""\n\n# This tag is used to declare what part of code tokenizes the evaluation data.\nPREPROC_TOKENIZE_EVAL = ""preproc_tokenize_eval""\n\n# The vocabulary size used for tokenization.\nPREPROC_VOCAB_SIZE = ""preproc_vocab_size""\n\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n# Input: Tags for the timed portion of the data input pipeline\n# //////////////////////////////////////////////////////////////////////////////\n\n# The number of examples in the training portion of the data pipeline. Generally\n# this should match PREPROC_NUM_TRAIN_EXAMPLES. If it does not (for instance\n# if certain examples are dropped in compliance with MLPerf rules), the\n# call which declares this tag is a good place for a comment stating why the\n# disparity is expected.\nINPUT_SIZE = ""input_size""\n\n# The size of a training minibatch size. If this value is variable, please emit\n# ""-1"" and then log an implementation specific characterization of the batch\n# size which is a reasonable analog to the reference. (For instance log that\n# all but the last batch has size 64, and the last batch is a partial batch)\nINPUT_BATCH_SIZE = ""input_batch_size""\n\n# This tag indicates where the location of the code which defines the order in\n# which training examples are traversed. It is not necessary to describe the\n# method in the tag emission (though comments are always welcome). Rather, this\n# should simply provide a good starting point to an interested party.\nINPUT_ORDER = ""input_order""\n\n# The shard size (in items) when shuffling in the input pipeline.\nINPUT_SHARD = ""input_shard""\n\n# The number of samples iver which BN stats are computed for normalization during training\nINPUT_BN_SPAN = ""input_bn_span""\n\n\n# --------------------------------------\n# -- Data Augmentation and Alteration --\n# --------------------------------------\n\n# ResNet random cropping\nINPUT_CENTRAL_CROP = ""input_central_crop""\n\nINPUT_CROP_USES_BBOXES = ""input_crop_uses_bboxes""\n\nINPUT_DISTORTED_CROP_MIN_OBJ_COV = ""input_distorted_crop_min_object_covered""\nINPUT_DISTORTED_CROP_RATIO_RANGE = ""input_distorted_crop_aspect_ratio_range""\nINPUT_DISTORTED_CROP_AREA_RANGE = ""input_distorted_crop_area_range""\nINPUT_DISTORTED_CROP_MAX_ATTEMPTS = ""input_distorted_crop_max_attempts""\n\nINPUT_MEAN_SUBTRACTION = ""input_mean_subtraction""\n\n# Random flip of an image for data augmentation\nINPUT_RANDOM_FLIP = ""input_random_flip""\n\nINPUT_RESIZE = ""input_resize""\nINPUT_RESIZE_ASPECT_PRESERVING = ""input_resize_aspect_preserving""\n\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n# Opt: Tags for declaring optimizer specific information. Submissions should\n#      declare and log explicit values rather than relying on defaults.\n# //////////////////////////////////////////////////////////////////////////////\n\n# The name of the optimizer used. (SGD, Adam, etc.)\nOPT_NAME = ""opt_name""\n\nOPT_LR = ""opt_learning_rate""\nOPT_MOMENTUM = ""opt_momentum""\n\nOPT_WEIGHT_DECAY = ""opt_weight_decay""\n\n# beta1, beta2, and epsilon are optimizer hyperparameters associated with the\n# Adam optimizer and its variants (e.g. LazyAdam).\nOPT_HP_ADAM_BETA1 = ""opt_hp_Adam_beta1""\nOPT_HP_ADAM_BETA2 = ""opt_hp_Adam_beta2""\nOPT_HP_ADAM_EPSILON = ""opt_hp_Adam_epsilon""\n\n# The number of warm-up steps (SGD).\nOPT_LR_WARMUP_STEPS = ""opt_learning_rate_warmup_steps""\n\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n#  Train: Tags for control flow during model training.\n# //////////////////////////////////////////////////////////////////////////////\n\n# This tag is emitted when a model first enters its training loop. This is not\n# necessarily when it begins to apply gradients; rather, it should be placed at\n# a location which logically partitions the submission code.\nTRAIN_LOOP = ""train_loop""\n\n# The current epoch as said epoch begins training.\nTRAIN_EPOCH = ""train_epoch""\n\n# This tag is used to indicate approximately where checkpoints are written. Some\n# frameworks abstract away checkpoint saving; in such cases simply choose a\n# logical place in the code which signals that the framework has been instructed\n# to save checkpoints, along with an explanatory comment.\nTRAIN_CHECKPOINT = ""train_checkpoint""\n\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n#  Eval: Tags for control flow during model evaluation.\n# //////////////////////////////////////////////////////////////////////////////\n\n# This tag should be emitted whenever the submission begins an evaluation pass\n# for a given set of weights.\nEVAL_START = ""eval_start""\n\n# The number of examples on which evaluation is performed.\nEVAL_SIZE = ""eval_size""\n\n# The target quality at which the model may stop training.\nEVAL_TARGET = ""eval_target""\n\n# The observed accuracy of the model at a given epoch.\nEVAL_ACCURACY = ""eval_accuracy""\n\n# This tag should be emitted whenever the submission ends an evaluation pass\n# for a given set of weights.\nEVAL_STOP = ""eval_stop""\n\n\n# The observed accuracy of the model at a given iteration. This is only for\n# models which evaluate at certain iterations instead of epochs.\nEVAL_ITERATION_ACCURACY = ""eval_iteration_accuracy""\n\n# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n#  Model: Tags for logging topology specific information.\n# //////////////////////////////////////////////////////////////////////////////\n\n# The loss function (cross entropy, squared error, etc.) used by the model. For\n# more exotic loss functions such as those encountered in object detection\n# models, additional benchmark specific subcomponents should also be logged.\nMODEL_HP_LOSS_FN = ""model_hp_loss_fn""\n\nMODEL_HP_INITIAL_SHAPE = ""model_hp_initial_shape""\nMODEL_HP_FINAL_SHAPE = ""model_hp_final_shape""\n\nMODEL_L2_REGULARIZATION = ""model_l2_regularization""\nMODEL_EXCLUDE_BN_FROM_L2 = ""model_exclude_bn_from_l2""\n\nMODEL_HP_RELU = ""model_hp_relu""\nMODEL_HP_CONV2D_FIXED_PADDING = ""model_hp_conv2d_fixed_padding""\nMODEL_HP_BATCH_NORM = ""model_hp_batch_norm""\nMODEL_HP_DENSE = ""model_hp_dense""\n\n\n# ==============================================================================\n# == Stdout tags ===============================================================\n# ==============================================================================\n\n# These tags are always logged to stdout. The rest will be logged to a file if\n# one is available.\nSTDOUT_TAG_SET = {\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n\n    EVAL_START,\n    EVAL_SIZE,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n}\n\n\n# ==============================================================================\n# == Benchmark tag sets ========================================================\n# ==============================================================================\nALL_USED_TAGS = set()\n\nGNMT_TAGS = (\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n    RUN_SET_RANDOM_SEED,\n    RUN_CLEAR_CACHES,\n\n    PREPROC_VOCAB_SIZE,\n    PREPROC_TOKENIZE_TRAINING,\n    PREPROC_TOKENIZE_EVAL,\n    PREPROC_NUM_TRAIN_EXAMPLES,\n    PREPROC_NUM_EVAL_EXAMPLES,\n\n    INPUT_SIZE,\n    INPUT_BATCH_SIZE,\n    INPUT_ORDER,\n    INPUT_SHARD,\n\n    OPT_NAME,\n    OPT_LR,\n    OPT_LR_WARMUP_STEPS,\n    OPT_HP_ADAM_BETA1,\n    OPT_HP_ADAM_BETA2,\n    OPT_HP_ADAM_EPSILON,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n    TRAIN_CHECKPOINT,\n    TRAIN_HP_MAX_SEQ_LEN,\n\n    EVAL_START,\n    EVAL_SIZE,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n    EVAL_HP_BEAM_SIZE,\n    EVAL_HP_MAX_SEQ_LEN,\n    EVAL_HP_LEN_NORM_CONST,\n    EVAL_HP_LEN_NORM_FACTOR,\n    EVAL_HP_COV_PENALTY_FACTOR,\n\n    MODEL_HP_LOSS_FN,\n    MODEL_HP_LOSS_SMOOTHING,\n    MODEL_HP_NUM_LAYERS,\n    MODEL_HP_HIDDEN_SIZE,\n    MODEL_HP_DROPOUT\n)\n\nMASKRCNN_TAGS = (\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n    RUN_CLEAR_CACHES,\n\n    INPUT_SIZE,\n    INPUT_BATCH_SIZE,\n    GLOBAL_BATCH_SIZE,\n    INPUT_ORDER,\n    INPUT_SHARD,\n\n    BACKBONE,\n    NMS_THRESHOLD,\n    NMS_MAX_DETECTIONS,\n\n    OPT_NAME,\n    OPT_LR,\n    OPT_LR_WARMUP_STEPS,\n    OPT_MOMENTUM,\n    OPT_WEIGHT_DECAY,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n\n    EVAL_START,\n    EVAL_SIZE,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n    INPUT_MEAN_SUBTRACTION,\n    INPUT_NORMALIZATION_STD,\n    INPUT_RESIZE,\n    INPUT_RESIZE_ASPECT_PRESERVING,\n    MIN_IMAGE_SIZE,\n    MAX_IMAGE_SIZE,\n    RUN_SET_RANDOM_SEED,\n    INPUT_RANDOM_FLIP,\n    RANDOM_FLIP_PROBABILITY,\n    FG_IOU_THRESHOLD,\n    BG_IOU_THRESHOLD,\n    RPN_PRE_NMS_TOP_N_TRAIN,\n    RPN_PRE_NMS_TOP_N_TEST,\n    RPN_POST_NMS_TOP_N_TRAIN,\n    RPN_POST_NMS_TOP_N_TEST,\n    BATCH_SIZE_TEST,\n    ASPECT_RATIOS,\n)\n\nMINIGO_TAGS = (\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n    RUN_SET_RANDOM_SEED,\n    RUN_CLEAR_CACHES,\n\n    INPUT_SHARD,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n\n    EVAL_START,\n    EVAL_SIZE,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n)\n\nNCF_TAGS = (\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n    RUN_CLEAR_CACHES,\n\n    PREPROC_HP_MIN_RATINGS,\n    PREPROC_HP_NUM_EVAL,\n    PREPROC_HP_SAMPLE_EVAL_REPLACEMENT,\n\n    INPUT_SIZE,\n    INPUT_BATCH_SIZE,\n    INPUT_ORDER,\n    INPUT_SHARD,\n    INPUT_HP_NUM_NEG,\n    INPUT_HP_SAMPLE_TRAIN_REPLACEMENT,\n    INPUT_STEP_TRAIN_NEG_GEN,\n    INPUT_STEP_EVAL_NEG_GEN,\n\n    OPT_NAME,\n    OPT_LR,\n    OPT_HP_ADAM_BETA1,\n    OPT_HP_ADAM_BETA2,\n    OPT_HP_ADAM_EPSILON,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n\n    EVAL_START,\n    EVAL_SIZE,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n    EVAL_HP_NUM_USERS,\n    EVAL_HP_NUM_NEG,\n\n    MODEL_HP_LOSS_FN,\n    MODEL_HP_MF_DIM,\n    MODEL_HP_MLP_LAYER_SIZES,\n)\n\nRESNET_TAGS = (\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n    RUN_SET_RANDOM_SEED,\n    RUN_CLEAR_CACHES,\n\n    PREPROC_NUM_TRAIN_EXAMPLES,\n    PREPROC_NUM_EVAL_EXAMPLES,\n\n    INPUT_SIZE,\n    INPUT_BATCH_SIZE,\n    INPUT_ORDER,\n    INPUT_SHARD,\n    INPUT_CENTRAL_CROP,\n    INPUT_CROP_USES_BBOXES,\n    INPUT_DISTORTED_CROP_MIN_OBJ_COV,\n    INPUT_DISTORTED_CROP_RATIO_RANGE,\n    INPUT_DISTORTED_CROP_AREA_RANGE,\n    INPUT_DISTORTED_CROP_MAX_ATTEMPTS,\n    INPUT_MEAN_SUBTRACTION,\n    INPUT_RANDOM_FLIP,\n    INPUT_RESIZE,\n    INPUT_RESIZE_ASPECT_PRESERVING,\n    INPUT_BN_SPAN,\n\n    OPT_NAME,\n    OPT_LR,\n    OPT_LR_WARMUP_STEPS,\n    OPT_MOMENTUM,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n\n    EVAL_START,\n    EVAL_SIZE,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n    EVAL_EPOCH_OFFSET,\n\n    MODEL_HP_LOSS_FN,\n    MODEL_L2_REGULARIZATION,\n    MODEL_EXCLUDE_BN_FROM_L2,\n\n    MODEL_HP_INITIAL_SHAPE,\n    MODEL_HP_FINAL_SHAPE,\n    MODEL_HP_INITIAL_MAX_POOL,\n    MODEL_HP_BEGIN_BLOCK,\n    MODEL_HP_END_BLOCK,\n    MODEL_HP_BLOCK_TYPE,\n    MODEL_HP_PROJECTION_SHORTCUT,\n    MODEL_HP_SHORTCUT_ADD,\n    MODEL_HP_RELU,\n    MODEL_HP_CONV2D_FIXED_PADDING,\n    MODEL_HP_BATCH_NORM,\n    MODEL_HP_DENSE,\n    MODEL_HP_RESNET_TOPOLOGY,\n)\n\nSSD_TAGS = (\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n    RUN_CLEAR_CACHES,\n\n    INPUT_SIZE,\n    INPUT_BATCH_SIZE,\n    INPUT_ORDER,\n    INPUT_SHARD,\n    INPUT_BN_SPAN,\n\n    BACKBONE,\n    FEATURE_SIZES,\n    STEPS,\n    SCALES,\n    ASPECT_RATIOS,\n    NUM_DEFAULTS_PER_CELL,\n    LOC_CONF_OUT_CHANNELS,\n    NUM_DEFAULTS,\n    NMS_THRESHOLD,\n    NMS_MAX_DETECTIONS,\n\n    NUM_CROPPING_ITERATIONS,\n    RANDOM_FLIP_PROBABILITY,\n    DATA_NORMALIZATION_MEAN,\n    DATA_NORMALIZATION_STD,\n\n    OPT_NAME,\n    OPT_LR,\n    OPT_MOMENTUM,\n    OPT_WEIGHT_DECAY,\n    OPT_LR_WARMUP_STEPS,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n\n    EVAL_START,\n    EVAL_SIZE,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n    EVAL_ITERATION_ACCURACY,\n)\n\nTRANSFORMER_TAGS = (\n    RUN_START,\n    RUN_STOP,\n    RUN_FINAL,\n    RUN_SET_RANDOM_SEED,\n    RUN_CLEAR_CACHES,\n\n    PREPROC_NUM_TRAIN_EXAMPLES,\n    PREPROC_NUM_EVAL_EXAMPLES,\n    PREPROC_TOKENIZE_TRAINING,\n    PREPROC_TOKENIZE_EVAL,\n    PREPROC_VOCAB_SIZE,\n\n    INPUT_BATCH_SIZE,\n    INPUT_MAX_LENGTH,\n    INPUT_ORDER,\n    INPUT_SHARD,\n\n    OPT_NAME,\n    OPT_LR,\n    OPT_LR_WARMUP_STEPS,\n    OPT_HP_ADAM_BETA1,\n    OPT_HP_ADAM_BETA2,\n    OPT_HP_ADAM_EPSILON,\n\n    TRAIN_LOOP,\n    TRAIN_EPOCH,\n\n    EVAL_START,\n    EVAL_TARGET,\n    EVAL_ACCURACY,\n    EVAL_STOP,\n\n    MODEL_HP_INITIALIZER_GAIN,\n    MODEL_HP_VOCAB_SIZE,\n    MODEL_HP_NUM_HIDDEN_LAYERS,\n    MODEL_HP_EMBEDDING_SHARED_WEIGHTS,\n    MODEL_HP_ATTENTION_DENSE,\n    MODEL_HP_ATTENTION_DROPOUT,\n    MODEL_HP_FFN_OUTPUT_DENSE,\n    MODEL_HP_FFN_FILTER_DENSE,\n    MODEL_HP_RELU_DROPOUT,\n    MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n    MODEL_HP_NORM,\n    MODEL_HP_SEQ_BEAM_SEARCH,\n)\n\nALL_USED_TAGS.update(GNMT_TAGS)\nALL_USED_TAGS.update(MASKRCNN_TAGS)\nALL_USED_TAGS.update(NCF_TAGS)\nALL_USED_TAGS.update(RESNET_TAGS)\nALL_USED_TAGS.update(SSD_TAGS)\nALL_USED_TAGS.update(TRANSFORMER_TAGS)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/test_tag_set.py,0,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Verification script to check model tag sets.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import defaultdict\nimport re\n\nfrom mlperf_compliance import _gnmt_tags\nfrom mlperf_compliance import _ncf_tags\nfrom mlperf_compliance import _resnet_tags\nfrom mlperf_compliance import _ssd_tags\nfrom mlperf_compliance import _transformer_tags\nfrom mlperf_compliance  import tags\n\n\n_MODEL_TAG_MODULES = [_gnmt_tags, _ncf_tags, _resnet_tags, _ssd_tags,\n                      _transformer_tags]\n\nTAG_PATTERN = re.compile(""^[A-Za-z0-9_]+$"")\n\n\ndef extract_tags(module):\n  output = []\n  for i in dir(module):\n    if i.startswith(""_"") or not isinstance(getattr(module, i), str):\n      continue\n    output.append(i)\n  return output\n\n\ndef check_collisions():\n  defining_modules = defaultdict(list)\n  for module in _MODEL_TAG_MODULES:\n    name = module.__name__\n    for tag in extract_tags(module):\n      defining_modules[tag].append(name)\n\n  duplicate_defs = {k: v for k, v in defining_modules.items() if len(v) > 1}\n  for key in duplicate_defs.keys():\n    print(""Variable {} defined multiple times"".format(key))\n\n\ndef check_format():\n  for tag in sorted(tags.ALL_USED_TAGS):\n    if not TAG_PATTERN.match(tag):\n      print(""Malformed tag: {}"".format(tag))\n\n\nif __name__ == ""__main__"":\n  check_collisions()\n  check_format()\n\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_compliance/tf_mlperf_log.py,18,"b'# Copyright 2018 MLBenchmark Group. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convenience function for extracting the values for logging calls.\n\nBecause TensorFlow generally defers computation of values to a session run call,\nit is impractical to log the values of tensors when they are defined. Instead,\nthe definition of a tensor is logged as normal using the log function in\nmlperf_log.py and a tf.print statement helper function can be used to report\nthe relevant values as they are computed.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport uuid\n\nimport tensorflow as tf\n\n\ndef log_deferred(op, log_id, every_n=1, first_n=None):\n  """"""Helper method inserting compliance logging ops.\n\n  Note: This helper is not guaranteed to be efficient, as it will insert ops\n        and control dependencies. If this proves to be a bottleneck, submitters\n        may wish to consider other methods such as extracting values from an\n        .events file.\n\n  Args:\n    op: A tf op to be printed.\n    log_id: a uuid provided by the logger in mlperf_log.py\n    every_n: If repeat is True, with what frequency should the input op be \'\n             logged. If repeat is False, this argument is ignored.\n    first_n: Only log this many values. This arg does not interact with every_n.\n             The first_n refers to the first n that would have been logged.\n  """"""\n\n  prefix = "":::MLPv0.5.0 [{}]"".format(log_id)\n  if not first_n is not None and first_n == 1:\n    return tf.compat.v1.Print(op, [tf.timestamp(), op], message=prefix, first_n=1)\n\n  counter = tf.Variable(tf.zeros(shape=(), dtype=tf.int32) - 1,\n                        aggregation=tf.VariableAggregation.MEAN)\n  increment = tf.compat.v1.assign_add(counter, 1, use_locking=True)\n  return tf.cond(\n      pred=tf.equal(tf.math.mod(increment, every_n), 0),\n      true_fn=lambda :tf.compat.v1.Print(op, [tf.timestamp(), op], message=prefix,\n                       first_n=first_n),\n      false_fn=lambda :op\n  )\n\n\ndef sum_metric(tensor, name):\n  sum_var = tf.compat.v1.Variable(\n    initial_value=tf.zeros(shape=(), dtype=tensor.dtype),\n    trainable=False,\n    collections=[\n      tf.compat.v1.GraphKeys.LOCAL_VARIABLES,\n      tf.compat.v1.GraphKeys.METRIC_VARIABLES,\n    ],\n    name=""{}_total"".format(name),\n    aggregation=tf.VariableAggregation.SUM\n  )\n\n  update_op = tf.identity(tf.compat.v1.assign_add(sum_var, tensor))\n  return tf.identity(sum_var, name=name), update_op\n\n\ndef _example():\n  for kwargs in [dict(first_n=1), dict(), dict(every_n=2),\n                 dict(first_n=2, every_n=2)]:\n    op = tf.compat.v1.assign_add(tf.Variable(tf.zeros(shape=(), dtype=tf.int32) - 1), 1)\n    op = log_deferred(op, str(uuid.uuid4()), **kwargs)\n    init = [tf.compat.v1.local_variables_initializer(), tf.compat.v1.global_variables_initializer()]\n    print(""-"" * 5)\n    with tf.compat.v1.Session().as_default() as sess:\n      sess.run(init)\n      for _ in range(6):\n        sess.run(op)\n\n\nif __name__ == ""__main__"":\n  _example()\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_resnet/__init__.py,0,b''
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_resnet/imagenet_main.py,16,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Runs a ResNet model on the ImageNet dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport random\n\nimport numpy.random\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom mlperf_compliance import mlperf_log\nfrom mlperf_resnet import imagenet_preprocessing\nfrom mlperf_resnet import resnet_model\nfrom mlperf_resnet import resnet_run_loop\n\n# import horovod if the above resnet_run_loop indiciates MPI\nif resnet_run_loop.is_mpi:\n  import horovod.tensorflow as hvd\n\n_DEFAULT_IMAGE_SIZE = 224\n_NUM_CHANNELS = 3\n_NUM_CLASSES = 1001\n\n_NUM_IMAGES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_NUM_TRAIN_FILES = 1024\n_SHUFFLE_BUFFER = 1500\n\n\n_BASE_LR = 0.128\n\n###############################################################################\n# Data processing\n###############################################################################\ndef get_filenames(is_training, data_dir):\n  """"""Return filenames for dataset.""""""\n  if is_training:\n    return [\n        os.path.join(data_dir, \'train-%05d-of-01024\' % i)\n        for i in range(_NUM_TRAIN_FILES)]\n  else:\n    return [\n        os.path.join(data_dir, \'validation-%05d-of-00128\' % i)\n        for i in range(128)]\n\n\ndef _parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields (values are included as examples):\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized, features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  return features[\'image/encoded\'], label\n\n\ndef parse_record(raw_record, is_training, dtype):\n  """"""Parses a record containing a training example of an image.\n\n  The input record is parsed into a label and image, and the image is passed\n  through preprocessing steps (cropping, flipping, and so on).\n\n  Args:\n    raw_record: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n    is_training: A boolean denoting whether the input is for training.\n    dtype: data type to use for images/features.\n\n  Returns:\n    Tuple with processed image tensor and one-hot-encoded label tensor.\n  """"""\n  image_buffer, label = _parse_example_proto(raw_record)\n\n  image = imagenet_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      output_height=_DEFAULT_IMAGE_SIZE,\n      output_width=_DEFAULT_IMAGE_SIZE,\n      num_channels=_NUM_CHANNELS,\n      is_training=is_training)\n  image = tf.cast(image, dtype)\n\n  return image, label\n\n\ndef input_fn(is_training, data_dir, batch_size, num_epochs=1, num_gpus=None,\n             dtype=tf.float32):\n  """"""Input function which provides batches for train or eval.\n\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: The directory containing the input data.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n    num_gpus: The number of gpus used for training.\n    dtype: Data type to use for images/features\n\n  Returns:\n    A dataset that can be used for iteration.\n  """"""\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_ORDER)\n  filenames = get_filenames(is_training, data_dir)\n  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n\n  if is_training:\n    # Shuffle the input files\n    dataset = dataset.shuffle(buffer_size=_NUM_TRAIN_FILES)\n\n  # Convert to individual records\n  dataset = dataset.flat_map(tf.data.TFRecordDataset)\n\n  return resnet_run_loop.process_record_dataset(\n      dataset=dataset,\n      is_training=is_training,\n      batch_size=batch_size,\n      shuffle_buffer=_SHUFFLE_BUFFER,\n      parse_record_fn=parse_record,\n      num_epochs=num_epochs,\n      num_gpus=num_gpus,\n      examples_per_epoch=_NUM_IMAGES[\'train\'] if is_training else None,\n      dtype=dtype\n  )\n\n\ndef get_synth_input_fn():\n  return resnet_run_loop.get_synth_input_fn(\n      _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, _NUM_CHANNELS, _NUM_CLASSES)\n\n\n###############################################################################\n# Running the model\n###############################################################################\nclass ImagenetModel(resnet_model.Model):\n  """"""Model class with appropriate defaults for Imagenet data.""""""\n\n  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES,\n               version=resnet_model.DEFAULT_VERSION,\n               dtype=resnet_model.DEFAULT_DTYPE):\n    """"""These are the parameters that work for Imagenet data.\n\n    Args:\n      resnet_size: The number of convolutional layers needed in the model.\n      data_format: Either \'channels_first\' or \'channels_last\', specifying which\n        data format to use when setting up the model.\n      num_classes: The number of output classes needed from the model. This\n        enables users to extend the same model to their own datasets.\n      version: Integer representing which version of the ResNet network to use.\n        See README for details. Valid values: [1, 2]\n      dtype: The TensorFlow dtype to use for calculations.\n    """"""\n\n    # For bigger models, we want to use ""bottleneck"" layers\n    if resnet_size < 50:\n      bottleneck = False\n      final_size = 512\n    else:\n      bottleneck = True\n      final_size = 2048\n\n    super(ImagenetModel, self).__init__(\n        resnet_size=resnet_size,\n        bottleneck=bottleneck,\n        num_classes=num_classes,\n        num_filters=64,\n        kernel_size=7,\n        conv_stride=2,\n        first_pool_size=3,\n        first_pool_stride=2,\n        second_pool_size=7,\n        second_pool_stride=1,\n        block_sizes=_get_block_sizes(resnet_size),\n        block_strides=[1, 2, 2, 2],\n        final_size=final_size,\n        version=version,\n        data_format=data_format,\n        dtype=dtype\n    )\n\n\ndef _get_block_sizes(resnet_size):\n  """"""Retrieve the size of each block_layer in the ResNet model.\n\n  The number of block layers used for the Resnet model varies according\n  to the size of the model. This helper grabs the layer set we want, throwing\n  an error if a non-standard size has been selected.\n\n  Args:\n    resnet_size: The number of convolutional layers needed in the model.\n\n  Returns:\n    A list of block sizes to use in building the model.\n\n  Raises:\n    KeyError: if invalid resnet_size is received.\n  """"""\n  choices = {\n      18: [2, 2, 2, 2],\n      34: [3, 4, 6, 3],\n      50: [3, 4, 6, 3],\n      101: [3, 4, 23, 3],\n      152: [3, 8, 36, 3],\n      200: [3, 24, 36, 3]\n  }\n\n  try:\n    return choices[resnet_size]\n  except KeyError:\n    err = (\'Could not find layers for selected Resnet size.\\n\'\n           \'Size received: {}; sizes allowed: {}.\'.format(\n               resnet_size, choices.keys()))\n    raise ValueError(err)\n\n\ndef imagenet_model_fn(features, labels, mode, params):\n  """"""Our model_fn for ResNet to be used with our Estimator.""""""\n\n  # Warmup and higher lr may not be valid for fine tuning with small batches\n  # and smaller numbers of training images.\n  if params[\'fine_tune\']:\n    base_lr = .1\n  else:\n    base_lr = .128\n\n  num_workers = 1 if resnet_run_loop.is_mpi == 0 else hvd.size()\n  global_batch_size = params[\'batch_size\'] * num_workers\n  learning_rate_fn = resnet_run_loop.learning_rate_with_decay(\n      batch_size=global_batch_size, batch_denom=256,\n      num_images=_NUM_IMAGES[\'train\'], boundary_epochs=[30, 60, 80, 90],\n      decay_rates=[1, 0.1, 0.01, 0.001, 1e-4], base_lr=_BASE_LR,\n      enable_lars=params[\'enable_lars\'])\n\n  return resnet_run_loop.resnet_model_fn(\n      features=features,\n      labels=labels,\n      mode=mode,\n      model_class=ImagenetModel,\n      resnet_size=params[\'resnet_size\'],\n      weight_decay=params[\'weight_decay\'],\n      learning_rate_fn=learning_rate_fn,\n      momentum=0.9,\n      data_format=params[\'data_format\'],\n      version=params[\'version\'],\n      loss_scale=params[\'loss_scale\'],\n      loss_filter_fn=None,\n      dtype=params[\'dtype\'],\n      label_smoothing=params[\'label_smoothing\'],\n      enable_lars=params[\'enable_lars\'],\n      use_bfloat16=params[\'use_bfloat16\']\n  )\n\n\ndef main(argv):\n  parser = resnet_run_loop.ResnetArgParser(\n      resnet_size_choices=[18, 34, 50, 101, 152, 200])\n\n  parser.set_defaults(\n       train_epochs=90,\n       version=1\n  )\n\n  flags = parser.parse_args(args=argv[2:])\n\n  seed = int(argv[1])\n  print(\'Setting random seed = \', seed)\n  print(\'special seeding\')\n  mlperf_log.resnet_print(key=mlperf_log.RUN_SET_RANDOM_SEED, value=seed)\n  random.seed(seed)\n  tf.compat.v1.set_random_seed(seed)\n  numpy.random.seed(seed)\n\n  mlperf_log.resnet_print(key=mlperf_log.PREPROC_NUM_TRAIN_EXAMPLES,\n                          value=_NUM_IMAGES[\'train\'])\n  mlperf_log.resnet_print(key=mlperf_log.PREPROC_NUM_EVAL_EXAMPLES,\n                          value=_NUM_IMAGES[\'validation\'])\n  input_function = flags.use_synthetic_data and get_synth_input_fn() or input_fn\n\n  resnet_run_loop.resnet_main(seed,\n      flags, imagenet_model_fn, input_function,\n      shape=[_DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, _NUM_CHANNELS])\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n  mlperf_log.ROOT_DIR_RESNET = os.path.split(os.path.abspath(__file__))[0]\n  main(argv=sys.argv)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_resnet/imagenet_preprocessing.py,22,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nTraining images are sampled using the provided bounding boxes, and subsequently\ncropped to the sampled bounding box. Images are additionally flipped randomly,\nthen resized to the target output size (without aspect-ratio preservation).\n\nImages used during evaluation are resized (with aspect-ratio preservation) and\ncentrally cropped.\n\nAll images undergo mean color subtraction.\n\nNote that these steps are colloquially referred to as ""ResNet preprocessing,""\nand they differ from ""VGG preprocessing,"" which does not use bounding boxes\nand instead does an aspect-preserving resize followed by random crop during\ntraining. (These both differ from ""Inception preprocessing,"" which introduces\ncolor distortion steps.)\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom mlperf_compliance import mlperf_log\n\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n_CHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\n\n\ndef _decode_crop_and_flip(image_buffer, num_channels):\n  """"""Crops the given image to a random part of the image, and randomly flips.\n\n  We use the fused decode_and_crop op, which performs better than the two ops\n  used separately in series, but note that this requires that the image be\n  passed in as an un-decoded string Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    num_channels: Integer depth of the image buffer for decoding.\n\n  Returns:\n    3-D tensor with cropped image.\n\n  """"""\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n\n  min_object_covered=0.1\n  aspect_ratio_range=[0.75, 1.33]\n  area_range=[0.05, 1.0]\n  max_attempts=100\n\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_MIN_OBJ_COV,\n                          value=min_object_covered)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_RATIO_RANGE,\n                          value=aspect_ratio_range)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_AREA_RANGE,\n                          value=area_range)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_MAX_ATTEMPTS,\n                          value=max_attempts)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_CROP_USES_BBOXES, value=False)\n\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                     dtype=tf.float32, shape=[1, 1, 4])   #From the entire image\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      image_size=tf.image.extract_jpeg_shape(image_buffer),\n      bounding_boxes=bbox,\n      min_object_covered=min_object_covered,\n      aspect_ratio_range=aspect_ratio_range,\n      area_range=area_range,\n      max_attempts=max_attempts,\n      use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n      image_buffer, crop_window, channels=num_channels)\n\n  # Flip to add a little more random distortion in.\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_RANDOM_FLIP)\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped\n\n\ndef _central_crop(image, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image: a 3-D image tensor\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  shape = tf.shape(input=image)\n  height, width = shape[0], shape[1]\n\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_CENTRAL_CROP,\n                          value=[crop_height, crop_width])\n\n  amount_to_be_cropped_h = (height - crop_height)\n  crop_top = amount_to_be_cropped_h // 2\n  amount_to_be_cropped_w = (width - crop_width)\n  crop_left = amount_to_be_cropped_w // 2\n  return tf.slice(\n      image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef _mean_image_subtraction(image, means, num_channels):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_MEAN_SUBTRACTION,\n                          value=means)\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  means = tf.expand_dims(tf.expand_dims(means, 0), 0)\n\n  return image - means\n\n\ndef _smallest_size_at_least(height, width, resize_min):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: an int32 scalar tensor indicating the new width.\n  """"""\n  resize_min = tf.cast(resize_min, tf.float32)\n\n  # Convert to floats to make subsequent calculations go smoothly.\n  height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32)\n\n  smaller_dim = tf.minimum(height, width)\n  scale_ratio = resize_min / smaller_dim\n\n  # Convert back to ints to make heights and widths that TF ops will accept.\n  new_height = tf.cast(height * scale_ratio, tf.int32)\n  new_width = tf.cast(width * scale_ratio, tf.int32)\n\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, resize_min):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_RESIZE_ASPECT_PRESERVING,\n                          value={""min"": resize_min})\n\n  shape = tf.shape(input=image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)\n\n\ndef _resize_image(image, height, width):\n  """"""Simple wrapper around tf.resize_images.\n\n  This is primarily to make sure we use the same `ResizeMethod` and other\n  details each time.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    height: The target height for the resized image.\n    width: The target width for the resized image.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image. The first two\n      dimensions have the shape [height, width].\n  """"""\n  return tf.image.resize(\n      image, [height, width], method=tf.image.ResizeMethod.BILINEAR)\n\n\ndef preprocess_image(image_buffer, output_height, output_width,\n                     num_channels, is_training=False):\n  """"""Preprocesses the given image.\n\n  Preprocessing includes decoding, cropping, and resizing for both training\n  and eval images. Training preprocessing, however, introduces some random\n  distortion of the image to improve accuracy.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    num_channels: Integer depth of the image buffer for decoding.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    # For training, we want to randomize some of the distortions.\n    image = _decode_crop_and_flip(image_buffer, num_channels)\n\n    mlperf_log.resnet_print(key=mlperf_log.INPUT_RESIZE,\n                            value=[output_height, output_width])\n    image = _resize_image(image, output_height, output_width)\n  else:\n    # For validation, we want to decode, resize, then just crop the middle.\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels)\n    image = _aspect_preserving_resize(image, _RESIZE_MIN)\n\n    mlperf_log.resnet_print(key=mlperf_log.INPUT_RESIZE,\n                            value=[output_height, output_width])\n    image = _central_crop(image, output_height, output_width)\n\n  image.set_shape([output_height, output_width, num_channels])\n\n  return _mean_image_subtraction(image, _CHANNEL_MEANS, num_channels)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_resnet/resnet_model.py,35,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for Residual Networks.\n\nResidual networks (\'v1\' ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant was introduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer\nrather than after.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom mlperf_compliance import mlperf_log\nfrom mlperf_compliance import resnet_log_helper\n\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-5\nDEFAULT_VERSION = 2\nDEFAULT_DTYPE = tf.float32\nCASTABLE_TYPES = (tf.float16,tf.bfloat16)\nALLOWED_TYPES = (DEFAULT_DTYPE,) + CASTABLE_TYPES\n\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format):\n  """"""Performs a batch normalization using a standard set of parameters.""""""\n  # We set fused=True for a significant performance boost. See\n  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n  outputs = tf.compat.v1.layers.batch_normalization(\n      inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n      momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n      scale=True, training=training, fused=True)\n\n  resnet_log_helper.log_batch_norm(\n      input_tensor=inputs, output_tensor=outputs, momentum=_BATCH_NORM_DECAY,\n      epsilon=_BATCH_NORM_EPSILON, center=True, scale=True, training=training)\n\n  return outputs\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                 Should be a positive integer.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    A tensor with the same format as the input with the data either intact\n    (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  pad_total = kernel_size - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n\n  if data_format == \'channels_first\':\n    padded_inputs = tf.pad(tensor=inputs, paddings=[[0, 0], [0, 0],\n                                    [pad_beg, pad_end], [pad_beg, pad_end]])\n  else:\n    padded_inputs = tf.pad(tensor=inputs, paddings=[[0, 0], [pad_beg, pad_end],\n                                    [pad_beg, pad_end], [0, 0]])\n  return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n  """"""Strided 2-D convolution with explicit padding.""""""\n  # The padding is consistent and is based only on `kernel_size`, not on the\n  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n\n  inputs_for_logging = inputs\n  if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format)\n\n  outputs = tf.compat.v1.layers.conv2d(\n      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n      padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n      kernel_initializer=tf.compat.v1.variance_scaling_initializer(\n          distribution=""truncated_normal""),\n      data_format=data_format)\n\n  resnet_log_helper.log_conv2d(\n      input_tensor=inputs_for_logging, output_tensor=outputs, stride=strides,\n      filters=filters, initializer=mlperf_log.TRUNCATED_NORMAL, use_bias=False)\n\n  return outputs\n\n\n################################################################################\n# ResNet block definitions.\n################################################################################\ndef _building_block_v1(inputs, filters, training, projection_shortcut, strides,\n                       data_format):\n  raise NotImplementedError\n\n\ndef _building_block_v2(inputs, filters, training, projection_shortcut, strides,\n                       data_format):\n  raise NotImplementedError\n\n\ndef _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n                         strides, data_format):\n  """"""A single block for ResNet v1, with a bottleneck.\n\n  Similar to _building_block_v1(), except using the ""bottleneck"" blocks\n  described in:\n    Convolution then batch normalization then ReLU as described by:\n      Deep Residual Learning for Image Recognition\n      https://arxiv.org/pdf/1512.03385.pdf\n      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference\n      mode. Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts\n      (typically a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n  resnet_log_helper.log_begin_block(\n      input_tensor=inputs, block_type=mlperf_log.BOTTLENECK_BLOCK)\n\n  shortcut = inputs\n\n  if projection_shortcut is not None:\n    shortcut = projection_shortcut(inputs)\n    resnet_log_helper.log_projection(input_tensor=inputs,\n                                     output_tensor=shortcut)\n    shortcut = batch_norm(inputs=shortcut, training=training,\n                          data_format=data_format)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n      data_format=data_format)\n  inputs = batch_norm(inputs, training, data_format)\n\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_RELU)\n  inputs = tf.nn.relu(inputs)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n      data_format=data_format)\n  inputs = batch_norm(inputs, training, data_format)\n\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_RELU)\n  inputs = tf.nn.relu(inputs)\n\n  inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n      data_format=data_format)\n  inputs = batch_norm(inputs, training, data_format)\n\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_SHORTCUT_ADD)\n  # TODO(nhasabni): temporarily replacing Add by AddN for performance.\n  # Remove it later once we optimize this in graph.\n  inputs = tf.math.add_n([inputs, shortcut])\n\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_RELU)\n  inputs = tf.nn.relu(inputs)\n\n  resnet_log_helper.log_end_block(output_tensor=inputs)\n  return inputs\n\n\ndef _bottleneck_block_v2(inputs, filters, training, projection_shortcut,\n                         strides, data_format):\n  raise NotImplementedError\n\n\ndef block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n                training, name, data_format):\n  """"""Creates one layer of blocks for the ResNet model.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the first convolution of the layer.\n    bottleneck: Is the block created a bottleneck block.\n    block_fn: The block to use within the model, either `building_block` or\n      `bottleneck_block`.\n    blocks: The number of blocks contained in the layer.\n    strides: The stride to use for the first convolution of the layer. If\n      greater than 1, this layer will ultimately downsample the input.\n    training: Either True or False, whether we are currently training the\n      model. Needed for batch norm.\n    name: A string name for the tensor output of the block layer.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block layer.\n  """"""\n\n  # Bottleneck blocks end with 4x the number of filters as they start with\n  filters_out = filters * 4 if bottleneck else filters\n\n  def projection_shortcut(inputs):\n    return conv2d_fixed_padding(\n        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n        data_format=data_format)\n\n  # Only the first block per block_layer uses projection_shortcut and strides\n  inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n                    data_format)\n\n  for _ in range(1, blocks):\n    inputs = block_fn(inputs, filters, training, None, 1, data_format)\n\n  return tf.identity(inputs, name)\n\n\nclass Model(object):\n  """"""Base class for building the Resnet Model.""""""\n\n  def __init__(self, resnet_size, bottleneck, num_classes, num_filters,\n               kernel_size,\n               conv_stride, first_pool_size, first_pool_stride,\n               second_pool_size, second_pool_stride, block_sizes, block_strides,\n               final_size, version=DEFAULT_VERSION, data_format=None,\n               dtype=DEFAULT_DTYPE):\n    """"""Creates a model for classifying an image.\n\n    Args:\n      resnet_size: A single integer for the size of the ResNet model.\n      bottleneck: Use regular blocks or bottleneck blocks.\n      num_classes: The number of classes used as labels.\n      num_filters: The number of filters to use for the first block layer\n        of the model. This number is then doubled for each subsequent block\n        layer.\n      kernel_size: The kernel size to use for convolution.\n      conv_stride: stride size for the initial convolutional layer\n      first_pool_size: Pool size to be used for the first pooling layer.\n        If none, the first pooling layer is skipped.\n      first_pool_stride: stride size for the first pooling layer. Not used\n        if first_pool_size is None.\n      second_pool_size: Pool size to be used for the second pooling layer.\n      second_pool_stride: stride size for the final pooling layer\n      block_sizes: A list containing n values, where n is the number of sets of\n        block layers desired. Each value should be the number of blocks in the\n        i-th set.\n      block_strides: List of integers representing the desired stride size for\n        each of the sets of block layers. Should be same length as block_sizes.\n      final_size: The expected size of the model after the second pooling.\n      version: Integer representing which version of the ResNet network to use.\n        See README for details. Valid values: [1, 2]\n      data_format: Input format (\'channels_last\', \'channels_first\', or None).\n        If set to None, the format is dependent on whether a GPU is available.\n      dtype: The TensorFlow dtype to use for calculations. If not specified\n        tf.float32 is used.\n\n    Raises:\n      ValueError: if invalid version is selected.\n    """"""\n    self.resnet_size = resnet_size\n\n    if not data_format:\n      data_format = (\n          \'channels_first\' if tf.test.is_built_with_cuda() else \'channels_last\')\n\n    self.resnet_version = version\n    if version not in (1, 2):\n      raise ValueError(\n          \'Resnet version should be 1 or 2. See README for citations.\')\n\n    self.bottleneck = bottleneck\n    if bottleneck:\n      if version == 1:\n        self.block_fn = _bottleneck_block_v1\n      else:\n        self.block_fn = _bottleneck_block_v2\n    else:\n      if version == 1:\n        self.block_fn = _building_block_v1\n      else:\n        self.block_fn = _building_block_v2\n\n    if dtype not in ALLOWED_TYPES:\n      raise ValueError(\'dtype must be one of: {}\'.format(ALLOWED_TYPES))\n\n    self.data_format = data_format\n    self.num_classes = num_classes\n    self.num_filters = num_filters\n    self.kernel_size = kernel_size\n    self.conv_stride = conv_stride\n    self.first_pool_size = first_pool_size\n    self.first_pool_stride = first_pool_stride\n    self.second_pool_size = second_pool_size\n    self.second_pool_stride = second_pool_stride\n    self.block_sizes = block_sizes\n    self.block_strides = block_strides\n    self.final_size = final_size\n    self.dtype = dtype\n    self.pre_activation = version == 2\n\n  def _custom_dtype_getter(self, getter, name, shape=None, dtype=DEFAULT_DTYPE,\n                           *args, **kwargs):\n    """"""Creates variables in fp32, then casts to fp16 if necessary.\n\n    This function is a custom getter. A custom getter is a function with the\n    same signature as tf.get_variable, except it has an additional getter\n    parameter. Custom getters can be passed as the `custom_getter` parameter of\n    tf.variable_scope. Then, tf.get_variable will call the custom getter,\n    instead of directly getting a variable itself. This can be used to change\n    the types of variables that are retrieved with tf.get_variable.\n    The `getter` parameter is the underlying variable getter, that would have\n    been called if no custom getter was used. Custom getters typically get a\n    variable with `getter`, then modify it in some way.\n\n    This custom getter will create an fp32 variable. If a low precision\n    (e.g. float16) variable was requested it will then cast the variable to the\n    requested dtype. The reason we do not directly create variables in low\n    precision dtypes is that applying small gradients to such variables may\n    cause the variable not to change.\n\n    Args:\n      getter: The underlying variable getter, that has the same signature as\n        tf.get_variable and returns a variable.\n      name: The name of the variable to get.\n      shape: The shape of the variable to get.\n      dtype: The dtype of the variable to get. Note that if this is a low\n        precision dtype, the variable will be created as a tf.float32 variable,\n        then cast to the appropriate dtype\n      *args: Additional arguments to pass unmodified to getter.\n      **kwargs: Additional keyword arguments to pass unmodified to getter.\n\n    Returns:\n      A variable which is cast to fp16 if necessary.\n    """"""\n\n    if dtype in CASTABLE_TYPES:\n      var = getter(name, shape, tf.float32, *args, **kwargs)\n      return tf.cast(var, dtype=dtype, name=name + \'_cast\')\n    else:\n      return getter(name, shape, dtype, *args, **kwargs)\n\n  def _model_variable_scope(self):\n    """"""Returns a variable scope that the model should be created under.\n\n    If self.dtype is a castable type, model variable will be created in fp32\n    then cast to self.dtype before being used.\n\n    Returns:\n      A variable scope for the model.\n    """"""\n\n    return tf.compat.v1.variable_scope(\'resnet_model\',\n                             custom_getter=self._custom_dtype_getter)\n\n  def __call__(self, inputs, training):\n    """"""Add operations to classify a batch of input images.\n\n    Args:\n      inputs: A Tensor representing a batch of input images.\n      training: A boolean. Set to True to add operations required only when\n        training the classifier.\n\n    Returns:\n      A logits Tensor with shape [<batch_size>, self.num_classes].\n    """"""\n\n    # Drop batch size from shape logging.\n    mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_INITIAL_SHAPE,\n                            value=inputs.shape.as_list()[1:])\n\n    with self._model_variable_scope():\n      if self.data_format == \'channels_first\':\n        # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n        # This provides a large performance boost on GPU. See\n        # https://www.tensorflow.org/performance/performance_guide#data_formats\n        inputs = tf.transpose(a=inputs, perm=[0, 3, 1, 2])\n\n      inputs = conv2d_fixed_padding(\n          inputs=inputs, filters=self.num_filters, kernel_size=self.kernel_size,\n          strides=self.conv_stride, data_format=self.data_format)\n      inputs = tf.identity(inputs, \'initial_conv\')\n\n      # We do not include batch normalization or activation functions in V2\n      # for the initial conv1 because the first ResNet unit will perform these\n      # for both the shortcut and non-shortcut paths as part of the first\n      # block\'s projection. Cf. Appendix of [2].\n      if self.resnet_version == 1:\n        inputs = batch_norm(inputs, training, self.data_format)\n\n        mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_RELU)\n        inputs = tf.nn.relu(inputs)\n\n      if self.first_pool_size:\n        pooled_inputs = tf.compat.v1.layers.max_pooling2d(\n            inputs=inputs, pool_size=self.first_pool_size,\n            strides=self.first_pool_stride, padding=\'SAME\',\n            data_format=self.data_format)\n        resnet_log_helper.log_max_pool(input_tensor=inputs, output_tensor=pooled_inputs)\n        inputs = tf.identity(pooled_inputs, \'initial_max_pool\')\n\n      for i, num_blocks in enumerate(self.block_sizes):\n        num_filters = self.num_filters * (2**i)\n        inputs = block_layer(\n            inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,\n            block_fn=self.block_fn, blocks=num_blocks,\n            strides=self.block_strides[i], training=training,\n            name=\'block_layer{}\'.format(i + 1), data_format=self.data_format)\n\n      # Only apply the BN and ReLU for model that does pre_activation in each\n      # building/bottleneck block, eg resnet V2.\n      if self.pre_activation:\n        inputs = batch_norm(inputs, training, self.data_format)\n\n        mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_RELU)\n        inputs = tf.nn.relu(inputs)\n\n      # The current top layer has shape\n      # `batch_size x pool_size x pool_size x final_size`.\n      # ResNet does an Average Pooling layer over pool_size,\n      # but that is the same as doing a reduce_mean. We do a reduce_mean\n      # here because it performs better than AveragePooling2D.\n      axes = [2, 3] if self.data_format == \'channels_first\' else [1, 2]\n      inputs = tf.reduce_mean(input_tensor=inputs, axis=axes, keepdims=True)\n      inputs = tf.identity(inputs, \'final_reduce_mean\')\n\n      inputs = tf.reshape(inputs, [-1, self.final_size])\n      mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_DENSE,\n                              value=self.num_classes)\n      inputs = tf.compat.v1.layers.dense(\n        inputs=inputs,\n        units=self.num_classes,\n        kernel_initializer=tf.compat.v1.random_normal_initializer(stddev=.01))\n      inputs = tf.identity(inputs, \'final_dense\')\n\n      # Drop batch size from shape logging.\n      mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_FINAL_SHAPE,\n                              value=inputs.shape.as_list()[1:])\n      return inputs\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_resnet/resnet_run_loop.py,59,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utility and supporting functions for ResNet.\n\n  This module contains ResNet code which does not directly build layers. This\nincludes dataset management, hyperparameter and optimizer code, and argument\nparsing. Code for defining the ResNet layers can be found in resnet_model.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom mlperf_compliance import mlperf_log\nfrom mlperf_compliance import tf_mlperf_log\nfrom mlperf_resnet import resnet_model\nfrom mlperf_utils.arg_parsers import parsers\nfrom mlperf_utils.export import export\nfrom mlperf_utils.logs import hooks_helper\nfrom mlperf_utils.logs import logger\nfrom mlperf_utils.misc import model_helpers\n\nglobal is_mpi\ntry:\n    import horovod.tensorflow as hvd\n    hvd.init()\n    is_mpi = hvd.size()\nexcept ImportError:\n    is_mpi = 0\n    print(""No MPI horovod support, this is running in no-MPI mode!"")\n\n\n_NUM_EXAMPLES_NAME = ""num_examples""\n_NUM_IMAGES = {\n        \'train\': 1281167,\n        \'validation\': 50000\n}\n\n\n################################################################################\n# Functions for input processing.\n################################################################################\ndef process_record_dataset(dataset, is_training, batch_size, shuffle_buffer,\n                           parse_record_fn, num_epochs=1, num_gpus=None,\n                           examples_per_epoch=None, dtype=tf.float32):\n  """"""Given a Dataset with raw records, return an iterator over the records.\n\n  Args:\n    dataset: A Dataset representing raw records\n    is_training: A boolean denoting whether the input is for training.\n    batch_size: The number of samples per batch.\n    shuffle_buffer: The buffer size to use when shuffling records. A larger\n      value results in better randomness, but smaller values reduce startup\n      time and use less memory.\n    parse_record_fn: A function that takes a raw record and returns the\n      corresponding (image, label) pair.\n    num_epochs: The number of epochs to repeat the dataset.\n    num_gpus: The number of gpus used for training.\n    examples_per_epoch: The number of examples in an epoch.\n    dtype: Data type to use for images/features.\n\n  Returns:\n    Dataset of (image, label) pairs ready for iteration.\n  """"""\n\n  # We prefetch a batch at a time, This can help smooth out the time taken to\n  # load input files as we go through shuffling and processing.\n  dataset = dataset.prefetch(buffer_size=batch_size)\n  if is_training:\n    if is_mpi:\n      dataset = dataset.shard(hvd.size(), hvd.rank())\n    # Shuffle the records. Note that we shuffle before repeating to ensure\n    # that the shuffling respects epoch boundaries.\n    mlperf_log.resnet_print(key=mlperf_log.INPUT_ORDER)\n    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n\n  # If we are training over multiple epochs before evaluating, repeat the\n  # dataset for the appropriate number of epochs.\n  dataset = dataset.repeat(num_epochs)\n\n  # Parse the raw records into images and labels. Testing has shown that setting\n  # num_parallel_batches > 1 produces no improvement in throughput, since\n  # batch_size is almost always much greater than the number of CPU cores.\n  dataset = dataset.apply(\n      tf.data.experimental.map_and_batch(\n          lambda value: parse_record_fn(value, is_training, dtype),\n          batch_size=batch_size,\n          num_parallel_batches=1))\n\n  # Operations between the final prefetch and the get_next call to the iterator\n  # will happen synchronously during run time. We prefetch here again to\n  # background all of the above processing work and keep it out of the\n  # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE\n  # allows DistributionStrategies to adjust how many batches to fetch based\n  # on how many devices are present.\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n  return dataset\n\n\ndef get_synth_input_fn(height, width, num_channels, num_classes):\n  """"""Returns an input function that returns a dataset with zeroes.\n\n  This is useful in debugging input pipeline performance, as it removes all\n  elements of file reading and image preprocessing.\n\n  Args:\n    height: Integer height that will be used to create a fake image tensor.\n    width: Integer width that will be used to create a fake image tensor.\n    num_channels: Integer depth that will be used to create a fake image tensor.\n    num_classes: Number of classes that should be represented in the fake labels\n      tensor\n\n  Returns:\n    An input_fn that can be used in place of a real one to return a dataset\n    that can be used for iteration.\n  """"""\n  def input_fn(is_training, data_dir, batch_size, *args, **kwargs):  # pylint: disable=unused-argument\n    images = tf.zeros((batch_size, height, width, num_channels), tf.float32)\n    labels = tf.zeros((batch_size, num_classes), tf.int32)\n    return tf.data.Dataset.from_tensors((images, labels)).repeat()\n\n  return input_fn\n\n\n################################################################################\n# Functions for running training/eval/validation loops for the model.\n################################################################################\ndef learning_rate_with_decay(\n    batch_size, batch_denom, num_images, boundary_epochs, decay_rates,\n    base_lr=0.1, enable_lars=False):\n  """"""Get a learning rate that decays step-wise as training progresses.\n\n  Args:\n    batch_size: the number of examples processed in each training batch.\n    batch_denom: this value will be used to scale the base learning rate.\n      `0.1 * batch size` is divided by this number, such that when\n      batch_denom == batch_size, the initial learning rate will be 0.1.\n    num_images: total number of images that will be used for training.\n    boundary_epochs: list of ints representing the epochs at which we\n      decay the learning rate.\n    decay_rates: list of floats representing the decay rates to be used\n      for scaling the learning rate. It should have one more element\n      than `boundary_epochs`, and all elements should have the same type.\n    base_lr: Initial learning rate scaled based on batch_denom.\n\n  Returns:\n    Returns a function that takes a single argument - the number of batches\n    trained so far (global_step)- and returns the learning rate to be used\n    for training the next batch.\n  """"""\n  initial_learning_rate = base_lr * batch_size / batch_denom\n  batches_per_epoch = num_images / batch_size\n\n  # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n  boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n  vals = [initial_learning_rate * decay for decay in decay_rates]\n\n  def learning_rate_fn(global_step):\n    lr = tf.compat.v1.train.piecewise_constant(global_step, boundaries, vals)\n    warmup_steps = int(batches_per_epoch * 5)\n    warmup_lr = (\n        initial_learning_rate * tf.cast(global_step, tf.float32) / tf.cast(\n        warmup_steps, tf.float32))\n    return tf.cond(pred=global_step < warmup_steps, true_fn=lambda: warmup_lr, false_fn=lambda: lr)\n\n  def poly_rate_fn(global_step):\n    """"""Handles linear scaling rule, gradual warmup, and LR decay.\n\n    The learning rate starts at 0, then it increases linearly per step.  After\n    flags.poly_warmup_epochs, we reach the base learning rate (scaled to account\n    for batch size). The learning rate is then decayed using a polynomial rate\n    decay schedule with power 2.0.\n\n    Args:\n    global_step: the current global_step\n\n    Returns:\n    returns the current learning rate\n    """"""\n\n    # Learning rate schedule for LARS polynomial schedule\n    if batch_size <= 4096:\n      plr = 5.0\n      w_epochs = 5\n    elif batch_size <= 8192:\n      plr = 10.0\n      w_epochs = 5\n    elif batch_size <= 16384:\n      plr = 25.0\n      w_epochs = 5\n    else: # e.g. 32768\n      plr = 33.0\n      w_epochs = 25\n\n    w_steps = int(w_epochs * batches_per_epoch)\n    wrate = (plr * tf.cast(global_step, tf.float32) / tf.cast(\n        w_steps, tf.float32))\n\n    num_epochs = flags.train_epochs\n    train_steps = batches_per_epoch * num_epochs\n\n    min_step = tf.constant(1, dtype=tf.int64)\n    decay_steps = tf.maximum(min_step, tf.subtract(global_step, w_steps))\n    poly_rate = tf.compat.v1.train.polynomial_decay(\n        plr,\n        decay_steps,\n        train_steps - w_steps + 1,\n        power=2.0)\n    return tf.compat.v1.where(global_step <= w_steps, wrate, poly_rate)\n\n  # For LARS we have a new learning rate schedule\n  if enable_lars:\n    return poly_rate_fn\n\n  return learning_rate_fn\n\n\ndef resnet_model_fn(features, labels, mode, model_class,\n                    resnet_size, weight_decay, learning_rate_fn, momentum,\n                    data_format, version, loss_scale, loss_filter_fn=None,\n                    dtype=resnet_model.DEFAULT_DTYPE,\n                    label_smoothing=0.0, enable_lars=False,\n                    use_bfloat16=False):\n  """"""Shared functionality for different resnet model_fns.\n\n  Initializes the ResnetModel representing the model layers\n  and uses that model to build the necessary EstimatorSpecs for\n  the `mode` in question. For training, this means building losses,\n  the optimizer, and the train op that get passed into the EstimatorSpec.\n  For evaluation and prediction, the EstimatorSpec is returned without\n  a train op, but with the necessary parameters for the given mode.\n\n  Args:\n    features: tensor representing input images\n    labels: tensor representing class labels for all input images\n    mode: current estimator mode; should be one of\n      `tf.estimator.ModeKeys.TRAIN`, `EVALUATE`, `PREDICT`\n    model_class: a class representing a TensorFlow model that has a __call__\n      function. We assume here that this is a subclass of ResnetModel.\n    resnet_size: A single integer for the size of the ResNet model.\n    weight_decay: weight decay loss rate used to regularize learned variables.\n    learning_rate_fn: function that returns the current learning rate given\n      the current global_step\n    momentum: momentum term used for optimization\n    data_format: Input format (\'channels_last\', \'channels_first\', or None).\n      If set to None, the format is dependent on whether a GPU is available.\n    version: Integer representing which version of the ResNet network to use.\n      See README for details. Valid values: [1, 2]\n    loss_scale: The factor to scale the loss for numerical stability. A detailed\n      summary is present in the arg parser help text.\n    loss_filter_fn: function that takes a string variable name and returns\n      True if the var should be included in loss calculation, and False\n      otherwise. If None, batch_normalization variables will be excluded\n      from the loss.\n    dtype: the TensorFlow dtype to use for calculations.\n    use_bfloat16: Whether to use bfloat16 type for calculations.\n\n  Returns:\n    EstimatorSpec parameterized according to the input params and the\n    current mode.\n  """"""\n\n  # Generate a summary node for the images\n  tf.compat.v1.summary.image(\'images\', features, max_outputs=6)\n\n  # Checks that features/images have same data type being used for calculations.\n  assert features.dtype == dtype\n\n  if use_bfloat16 == True:\n    dtype = tf.bfloat16\n\n  features = tf.cast(features, dtype)\n\n  model = model_class(resnet_size, data_format, version=version, dtype=dtype)\n\n  logits = model(features, mode == tf.estimator.ModeKeys.TRAIN)\n\n  # This acts as a no-op if the logits are already in fp32 (provided logits are\n  # not a SparseTensor). If dtype is is low precision, logits must be cast to\n  # fp32 for numerical stability.\n  logits = tf.cast(logits, tf.float32)\n\n  num_examples_metric = tf_mlperf_log.sum_metric(tensor=tf.shape(input=logits)[0], name=_NUM_EXAMPLES_NAME)\n\n  predictions = {\n      \'classes\': tf.argmax(input=logits, axis=1),\n      \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\')\n  }\n\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    # Return the predictions and the specification for serving a SavedModel\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        export_outputs={\n            \'predict\': tf.estimator.export.PredictOutput(predictions)\n        })\n\n  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_HP_LOSS_FN, value=mlperf_log.CCE)\n\n  if label_smoothing != 0.0:\n    one_hot_labels = tf.one_hot(labels, 1001)\n    cross_entropy = tf.compat.v1.losses.softmax_cross_entropy(\n        logits=logits, onehot_labels=one_hot_labels,\n        label_smoothing=label_smoothing)\n  else:\n    cross_entropy = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n        logits=logits, labels=labels)\n\n  # Create a tensor named cross_entropy for logging purposes.\n  tf.identity(cross_entropy, name=\'cross_entropy\')\n  tf.compat.v1.summary.scalar(\'cross_entropy\', cross_entropy)\n\n  # If no loss_filter_fn is passed, assume we want the default behavior,\n  # which is that batch_normalization variables are excluded from loss.\n  def exclude_batch_norm(name):\n    return \'batch_normalization\' not in name\n  loss_filter_fn = loss_filter_fn or exclude_batch_norm\n\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_EXCLUDE_BN_FROM_L2,\n                          value=not loss_filter_fn(\'batch_normalization\'))\n\n  # Add weight decay to the loss.\n  mlperf_log.resnet_print(key=mlperf_log.MODEL_L2_REGULARIZATION,\n                          value=weight_decay)\n  l2_loss = weight_decay * tf.add_n(\n      # loss is computed using fp32 for numerical stability.\n      [tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.compat.v1.trainable_variables()\n       if loss_filter_fn(v.name)])\n  tf.compat.v1.summary.scalar(\'l2_loss\', l2_loss)\n  loss = cross_entropy + l2_loss\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n\n    learning_rate = learning_rate_fn(global_step)\n\n    log_id = mlperf_log.resnet_print(key=mlperf_log.OPT_LR, deferred=True)\n    learning_rate = tf_mlperf_log.log_deferred(op=learning_rate, log_id=log_id,\n                                               every_n=100)\n\n    # Create a tensor named learning_rate for logging purposes\n    tf.identity(learning_rate, name=\'learning_rate\')\n    tf.compat.v1.summary.scalar(\'learning_rate\', learning_rate)\n\n    mlperf_log.resnet_print(key=mlperf_log.OPT_NAME,\n                            value=mlperf_log.SGD_WITH_MOMENTUM)\n    mlperf_log.resnet_print(key=mlperf_log.OPT_MOMENTUM, value=momentum)\n\n    if enable_lars:\n      optimizer = tf.contrib.opt.LARSOptimizer(\n          learning_rate,\n          momentum=momentum,\n          weight_decay=weight_decay,\n          skip_list=[\'batch_normalization\', \'bias\'])\n    else:\n      optimizer = tf.compat.v1.train.MomentumOptimizer(\n          learning_rate=learning_rate,\n          momentum=momentum\n      )\n    if is_mpi:\n      optimizer = hvd.DistributedOptimizer(optimizer)\n\n    if loss_scale != 1:\n      # When computing fp16 gradients, often intermediate tensor values are\n      # so small, they underflow to 0. To avoid this, we multiply the loss by\n      # loss_scale to make these tensor values loss_scale times bigger.\n      scaled_grad_vars = optimizer.compute_gradients(loss * loss_scale)\n\n      # Once the gradient computation is complete we can scale the gradients\n      # back to the correct scale before passing them to the optimizer.\n      unscaled_grad_vars = [(grad / loss_scale, var)\n                            for grad, var in scaled_grad_vars]\n      minimize_op = optimizer.apply_gradients(unscaled_grad_vars, global_step)\n    else:\n      minimize_op = optimizer.minimize(loss, global_step)\n\n    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n    train_op = tf.group(minimize_op, update_ops, num_examples_metric[1])\n  else:\n    train_op = None\n\n  accuracy = tf.compat.v1.metrics.accuracy(labels, predictions[\'classes\'])\n  accuracy_top_5 = tf.compat.v1.metrics.mean(tf.nn.in_top_k(predictions=logits,\n                                                  targets=labels,\n                                                  k=5,\n                                                  name=\'top_5_op\'))\n\n  metrics = {\'accuracy\': accuracy,\n             \'accuracy_top_5\': accuracy_top_5,\n             _NUM_EXAMPLES_NAME: num_examples_metric}\n\n  # Create a tensor named train_accuracy for logging purposes\n  tf.identity(accuracy[1], name=\'train_accuracy\')\n  tf.identity(accuracy_top_5[1], name=\'train_accuracy_top_5\')\n  tf.compat.v1.summary.scalar(\'train_accuracy\', accuracy[1])\n  tf.compat.v1.summary.scalar(\'train_accuracy_top_5\', accuracy_top_5[1])\n\n  return tf.estimator.EstimatorSpec(\n      mode=mode,\n      predictions=predictions,\n      loss=loss,\n      train_op=train_op,\n      eval_metric_ops=metrics)\n\n\ndef per_device_batch_size(batch_size, num_gpus):\n  """"""For multi-gpu, batch-size must be a multiple of the number of GPUs.\n\n  Note that this should eventually be handled by DistributionStrategies\n  directly. Multi-GPU support is currently experimental, however,\n  so doing the work here until that feature is in place.\n\n  Args:\n    batch_size: Global batch size to be divided among devices. This should be\n      equal to num_gpus times the single-GPU batch_size for multi-gpu training.\n    num_gpus: How many GPUs are used with DistributionStrategies.\n\n  Returns:\n    Batch size per device.\n\n  Raises:\n    ValueError: if batch_size is not divisible by number of devices\n  """"""\n  if num_gpus <= 1:\n    return batch_size\n\n  remainder = batch_size % num_gpus\n  if remainder:\n    err = (\'When running with multiple GPUs, batch size \'\n           \'must be a multiple of the number of available GPUs. Found {} \'\n           \'GPUs with a batch size of {}; try --batch_size={} instead.\'\n          ).format(num_gpus, batch_size, batch_size - remainder)\n    raise ValueError(err)\n  return int(batch_size / num_gpus)\n\n\ndef resnet_main(seed, flags, model_function, input_function, shape=None):\n  """"""Shared main loop for ResNet Models.\n\n  Args:\n    flags: FLAGS object that contains the params for running. See\n      ResnetArgParser for created flags.\n    model_function: the function that instantiates the Model and builds the\n      ops for train/eval. This will be passed directly into the estimator.\n    input_function: the function that processes the dataset and returns a\n      dataset that the estimator can train on. This will be wrapped with\n      all the relevant flags for running and passed to estimator.\n    shape: list of ints representing the shape of the images used for training.\n      This is only used if flags.export_dir is passed.\n  """"""\n\n  mlperf_log.resnet_print(key=mlperf_log.RUN_START)\n\n  # Using the Winograd non-fused algorithms provides a small performance boost.\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  # Create session config based on values of inter_op_parallelism_threads and\n  # intra_op_parallelism_threads. Note that we default to having\n  # allow_soft_placement = True, which is required for multi-GPU and not\n  # harmful for other modes.\n  session_config = tf.compat.v1.ConfigProto(\n      inter_op_parallelism_threads=flags.inter_op_parallelism_threads,\n      intra_op_parallelism_threads=flags.intra_op_parallelism_threads,\n      allow_soft_placement=True)\n\n  if flags.num_gpus == 0:\n    distribution = tf.distribute.OneDeviceStrategy(\'device:CPU:0\')\n  elif flags.num_gpus == 1:\n    distribution = tf.distribute.OneDeviceStrategy(\'device:GPU:0\')\n  else:\n    distribution = tf.distribute.MirroredStrategy(\n        num_gpus=flags.num_gpus\n    )\n\n  mlperf_log.resnet_print(key=mlperf_log.RUN_SET_RANDOM_SEED, value=seed)\n  run_config = tf.estimator.RunConfig(train_distribute=distribution,\n                                      session_config=session_config,\n                                      log_step_count_steps=10, # output logs more frequently\n                                      tf_random_seed=seed)\n\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_BATCH_SIZE,\n                          value=flags.batch_size)\n\n  if is_mpi:\n      if hvd.rank() == 0:\n          model_dir = os.path.join(flags.model_dir,""main"")\n      else:\n          model_dir = os.path.join(flags.model_dir,""tmp{}"".format(hvd.rank()))\n      benchmark_log_dir = flags.benchmark_log_dir if hvd.rank() == 0 else None\n  else:\n      model_dir = flags.model_dir\n      benchmark_log_dir = flags.benchmark_log_dir\n\n  classifier = tf.estimator.Estimator(\n      model_fn=model_function, model_dir=model_dir, config=run_config,\n      params={\n          \'resnet_size\': flags.resnet_size,\n          \'data_format\': flags.data_format,\n          \'batch_size\': flags.batch_size,\n          \'version\': flags.version,\n          \'loss_scale\': flags.loss_scale,\n          \'dtype\': flags.dtype,\n          \'label_smoothing\': flags.label_smoothing,\n          \'enable_lars\': flags.enable_lars,\n          \'weight_decay\': flags.weight_decay,\n          \'fine_tune\': flags.fine_tune,\n          \'use_bfloat16\': flags.use_bfloat16\n      })\n\n  if benchmark_log_dir is not None:\n    benchmark_logger = logger.BenchmarkLogger(benchmark_log_dir)\n    benchmark_logger.log_run_info(\'resnet\')\n  else:\n    benchmark_logger = None\n\n  mlperf_log.resnet_print(key=mlperf_log.TRAIN_LOOP)\n\n  # for MPI only to figure out the steps per epoch or per eval, per worker \n  if is_mpi:\n    num_eval_steps = _NUM_IMAGES[\'validation\'] // flags.batch_size\n    steps_per_epoch = _NUM_IMAGES[\'train\'] // flags.batch_size\n    steps_per_epoch_per_worker = steps_per_epoch // hvd.size()\n    steps_per_eval_per_worker = steps_per_epoch_per_worker * flags.epochs_between_evals\n\n  # The reference performs the first evaluation on the fourth epoch. (offset\n  # eval by 3 epochs)\n  mlperf_log.resnet_print(key=mlperf_log.EVAL_EPOCH_OFFSET, value=3)\n  success = False\n  for i in range(flags.train_epochs // flags.epochs_between_evals):\n    # Data for epochs_between_evals (i.e. 4 epochs between evals) worth of\n    # epochs is concatenated and run as a single block inside a session. For\n    # this reason we declare all of the epochs that will be run at the start.\n    # Submitters may report in a way which is reasonable for their control flow.\n    for j in range(flags.epochs_between_evals):\n      mlperf_log.resnet_print(key=mlperf_log.TRAIN_EPOCH,\n                              value=i * flags.epochs_between_evals + j)\n\n    flags.hooks += [""examplespersecondhook""]\n    if is_mpi:\n      train_hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n      train_hooks = train_hooks + hooks_helper.get_train_hooks(\n          flags.hooks,\n          batch_size=flags.batch_size*hvd.size(),\n          benchmark_log_dir=flags.benchmark_log_dir)\n    else:\n      train_hooks = hooks_helper.get_train_hooks(\n          flags.hooks,\n          batch_size=flags.batch_size,\n          benchmark_log_dir=flags.benchmark_log_dir)\n\n    _log_cache = []\n    def formatter(x):\n      """"""Abuse side effects to get tensors out of the model_fn.""""""\n      if _log_cache:\n        _log_cache.pop()\n      _log_cache.append(x.copy())\n      return str(x)\n\n    compliance_hook = tf.estimator.LoggingTensorHook(\n      tensors={_NUM_EXAMPLES_NAME: _NUM_EXAMPLES_NAME},\n      every_n_iter=int(1e10),\n      at_end=True,\n      formatter=formatter)\n\n    print(\'Starting a training cycle.\')\n\n    def input_fn_train():\n      return input_function(\n          is_training=True,\n          data_dir=flags.data_dir,\n          batch_size=per_device_batch_size(flags.batch_size, flags.num_gpus),\n          num_epochs=flags.epochs_between_evals,\n          num_gpus=flags.num_gpus,\n          dtype=flags.dtype\n      )\n    if is_mpi:\n      # if max step is set, use max_step, not the steps_per_eval_per_worker\n      # assuming max_train_steps is smaller than steps_per_eval_per_worker\n      # Also assuming when -- steps is specified, the train epochs should\n      # be set to be equal to epochs_between_evals so that the\n      # range(flags.train_epochs // flags.epochs_between_evals) gets to be 1\n      if flags.max_train_steps < steps_per_eval_per_worker:\n          train_steps = flags.max_train_steps\n      else:\n          train_steps = steps_per_eval_per_worker\n\n      classifier.train(input_fn=input_fn_train, hooks=train_hooks + [compliance_hook],\n              steps=train_steps)\n    else:\n      classifier.train(input_fn=input_fn_train, hooks=train_hooks + [compliance_hook], max_steps=flags.max_train_steps)\n\n    #train_examples = int(_log_cache.pop()[_NUM_EXAMPLES_NAME])\n    #mlperf_log.resnet_print(key=mlperf_log.INPUT_SIZE, value=train_examples)\n\n    print(\'Starting to evaluate.\')\n    # Evaluate the model and print results\n    def input_fn_eval():\n      return input_function(\n          is_training=False,\n          data_dir=flags.data_dir,\n          batch_size=per_device_batch_size(flags.batch_size, flags.num_gpus),\n          num_epochs=1,\n          dtype=flags.dtype\n      )\n\n\n    mlperf_log.resnet_print(key=mlperf_log.EVAL_START)\n    # flags.max_train_steps is generally associated with testing and profiling.\n    # As a result it is frequently called with synthetic data, which will\n    # iterate forever. Passing steps=flags.max_train_steps allows the eval\n    # (which is generally unimportant in those circumstances) to terminate.\n    # Note that eval will run for max_train_steps each loop, regardless of the\n    # global_step count.\n    eval_results = classifier.evaluate(input_fn=input_fn_eval,\n                                       steps=flags.max_train_steps)\n    mlperf_log.resnet_print(key=mlperf_log.EVAL_STOP)\n    mlperf_log.resnet_print(key=mlperf_log.EVAL_SIZE, value=int(eval_results[_NUM_EXAMPLES_NAME]))\n    mlperf_log.resnet_print(key=mlperf_log.EVAL_ACCURACY, value=float(eval_results[\'accuracy\']))\n    mlperf_log.resnet_print(key=mlperf_log.EVAL_TARGET, value=flags.stop_threshold)\n    print(eval_results)\n\n    if benchmark_logger:\n      benchmark_logger.log_estimator_evaluation_result(eval_results)\n\n    if model_helpers.past_stop_threshold(\n        flags.stop_threshold, eval_results[\'accuracy\']):\n      success = True\n      break\n\n  mlperf_log.resnet_print(key=mlperf_log.RUN_STOP, value={""success"": success})\n  mlperf_log.resnet_print(key=mlperf_log.RUN_FINAL)\n\n\nclass ResnetArgParser(argparse.ArgumentParser):\n  """"""Arguments for configuring and running a Resnet Model.""""""\n\n  def __init__(self, resnet_size_choices=None):\n    super(ResnetArgParser, self).__init__(parents=[\n        parsers.BaseParser(multi_gpu=False),\n        parsers.PerformanceParser(num_parallel_calls=False),\n        parsers.ImageModelParser(),\n        parsers.ExportParser(),\n        parsers.BenchmarkParser(),\n    ])\n\n    self.add_argument(\n        \'--version\', \'-v\', type=int, choices=[1, 2],\n        default=resnet_model.DEFAULT_VERSION,\n        help=\'Version of ResNet. (1 or 2) See README.md for details.\'\n    )\n\n    self.add_argument(\n        \'--resnet_size\', \'-rs\', type=int, default=50,\n        choices=resnet_size_choices,\n        help=\'[default: %(default)s] The size of the ResNet model to use.\',\n        metavar=\'<RS>\' if resnet_size_choices is None else None\n    )\n\n    self.add_argument(\n        \'--use_bfloat16\', action=\'store_true\', default=False,\n        help=\'Whether to use bfloat16 type for computations.\'\n    )\n\n  def parse_args(self, args=None, namespace=None):\n    args = super(ResnetArgParser, self).parse_args(\n        args=args, namespace=namespace)\n\n    # handle coupling between dtype and loss_scale\n    parsers.parse_dtype_info(args)\n\n    return args\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/__init__.py,0,b''
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/compute_bleu.py,5,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Script to compute official BLEU score.\n\nSource:\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/bleu_hook.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport sys\nimport unicodedata\n\n# pylint: disable=g-bad-import-order\nimport six\nfrom absl import app as absl_app\nfrom absl import flags\nimport tensorflow as tf\n# pylint: enable=g-bad-import-order\n\nfrom utils import metrics\nfrom utils import tokenizer\nfrom official.utils.flags import core as flags_core\n\n\nclass UnicodeRegex(object):\n  """"""Ad-hoc hack to recognize all punctuation and symbols.""""""\n\n  def __init__(self):\n    punctuation = self.property_chars(""P"")\n    self.nondigit_punct_re = re.compile(r""([^\\d])(["" + punctuation + r""])"")\n    self.punct_nondigit_re = re.compile(r""(["" + punctuation + r""])([^\\d])"")\n    self.symbol_re = re.compile(""(["" + self.property_chars(""S"") + ""])"")\n\n  def property_chars(self, prefix):\n    return """".join(six.unichr(x) for x in range(sys.maxunicode)\n                   if unicodedata.category(six.unichr(x)).startswith(prefix))\n\n\nuregex = UnicodeRegex()\n\n\ndef bleu_tokenize(string):\n  r""""""Tokenize a string following the official BLEU implementation.\n\n  See https://github.com/moses-smt/mosesdecoder/\'\n           \'blob/master/scripts/generic/mteval-v14.pl#L954-L983\n  In our case, the input string is expected to be just one line\n  and no HTML entities de-escaping is needed.\n  So we just tokenize on punctuation and symbols,\n  except when a punctuation is preceded and followed by a digit\n  (e.g. a comma/dot as a thousand/decimal separator).\n\n  Note that a numer (e.g. a year) followed by a dot at the end of sentence\n  is NOT tokenized,\n  i.e. the dot stays with the number because `s/(\\p{P})(\\P{N})/ $1 $2/g`\n  does not match this case (unless we add a space after each sentence).\n  However, this error is already in the original mteval-v14.pl\n  and we want to be consistent with it.\n\n  Args:\n    string: the input string\n\n  Returns:\n    a list of tokens\n  """"""\n  string = uregex.nondigit_punct_re.sub(r""\\1 \\2 "", string)\n  string = uregex.punct_nondigit_re.sub(r"" \\1 \\2"", string)\n  string = uregex.symbol_re.sub(r"" \\1 "", string)\n  return string.split()\n\n\ndef bleu_wrapper(ref_filename, hyp_filename, case_sensitive=False):\n  """"""Compute BLEU for two files (reference and hypothesis translation).""""""\n  ref_lines = tokenizer.native_to_unicode(\n      tf.io.gfile.GFile(ref_filename).read()).strip().splitlines()\n  hyp_lines = tokenizer.native_to_unicode(\n      tf.io.gfile.GFile(hyp_filename).read()).strip().splitlines()\n\n  if len(ref_lines) != len(hyp_lines):\n    raise ValueError(""Reference and translation files have different number of ""\n                     ""lines. If training only a few steps (100-200), the ""\n                     ""translation may be empty."")\n  if not case_sensitive:\n    ref_lines = [x.lower() for x in ref_lines]\n    hyp_lines = [x.lower() for x in hyp_lines]\n  ref_tokens = [bleu_tokenize(x) for x in ref_lines]\n  hyp_tokens = [bleu_tokenize(x) for x in hyp_lines]\n  return metrics.compute_bleu(ref_tokens, hyp_tokens) * 100\n\n\ndef main(unused_argv):\n  if FLAGS.bleu_variant in (""both"", ""uncased""):\n    score = bleu_wrapper(FLAGS.reference, FLAGS.translation, False)\n    tf.compat.v1.logging.info(""Case-insensitive results: %f"" % score)\n\n  if FLAGS.bleu_variant in (""both"", ""cased""):\n    score = bleu_wrapper(FLAGS.reference, FLAGS.translation, True)\n    tf.compat.v1.logging.info(""Case-sensitive results: %f"" % score)\n\n\ndef define_compute_bleu_flags():\n  """"""Add flags for computing BLEU score.""""""\n  flags.DEFINE_string(\n      name=""translation"", default=None,\n      help=flags_core.help_wrap(""File containing translated text.""))\n  flags.mark_flag_as_required(""translation"")\n\n  flags.DEFINE_string(\n      name=""reference"", default=None,\n      help=flags_core.help_wrap(""File containing reference translation.""))\n  flags.mark_flag_as_required(""reference"")\n\n  flags.DEFINE_enum(\n      name=""bleu_variant"", short_name=""bv"", default=""both"",\n      enum_values=[""both"", ""uncased"", ""cased""], case_sensitive=False,\n      help=flags_core.help_wrap(\n          ""Specify one or more BLEU variants to calculate. Variants: \\""cased\\""""\n          "", \\""uncased\\"", or \\""both\\"".""))\n\n\nif __name__ == ""__main__"":\n  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n  define_compute_bleu_flags()\n  FLAGS = flags.FLAGS\n  absl_app.run(main)\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/infer_ab.py,9,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom google.protobuf import text_format\n\nfrom tensorflow.core.framework import graph_pb2\nimport numpy as np\nfrom utils import tokenizer\nfrom utils.tokenizer import Subtokenizer\n\nfrom tensorflow.python.platform import flags as flags_lib\nfrom tensorflow.python.platform import app\nimport time\nimport pandas as pd\nfrom timeit import default_timer as timer\n\nflags = flags_lib\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""in_graph"", \'fp32_graphdef.pb\',\n                    """"""TensorFlow \'GraphDef\' file for FP32 to load."""""")\nflags.DEFINE_bool(""input_binary"", True,\n                  """"""Whether the input files are in binary format."""""")\nflags.DEFINE_string(\n      ""vocab_file"", ""vocab.ende.32768"",\n              ""Path to subtoken vocabulary file."")\nflags.DEFINE_string(\n      ""file"", ""newstest2014.en"",\n              """"""File saved to an output file."""""")\nflags.DEFINE_string(\n    ""file_out"", ""translate.txt"",\n          """"""If --file flag is specified, save translation to this file."""""")\nflags.DEFINE_integer(""batch_size"", 64,\n                     """"""The validation batch size"""""")\nflags.DEFINE_integer(""num_inter"", 1,\n                     """"""Number of sentences to exclude from validation file."""""")\nflags.DEFINE_integer(""num_intra"", 40,\n                     """"""Number of sentences to exclude from validation file."""""")\nflags.DEFINE_integer(""num_batches"", 0,\n                     """"""Number of batches of sentences to run inference for"""""")\nflags.DEFINE_bool(""sort_input_sentences"", None,\n                  """"""Sort the sequence of sentences in validation file. \n                  Sorting improves batch processing time"""""")\n\ndef input_generator_ts():\n  """"""Read and sort lines based on token count from the file\n     sorted by decreasing length based on token sorting.\n  Args:\n    filename: String name of file to read inputs from.\n  Returns:\n    Sorted list of inputs, and dictionary mapping original index->sorted index\n    of each element.\n  """"""\n  with tf.io.gfile.GFile(FLAGS.file) as f:\n    records = f.read().split(""\\n"")\n    inputs = [record.strip() for record in records]\n    if not inputs[-1]:\n      inputs.pop()\n\n  subtokenizer = Subtokenizer(FLAGS.vocab_file)\n\n  batch = []\n  token_lens=[]\n  for i, line in enumerate(inputs):\n    enc = subtokenizer.encode(line, add_eos=True)\n    token_lens.append((i, len(enc)))\n\n  sorted_by_token_input_lens = sorted(token_lens, key=lambda x: x[1], reverse=True)\n\n  #print(\'sorted_by_token_input_lens:{}\'.format(sorted_by_token_input_lens))\n\n  sorted_inputs = [None] * len(sorted_by_token_input_lens)\n  sorted_keys = [0] * len(sorted_by_token_input_lens)\n\n  for i, (index, _) in enumerate(sorted_by_token_input_lens):\n    sorted_inputs[i] = inputs[index]\n    sorted_keys[index] = i\n    enc=subtokenizer.encode(sorted_inputs[i], add_eos=True)\n    batch.append(enc)\n\n  return batch,sorted_keys\n\ndef input_generator_ws():\n  """"""Read and sort lines from the file sorted by decreasing length based on word counts.\n  Args:\n    filename: String name of file to read inputs from.\n  Returns:\n    Sorted list of inputs, and dictionary mapping original index->sorted index\n    of each element.\n  """"""\n  with tf.io.gfile.GFile(FLAGS.file) as f:\n    records = f.read().split(""\\n"")\n    inputs = [record.strip() for record in records]\n    if not inputs[-1]:\n      inputs.pop()\n\n  batch = []\n\n  subtokenizer = Subtokenizer(FLAGS.vocab_file)\n\n  input_lens = [(i, len(line.split())) for i, line in enumerate(inputs)]\n  sorted_input_lens = sorted(input_lens, key=lambda x: x[1], reverse=True)\n\n  sorted_inputs = [None] * len(sorted_input_lens)\n  sorted_keys = [0] * len(sorted_input_lens)\n  for i, (index, _) in enumerate(sorted_input_lens):\n    sorted_inputs[i] = inputs[index]\n    sorted_keys[index] = i\n    enc=subtokenizer.encode(sorted_inputs[i], add_eos=True)\n    batch.append(enc)\n  return batch,sorted_keys\n\ndef _trim_and_decode(ids):\n  """"""Trim EOS and PAD tokens from ids, and decode to return a string.""""""\n  subtokenizer = Subtokenizer(FLAGS.vocab_file)\n  try:\n    index = list(ids).index(tokenizer.EOS_ID)\n    return subtokenizer.decode(ids[:index])\n  except ValueError:  # No EOS found in sequence\n    return subtokenizer.decode(ids)\n\ndef main(unused_args):\n\n  graph_def = graph_pb2.GraphDef()\n  graph_file=FLAGS.in_graph\n\n\n  start=timer()\n  with open(graph_file, ""rb"") as f:\n    if FLAGS.input_binary:\n      graph_def.ParseFromString(f.read())\n    else:\n      text_format.Merge(f.read(), graph_def)\n  end=timer()\n  graph_parse_time = end-start\n  print(""Graph parsed in %f s"" % (end-start))\n\n  start=timer()\n  with tf.Graph().as_default() as graph:\n    y = tf.import_graph_def(graph_def,return_elements=[""model/Transformer/strided_slice_19:0""], name=\'\')\n  end=timer()\n  print(""import_graph_def took %fs"" % (end-start))\n\n  start=timer()\n  batches,sorted_keys = input_generator_ts()\n  end=timer()\n  sort_time = end-start\n  print(""tokenizer took %f s"" % (sort_time))\n\n  DATASET_SIZE=len(batches)\n  print(""Translating {} sentences from English to German."".format(DATASET_SIZE))\n\n  session_config = tf.compat.v1.ConfigProto(\n      inter_op_parallelism_threads=FLAGS.num_inter,\n      intra_op_parallelism_threads=FLAGS.num_intra)\n\n  with tf.compat.v1.Session(config=session_config, graph=graph) as sess:\n\n    run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n    run_metadata = tf.compat.v1.RunMetadata()\n\n    translations = []\n\n    batch = []\n    batch_num=0\n    line_count=0\n\n    total_start_time = time.time()\n    inference_time = 0.0\n\n    for i, line in enumerate(batches): \n      batch.append(line)\n      duration = 0.0\n      if i % FLAGS.batch_size == 0:\n        line_count += 1\n        batch_num = (i // FLAGS.batch_size) + 1\n        start_time = time.time()\n        dec_tensor = sess.run(y, feed_dict={\'input_tensor:0\': pd.DataFrame(batch).fillna(0).values.astype(np.int32)})\n        duration = time.time() - start_time\n        translations.append(dec_tensor)\n        #print(\'Batch inferencing time:%s for batch size:%d and batch:%d\' % (duration, FLAGS.batch_size, batch_num))\n        batch = []\n      elif i % (len(batches) - 1) == 0:\n        batch_num = (i // FLAGS.batch_size) + 1\n        start_time = time.time()\n        dec_tensor = sess.run(y, feed_dict={\'input_tensor:0\': pd.DataFrame(batch).fillna(0).values.astype(np.int32)})\n        duration = time.time() - start_time\n        translations.append(dec_tensor)\n        #print(\'Batch inferencing time:%s for batch size:%d and batch:%d\' % (duration, FLAGS.batch_size, batch_num))\n        batch = []\n      inference_time += duration\n\n    inference_time += graph_parse_time\n    inference_time += sort_time\n    print(\'Total inferencing time:%s\' %(inference_time))\n    print(\'Throughput:{} sentences/second\'.format((DATASET_SIZE)/inference_time))\n\n    translation_count = 0\n\n    decoded_translations=[]\n    for i,tr in enumerate(translations):\n      for j,itr in enumerate(tr):\n        for k,otr in enumerate(itr):\n          translation_count += 1\n          decoded_translations.append(_trim_and_decode(otr))\n\n    print(\'Total number of sentences translated:%d\' % (translation_count))\n\n    with tf.io.gfile.GFile(FLAGS.file_out, ""w"") as f:\n      for i in sorted_keys:\n        f.write(""%s\\n"" % decoded_translations[i])\n\nif __name__ == ""__main__"":\n  app.run()\n'"
models/object_detection/tensorflow/rfcn/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/object_detection/tensorflow/rfcn/inference/fp32/dataset_util.py,17,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility functions for creating TFRecord data sets.""""""\n\nimport tensorflow as tf\n\n\ndef int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef int64_list_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef read_examples_list(path):\n  """"""Read list of training or validation examples.\n\n  The file is assumed to contain a single example per line where the first\n  token in the line is an identifier that allows us to find the image and\n  annotation xml for that example.\n\n  For example, the line:\n  xyz 3\n  would allow us to find files xyz.jpg and xyz.xml (the 3 would be ignored).\n\n  Args:\n    path: absolute path to examples list file.\n\n  Returns:\n    list of example identifiers (strings).\n  """"""\n  with tf.io.gfile.GFile(path) as fid:\n    lines = fid.readlines()\n  return [line.strip().split(\' \')[0] for line in lines]\n\n\ndef recursive_parse_xml_to_dict(xml):\n  """"""Recursively parses XML contents to python dict.\n\n  We assume that `object` tags are the only ones that can appear\n  multiple times at the same level of a tree.\n\n  Args:\n    xml: xml tree obtained by parsing XML file contents using lxml.etree\n\n  Returns:\n    Python dictionary holding XML contents.\n  """"""\n  if not xml:\n    return {xml.tag: xml.text}\n  result = {}\n  for child in xml:\n    child_result = recursive_parse_xml_to_dict(child)\n    if child.tag != \'object\':\n      result[child.tag] = child_result[child.tag]\n    else:\n      if child.tag not in result:\n        result[child.tag] = []\n      result[child.tag].append(child_result[child.tag])\n  return {xml.tag: result}\n\n\ndef make_initializable_iterator(dataset):\n  """"""Creates an iterator, and initializes tables.\n\n  This is useful in cases where make_one_shot_iterator wouldn\'t work because\n  the graph contains a hash table that needs to be initialized.\n\n  Args:\n    dataset: A `tf.data.Dataset` object.\n\n  Returns:\n    A `tf.data.Iterator`.\n  """"""\n  iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n  tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n  return iterator\n\n\ndef read_dataset(file_read_func, decode_func, input_files, config):\n  """"""Reads a dataset, and handles repetition and shuffling.\n\n  Args:\n    file_read_func: Function to use in tf.data.Dataset.interleave, to read\n      every individual file into a tf.data.Dataset.\n    decode_func: Function to apply to all records.\n    input_files: A list of file paths to read.\n    config: A input_reader_builder.InputReader object.\n\n  Returns:\n    A tf.data.Dataset based on config.\n  """"""\n  # Shard, shuffle, and read files.\n  filenames = tf.concat([tf.io.matching_files(pattern) for pattern in input_files],\n                        0)\n  filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n  if config.shuffle:\n    filename_dataset = filename_dataset.shuffle(\n        config.filenames_shuffle_buffer_size)\n  elif config.num_readers > 1:\n    tf.compat.v1.logging.warning(\'`shuffle` is false, but the input data stream is \'\n                       \'still slightly shuffled since `num_readers` > 1.\')\n\n  filename_dataset = filename_dataset.repeat(config.num_epochs or None)\n\n  records_dataset = filename_dataset.apply(\n      tf.data.experimental.parallel_interleave(\n          file_read_func, cycle_length=config.num_readers,\n          block_length=config.read_block_length, sloppy=True))\n  if config.shuffle:\n    records_dataset.shuffle(config.shuffle_buffer_size)\n  tensor_dataset = records_dataset.map(\n      decode_func, num_parallel_calls=config.num_parallel_map_calls)\n  return tensor_dataset.prefetch(config.prefetch_size)\n'"
models/object_detection/tensorflow/rfcn/inference/fp32/eval.py,7,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Evaluation executable for detection models.\n\nThis executable is used to evaluate DetectionModels. There are two ways of\nconfiguring the eval job.\n\n1) A single pipeline_pb2.TrainEvalPipelineConfig file maybe specified instead.\nIn this mode, the --eval_training_data flag may be given to force the pipeline\nto evaluate on training data instead.\n\nExample usage:\n    ./eval \\\n        --logtostderr \\\n        --checkpoint_dir=path/to/checkpoint_dir \\\n        --eval_dir=path/to/eval_dir \\\n        --pipeline_config_path=pipeline_config.pbtxt\n\n2) Three configuration files may be provided: a model_pb2.DetectionModel\nconfiguration file to define what type of DetectionModel is being evaluated, an\ninput_reader_pb2.InputReader file to specify what data the model is evaluating\nand an eval_pb2.EvalConfig file to configure evaluation parameters.\n\nExample usage:\n    ./eval \\\n        --logtostderr \\\n        --checkpoint_dir=path/to/checkpoint_dir \\\n        --eval_dir=path/to/eval_dir \\\n        --eval_config_path=eval_config.pbtxt \\\n        --model_config_path=model_config.pbtxt \\\n        --input_config_path=eval_input_config.pbtxt\n""""""\nimport functools\nimport os\nimport tensorflow as tf\n\nimport evaluator\nimport dataset_util\nfrom object_detection.builders import dataset_builder\nfrom object_detection.builders import model_builder\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\n\n\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\nimport logging\n# logging.basicConfig(level=logging.INFO)\n\nflags = tf.app.flags\nflags.DEFINE_boolean(\'eval_training_data\', False,\n                     \'If training data should be evaluated for this job.\')\nflags.DEFINE_string(\'checkpoint_dir\', \'\',\n                    \'Directory containing checkpoints to evaluate, typically \'\n                    \'set to `train_dir` used in the training job.\')\nflags.DEFINE_string(\'eval_dir\', \'\',\n                    \'Directory to write eval summaries to.\')\nflags.DEFINE_string(\'pipeline_config_path\', \'\',\n                    \'Path to a pipeline_pb2.TrainEvalPipelineConfig config \'\n                    \'file. If provided, other configs are ignored\')\nflags.DEFINE_string(\'eval_config_path\', \'\',\n                    \'Path to an eval_pb2.EvalConfig config file.\')\nflags.DEFINE_string(\'input_config_path\', \'\',\n                    \'Path to an input_reader_pb2.InputReader config file.\')\nflags.DEFINE_string(\'model_config_path\', \'\',\n                    \'Path to a model_pb2.DetectionModel config file.\')\nflags.DEFINE_boolean(\'run_once\', False, \'Option to only run a single pass of \'\n                     \'evaluation. Overrides the `max_evals` parameter in the \'\n                     \'provided config.\')\nflags.DEFINE_integer(\'omp\', 0, \'number of OMP threads\')\nflags.DEFINE_integer(\'intra_op\', 0, \'number of intra_op threads\')\nflags.DEFINE_integer(\'inter_op\', 0, \'number of inter_op threads\')\nflags.DEFINE_integer(\'blocktime\', 1, \'Blocktime value\')\nflags.DEFINE_integer(\'batch_size\', 1, \'batch size\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(unused_argv):\n  if (FLAGS.omp > 0):\n    if not os.environ.get(""OMP_NUM_THREADS""):\n      logging.info(\'OMP_NUM_THREADS value= %d\', FLAGS.omp)\n      os.environ[""OMP_NUM_THREADS""] = str(FLAGS.omp)\n    if not os.environ.get(""KMP_BLOCKTIME""):\n      logging.info(\'KMP_BLOCKTIME value= %d\', FLAGS.blocktime)\n      os.environ[""KMP_BLOCKTIME""] = str(FLAGS.blocktime)\n    if not os.environ.get(""KMP_SETTINGS""):\n      os.environ[""KMP_SETTINGS""] = ""1""\n    # os.environ[""KMP_AFFINITY""]= ""granularity=fine,verbose,compact,1,0""\n  assert FLAGS.checkpoint_dir, \'`checkpoint_dir` is missing.\'\n  assert FLAGS.eval_dir, \'`eval_dir` is missing.\'\n  tf.io.gfile.makedirs(FLAGS.eval_dir)\n  if FLAGS.pipeline_config_path:\n    configs = config_util.get_configs_from_pipeline_file(\n        FLAGS.pipeline_config_path)\n    tf.io.gfile.copy(FLAGS.pipeline_config_path,\n                  os.path.join(FLAGS.eval_dir, \'pipeline.config\'),\n                  overwrite=True)\n  else:\n    configs = config_util.get_configs_from_multiple_files(\n        model_config_path=FLAGS.model_config_path,\n        eval_config_path=FLAGS.eval_config_path,\n        eval_input_config_path=FLAGS.input_config_path)\n    for name, config in [(\'model.config\', FLAGS.model_config_path),\n                         (\'eval.config\', FLAGS.eval_config_path),\n                         (\'input.config\', FLAGS.input_config_path)]:\n      tf.io.gfile.copy(config,\n                    os.path.join(FLAGS.eval_dir, name),\n                    overwrite=True)\n\n  model_config = configs[\'model\']\n  eval_config = configs[\'eval_config\']\n  input_config = configs[\'eval_input_config\']\n  if FLAGS.eval_training_data:\n    input_config = configs[\'train_input_config\']\n\n  model_fn = functools.partial(\n      model_builder.build,\n      model_config=model_config,\n      is_training=False)\n\n  def get_next(config):\n    return tf.compat.v1.data.make_initializable_iterator(\n        dataset_util, dataset_builder.build(config)).get_next()\n\n  create_input_dict_fn = functools.partial(get_next, input_config)\n\n  label_map = label_map_util.load_labelmap(input_config.label_map_path)\n  max_num_classes = max([item.id for item in label_map.item])\n  categories = label_map_util.convert_label_map_to_categories(\n      label_map, max_num_classes)\n\n  if FLAGS.run_once:\n    eval_config.max_evals = 1\n\n  evaluator.evaluate(create_input_dict_fn, model_fn, eval_config, categories,\n                     FLAGS.checkpoint_dir, FLAGS.eval_dir, intra_op=FLAGS.intra_op, inter_op=FLAGS.inter_op)\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.app.run()\n'"
models/object_detection/tensorflow/rfcn/inference/fp32/eval_util.py,39,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common utility functions for evaluation.""""""\nimport collections\nimport logging\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import keypoint_ops\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.metrics import coco_evaluation\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import ops\nfrom object_detection.utils import visualization_utils as vis_utils\n\ndef write_metrics(metrics, global_step, summary_dir):\n  """"""Write metrics to a summary directory.\n\n  Args:\n    metrics: A dictionary containing metric names and values.\n    global_step: Global step at which the metrics are computed.\n    summary_dir: Directory to write tensorflow summaries to.\n  """"""\n  logging.info(\'Writing metrics to tf summary.\')\n  summary_writer = tf.compat.v1.summary.FileWriterCache.get(summary_dir)\n  for key in sorted(metrics):\n    summary = tf.compat.v1.Summary(value=[\n        tf.compat.v1.Summary.Value(tag=key, simple_value=metrics[key]),\n    ])\n    summary_writer.add_summary(summary, global_step)\n    logging.info(\'%s: %f\', key, metrics[key])\n  logging.info(\'Metrics written to tf summary.\')\n\n\n# TODO(rathodv): Add tests.\ndef visualize_detection_results(result_dict,\n                                tag,\n                                global_step,\n                                categories,\n                                summary_dir=\'\',\n                                export_dir=\'\',\n                                agnostic_mode=False,\n                                show_groundtruth=False,\n                                groundtruth_box_visualization_color=\'black\',\n                                min_score_thresh=.5,\n                                max_num_predictions=20,\n                                skip_scores=False,\n                                skip_labels=False,\n                                keep_image_id_for_visualization_export=False):\n  """"""Visualizes detection results and writes visualizations to image summaries.\n\n  This function visualizes an image with its detected bounding boxes and writes\n  to image summaries which can be viewed on tensorboard.  It optionally also\n  writes images to a directory. In the case of missing entry in the label map,\n  unknown class name in the visualization is shown as ""N/A"".\n\n  Args:\n    result_dict: a dictionary holding groundtruth and detection\n      data corresponding to each image being evaluated.  The following keys\n      are required:\n        \'original_image\': a numpy array representing the image with shape\n          [1, height, width, 3] or [1, height, width, 1]\n        \'detection_boxes\': a numpy array of shape [N, 4]\n        \'detection_scores\': a numpy array of shape [N]\n        \'detection_classes\': a numpy array of shape [N]\n      The following keys are optional:\n        \'groundtruth_boxes\': a numpy array of shape [N, 4]\n        \'groundtruth_keypoints\': a numpy array of shape [N, num_keypoints, 2]\n      Detections are assumed to be provided in decreasing order of score and for\n      display, and we assume that scores are probabilities between 0 and 1.\n    tag: tensorboard tag (string) to associate with image.\n    global_step: global step at which the visualization are generated.\n    categories: a list of dictionaries representing all possible categories.\n      Each dict in this list has the following keys:\n          \'id\': (required) an integer id uniquely identifying this category\n          \'name\': (required) string representing category name\n            e.g., \'cat\', \'dog\', \'pizza\'\n          \'supercategory\': (optional) string representing the supercategory\n            e.g., \'animal\', \'vehicle\', \'food\', etc\n    summary_dir: the output directory to which the image summaries are written.\n    export_dir: the output directory to which images are written.  If this is\n      empty (default), then images are not exported.\n    agnostic_mode: boolean (default: False) controlling whether to evaluate in\n      class-agnostic mode or not.\n    show_groundtruth: boolean (default: False) controlling whether to show\n      groundtruth boxes in addition to detected boxes\n    groundtruth_box_visualization_color: box color for visualizing groundtruth\n      boxes\n    min_score_thresh: minimum score threshold for a box to be visualized\n    max_num_predictions: maximum number of detections to visualize\n    skip_scores: whether to skip score when drawing a single detection\n    skip_labels: whether to skip label when drawing a single detection\n    keep_image_id_for_visualization_export: whether to keep image identifier in\n      filename when exported to export_dir\n  Raises:\n    ValueError: if result_dict does not contain the expected keys (i.e.,\n      \'original_image\', \'detection_boxes\', \'detection_scores\',\n      \'detection_classes\')\n  """"""\n  detection_fields = fields.DetectionResultFields\n  input_fields = fields.InputDataFields\n  if not set([\n      input_fields.original_image,\n      detection_fields.detection_boxes,\n      detection_fields.detection_scores,\n      detection_fields.detection_classes,\n  ]).issubset(set(result_dict.keys())):\n    raise ValueError(\'result_dict does not contain all expected keys.\')\n  if show_groundtruth and input_fields.groundtruth_boxes not in result_dict:\n    raise ValueError(\'If show_groundtruth is enabled, result_dict must contain \'\n                     \'groundtruth_boxes.\')\n  logging.info(\'Creating detection visualizations.\')\n  category_index = label_map_util.create_category_index(categories)\n\n  image = np.squeeze(result_dict[input_fields.original_image], axis=0)\n  if image.shape[2] == 1:  # If one channel image, repeat in RGB.\n    image = np.tile(image, [1, 1, 3])\n  detection_boxes = result_dict[detection_fields.detection_boxes]\n  detection_scores = result_dict[detection_fields.detection_scores]\n  detection_classes = np.int32((result_dict[\n      detection_fields.detection_classes]))\n  detection_keypoints = result_dict.get(detection_fields.detection_keypoints)\n  detection_masks = result_dict.get(detection_fields.detection_masks)\n  detection_boundaries = result_dict.get(detection_fields.detection_boundaries)\n\n  # Plot groundtruth underneath detections\n  if show_groundtruth:\n    groundtruth_boxes = result_dict[input_fields.groundtruth_boxes]\n    groundtruth_keypoints = result_dict.get(input_fields.groundtruth_keypoints)\n    vis_utils.visualize_boxes_and_labels_on_image_array(\n        image=image,\n        boxes=groundtruth_boxes,\n        classes=None,\n        scores=None,\n        category_index=category_index,\n        keypoints=groundtruth_keypoints,\n        use_normalized_coordinates=False,\n        max_boxes_to_draw=None,\n        groundtruth_box_visualization_color=groundtruth_box_visualization_color)\n  vis_utils.visualize_boxes_and_labels_on_image_array(\n      image,\n      detection_boxes,\n      detection_classes,\n      detection_scores,\n      category_index,\n      instance_masks=detection_masks,\n      instance_boundaries=detection_boundaries,\n      keypoints=detection_keypoints,\n      use_normalized_coordinates=False,\n      max_boxes_to_draw=max_num_predictions,\n      min_score_thresh=min_score_thresh,\n      agnostic_mode=agnostic_mode,\n      skip_scores=skip_scores,\n      skip_labels=skip_labels)\n\n  if export_dir:\n    if keep_image_id_for_visualization_export and result_dict[fields.\n                                                              InputDataFields()\n                                                              .key]:\n      export_path = os.path.join(export_dir, \'export-{}-{}.png\'.format(\n          tag, result_dict[fields.InputDataFields().key]))\n    else:\n      export_path = os.path.join(export_dir, \'export-{}.png\'.format(tag))\n    vis_utils.save_image_array_as_png(image, export_path)\n\n  summary = tf.compat.v1.Summary(value=[\n      tf.compat.v1.Summary.Value(\n          tag=tag,\n          image=tf.compat.v1.Summary.Image(\n              encoded_image_string=vis_utils.encode_image_array_as_png_str(\n                  image)))\n  ])\n  summary_writer = tf.compat.v1.summary.FileWriterCache.get(summary_dir)\n  summary_writer.add_summary(summary, global_step)\n\n  logging.info(\'Detection visualizations written to summary with tag %s.\', tag)\n\n\ndef _run_checkpoint_once(tensor_dict,\n                         evaluators=None,\n                         batch_processor=None,\n                         checkpoint_dirs=None,\n                         variables_to_restore=None,\n                         restore_fn=None,\n                         num_batches=1,\n                         master=\'\',\n                         save_graph=False,\n                         save_graph_dir=\'\',\n                         losses_dict=None,\n                         intra_op=0,\n                         inter_op=0):\n\n  """"""Evaluates metrics defined in evaluators and returns summaries.\n\n  This function loads the latest checkpoint in checkpoint_dirs and evaluates\n  all metrics defined in evaluators. The metrics are processed in batch by the\n  batch_processor.\n\n  Args:\n    tensor_dict: a dictionary holding tensors representing a batch of detections\n      and corresponding groundtruth annotations.\n    evaluators: a list of object of type DetectionEvaluator to be used for\n      evaluation. Note that the metric names produced by different evaluators\n      must be unique.\n    batch_processor: a function taking four arguments:\n      1. tensor_dict: the same tensor_dict that is passed in as the first\n        argument to this function.\n      2. sess: a tensorflow session\n      3. batch_index: an integer representing the index of the batch amongst\n        all batches\n      By default, batch_processor is None, which defaults to running:\n        return sess.run(tensor_dict)\n      To skip an image, it suffices to return an empty dictionary in place of\n      result_dict.\n    checkpoint_dirs: list of directories to load into an EnsembleModel. If it\n      has only one directory, EnsembleModel will not be used --\n        a DetectionModel\n      will be instantiated directly. Not used if restore_fn is set.\n    variables_to_restore: None, or a dictionary mapping variable names found in\n      a checkpoint to model variables. The dictionary would normally be\n      generated by creating a tf.train.ExponentialMovingAverage object and\n      calling its variables_to_restore() method. Not used if restore_fn is set.\n    restore_fn: None, or a function that takes a tf.Session object and correctly\n      restores all necessary variables from the correct checkpoint file. If\n      None, attempts to restore from the first directory in checkpoint_dirs.\n    num_batches: the number of batches to use for evaluation.\n    master: the location of the Tensorflow session.\n    save_graph: whether or not the Tensorflow graph is stored as a pbtxt file.\n    save_graph_dir: where to store the Tensorflow graph on disk. If save_graph\n      is True this must be non-empty.\n    losses_dict: optional dictionary of scalar detection losses.\n\n  Returns:\n    global_step: the count of global steps.\n    all_evaluator_metrics: A dictionary containing metric names and values.\n\n  Raises:\n    ValueError: if restore_fn is None and checkpoint_dirs doesn\'t have at least\n      one element.\n    ValueError: if save_graph is True and save_graph_dir is not defined.\n  """"""\n  if save_graph and not save_graph_dir:\n    raise ValueError(\'`save_graph_dir` must be defined.\')\n  if (inter_op > 0 or intra_op > 0):\n    config = tf.compat.v1.ConfigProto(inter_op_parallelism_threads=inter_op,\n                            intra_op_parallelism_threads=intra_op)\n    logging.info(\'inter_op value= %d\', inter_op)\n    logging.info(\'intra_op value= %d\', intra_op)\n  else:\n    config = None\n  sess = tf.compat.v1.Session(master, graph=tf.compat.v1.get_default_graph(), config=config)\n  sess.run(tf.compat.v1.global_variables_initializer())\n  sess.run(tf.compat.v1.local_variables_initializer())\n  sess.run(tf.compat.v1.tables_initializer())\n  if restore_fn:\n    restore_fn(sess)\n  else:\n    if not checkpoint_dirs:\n      raise ValueError(\'`checkpoint_dirs` must have at least one entry.\')\n    checkpoint_file = tf.train.latest_checkpoint(checkpoint_dirs[0])\n    saver = tf.compat.v1.train.Saver(variables_to_restore)\n    saver.restore(sess, checkpoint_file)\n\n  if save_graph:\n    tf.io.write_graph(sess.graph_def, save_graph_dir, \'eval.pbtxt\')\n\n  counters = {\'skipped\': 0, \'success\': 0}\n  aggregate_result_losses_dict = collections.defaultdict(list)\n  with tf.compat.v1.train.QueueRunner(sess):\n    try:\n      loop_start_time = time.time()\n      for batch in range(int(num_batches)):\n        if (batch + 1) % 100 == 0:\n          logging.info(\'Running eval ops batch %d/%d\', batch + 1, num_batches)\n        start_time = time.time()\n        if not batch_processor:\n          try:\n            if not losses_dict:\n              losses_dict = {}\n            result_dict, result_losses_dict = sess.run([tensor_dict,\n                                                        losses_dict])\n            counters[\'success\'] += 1\n          except tf.errors.InvalidArgumentError:\n            logging.info(\'Skipping image\')\n            counters[\'skipped\'] += 1\n            result_dict = {}\n        else:\n          result_dict, result_losses_dict = batch_processor(\n              tensor_dict, sess, batch, counters, losses_dict=losses_dict)\n        if ((batch % 100) == 0):\n          logging.info(\'Iteration %d: %.3f sec\', batch, time.time() - start_time)\n        if not result_dict:\n          continue\n        for key, value in iter(result_losses_dict.items()):\n          aggregate_result_losses_dict[key].append(value)\n        for evaluator in evaluators:\n          # TODO(b/65130867): Use image_id tensor once we fix the input data\n          # decoders to return correct image_id.\n          # TODO(akuznetsa): result_dict contains batches of images, while\n          # add_single_ground_truth_image_info expects a single image. Fix\n          evaluator.add_single_ground_truth_image_info(\n              image_id=batch, groundtruth_dict=result_dict)\n          evaluator.add_single_detected_image_info(\n              image_id=batch, detections_dict=result_dict)\n      loop_end_time = time.time()\n      total_run_time = loop_end_time - loop_start_time\n      avg_time_per_batch = total_run_time / num_batches\n      print(\'Average time per step: %.3f sec\' % avg_time_per_batch)\n      logging.info(\'Running eval batches done.\')\n    except tf.errors.OutOfRangeError:\n      logging.info(\'Done evaluating -- epoch limit reached\')\n    finally:\n      # When done, ask the threads to stop.\n      logging.info(\'# success: %d\', counters[\'success\'])\n      logging.info(\'# skipped: %d\', counters[\'skipped\'])\n      all_evaluator_metrics = {}\n      for evaluator in evaluators:\n        metrics = evaluator.evaluate()\n        evaluator.clear()\n        if any(key in all_evaluator_metrics for key in metrics):\n          raise ValueError(\'Metric names between evaluators must not collide.\')\n        all_evaluator_metrics.update(metrics)\n      global_step = tf.compat.v1.train.global_step(sess, tf.compat.v1.train.get_global_step())\n\n      for key, value in iter(aggregate_result_losses_dict.items()):\n        all_evaluator_metrics[\'Losses/\' + key] = np.mean(value)\n  sess.close()\n  return (global_step, all_evaluator_metrics)\n\n\n# TODO(rathodv): Add tests.\ndef repeated_checkpoint_run(tensor_dict,\n                            summary_dir,\n                            evaluators,\n                            batch_processor=None,\n                            checkpoint_dirs=None,\n                            variables_to_restore=None,\n                            restore_fn=None,\n                            num_batches=1,\n                            eval_interval_secs=120,\n                            max_number_of_evaluations=None,\n                            master=\'\',\n                            save_graph=False,\n                            save_graph_dir=\'\',\n                            losses_dict=None):\n  """"""Periodically evaluates desired tensors using checkpoint_dirs or restore_fn.\n\n  This function repeatedly loads a checkpoint and evaluates a desired\n  set of tensors (provided by tensor_dict) and hands the resulting numpy\n  arrays to a function result_processor which can be used to further\n  process/save/visualize the results.\n\n  Args:\n    tensor_dict: a dictionary holding tensors representing a batch of detections\n      and corresponding groundtruth annotations.\n    summary_dir: a directory to write metrics summaries.\n    evaluators: a list of object of type DetectionEvaluator to be used for\n      evaluation. Note that the metric names produced by different evaluators\n      must be unique.\n    batch_processor: a function taking three arguments:\n      1. tensor_dict: the same tensor_dict that is passed in as the first\n        argument to this function.\n      2. sess: a tensorflow session\n      3. batch_index: an integer representing the index of the batch amongst\n        all batches\n      By default, batch_processor is None, which defaults to running:\n        return sess.run(tensor_dict)\n    checkpoint_dirs: list of directories to load into a DetectionModel or an\n      EnsembleModel if restore_fn isn\'t set. Also used to determine when to run\n      next evaluation. Must have at least one element.\n    variables_to_restore: None, or a dictionary mapping variable names found in\n      a checkpoint to model variables. The dictionary would normally be\n      generated by creating a tf.train.ExponentialMovingAverage object and\n      calling its variables_to_restore() method. Not used if restore_fn is set.\n    restore_fn: a function that takes a tf.Session object and correctly restores\n      all necessary variables from the correct checkpoint file.\n    num_batches: the number of batches to use for evaluation.\n    eval_interval_secs: the number of seconds between each evaluation run.\n    max_number_of_evaluations: the max number of iterations of the evaluation.\n      If the value is left as None the evaluation continues indefinitely.\n    master: the location of the Tensorflow session.\n    save_graph: whether or not the Tensorflow graph is saved as a pbtxt file.\n    save_graph_dir: where to save on disk the Tensorflow graph. If store_graph\n      is True this must be non-empty.\n    losses_dict: optional dictionary of scalar detection losses.\n\n  Returns:\n    metrics: A dictionary containing metric names and values in the latest\n      evaluation.\n\n  Raises:\n    ValueError: if max_num_of_evaluations is not None or a positive number.\n    ValueError: if checkpoint_dirs doesn\'t have at least one element.\n  """"""\n  if max_number_of_evaluations and max_number_of_evaluations <= 0:\n    raise ValueError(\n        \'`number_of_steps` must be either None or a positive number.\')\n\n  if not checkpoint_dirs:\n    raise ValueError(\'`checkpoint_dirs` must have at least one entry.\')\n\n  last_evaluated_model_path = None\n  number_of_evaluations = 0\n  while True:\n    start = time.time()\n    logging.info(\'Starting evaluation at \' + time.strftime(\n        \'%Y-%m-%d-%H:%M:%S\', time.gmtime()))\n    model_path = tf.train.latest_checkpoint(checkpoint_dirs[0])\n    if not model_path:\n      logging.info(\'No model found in %s. Will try again in %d seconds\',\n                   checkpoint_dirs[0], eval_interval_secs)\n    elif model_path == last_evaluated_model_path:\n      logging.info(\'Found already evaluated checkpoint. Will try again in %d \'\n                   \'seconds\', eval_interval_secs)\n    else:\n      last_evaluated_model_path = model_path\n      global_step, metrics = _run_checkpoint_once(tensor_dict, evaluators,\n                                                  batch_processor,\n                                                  checkpoint_dirs,\n                                                  variables_to_restore,\n                                                  restore_fn, num_batches,\n                                                  master, save_graph,\n                                                  save_graph_dir,\n                                                  losses_dict=losses_dict)\n      write_metrics(metrics, global_step, summary_dir)\n    number_of_evaluations += 1\n\n    if (max_number_of_evaluations and\n        number_of_evaluations >= max_number_of_evaluations):\n      logging.info(\'Finished evaluation!\')\n      break\n    time_to_next_eval = start + eval_interval_secs - time.time()\n    if time_to_next_eval > 0:\n      time.sleep(time_to_next_eval)\n\n  return metrics\n\n\ndef result_dict_for_single_example(image,\n                                   key,\n                                   detections,\n                                   groundtruth=None,\n                                   class_agnostic=False,\n                                   scale_to_absolute=False):\n  """"""Merges all detection and groundtruth information for a single example.\n\n  Note that evaluation tools require classes that are 1-indexed, and so this\n  function performs the offset. If `class_agnostic` is True, all output classes\n  have label 1.\n\n  Args:\n    image: A single 4D uint8 image tensor of shape [1, H, W, C].\n    key: A single string tensor identifying the image.\n    detections: A dictionary of detections, returned from\n      DetectionModel.postprocess().\n    groundtruth: (Optional) Dictionary of groundtruth items, with fields:\n      \'groundtruth_boxes\': [num_boxes, 4] float32 tensor of boxes, in\n        normalized coordinates.\n      \'groundtruth_classes\': [num_boxes] int64 tensor of 1-indexed classes.\n      \'groundtruth_area\': [num_boxes] float32 tensor of bbox area. (Optional)\n      \'groundtruth_is_crowd\': [num_boxes] int64 tensor. (Optional)\n      \'groundtruth_difficult\': [num_boxes] int64 tensor. (Optional)\n      \'groundtruth_group_of\': [num_boxes] int64 tensor. (Optional)\n      \'groundtruth_instance_masks\': 3D int64 tensor of instance masks\n        (Optional).\n    class_agnostic: Boolean indicating whether the detections are class-agnostic\n      (i.e. binary). Default False.\n    scale_to_absolute: Boolean indicating whether boxes and keypoints should be\n      scaled to absolute coordinates. Note that for IoU based evaluations, it\n      does not matter whether boxes are expressed in absolute or relative\n      coordinates. Default False.\n\n  Returns:\n    A dictionary with:\n    \'original_image\': A [1, H, W, C] uint8 image tensor.\n    \'key\': A string tensor with image identifier.\n    \'detection_boxes\': [max_detections, 4] float32 tensor of boxes, in\n      normalized or absolute coordinates, depending on the value of\n      `scale_to_absolute`.\n    \'detection_scores\': [max_detections] float32 tensor of scores.\n    \'detection_classes\': [max_detections] int64 tensor of 1-indexed classes.\n    \'detection_masks\': [max_detections, H, W] float32 tensor of binarized\n      masks, reframed to full image masks.\n    \'groundtruth_boxes\': [num_boxes, 4] float32 tensor of boxes, in\n      normalized or absolute coordinates, depending on the value of\n      `scale_to_absolute`. (Optional)\n    \'groundtruth_classes\': [num_boxes] int64 tensor of 1-indexed classes.\n      (Optional)\n    \'groundtruth_area\': [num_boxes] float32 tensor of bbox area. (Optional)\n    \'groundtruth_is_crowd\': [num_boxes] int64 tensor. (Optional)\n    \'groundtruth_difficult\': [num_boxes] int64 tensor. (Optional)\n    \'groundtruth_group_of\': [num_boxes] int64 tensor. (Optional)\n    \'groundtruth_instance_masks\': 3D int64 tensor of instance masks\n      (Optional).\n\n  """"""\n  label_id_offset = 1  # Applying label id offset (b/63711816)\n\n  input_data_fields = fields.InputDataFields\n  output_dict = {\n      input_data_fields.original_image: image,\n      input_data_fields.key: key,\n  }\n\n  detection_fields = fields.DetectionResultFields\n  detection_boxes = detections[detection_fields.detection_boxes][0]\n  image_shape = tf.shape(input=image)\n  detection_scores = detections[detection_fields.detection_scores][0]\n\n  if class_agnostic:\n    detection_classes = tf.ones_like(detection_scores, dtype=tf.int64)\n  else:\n    detection_classes = (\n        tf.cast(detections[detection_fields.detection_classes][0], dtype=tf.int64) +\n        label_id_offset)\n\n  num_detections = tf.cast(detections[detection_fields.num_detections][0], dtype=tf.int32)\n  detection_boxes = tf.slice(\n      detection_boxes, begin=[0, 0], size=[num_detections, -1])\n  detection_classes = tf.slice(\n      detection_classes, begin=[0], size=[num_detections])\n  detection_scores = tf.slice(\n      detection_scores, begin=[0], size=[num_detections])\n\n  if scale_to_absolute:\n    absolute_detection_boxlist = box_list_ops.to_absolute_coordinates(\n        box_list.BoxList(detection_boxes), image_shape[1], image_shape[2])\n    output_dict[detection_fields.detection_boxes] = (\n        absolute_detection_boxlist.get())\n  else:\n    output_dict[detection_fields.detection_boxes] = detection_boxes\n  output_dict[detection_fields.detection_classes] = detection_classes\n  output_dict[detection_fields.detection_scores] = detection_scores\n\n  if detection_fields.detection_masks in detections:\n    detection_masks = detections[detection_fields.detection_masks][0]\n    # TODO(rathodv): This should be done in model\'s postprocess\n    # function ideally.\n    detection_masks = tf.slice(\n        detection_masks, begin=[0, 0, 0], size=[num_detections, -1, -1])\n    detection_masks_reframed = ops.reframe_box_masks_to_image_masks(\n        detection_masks, detection_boxes, image_shape[1], image_shape[2])\n    detection_masks_reframed = tf.cast(\n        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n    output_dict[detection_fields.detection_masks] = detection_masks_reframed\n  if detection_fields.detection_keypoints in detections:\n    detection_keypoints = detections[detection_fields.detection_keypoints][0]\n    output_dict[detection_fields.detection_keypoints] = detection_keypoints\n    if scale_to_absolute:\n      absolute_detection_keypoints = keypoint_ops.scale(\n          detection_keypoints, image_shape[1], image_shape[2])\n      output_dict[detection_fields.detection_keypoints] = (\n          absolute_detection_keypoints)\n\n  if groundtruth:\n    if input_data_fields.groundtruth_instance_masks in groundtruth:\n      groundtruth[input_data_fields.groundtruth_instance_masks] = tf.cast(\n          groundtruth[input_data_fields.groundtruth_instance_masks], tf.uint8)\n    output_dict.update(groundtruth)\n    if scale_to_absolute:\n      groundtruth_boxes = groundtruth[input_data_fields.groundtruth_boxes]\n      absolute_gt_boxlist = box_list_ops.to_absolute_coordinates(\n          box_list.BoxList(groundtruth_boxes), image_shape[1], image_shape[2])\n      output_dict[input_data_fields.groundtruth_boxes] = (\n          absolute_gt_boxlist.get())\n    # For class-agnostic models, groundtruth classes all become 1.\n    if class_agnostic:\n      groundtruth_classes = groundtruth[input_data_fields.groundtruth_classes]\n      groundtruth_classes = tf.ones_like(groundtruth_classes, dtype=tf.int64)\n      output_dict[input_data_fields.groundtruth_classes] = groundtruth_classes\n\n  return output_dict\n\n\ndef get_eval_metric_ops_for_evaluators(evaluation_metrics,\n                                       categories,\n                                       eval_dict,\n                                       include_metrics_per_category=False):\n  """"""Returns a dictionary of eval metric ops to use with `tf.EstimatorSpec`.\n\n  Args:\n    evaluation_metrics: List of evaluation metric names. Current options are\n      \'coco_detection_metrics\' and \'coco_mask_metrics\'.\n    categories: A list of dicts, each of which has the following keys -\n        \'id\': (required) an integer id uniquely identifying this category.\n        \'name\': (required) string representing category name e.g., \'cat\', \'dog\'.\n    eval_dict: An evaluation dictionary, returned from\n      result_dict_for_single_example().\n    include_metrics_per_category: If True, include metrics for each category.\n\n  Returns:\n    A dictionary of metric names to tuple of value_op and update_op that can be\n    used as eval metric ops in tf.EstimatorSpec.\n\n  Raises:\n    ValueError: If any of the metrics in `evaluation_metric` is not\n    \'coco_detection_metrics\' or \'coco_mask_metrics\'.\n  """"""\n  evaluation_metrics = list(set(evaluation_metrics))\n\n  input_data_fields = fields.InputDataFields\n  detection_fields = fields.DetectionResultFields\n  eval_metric_ops = {}\n  for metric in evaluation_metrics:\n    if metric == \'coco_detection_metrics\':\n      coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n          categories, include_metrics_per_category=include_metrics_per_category)\n      eval_metric_ops.update(\n          coco_evaluator.get_estimator_eval_metric_ops(\n              image_id=eval_dict[input_data_fields.key],\n              groundtruth_boxes=eval_dict[input_data_fields.groundtruth_boxes],\n              groundtruth_classes=eval_dict[\n                  input_data_fields.groundtruth_classes],\n              detection_boxes=eval_dict[detection_fields.detection_boxes],\n              detection_scores=eval_dict[detection_fields.detection_scores],\n              detection_classes=eval_dict[detection_fields.detection_classes]))\n    elif metric == \'coco_mask_metrics\':\n      coco_mask_evaluator = coco_evaluation.CocoMaskEvaluator(\n          categories, include_metrics_per_category=include_metrics_per_category)\n      eval_metric_ops.update(\n          coco_mask_evaluator.get_estimator_eval_metric_ops(\n              image_id=eval_dict[input_data_fields.key],\n              groundtruth_boxes=eval_dict[input_data_fields.groundtruth_boxes],\n              groundtruth_classes=eval_dict[\n                  input_data_fields.groundtruth_classes],\n              groundtruth_instance_masks=eval_dict[\n                  input_data_fields.groundtruth_instance_masks],\n              detection_scores=eval_dict[detection_fields.detection_scores],\n              detection_classes=eval_dict[detection_fields.detection_classes],\n              detection_masks=eval_dict[detection_fields.detection_masks]))\n    else:\n      raise ValueError(\'The only evaluation metrics supported are \'\n                       \'""coco_detection_metrics"" and ""coco_mask_metrics"". \'\n                       \'Found {} in the evaluation metrics\'.format(metric))\n\n  return eval_metric_ops\n'"
models/object_detection/tensorflow/rfcn/inference/fp32/evaluator.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Detection model evaluator.\n\nThis file provides a generic evaluation method that can be used to evaluate a\nDetectionModel.\n""""""\n\nimport logging\nimport tensorflow as tf\nimport os\nimport time\n\nimport eval_util\nfrom object_detection.core import prefetcher\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.metrics import coco_evaluation\nfrom object_detection.utils import object_detection_evaluation\nfrom tensorflow.python.client import timeline\n\n# A dictionary of metric names to classes that implement the metric. The classes\n# in the dictionary must implement\n# utils.object_detection_evaluation.DetectionEvaluator interface.\nEVAL_METRICS_CLASS_DICT = {\n    \'pascal_voc_detection_metrics\':\n        object_detection_evaluation.PascalDetectionEvaluator,\n    \'weighted_pascal_voc_detection_metrics\':\n        object_detection_evaluation.WeightedPascalDetectionEvaluator,\n    \'pascal_voc_instance_segmentation_metrics\':\n        object_detection_evaluation.PascalInstanceSegmentationEvaluator,\n    \'weighted_pascal_voc_instance_segmentation_metrics\':\n        object_detection_evaluation.WeightedPascalInstanceSegmentationEvaluator,\n    \'open_images_detection_metrics\':\n        object_detection_evaluation.OpenImagesDetectionEvaluator,\n    \'coco_detection_metrics\':\n        coco_evaluation.CocoDetectionEvaluator,\n    \'coco_mask_metrics\':\n        coco_evaluation.CocoMaskEvaluator,\n}\n\nEVAL_DEFAULT_METRIC = \'pascal_voc_detection_metrics\'\n\n\ndef _extract_predictions_and_losses(model,\n                                    create_input_dict_fn,\n                                    ignore_groundtruth=False):\n  """"""Constructs tensorflow detection graph and returns output tensors.\n\n  Args:\n    model: model to perform predictions with.\n    create_input_dict_fn: function to create input tensor dictionaries.\n    ignore_groundtruth: whether groundtruth should be ignored.\n\n  Returns:\n    prediction_groundtruth_dict: A dictionary with postprocessed tensors (keyed\n      by standard_fields.DetectionResultsFields) and optional groundtruth\n      tensors (keyed by standard_fields.InputDataFields).\n    losses_dict: A dictionary containing detection losses. This is empty when\n      ignore_groundtruth is true.\n  """"""\n  input_dict = create_input_dict_fn()\n  prefetch_queue = prefetcher.prefetch(input_dict, capacity=500)\n  input_dict = prefetch_queue.dequeue()\n  original_image = tf.expand_dims(input_dict[fields.InputDataFields.image], 0)\n  preprocessed_image, true_image_shapes = model.preprocess(\n      tf.cast(original_image, dtype=tf.float32))\n  prediction_dict = model.predict(preprocessed_image, true_image_shapes)\n  detections = model.postprocess(prediction_dict, true_image_shapes)\n\n  groundtruth = None\n  losses_dict = {}\n  if not ignore_groundtruth:\n    groundtruth = {\n        fields.InputDataFields.groundtruth_boxes:\n            input_dict[fields.InputDataFields.groundtruth_boxes],\n        fields.InputDataFields.groundtruth_classes:\n            input_dict[fields.InputDataFields.groundtruth_classes],\n        fields.InputDataFields.groundtruth_area:\n            input_dict[fields.InputDataFields.groundtruth_area],\n        fields.InputDataFields.groundtruth_is_crowd:\n            input_dict[fields.InputDataFields.groundtruth_is_crowd],\n        fields.InputDataFields.groundtruth_difficult:\n            input_dict[fields.InputDataFields.groundtruth_difficult]\n    }\n    if fields.InputDataFields.groundtruth_group_of in input_dict:\n      groundtruth[fields.InputDataFields.groundtruth_group_of] = (\n          input_dict[fields.InputDataFields.groundtruth_group_of])\n    groundtruth_masks_list = None\n    if fields.DetectionResultFields.detection_masks in detections:\n      groundtruth[fields.InputDataFields.groundtruth_instance_masks] = (\n          input_dict[fields.InputDataFields.groundtruth_instance_masks])\n      groundtruth_masks_list = [\n          input_dict[fields.InputDataFields.groundtruth_instance_masks]]\n    groundtruth_keypoints_list = None\n    if fields.DetectionResultFields.detection_keypoints in detections:\n      groundtruth[fields.InputDataFields.groundtruth_keypoints] = (\n          input_dict[fields.InputDataFields.groundtruth_keypoints])\n      groundtruth_keypoints_list = [\n          input_dict[fields.InputDataFields.groundtruth_keypoints]]\n    label_id_offset = 1\n    model.provide_groundtruth(\n        [input_dict[fields.InputDataFields.groundtruth_boxes]],\n        [tf.one_hot(input_dict[fields.InputDataFields.groundtruth_classes]\n                    - label_id_offset, depth=model.num_classes)],\n        groundtruth_masks_list, groundtruth_keypoints_list)\n    losses_dict.update(model.loss(prediction_dict, true_image_shapes))\n\n  result_dict = eval_util.result_dict_for_single_example(\n      original_image,\n      input_dict[fields.InputDataFields.source_id],\n      detections,\n      groundtruth,\n      class_agnostic=(\n          fields.DetectionResultFields.detection_classes not in detections),\n      scale_to_absolute=True)\n  return result_dict, losses_dict\n\n\ndef get_evaluators(eval_config, categories):\n  """"""Returns the evaluator class according to eval_config, valid for categories.\n\n  Args:\n    eval_config: evaluation configurations.\n    categories: a list of categories to evaluate.\n  Returns:\n    An list of instances of DetectionEvaluator.\n\n  Raises:\n    ValueError: if metric is not in the metric class dictionary.\n  """"""\n  eval_metric_fn_keys = eval_config.metrics_set\n  if not eval_metric_fn_keys:\n    eval_metric_fn_keys = [EVAL_DEFAULT_METRIC]\n  evaluators_list = []\n  for eval_metric_fn_key in eval_metric_fn_keys:\n    if eval_metric_fn_key not in EVAL_METRICS_CLASS_DICT:\n      raise ValueError(\'Metric not found: {}\'.format(eval_metric_fn_key))\n    evaluators_list.append(\n        EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](categories=categories))\n  return evaluators_list\n\n\ndef evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,\n             checkpoint_dir, eval_dir, graph_hook_fn=None, evaluator_list=None, intra_op=0, inter_op=0):\n  """"""Evaluation function for detection models.\n\n  Args:\n    create_input_dict_fn: a function to create a tensor input dictionary.\n    create_model_fn: a function that creates a DetectionModel.\n    eval_config: a eval_pb2.EvalConfig protobuf.\n    categories: a list of category dictionaries. Each dict in the list should\n                have an integer \'id\' field and string \'name\' field.\n    checkpoint_dir: directory to load the checkpoints to evaluate from.\n    eval_dir: directory to write evaluation metrics summary to.\n    graph_hook_fn: Optional function that is called after the training graph is\n      completely built. This is helpful to perform additional changes to the\n      training graph such as optimizing batchnorm. The function should modify\n      the default graph.\n    evaluator_list: Optional list of instances of DetectionEvaluator. If not\n      given, this list of metrics is created according to the eval_config.\n\n  Returns:\n    metrics: A dictionary containing metric names and values from the latest\n      run.\n  """"""\n\n  model = create_model_fn()\n\n  if eval_config.ignore_groundtruth and not eval_config.export_path:\n    logging.fatal(\'If ignore_groundtruth=True then an export_path is \'\n                  \'required. Aborting!!!\')\n\n  tensor_dict, losses_dict = _extract_predictions_and_losses(\n      model=model,\n      create_input_dict_fn=create_input_dict_fn,\n      ignore_groundtruth=eval_config.ignore_groundtruth)\n\n  def _process_batch(tensor_dict, sess, batch_index, counters,\n                     losses_dict=None):\n    """"""Evaluates tensors in tensor_dict, losses_dict and visualizes examples.\n\n    This function calls sess.run on tensor_dict, evaluating the original_image\n    tensor only on the first K examples and visualizing detections overlaid\n    on this original_image.\n\n    Args:\n      tensor_dict: a dictionary of tensors\n      sess: tensorflow session\n      batch_index: the index of the batch amongst all batches in the run.\n      counters: a dictionary holding \'success\' and \'skipped\' fields which can\n        be updated to keep track of number of successful and failed runs,\n        respectively.  If these fields are not updated, then the success/skipped\n        counter values shown at the end of evaluation will be incorrect.\n      losses_dict: Optional dictonary of scalar loss tensors.\n\n    Returns:\n      result_dict: a dictionary of numpy arrays\n      result_losses_dict: a dictionary of scalar losses. This is empty if input\n        losses_dict is None.\n    """"""\n    try:\n      if not losses_dict:\n        losses_dict = {}\n      trace = False\n      if batch_index == 0 and trace:\n        run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n        run_metadata = tf.compat.v1.RunMetadata()\n      else:\n        run_options = None\n        run_metadata = None\n      start_time = time.time()\n      result_dict, result_losses_dict = sess.run(\n          [tensor_dict, losses_dict], options=run_options, run_metadata=run_metadata)\n      if (batch_index % 100 == 0):\n        logging.info(\'Step %d: %.3f sec\', batch_index, time.time() - start_time)\n      if batch_index == 0 and trace:\n        trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n        dir = \'logs\'\n        if not os.path.exists(dir):\n          os.makedirs(dir)\n        with open(dir + \'/rfcn-timeline-\' + time.strftime(""%Y%m%d-%H%M%S"") + \'.json\', \'w\') as file:\n          file.write(trace.generate_chrome_trace_format(show_memory=False))\n      counters[\'success\'] += 1\n    except tf.errors.InvalidArgumentError:\n      logging.info(\'Skipping image\')\n      counters[\'skipped\'] += 1\n      return {}, {}\n    global_step = tf.compat.v1.train.global_step(sess, tf.compat.v1.train.get_global_step())\n    if batch_index < eval_config.num_visualizations:\n      tag = \'image-{}\'.format(batch_index)\n      eval_util.visualize_detection_results(\n          result_dict,\n          tag,\n          global_step,\n          categories=categories,\n          summary_dir=eval_dir,\n          export_dir=eval_config.visualization_export_dir,\n          show_groundtruth=eval_config.visualize_groundtruth_boxes,\n          groundtruth_box_visualization_color=eval_config.\n          groundtruth_box_visualization_color,\n          min_score_thresh=eval_config.min_score_threshold,\n          max_num_predictions=eval_config.max_num_boxes_to_visualize,\n          skip_scores=eval_config.skip_scores,\n          skip_labels=eval_config.skip_labels,\n          keep_image_id_for_visualization_export=eval_config.\n          keep_image_id_for_visualization_export)\n    return result_dict, result_losses_dict\n\n  variables_to_restore = tf.compat.v1.global_variables()\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  variables_to_restore.append(global_step)\n\n  if graph_hook_fn: graph_hook_fn()\n\n  if eval_config.use_moving_averages:\n    variable_averages = tf.train.ExponentialMovingAverage(0.0)\n    variables_to_restore = variable_averages.variables_to_restore()\n  saver = tf.compat.v1.train.Saver(variables_to_restore)\n\n  def _restore_latest_checkpoint(sess):\n    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n    saver.restore(sess, latest_checkpoint)\n\n  if not evaluator_list:\n    evaluator_list = get_evaluators(eval_config, categories)\n\n  metrics = eval_util.repeated_checkpoint_run(\n      tensor_dict=tensor_dict,\n      summary_dir=eval_dir,\n      evaluators=evaluator_list,\n      batch_processor=_process_batch,\n      checkpoint_dirs=[checkpoint_dir],\n      variables_to_restore=None,\n      restore_fn=_restore_latest_checkpoint,\n      num_batches=eval_config.num_examples,\n      eval_interval_secs=eval_config.eval_interval_secs,\n      max_number_of_evaluations=(1 if eval_config.ignore_groundtruth else\n                                 eval_config.max_evals\n                                 if eval_config.max_evals else None),\n      master=eval_config.eval_master,\n      save_graph=eval_config.save_graph,\n      save_graph_dir=(eval_dir if eval_config.save_graph else \'\'),\n      losses_dict=losses_dict)\n\n  return metrics\n'"
models/object_detection/tensorflow/rfcn/inference/fp32/run_rfcn_inference.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nimport numpy as np\nimport os\nimport six.moves.urllib as urllib\nimport sys\nimport tarfile\nimport tensorflow as tf\nimport zipfile\nimport subprocess\n\nfrom collections import defaultdict\nfrom io import StringIO\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nimport time\nimport argparse\nfrom tensorflow.python.client import timeline\nimport importlib\n\nclass RFCNRunner:\n  \'\'\'Add code here to detect the environment and set necessary variables before launching the model\'\'\'\n  args=None\n  custom_args=None\n  RESEARCH_DIR = \'research\'\n  OBJ_DETECTION_DIR = \'object_detection\'\n  DATA_DIR = \'data\'\n  label_map_file = \'mscoco_label_map.pbtxt\'\n  research_dir = \'\'\n  NUM_CLASSES = 90\n  detection_graph = None\n  test_image_paths = []\n  TEST_IMG_FILE = ""000000578871.jpg""\n  DEFAULT_INTEROP_THREADS = 2\n  RFCN_OUTPUTS = [\'num_detections\', \'detection_boxes\', \'detection_scores\',\n              \'detection_classes\', \'detection_masks\']\n  STEP_SIZE = 10\n  MAX_STEPS = 5000\n  label_map_util = None\n  vis_util = None\n  # Size, in inches, of the output images.\n  IMAGE_SIZE = (12, 8)\n\n  def __init__(self, args):\n    self.args = args\n    self.parse_args()\n    self.log(\'Received these standard args: {}\'.format(self.args))\n\n  def parse_args(self):\n    parser = argparse.ArgumentParser()\n    mutex_group = parser.add_mutually_exclusive_group()\n    mutex_group.add_argument(\'-x\', \'--number_of_steps\', help=\'Run for n number of steps\', type=int, default=None)\n    mutex_group.add_argument(\'-z\', \'--visualize\', help=\'Whether to visulize the output image\', action=\'store_true\' )\n    parser.add_argument(\'-v\', \'--verbose\', help=\'Print some useful info.\', action=\'store_true\' )\n    parser.add_argument(\'-t\', \'--timeline\', help=\'Output file name for TF timeline\', type=str, default=None)\n    parser.add_argument(\'-e\', \'--evaluate_tensor\', help=\'Full tensor name to evaluate\', type=str, default=None)\n    parser.add_argument(\'-p\', \'--print_accuracy\', help=\'Print accuracy results\', action=\'store_true\')\n    parser.add_argument(\'-g\', \'--input_graph\', help=\'The input frozen graph pb file\', dest=\'input_graph\', required=True, default=None)\n    parser.add_argument(\'-d\', \'--data_location\', help=\'The location of the image data to be analyzed.\', dest=\'data_location\', default=None, required=True)\n    parser.add_argument(\'-m\', \'--tensorflow-models-path\',\n        help=\'Path to the tensorflow-models directory (or clone of github.com/tensorflow/models\',\n        dest=\'tf_models_path\', default=None, required=True)\n    parser.add_argument(\n        \'--num-inter-threads\', dest=\'num_inter_threads\',\n        help=\'number threads across operators\',\n        type=int, default=2)\n    parser.add_argument(\n        \'--num-intra-threads\', dest=\'num_intra_threads\',\n        help=\'number threads for an operator\',\n        type=int, default=56)\n    self.args = parser.parse_args()\n    self.validate_args()\n    self.finish_import()\n\n  def log(self, msg):\n    if self.args.verbose: print(msg)\n\n  def validate_args(self):\n    self.log(\'Validating Args...\')\n    self.research_dir = os.path.join(self.args.tf_models_path, self.RESEARCH_DIR)\n    if not ( self.args.data_location and\n        os.path.exists(os.path.join(self.args.data_location, self.TEST_IMG_FILE))):\n      raise ValueError (""Unable to locate images for evaluation at {}"".format(self.args.data_location))\n    if os.path.isdir(self.research_dir):\n      # List of the strings that is used to add correct label for each box.\n      self.label_map_file = os.path.join(self.research_dir,\n                                          self.OBJ_DETECTION_DIR,\n                                          self.DATA_DIR,\n                                          self.label_map_file)\n      if not os.path.exists(self.label_map_file):\n        raise ValueError (""Unable to locate label map file at {}"".format(self.label_map_file))\n    else:\n      raise ValueError (""{} is not a valid path to the TensorFlow models."".format(self.args.tf_models_path))\n\n    if not os.path.exists(self.args.input_graph):\n      raise ValueError(""Unable to find the input graph protobuf file: {}"".format(self.args.input_graph))\n\n  def finish_import(self):\n    # This is needed since the notebook is stored in the object_detection folder.\n    sys.path.append(self.research_dir)\n    # This is needed to display the images.\n    if (self.args.visualize and self.args.evaluate_tensor is None):\n      from IPython import get_ipython\n      get_ipython().run_line_magic(\'matplotlib\', \'tk\')\n    self.label_map_util = importlib.import_module(\'..label_map_util\', package=\'object_detection.utils.label_map_util\')\n    self.vis_util = importlib.import_module(\'..visualization_utils\', package=\'object_detection.utils.visualization_utils\')\n\n  def run(self):\n      self.log(""Running performance test"")\n      self.read_graph()\n      self.get_image_paths()\n      #self.load_label_map()\n      # Actual detection.\n      output_dict, image_np = self.run_inference(self.detection_graph)\n      self.visualize(output_dict, image_np)\n\n  def visualize(self, output_dict, image_np):\n    # Visualization of the results of a detection.\n    if (self.args.visualize and\n        self.args.evaluate_tensor is None and\n        self.category_index and\n        output_dict and\n        image_np ):\n      self.vis_util.visualize_boxes_and_labels_on_image_array(\n          image_np,\n          output_dict[\'detection_boxes\'],\n          output_dict[\'detection_classes\'],\n          output_dict[\'detection_scores\'],\n          self.category_index,\n          instance_masks=output_dict.get(\'detection_masks\'),\n          use_normalized_coordinates=True,\n          line_thickness=8)\n      plt.figure(figsize=self.IMAGE_SIZE)\n      plt.imshow(image_np)\n\n  def read_graph(self):\n    self.detection_graph = tf.Graph()\n    with self.detection_graph.as_default():\n      od_graph_def = tf.compat.v1.GraphDef()\n      with tf.io.gfile.GFile(self.args.input_graph, \'rb\') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name=\'\')\n\n\n  def get_image_paths(self):\n    if (self.args.visualize):\n      self.test_image_paths = [os.path.join(self.args.data_location, self.TEST_IMG_FILE)]\n    else:\n      self.test_image_paths = []\n      for root, dirs, files in os.walk(self.args.data_location):\n        for file in files:\n          self.test_image_paths.append(os.path.join(self.args.data_location, file))\n\n  def load_label_map(self):\n    label_map = self.label_map_util.load_labelmap(self.label_map_file)\n    categories = self.label_map_util.convert_label_map_to_categories(\n        label_map, max_num_classes=self.NUM_CLASSES, use_display_name=True)\n    if (self.args.visualize and self.args.evaluate_tensor is None):\n      self.category_index = self.label_map_util.create_category_index(categories)\n\n  def load_image_into_numpy_array(self, image):\n    (im_width, im_height) = image.size\n    if image.mode == \'L\':\n      np_image = np.array(image.getdata()).reshape(\n          (im_height, im_width)).astype(np.uint8)\n      return np.stack((np_image,)*3, -1)\n    else:\n      return np.array(image.getdata()).reshape(\n          (im_height, im_width, 3)).astype(np.uint8)\n\n  def run_inference(self,graph):\n    sess_config = tf.compat.v1.ConfigProto()\n    sess_config.intra_op_parallelism_threads = self.args.num_intra_threads\n    sess_config.inter_op_parallelism_threads = self.args.num_inter_threads\n    with self.detection_graph.as_default():\n      with tf.compat.v1.Session(config=sess_config) as sess:\n        # Get handles to input and output tensors\n        tensor_dict = {}\n        if not self.args.evaluate_tensor:\n          ops = tf.compat.v1.get_default_graph().get_operations()\n          all_tensor_names = {output.name for op in ops for output in op.outputs}\n          for key in self.RFCN_OUTPUTS:\n            tensor_name = key + \':0\'\n            if tensor_name in all_tensor_names:\n              tensor_dict[key] = tf.compat.v1.get_default_graph().get_tensor_by_name(\n                  tensor_name)\n        else:\n          our_op = tf.compat.v1.get_default_graph().get_operation_by_name(self.args.evaluate_tensor)\n          tensor_names = our_op.outputs\n          list_ops = []\n          for i, tensor in enumerate(tensor_names):\n            list_ops.append(tensor.name)\n          tensor_dict[self.args.evaluate_tensor] = list_ops\n\n        run_options = None\n        run_metadata = None\n        if self.args.timeline:\n          run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n          run_metadata = tf.compat.v1.RunMetadata()\n\n        total_duration = 0\n        for index, image_path in enumerate(self.test_image_paths):\n          image = Image.open(image_path)\n          # the array based representation of the image will be used later in order to prepare the\n          # result image with boxes and labels on it.\n          image_np = self.load_image_into_numpy_array(image)\n          image_tensor = tf.compat.v1.get_default_graph().get_tensor_by_name(\'image_tensor:0\')\n\n          # Run inference\n          start_time = time.time()\n          #if self.args.timeline:\n          output_dict = sess.run(tensor_dict,\n                       feed_dict={image_tensor: np.expand_dims(image_np, 0)},\n                       options=run_options, run_metadata=run_metadata)\n          # else:\n          #   output_dict = sess.run(tensor_dict,\n          #                         feed_dict={image_tensor: np.expand_dims(image_np, 0)})\n          step_duration = time.time() - start_time\n          total_duration = total_duration + step_duration\n\n          if (self.args.visualize):\n            if index == 0:\n              print (\'Avg. Duration per Step:\' + str(total_duration / 1))\n          else:\n            if (index % self.STEP_SIZE == 0):\n              print (\'Step \' + str(index) + \': \' + str(step_duration) + \' seconds\')\n            if index == self.MAX_STEPS - 1:\n              print (\'Avg. Duration per Step:\' + str(total_duration / self.MAX_STEPS))\n\n          if self.args.number_of_steps and index == (self.args.number_of_steps - 1):\n              print (\'Avg. Duration per Step:\' +\n                    str(total_duration / self.args.number_of_steps))\n              break\n\n          if self.args.timeline:\n            trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n            with open(\'tl-\' + time.strftime(""%Y%m%d-%H%M%S"") + \'-\' + self.args.timeline, \'w\') as file:\n              file.write(trace.generate_chrome_trace_format(show_memory=False))\n\n          if self.args.evaluate_tensor:\n            for tensor in output_dict[self.args.evaluate_tensor]:\n              print (tensor.shape)\n            return None, None\n\n          # all outputs are float32 numpy arrays, so convert types as appropriate\n          output_dict[\'num_detections\'] = int(output_dict[\'num_detections\'][0])\n          output_dict[\'detection_classes\'] = output_dict[\n              \'detection_classes\'][0].astype(np.uint8)\n          output_dict[\'detection_boxes\'] = output_dict[\'detection_boxes\'][0]\n          output_dict[\'detection_scores\'] = output_dict[\'detection_scores\'][0]\n\n          if (self.args.print_accuracy):\n            print (\'num_detections:\\n\' + str(output_dict[\'num_detections\']))\n            print (\'detection_classes:\\n\' + str(output_dict[\'detection_classes\']))\n            print (\'detection_boxes:\\n\' + str(output_dict[\'detection_boxes\']))\n            print (\'detection_scores:\\n\' + str(output_dict[\'detection_scores\']))\n\n          if \'detection_masks\' in output_dict:\n            output_dict[\'detection_masks\'] = output_dict[\'detection_masks\'][0]\n    return output_dict, image_np\n\nif __name__ == ""__main__"":\n  rr = RFCNRunner(sys.argv)\n  rr.run()\n'"
models/object_detection/tensorflow/rfcn/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/object_detection/tensorflow/rfcn/inference/int8/run_rfcn_inference.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nimport numpy as np\nimport os\nimport six.moves.urllib as urllib\nimport sys\nimport tarfile\nimport tensorflow as tf\nimport zipfile\nimport subprocess\n\nfrom collections import defaultdict\nfrom io import StringIO\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nimport time\nimport argparse\nfrom tensorflow.python.client import timeline\nimport importlib\n\nclass RFCNRunner:\n  \'\'\'Add code here to detect the environment and set necessary variables before launching the model\'\'\'\n  args=None\n  custom_args=None\n  RESEARCH_DIR = \'research\'\n  OBJ_DETECTION_DIR = \'object_detection\'\n  DATA_DIR = \'data\'\n  label_map_file = \'mscoco_label_map.pbtxt\'\n  research_dir = \'\'\n  NUM_CLASSES = 90\n  detection_graph = None\n  test_image_paths = []\n  TEST_IMG_FILE = ""000000578871.jpg""\n  DEFAULT_INTEROP_THREADS = 2\n  RFCN_OUTPUTS = [\'num_detections\', \'detection_boxes\', \'detection_scores\',\n              \'detection_classes\', \'detection_masks\']\n  STEP_SIZE = 10\n  MAX_STEPS = 5000\n  label_map_util = None\n  vis_util = None\n  # Size, in inches, of the output images.\n  IMAGE_SIZE = (12, 8)\n\n  def __init__(self, args):\n    self.args = args\n    self.parse_args()\n    self.log(\'Received these standard args: {}\'.format(self.args))\n\n  def parse_args(self):\n    parser = argparse.ArgumentParser()\n    mutex_group = parser.add_mutually_exclusive_group()\n    mutex_group.add_argument(\'-x\', \'--number_of_steps\', help=\'Run for n number of steps\', type=int, default=None)\n    mutex_group.add_argument(\'-z\', \'--visualize\', help=\'Whether to visulize the output image\', action=\'store_true\' )\n    parser.add_argument(\'-v\', \'--verbose\', help=\'Print some useful info.\', action=\'store_true\' )\n    parser.add_argument(\'-t\', \'--timeline\', help=\'Output file name for TF timeline\', type=str, default=None)\n    parser.add_argument(\'-e\', \'--evaluate_tensor\', help=\'Full tensor name to evaluate\', type=str, default=None)\n    parser.add_argument(\'-p\', \'--print_accuracy\', help=\'Print accuracy results\', action=\'store_true\')\n    parser.add_argument(\'-g\', \'--input_graph\', help=\'The input frozen graph pb file\', dest=\'input_graph\', required=True, default=None)\n    parser.add_argument(\'-d\', \'--data_location\', help=\'The location of the image data to be analyzed.\', dest=\'data_location\', default=None, required=True)\n    parser.add_argument(\'-m\', \'--tensorflow-models-path\', \n        help=\'Path to the tensorflow-models directory (or clone of github.com/tensorflow/models\', \n        dest=\'tf_models_path\', default=None, required=True)\n    parser.add_argument(\n        \'--num-inter-threads\', dest=\'num_inter_threads\',\n        help=\'number threads across operators\',\n        type=int, default=2)\n    parser.add_argument(\n        \'--num-intra-threads\', dest=\'num_intra_threads\',\n        help=\'number threads for an operator\',\n        type=int, default=56)\n    self.args = parser.parse_args()\n    self.validate_args()\n    self.finish_import()\n\n  def log(self, msg):\n    if self.args.verbose: print(msg)\n\n  def validate_args(self):\n    self.log(\'Validating Args...\')\n    self.research_dir = os.path.join(self.args.tf_models_path, self.RESEARCH_DIR)\n    if not ( self.args.data_location and \n        os.path.exists(os.path.join(self.args.data_location, self.TEST_IMG_FILE))):\n      raise ValueError (""Unable to locate images for evaluation at {}"".format(self.args.data_location))\n    if os.path.isdir(self.research_dir):\n      # List of the strings that is used to add correct label for each box.\n      self.label_map_file = os.path.join(self.research_dir, \n                                          self.OBJ_DETECTION_DIR, \n                                          self.DATA_DIR, \n                                          self.label_map_file)\n      if not os.path.exists(self.label_map_file):\n        raise ValueError (""Unable to locate label map file at {}"".format(self.label_map_file))\n    else:\n      raise ValueError (""{} is not a valid path to the TensorFlow models."".format(self.args.tf_models_path))\n    \n    if not os.path.exists(self.args.input_graph):\n      raise ValueError(""Unable to find the input graph protobuf file: {}"".format(self.args.input_graph))\n\n  def finish_import(self):\n    # This is needed since the notebook is stored in the object_detection folder.\n    sys.path.append(self.research_dir)\n    # This is needed to display the images.\n    if (self.args.visualize and self.args.evaluate_tensor is None):\n      from IPython import get_ipython\n      get_ipython().run_line_magic(\'matplotlib\', \'tk\')\n    self.label_map_util = importlib.import_module(\'..label_map_util\', package=\'object_detection.utils.label_map_util\')\n    self.vis_util = importlib.import_module(\'..visualization_utils\', package=\'object_detection.utils.visualization_utils\')\n\n  def run(self):\n      self.log(""Running performance test"")\n      self.read_graph()\n      self.get_image_paths()\n      #self.load_label_map()\n      # Actual detection.\n      output_dict, image_np = self.run_inference(self.detection_graph)\n      self.visualize(output_dict, image_np)\n\n  def visualize(self, output_dict, image_np):\n    # Visualization of the results of a detection.\n    if (self.args.visualize and \n        self.args.evaluate_tensor is None and \n        self.category_index and\n        output_dict and\n        image_np ):\n      self.vis_util.visualize_boxes_and_labels_on_image_array(\n          image_np,\n          output_dict[\'detection_boxes\'],\n          output_dict[\'detection_classes\'],\n          output_dict[\'detection_scores\'],\n          self.category_index,\n          instance_masks=output_dict.get(\'detection_masks\'),\n          use_normalized_coordinates=True,\n          line_thickness=8)\n      plt.figure(figsize=self.IMAGE_SIZE)\n      plt.imshow(image_np)\n\n  def read_graph(self):\n    self.detection_graph = tf.Graph()\n    with self.detection_graph.as_default():\n      od_graph_def = tf.compat.v1.GraphDef()\n      with tf.io.gfile.GFile(self.args.input_graph, \'rb\') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name=\'\')\n\n\n  def get_image_paths(self):\n    if (self.args.visualize):\n      self.test_image_paths = [os.path.join(self.args.data_location, self.TEST_IMG_FILE)]\n    else:\n      self.test_image_paths = []\n      for root, dirs, files in os.walk(self.args.data_location):\n        for file in files:\n          self.test_image_paths.append(os.path.join(self.args.data_location, file))\n\n  def load_label_map(self):\n    label_map = self.label_map_util.load_labelmap(self.label_map_file)\n    categories = self.label_map_util.convert_label_map_to_categories(\n        label_map, max_num_classes=self.NUM_CLASSES, use_display_name=True)\n    if (self.args.visualize and self.args.evaluate_tensor is None):\n      self.category_index = self.label_map_util.create_category_index(categories)\n\n  def load_image_into_numpy_array(self, image):\n    (im_width, im_height) = image.size\n    if image.mode == \'L\':\n      np_image = np.array(image.getdata()).reshape(\n          (im_height, im_width)).astype(np.uint8)\n      return np.stack((np_image,)*3, -1)\n    else:\n      return np.array(image.getdata()).reshape(\n          (im_height, im_width, 3)).astype(np.uint8)\n\n  def run_inference(self,graph):\n    sess_config = tf.compat.v1.ConfigProto()\n    sess_config.intra_op_parallelism_threads = self.args.num_intra_threads\n    sess_config.inter_op_parallelism_threads = self.args.num_inter_threads\n    with self.detection_graph.as_default():\n      with tf.compat.v1.Session(config=sess_config) as sess:\n        # Get handles to input and output tensors\n        tensor_dict = {}\n        if not self.args.evaluate_tensor:\n          ops = tf.compat.v1.get_default_graph().get_operations()\n          all_tensor_names = {output.name for op in ops for output in op.outputs}\n          for key in self.RFCN_OUTPUTS:\n            tensor_name = key + \':0\'\n            if tensor_name in all_tensor_names:\n              tensor_dict[key] = tf.compat.v1.get_default_graph().get_tensor_by_name(\n                  tensor_name)\n        else:\n          our_op = tf.compat.v1.get_default_graph().get_operation_by_name(self.args.evaluate_tensor)\n          tensor_names = our_op.outputs\n          list_ops = []\n          for i, tensor in enumerate(tensor_names):\n            list_ops.append(tensor.name)\n          tensor_dict[self.args.evaluate_tensor] = list_ops\n\n        run_options = None\n        run_metadata = None\n        if self.args.timeline:\n          run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n          run_metadata = tf.compat.v1.RunMetadata()\n\n        total_duration = 0\n        for index, image_path in enumerate(self.test_image_paths):\n          image = Image.open(image_path)\n          # the array based representation of the image will be used later in order to prepare the\n          # result image with boxes and labels on it.\n          image_np = self.load_image_into_numpy_array(image)\n          image_tensor = tf.compat.v1.get_default_graph().get_tensor_by_name(\'image_tensor:0\')\n\n          # Run inference\n          start_time = time.time()\n          #if self.args.timeline:\n          output_dict = sess.run(tensor_dict,\n                       feed_dict={image_tensor: np.expand_dims(image_np, 0)},\n                       options=run_options, run_metadata=run_metadata)\n          # else:\n          #   output_dict = sess.run(tensor_dict,\n          #                         feed_dict={image_tensor: np.expand_dims(image_np, 0)})\n          step_duration = time.time() - start_time\n          total_duration = total_duration + step_duration\n\n          if (self.args.visualize):\n            if index == 0:\n              print (\'Avg. Duration per Step:\' + str(total_duration / 1))\n          else:\n            if (index % self.STEP_SIZE == 0):\n              print (\'Step \' + str(index) + \': \' + str(step_duration) + \' seconds\')\n            if index == self.MAX_STEPS - 1:\n              print (\'Avg. Duration per Step:\' + str(total_duration / self.MAX_STEPS))\n\n          if self.args.number_of_steps and index == (self.args.number_of_steps - 1):\n              print (\'Avg. Duration per Step:\' +\n                    str(total_duration / self.args.number_of_steps))\n              break\n\n          if self.args.timeline:\n            trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n            with open(\'tl-\' + time.strftime(""%Y%m%d-%H%M%S"") + \'-\' + self.args.timeline, \'w\') as file:\n              file.write(trace.generate_chrome_trace_format(show_memory=False))\n\n          if self.args.evaluate_tensor:\n            for tensor in output_dict[self.args.evaluate_tensor]:\n              print (tensor.shape)\n            return None, None\n\n          # all outputs are float32 numpy arrays, so convert types as appropriate\n          output_dict[\'num_detections\'] = int(output_dict[\'num_detections\'][0])\n          output_dict[\'detection_classes\'] = output_dict[\n              \'detection_classes\'][0].astype(np.uint8)\n          output_dict[\'detection_boxes\'] = output_dict[\'detection_boxes\'][0]\n          output_dict[\'detection_scores\'] = output_dict[\'detection_scores\'][0]\n\n          if (self.args.print_accuracy):\n            print (\'num_detections:\\n\' + str(output_dict[\'num_detections\']))\n            print (\'detection_classes:\\n\' + str(output_dict[\'detection_classes\']))\n            print (\'detection_boxes:\\n\' + str(output_dict[\'detection_boxes\']))\n            print (\'detection_scores:\\n\' + str(output_dict[\'detection_scores\']))\n\n          if \'detection_masks\' in output_dict:\n            output_dict[\'detection_masks\'] = output_dict[\'detection_masks\'][0]\n    return output_dict, image_np\n\nif __name__ == ""__main__"":\n  rr = RFCNRunner(sys.argv)\n  rr.run()\n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/fp32/infer_detections.py,28,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nfrom __future__ import division\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nfrom tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\nfrom tensorflow.python.framework import dtypes\nimport time\n\nfrom argparse import ArgumentParser\nfrom inference.coco_detection_evaluator import CocoDetectionEvaluator\nfrom inference.coco_label_map import category_map\n\nIMAGE_SIZE = 300\nCOCO_NUM_VAL_IMAGES = 4952\n\nimport os\n\nimport numpy as np\n\ndef parse_and_preprocess(serialized_example):\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.compat.v1.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/object/class/text\': tf.compat.v1.VarLenFeature(dtype=tf.string),\n      \'image/source_id\': tf.compat.v1.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n  }\n  sparse_float32 = tf.compat.v1.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.compat.v1.parse_single_example(serialized_example, feature_map)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(bbox, [0, 2, 1])\n\n  encoded_image = features[\'image/encoded\']\n  image_tensor = tf.image.decode_image(encoded_image, channels=3)\n  image_tensor.set_shape([None, None, 3])\n\n  label = features[\'image/object/class/text\'].values\n\n  image_id = features[\'image/source_id\']\n\n  return image_tensor, bbox[0], label, image_id\n\nclass model_infer:\n\n  def __init__(self):\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--inter-op-parallelism-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--intra-op-parallelism-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph.\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n\n    arg_parser.add_argument(\'-i\', ""--iter"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'total_iter\', default=1000, type=int)\n\n    arg_parser.add_argument(\'-w\', ""--warmup_iter"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'warmup_iter\', default=200, type=int)\n\n    # parse the arguments\n    self.args = arg_parser.parse_args()\n\n    self.config = tf.compat.v1.ConfigProto()\n    self.config.intra_op_parallelism_threads = self.args.num_intra_threads\n    self.config.inter_op_parallelism_threads = self.args.num_inter_threads\n    self.config.use_per_session_threads = 1\n\n    self.input_layer = \'image_tensor\'\n    self.output_layers = [\'num_detections\', \'detection_boxes\', \'detection_scores\', \'detection_classes\']\n    self.load_graph()\n\n    if self.args.batch_size == -1:\n      self.args.batch_size = 1\n\n    \n    self.input_tensor = self.infer_graph.get_tensor_by_name(self.input_layer + "":0"")\n    self.output_tensors = [self.infer_graph.get_tensor_by_name(x + "":0"") for x in self.output_layers]\n  \n    self.category_map_reverse = {v : k for k, v in category_map.items()}\n\n  def build_data_sess(self):\n    data_graph = tf.Graph()\n    with data_graph.as_default():\n      self.input_images, self.bbox, self.label, self.image_id = self.get_input()\n    self.data_sess = tf.compat.v1.Session(graph=data_graph, config=self.config)\n\n  def load_graph(self):\n    print(\'load graph from: \' + self.args.input_graph)\n\n    self.infer_graph = tf.Graph()\n    with self.infer_graph.as_default():\n      graph_def = tf.compat.v1.GraphDef()\n      with tf.compat.v1.gfile.FastGFile(self.args.input_graph, \'rb\') as input_file:\n        input_graph_content = input_file.read()\n        graph_def.ParseFromString(input_graph_content)\n      output_graph = optimize_for_inference(graph_def, [self.input_layer],\n                              self.output_layers, dtypes.uint8.as_datatype_enum, False)\n      tf.import_graph_def(output_graph, name=\'\')\n\n  def run_benchmark(self):\n    if self.args.data_location:\n      print(""Inference with real data."")\n    else:\n      print(""Inference with dummy data."")\n          \n    with tf.compat.v1.Session(graph=self.infer_graph, config=self.config) as sess:\n      \n      if self.args.data_location:\n        self.build_data_sess()\n      else:\n        input_images = sess.run(tf.random.truncated_normal(\n          [self.args.batch_size, IMAGE_SIZE, IMAGE_SIZE, 3],\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\'))\n\n      total_iter = self.args.total_iter\n      warmup_iter = self.args.warmup_iter\n      ttime = 0.0\n\n      print(\'total iteration is {0}\'.format(str(total_iter)))\n      print(\'warm up iteration is {0}\'.format(str(warmup_iter)))\n\n      for step in range(total_iter):\n        start_time = time.time()\n        if self.args.data_location:\n          input_images = self.data_sess.run([self.input_images])\n          input_images = input_images[0]\n        _ = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        end_time = time.time()\n\n        duration = end_time - start_time\n        if (step + 1) % 10 == 0:\n          print(\'steps = {0}, {1} sec\'.format(str(step), str(duration)))\n        \n        if step + 1 > warmup_iter:\n          ttime += duration\n        \n      total_batches = total_iter - warmup_iter\n      print (\'Batchsize: {0}\'.format(str(self.args.batch_size)))\n      print (\'Time spent per BATCH: {0:10.4f} ms\'.format(ttime / total_batches * 1000))\n      print (\'Total samples/sec: {0:10.4f} samples/s\'.format(total_batches * self.args.batch_size / ttime))\n  \n\n  def get_input(self):\n    tfrecord_paths = [self.args.data_location]\n    ds = tf.data.TFRecordDataset.list_files(tfrecord_paths)\n\n    ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=28, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n    ds = ds.prefetch(buffer_size=10000)\n    ds = ds.apply(\n        map_and_batch(\n          map_func=parse_and_preprocess,\n          batch_size=self.args.batch_size,\n          num_parallel_batches=28,\n          num_parallel_calls=None))\n    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n    images, bbox, label, image_id = ds_iterator.get_next()\n\n    return images, bbox, label, image_id\n\n  def accuracy_check(self):\n    print(""Inference for accuracy check."")\n    self.build_data_sess()\n    evaluator = CocoDetectionEvaluator()\n    with tf.compat.v1.Session(graph=self.infer_graph, config=self.config) as sess:\n      iter = 0\n      while True:\n        print(\'Run {0} iter\'.format(iter))\n        iter += 1\n        input_images, bbox, label, image_id = self.data_sess.run([self.input_images, self.bbox, self.label, self.image_id])\n        ground_truth = {}\n        ground_truth[\'boxes\'] = np.asarray(bbox[0])\n        label = [x if type(x) == \'str\' else x.decode(\'utf-8\') for x in label[0]]\n        ground_truth[\'classes\'] = np.asarray([self.category_map_reverse[x] for x in label])\n        image_id = image_id[0] if type(image_id[0]) == \'str\' else image_id[0].decode(\'utf-8\')\n        evaluator.add_single_ground_truth_image_info(image_id, ground_truth)\n        num, boxes, scores, labels = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        detection = {}\n        num = int(num[0])\n        detection[\'boxes\'] = np.asarray(boxes[0])[0:num]\n        detection[\'scores\'] = np.asarray(scores[0])[0:num]\n        detection[\'classes\'] = np.asarray(labels[0])[0:num]\n        evaluator.add_single_detected_image_info(image_id, detection)\n        if iter * self.args.batch_size >= COCO_NUM_VAL_IMAGES:\n          evaluator.evaluate()\n          break\n\n  def run(self):\n    if self.args.accuracy_only:\n      self.accuracy_check()\n    else:\n      self.run_benchmark()\n\n\n\nif __name__ == ""__main__"":\n  infer = model_infer()\n  infer.run()\n\n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/object_detection/tensorflow/ssd-mobilenet/inference/int8/infer_detections.py,33,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n\nfrom __future__ import division\n\nimport tensorflow as tf\nfrom tensorflow.python.data.experimental import parallel_interleave\nfrom tensorflow.python.data.experimental import map_and_batch\nimport time\n\nfrom argparse import ArgumentParser\nfrom inference.coco_detection_evaluator import CocoDetectionEvaluator\nfrom inference.coco_label_map import category_map\n\nIMAGE_SIZE = 300\nCOCO_NUM_VAL_IMAGES = 4952\n\nimport os\n\nimport numpy as np\n\ndef parse_and_preprocess(serialized_example):\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.compat.v1.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/object/class/text\': tf.compat.v1.VarLenFeature(dtype=tf.string),\n      \'image/source_id\': tf.compat.v1.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n  }\n  sparse_float32 = tf.compat.v1.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.compat.v1.parse_single_example(serialized_example, feature_map)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(bbox, [0, 2, 1])\n\n  encoded_image = features[\'image/encoded\']\n  image_tensor = tf.image.decode_image(encoded_image, channels=3)\n  image_tensor.set_shape([None, None, 3])\n\n  label = features[\'image/object/class/text\'].values\n\n  image_id = features[\'image/source_id\']\n\n  return image_tensor, bbox[0], label, image_id\n\nclass model_infer:\n\n  def __init__(self):\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--inter-op-parallelism-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--intra-op-parallelism-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph.\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n\n    arg_parser.add_argument(\'-i\', ""--iter"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'total_iter\', default=1000, type=int)\n\n    arg_parser.add_argument(\'-w\', ""--warmup_iter"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'warmup_iter\', default=200, type=int)\n\n    # parse the arguments\n    self.args = arg_parser.parse_args()\n\n    self.config = tf.compat.v1.ConfigProto()\n    self.config.intra_op_parallelism_threads = self.args.num_intra_threads\n    self.config.inter_op_parallelism_threads = self.args.num_inter_threads\n    self.config.use_per_session_threads = 1\n\n    self.load_graph()\n\n    if self.args.batch_size == -1:\n      self.args.batch_size = 1\n\n    input_layer = \'Preprocessor/subpart2\'\n    output_layers = [\'num_detections\', \'detection_boxes\', \'detection_scores\', \'detection_classes\']\n    self.input_tensor = self.infer_graph.get_tensor_by_name(input_layer + "":0"")\n    self.output_tensors = [self.infer_graph.get_tensor_by_name(x + "":0"") for x in output_layers]\n  \n    self.category_map_reverse = {v : k for k, v in category_map.items()}\n\n  def build_data_sess(self):\n    data_graph = tf.Graph()\n    with data_graph.as_default():\n      self.input_images, self.bbox, self.label, self.image_id = self.get_input()\n    self.data_sess = tf.compat.v1.Session(graph=data_graph, config=self.config)\n\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    preprocess_graph = tf.Graph()\n    with preprocess_graph.as_default():\n      graph_def = tf.compat.v1.GraphDef()\n      with tf.compat.v1.gfile.FastGFile(os.path.join(os.path.dirname(dir_path), \'ssdmobilenet_preprocess.pb\'), \'rb\') as input_file:\n        input_graph_content = input_file.read()\n        graph_def.ParseFromString(input_graph_content)\n\n      tf.import_graph_def(graph_def, name=\'\')\n    \n    self.pre_sess = tf.compat.v1.Session(graph=preprocess_graph, config=self.config)\n    self.pre_output = preprocess_graph.get_tensor_by_name(""Preprocessor/sub:0"")\n    self.pre_input = preprocess_graph.get_tensor_by_name(""image_tensor:0"")\n\n\n  def load_graph(self):\n    print(\'load graph from: \' + self.args.input_graph)\n\n    self.infer_graph = tf.Graph()\n    with self.infer_graph.as_default():\n      graph_def = tf.compat.v1.GraphDef()\n      with tf.compat.v1.gfile.FastGFile(self.args.input_graph, \'rb\') as input_file:\n        input_graph_content = input_file.read()\n        graph_def.ParseFromString(input_graph_content)\n\n      tf.import_graph_def(graph_def, name=\'\')\n\n  def run_benchmark(self):\n    if self.args.data_location:\n      print(""Inference with real data."")\n    else:\n      print(""Inference with dummy data."")\n          \n    with tf.compat.v1.Session(graph=self.infer_graph, config=self.config) as sess:\n      \n      if self.args.data_location:\n        self.build_data_sess()\n      else:\n        input_images = sess.run(tf.random.truncated_normal(\n          [self.args.batch_size, IMAGE_SIZE, IMAGE_SIZE, 3],\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\'))\n\n      total_iter = self.args.total_iter\n      warmup_iter = self.args.warmup_iter\n      ttime = 0.0\n\n      print(\'total iteration is {0}\'.format(str(total_iter)))\n      print(\'warm up iteration is {0}\'.format(str(warmup_iter)))\n\n      for step in range(total_iter):\n        start_time = time.time()\n        if self.args.data_location:\n          input_images = self.data_sess.run([self.input_images])\n          input_images = input_images[0]\n          input_images = self.pre_sess.run(self.pre_output, {self.pre_input: input_images})\n        _ = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        end_time = time.time()\n\n        duration = end_time - start_time\n        if (step + 1) % 10 == 0:\n          print(\'steps = {0}, {1} sec\'.format(str(step), str(duration)))\n        \n        if step + 1 > warmup_iter:\n          ttime += duration\n        \n      total_batches = total_iter - warmup_iter\n      print (\'Batchsize: {0}\'.format(str(self.args.batch_size)))\n      print (\'Time spent per BATCH: {0:10.4f} ms\'.format(ttime / total_batches * 1000))\n      print (\'Total samples/sec: {0:10.4f} samples/s\'.format(total_batches * self.args.batch_size / ttime))\n  \n\n  def get_input(self):\n    tfrecord_paths = [self.args.data_location]\n    ds = tf.data.TFRecordDataset.list_files(tfrecord_paths)\n\n    ds = ds.apply(\n        parallel_interleave(\n          tf.data.TFRecordDataset, cycle_length=28, block_length=5,\n          sloppy=True,\n          buffer_output_elements=10000, prefetch_input_elements=10000))\n    ds = ds.prefetch(buffer_size=10000)\n    ds = ds.apply(\n        map_and_batch(\n          map_func=parse_and_preprocess,\n          batch_size=self.args.batch_size,\n          num_parallel_batches=28,\n          num_parallel_calls=None))\n    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    ds_iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n    images, bbox, label, image_id = ds_iterator.get_next()\n\n    return images, bbox, label, image_id\n\n  def accuracy_check(self):\n    print(""Inference for accuracy check."")\n    self.build_data_sess()\n    evaluator = CocoDetectionEvaluator()\n    with tf.compat.v1.Session(graph=self.infer_graph, config=self.config) as sess:\n      iter = 0\n      while True:\n        print(\'Run {0} iter\'.format(iter))\n        iter += 1\n        input_images, bbox, label, image_id = self.data_sess.run([self.input_images, self.bbox, self.label, self.image_id])\n        ground_truth = {}\n        ground_truth[\'boxes\'] = np.asarray(bbox[0])\n        label = [x if type(x) == \'str\' else x.decode(\'utf-8\') for x in label[0]]\n        ground_truth[\'classes\'] = np.asarray([self.category_map_reverse[x] for x in label])\n        image_id = image_id[0] if type(image_id[0]) == \'str\' else image_id[0].decode(\'utf-8\')\n        evaluator.add_single_ground_truth_image_info(image_id, ground_truth)\n        input_images = self.pre_sess.run(self.pre_output, {self.pre_input: input_images})\n        num, boxes, scores, labels = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        detection = {}\n        num = int(num[0])\n        detection[\'boxes\'] = np.asarray(boxes[0])[0:num]\n        detection[\'scores\'] = np.asarray(scores[0])[0:num]\n        detection[\'classes\'] = np.asarray(labels[0])[0:num]\n        evaluator.add_single_detected_image_info(image_id, detection)\n        if iter * self.args.batch_size >= COCO_NUM_VAL_IMAGES:\n          evaluator.evaluate()\n          break\n\n  def run(self):\n    if self.args.accuracy_only:\n      self.accuracy_check()\n    else:\n      self.run_benchmark()\n\n\n\nif __name__ == ""__main__"":\n  infer = model_infer()\n  infer.run()\n\n'"
models/object_detection/tensorflow/ssd-resnet34/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/object_detection/tensorflow/ssd-resnet34/inference/fp32/infer_detections.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport tensorflow as tf\nimport time\n\nfrom argparse import ArgumentParser\n\nimport benchmark_cnn\nimport datasets\nimport ssd_constants\nfrom models import ssd_model\nfrom preprocessing import COCOPreprocessor\n\nIMAGE_SIZE = 300\n\nimport os\n\nclass ssd_resnet34_infer:\n\n  def __init__(self):\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--inter-op-parallelism-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--intra-op-parallelism-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph.\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n\n    arg_parser.add_argument(""--results-file-path"",\n                            help=""File path for the inference results"",\n                            dest=""results_file_path"", default=None)\n\n    # parse the arguments\n    self.args = arg_parser.parse_args()\n\n    self.freeze_graph = self.load_graph(self.args.input_graph)\n    self.config = tf.compat.v1.ConfigProto()\n    self.config.intra_op_parallelism_threads = self.args.num_intra_threads\n    self.config.inter_op_parallelism_threads = self.args.num_inter_threads\n\n    if self.args.batch_size == -1:\n      self.args.batch_size = 64\n\n    self.num_batches = (ssd_constants.COCO_NUM_VAL_IMAGES // self.args.batch_size) + \\\n                       (ssd_constants.COCO_NUM_VAL_IMAGES % self.args.batch_size > 0)\n    \n    input_layer = \'input\'\n    output_layers = [\'v/stack\', \'v/Softmax\']\n    self.input_tensor = self.freeze_graph.get_tensor_by_name(input_layer + "":0"")\n    self.output_tensors = [self.freeze_graph.get_tensor_by_name(x + "":0"") for x in output_layers]\n    \n\n  def load_graph(self, frozen_graph_filename):\n    print(\'load graph from: \' + frozen_graph_filename)\n    with tf.io.gfile.GFile(frozen_graph_filename, ""rb"") as f:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Then, we import the graph_def into a new Graph and returns it\n    with tf.Graph().as_default() as graph:\n        # Since we load everything in a new graph, this is not needed\n        tf.import_graph_def(graph_def, name=\'\')\n    return graph\n\n  def run_benchmark(self):\n    print(""Inference with dummy data."")\n    with tf.compat.v1.Session(graph=self.freeze_graph, config=self.config) as sess:\n      \n      input_images = sess.run(tf.random.truncated_normal(\n          [self.args.batch_size, IMAGE_SIZE, IMAGE_SIZE, 3],\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\'))\n\n      total_iter = 1000\n      warmup_iter = 200\n      ttime = 0.0\n\n      print(\'total iteration is {0}\'.format(str(total_iter)))\n      print(\'warm up iteration is {0}\'.format(str(warmup_iter)))\n\n      for step in range(total_iter):\n        start_time = time.time()\n        _ = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        end_time = time.time()\n\n        duration = end_time - start_time\n        if (step + 1) % 10 == 0:\n          print(\'steps = {0}, {1} sec\'.format(str(step), str(duration)))\n        \n        if step + 1 > warmup_iter:\n          ttime += duration\n        \n      total_batches = total_iter - warmup_iter\n      print (\'Batchsize: {0}\'.format(str(self.args.batch_size)))\n      print (\'Time spent per BATCH: {0:10.4f} ms\'.format(ttime / total_batches * 1000))\n      print (\'Total samples/sec: {0:10.4f} samples/s\'.format(total_batches * self.args.batch_size / ttime))\n  \n\n  def __get_input(self):\n    preprocessor = COCOPreprocessor(\n      batch_size=self.args.batch_size,\n      output_shapes=[[self.args.batch_size, IMAGE_SIZE, IMAGE_SIZE, 3]],\n      num_splits=1,\n      dtype=tf.float32,\n      train=False,\n      distortions=True,\n      resize_method=None,\n      shift_ratio=0\n    )\n\n    class params:\n      datasets_repeat_cached_sample = False\n\n    self.params = params()\n    self.dataset = datasets.create_dataset(self.args.data_location, \'coco\')\n    \n    return preprocessor.minibatch(\n      self.dataset,\n      subset=\'validation\',\n      params=self.params,\n      shift_ratio=0)\n\n\n  def accuracy_check(self):\n    print(self.args)\n    input_list = self.__get_input()\n    ds_init = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TABLE_INITIALIZERS)\n\n    ds_sess = tf.compat.v1.Session()\n    params = benchmark_cnn.make_params(data_dir=self.args.data_location)\n    self.model = ssd_model.SSD300Model(params=params)\n\n    print(""Inference for accuracy check."")\n    with tf.compat.v1.Session(graph=self.freeze_graph, config=self.config) as sess:\n      ds_sess.run(ds_init)\n      global_step = 0\n\n      for _ in range(self.num_batches):\n        results = {}\n        input_lists = ds_sess.run(input_list)\n        input_images = input_lists[0][0]\n        input_ids = input_lists[3][0]\n        input_raw_shapes = input_lists[4][0]\n\n        result = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        # Make global_step available in results for postprocessing.\n        results[\'global_step\'] = global_step\n        results[ssd_constants.SOURCE_ID] = input_ids\n        results[ssd_constants.RAW_SHAPE] = input_raw_shapes\n\n        results[ssd_constants.PRED_BOXES] = result[0]\n        results[ssd_constants.PRED_SCORES] = result[1]\n\n        results = self.model.postprocess(results)\n\n\n\n  def run(self):\n    if self.args.accuracy_only:\n      self.accuracy_check()\n    else:\n      self.run_benchmark()\n\n\n\nif __name__ == ""__main__"":\n  infer = ssd_resnet34_infer()\n  infer.run()\n\n'"
models/object_detection/tensorflow/ssd-resnet34/inference/int8/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n'"
models/object_detection/tensorflow/ssd-resnet34/inference/int8/infer_detections.py,12,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2019 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\nimport tensorflow as tf\nimport time\n\nfrom argparse import ArgumentParser\n\nimport benchmark_cnn\nimport datasets\nimport ssd_constants\nfrom models import ssd_model\nfrom preprocessing import COCOPreprocessor\n\nIMAGE_SIZE = 300\n\nimport os\n\nclass ssd_resnet34_infer:\n\n  def __init__(self):\n    arg_parser = ArgumentParser(description=\'Parse args\')\n\n    arg_parser.add_argument(\'-b\', ""--batch-size"",\n                            help=""Specify the batch size. If this "" \\\n                                 ""parameter is not specified or is -1, the "" \\\n                                 ""largest ideal batch size for the model will "" \\\n                                 ""be used."",\n                            dest=""batch_size"", type=int, default=-1)\n\n    arg_parser.add_argument(\'-e\', ""--inter-op-parallelism-threads"",\n                            help=\'The number of inter-thread.\',\n                            dest=\'num_inter_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-a\', ""--intra-op-parallelism-threads"",\n                            help=\'The number of intra-thread.\',\n                            dest=\'num_intra_threads\', type=int, default=0)\n\n    arg_parser.add_argument(\'-g\', ""--input-graph"",\n                            help=\'Specify the input graph.\',\n                            dest=\'input_graph\')\n\n    arg_parser.add_argument(\'-d\', ""--data-location"",\n                            help=\'Specify the location of the data. \'\n                                 \'If this parameter is not specified, \'\n                                 \'the benchmark will use random/dummy data.\',\n                            dest=""data_location"", default=None)\n\n    arg_parser.add_argument(\'-r\', ""--accuracy-only"",\n                            help=\'For accuracy measurement only.\',\n                            dest=\'accuracy_only\', action=\'store_true\')\n\n    arg_parser.add_argument(""--results-file-path"",\n                            help=""File path for the inference results"",\n                            dest=""results_file_path"", default=None)\n\n    # parse the arguments\n    self.args = arg_parser.parse_args()\n\n    self.freeze_graph = self.load_graph(self.args.input_graph)\n    self.config = tf.compat.v1.ConfigProto()\n    self.config.intra_op_parallelism_threads = self.args.num_intra_threads\n    self.config.inter_op_parallelism_threads = self.args.num_inter_threads\n\n    if self.args.batch_size == -1:\n      self.args.batch_size = 64\n\n    self.num_batches = (ssd_constants.COCO_NUM_VAL_IMAGES // self.args.batch_size) + \\\n                       (ssd_constants.COCO_NUM_VAL_IMAGES % self.args.batch_size > 0)\n    \n    input_layer = \'input\'\n    output_layers = [\'v/stack\', \'v/Softmax\']\n    self.input_tensor = self.freeze_graph.get_tensor_by_name(input_layer + "":0"")\n    self.output_tensors = [self.freeze_graph.get_tensor_by_name(x + "":0"") for x in output_layers]\n    \n\n  def load_graph(self, frozen_graph_filename):\n    print(\'load graph from: \' + frozen_graph_filename)\n    with tf.io.gfile.GFile(frozen_graph_filename, ""rb"") as f:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Then, we import the graph_def into a new Graph and returns it\n    with tf.Graph().as_default() as graph:\n        # Since we load everything in a new graph, this is not needed\n        tf.import_graph_def(graph_def, name=\'\')\n    return graph\n\n  def run_benchmark(self):\n    print(""Inference with dummy data."")\n    with tf.compat.v1.Session(graph=self.freeze_graph, config=self.config) as sess:\n      \n      input_images = sess.run(tf.random.truncated_normal(\n          [self.args.batch_size, IMAGE_SIZE, IMAGE_SIZE, 3],\n          dtype=tf.float32,\n          stddev=10,\n          name=\'synthetic_images\'))\n\n      total_iter = 1000\n      warmup_iter = 200\n      ttime = 0.0\n\n      print(\'total iteration is {0}\'.format(str(total_iter)))\n      print(\'warm up iteration is {0}\'.format(str(warmup_iter)))\n\n      for step in range(total_iter):\n        start_time = time.time()\n        _ = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        end_time = time.time()\n\n        duration = end_time - start_time\n        if (step + 1) % 10 == 0:\n          print(\'steps = {0}, {1} sec\'.format(str(step), str(duration)))\n        \n        if step + 1 > warmup_iter:\n          ttime += duration\n        \n      total_batches = total_iter - warmup_iter\n      print (\'Batchsize: {0}\'.format(str(self.args.batch_size)))\n      print (\'Time spent per BATCH: {0:10.4f} ms\'.format(ttime / total_batches * 1000))\n      print (\'Total samples/sec: {0:10.4f} samples/s\'.format(total_batches * self.args.batch_size / ttime))\n  \n\n  def __get_input(self):\n    preprocessor = COCOPreprocessor(\n      batch_size=self.args.batch_size,\n      output_shapes=[[self.args.batch_size, IMAGE_SIZE, IMAGE_SIZE, 3]],\n      num_splits=1,\n      dtype=tf.float32,\n      train=False,\n      distortions=True,\n      resize_method=None,\n      shift_ratio=0\n    )\n\n    class params:\n      datasets_repeat_cached_sample = False\n\n    self.params = params()\n    self.dataset = datasets.create_dataset(self.args.data_location, \'coco\')\n    \n    return preprocessor.minibatch(\n      self.dataset,\n      subset=\'validation\',\n      params=self.params,\n      shift_ratio=0)\n\n\n  def accuracy_check(self):\n    print(self.args)\n    input_list = self.__get_input()\n    ds_init = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TABLE_INITIALIZERS)\n\n    ds_sess = tf.compat.v1.Session()\n    params = benchmark_cnn.make_params(data_dir=self.args.data_location)\n    self.model = ssd_model.SSD300Model(params=params)\n\n    print(""Inference for accuracy check."")\n    with tf.compat.v1.Session(graph=self.freeze_graph, config=self.config) as sess:\n      ds_sess.run(ds_init)\n      global_step = 0\n\n      for _ in range(self.num_batches):\n        results = {}\n        input_lists = ds_sess.run(input_list)\n        input_images = input_lists[0][0]\n        input_ids = input_lists[3][0]\n        input_raw_shapes = input_lists[4][0]\n\n        result = sess.run(self.output_tensors, {self.input_tensor: input_images})\n        # Make global_step available in results for postprocessing.\n        results[\'global_step\'] = global_step\n        results[ssd_constants.SOURCE_ID] = input_ids\n        results[ssd_constants.RAW_SHAPE] = input_raw_shapes\n\n        results[ssd_constants.PRED_BOXES] = result[0]\n        results[ssd_constants.PRED_SCORES] = result[1]\n\n        results = self.model.postprocess(results)\n\n\n\n  def run(self):\n    if self.args.accuracy_only:\n      self.accuracy_check()\n    else:\n      self.run_benchmark()\n\n\n\nif __name__ == ""__main__"":\n  infer = ssd_resnet34_infer()\n  infer.run()\n\n'"
models/recommendation/tensorflow/wide_deep/inference/fp32/__init__.py,0,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n'"
models/recommendation/tensorflow/wide_deep/inference/fp32/wide_deep_inference.py,34,"b'#\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2018 Intel Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n#\n\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Example code for TensorFlow Wide & Deep Tutorial using tf.estimator API.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\nimport time\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\nfrom official.utils.arg_parsers import parsers\nfrom official.utils.logs import hooks_helper\n\n_CSV_COLUMNS = [\n    \'age\', \'workclass\', \'fnlwgt\', \'education\', \'education_num\',\n    \'marital_status\', \'occupation\', \'relationship\', \'race\', \'gender\',\n    \'capital_gain\', \'capital_loss\', \'hours_per_week\', \'native_country\',\n    \'income_bracket\'\n]\n\n_CSV_COLUMN_DEFAULTS = [[0], [\'\'], [0], [\'\'], [0], [\'\'], [\'\'], [\'\'],\n                        [\'\'], [\'\'], [0], [0], [0], [\'\'], [\'\']]\n\n_NUM_EXAMPLES = {\n    \'train\': 32561,\n    \'validation\': 16281,\n}\n\n\nLOSS_PREFIX = {\'wide\': \'linear/\', \'deep\': \'dnn/\'}\n\n\ndef build_model_columns():\n    """"""Builds a set of wide and deep feature columns.""""""\n    # Continuous columns\n    age = tf.feature_column.numeric_column(\'age\')\n    education_num = tf.feature_column.numeric_column(\'education_num\')\n    capital_gain = tf.feature_column.numeric_column(\'capital_gain\')\n    capital_loss = tf.feature_column.numeric_column(\'capital_loss\')\n    hours_per_week = tf.feature_column.numeric_column(\'hours_per_week\')\n\n    education = tf.feature_column.categorical_column_with_vocabulary_list(\n        \'education\', [\n            \'Bachelors\', \'HS-grad\', \'11th\', \'Masters\', \'9th\', \'Some-college\',\n            \'Assoc-acdm\', \'Assoc-voc\', \'7th-8th\', \'Doctorate\', \'Prof-school\',\n            \'5th-6th\', \'10th\', \'1st-4th\', \'Preschool\', \'12th\'])\n\n    marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n        \'marital_status\', [\n            \'Married-civ-spouse\', \'Divorced\', \'Married-spouse-absent\',\n            \'Never-married\', \'Separated\', \'Married-AF-spouse\', \'Widowed\'])\n\n    relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n        \'relationship\', [\n            \'Husband\', \'Not-in-family\', \'Wife\', \'Own-child\', \'Unmarried\',\n            \'Other-relative\'])\n\n    workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n        \'workclass\', [\n            \'Self-emp-not-inc\', \'Private\', \'State-gov\', \'Federal-gov\',\n            \'Local-gov\', \'?\', \'Self-emp-inc\', \'Without-pay\', \'Never-worked\'])\n\n    # To show an example of hashing:\n    occupation = tf.feature_column.categorical_column_with_hash_bucket(\n        \'occupation\', hash_bucket_size=1000)\n\n    # Transformations.\n    age_buckets = tf.feature_column.bucketized_column(\n        age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n\n    # Wide columns and deep columns.\n    base_columns = [\n        education, marital_status, relationship, workclass, occupation,\n        age_buckets,\n    ]\n\n    crossed_columns = [\n        tf.feature_column.crossed_column(\n            [\'education\', \'occupation\'], hash_bucket_size=1000),\n        tf.feature_column.crossed_column(\n            [age_buckets, \'education\', \'occupation\'], hash_bucket_size=1000),\n    ]\n\n    wide_columns = base_columns + crossed_columns\n\n    deep_columns = [\n        age,\n        education_num,\n        capital_gain,\n        capital_loss,\n        hours_per_week,\n        tf.feature_column.indicator_column(workclass),\n        tf.feature_column.indicator_column(education),\n        tf.feature_column.indicator_column(marital_status),\n        tf.feature_column.indicator_column(relationship),\n        # To show an example of embedding\n        tf.feature_column.embedding_column(occupation, dimension=8),\n    ]\n\n    return wide_columns, deep_columns\n\n\ndef build_estimator(model_dir, model_type):\n    """"""Build an estimator appropriate for the given model type.""""""\n    wide_columns, deep_columns = build_model_columns()\n    hidden_units = [100, 75, 50, 25]\n\n    # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n    # trains faster than GPU for this model.\n    run_config = tf.estimator.RunConfig().replace(\n        session_config=tf.compat.v1.ConfigProto(device_count={\'GPU\': 0}))\n\n    if model_type == \'wide\':\n        return tf.estimator.LinearClassifier(\n            model_dir=model_dir,\n            feature_columns=wide_columns,\n            config=run_config, loss_reduction=tf.compat.v1.losses.Reduction.SUM)\n    elif model_type == \'deep\':\n        return tf.estimator.DNNClassifier(\n            model_dir=model_dir,\n            feature_columns=deep_columns,\n            hidden_units=hidden_units,\n            config=run_config, loss_reduction=tf.compat.v1.losses.Reduction.SUM)\n    else:\n        return tf.estimator.DNNLinearCombinedClassifier(\n            model_dir=model_dir,\n            linear_feature_columns=wide_columns,\n            dnn_feature_columns=deep_columns,\n            dnn_hidden_units=hidden_units,\n            config=run_config, loss_reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n            #config=run_config, loss_reduction=tf.compat.v1.losses.Reduction.SUM)\n\n\ndef input_fn(data_file, num_epochs, shuffle, batch_size):\n    """"""Generate an input function for the Estimator.""""""\n    assert tf.io.gfile.exists(data_file), (\n        \'%s not found. Please make sure you have run data_download.py and \'\n        \'set the --data_dir argument to the correct path.\' % data_file)\n\n    def parse_csv(value):\n        print(\'Parsing\', data_file)\n        columns = tf.io.decode_csv(records=value, record_defaults=_CSV_COLUMN_DEFAULTS)\n        features = dict(zip(_CSV_COLUMNS, columns))\n        labels = features.pop(\'income_bracket\')\n        return features, tf.equal(labels, \'>50K\')\n\n    # Extract lines from input files using the Dataset API.\n    dataset = tf.data.TextLineDataset(data_file)\n\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES[\'train\'])\n\n    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n\n    # We call repeat after shuffling, rather than before, to prevent separate\n    # epochs from blending together.\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\n\ndef main(argv):\n    parser = WideDeepArgParser()\n    flags = parser.parse_args(args=argv[1:])\n\n    # Clean up the model directory if present\n    # shutil.rmtree(flags.model_dir, ignore_errors=True)\n    model = build_estimator(flags.model_dir, flags.model_type)\n\n    train_file = os.path.join(flags.data_dir, \'adult.data\')\n    test_file = os.path.join(flags.data_dir, \'adult.test\')\n\n    num_records = sum(1 for line in open(test_file))\n\n    # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n    def train_input_fn():\n        return input_fn(\n            train_file, flags.epochs_between_evals, True, flags.batch_size)\n\n    def eval_input_fn():\n        return input_fn(test_file, 1, False, flags.batch_size)\n\n    loss_prefix = LOSS_PREFIX.get(flags.model_type, \'\')\n    train_hooks = hooks_helper.get_train_hooks(\n        flags.hooks, batch_size=flags.batch_size,\n        tensors_to_log={\'average_loss\': loss_prefix + \'head/truediv\',\n                        \'loss\': loss_prefix + \'head/weighted_loss/Sum\'})\n\n    inference_start = time.time()\n    # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n    results = model.evaluate(input_fn=eval_input_fn)\n\n    # Display evaluation metrics\n    print(\'-\' * 60)\n\n    for key in sorted(results):\n        print(\'%s: %s\' % (key, results[key]))\n    main_end = time.time()\n    E2Eduration = main_end - main_start\n    print (\'End-to-End duration is %s\', E2Eduration)\n    evaluate_duration = main_end - inference_start\n    print (\'Evaluation duration is %s\', evaluate_duration)\n\n    if flags.batch_size == 1:\n        print(\'Latency is: %s\', E2Eduration / num_records)\n    else:\n        print(\'Throughput is: %s\', num_records / evaluate_duration)\n\n\nclass WideDeepArgParser(argparse.ArgumentParser):\n    """"""Argument parser for running the wide deep model.""""""\n\n    def __init__(self):\n        super(WideDeepArgParser, self).__init__(parents=[parsers.BaseParser()])\n        self.add_argument(\n            \'--model_type\', \'-mt\', type=str, default=\'wide_deep\',\n            choices=[\'wide\', \'deep\', \'wide_deep\'],\n            help=\'[default %(default)s] Valid model types:\'\n                 \' wide, deep, wide_deep.\',\n            metavar=\'<MT>\')\n        self.set_defaults(\n            data_dir=\'/tmp/census_data\',\n            model_dir=\'/tmp/census_model\',\n            train_epochs=40,\n            epochs_between_evals=2,\n            batch_size=40)\n\n\nif __name__ == \'__main__\':\n    main_start = time.time()\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n    main(argv=sys.argv)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/arg_parsers/__init__.py,0,b''
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/arg_parsers/parsers.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Collection of parsers which are shared among the official models.\n\nThe parsers in this module are intended to be used as parents to all arg\nparsers in official models. For instance, one might define a new class:\n\nclass ExampleParser(argparse.ArgumentParser):\n  def __init__(self):\n    super(ExampleParser, self).__init__(parents=[\n      arg_parsers.LocationParser(data_dir=True, model_dir=True),\n      arg_parsers.DummyParser(use_synthetic_data=True),\n    ])\n\n    self.add_argument(\n      ""--application_specific_arg"", ""-asa"", type=int, default=123,\n      help=""[default: %(default)s] This arg is application specific."",\n      metavar=""<ASA>""\n    )\n\nNotes about add_argument():\n    Argparse will automatically template in default values in help messages if\n  the ""%(default)s"" string appears in the message. Using the example above:\n\n    parser = ExampleParser()\n    parser.set_defaults(application_specific_arg=3141592)\n    parser.parse_args([""-h""])\n\n    When the help text is generated, it will display 3141592 to the user. (Even\n  though the default was 123 when the flag was created.)\n\n\n    The metavar variable determines how the flag will appear in help text. If\n  not specified, the convention is to use name.upper(). Thus rather than:\n\n    --app_specific_arg APP_SPECIFIC_ARG, -asa APP_SPECIFIC_ARG\n\n  if metavar=""<ASA>"" is set, the user sees:\n\n    --app_specific_arg <ASA>, -asa <ASA>\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\nimport tensorflow as tf\n\n\n# Map string to (TensorFlow dtype, default loss scale)\nDTYPE_MAP = {\n    ""fp16"": (tf.float16, 128),\n    ""fp32"": (tf.float32, 1),\n}\n\n\ndef parse_dtype_info(flags):\n  """"""Convert dtype string to tf dtype, and set loss_scale default as needed.\n\n  Args:\n    flags: namespace object returned by arg parser.\n\n  Raises:\n    ValueError: If an invalid dtype is provided.\n  """"""\n  if flags.dtype in (i[0] for i in DTYPE_MAP.values()):\n    return  # Make function idempotent\n\n  try:\n    flags.dtype, default_loss_scale = DTYPE_MAP[flags.dtype]\n  except KeyError:\n    raise ValueError(""Invalid dtype: {}"".format(flags.dtype))\n\n  flags.loss_scale = flags.loss_scale or default_loss_scale\n\n\nclass BaseParser(argparse.ArgumentParser):\n  """"""Parser to contain flags which will be nearly universal across models.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    data_dir: Create a flag for specifying the input data directory.\n    model_dir: Create a flag for specifying the model file directory.\n    train_epochs: Create a flag to specify the number of training epochs.\n    epochs_between_evals: Create a flag to specify the frequency of testing.\n    stop_threshold: Create a flag to specify a threshold accuracy or other\n      eval metric which should trigger the end of training.\n    batch_size: Create a flag to specify the global batch size.\n    multi_gpu: Create a flag to allow the use of all available GPUs.\n    num_gpu: Create a flag to specify the number of GPUs used.\n    hooks: Create a flag to specify hooks for logging.\n  """"""\n\n  def __init__(self, add_help=False, data_dir=True, model_dir=True,\n               train_epochs=True, epochs_between_evals=True,\n               stop_threshold=True, batch_size=True,\n               multi_gpu=False, num_gpu=True, hooks=True,\n               enable_lars=True, label_smoothing=True, weight_decay=True, fine_tune=True):\n    super(BaseParser, self).__init__(add_help=add_help)\n\n    if data_dir:\n      self.add_argument(\n          ""--data_dir"", ""-dd"", default=""/tmp"",\n          help=""[default: %(default)s] The location of the input data."",\n          metavar=""<DD>"",\n      )\n\n    if model_dir:\n      self.add_argument(\n          ""--model_dir"", ""-md"", default=""/tmp"",\n          help=""[default: %(default)s] The location of the model checkpoint ""\n               ""files."",\n          metavar=""<MD>"",\n      )\n\n    if train_epochs:\n      self.add_argument(\n          ""--train_epochs"", ""-te"", type=int, default=1,\n          help=""[default: %(default)s] The number of epochs used to train."",\n          metavar=""<TE>""\n      )\n\n    if epochs_between_evals:\n      self.add_argument(\n          ""--epochs_between_evals"", ""-ebe"", type=int, default=1,\n          help=""[default: %(default)s] The number of training epochs to run ""\n               ""between evaluations."",\n          metavar=""<EBE>""\n      )\n\n    if stop_threshold:\n      self.add_argument(\n          ""--stop_threshold"", ""-st"", type=float, default=None,\n          help=""[default: %(default)s] If passed, training will stop at ""\n          ""the earlier of train_epochs and when the evaluation metric is ""\n          ""greater than or equal to stop_threshold."",\n          metavar=""<ST>""\n      )\n\n    if batch_size:\n      self.add_argument(\n          ""--batch_size"", ""-bs"", type=int, default=32,\n          help=""[default: %(default)s] Global batch size for training and ""\n               ""evaluation."",\n          metavar=""<BS>""\n      )\n\n    if enable_lars:\n      self.add_argument(\n         ""--enable_lars"", ""-el"", action=\'store_true\',\n         help=\'[default: %(default)s] Enable LARS optimizer for large batch training.\'\n      )\n\n    if label_smoothing:\n      self.add_argument(\n         ""--label_smoothing"", ""-lsm"", type=float, default=0.0,\n         help=\'[default: %(default)s] Label smoothing parameter used in the softmax_cross_entropy\',\n         metavar=""<LSM>""\n      )\n\n    if weight_decay:\n      self.add_argument(\n         ""--weight_decay"", ""-wd"", type=float, default=1e-4,\n         help=\'[default: %(default)s] Weight decay coefficiant for l2 regularization.\',\n         metavar=""<WD>""\n      )\n\n    if fine_tune:\n      self.add_argument(\n         ""--fine_tune"", ""-ft"", action=\'store_true\',\n          help=""[default: %(default)s] fine_tune: If True only train the dense layers(final layers.""\n      )\n\n    assert not (multi_gpu and num_gpu)\n\n    if multi_gpu:\n      self.add_argument(\n          ""--multi_gpu"", action=""store_true"",\n          help=""If set, run across all available GPUs.""\n      )\n\n    if num_gpu:\n      self.add_argument(\n          ""--num_gpus"", ""-ng"",\n          type=int,\n          default=1 if tf.test.is_built_with_cuda() else 0,\n          help=""[default: %(default)s] How many GPUs to use with the ""\n               ""DistributionStrategies API. The default is 1 if TensorFlow was""\n               ""built with CUDA, and 0 otherwise."",\n          metavar=""<NG>""\n      )\n\n    if hooks:\n      self.add_argument(\n          ""--hooks"", ""-hk"", nargs=""+"", default=[""LoggingTensorHook""],\n          help=""[default: %(default)s] A list of strings to specify the names ""\n               ""of train hooks. ""\n               ""Example: --hooks LoggingTensorHook ExamplesPerSecondHook. ""\n               ""Allowed hook names (case-insensitive): LoggingTensorHook, ""\n               ""ProfilerHook, ExamplesPerSecondHook, LoggingMetricHook.""\n               ""See official.utils.logs.hooks_helper for details."",\n          metavar=""<HK>""\n      )\n\n\nclass PerformanceParser(argparse.ArgumentParser):\n  """"""Default parser for specifying performance tuning arguments.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    num_parallel_calls: Create a flag to specify parallelism of data loading.\n    inter_op: Create a flag to allow specification of inter op threads.\n    intra_op: Create a flag to allow specification of intra op threads.\n  """"""\n\n  def __init__(self, add_help=False, num_parallel_calls=True, inter_op=True,\n               intra_op=True, use_synthetic_data=True, max_train_steps=True,\n               dtype=True):\n    super(PerformanceParser, self).__init__(add_help=add_help)\n\n    if num_parallel_calls:\n      self.add_argument(\n          ""--num_parallel_calls"", ""-npc"",\n          type=int, default=5,\n          help=""[default: %(default)s] The number of records that are ""\n               ""processed in parallel  during input processing. This can be ""\n               ""optimized per data set but for generally homogeneous data ""\n               ""sets, should be approximately the number of available CPU ""\n               ""cores."",\n          metavar=""<NPC>""\n      )\n\n    if inter_op:\n      self.add_argument(\n          ""--inter_op_parallelism_threads"", ""-inter"",\n          type=int, default=0,\n          help=""[default: %(default)s Number of inter_op_parallelism_threads ""\n               ""to use for CPU. See TensorFlow config.proto for details."",\n          metavar=""<INTER>""\n      )\n\n    if intra_op:\n      self.add_argument(\n          ""--intra_op_parallelism_threads"", ""-intra"",\n          type=int, default=0,\n          help=""[default: %(default)s Number of intra_op_parallelism_threads ""\n               ""to use for CPU. See TensorFlow config.proto for details."",\n          metavar=""<INTRA>""\n      )\n\n    if use_synthetic_data:\n      self.add_argument(\n          ""--use_synthetic_data"", ""-synth"",\n          action=""store_true"",\n          help=""If set, use fake data (zeroes) instead of a real dataset. ""\n               ""This mode is useful for performance debugging, as it removes ""\n               ""input processing steps, but will not learn anything.""\n      )\n\n    if max_train_steps:\n      self.add_argument(\n          ""--max_train_steps"", ""-mts"", type=int, default=None,\n          help=""[default: %(default)s] The model will stop training if the ""\n               ""global_step reaches this value. If not set, training will run""\n               ""until the specified number of epochs have run as usual. It is""\n               ""generally recommended to set --train_epochs=1 when using this""\n               ""flag."",\n          metavar=""<MTS>""\n      )\n\n    if dtype:\n      self.add_argument(\n          ""--dtype"", ""-dt"",\n          default=""fp32"",\n          choices=list(DTYPE_MAP.keys()),\n          help=""[default: %(default)s] {%(choices)s} The TensorFlow datatype ""\n               ""used for calculations. Variables may be cast to a higher""\n               ""precision on a case-by-case basis for numerical stability."",\n          metavar=""<DT>""\n      )\n\n      self.add_argument(\n          ""--loss_scale"", ""-ls"",\n          type=int,\n          help=""[default: %(default)s] The amount to scale the loss by when ""\n               ""the model is run. Before gradients are computed, the loss is ""\n               ""multiplied by the loss scale, making all gradients loss_scale ""\n               ""times larger. To adjust for this, gradients are divided by the ""\n               ""loss scale before being applied to variables. This is ""\n               ""mathematically equivalent to training without a loss scale, ""\n               ""but the loss scale helps avoid some intermediate gradients ""\n               ""from underflowing to zero. If not provided the default for ""\n               ""fp16 is 128 and 1 for all other dtypes."",\n      )\n\n\nclass ImageModelParser(argparse.ArgumentParser):\n  """"""Default parser for specification image specific behavior.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    data_format: Create a flag to specify image axis convention.\n  """"""\n\n  def __init__(self, add_help=False, data_format=True):\n    super(ImageModelParser, self).__init__(add_help=add_help)\n    if data_format:\n      self.add_argument(\n          ""--data_format"", ""-df"",\n          default=None,\n          choices=[""channels_first"", ""channels_last""],\n          help=""A flag to override the data format used in the model. ""\n               ""channels_first provides a performance boost on GPU but is not ""\n               ""always compatible with CPU. If left unspecified, the data ""\n               ""format will be chosen automatically based on whether TensorFlow""\n               ""was built for CPU or GPU."",\n          metavar=""<CF>""\n      )\n\n\nclass ExportParser(argparse.ArgumentParser):\n  """"""Parsing options for exporting saved models or other graph defs.\n\n  This is a separate parser for now, but should be made part of BaseParser\n  once all models are brought up to speed.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    export_dir: Create a flag to specify where a SavedModel should be exported.\n  """"""\n\n  def __init__(self, add_help=False, export_dir=True):\n    super(ExportParser, self).__init__(add_help=add_help)\n    if export_dir:\n      self.add_argument(\n          ""--export_dir"", ""-ed"",\n          help=""[default: %(default)s] If set, a SavedModel serialization of ""\n               ""the model will be exported to this directory at the end of ""\n               ""training. See the README for more details and relevant links."",\n          metavar=""<ED>""\n      )\n\n\nclass BenchmarkParser(argparse.ArgumentParser):\n  """"""Default parser for benchmark logging.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    benchmark_log_dir: Create a flag to specify location for benchmark logging.\n  """"""\n\n  def __init__(self, add_help=False, benchmark_log_dir=True,\n               bigquery_uploader=True):\n    super(BenchmarkParser, self).__init__(add_help=add_help)\n    if benchmark_log_dir:\n      self.add_argument(\n          ""--benchmark_log_dir"", ""-bld"", default=None,\n          help=""[default: %(default)s] The location of the benchmark logging."",\n          metavar=""<BLD>""\n      )\n    if bigquery_uploader:\n      self.add_argument(\n          ""--gcp_project"", ""-gp"", default=None,\n          help=""[default: %(default)s] The GCP project name where the benchmark""\n               "" will be uploaded."",\n          metavar=""<GP>""\n      )\n      self.add_argument(\n          ""--bigquery_data_set"", ""-bds"", default=""test_benchmark"",\n          help=""[default: %(default)s] The Bigquery dataset name where the""\n               "" benchmark will be uploaded."",\n          metavar=""<BDS>""\n      )\n      self.add_argument(\n          ""--bigquery_run_table"", ""-brt"", default=""benchmark_run"",\n          help=""[default: %(default)s] The Bigquery table name where the""\n               "" benchmark run information will be uploaded."",\n          metavar=""<BRT>""\n      )\n      self.add_argument(\n          ""--bigquery_metric_table"", ""-bmt"", default=""benchmark_metric"",\n          help=""[default: %(default)s] The Bigquery table name where the""\n               "" benchmark metric information will be uploaded."",\n          metavar=""<BMT>""\n      )\n\n\nclass EagerParser(BaseParser):\n  """"""Remove options not relevant for Eager from the BaseParser.""""""\n\n  def __init__(self, add_help=False, data_dir=True, model_dir=True,\n               train_epochs=True, batch_size=True):\n    super(EagerParser, self).__init__(\n        add_help=add_help, data_dir=data_dir, model_dir=model_dir,\n        train_epochs=train_epochs, epochs_between_evals=False,\n        stop_threshold=False, batch_size=batch_size, multi_gpu=False,\n        hooks=False)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/export/__init__.py,0,b''
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/export/export.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convenience functions for exporting models as SavedModels or other types.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef build_tensor_serving_input_receiver_fn(shape, dtype=tf.float32,\n                                           batch_size=1):\n  """"""Returns a input_receiver_fn that can be used during serving.\n\n  This expects examples to come through as float tensors, and simply\n  wraps them as TensorServingInputReceivers.\n\n  Arguably, this should live in tf.estimator.export. Testing here first.\n\n  Args:\n    shape: list representing target size of a single example.\n    dtype: the expected datatype for the input example\n    batch_size: number of input tensors that will be passed for prediction\n\n  Returns:\n    A function that itself returns a TensorServingInputReceiver.\n  """"""\n  def serving_input_receiver_fn():\n    # Prep a placeholder where the input example will be fed in\n    features = tf.compat.v1.placeholder(\n        dtype=dtype, shape=[batch_size] + shape, name=\'input_tensor\')\n\n    return tf.estimator.export.TensorServingInputReceiver(\n        features=features, receiver_tensors=features)\n\n  return serving_input_receiver_fn\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/logs/__init__.py,0,b''
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/logs/benchmark_uploader.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Library to upload benchmark generated by BenchmarkLogger to remote repo.\n\nThis library require google cloud bigquery lib as dependency, which can be\ninstalled with:\n  > pip install --upgrade google-cloud-bigquery\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport sys\nimport uuid\n\nfrom google.cloud import bigquery\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.arg_parsers import parsers\nfrom official.utils.logs import logger\n\n\nclass BigQueryUploader(object):\n  """"""Upload the benchmark and metric info to BigQuery.""""""\n\n  def __init__(self, logging_dir, gcp_project=None, credentials=None):\n    """"""Initialized BigQueryUploader with proper setting.\n\n    Args:\n      logging_dir: string, logging directory that contains the benchmark log.\n      gcp_project: string, the name of the GCP project that the log will be\n        uploaded to. The default project name will be detected from local\n        environment if no value is provided.\n      credentials: google.auth.credentials. The credential to access the\n        BigQuery service. The default service account credential will be\n        detected from local environment if no value is provided. Please use\n        google.oauth2.service_account.Credentials to load credential from local\n        file for the case that the test is run out side of GCP.\n    """"""\n    self._logging_dir = logging_dir\n    self._bq_client = bigquery.Client(\n        project=gcp_project, credentials=credentials)\n\n  def upload_benchmark_run(self, dataset_name, table_name, run_id):\n    """"""Upload benchmark run information to Bigquery.\n\n    Args:\n      dataset_name: string, the name of bigquery dataset where the data will be\n        uploaded.\n      table_name: string, the name of bigquery table under the dataset where\n        the data will be uploaded.\n      run_id: string, a unique ID that will be attached to the data, usually\n        this is a UUID4 format.\n    """"""\n    expected_file = os.path.join(\n        self._logging_dir, logger.BENCHMARK_RUN_LOG_FILE_NAME)\n    with tf.io.gfile.GFile(expected_file) as f:\n      benchmark_json = json.load(f)\n      benchmark_json[""model_id""] = run_id\n      table_ref = self._bq_client.dataset(dataset_name).table(table_name)\n      errors = self._bq_client.insert_rows_json(table_ref, [benchmark_json])\n      if errors:\n        tf.compat.v1.logging.error(\n            ""Failed to upload benchmark info to bigquery: {}"".format(errors))\n\n  def upload_metric(self, dataset_name, table_name, run_id):\n    """"""Upload metric information to Bigquery.\n\n    Args:\n      dataset_name: string, the name of bigquery dataset where the data will be\n        uploaded.\n      table_name: string, the name of bigquery table under the dataset where\n        the metric data will be uploaded. This is different from the\n        benchmark_run table.\n      run_id: string, a unique ID that will be attached to the data, usually\n        this is a UUID4 format. This should be the same as the benchmark run_id.\n    """"""\n    expected_file = os.path.join(\n        self._logging_dir, logger.METRIC_LOG_FILE_NAME)\n    with tf.io.gfile.GFile(expected_file) as f:\n      lines = f.readlines()\n      metrics = []\n      for line in filter(lambda l: l.strip(), lines):\n        metric = json.loads(line)\n        metric[""run_id""] = run_id\n        metrics.append(metric)\n      table_ref = self._bq_client.dataset(dataset_name).table(table_name)\n      errors = self._bq_client.insert_rows_json(table_ref, metrics)\n      if errors:\n        tf.compat.v1.logging.error(\n            ""Failed to upload benchmark info to bigquery: {}"".format(errors))\n\n\ndef main(argv):\n  parser = parsers.BenchmarkParser()\n  flags = parser.parse_args(args=argv[1:])\n  if not flags.benchmark_log_dir:\n    print(""Usage: benchmark_uploader.py --benchmark_log_dir=/some/dir"")\n    sys.exit(1)\n\n  uploader = BigQueryUploader(\n      flags.benchmark_log_dir,\n      gcp_project=flags.gcp_project)\n  run_id = str(uuid.uuid4())\n  uploader.upload_benchmark_run(\n      flags.bigquery_data_set, flags.bigquery_run_table, run_id)\n  uploader.upload_metric(\n      flags.bigquery_data_set, flags.bigquery_metric_table, run_id)\n\n\nif __name__ == ""__main__"":\n  main(argv=sys.argv)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/logs/hooks.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hook that counts examples per second every N steps or seconds.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass ExamplesPerSecondHook(tf.estimator.SessionRunHook):\n  """"""Hook to print out examples per second.\n\n  Total time is tracked and then divided by the total number of steps\n  to get the average step time and then batch_size is used to determine\n  the running average of examples per second. The examples per second for the\n  most recent interval is also logged.\n  """"""\n\n  def __init__(self,\n               batch_size,\n               every_n_steps=None,\n               every_n_secs=None,\n               warm_steps=0):\n    """"""Initializer for ExamplesPerSecondHook.\n\n    Args:\n      batch_size: Total batch size across all workers used to calculate\n        examples/second from global time.\n      every_n_steps: Log stats every n steps.\n      every_n_secs: Log stats every n seconds. Exactly one of the\n        `every_n_steps` or `every_n_secs` should be set.\n      warm_steps: The number of steps to be skipped before logging and running\n        average calculation. warm_steps steps refers to global steps across all\n        workers, not on each worker\n\n    Raises:\n      ValueError: if neither `every_n_steps` or `every_n_secs` is set, or\n      both are set.\n    """"""\n\n    if (every_n_steps is None) == (every_n_secs is None):\n      raise ValueError(\'exactly one of every_n_steps\'\n                       \' and every_n_secs should be provided.\')\n\n    self._timer = tf.estimator.SecondOrStepTimer(\n        every_steps=every_n_steps, every_secs=every_n_secs)\n\n    self._step_train_time = 0\n    self._total_steps = 0\n    self._total_measured_steps = 0\n    self._batch_size = batch_size\n    self._warm_steps = warm_steps\n\n  def begin(self):\n    """"""Called once before using the session to check global step.""""""\n    self._global_step_tensor = tf.compat.v1.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          \'Global step should be created to use StepCounterHook.\')\n\n  def before_run(self, run_context):  # pylint: disable=unused-argument\n    """"""Called before each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n\n    Returns:\n      A SessionRunArgs object or None if never triggered.\n    """"""\n    return tf.estimator.SessionRunArgs(self._global_step_tensor)\n\n  def after_run(self, run_context, run_values):  # pylint: disable=unused-argument\n    """"""Called after each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n      run_values: A SessionRunValues object.\n    """"""\n    global_step = run_values.results\n    if self._timer.should_trigger_for_step(global_step):\n      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(global_step)\n      if elapsed_time is not None:\n        # current examples per second is based on the elapsed training steps\n        # and training time per batch\n        current_examples_per_sec = self._batch_size * (\n            elapsed_steps / elapsed_time)\n        self._total_steps += elapsed_steps\n        if global_step > self._warm_steps:\n          self._step_train_time += elapsed_time\n          self._total_measured_steps += elapsed_steps\n          # average examples per second is based on the total (accumulative)\n          # training steps and training time so far\n          average_examples_per_sec = self._batch_size * (\n              self._total_measured_steps / self._step_train_time)\n          # Current examples/sec followed by average examples/sec\n          tf.compat.v1.logging.info(\'Batch [%g]:  last %g steps exp/sec = %g, total average exp/sec = \'\n                          \'%g\', self._total_steps, elapsed_steps, current_examples_per_sec, average_examples_per_sec)\n        else:\n          # Current examples/sec followed by completed warmup steps\n          tf.compat.v1.logging.info(\'Batch [%g]:  last %g steps exp/sec = %g, completed %g/%g wamrup steps\'\n                          , self._total_steps, elapsed_steps, current_examples_per_sec, self._total_steps, self._warm_steps)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/logs/hooks_helper.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hooks helper to return a list of TensorFlow hooks for training by name.\n\nMore hooks can be added to this set. To add a new hook, 1) add the new hook to\nthe registry in HOOKS, 2) add a corresponding function that parses out necessary\nparameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom mlperf_utils.logs import hooks\nfrom mlperf_utils.logs import metric_hook\n\n_TENSORS_TO_LOG = dict((x, x) for x in [\'learning_rate\',\n                                        \'cross_entropy\',\n                                        \'train_accuracy\'])\n\n\ndef get_train_hooks(name_list, **kwargs):\n  """"""Factory for getting a list of TensorFlow hooks for training by name.\n\n  Args:\n    name_list: a list of strings to name desired hook classes. Allowed:\n      LoggingTensorHook, ProfilerHook, ExamplesPerSecondHook, which are defined\n      as keys in HOOKS\n    **kwargs: a dictionary of arguments to the hooks.\n\n  Returns:\n    list of instantiated hooks, ready to be used in a classifier.train call.\n\n  Raises:\n    ValueError: if an unrecognized name is passed.\n  """"""\n\n  if not name_list:\n    return []\n\n  train_hooks = []\n  for name in name_list:\n    hook_name = HOOKS.get(name.strip().lower())\n    if hook_name is None:\n      raise ValueError(\'Unrecognized training hook requested: {}\'.format(name))\n    else:\n      train_hooks.append(hook_name(**kwargs))\n\n  return train_hooks\n\n\ndef get_logging_tensor_hook(every_n_iter=100, tensors_to_log=None, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get LoggingTensorHook.\n\n  Args:\n    every_n_iter: `int`, print the values of `tensors` once every N local\n      steps taken on the current worker.\n    tensors_to_log: List of tensor names or dictionary mapping labels to tensor\n      names. If not set, log _TENSORS_TO_LOG by default.\n    **kwargs: a dictionary of arguments to LoggingTensorHook.\n\n  Returns:\n    Returns a LoggingTensorHook with a standard set of tensors that will be\n    printed to stdout.\n  """"""\n  if tensors_to_log is None:\n    tensors_to_log = _TENSORS_TO_LOG\n\n  return tf.estimator.LoggingTensorHook(\n      tensors=tensors_to_log,\n      every_n_iter=every_n_iter)\n\n\ndef get_profiler_hook(save_steps=1000, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ProfilerHook.\n\n  Args:\n    save_steps: `int`, print profile traces every N steps.\n    **kwargs: a dictionary of arguments to ProfilerHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return tf.estimator.ProfilerHook(save_steps=save_steps)\n\n\ndef get_examples_per_second_hook(every_n_steps=100,\n                                 batch_size=128,\n                                 warm_steps=500,\n                                 **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ExamplesPerSecondHook.\n\n  Args:\n    every_n_steps: `int`, print current and average examples per second every\n      N steps.\n    batch_size: `int`, total batch size used to calculate examples/second from\n      global time.\n    warm_steps: skip this number of steps before logging and running average.\n    **kwargs: a dictionary of arguments to ExamplesPerSecondHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return hooks.ExamplesPerSecondHook(every_n_steps=every_n_steps,\n                                     batch_size=batch_size,\n                                     warm_steps=warm_steps)\n\n\ndef get_logging_metric_hook(benchmark_log_dir=None,\n                            tensors_to_log=None,\n                            every_n_secs=600,\n                            **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get LoggingMetricHook.\n\n  Args:\n    benchmark_log_dir: `string`, directory path to save the metric log.\n    tensors_to_log: List of tensor names or dictionary mapping labels to tensor\n      names. If not set, log _TENSORS_TO_LOG by default.\n    every_n_secs: `int`, the frequency for logging the metric. Default to every\n      10 mins.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  if benchmark_log_dir is None:\n    raise ValueError(""metric_log_dir should be provided to use metric logger"")\n  if tensors_to_log is None:\n    tensors_to_log = _TENSORS_TO_LOG\n  return metric_hook.LoggingMetricHook(\n      tensors=tensors_to_log,\n      log_dir=benchmark_log_dir,\n      every_n_secs=every_n_secs)\n\n\n# A dictionary to map one hook name and its corresponding function\nHOOKS = {\n    \'loggingtensorhook\': get_logging_tensor_hook,\n    \'profilerhook\': get_profiler_hook,\n    \'examplespersecondhook\': get_examples_per_second_hook,\n    \'loggingmetrichook\': get_logging_metric_hook,\n}\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/logs/logger.py,11,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Logging utilities for benchmark.\n\nFor collecting local environment metrics like CPU and memory, certain python\npackages need be installed. See README for details.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport json\nimport multiprocessing\nimport numbers\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nMETRIC_LOG_FILE_NAME = ""metric.log""\nBENCHMARK_RUN_LOG_FILE_NAME = ""benchmark_run.log""\n_DATE_TIME_FORMAT_PATTERN = ""%Y-%m-%dT%H:%M:%S.%fZ""\n\n\nclass BenchmarkLogger(object):\n  """"""Class to log the benchmark information to local disk.""""""\n\n  def __init__(self, logging_dir):\n    self._logging_dir = logging_dir\n    if not tf.io.gfile.isdir(self._logging_dir):\n      tf.io.gfile.makedirs(self._logging_dir)\n\n  def log_estimator_evaluation_result(self, eval_results):\n    """"""Log the evaluation result for a estimator.\n\n    The evaluate result is a directory that contains metrics defined in\n    model_fn. It also contains a entry for global_step which contains the value\n    of the global step when evaluation was performed.\n\n    Args:\n      eval_results: dict, the result of evaluate() from a estimator.\n    """"""\n    if not isinstance(eval_results, dict):\n      tf.compat.v1.logging.warning(""eval_results should be directory for logging. Got %s"",\n                         type(eval_results))\n      return\n    global_step = eval_results[tf.compat.v1.GraphKeys.GLOBAL_STEP]\n    for key in sorted(eval_results):\n      if key != tf.compat.v1.GraphKeys.GLOBAL_STEP:\n        self.log_metric(key, eval_results[key], global_step=global_step)\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to local file.\n\n    Currently the logging is done in a synchronized way. This should be updated\n    to log asynchronously.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    if not isinstance(value, numbers.Number):\n      tf.compat.v1.logging.warning(\n          ""Metric value to log should be a number. Got %s"", type(value))\n      return\n    if extras:\n      extras = [{""name"": k, ""value"": v} for k, v in sorted(extras.items())]\n    else:\n      extras = []\n    with tf.io.gfile.GFile(\n        os.path.join(self._logging_dir, METRIC_LOG_FILE_NAME), ""a"") as f:\n      metric = {\n          ""name"": name,\n          ""value"": float(value),\n          ""unit"": unit,\n          ""global_step"": global_step,\n          ""timestamp"": datetime.datetime.now().strftime(\n              _DATE_TIME_FORMAT_PATTERN),\n          ""extras"": extras}\n      try:\n        json.dump(metric, f)\n        f.write(""\\n"")\n      except (TypeError, ValueError) as e:\n        tf.compat.v1.logging.warning(""Failed to dump metric to log file: ""\n                           ""name %s, value %s, error %s"", name, value, e)\n\n  def log_run_info(self, model_name):\n    """"""Collect most of the TF runtime information for the local env.\n\n    The schema of the run info follows official/benchmark/datastore/schema.\n\n    Args:\n      model_name: string, the name of the model.\n    """"""\n    run_info = {\n        ""model_name"": model_name,\n        ""machine_config"": {},\n        ""run_date"": datetime.datetime.now().strftime(_DATE_TIME_FORMAT_PATTERN)}\n    _collect_tensorflow_info(run_info)\n    _collect_tensorflow_environment_variables(run_info)\n    _collect_cpu_info(run_info)\n    _collect_gpu_info(run_info)\n    _collect_memory_info(run_info)\n\n    with tf.io.gfile.GFile(os.path.join(\n        self._logging_dir, BENCHMARK_RUN_LOG_FILE_NAME), ""w"") as f:\n      try:\n        json.dump(run_info, f)\n        f.write(""\\n"")\n      except (TypeError, ValueError) as e:\n        tf.compat.v1.logging.warning(""Failed to dump benchmark run info to log file: %s"",\n                           e)\n\n\ndef _collect_tensorflow_info(run_info):\n  run_info[""tensorflow_version""] = {\n      ""version"": tf.version.VERSION, ""git_hash"": tf.version.GIT_VERSION}\n\n\ndef _collect_tensorflow_environment_variables(run_info):\n  run_info[""tensorflow_environment_variables""] = [\n      {""name"": k, ""value"": v}\n      for k, v in sorted(os.environ.items()) if k.startswith(""TF_"")]\n\n\n# The following code is mirrored from tensorflow/tools/test/system_info_lib\n# which is not exposed for import.\ndef _collect_cpu_info(run_info):\n  """"""Collect the CPU information for the local environment.""""""\n  cpu_info = {}\n\n  cpu_info[""num_cores""] = multiprocessing.cpu_count()\n\n  # Note: cpuinfo is not installed in the TensorFlow OSS tree.\n  # It is installable via pip.\n  import cpuinfo    # pylint: disable=g-import-not-at-top\n\n  info = cpuinfo.get_cpu_info()\n  cpu_info[""cpu_info""] = info[""brand""]\n  cpu_info[""mhz_per_cpu""] = info[""hz_advertised_raw""][0] / 1.0e6\n\n  run_info[""machine_config""][""cpu_info""] = cpu_info\n\n\ndef _collect_gpu_info(run_info):\n  """"""Collect local GPU information by TF device library.""""""\n  gpu_info = {}\n  local_device_protos = device_lib.list_local_devices()\n\n  gpu_info[""count""] = len([d for d in local_device_protos\n                           if d.device_type == ""GPU""])\n  # The device description usually is a JSON string, which contains the GPU\n  # model info, eg:\n  # ""device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0""\n  for d in local_device_protos:\n    if d.device_type == ""GPU"":\n      gpu_info[""model""] = _parse_gpu_model(d.physical_device_desc)\n      # Assume all the GPU connected are same model\n      break\n  run_info[""machine_config""][""gpu_info""] = gpu_info\n\n\ndef _collect_memory_info(run_info):\n  # Note: psutil is not installed in the TensorFlow OSS tree.\n  # It is installable via pip.\n  import psutil   # pylint: disable=g-import-not-at-top\n  vmem = psutil.virtual_memory()\n  run_info[""machine_config""][""memory_total""] = vmem.total\n  run_info[""machine_config""][""memory_available""] = vmem.available\n\n\ndef _parse_gpu_model(physical_device_desc):\n  # Assume all the GPU connected are same model\n  for kv in physical_device_desc.split("",""):\n    k, _, v = kv.partition("":"")\n    if k.strip() == ""name"":\n      return v.strip()\n  return None\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/logs/metric_hook.py,3,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Session hook for logging benchmark metric.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom mlperf_utils.logs import logger\n\n\nclass LoggingMetricHook(tf.estimator.LoggingTensorHook):\n  """"""Hook to log benchmark metric information.\n\n  This hook is very similar as tf.train.LoggingTensorHook, which logs given\n  tensors every N local steps, every N seconds, or at the end. The metric\n  information will be logged to given log_dir or via metric_logger in JSON\n  format, which can be consumed by data analysis pipeline later.\n\n  Note that if `at_end` is True, `tensors` should not include any tensor\n  whose evaluation produces a side effect such as consuming additional inputs.\n  """"""\n\n  def __init__(self, tensors, log_dir=None, metric_logger=None,\n               every_n_iter=None, every_n_secs=None, at_end=False):\n    """"""Initializer for LoggingMetricHook.\n\n    Args:\n      tensors: `dict` that maps string-valued tags to tensors/tensor names,\n          or `iterable` of tensors/tensor names.\n      log_dir: `string`, directory path that metric hook should write log to.\n      metric_logger: instance of `BenchmarkLogger`, the benchmark logger that\n          hook should use to write the log. Exactly one of the `log_dir` and\n          `metric_logger` should be provided.\n      every_n_iter: `int`, print the values of `tensors` once every N local\n          steps taken on the current worker.\n      every_n_secs: `int` or `float`, print the values of `tensors` once every N\n          seconds. Exactly one of `every_n_iter` and `every_n_secs` should be\n          provided.\n      at_end: `bool` specifying whether to print the values of `tensors` at the\n          end of the run.\n\n    Raises:\n      ValueError:\n        1. `every_n_iter` is non-positive, or\n        2. Exactly one of every_n_iter and every_n_secs should be provided.\n        3. Exactly one of log_dir and metric_logger should be provided.\n    """"""\n    super(LoggingMetricHook, self).__init__(\n        tensors=tensors,\n        every_n_iter=every_n_iter,\n        every_n_secs=every_n_secs,\n        at_end=at_end)\n\n    if (log_dir is None) == (metric_logger is None):\n      raise ValueError(\n          ""exactly one of log_dir and metric_logger should be provided."")\n\n    if log_dir is not None:\n      self._logger = logger.BenchmarkLogger(log_dir)\n    else:\n      self._logger = metric_logger\n\n  def begin(self):\n    super(LoggingMetricHook, self).begin()\n    self._global_step_tensor = tf.compat.v1.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          ""Global step should be created to use LoggingMetricHook."")\n    if self._global_step_tensor.name not in self._current_tensors:\n      self._current_tensors[self._global_step_tensor.name] = (\n          self._global_step_tensor)\n\n  def after_run(self, unused_run_context, run_values):\n    # should_trigger is a internal state that populated at before_run, and it is\n    # using self_timer to determine whether it should trigger.\n    if self._should_trigger:\n      self._log_metric(run_values.results)\n\n    self._iter_count += 1\n\n  def end(self, session):\n    if self._log_at_end:\n      values = session.run(self._current_tensors)\n      self._log_metric(values)\n\n  def _log_metric(self, tensor_values):\n    self._timer.update_last_triggered_step(self._iter_count)\n    global_step = tensor_values[self._global_step_tensor.name]\n    # self._tag_order is populated during the init of LoggingTensorHook\n    for tag in self._tag_order:\n      self._logger.log_metric(tag, tensor_values[tag], global_step=global_step)\n'"
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/misc/__init__.py,0,b''
models/image_recognition/tensorflow/resnet50v1_5/training/mlperf_utils/misc/model_helpers.py,1,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Miscellaneous functions that can be called by models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numbers\n\nimport tensorflow as tf\n\n\ndef past_stop_threshold(stop_threshold, eval_metric):\n  """"""Return a boolean representing whether a model should be stopped.\n\n  Args:\n    stop_threshold: float, the threshold above which a model should stop\n      training.\n    eval_metric: float, the current value of the relevant metric to check.\n\n  Returns:\n    True if training should stop, False otherwise.\n\n  Raises:\n    ValueError: if either stop_threshold or eval_metric is not a number\n  """"""\n  if stop_threshold is None:\n    return False\n\n  if not isinstance(stop_threshold, numbers.Number):\n    raise ValueError(""Threshold for checking stop conditions must be a number."")\n  if not isinstance(eval_metric, numbers.Number):\n    raise ValueError(""Eval metric being checked against stop conditions ""\n                     ""must be a number."")\n\n  if eval_metric >= stop_threshold:\n    tf.compat.v1.logging.info(\n        ""Stop threshold of {} was passed with metric value {}."".format(\n            stop_threshold, eval_metric))\n    return True\n\n  return False\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/utils/__init__.py,0,b''
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/utils/metrics.py,50,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions for calculating loss, accuracy, and other model metrics.\n\nMetrics:\n - Padded loss, accuracy, and negative log perplexity. Source:\n     https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/metrics.py\n - BLEU approximation. Source:\n     https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/bleu_hook.py\n - ROUGE score. Source:\n     https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/rouge.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\n\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\ndef _pad_tensors_to_same_length(x, y):\n  """"""Pad x and y so that the results have the same length (second dimension).""""""\n  with tf.name_scope(""pad_to_same_length""):\n    x_length = tf.shape(x)[1]\n    y_length = tf.shape(y)[1]\n\n    max_length = tf.maximum(x_length, y_length)\n\n    x = tf.pad(x, [[0, 0], [0, max_length - x_length], [0, 0]])\n    y = tf.pad(y, [[0, 0], [0, max_length - y_length]])\n    return x, y\n\n\ndef padded_cross_entropy_loss(logits, labels, smoothing, vocab_size):\n  """"""Calculate cross entropy loss while ignoring padding.\n\n  Args:\n    logits: Tensor of size [batch_size, length_logits, vocab_size]\n    labels: Tensor of size [batch_size, length_labels]\n    smoothing: Label smoothing constant, used to determine the on and off values\n    vocab_size: int size of the vocabulary\n  Returns:\n    Returns the cross entropy loss and weight tensors: float32 tensors with\n      shape [batch_size, max(length_logits, length_labels)]\n  """"""\n  with tf.name_scope(""loss"", values=[logits, labels]):\n    logits, labels = _pad_tensors_to_same_length(logits, labels)\n\n    # Calculate smoothing cross entropy\n    with tf.name_scope(""smoothing_cross_entropy"", values=[logits, labels]):\n      confidence = 1.0 - smoothing\n      low_confidence = (1.0 - confidence) / tf.to_float(vocab_size - 1)\n      soft_targets = tf.one_hot(\n          tf.cast(labels, tf.int32),\n          depth=vocab_size,\n          on_value=confidence,\n          off_value=low_confidence)\n      xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n          logits=logits, labels=soft_targets)\n\n      # Calculate the best (lowest) possible value of cross entropy, and\n      # subtract from the cross entropy loss.\n      normalizing_constant = -(\n          confidence * tf.log(confidence) + tf.to_float(vocab_size - 1) *\n          low_confidence * tf.log(low_confidence + 1e-20))\n      xentropy -= normalizing_constant\n\n    weights = tf.to_float(tf.not_equal(labels, 0))\n    return xentropy * weights, weights\n\n\ndef _convert_to_eval_metric(metric_fn):\n  """"""Wrap a metric fn that returns scores and weights as an eval metric fn.\n\n  The input metric_fn returns values for the current batch. The wrapper\n  aggregates the return values collected over all of the batches evaluated.\n\n  Args:\n    metric_fn: function that returns scores and weights for the current batch\'s\n      logits and predicted labels.\n\n  Returns:\n    function that aggregates the scores and weights from metric_fn.\n  """"""\n  def problem_metric_fn(*args):\n    """"""Returns an aggregation of the metric_fn\'s returned values.""""""\n    (scores, weights) = metric_fn(*args)\n\n    # The tf.metrics.mean function assures correct aggregation.\n    return tf.metrics.mean(scores, weights)\n  return problem_metric_fn\n\n\ndef get_eval_metrics(logits, labels, params):\n  """"""Return dictionary of model evaluation metrics.""""""\n  metrics = {\n      ""accuracy"": _convert_to_eval_metric(padded_accuracy)(logits, labels),\n      ""accuracy_top5"": _convert_to_eval_metric(padded_accuracy_top5)(\n          logits, labels),\n      ""accuracy_per_sequence"": _convert_to_eval_metric(\n          padded_sequence_accuracy)(logits, labels),\n      ""neg_log_perplexity"": _convert_to_eval_metric(padded_neg_log_perplexity)(\n          logits, labels, params[""vocab_size""]),\n  }\n\n  if not params[""use_tpu""]:\n    # TPU does not support tf.py_func\n    metrics.update({\n        ""approx_bleu_score"": _convert_to_eval_metric(\n            bleu_score)(logits, labels),\n        ""rouge_2_fscore"": _convert_to_eval_metric(\n            rouge_2_fscore)(logits, labels),\n        ""rouge_L_fscore"": _convert_to_eval_metric(\n            rouge_l_fscore)(logits, labels),\n    })\n\n  # Prefix each of the metric names with ""metrics/"". This allows the metric\n  # graphs to display under the ""metrics"" category in TensorBoard.\n  metrics = {""metrics/%s"" % k: v for k, v in six.iteritems(metrics)}\n  return metrics\n\n\ndef padded_accuracy(logits, labels):\n  """"""Percentage of times that predictions matches labels on non-0s.""""""\n  with tf.variable_scope(""padded_accuracy"", values=[logits, labels]):\n    logits, labels = _pad_tensors_to_same_length(logits, labels)\n    weights = tf.to_float(tf.not_equal(labels, 0))\n    outputs = tf.to_int32(tf.argmax(logits, axis=-1))\n    padded_labels = tf.to_int32(labels)\n    return tf.to_float(tf.equal(outputs, padded_labels)), weights\n\n\ndef padded_accuracy_topk(logits, labels, k):\n  """"""Percentage of times that top-k predictions matches labels on non-0s.""""""\n  with tf.variable_scope(""padded_accuracy_topk"", values=[logits, labels]):\n    logits, labels = _pad_tensors_to_same_length(logits, labels)\n    weights = tf.to_float(tf.not_equal(labels, 0))\n    effective_k = tf.minimum(k, tf.shape(logits)[-1])\n    _, outputs = tf.nn.top_k(logits, k=effective_k)\n    outputs = tf.to_int32(outputs)\n    padded_labels = tf.to_int32(labels)\n    padded_labels = tf.expand_dims(padded_labels, axis=-1)\n    padded_labels += tf.zeros_like(outputs)  # Pad to same shape.\n    same = tf.to_float(tf.equal(outputs, padded_labels))\n    same_topk = tf.reduce_sum(same, axis=-1)\n    return same_topk, weights\n\n\ndef padded_accuracy_top5(logits, labels):\n  return padded_accuracy_topk(logits, labels, 5)\n\n\ndef padded_sequence_accuracy(logits, labels):\n  """"""Percentage of times that predictions matches labels everywhere (non-0).""""""\n  with tf.variable_scope(""padded_sequence_accuracy"", values=[logits, labels]):\n    logits, labels = _pad_tensors_to_same_length(logits, labels)\n    weights = tf.to_float(tf.not_equal(labels, 0))\n    outputs = tf.to_int32(tf.argmax(logits, axis=-1))\n    padded_labels = tf.to_int32(labels)\n    not_correct = tf.to_float(tf.not_equal(outputs, padded_labels)) * weights\n    axis = list(range(1, len(outputs.get_shape())))\n    correct_seq = 1.0 - tf.minimum(1.0, tf.reduce_sum(not_correct, axis=axis))\n    return correct_seq, tf.constant(1.0)\n\n\ndef padded_neg_log_perplexity(logits, labels, vocab_size):\n  """"""Average log-perplexity excluding padding 0s. No smoothing.""""""\n  num, den = padded_cross_entropy_loss(logits, labels, 0, vocab_size)\n  return -num, den\n\n\ndef bleu_score(logits, labels):\n  """"""Approximate BLEU score computation between labels and predictions.\n\n  An approximate BLEU scoring method since we do not glue word pieces or\n  decode the ids and tokenize the output. By default, we use ngram order of 4\n  and use brevity penalty. Also, this does not have beam search.\n\n  Args:\n    logits: Tensor of size [batch_size, length_logits, vocab_size]\n    labels: Tensor of size [batch-size, length_labels]\n\n  Returns:\n    bleu: int, approx bleu score\n  """"""\n  predictions = tf.to_int32(tf.argmax(logits, axis=-1))\n  # TODO: Look into removing use of py_func\n  bleu = tf.py_func(compute_bleu, (labels, predictions), tf.float32)\n  return bleu, tf.constant(1.0)\n\n\ndef _get_ngrams_with_counter(segment, max_order):\n  """"""Extracts all n-grams up to a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  """"""\n  ngram_counts = collections.Counter()\n  for order in xrange(1, max_order + 1):\n    for i in xrange(0, len(segment) - order + 1):\n      ngram = tuple(segment[i:i + order])\n      ngram_counts[ngram] += 1\n  return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 use_bp=True):\n  """"""Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    use_bp: boolean, whether to apply brevity penalty.\n\n  Returns:\n    BLEU score.\n  """"""\n  reference_length = 0\n  translation_length = 0\n  bp = 1.0\n  geo_mean = 0\n\n  matches_by_order = [0] * max_order\n  possible_matches_by_order = [0] * max_order\n  precisions = []\n\n  for (references, translations) in zip(reference_corpus, translation_corpus):\n    reference_length += len(references)\n    translation_length += len(translations)\n    ref_ngram_counts = _get_ngrams_with_counter(references, max_order)\n    translation_ngram_counts = _get_ngrams_with_counter(translations, max_order)\n\n    overlap = dict((ngram,\n                    min(count, translation_ngram_counts[ngram]))\n                   for ngram, count in ref_ngram_counts.items())\n\n    for ngram in overlap:\n      matches_by_order[len(ngram) - 1] += overlap[ngram]\n    for ngram in translation_ngram_counts:\n      possible_matches_by_order[len(ngram) - 1] += translation_ngram_counts[\n          ngram]\n\n  precisions = [0] * max_order\n  smooth = 1.0\n\n  for i in xrange(0, max_order):\n    if possible_matches_by_order[i] > 0:\n      precisions[i] = float(matches_by_order[i]) / possible_matches_by_order[i]\n      if matches_by_order[i] > 0:\n        precisions[i] = float(matches_by_order[i]) / possible_matches_by_order[\n            i]\n      else:\n        smooth *= 2\n        precisions[i] = 1.0 / (smooth * possible_matches_by_order[i])\n    else:\n      precisions[i] = 0.0\n\n  if max(precisions) > 0:\n    p_log_sum = sum(math.log(p) for p in precisions if p)\n    geo_mean = math.exp(p_log_sum / max_order)\n\n  if use_bp:\n    ratio = translation_length / reference_length\n    bp = math.exp(1 - 1. / ratio) if ratio < 1.0 else 1.0\n  bleu = geo_mean * bp\n  return np.float32(bleu)\n\n\ndef rouge_2_fscore(logits, labels):\n  """"""ROUGE-2 F1 score computation between labels and predictions.\n\n  This is an approximate ROUGE scoring method since we do not glue word pieces\n  or decode the ids and tokenize the output.\n\n  Args:\n    logits: tensor, model predictions\n    labels: tensor, gold output.\n\n  Returns:\n    rouge2_fscore: approx rouge-2 f1 score.\n  """"""\n  predictions = tf.to_int32(tf.argmax(logits, axis=-1))\n  # TODO: Look into removing use of py_func\n  rouge_2_f_score = tf.py_func(rouge_n, (predictions, labels), tf.float32)\n  return rouge_2_f_score, tf.constant(1.0)\n\n\ndef _get_ngrams(n, text):\n  """"""Calculates n-grams.\n\n  Args:\n    n: which n-grams to calculate\n    text: An array of tokens\n\n  Returns:\n    A set of n-grams\n  """"""\n  ngram_set = set()\n  text_length = len(text)\n  max_index_ngram_start = text_length - n\n  for i in range(max_index_ngram_start + 1):\n    ngram_set.add(tuple(text[i:i + n]))\n  return ngram_set\n\n\ndef rouge_n(eval_sentences, ref_sentences, n=2):\n  """"""Computes ROUGE-N f1 score of two text collections of sentences.\n\n  Source: https://www.microsoft.com/en-us/research/publication/\n  rouge-a-package-for-automatic-evaluation-of-summaries/\n\n  Args:\n    eval_sentences: Predicted sentences.\n    ref_sentences: Sentences from the reference set\n    n: Size of ngram.  Defaults to 2.\n\n  Returns:\n    f1 score for ROUGE-N\n  """"""\n  f1_scores = []\n  for eval_sentence, ref_sentence in zip(eval_sentences, ref_sentences):\n    eval_ngrams = _get_ngrams(n, eval_sentence)\n    ref_ngrams = _get_ngrams(n, ref_sentence)\n    ref_count = len(ref_ngrams)\n    eval_count = len(eval_ngrams)\n\n    # Count the overlapping ngrams between evaluated and reference\n    overlapping_ngrams = eval_ngrams.intersection(ref_ngrams)\n    overlapping_count = len(overlapping_ngrams)\n\n    # Handle edge case. This isn\'t mathematically correct, but it\'s good enough\n    if eval_count == 0:\n      precision = 0.0\n    else:\n      precision = float(overlapping_count) / eval_count\n    if ref_count == 0:\n      recall = 0.0\n    else:\n      recall = float(overlapping_count) / ref_count\n    f1_scores.append(2.0 * ((precision * recall) / (precision + recall + 1e-8)))\n\n  # return overlapping_count / reference_count\n  return np.mean(f1_scores, dtype=np.float32)\n\n\ndef rouge_l_fscore(predictions, labels):\n  """"""ROUGE scores computation between labels and predictions.\n\n  This is an approximate ROUGE scoring method since we do not glue word pieces\n  or decode the ids and tokenize the output.\n\n  Args:\n    predictions: tensor, model predictions\n    labels: tensor, gold output.\n\n  Returns:\n    rouge_l_fscore: approx rouge-l f1 score.\n  """"""\n  outputs = tf.to_int32(tf.argmax(predictions, axis=-1))\n  rouge_l_f_score = tf.py_func(rouge_l_sentence_level, (outputs, labels),\n                               tf.float32)\n  return rouge_l_f_score, tf.constant(1.0)\n\n\ndef rouge_l_sentence_level(eval_sentences, ref_sentences):\n  """"""Computes ROUGE-L (sentence level) of two collections of sentences.\n\n  Source: https://www.microsoft.com/en-us/research/publication/\n  rouge-a-package-for-automatic-evaluation-of-summaries/\n\n  Calculated according to:\n  R_lcs = LCS(X,Y)/m\n  P_lcs = LCS(X,Y)/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  X = reference summary\n  Y = Candidate summary\n  m = length of reference summary\n  n = length of candidate summary\n\n  Args:\n    eval_sentences: The sentences that have been picked by the summarizer\n    ref_sentences: The sentences from the reference set\n\n  Returns:\n    A float: F_lcs\n  """"""\n\n  f1_scores = []\n  for eval_sentence, ref_sentence in zip(eval_sentences, ref_sentences):\n    m = float(len(ref_sentence))\n    n = float(len(eval_sentence))\n    lcs = _len_lcs(eval_sentence, ref_sentence)\n    f1_scores.append(_f_lcs(lcs, m, n))\n  return np.mean(f1_scores, dtype=np.float32)\n\n\ndef _len_lcs(x, y):\n  """"""Returns the length of the Longest Common Subsequence between two seqs.\n\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns\n    integer: Length of LCS between x and y\n  """"""\n  table = _lcs(x, y)\n  n, m = len(x), len(y)\n  return table[n, m]\n\n\ndef _lcs(x, y):\n  """"""Computes the length of the LCS between two seqs.\n\n  The implementation below uses a DP programming algorithm and runs\n  in O(nm) time where n = len(x) and m = len(y).\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: collection of words\n    y: collection of words\n\n  Returns:\n    Table of dictionary of coord and len lcs\n  """"""\n  n, m = len(x), len(y)\n  table = dict()\n  for i in range(n + 1):\n    for j in range(m + 1):\n      if i == 0 or j == 0:\n        table[i, j] = 0\n      elif x[i - 1] == y[j - 1]:\n        table[i, j] = table[i - 1, j - 1] + 1\n      else:\n        table[i, j] = max(table[i - 1, j], table[i, j - 1])\n  return table\n\n\ndef _f_lcs(llcs, m, n):\n  """"""Computes the LCS-based F-measure score.\n\n  Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Args:\n    llcs: Length of LCS\n    m: number of words in reference summary\n    n: number of words in candidate summary\n\n  Returns:\n    Float. LCS-based F-measure score\n  """"""\n  r_lcs = llcs / m\n  p_lcs = llcs / n\n  beta = p_lcs / (r_lcs + 1e-12)\n  num = (1 + (beta ** 2)) * r_lcs * p_lcs\n  denom = r_lcs + ((beta ** 2) * p_lcs)\n  f_lcs = num / (denom + 1e-12)\n  return f_lcs\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/utils/tokenizer.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Defines Subtokenizer class to encode and decode strings.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport sys\nimport unicodedata\n\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nPAD = ""<pad>""\nPAD_ID = 0\nEOS = ""<EOS>""\nEOS_ID = 1\nRESERVED_TOKENS = [PAD, EOS]\n\n# Set of characters that will be used in the function _escape_token() (see func\n# docstring for more details).\n# This set is added to the alphabet list to ensure that all escaped tokens can\n# be encoded.\n_ESCAPE_CHARS = set(u""\\\\_u;0123456789"")\n# Regex for the function _unescape_token(), the inverse of _escape_token().\n# This is used to find ""\\u"", ""\\\\"", and ""\\###;"" substrings in the token.\n_UNESCAPE_REGEX = re.compile(r""\\\\u|\\\\\\\\|\\\\([0-9]+);"")\n\n_UNDEFINED_UNICODE = u""\\u3013""\n\n# Set contains all letter and number characters.\n_ALPHANUMERIC_CHAR_SET = set(\n    six.unichr(i) for i in xrange(sys.maxunicode)\n    if (unicodedata.category(six.unichr(i)).startswith(""L"") or\n        unicodedata.category(six.unichr(i)).startswith(""N"")))\n\n# min_count is the minimum number of times a subtoken must appear in the data\n# before before it is added to the vocabulary. The value is found using binary\n# search to obtain the target vocabulary size.\n_MIN_MIN_COUNT = 1     # min value to use when binary searching for min_count\n_MAX_MIN_COUNT = 1000  # max value to use when binary searching for min_count\n\n\nclass Subtokenizer(object):\n  """"""Encodes and decodes strings to/from integer IDs.""""""\n\n  def __init__(self, vocab_file, reserved_tokens=None):\n    """"""Initializes class, creating a vocab file if data_files is provided.""""""\n    tf.compat.v1.logging.info(""Initializing Subtokenizer from file %s."" %\n                              vocab_file)\n\n    if reserved_tokens is None:\n      reserved_tokens = RESERVED_TOKENS\n\n    self.subtoken_list = _load_vocab_file(vocab_file, reserved_tokens)\n    self.alphabet = _generate_alphabet_dict(self.subtoken_list)\n    self.subtoken_to_id_dict = _list_to_index_dict(self.subtoken_list)\n\n    self.max_subtoken_length = 0\n    for subtoken in self.subtoken_list:\n      self.max_subtoken_length = max(self.max_subtoken_length, len(subtoken))\n\n    # Create cache to speed up subtokenization\n    self._cache_size = 2 ** 20\n    self._cache = [(None, None)] * self._cache_size\n\n  @staticmethod\n  def init_from_files(\n      vocab_file, files, target_vocab_size, threshold, min_count=None,\n      file_byte_limit=1e6, reserved_tokens=None, correct_strip=True):\n    """"""Create subtoken vocabulary based on files, and save vocab to file.\n\n    Args:\n      vocab_file: String name of vocab file to store subtoken vocabulary.\n      files: List of file paths that will be used to generate vocabulary.\n      target_vocab_size: target vocabulary size to generate.\n      threshold: int threshold of vocabulary size to accept.\n      min_count: int minimum count to use for generating the vocabulary. The min\n        count is the minimum number of times a subtoken should appear in the\n        files before it is added to the vocabulary. If set to none, this value\n        is found using binary search.\n      file_byte_limit: (Default 1e6) Maximum number of bytes of sample text that\n        will be drawn from the files.\n      reserved_tokens: List of string tokens that are guaranteed to be at the\n        beginning of the subtoken vocabulary list.\n      correct_strip: Whether to convert text to unicode before strip.\n\n    Returns:\n      Subtokenizer object\n    """"""\n    if reserved_tokens is None:\n      reserved_tokens = RESERVED_TOKENS\n\n    if tf.io.gfile.exists(vocab_file):\n      tf.compat.v1.logging.info(""Vocab file already exists (%s)"" % vocab_file)\n    else:\n      tf.compat.v1.logging.info(""Begin steps to create subtoken vocabulary..."")\n      token_counts = _count_tokens(files, file_byte_limit, correct_strip)\n      alphabet = _generate_alphabet_dict(token_counts)\n      subtoken_list = _generate_subtokens_with_target_vocab_size(\n          token_counts, alphabet, target_vocab_size, threshold, min_count,\n          reserved_tokens)\n      tf.compat.v1.logging.info(""Generated vocabulary with %d subtokens."" %\n                                len(subtoken_list))\n      _save_vocab_file(vocab_file, subtoken_list)\n    return Subtokenizer(vocab_file)\n\n  def encode(self, raw_string, add_eos=False):\n    """"""Encodes a string into a list of int subtoken ids.""""""\n    ret = []\n    tokens = _split_string_to_tokens(native_to_unicode(raw_string))\n    for token in tokens:\n      ret.extend(self._token_to_subtoken_ids(token))\n    if add_eos:\n      ret.append(EOS_ID)\n    return ret\n\n  def _token_to_subtoken_ids(self, token):\n    """"""Encode a single token into a list of subtoken ids.""""""\n    cache_location = hash(token) % self._cache_size\n    cache_key, cache_value = self._cache[cache_location]\n    if cache_key == token:\n      return cache_value\n\n    ret = _split_token_to_subtokens(\n        _escape_token(token, self.alphabet), self.subtoken_to_id_dict,\n        self.max_subtoken_length)\n    ret = [self.subtoken_to_id_dict[subtoken_id] for subtoken_id in ret]\n\n    self._cache[cache_location] = (token, ret)\n    return ret\n\n  def decode(self, subtokens):\n    """"""Converts list of int subtokens ids into a string.""""""\n    if isinstance(subtokens, np.ndarray):\n      # Note that list(subtokens) converts subtokens to a python list, but the\n      # items remain as np.int32. This converts both the array and its items.\n      subtokens = subtokens.tolist()\n\n    if not subtokens:\n      return """"\n\n    assert isinstance(subtokens, list) and isinstance(subtokens[0], int), (\n        ""Subtokens argument passed into decode() must be a list of integers."")\n\n    return _unicode_to_native(\n        _join_tokens_to_string(self._subtoken_ids_to_tokens(subtokens)))\n\n  def _subtoken_ids_to_tokens(self, subtokens):\n    """"""Convert list of int subtoken ids to a list of string tokens.""""""\n    escaped_tokens = """".join([\n        self.subtoken_list[s] for s in subtokens\n        if s < len(self.subtoken_list)])\n    escaped_tokens = escaped_tokens.split(""_"")\n\n    # All tokens in the vocabulary list have been escaped (see _escape_token())\n    # so each token must be unescaped when decoding.\n    ret = []\n    for token in escaped_tokens:\n      if token:\n        ret.append(_unescape_token(token))\n    return ret\n\n\ndef _save_vocab_file(vocab_file, subtoken_list):\n  """"""Save subtokens to file.""""""\n  with tf.io.gfile.GFile(vocab_file, mode=""w"") as f:\n    for subtoken in subtoken_list:\n      f.write(""\'%s\'\\n"" % _unicode_to_native(subtoken))\n\n\ndef _load_vocab_file(vocab_file, reserved_tokens=None):\n  """"""Load vocabulary while ensuring reserved tokens are at the top.""""""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  subtoken_list = []\n  with tf.io.gfile.GFile(vocab_file, mode=""r"") as f:\n    for line in f:\n      subtoken = native_to_unicode(line.strip())\n      subtoken = subtoken[1:-1]  # Remove surrounding single-quotes\n      if subtoken in reserved_tokens:\n        continue\n      subtoken_list.append(native_to_unicode(subtoken))\n  return reserved_tokens + subtoken_list\n\n\ndef native_to_unicode(s):\n  """"""Convert string to unicode (required in Python 2).""""""\n  try:               # Python 2\n    return s if isinstance(s, unicode) else s.decode(""utf-8"")\n  except NameError:  # Python 3\n    return s\n\n\ndef _unicode_to_native(s):\n  """"""Convert string from unicode to native format (required in Python 2).""""""\n  try:               # Python 2\n    return s.encode(""utf-8"") if isinstance(s, unicode) else s\n  except NameError:  # Python 3\n    return s\n\n\ndef _split_string_to_tokens(text):\n  """"""Splits text to a list of string tokens.""""""\n  if not text:\n    return []\n  ret = []\n  token_start = 0\n  # Classify each character in the input string\n  is_alnum = [c in _ALPHANUMERIC_CHAR_SET for c in text]\n  for pos in xrange(1, len(text)):\n    if is_alnum[pos] != is_alnum[pos - 1]:\n      token = text[token_start:pos]\n      if token != u"" "" or token_start == 0:\n        ret.append(token)\n      token_start = pos\n  final_token = text[token_start:]\n  ret.append(final_token)\n  return ret\n\n\ndef _join_tokens_to_string(tokens):\n  """"""Join a list of string tokens into a single string.""""""\n  token_is_alnum = [t[0] in _ALPHANUMERIC_CHAR_SET for t in tokens]\n  ret = []\n  for i, token in enumerate(tokens):\n    if i > 0 and token_is_alnum[i - 1] and token_is_alnum[i]:\n      ret.append(u"" "")\n    ret.append(token)\n  return """".join(ret)\n\n\ndef _escape_token(token, alphabet):\n  r""""""Replace characters that aren\'t in the alphabet and append ""_"" to token.\n\n  Apply three transformations to the token:\n    1. Replace underline character ""_"" with ""\\u"", and backslash ""\\"" with ""\\\\"".\n    2. Replace characters outside of the alphabet with ""\\###;"", where ### is the\n       character\'s Unicode code point.\n    3. Appends ""_"" to mark the end of a token.\n\n  Args:\n    token: unicode string to be escaped\n    alphabet: list of all known characters\n\n  Returns:\n    escaped string\n  """"""\n  token = token.replace(u""\\\\"", u""\\\\\\\\"").replace(u""_"", u""\\\\u"")\n  ret = [c if c in alphabet and c != u""\\n"" else r""\\%d;"" % ord(c) for c in token]\n  return u"""".join(ret) + ""_""\n\n\ndef _unescape_token(token):\n  r""""""Replaces escaped characters in the token with their unescaped versions.\n\n  Applies inverse transformations as _escape_token():\n    1. Replace ""\\u"" with ""_"", and ""\\\\"" with ""\\"".\n    2. Replace ""\\###;"" with the unicode character the ### refers to.\n\n  Args:\n    token: escaped string\n\n  Returns:\n    unescaped string\n  """"""\n\n  def match(m):\n    r""""""Returns replacement string for matched object.\n\n    Matched objects contain one of the strings that matches the regex pattern:\n      r""\\\\u|\\\\\\\\|\\\\([0-9]+);""\n    The strings can be \'\\u\', \'\\\\\', or \'\\###;\' (### is any digit number).\n\n    m.group(0) refers to the entire matched string (\'\\u\', \'\\\\\', or \'\\###;\').\n    m.group(1) refers to the first parenthesized subgroup (\'###\').\n\n    m.group(0) exists for all match objects, while m.group(1) exists only for\n    the string \'\\###;\'.\n\n    This function looks to see if m.group(1) exists. If it doesn\'t, then the\n    matched string must be \'\\u\' or \'\\\\\' . In this case, the corresponding\n    replacement (\'_\' and \'\\\') are returned. Note that in python, a single\n    backslash is written as \'\\\\\', and double backslash as \'\\\\\\\\\'.\n\n    If m.goup(1) exists, then use the integer in m.group(1) to return a\n    unicode character.\n\n    Args:\n      m: match object\n\n    Returns:\n      String to replace matched object with.\n    """"""\n    # Check if the matched strings are \'\\u\' or \'\\\\\'.\n    if m.group(1) is None:\n      return u""_"" if m.group(0) == u""\\\\u"" else u""\\\\""\n\n    # If m.group(1) exists, try and return unicode character.\n    try:\n      return six.unichr(int(m.group(1)))\n    except (ValueError, OverflowError) as _:\n      return _UNDEFINED_UNICODE\n\n  # Use match function to replace escaped substrings in the token.\n  return _UNESCAPE_REGEX.sub(match, token)\n\n\ndef _count_tokens(files, file_byte_limit=1e6, correct_strip=True):\n  """"""Return token counts of words in the files.\n\n  Samples file_byte_limit bytes from each file, and counts the words that appear\n  in the samples. The samples are semi-evenly distributed across the file.\n\n  Args:\n    files: List of filepaths\n    file_byte_limit: Max number of bytes that will be read from each file.\n    correct_strip: Whether to convert text to unicode before strip. This affects\n      vocabulary generation for PY2. Sets correct_strip to False in PY2 to\n      reproduce previous common public result. Sets correct_strip to True will\n      let PY2 and PY3 get a consistent vocabulary.\n\n  Returns:\n    Dictionary mapping tokens to the number of times they appear in the sampled\n    lines from the files.\n  """"""\n  token_counts = collections.defaultdict(int)\n\n  for filepath in files:\n    with tf.io.gfile.GFile(filepath, mode=""r"") as reader:\n      file_byte_budget = file_byte_limit\n      counter = 0\n      lines_to_skip = int(reader.size() / (file_byte_budget * 2))\n      for line in reader:\n        if counter < lines_to_skip:\n          counter += 1\n        else:\n          if file_byte_budget < 0:\n            break\n          if correct_strip:\n            line = native_to_unicode(line)\n          line = line.strip()\n          file_byte_budget -= len(line)\n          counter = 0\n\n          # Add words to token counts\n          for token in _split_string_to_tokens(native_to_unicode(line)):\n            token_counts[token] += 1\n  return token_counts\n\n\ndef _list_to_index_dict(lst):\n  """"""Create dictionary mapping list items to their indices in the list.""""""\n  return {item: n for n, item in enumerate(lst)}\n\n\ndef _split_token_to_subtokens(token, subtoken_dict, max_subtoken_length):\n  """"""Splits a token into subtokens defined in the subtoken dict.""""""\n  ret = []\n  start = 0\n  token_len = len(token)\n  while start < token_len:\n    # Find the longest subtoken, so iterate backwards.\n    for end in xrange(min(token_len, start + max_subtoken_length), start, -1):\n      subtoken = token[start:end]\n      if subtoken in subtoken_dict:\n        ret.append(subtoken)\n        start = end\n        break\n    else:  # Did not break\n      # If there is no possible encoding of the escaped token then one of the\n      # characters in the token is not in the alphabet. This should be\n      # impossible and would be indicative of a bug.\n      raise ValueError(""Was unable to split token \\""%s\\"" into subtokens."" %\n                       token)\n  return ret\n\n\ndef _generate_subtokens_with_target_vocab_size(\n    token_counts, alphabet, target_size, threshold, min_count=None,\n    reserved_tokens=None):\n  """"""Generate subtoken vocabulary close to the target size.""""""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  if min_count is not None:\n    tf.compat.v1.logging.info(\n        ""Using min_count=%d to generate vocab with target size %d"" %\n        (min_count, target_size))\n    return _generate_subtokens(\n        token_counts, alphabet, min_count, reserved_tokens=reserved_tokens)\n\n  def bisect(min_val, max_val):\n    """"""Recursive function to binary search for subtoken vocabulary.""""""\n    cur_count = (min_val + max_val) // 2\n    tf.compat.v1.logging.info(""Binary search: trying min_count=%d (%d %d)"" %\n                              (cur_count, min_val, max_val))\n    subtoken_list = _generate_subtokens(\n        token_counts, alphabet, cur_count, reserved_tokens=reserved_tokens)\n\n    val = len(subtoken_list)\n    tf.compat.v1.logging.info(\n        ""Binary search: min_count=%d resulted in %d tokens"" % (cur_count, val))\n\n    within_threshold = abs(val - target_size) < threshold\n    if within_threshold or min_val >= max_val or cur_count < 2:\n      return subtoken_list\n    if val > target_size:\n      other_subtoken_list = bisect(cur_count + 1, max_val)\n    else:\n      other_subtoken_list = bisect(min_val, cur_count - 1)\n\n    # Return vocabulary dictionary with the closest number of tokens.\n    other_val = len(other_subtoken_list)\n    if abs(other_val - target_size) < abs(val - target_size):\n      return other_subtoken_list\n    return subtoken_list\n\n  tf.compat.v1.logging.info(""Finding best min_count to get target size of %d"" %\n                            target_size)\n  return bisect(_MIN_MIN_COUNT, _MAX_MIN_COUNT)\n\n\ndef _generate_alphabet_dict(iterable, reserved_tokens=None):\n  """"""Create set of characters that appear in any element in the iterable.""""""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n  alphabet = {c for token in iterable for c in token}\n  alphabet |= {c for token in reserved_tokens for c in token}\n  alphabet |= _ESCAPE_CHARS  # Add escape characters to alphabet set.\n  return alphabet\n\n\ndef _count_and_gen_subtokens(\n    token_counts, alphabet, subtoken_dict, max_subtoken_length):\n  """"""Count number of times subtokens appear, and generate new subtokens.\n\n  Args:\n    token_counts: dict mapping tokens to the number of times they appear in the\n      original files.\n    alphabet: list of allowed characters. Used to escape the tokens, which\n      guarantees that all tokens can be split into subtokens.\n    subtoken_dict: dict mapping subtokens to ids.\n    max_subtoken_length: maximum length of subtoken in subtoken_dict.\n\n  Returns:\n    A defaultdict mapping subtokens to the number of times they appear in the\n    tokens. The dict may contain new subtokens.\n  """"""\n  subtoken_counts = collections.defaultdict(int)\n  for token, count in six.iteritems(token_counts):\n    token = _escape_token(token, alphabet)\n    subtokens = _split_token_to_subtokens(\n        token, subtoken_dict, max_subtoken_length)\n\n    # Generate new subtokens by taking substrings from token.\n    start = 0\n    for subtoken in subtokens:\n      for end in xrange(start + 1, len(token) + 1):\n        new_subtoken = token[start:end]\n        subtoken_counts[new_subtoken] += count\n      start += len(subtoken)\n\n  return subtoken_counts\n\n\ndef _filter_and_bucket_subtokens(subtoken_counts, min_count):\n  """"""Return a bucketed list of subtokens that are filtered by count.\n\n  Args:\n    subtoken_counts: defaultdict mapping subtokens to their counts\n    min_count: int count used to filter subtokens\n\n  Returns:\n    List of subtoken sets, where subtokens in set i have the same length=i.\n  """"""\n  # Create list of buckets, where subtokens in bucket i have length i.\n  subtoken_buckets = []\n  for subtoken, count in six.iteritems(subtoken_counts):\n    if count < min_count:  # Filter out subtokens that don\'t appear enough\n      continue\n    while len(subtoken_buckets) <= len(subtoken):\n      subtoken_buckets.append(set())\n    subtoken_buckets[len(subtoken)].add(subtoken)\n  return subtoken_buckets\n\n\ndef _gen_new_subtoken_list(\n    subtoken_counts, min_count, alphabet, reserved_tokens=None):\n  """"""Generate candidate subtokens ordered by count, and new max subtoken length.\n\n  Add subtokens to the candiate list in order of length (longest subtokens\n  first). When a subtoken is added, the counts of each of its prefixes are\n  decreased. Prefixes that don\'t appear much outside the subtoken are not added\n  to the candidate list.\n\n  For example:\n    subtoken being added to candidate list: \'translate\'\n    subtoken_counts: {\'translate\':10, \'t\':40, \'tr\':16, \'tra\':12, ...}\n    min_count: 5\n\n  When \'translate\' is added, subtoken_counts is updated to:\n    {\'translate\':0, \'t\':30, \'tr\':6, \'tra\': 2, ...}\n\n  The subtoken \'tra\' will not be added to the candidate list, because it appears\n  twice (less than min_count) outside of \'translate\'.\n\n  Args:\n    subtoken_counts: defaultdict mapping str subtokens to int counts\n    min_count: int minumum count requirement for subtokens\n    alphabet: set of characters. Each character is added to the subtoken list to\n      guarantee that all tokens can be encoded.\n    reserved_tokens: list of tokens that will be added to the beginning of the\n      returned subtoken list.\n\n  Returns:\n    List of candidate subtokens in decreasing count order, and maximum subtoken\n    length\n  """"""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  # Create a list of (count, subtoken) for each candidate subtoken.\n  subtoken_candidates = []\n\n  # Use bucketted list to iterate through subtokens in order of length.\n  # subtoken_buckets[i] = set(subtokens), where each subtoken has length i.\n  subtoken_buckets = _filter_and_bucket_subtokens(subtoken_counts, min_count)\n  max_subtoken_length = len(subtoken_buckets) - 1\n\n  # Go through the list in reverse order to consider longer subtokens first.\n  for subtoken_len in xrange(max_subtoken_length, 0, -1):\n    for subtoken in subtoken_buckets[subtoken_len]:\n      count = subtoken_counts[subtoken]\n\n      # Possible if this subtoken is a prefix of another token.\n      if count < min_count:\n        continue\n\n      # Ignore alphabet/reserved tokens, which will be added manually later.\n      if subtoken not in alphabet and subtoken not in reserved_tokens:\n        subtoken_candidates.append((count, subtoken))\n\n      # Decrement count of the subtoken\'s prefixes (if a longer subtoken is\n      # added, its prefixes lose priority to be added).\n      for end in xrange(1, subtoken_len):\n        subtoken_counts[subtoken[:end]] -= count\n\n  # Add alphabet subtokens (guarantees that all strings are encodable).\n  subtoken_candidates.extend((subtoken_counts.get(a, 0), a) for a in alphabet)\n\n  # Order subtoken candidates by decreasing count.\n  subtoken_list = [t for _, t in sorted(subtoken_candidates, reverse=True)]\n\n  # Add reserved tokens to beginning of the list.\n  subtoken_list = reserved_tokens + subtoken_list\n  return subtoken_list, max_subtoken_length\n\n\ndef _generate_subtokens(\n    token_counts, alphabet, min_count, num_iterations=4,\n    reserved_tokens=None):\n  """"""Create a list of subtokens in decreasing order of frequency.\n\n  Args:\n    token_counts: dict mapping str tokens -> int count\n    alphabet: set of characters\n    min_count: int minimum number of times a subtoken must appear before it is\n      added to the vocabulary.\n    num_iterations: int number of iterations to generate new tokens.\n    reserved_tokens: list of tokens that will be added to the beginning to the\n      returned subtoken list.\n\n  Returns:\n    Sorted list of subtokens (most frequent first)\n  """"""\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n\n  # Use alphabet set to create initial list of subtokens\n  subtoken_list = reserved_tokens + list(alphabet)\n  max_subtoken_length = 1\n\n  # On each iteration, segment all words using the subtokens defined in\n  # subtoken_dict, count how often the resulting subtokens appear, and update\n  # the dictionary with subtokens w/ high enough counts.\n  for i in xrange(num_iterations):\n    tf.compat.v1.logging.info(""\\tGenerating subtokens: iteration %d"" % i)\n    # Generate new subtoken->id dictionary using the new subtoken list.\n    subtoken_dict = _list_to_index_dict(subtoken_list)\n\n    # Create dict mapping subtoken->count, with additional subtokens created\n    # from substrings taken from the tokens.\n    subtoken_counts = _count_and_gen_subtokens(\n        token_counts, alphabet, subtoken_dict, max_subtoken_length)\n\n    # Generate new list of subtokens sorted by subtoken count.\n    subtoken_list, max_subtoken_length = _gen_new_subtoken_list(\n        subtoken_counts, min_count, alphabet, reserved_tokens)\n\n    tf.compat.v1.logging.info(""\\tVocab size: %d"" % len(subtoken_list))\n  return subtoken_list\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/utils/tokenizer_test.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test Subtokenizer and string helper methods.""""""\n\nimport collections\nimport tempfile\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.transformer.utils import tokenizer\n\n\nclass SubtokenizerTest(tf.test.TestCase):\n\n  def _init_subtokenizer(self, vocab_list):\n    temp_file = tempfile.NamedTemporaryFile(delete=False)\n    with tf.io.gfile.GFile(temp_file.name, ""w"") as w:\n      for subtoken in vocab_list:\n        w.write(""\'%s\'"" % subtoken)\n        w.write(""\\n"")\n    return tokenizer.Subtokenizer(temp_file.name, reserved_tokens=[])\n\n  def test_encode(self):\n    vocab_list = [""123_"", ""test"", ""ing_""]\n    subtokenizer = self._init_subtokenizer(vocab_list)\n    s = ""testing 123""\n    encoded_list = subtokenizer.encode(s)\n    self.assertEqual([1, 2, 0], encoded_list)\n\n  def test_decode(self):\n    vocab_list = [""123_"", ""test"", ""ing_""]\n    subtokenizer = self._init_subtokenizer(vocab_list)\n    encoded_list = [1, 2, 0]  # testing 123\n    decoded_str = subtokenizer.decode(encoded_list)\n    self.assertEqual(""testing 123"", decoded_str)\n\n  def test_subtoken_ids_to_tokens(self):\n    vocab_list = [""123_"", ""test"", ""ing_""]\n    subtokenizer = self._init_subtokenizer(vocab_list)\n    encoded_list = [1, 2, 0]  # testing 123\n    token_list = subtokenizer._subtoken_ids_to_tokens(encoded_list)\n    self.assertEqual([u""testing"", u""123""], token_list)\n\n\nclass StringHelperTest(tf.test.TestCase):\n\n  def test_split_string_to_tokens(self):\n    text = ""test? testing 123.""\n\n    tokens = tokenizer._split_string_to_tokens(text)\n    self.assertEqual([""test"", ""? "", ""testing"", ""123"", "".""], tokens)\n\n  def test_join_tokens_to_string(self):\n    tokens = [""test"", ""? "", ""testing"", ""123"", "".""]\n\n    s = tokenizer._join_tokens_to_string(tokens)\n    self.assertEqual(""test? testing 123."", s)\n\n  def test_escape_token(self):\n    token = u""abc_\\\\4""\n    alphabet = set(""abc_\\\\u;"")\n\n    escaped_token = tokenizer._escape_token(token, alphabet)\n    self.assertEqual(""abc\\\\u\\\\\\\\\\\\52;_"", escaped_token)\n\n  def test_unescape_token(self):\n    escaped_token = u""Underline: \\\\u, Backslash: \\\\\\\\, Unicode: \\\\52;""\n\n    unescaped_token = tokenizer._unescape_token(escaped_token)\n    self.assertEqual(\n        ""Underline: _, Backslash: \\\\, Unicode: 4"", unescaped_token)\n\n  def test_list_to_index_dict(self):\n    lst = [""test"", ""strings""]\n\n    d = tokenizer._list_to_index_dict(lst)\n    self.assertDictEqual({""test"": 0, ""strings"": 1}, d)\n\n  def test_split_token_to_subtokens(self):\n    token = ""abc""\n    subtoken_dict = {""a"": 0, ""b"": 1, ""c"": 2, ""ab"": 3}\n    max_subtoken_length = 2\n\n    subtokens = tokenizer._split_token_to_subtokens(\n        token, subtoken_dict, max_subtoken_length)\n    self.assertEqual([""ab"", ""c""], subtokens)\n\n  def test_generate_alphabet_dict(self):\n    s = [""testing"", ""123""]\n    reserved_tokens = [""???""]\n\n    alphabet = tokenizer._generate_alphabet_dict(s, reserved_tokens)\n    self.assertIn(""?"", alphabet)\n    self.assertIn(""t"", alphabet)\n    self.assertIn(""e"", alphabet)\n    self.assertIn(""s"", alphabet)\n    self.assertIn(""i"", alphabet)\n    self.assertIn(""n"", alphabet)\n    self.assertIn(""g"", alphabet)\n    self.assertIn(""1"", alphabet)\n    self.assertIn(""2"", alphabet)\n    self.assertIn(""3"", alphabet)\n\n  def test_count_and_gen_subtokens(self):\n    token_counts = {""abc"": 5}\n    alphabet = set(""abc_"")\n    subtoken_dict = {""a"": 0, ""b"": 1, ""c"": 2, ""_"": 3}\n    max_subtoken_length = 2\n\n    subtoken_counts = tokenizer._count_and_gen_subtokens(\n        token_counts, alphabet, subtoken_dict, max_subtoken_length)\n\n    self.assertIsInstance(subtoken_counts, collections.defaultdict)\n    self.assertDictEqual(\n        {""a"": 5, ""b"": 5, ""c"": 5, ""_"": 5, ""ab"": 5, ""bc"": 5, ""c_"": 5,\n         ""abc"": 5, ""bc_"": 5, ""abc_"": 5}, subtoken_counts)\n\n  def test_filter_and_bucket_subtokens(self):\n    subtoken_counts = collections.defaultdict(\n        int, {""a"": 2, ""b"": 4, ""c"": 1, ""ab"": 6, ""ac"": 3, ""abbc"": 5})\n    min_count = 3\n\n    subtoken_buckets = tokenizer._filter_and_bucket_subtokens(\n        subtoken_counts, min_count)\n\n    self.assertEqual(len(subtoken_buckets[0]), 0)\n    self.assertEqual(set(""b""), subtoken_buckets[1])\n    self.assertEqual(set([""ab"", ""ac""]), subtoken_buckets[2])\n    self.assertEqual(len(subtoken_buckets[3]), 0)\n    self.assertEqual(set([""abbc""]), subtoken_buckets[4])\n\n  def test_gen_new_subtoken_list(self):\n    subtoken_counts = collections.defaultdict(\n        int, {""translate"": 10, ""t"": 40, ""tr"": 16, ""tra"": 12})\n    min_count = 5\n    alphabet = set(""translate"")\n    reserved_tokens = [""reserved"", ""tokens""]\n\n    subtoken_list, max_token_length = tokenizer._gen_new_subtoken_list(\n        subtoken_counts, min_count, alphabet, reserved_tokens)\n\n    # Check that ""tra"" isn""t in the list (its count should be decremented to 2,\n    # so it should not be added to the canddiate list).\n    self.assertNotIn(""tra"", subtoken_list)\n\n    self.assertIn(""tr"", subtoken_list)\n    self.assertIn(""t"", subtoken_list)\n\n    self.assertEqual(len(""translate""), max_token_length)\n\n  def test_generate_subtokens(self):\n    token_counts = {""ab"": 1, ""bc"": 3, ""abc"": 5}\n    alphabet = set(""abc_"")\n    min_count = 100\n    num_iterations = 1\n    reserved_tokens = [""reserved"", ""tokens""]\n\n    vocab_list = tokenizer._generate_subtokens(\n        token_counts, alphabet, min_count, num_iterations, reserved_tokens)\n\n    # Check that reserved tokens are at the front of the list\n    self.assertEqual(vocab_list[:2], reserved_tokens)\n\n    # Check that each character in alphabet is in the vocab list\n    for c in alphabet:\n      self.assertIn(c, vocab_list)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/__init__.py,0,b''
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/hyperparams_flags.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common flags for importing hyperparameters.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom official.utils.flags import core as flags_core\n\nFLAGS = flags.FLAGS\n\n\ndef define_common_hparams_flags():\n  """"""Define the common flags across models.""""""\n\n  flags.DEFINE_string(\n      \'model_dir\',\n      default=None,\n      help=(\'The directory where the model and training/evaluation summaries\'\n            \'are stored.\'))\n\n  flags.DEFINE_integer(\n      \'train_batch_size\', default=None, help=\'Batch size for training.\')\n\n  flags.DEFINE_integer(\n      \'eval_batch_size\', default=None, help=\'Batch size for evaluation.\')\n\n  flags.DEFINE_string(\n      \'precision\',\n      default=None,\n      help=(\'Precision to use; one of: {bfloat16, float32}\'))\n\n  flags.DEFINE_string(\n      \'config_file\',\n      default=None,\n      help=(\'A YAML file which specifies overrides. Note that this file can be \'\n            \'used as an override template to override the default parameters \'\n            \'specified in Python. If the same parameter is specified in both \'\n            \'`--config_file` and `--params_override`, the one in \'\n            \'`--params_override` will be used finally.\'))\n\n  flags.DEFINE_string(\n      \'params_override\',\n      default=None,\n      help=(\'a YAML/JSON string or a YAML file which specifies additional \'\n            \'overrides over the default parameters and those specified in \'\n            \'`--config_file`. Note that this is supposed to be used only to \'\n            \'override the model parameters, but not the parameters like TPU \'\n            \'specific flags. One canonical use case of `--config_file` and \'\n            \'`--params_override` is users first define a template config file \'\n            \'using `--config_file`, then use `--params_override` to adjust the \'\n            \'minimal set of tuning parameters, for example setting up different\'\n            \' `train_batch_size`. \'\n            \'The final override order of parameters: default_model_params --> \'\n            \'params from config_file --> params in params_override.\'\n            \'See also the help message of `--config_file`.\'))\n  flags.DEFINE_integer(\'save_checkpoint_freq\', None,\n                       \'Number of steps to save checkpoint.\')\n\n\ndef initialize_common_flags():\n  """"""Define the common flags across models.""""""\n  define_common_hparams_flags()\n\n  flags_core.define_device(tpu=True)\n  flags_core.define_base(\n      num_gpu=True, model_dir=False, data_dir=False, batch_size=False)\n  flags_core.define_distribution(worker_hosts=True, task_index=True)\n  flags_core.define_performance(all_reduce_alg=True, num_packs=True)\n\n  # Reset the default value of num_gpus to zero.\n  FLAGS.num_gpus = 0\n\n  flags.DEFINE_string(\n      \'strategy_type\', \'mirrored\', \'Type of distribute strategy.\'\n      \'One of mirrored, tpu and multiworker.\')\n\n\ndef strategy_flags_dict():\n  """"""Returns TPU and/or GPU related flags in a dictionary.""""""\n  return {\n      # TPUStrategy related flags.\n      \'tpu\': FLAGS.tpu,\n      # MultiWorkerMirroredStrategy related flags.\n      \'all_reduce_alg\': FLAGS.all_reduce_alg,\n      \'worker_hosts\': FLAGS.worker_hosts,\n      \'task_index\': FLAGS.task_index,\n      # MirroredStrategy and OneDeviceStrategy\n      \'num_gpus\': FLAGS.num_gpus,\n      \'num_packs\': FLAGS.num_packs,\n  }\n\n\ndef hparam_flags_dict():\n  """"""Returns model params related flags in a dictionary.""""""\n  return {\n      \'data_dir\': FLAGS.data_dir,\n      \'model_dir\': FLAGS.model_dir,\n      \'train_batch_size\': FLAGS.train_batch_size,\n      \'eval_batch_size\': FLAGS.eval_batch_size,\n      \'precision\': FLAGS.precision,\n      \'config_file\': FLAGS.config_file,\n      \'params_override\': FLAGS.params_override,\n  }\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/__init__.py,0,b''
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/_base.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Flags which will be nearly universal across models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom official.utils.flags._conventions import help_wrap\nfrom official.utils.logs import hooks_helper\n\n\ndef define_base(data_dir=True, model_dir=True, clean=False, train_epochs=False,\n                epochs_between_evals=False, stop_threshold=False,\n                batch_size=True, num_gpu=False, hooks=False, export_dir=False,\n                distribution_strategy=False, run_eagerly=False):\n  """"""Register base flags.\n\n  Args:\n    data_dir: Create a flag for specifying the input data directory.\n    model_dir: Create a flag for specifying the model file directory.\n    clean: Create a flag for removing the model_dir.\n    train_epochs: Create a flag to specify the number of training epochs.\n    epochs_between_evals: Create a flag to specify the frequency of testing.\n    stop_threshold: Create a flag to specify a threshold accuracy or other\n      eval metric which should trigger the end of training.\n    batch_size: Create a flag to specify the batch size.\n    num_gpu: Create a flag to specify the number of GPUs used.\n    hooks: Create a flag to specify hooks for logging.\n    export_dir: Create a flag to specify where a SavedModel should be exported.\n    distribution_strategy: Create a flag to specify which Distribution Strategy\n      to use.\n    run_eagerly: Create a flag to specify to run eagerly op by op.\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n  key_flags = []\n\n  if data_dir:\n    flags.DEFINE_string(\n        name=""data_dir"", short_name=""dd"", default=""/tmp"",\n        help=help_wrap(""The location of the input data.""))\n    key_flags.append(""data_dir"")\n\n  if model_dir:\n    flags.DEFINE_string(\n        name=""model_dir"", short_name=""md"", default=""/tmp"",\n        help=help_wrap(""The location of the model checkpoint files.""))\n    key_flags.append(""model_dir"")\n\n  if clean:\n    flags.DEFINE_boolean(\n        name=""clean"", default=False,\n        help=help_wrap(""If set, model_dir will be removed if it exists.""))\n    key_flags.append(""clean"")\n\n  if train_epochs:\n    flags.DEFINE_integer(\n        name=""train_epochs"", short_name=""te"", default=1,\n        help=help_wrap(""The number of epochs used to train.""))\n    key_flags.append(""train_epochs"")\n\n  if epochs_between_evals:\n    flags.DEFINE_integer(\n        name=""epochs_between_evals"", short_name=""ebe"", default=1,\n        help=help_wrap(""The number of training epochs to run between ""\n                       ""evaluations.""))\n    key_flags.append(""epochs_between_evals"")\n\n  if stop_threshold:\n    flags.DEFINE_float(\n        name=""stop_threshold"", short_name=""st"",\n        default=None,\n        help=help_wrap(""If passed, training will stop at the earlier of ""\n                       ""train_epochs and when the evaluation metric is  ""\n                       ""greater than or equal to stop_threshold.""))\n\n  if batch_size:\n    flags.DEFINE_integer(\n        name=""batch_size"", short_name=""bs"", default=32,\n        help=help_wrap(""Batch size for training and evaluation. When using ""\n                       ""multiple gpus, this is the global batch size for ""\n                       ""all devices. For example, if the batch size is 32 ""\n                       ""and there are 4 GPUs, each GPU will get 8 examples on ""\n                       ""each step.""))\n    key_flags.append(""batch_size"")\n\n  if num_gpu:\n    flags.DEFINE_integer(\n        name=""num_gpus"", short_name=""ng"",\n        default=1,\n        help=help_wrap(\n            ""How many GPUs to use at each worker with the ""\n            ""DistributionStrategies API. The default is 1.""))\n\n  if run_eagerly:\n    flags.DEFINE_boolean(\n        name=""run_eagerly"", default=False,\n        help=""Run the model op by op without building a model function."")\n\n  if hooks:\n    # Construct a pretty summary of hooks.\n    hook_list_str = (\n        u""\\ufeff  Hook:\\n"" + u""\\n"".join([u""\\ufeff    {}"".format(key) for key\n                                         in hooks_helper.HOOKS]))\n    flags.DEFINE_list(\n        name=""hooks"", short_name=""hk"", default=""LoggingTensorHook"",\n        help=help_wrap(\n            u""A list of (case insensitive) strings to specify the names of ""\n            u""training hooks.\\n{}\\n\\ufeff  Example: `--hooks ProfilerHook,""\n            u""ExamplesPerSecondHook`\\n See official.utils.logs.hooks_helper ""\n            u""for details."".format(hook_list_str))\n    )\n    key_flags.append(""hooks"")\n\n  if export_dir:\n    flags.DEFINE_string(\n        name=""export_dir"", short_name=""ed"", default=None,\n        help=help_wrap(""If set, a SavedModel serialization of the model will ""\n                       ""be exported to this directory at the end of training. ""\n                       ""See the README for more details and relevant links."")\n    )\n    key_flags.append(""export_dir"")\n\n  if distribution_strategy:\n    flags.DEFINE_string(\n        name=""distribution_strategy"", short_name=""ds"", default=""mirrored"",\n        help=help_wrap(""The Distribution Strategy to use for training. ""\n                       ""Accepted values are \'off\', \'one_device\', ""\n                       ""\'mirrored\', \'parameter_server\', \'collective\', ""\n                       ""case insensitive. \'off\' means not to use ""\n                       ""Distribution Strategy; \'default\' means to choose ""\n                       ""from `MirroredStrategy` or `OneDeviceStrategy` ""\n                       ""according to the number of GPUs."")\n    )\n\n\n  return key_flags\n\n\ndef get_num_gpus(flags_obj):\n  """"""Treat num_gpus=-1 as \'use all\'.""""""\n  if flags_obj.num_gpus != -1:\n    return flags_obj.num_gpus\n\n  from tensorflow.python.client import device_lib  # pylint: disable=g-import-not-at-top\n  local_device_protos = device_lib.list_local_devices()\n  return sum([1 for d in local_device_protos if d.device_type == ""GPU""])\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/_benchmark.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Flags for benchmarking models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nfrom official.utils.flags._conventions import help_wrap\n\n\ndef define_benchmark(benchmark_log_dir=True, bigquery_uploader=True):\n  """"""Register benchmarking flags.\n\n  Args:\n    benchmark_log_dir: Create a flag to specify location for benchmark logging.\n    bigquery_uploader: Create flags for uploading results to BigQuery.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n\n  flags.DEFINE_enum(\n      name=""benchmark_logger_type"", default=""BaseBenchmarkLogger"",\n      enum_values=[""BaseBenchmarkLogger"", ""BenchmarkFileLogger"",\n                   ""BenchmarkBigQueryLogger""],\n      help=help_wrap(""The type of benchmark logger to use. Defaults to using ""\n                     ""BaseBenchmarkLogger which logs to STDOUT. Different ""\n                     ""loggers will require other flags to be able to work.""))\n  flags.DEFINE_string(\n      name=""benchmark_test_id"", short_name=""bti"", default=None,\n      help=help_wrap(""The unique test ID of the benchmark run. It could be the ""\n                     ""combination of key parameters. It is hardware ""\n                     ""independent and could be used compare the performance ""\n                     ""between different test runs. This flag is designed for ""\n                     ""human consumption, and does not have any impact within ""\n                     ""the system.""))\n\n  flags.DEFINE_integer(\n      name=\'log_steps\', default=100,\n      help=\'For every log_steps, we log the timing information such as \'\n      \'examples per second. Besides, for every log_steps, we store the \'\n      \'timestamp of a batch end.\')\n\n  if benchmark_log_dir:\n    flags.DEFINE_string(\n        name=""benchmark_log_dir"", short_name=""bld"", default=None,\n        help=help_wrap(""The location of the benchmark logging."")\n    )\n\n  if bigquery_uploader:\n    flags.DEFINE_string(\n        name=""gcp_project"", short_name=""gp"", default=None,\n        help=help_wrap(\n            ""The GCP project name where the benchmark will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_data_set"", short_name=""bds"", default=""test_benchmark"",\n        help=help_wrap(\n            ""The Bigquery dataset name where the benchmark will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_run_table"", short_name=""brt"", default=""benchmark_run"",\n        help=help_wrap(""The Bigquery table name where the benchmark run ""\n                       ""information will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_run_status_table"", short_name=""brst"",\n        default=""benchmark_run_status"",\n        help=help_wrap(""The Bigquery table name where the benchmark run ""\n                       ""status information will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_metric_table"", short_name=""bmt"",\n        default=""benchmark_metric"",\n        help=help_wrap(""The Bigquery table name where the benchmark metric ""\n                       ""information will be uploaded.""))\n\n  @flags.multi_flags_validator(\n      [""benchmark_logger_type"", ""benchmark_log_dir""],\n      message=""--benchmark_logger_type=BenchmarkFileLogger will require ""\n              ""--benchmark_log_dir being set"")\n  def _check_benchmark_log_dir(flags_dict):\n    benchmark_logger_type = flags_dict[""benchmark_logger_type""]\n    if benchmark_logger_type == ""BenchmarkFileLogger"":\n      return flags_dict[""benchmark_log_dir""]\n    return True\n\n  return key_flags\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/_conventions.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Central location for shared argparse convention definitions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport codecs\nimport functools\n\nfrom absl import app as absl_app\nfrom absl import flags\n\n\n# This codifies help string conventions and makes it easy to update them if\n# necessary. Currently the only major effect is that help bodies start on the\n# line after flags are listed. All flag definitions should wrap the text bodies\n# with help wrap when calling DEFINE_*.\n_help_wrap = functools.partial(flags.text_wrap, length=80, indent="""",\n                               firstline_indent=""\\n"")\n\n\n# Pretty formatting causes issues when utf-8 is not installed on a system.\ndef _stdout_utf8():\n  try:\n    codecs.lookup(""utf-8"")\n  except LookupError:\n    return False\n  return sys.stdout.encoding == ""UTF-8""\n\n\nif _stdout_utf8():\n  help_wrap = _help_wrap\nelse:\n  def help_wrap(text, *args, **kwargs):\n    return _help_wrap(text, *args, **kwargs).replace(u""\\ufeff"", u"""")\n\n\n# Replace None with h to also allow -h\nabsl_app.HelpshortFlag.SHORT_NAME = ""h""\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/_device.py,1,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Flags for managing compute devices. Currently only contains TPU flags.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom official.utils.flags._conventions import help_wrap\n\n\ndef require_cloud_storage(flag_names):\n  """"""Register a validator to check directory flags.\n  Args:\n    flag_names: An iterable of strings containing the names of flags to be\n      checked.\n  """"""\n  msg = ""TPU requires GCS path for {}"".format("", "".join(flag_names))\n  @flags.multi_flags_validator([""tpu""] + flag_names, message=msg)\n  def _path_check(flag_values):  # pylint: disable=missing-docstring\n    if flag_values[""tpu""] is None:\n      return True\n\n    valid_flags = True\n    for key in flag_names:\n      if not flag_values[key].startswith(""gs://""):\n        tf.compat.v1.logging.error(""{} must be a GCS path."".format(key))\n        valid_flags = False\n\n    return valid_flags\n\n\ndef define_device(tpu=True):\n  """"""Register device specific flags.\n  Args:\n    tpu: Create flags to specify TPU operation.\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n\n  if tpu:\n    flags.DEFINE_string(\n        name=""tpu"", default=None,\n        help=help_wrap(\n            ""The Cloud TPU to use for training. This should be either the name ""\n            ""used when creating the Cloud TPU, or a ""\n            ""grpc://ip.address.of.tpu:8470 url. Passing `local` will use the""\n            ""CPU of the local instance instead. (Good for debugging.)""))\n    key_flags.append(""tpu"")\n\n    flags.DEFINE_string(\n        name=""tpu_zone"", default=None,\n        help=help_wrap(\n            ""[Optional] GCE zone where the Cloud TPU is located in. If not ""\n            ""specified, we will attempt to automatically detect the GCE ""\n            ""project from metadata.""))\n\n    flags.DEFINE_string(\n        name=""tpu_gcp_project"", default=None,\n        help=help_wrap(\n            ""[Optional] Project name for the Cloud TPU-enabled project. If not ""\n            ""specified, we will attempt to automatically detect the GCE ""\n            ""project from metadata.""))\n\n    flags.DEFINE_integer(name=""num_tpu_shards"", default=8,\n                         help=help_wrap(""Number of shards (TPU chips).""))\n\n  return key_flags\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/_distribution.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Flags related to distributed execution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom official.utils.flags._conventions import help_wrap\n\n\ndef define_distribution(worker_hosts=True, task_index=True):\n  """"""Register distributed execution flags.\n\n  Args:\n    worker_hosts: Create a flag for specifying comma-separated list of workers.\n    task_index: Create a flag for specifying index of task.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n  key_flags = []\n\n  if worker_hosts:\n    flags.DEFINE_string(\n        name=\'worker_hosts\', default=None,\n        help=help_wrap(\n            \'Comma-separated list of worker ip:port pairs for running \'\n            \'multi-worker models with DistributionStrategy.  The user would \'\n            \'start the program on each host with identical value for this \'\n            \'flag.\'))\n\n  if task_index:\n    flags.DEFINE_integer(\n        name=\'task_index\', default=-1,\n        help=help_wrap(\'If multi-worker training, the task_index of this \'\n                       \'worker.\'))\n\n  return key_flags\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/_misc.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Misc flags.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nfrom official.utils.flags._conventions import help_wrap\n\n\ndef define_image(data_format=True):\n  """"""Register image specific flags.\n\n  Args:\n    data_format: Create a flag to specify image axis convention.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n\n  if data_format:\n    flags.DEFINE_enum(\n        name=""data_format"", short_name=""df"", default=None,\n        enum_values=[""channels_first"", ""channels_last""],\n        help=help_wrap(\n            ""A flag to override the data format used in the model. ""\n            ""channels_first provides a performance boost on GPU but is not ""\n            ""always compatible with CPU. If left unspecified, the data format ""\n            ""will be chosen automatically based on whether TensorFlow was ""\n            ""built for CPU or GPU.""))\n    key_flags.append(""data_format"")\n\n  return key_flags\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/_performance.py,15,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Register flags for optimizing performance.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport multiprocessing\n\nfrom absl import flags    # pylint: disable=g-bad-import-order\nimport tensorflow as tf   # pylint: disable=g-bad-import-order\n\nfrom official.utils.flags._conventions import help_wrap\n\n\n# Map string to TensorFlow dtype\nDTYPE_MAP = {\n    ""fp16"": tf.float16,\n    ""bf16"": tf.bfloat16,\n    ""fp32"": tf.float32,\n}\n\n\ndef get_tf_dtype(flags_obj):\n  if getattr(flags_obj, ""fp16_implementation"", None) == ""graph_rewrite"":\n    # If the graph_rewrite is used, we build the graph with fp32, and let the\n    # graph rewrite change ops to fp16.\n    return tf.float32\n  return DTYPE_MAP[flags_obj.dtype]\n\n\ndef get_loss_scale(flags_obj, default_for_fp16):\n  if flags_obj.loss_scale == ""dynamic"":\n    return flags_obj.loss_scale\n  elif flags_obj.loss_scale is not None:\n    return float(flags_obj.loss_scale)\n  elif flags_obj.dtype == ""fp32"":\n    return 1  # No loss scaling is needed for fp32\n  else:\n    assert flags_obj.dtype == ""fp16""\n    return default_for_fp16\n\n\ndef define_performance(num_parallel_calls=False, inter_op=False, intra_op=False,\n                       synthetic_data=False, max_train_steps=False, dtype=False,\n                       all_reduce_alg=False, num_packs=False,\n                       tf_gpu_thread_mode=False,\n                       datasets_num_private_threads=False,\n                       datasets_num_parallel_batches=False,\n                       dynamic_loss_scale=False, fp16_implementation=False,\n                       loss_scale=False,\n                       tf_data_experimental_slack=False, enable_xla=False,\n                       force_v2_in_keras_compile=False,\n                       training_dataset_cache=False):\n  """"""Register flags for specifying performance tuning arguments.\n\n  Args:\n    num_parallel_calls: Create a flag to specify parallelism of data loading.\n    inter_op: Create a flag to allow specification of inter op threads.\n    intra_op: Create a flag to allow specification of intra op threads.\n    synthetic_data: Create a flag to allow the use of synthetic data.\n    max_train_steps: Create a flags to allow specification of maximum number\n      of training steps\n    dtype: Create flags for specifying dtype.\n    all_reduce_alg: If set forces a specific algorithm for multi-gpu.\n    num_packs: If set provides number of packs for MirroredStrategy\'s cross\n      device ops.\n    tf_gpu_thread_mode: gpu_private triggers us of private thread pool.\n    datasets_num_private_threads: Number of private threads for datasets.\n    datasets_num_parallel_batches: Determines how many batches to process in\n    parallel when using map and batch from tf.data.\n    dynamic_loss_scale: Allow the ""loss_scale"" flag to take on the value\n      ""dynamic"". Only valid if `dtype` is True.\n    fp16_implementation: Create fp16_implementation flag.\n    loss_scale: Controls the loss scaling, normally for mixed-precision\n      training. Can only be turned on if dtype is also True.\n    tf_data_experimental_slack: Determines whether to enable tf.data\'s\n      `experimental_slack` option.\n    enable_xla: Determines if XLA (auto clustering) is turned on.\n    force_v2_in_keras_compile: Forces the use of run_distribued path even if not\n      using a `strategy`. This is not the same as\n      `tf.distribute.OneDeviceStrategy`\n    training_dataset_cache: Whether to cache the training dataset on workers.\n       Typically used to improve training performance when training data is in\n       remote storage and can fit into worker memory.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n  if num_parallel_calls:\n    flags.DEFINE_integer(\n        name=""num_parallel_calls"", short_name=""npc"",\n        default=multiprocessing.cpu_count(),\n        help=help_wrap(""The number of records that are  processed in parallel ""\n                       ""during input processing. This can be optimized per ""\n                       ""data set but for generally homogeneous data sets, ""\n                       ""should be approximately the number of available CPU ""\n                       ""cores. (default behavior)""))\n\n  if inter_op:\n    flags.DEFINE_integer(\n        name=""inter_op_parallelism_threads"", short_name=""inter"", default=0,\n        help=help_wrap(""Number of inter_op_parallelism_threads to use for CPU. ""\n                       ""See TensorFlow config.proto for details."")\n    )\n\n  if intra_op:\n    flags.DEFINE_integer(\n        name=""intra_op_parallelism_threads"", short_name=""intra"", default=0,\n        help=help_wrap(""Number of intra_op_parallelism_threads to use for CPU. ""\n                       ""See TensorFlow config.proto for details.""))\n\n  if synthetic_data:\n    flags.DEFINE_bool(\n        name=""use_synthetic_data"", short_name=""synth"", default=False,\n        help=help_wrap(\n            ""If set, use fake data (zeroes) instead of a real dataset. ""\n            ""This mode is useful for performance debugging, as it removes ""\n            ""input processing steps, but will not learn anything.""))\n\n  if max_train_steps:\n    flags.DEFINE_integer(\n        name=""max_train_steps"", short_name=""mts"", default=None, help=help_wrap(\n            ""The model will stop training if the global_step reaches this ""\n            ""value. If not set, training will run until the specified number ""\n            ""of epochs have run as usual. It is generally recommended to set ""\n            ""--train_epochs=1 when using this flag.""\n        ))\n\n  if dtype:\n    flags.DEFINE_enum(\n        name=""dtype"", short_name=""dt"", default=""fp32"",\n        enum_values=DTYPE_MAP.keys(),\n        help=help_wrap(""The TensorFlow datatype used for calculations. ""\n                       ""Variables may be cast to a higher precision on a ""\n                       ""case-by-case basis for numerical stability.""))\n\n    loss_scale_help_text = (\n        ""The amount to scale the loss by when the model is run. {}. Before ""\n        ""gradients are computed, the loss is multiplied by the loss scale, ""\n        ""making all gradients loss_scale times larger. To adjust for this, ""\n        ""gradients are divided by the loss scale before being applied to ""\n        ""variables. This is mathematically equivalent to training without ""\n        ""a loss scale, but the loss scale helps avoid some intermediate ""\n        ""gradients from underflowing to zero. If not provided the default ""\n        ""for fp16 is 128 and 1 for all other dtypes.{}""\n    )\n    if dynamic_loss_scale:\n      loss_scale_help_text = loss_scale_help_text.format(\n          ""This can be an int/float or the string \'dynamic\'"",\n          "" The string \'dynamic\' can be used to dynamically determine the ""\n          ""optimal loss scale during training, but currently this ""\n          ""significantly slows down performance"")\n      loss_scale_validation_msg = (""loss_scale should be a positive int/float ""\n                                   ""or the string \'dynamic\'."")\n    else:\n      loss_scale_help_text = loss_scale_help_text.format(\n          ""This must be an int/float"", """")\n      loss_scale_validation_msg = ""loss_scale should be a positive int/float.""\n    if loss_scale:\n      flags.DEFINE_string(\n          name=""loss_scale"", short_name=""ls"", default=None,\n          help=help_wrap(loss_scale_help_text))\n\n      @flags.validator(flag_name=""loss_scale"",\n                       message=loss_scale_validation_msg)\n      def _check_loss_scale(loss_scale):  # pylint: disable=unused-variable\n        """"""Validator to check the loss scale flag is valid.""""""\n        if loss_scale is None:\n          return True  # null case is handled in get_loss_scale()\n\n        if loss_scale == ""dynamic"" and dynamic_loss_scale:\n          return True\n\n        try:\n          loss_scale = float(loss_scale)\n        except ValueError:\n          return False\n\n        return loss_scale > 0\n\n    if fp16_implementation:\n      flags.DEFINE_enum(\n          name=""fp16_implementation"", default=""keras"",\n          enum_values=(""keras\', \'graph_rewrite""),\n          help=help_wrap(\n              ""When --dtype=fp16, how fp16 should be implemented. This has no ""\n              ""impact on correctness. \'keras\' uses the ""\n              ""tf.keras.mixed_precision API. \'graph_rewrite\' uses the ""\n              ""tf.train.experimental.enable_mixed_precision_graph_rewrite ""\n              ""API.""))\n\n      @flags.multi_flags_validator([""fp16_implementation"", ""dtype"",\n                                    ""loss_scale""])\n      def _check_fp16_implementation(flags_dict):\n        """"""Validator to check fp16_implementation flag is valid.""""""\n        if (flags_dict[""fp16_implementation""] == ""graph_rewrite"" and\n            flags_dict[""dtype""] != ""fp16""):\n          raise flags.ValidationError(""--fp16_implementation should not be ""\n                                      ""specified unless --dtype=fp16"")\n        return True\n\n  if all_reduce_alg:\n    flags.DEFINE_string(\n        name=""all_reduce_alg"", short_name=""ara"", default=None,\n        help=help_wrap(""Defines the algorithm to use for performing all-reduce.""\n                       ""When specified with MirroredStrategy for single ""\n                       ""worker, this controls ""\n                       ""tf.contrib.distribute.AllReduceCrossTowerOps.  When ""\n                       ""specified with MultiWorkerMirroredStrategy, this ""\n                       ""controls ""\n                       ""tf.distribute.experimental.CollectiveCommunication; ""\n                       ""valid options are `ring` and `nccl`.""))\n\n  if num_packs:\n    flags.DEFINE_integer(\n        name=""num_packs"", default=1,\n        help=help_wrap(""Sets `num_packs` in the cross device ops used in ""\n                       ""MirroredStrategy.  For details, see ""\n                       ""tf.distribute.NcclAllReduce.""))\n\n  if tf_gpu_thread_mode:\n    flags.DEFINE_string(\n        name=""tf_gpu_thread_mode"", short_name=""gt_mode"", default=None,\n        help=help_wrap(\n            ""Whether and how the GPU device uses its own threadpool."")\n    )\n\n    flags.DEFINE_integer(\n        name=""per_gpu_thread_count"", short_name=""pgtc"", default=0,\n        help=help_wrap(\n            ""The number of threads to use for GPU. Only valid when ""\n            ""tf_gpu_thread_mode is not global."")\n    )\n\n  if datasets_num_private_threads:\n    flags.DEFINE_integer(\n        name=""datasets_num_private_threads"",\n        default=None,\n        help=help_wrap(\n            ""Number of threads for a private threadpool created for all""\n            ""datasets computation.."")\n    )\n\n  if datasets_num_parallel_batches:\n    flags.DEFINE_integer(\n        name=""datasets_num_parallel_batches"",\n        default=None,\n        help=help_wrap(\n            ""Determines how many batches to process in parallel when using ""\n            ""map and batch from tf.data."")\n    )\n\n  if training_dataset_cache:\n    flags.DEFINE_boolean(\n        name=""training_dataset_cache"",\n        default=False,\n        help=help_wrap(\n            ""Determines whether to cache the training dataset on workers. ""\n            ""Typically used to improve training performance when training ""\n            ""data is in remote storage and can fit into worker memory."")\n    )\n\n  if tf_data_experimental_slack:\n    flags.DEFINE_boolean(\n        name=""tf_data_experimental_slack"",\n        default=False,\n        help=help_wrap(\n            ""Whether to enable tf.data\'s `experimental_slack` option."")\n    )\n\n  if enable_xla:\n    flags.DEFINE_boolean(\n        name=""enable_xla"", default=False,\n        help=""Whether to enable XLA auto jit compilation"")\n\n  if force_v2_in_keras_compile:\n    flags.DEFINE_boolean(\n        name=""force_v2_in_keras_compile"", default=None,\n        help=""Forces the use of run_distribued path even if not""\n             ""using a `strategy`. This is not the same as""\n             ""`tf.distribute.OneDeviceStrategy`"")\n\n  return key_flags\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/core.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Public interface for flag definition.\n\nSee _example.py for detailed instructions on defining flags.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nfrom six.moves import shlex_quote\n\nfrom absl import app as absl_app\nfrom absl import flags\n\nfrom official.utils.flags import _base\nfrom official.utils.flags import _benchmark\nfrom official.utils.flags import _conventions\nfrom official.utils.flags import _device\nfrom official.utils.flags import _distribution\nfrom official.utils.flags import _misc\nfrom official.utils.flags import _performance\n\n\ndef set_defaults(**kwargs):\n  for key, value in kwargs.items():\n    flags.FLAGS.set_default(name=key, value=value)\n\n\ndef parse_flags(argv=None):\n  """"""Reset flags and reparse. Currently only used in testing.""""""\n  flags.FLAGS.unparse_flags()\n  absl_app.parse_flags_with_usage(argv or sys.argv)\n\n\ndef register_key_flags_in_core(f):\n  """"""Defines a function in core.py, and registers its key flags.\n\n  absl uses the location of a flags.declare_key_flag() to determine the context\n  in which a flag is key. By making all declares in core, this allows model\n  main functions to call flags.adopt_module_key_flags() on core and correctly\n  chain key flags.\n\n  Args:\n    f:  The function to be wrapped\n\n  Returns:\n    The ""core-defined"" version of the input function.\n  """"""\n\n  def core_fn(*args, **kwargs):\n    key_flags = f(*args, **kwargs)\n    [flags.declare_key_flag(fl) for fl in key_flags]  # pylint: disable=expression-not-assigned\n  return core_fn\n\n\ndefine_base = register_key_flags_in_core(_base.define_base)\n# We have define_base_eager for compatibility, since it used to be a separate\n# function from define_base.\ndefine_base_eager = define_base\ndefine_benchmark = register_key_flags_in_core(_benchmark.define_benchmark)\ndefine_device = register_key_flags_in_core(_device.define_device)\ndefine_image = register_key_flags_in_core(_misc.define_image)\ndefine_performance = register_key_flags_in_core(_performance.define_performance)\ndefine_distribution = register_key_flags_in_core(\n    _distribution.define_distribution)\n\n\nhelp_wrap = _conventions.help_wrap\n\n\nget_num_gpus = _base.get_num_gpus\nget_tf_dtype = _performance.get_tf_dtype\nget_loss_scale = _performance.get_loss_scale\nDTYPE_MAP = _performance.DTYPE_MAP\nrequire_cloud_storage = _device.require_cloud_storage\n\ndef _get_nondefault_flags_as_dict():\n  """"""Returns the nondefault flags as a dict from flag name to value.""""""\n  nondefault_flags = {}\n  for flag_name in flags.FLAGS:\n    flag_value = getattr(flags.FLAGS, flag_name)\n    if (flag_name != flags.FLAGS[flag_name].short_name and\n        flag_value != flags.FLAGS[flag_name].default):\n      nondefault_flags[flag_name] = flag_value\n  return nondefault_flags\n\n\ndef get_nondefault_flags_as_str():\n  """"""Returns flags as a string that can be passed as command line arguments.\n\n  E.g., returns: ""--batch_size=256 --use_synthetic_data"" for the following code\n  block:\n\n  ```\n  flags.FLAGS.batch_size = 256\n  flags.FLAGS.use_synthetic_data = True\n  print(get_nondefault_flags_as_str())\n  ```\n\n  Only flags with nondefault values are returned, as passing default flags as\n  command line arguments has no effect.\n\n  Returns:\n    A string with the flags, that can be passed as command line arguments to a\n    program to use the flags.\n  """"""\n  nondefault_flags = _get_nondefault_flags_as_dict()\n  flag_strings = []\n  for name, value in sorted(nondefault_flags.items()):\n    if isinstance(value, bool):\n      flag_str = \'--{}\'.format(name) if value else \'--no{}\'.format(name)\n    elif isinstance(value, list):\n      flag_str = \'--{}={}\'.format(name, \',\'.join(value))\n    else:\n      flag_str = \'--{}={}\'.format(name, value)\n    flag_strings.append(flag_str)\n  return \' \'.join(shlex_quote(flag_str) for flag_str in flag_strings)\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/flags/flags_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom official.utils.flags import core as flags_core  # pylint: disable=g-bad-import-order\n\n\ndef define_flags():\n  flags_core.define_base(clean=True, num_gpu=False, stop_threshold=True,\n                         hooks=True, train_epochs=True,\n                         epochs_between_evals=True)\n  flags_core.define_performance(\n      num_parallel_calls=True, inter_op=True,  intra_op=True,\n      dynamic_loss_scale=True, loss_scale=True, synthetic_data=True,\n      dtype=True)\n  flags_core.define_image()\n  flags_core.define_benchmark()\n\n\nclass BaseTester(unittest.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(BaseTester, cls).setUpClass()\n    define_flags()\n\n  def test_default_setting(self):\n    """"""Test to ensure fields exist and defaults can be set.\n    """"""\n\n    defaults = dict(\n        data_dir=""dfgasf"",\n        model_dir=""dfsdkjgbs"",\n        train_epochs=534,\n        epochs_between_evals=15,\n        batch_size=256,\n        hooks=[""LoggingTensorHook""],\n        num_parallel_calls=18,\n        inter_op_parallelism_threads=5,\n        intra_op_parallelism_threads=10,\n        data_format=""channels_first""\n    )\n\n    flags_core.set_defaults(**defaults)\n    flags_core.parse_flags()\n\n    for key, value in defaults.items():\n      assert flags.FLAGS.get_flag_value(name=key, default=None) == value\n\n  def test_benchmark_setting(self):\n    defaults = dict(\n        hooks=[""LoggingMetricHook""],\n        benchmark_log_dir=""/tmp/12345"",\n        gcp_project=""project_abc"",\n    )\n\n    flags_core.set_defaults(**defaults)\n    flags_core.parse_flags()\n\n    for key, value in defaults.items():\n      assert flags.FLAGS.get_flag_value(name=key, default=None) == value\n\n  def test_booleans(self):\n    """"""Test to ensure boolean flags trigger as expected.\n    """"""\n\n    flags_core.parse_flags([__file__, ""--use_synthetic_data""])\n\n    assert flags.FLAGS.use_synthetic_data\n\n  def test_parse_dtype_info(self):\n    flags_core.parse_flags([__file__, ""--dtype"", ""fp16""])\n    self.assertEqual(flags_core.get_tf_dtype(flags.FLAGS), tf.float16)\n    self.assertEqual(flags_core.get_loss_scale(flags.FLAGS,\n                                               default_for_fp16=2), 2)\n\n    flags_core.parse_flags(\n        [__file__, ""--dtype"", ""fp16"", ""--loss_scale"", ""5""])\n    self.assertEqual(flags_core.get_loss_scale(flags.FLAGS,\n                                               default_for_fp16=2), 5)\n\n    flags_core.parse_flags(\n        [__file__, ""--dtype"", ""fp16"", ""--loss_scale"", ""dynamic""])\n    self.assertEqual(flags_core.get_loss_scale(flags.FLAGS,\n                                               default_for_fp16=2), ""dynamic"")\n\n    flags_core.parse_flags([__file__, ""--dtype"", ""fp32""])\n    self.assertEqual(flags_core.get_tf_dtype(flags.FLAGS), tf.float32)\n    self.assertEqual(flags_core.get_loss_scale(flags.FLAGS,\n                                               default_for_fp16=2), 1)\n\n    flags_core.parse_flags([__file__, ""--dtype"", ""fp32"", ""--loss_scale"", ""5""])\n    self.assertEqual(flags_core.get_loss_scale(flags.FLAGS,\n                                               default_for_fp16=2), 5)\n\n\n    with self.assertRaises(SystemExit):\n      flags_core.parse_flags([__file__, ""--dtype"", ""int8""])\n\n    with self.assertRaises(SystemExit):\n      flags_core.parse_flags([__file__, ""--dtype"", ""fp16"",\n                              ""--loss_scale"", ""abc""])\n\n  def test_get_nondefault_flags_as_str(self):\n    defaults = dict(\n        clean=True,\n        data_dir=""abc"",\n        hooks=[""LoggingTensorHook""],\n        stop_threshold=1.5,\n        use_synthetic_data=False\n    )\n    flags_core.set_defaults(**defaults)\n    flags_core.parse_flags()\n\n    expected_flags = """"\n    self.assertEqual(flags_core.get_nondefault_flags_as_str(), expected_flags)\n\n    flags.FLAGS.clean = False\n    expected_flags += ""--noclean""\n    self.assertEqual(flags_core.get_nondefault_flags_as_str(), expected_flags)\n\n    flags.FLAGS.data_dir = ""xyz""\n    expected_flags += "" --data_dir=xyz""\n    self.assertEqual(flags_core.get_nondefault_flags_as_str(), expected_flags)\n\n    flags.FLAGS.hooks = [""aaa"", ""bbb"", ""ccc""]\n    expected_flags += "" --hooks=aaa,bbb,ccc""\n    self.assertEqual(flags_core.get_nondefault_flags_as_str(), expected_flags)\n\n    flags.FLAGS.stop_threshold = 3.\n    expected_flags += "" --stop_threshold=3.0""\n    self.assertEqual(flags_core.get_nondefault_flags_as_str(), expected_flags)\n\n    flags.FLAGS.use_synthetic_data = True\n    expected_flags += "" --use_synthetic_data""\n    self.assertEqual(flags_core.get_nondefault_flags_as_str(), expected_flags)\n\n    # Assert that explicit setting a flag to its default value does not cause it\n    # to appear in the string\n    flags.FLAGS.use_synthetic_data = False\n    expected_flags = expected_flags[:-len("" --use_synthetic_data"")]\n    self.assertEqual(flags_core.get_nondefault_flags_as_str(), expected_flags)\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/__init__.py,0,b''
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/cloud_lib.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities that interact with cloud service.\n""""""\n\nimport requests\n\nGCP_METADATA_URL = ""http://metadata/computeMetadata/v1/instance/hostname""\nGCP_METADATA_HEADER = {""Metadata-Flavor"": ""Google""}\n\n\ndef on_gcp():\n  """"""Detect whether the current running environment is on GCP.""""""\n  try:\n    # Timeout in 5 seconds, in case the test environment has connectivity issue.\n    # There is not default timeout, which means it might block forever.\n    response = requests.get(\n        GCP_METADATA_URL, headers=GCP_METADATA_HEADER, timeout=5)\n    return response.status_code == 200\n  except requests.exceptions.RequestException:\n    return False\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/cloud_lib_test.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for cloud_lib.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport mock\nimport requests\n\nfrom official.utils.logs import cloud_lib\n\n\nclass CloudLibTest(unittest.TestCase):\n\n  @mock.patch(""requests.get"")\n  def test_on_gcp(self, mock_requests_get):\n    mock_response = mock.MagicMock()\n    mock_requests_get.return_value = mock_response\n    mock_response.status_code = 200\n\n    self.assertEqual(cloud_lib.on_gcp(), True)\n\n  @mock.patch(""requests.get"")\n  def test_not_on_gcp(self, mock_requests_get):\n    mock_requests_get.side_effect = requests.exceptions.ConnectionError()\n\n    self.assertEqual(cloud_lib.on_gcp(), False)\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/hooks.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hook that counts examples per second every N steps or seconds.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import logger\n\n\nclass ExamplesPerSecondHook(tf.estimator.SessionRunHook):\n  """"""Hook to print out examples per second.\n\n  Total time is tracked and then divided by the total number of steps\n  to get the average step time and then batch_size is used to determine\n  the running average of examples per second. The examples per second for the\n  most recent interval is also logged.\n  """"""\n\n  def __init__(self,\n               batch_size,\n               every_n_steps=None,\n               every_n_secs=None,\n               warm_steps=0,\n               metric_logger=None):\n    """"""Initializer for ExamplesPerSecondHook.\n\n    Args:\n      batch_size: Total batch size across all workers used to calculate\n        examples/second from global time.\n      every_n_steps: Log stats every n steps.\n      every_n_secs: Log stats every n seconds. Exactly one of the\n        `every_n_steps` or `every_n_secs` should be set.\n      warm_steps: The number of steps to be skipped before logging and running\n        average calculation. warm_steps steps refers to global steps across all\n        workers, not on each worker\n      metric_logger: instance of `BenchmarkLogger`, the benchmark logger that\n          hook should use to write the log. If None, BaseBenchmarkLogger will\n          be used.\n\n    Raises:\n      ValueError: if neither `every_n_steps` or `every_n_secs` is set, or\n      both are set.\n    """"""\n\n    if (every_n_steps is None) == (every_n_secs is None):\n      raise ValueError(""exactly one of every_n_steps""\n                       "" and every_n_secs should be provided."")\n\n    self._logger = metric_logger or logger.BaseBenchmarkLogger()\n\n    self._timer = tf.estimator.SecondOrStepTimer(\n        every_steps=every_n_steps, every_secs=every_n_secs)\n\n    self._step_train_time = 0\n    self._total_steps = 0\n    self._batch_size = batch_size\n    self._warm_steps = warm_steps\n    # List of examples per second logged every_n_steps.\n    self.current_examples_per_sec_list = []\n\n  def begin(self):\n    """"""Called once before using the session to check global step.""""""\n    self._global_step_tensor = tf.compat.v1.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          ""Global step should be created to use StepCounterHook."")\n\n  def before_run(self, run_context):  # pylint: disable=unused-argument\n    """"""Called before each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n\n    Returns:\n      A SessionRunArgs object or None if never triggered.\n    """"""\n    return tf.estimator.SessionRunArgs(self._global_step_tensor)\n\n  def after_run(self, run_context, run_values):  # pylint: disable=unused-argument\n    """"""Called after each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n      run_values: A SessionRunValues object.\n    """"""\n    global_step = run_values.results\n\n    if self._timer.should_trigger_for_step(\n        global_step) and global_step > self._warm_steps:\n      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(\n          global_step)\n      if elapsed_time is not None:\n        self._step_train_time += elapsed_time\n        self._total_steps += elapsed_steps\n\n        # average examples per second is based on the total (accumulative)\n        # training steps and training time so far\n        average_examples_per_sec = self._batch_size * (\n            self._total_steps / self._step_train_time)\n        # current examples per second is based on the elapsed training steps\n        # and training time per batch\n        current_examples_per_sec = self._batch_size * (\n            elapsed_steps / elapsed_time)\n        # Logs entries to be read from hook during or after run.\n        self.current_examples_per_sec_list.append(current_examples_per_sec)\n        self._logger.log_metric(\n            ""average_examples_per_sec"", average_examples_per_sec,\n            global_step=global_step)\n\n        self._logger.log_metric(\n            ""current_examples_per_sec"", current_examples_per_sec,\n            global_step=global_step)\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/hooks_helper.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hooks helper to return a list of TensorFlow hooks for training by name.\n\nMore hooks can be added to this set. To add a new hook, 1) add the new hook to\nthe registry in HOOKS, 2) add a corresponding function that parses out necessary\nparameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import hooks\nfrom official.utils.logs import logger\nfrom official.utils.logs import metric_hook\n\n_TENSORS_TO_LOG = dict((x, x) for x in [\'learning_rate\',\n                                        \'cross_entropy\',\n                                        \'train_accuracy\'])\n\n\ndef get_train_hooks(name_list, use_tpu=False, **kwargs):\n  """"""Factory for getting a list of TensorFlow hooks for training by name.\n\n  Args:\n    name_list: a list of strings to name desired hook classes. Allowed:\n      LoggingTensorHook, ProfilerHook, ExamplesPerSecondHook, which are defined\n      as keys in HOOKS\n    use_tpu: Boolean of whether computation occurs on a TPU. This will disable\n      hooks altogether.\n    **kwargs: a dictionary of arguments to the hooks.\n\n  Returns:\n    list of instantiated hooks, ready to be used in a classifier.train call.\n\n  Raises:\n    ValueError: if an unrecognized name is passed.\n  """"""\n\n  if not name_list:\n    return []\n\n  if use_tpu:\n    tf.compat.v1.logging.warning(\'hooks_helper received name_list `{}`, but a \'\n                                 \'TPU is specified. No hooks will be used.\'\n                                 .format(name_list))\n    return []\n\n  train_hooks = []\n  for name in name_list:\n    hook_name = HOOKS.get(name.strip().lower())\n    if hook_name is None:\n      raise ValueError(\'Unrecognized training hook requested: {}\'.format(name))\n    else:\n      train_hooks.append(hook_name(**kwargs))\n\n  return train_hooks\n\n\ndef get_logging_tensor_hook(every_n_iter=100, tensors_to_log=None, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get LoggingTensorHook.\n\n  Args:\n    every_n_iter: `int`, print the values of `tensors` once every N local\n      steps taken on the current worker.\n    tensors_to_log: List of tensor names or dictionary mapping labels to tensor\n      names. If not set, log _TENSORS_TO_LOG by default.\n    **kwargs: a dictionary of arguments to LoggingTensorHook.\n\n  Returns:\n    Returns a LoggingTensorHook with a standard set of tensors that will be\n    printed to stdout.\n  """"""\n  if tensors_to_log is None:\n    tensors_to_log = _TENSORS_TO_LOG\n\n  return tf.estimator.LoggingTensorHook(\n      tensors=tensors_to_log,\n      every_n_iter=every_n_iter)\n\n\ndef get_profiler_hook(model_dir, save_steps=1000, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ProfilerHook.\n\n  Args:\n    model_dir: The directory to save the profile traces to.\n    save_steps: `int`, print profile traces every N steps.\n    **kwargs: a dictionary of arguments to ProfilerHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return tf.estimator.ProfilerHook(save_steps=save_steps, output_dir=model_dir)\n\n\ndef get_examples_per_second_hook(every_n_steps=100,\n                                 batch_size=128,\n                                 warm_steps=5,\n                                 **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ExamplesPerSecondHook.\n\n  Args:\n    every_n_steps: `int`, print current and average examples per second every\n      N steps.\n    batch_size: `int`, total batch size used to calculate examples/second from\n      global time.\n    warm_steps: skip this number of steps before logging and running average.\n    **kwargs: a dictionary of arguments to ExamplesPerSecondHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return hooks.ExamplesPerSecondHook(\n      batch_size=batch_size, every_n_steps=every_n_steps,\n      warm_steps=warm_steps, metric_logger=logger.get_benchmark_logger())\n\n\ndef get_logging_metric_hook(tensors_to_log=None,\n                            every_n_secs=600,\n                            **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get LoggingMetricHook.\n\n  Args:\n    tensors_to_log: List of tensor names or dictionary mapping labels to tensor\n      names. If not set, log _TENSORS_TO_LOG by default.\n    every_n_secs: `int`, the frequency for logging the metric. Default to every\n      10 mins.\n    **kwargs: a dictionary of arguments.\n\n  Returns:\n    Returns a LoggingMetricHook that saves tensor values in a JSON format.\n  """"""\n  if tensors_to_log is None:\n    tensors_to_log = _TENSORS_TO_LOG\n  return metric_hook.LoggingMetricHook(\n      tensors=tensors_to_log,\n      metric_logger=logger.get_benchmark_logger(),\n      every_n_secs=every_n_secs)\n\n\ndef get_step_counter_hook(**kwargs):\n  """"""Function to get StepCounterHook.""""""\n  del kwargs\n  return tf.estimator.StepCounterHook()\n\n\n# A dictionary to map one hook name and its corresponding function\nHOOKS = {\n    \'loggingtensorhook\': get_logging_tensor_hook,\n    \'profilerhook\': get_profiler_hook,\n    \'examplespersecondhook\': get_examples_per_second_hook,\n    \'loggingmetrichook\': get_logging_metric_hook,\n    \'stepcounterhook\': get_step_counter_hook\n}\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/hooks_helper_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for hooks_helper.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import hooks_helper\nfrom official.utils.misc import keras_utils\n\n\nclass BaseTest(unittest.TestCase):\n\n  def setUp(self):\n    super(BaseTest, self).setUp()\n    if keras_utils.is_v2_0:\n      tf.compat.v1.disable_eager_execution()\n\n  def test_raise_in_non_list_names(self):\n    with self.assertRaises(ValueError):\n      hooks_helper.get_train_hooks(\n          \'LoggingTensorHook, ProfilerHook\', model_dir="""", batch_size=256)\n\n  def test_raise_in_invalid_names(self):\n    invalid_names = [\'StepCounterHook\', \'StopAtStepHook\']\n    with self.assertRaises(ValueError):\n      hooks_helper.get_train_hooks(invalid_names, model_dir="""", batch_size=256)\n\n  def validate_train_hook_name(self,\n                               test_hook_name,\n                               expected_hook_name,\n                               **kwargs):\n    returned_hook = hooks_helper.get_train_hooks(\n        [test_hook_name], model_dir="""", **kwargs)\n    self.assertEqual(len(returned_hook), 1)\n    self.assertIsInstance(returned_hook[0], tf.estimator.SessionRunHook)\n    self.assertEqual(returned_hook[0].__class__.__name__.lower(),\n                     expected_hook_name)\n\n  def test_get_train_hooks_logging_tensor_hook(self):\n    self.validate_train_hook_name(\'LoggingTensorHook\', \'loggingtensorhook\')\n\n  def test_get_train_hooks_profiler_hook(self):\n    self.validate_train_hook_name(\'ProfilerHook\', \'profilerhook\')\n\n  def test_get_train_hooks_examples_per_second_hook(self):\n    self.validate_train_hook_name(\'ExamplesPerSecondHook\',\n                                  \'examplespersecondhook\')\n\n  def test_get_logging_metric_hook(self):\n    test_hook_name = \'LoggingMetricHook\'\n    self.validate_train_hook_name(test_hook_name, \'loggingmetrichook\')\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/hooks_test.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for hooks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import hooks\nfrom official.utils.testing import mock_lib\n\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)\n\n\nclass ExamplesPerSecondHookTest(tf.test.TestCase):\n  """"""Tests for the ExamplesPerSecondHook.\n\n  In the test, we explicitly run global_step tensor after train_op in order to\n  keep the global_step value and the train_op (which increase the glboal_step\n  by 1) consistent. This is to correct the discrepancies in reported global_step\n  value when running on GPUs.\n  """"""\n\n  def setUp(self):\n    """"""Mock out logging calls to verify if correct info is being monitored.""""""\n    self._logger = mock_lib.MockBenchmarkLogger()\n\n    self.graph = tf.Graph()\n    with self.graph.as_default():\n      tf.compat.v1.train.create_global_step()\n      self.train_op = tf.compat.v1.assign_add(\n          tf.compat.v1.train.get_global_step(), 1)\n      self.global_step = tf.compat.v1.train.get_global_step()\n\n  def test_raise_in_both_secs_and_steps(self):\n    with self.assertRaises(ValueError):\n      hooks.ExamplesPerSecondHook(\n          batch_size=256,\n          every_n_steps=10,\n          every_n_secs=20,\n          metric_logger=self._logger)\n\n  def test_raise_in_none_secs_and_steps(self):\n    with self.assertRaises(ValueError):\n      hooks.ExamplesPerSecondHook(\n          batch_size=256,\n          every_n_steps=None,\n          every_n_secs=None,\n          metric_logger=self._logger)\n\n  def _validate_log_every_n_steps(self, every_n_steps, warm_steps):\n    hook = hooks.ExamplesPerSecondHook(\n        batch_size=256,\n        every_n_steps=every_n_steps,\n        warm_steps=warm_steps,\n        metric_logger=self._logger)\n\n    with tf.compat.v1.train.MonitoredSession(\n        tf.compat.v1.train.ChiefSessionCreator(), [hook]) as mon_sess:\n      for _ in range(every_n_steps):\n        # Explicitly run global_step after train_op to get the accurate\n        # global_step value\n        mon_sess.run(self.train_op)\n        mon_sess.run(self.global_step)\n        # Nothing should be in the list yet\n        self.assertFalse(self._logger.logged_metric)\n\n      mon_sess.run(self.train_op)\n      global_step_val = mon_sess.run(self.global_step)\n\n      if global_step_val > warm_steps:\n        self._assert_metrics()\n      else:\n        # Nothing should be in the list yet\n        self.assertFalse(self._logger.logged_metric)\n\n      # Add additional run to verify proper reset when called multiple times.\n      prev_log_len = len(self._logger.logged_metric)\n      mon_sess.run(self.train_op)\n      global_step_val = mon_sess.run(self.global_step)\n\n      if every_n_steps == 1 and global_step_val > warm_steps:\n        # Each time, we log two additional metrics. Did exactly 2 get added?\n        self.assertEqual(len(self._logger.logged_metric), prev_log_len + 2)\n      else:\n        # No change in the size of the metric list.\n        self.assertEqual(len(self._logger.logged_metric), prev_log_len)\n\n  def test_examples_per_sec_every_1_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(1, 0)\n\n  def test_examples_per_sec_every_5_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(5, 0)\n\n  def test_examples_per_sec_every_1_steps_with_warm_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(1, 10)\n\n  def test_examples_per_sec_every_5_steps_with_warm_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(5, 10)\n\n  def _validate_log_every_n_secs(self, every_n_secs):\n    hook = hooks.ExamplesPerSecondHook(\n        batch_size=256,\n        every_n_steps=None,\n        every_n_secs=every_n_secs,\n        metric_logger=self._logger)\n\n    with tf.compat.v1.train.MonitoredSession(\n        tf.compat.v1.train.ChiefSessionCreator(), [hook]) as mon_sess:\n      # Explicitly run global_step after train_op to get the accurate\n      # global_step value\n      mon_sess.run(self.train_op)\n      mon_sess.run(self.global_step)\n      # Nothing should be in the list yet\n      self.assertFalse(self._logger.logged_metric)\n      time.sleep(every_n_secs)\n\n      mon_sess.run(self.train_op)\n      mon_sess.run(self.global_step)\n      self._assert_metrics()\n\n  def test_examples_per_sec_every_1_secs(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_secs(1)\n\n  def test_examples_per_sec_every_5_secs(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_secs(5)\n\n  def _assert_metrics(self):\n    metrics = self._logger.logged_metric\n    self.assertEqual(metrics[-2][""name""], ""average_examples_per_sec"")\n    self.assertEqual(metrics[-1][""name""], ""current_examples_per_sec"")\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/logger.py,15,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Logging utilities for benchmark.\n\nFor collecting local environment metrics like CPU and memory, certain python\npackages need be installed. See README for details.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport datetime\nimport json\nimport multiprocessing\nimport numbers\nimport os\nimport threading\nimport uuid\n\nfrom six.moves import _thread as thread\nfrom absl import flags\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nfrom official.utils.logs import cloud_lib\n\nMETRIC_LOG_FILE_NAME = ""metric.log""\nBENCHMARK_RUN_LOG_FILE_NAME = ""benchmark_run.log""\n_DATE_TIME_FORMAT_PATTERN = ""%Y-%m-%dT%H:%M:%S.%fZ""\nGCP_TEST_ENV = ""GCP""\nRUN_STATUS_SUCCESS = ""success""\nRUN_STATUS_FAILURE = ""failure""\nRUN_STATUS_RUNNING = ""running""\n\n\nFLAGS = flags.FLAGS\n\n# Don\'t use it directly. Use get_benchmark_logger to access a logger.\n_benchmark_logger = None\n_logger_lock = threading.Lock()\n\n\ndef config_benchmark_logger(flag_obj=None):\n  """"""Config the global benchmark logger.""""""\n  _logger_lock.acquire()\n  try:\n    global _benchmark_logger\n    if not flag_obj:\n      flag_obj = FLAGS\n\n    if (not hasattr(flag_obj, ""benchmark_logger_type"") or\n        flag_obj.benchmark_logger_type == ""BaseBenchmarkLogger""):\n      _benchmark_logger = BaseBenchmarkLogger()\n    elif flag_obj.benchmark_logger_type == ""BenchmarkFileLogger"":\n      _benchmark_logger = BenchmarkFileLogger(flag_obj.benchmark_log_dir)\n    elif flag_obj.benchmark_logger_type == ""BenchmarkBigQueryLogger"":\n      from official.benchmark import benchmark_uploader as bu  # pylint: disable=g-import-not-at-top\n      bq_uploader = bu.BigQueryUploader(gcp_project=flag_obj.gcp_project)\n      _benchmark_logger = BenchmarkBigQueryLogger(\n          bigquery_uploader=bq_uploader,\n          bigquery_data_set=flag_obj.bigquery_data_set,\n          bigquery_run_table=flag_obj.bigquery_run_table,\n          bigquery_run_status_table=flag_obj.bigquery_run_status_table,\n          bigquery_metric_table=flag_obj.bigquery_metric_table,\n          run_id=str(uuid.uuid4()))\n    else:\n      raise ValueError(""Unrecognized benchmark_logger_type: %s""\n                       % flag_obj.benchmark_logger_type)\n\n  finally:\n    _logger_lock.release()\n  return _benchmark_logger\n\n\ndef get_benchmark_logger():\n  if not _benchmark_logger:\n    config_benchmark_logger()\n  return _benchmark_logger\n\n\n@contextlib.contextmanager\ndef benchmark_context(flag_obj):\n  """"""Context of benchmark, which will update status of the run accordingly.""""""\n  benchmark_logger = config_benchmark_logger(flag_obj)\n  try:\n    yield\n    benchmark_logger.on_finish(RUN_STATUS_SUCCESS)\n  except Exception:  # pylint: disable=broad-except\n    # Catch all the exception, update the run status to be failure, and re-raise\n    benchmark_logger.on_finish(RUN_STATUS_FAILURE)\n    raise\n\n\nclass BaseBenchmarkLogger(object):\n  """"""Class to log the benchmark information to STDOUT.""""""\n\n  def log_evaluation_result(self, eval_results):\n    """"""Log the evaluation result.\n\n    The evaluate result is a dictionary that contains metrics defined in\n    model_fn. It also contains a entry for global_step which contains the value\n    of the global step when evaluation was performed.\n\n    Args:\n      eval_results: dict, the result of evaluate.\n    """"""\n    if not isinstance(eval_results, dict):\n      tf.compat.v1.logging.warning(\n          ""eval_results should be dictionary for logging. Got %s"",\n          type(eval_results))\n      return\n    global_step = eval_results[tf.compat.v1.GraphKeys.GLOBAL_STEP]\n    for key in sorted(eval_results):\n      if key != tf.compat.v1.GraphKeys.GLOBAL_STEP:\n        self.log_metric(key, eval_results[key], global_step=global_step)\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to local file.\n\n    Currently the logging is done in a synchronized way. This should be updated\n    to log asynchronously.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    metric = _process_metric_to_json(name, value, unit, global_step, extras)\n    if metric:\n      tf.compat.v1.logging.info(""Benchmark metric: %s"", metric)\n\n  def log_run_info(self, model_name, dataset_name, run_params, test_id=None):\n    tf.compat.v1.logging.info(\n        ""Benchmark run: %s"", _gather_run_info(model_name, dataset_name,\n                                              run_params, test_id))\n\n  def on_finish(self, status):\n    pass\n\n\nclass BenchmarkFileLogger(BaseBenchmarkLogger):\n  """"""Class to log the benchmark information to local disk.""""""\n\n  def __init__(self, logging_dir):\n    super(BenchmarkFileLogger, self).__init__()\n    self._logging_dir = logging_dir\n    if not tf.io.gfile.isdir(self._logging_dir):\n      tf.io.gfile.makedirs(self._logging_dir)\n    self._metric_file_handler = tf.io.gfile.GFile(\n        os.path.join(self._logging_dir, METRIC_LOG_FILE_NAME), ""a"")\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to local file.\n\n    Currently the logging is done in a synchronized way. This should be updated\n    to log asynchronously.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    metric = _process_metric_to_json(name, value, unit, global_step, extras)\n    if metric:\n      try:\n        json.dump(metric, self._metric_file_handler)\n        self._metric_file_handler.write(""\\n"")\n        self._metric_file_handler.flush()\n      except (TypeError, ValueError) as e:\n        tf.compat.v1.logging.warning(\n            ""Failed to dump metric to log file: name %s, value %s, error %s"",\n            name, value, e)\n\n  def log_run_info(self, model_name, dataset_name, run_params, test_id=None):\n    """"""Collect most of the TF runtime information for the local env.\n\n    The schema of the run info follows official/benchmark/datastore/schema.\n\n    Args:\n      model_name: string, the name of the model.\n      dataset_name: string, the name of dataset for training and evaluation.\n      run_params: dict, the dictionary of parameters for the run, it could\n        include hyperparameters or other params that are important for the run.\n      test_id: string, the unique name of the test run by the combination of key\n        parameters, eg batch size, num of GPU. It is hardware independent.\n    """"""\n    run_info = _gather_run_info(model_name, dataset_name, run_params, test_id)\n\n    with tf.io.gfile.GFile(os.path.join(\n        self._logging_dir, BENCHMARK_RUN_LOG_FILE_NAME), ""w"") as f:\n      try:\n        json.dump(run_info, f)\n        f.write(""\\n"")\n      except (TypeError, ValueError) as e:\n        tf.compat.v1.logging.warning(\n            ""Failed to dump benchmark run info to log file: %s"", e)\n\n  def on_finish(self, status):\n    self._metric_file_handler.flush()\n    self._metric_file_handler.close()\n\n\nclass BenchmarkBigQueryLogger(BaseBenchmarkLogger):\n  """"""Class to log the benchmark information to BigQuery data store.""""""\n\n  def __init__(self,\n               bigquery_uploader,\n               bigquery_data_set,\n               bigquery_run_table,\n               bigquery_run_status_table,\n               bigquery_metric_table,\n               run_id):\n    super(BenchmarkBigQueryLogger, self).__init__()\n    self._bigquery_uploader = bigquery_uploader\n    self._bigquery_data_set = bigquery_data_set\n    self._bigquery_run_table = bigquery_run_table\n    self._bigquery_run_status_table = bigquery_run_status_table\n    self._bigquery_metric_table = bigquery_metric_table\n    self._run_id = run_id\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to bigquery.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    metric = _process_metric_to_json(name, value, unit, global_step, extras)\n    if metric:\n      # Starting new thread for bigquery upload in case it might take long time\n      # and impact the benchmark and performance measurement. Starting a new\n      # thread might have potential performance impact for model that run on\n      # CPU.\n      thread.start_new_thread(\n          self._bigquery_uploader.upload_benchmark_metric_json,\n          (self._bigquery_data_set,\n           self._bigquery_metric_table,\n           self._run_id,\n           [metric]))\n\n  def log_run_info(self, model_name, dataset_name, run_params, test_id=None):\n    """"""Collect most of the TF runtime information for the local env.\n\n    The schema of the run info follows official/benchmark/datastore/schema.\n\n    Args:\n      model_name: string, the name of the model.\n      dataset_name: string, the name of dataset for training and evaluation.\n      run_params: dict, the dictionary of parameters for the run, it could\n        include hyperparameters or other params that are important for the run.\n      test_id: string, the unique name of the test run by the combination of key\n        parameters, eg batch size, num of GPU. It is hardware independent.\n    """"""\n    run_info = _gather_run_info(model_name, dataset_name, run_params, test_id)\n    # Starting new thread for bigquery upload in case it might take long time\n    # and impact the benchmark and performance measurement. Starting a new\n    # thread might have potential performance impact for model that run on CPU.\n    thread.start_new_thread(\n        self._bigquery_uploader.upload_benchmark_run_json,\n        (self._bigquery_data_set,\n         self._bigquery_run_table,\n         self._run_id,\n         run_info))\n    thread.start_new_thread(\n        self._bigquery_uploader.insert_run_status,\n        (self._bigquery_data_set,\n         self._bigquery_run_status_table,\n         self._run_id,\n         RUN_STATUS_RUNNING))\n\n  def on_finish(self, status):\n    self._bigquery_uploader.update_run_status(\n        self._bigquery_data_set,\n        self._bigquery_run_status_table,\n        self._run_id,\n        status)\n\n\ndef _gather_run_info(model_name, dataset_name, run_params, test_id):\n  """"""Collect the benchmark run information for the local environment.""""""\n  run_info = {\n      ""model_name"": model_name,\n      ""dataset"": {""name"": dataset_name},\n      ""machine_config"": {},\n      ""test_id"": test_id,\n      ""run_date"": datetime.datetime.utcnow().strftime(\n          _DATE_TIME_FORMAT_PATTERN)}\n  _collect_tensorflow_info(run_info)\n  _collect_tensorflow_environment_variables(run_info)\n  _collect_run_params(run_info, run_params)\n  _collect_cpu_info(run_info)\n  _collect_memory_info(run_info)\n  _collect_test_environment(run_info)\n  return run_info\n\n\ndef _process_metric_to_json(\n    name, value, unit=None, global_step=None, extras=None):\n  """"""Validate the metric data and generate JSON for insert.""""""\n  if not isinstance(value, numbers.Number):\n    tf.compat.v1.logging.warning(\n        ""Metric value to log should be a number. Got %s"", type(value))\n    return None\n\n  extras = _convert_to_json_dict(extras)\n  return {\n      ""name"": name,\n      ""value"": float(value),\n      ""unit"": unit,\n      ""global_step"": global_step,\n      ""timestamp"": datetime.datetime.utcnow().strftime(\n          _DATE_TIME_FORMAT_PATTERN),\n      ""extras"": extras}\n\n\ndef _collect_tensorflow_info(run_info):\n  run_info[""tensorflow_version""] = {\n      ""version"": tf.version.VERSION, ""git_hash"": tf.version.GIT_VERSION}\n\n\ndef _collect_run_params(run_info, run_params):\n  """"""Log the parameter information for the benchmark run.""""""\n  def process_param(name, value):\n    type_check = {\n        str: {""name"": name, ""string_value"": value},\n        int: {""name"": name, ""long_value"": value},\n        bool: {""name"": name, ""bool_value"": str(value)},\n        float: {""name"": name, ""float_value"": value},\n    }\n    return type_check.get(type(value),\n                          {""name"": name, ""string_value"": str(value)})\n  if run_params:\n    run_info[""run_parameters""] = [\n        process_param(k, v) for k, v in sorted(run_params.items())]\n\n\ndef _collect_tensorflow_environment_variables(run_info):\n  run_info[""tensorflow_environment_variables""] = [\n      {""name"": k, ""value"": v}\n      for k, v in sorted(os.environ.items()) if k.startswith(""TF_"")]\n\n\n# The following code is mirrored from tensorflow/tools/test/system_info_lib\n# which is not exposed for import.\ndef _collect_cpu_info(run_info):\n  """"""Collect the CPU information for the local environment.""""""\n  cpu_info = {}\n\n  cpu_info[""num_cores""] = multiprocessing.cpu_count()\n\n  try:\n    # Note: cpuinfo is not installed in the TensorFlow OSS tree.\n    # It is installable via pip.\n    import cpuinfo    # pylint: disable=g-import-not-at-top\n\n    info = cpuinfo.get_cpu_info()\n    cpu_info[""cpu_info""] = info[""brand""]\n    cpu_info[""mhz_per_cpu""] = info[""hz_advertised_raw""][0] / 1.0e6\n\n    run_info[""machine_config""][""cpu_info""] = cpu_info\n  except ImportError:\n    tf.compat.v1.logging.warn(\n        ""\'cpuinfo\' not imported. CPU info will not be logged."")\n\n\ndef _collect_memory_info(run_info):\n  try:\n    # Note: psutil is not installed in the TensorFlow OSS tree.\n    # It is installable via pip.\n    import psutil   # pylint: disable=g-import-not-at-top\n    vmem = psutil.virtual_memory()\n    run_info[""machine_config""][""memory_total""] = vmem.total\n    run_info[""machine_config""][""memory_available""] = vmem.available\n  except ImportError:\n    tf.compat.v1.logging.warn(\n        ""\'psutil\' not imported. Memory info will not be logged."")\n\n\ndef _collect_test_environment(run_info):\n  """"""Detect the local environment, eg GCE, AWS or DGX, etc.""""""\n  if cloud_lib.on_gcp():\n    run_info[""test_environment""] = GCP_TEST_ENV\n  # TODO(scottzhu): Add more testing env detection for other platform\n\n\ndef _parse_gpu_model(physical_device_desc):\n  # Assume all the GPU connected are same model\n  for kv in physical_device_desc.split("",""):\n    k, _, v = kv.partition("":"")\n    if k.strip() == ""name"":\n      return v.strip()\n  return None\n\n\ndef _convert_to_json_dict(input_dict):\n  if input_dict:\n    return [{""name"": k, ""value"": v} for k, v in sorted(input_dict.items())]\n  else:\n    return []\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/logger_test.py,28,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for benchmark logger.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport tempfile\nimport time\nimport unittest\n\nimport mock\nfrom absl.testing import flagsaver\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\ntry:\n  from google.cloud import bigquery\nexcept ImportError:\n  bigquery = None\n\nfrom official.utils.misc import keras_utils\nfrom official.utils.flags import core as flags_core\nfrom official.utils.logs import logger\n\n\nclass BenchmarkLoggerTest(tf.test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):  # pylint: disable=invalid-name\n    super(BenchmarkLoggerTest, cls).setUpClass()\n    flags_core.define_benchmark()\n\n  def test_get_default_benchmark_logger(self):\n    with flagsaver.flagsaver(benchmark_logger_type=""foo""):\n      self.assertIsInstance(logger.get_benchmark_logger(),\n                            logger.BaseBenchmarkLogger)\n\n  def test_config_base_benchmark_logger(self):\n    with flagsaver.flagsaver(benchmark_logger_type=""BaseBenchmarkLogger""):\n      logger.config_benchmark_logger()\n      self.assertIsInstance(logger.get_benchmark_logger(),\n                            logger.BaseBenchmarkLogger)\n\n  def test_config_benchmark_file_logger(self):\n    # Set the benchmark_log_dir first since the benchmark_logger_type will need\n    # the value to be set when it does the validation.\n    with flagsaver.flagsaver(benchmark_log_dir=""/tmp""):\n      with flagsaver.flagsaver(benchmark_logger_type=""BenchmarkFileLogger""):\n        logger.config_benchmark_logger()\n        self.assertIsInstance(logger.get_benchmark_logger(),\n                              logger.BenchmarkFileLogger)\n\n  @unittest.skipIf(bigquery is None, ""Bigquery dependency is not installed."")\n  @mock.patch.object(bigquery, ""Client"")\n  def test_config_benchmark_bigquery_logger(self, mock_bigquery_client):\n    with flagsaver.flagsaver(benchmark_logger_type=""BenchmarkBigQueryLogger""):\n      logger.config_benchmark_logger()\n      self.assertIsInstance(logger.get_benchmark_logger(),\n                            logger.BenchmarkBigQueryLogger)\n\n  @mock.patch(""official.utils.logs.logger.config_benchmark_logger"")\n  def test_benchmark_context(self, mock_config_benchmark_logger):\n    mock_logger = mock.MagicMock()\n    mock_config_benchmark_logger.return_value = mock_logger\n    with logger.benchmark_context(None):\n      tf.compat.v1.logging.info(""start benchmarking"")\n    mock_logger.on_finish.assert_called_once_with(logger.RUN_STATUS_SUCCESS)\n\n  @mock.patch(""official.utils.logs.logger.config_benchmark_logger"")\n  def test_benchmark_context_failure(self, mock_config_benchmark_logger):\n    mock_logger = mock.MagicMock()\n    mock_config_benchmark_logger.return_value = mock_logger\n    with self.assertRaises(RuntimeError):\n      with logger.benchmark_context(None):\n        raise RuntimeError(""training error"")\n    mock_logger.on_finish.assert_called_once_with(logger.RUN_STATUS_FAILURE)\n\n\nclass BaseBenchmarkLoggerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(BaseBenchmarkLoggerTest, self).setUp()\n    self._actual_log = tf.compat.v1.logging.info\n    self.logged_message = None\n\n    def mock_log(*args, **kwargs):\n      self.logged_message = args\n      self._actual_log(*args, **kwargs)\n\n    tf.compat.v1.logging.info = mock_log\n\n  def tearDown(self):\n    super(BaseBenchmarkLoggerTest, self).tearDown()\n    tf.compat.v1.logging.info = self._actual_log\n\n  def test_log_metric(self):\n    log = logger.BaseBenchmarkLogger()\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n\n    expected_log_prefix = ""Benchmark metric:""\n    self.assertRegexpMatches(str(self.logged_message), expected_log_prefix)\n\n\nclass BenchmarkFileLoggerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(BenchmarkFileLoggerTest, self).setUp()\n    # Avoid pulling extra env vars from test environment which affects the test\n    # result, eg. Kokoro test has a TF_PKG env which affect the test case\n    # test_collect_tensorflow_environment_variables()\n    self.original_environ = dict(os.environ)\n    os.environ.clear()\n\n  def tearDown(self):\n    super(BenchmarkFileLoggerTest, self).tearDown()\n    tf.io.gfile.rmtree(self.get_temp_dir())\n    os.environ.clear()\n    os.environ.update(self.original_environ)\n\n  def test_create_logging_dir(self):\n    non_exist_temp_dir = os.path.join(self.get_temp_dir(), ""unknown_dir"")\n    self.assertFalse(tf.io.gfile.isdir(non_exist_temp_dir))\n\n    logger.BenchmarkFileLogger(non_exist_temp_dir)\n    self.assertTrue(tf.io.gfile.isdir(non_exist_temp_dir))\n\n  def test_log_metric(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.io.gfile.exists(metric_log))\n    with tf.io.gfile.GFile(metric_log) as f:\n      metric = json.loads(f.readline())\n      self.assertEqual(metric[""name""], ""accuracy"")\n      self.assertEqual(metric[""value""], 0.999)\n      self.assertEqual(metric[""unit""], None)\n      self.assertEqual(metric[""global_step""], 1e4)\n      self.assertEqual(metric[""extras""], [{""name"": ""name"", ""value"": ""value""}])\n\n  def test_log_multiple_metrics(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n    log.log_metric(""loss"", 0.02, global_step=1e4)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.io.gfile.exists(metric_log))\n    with tf.io.gfile.GFile(metric_log) as f:\n      accuracy = json.loads(f.readline())\n      self.assertEqual(accuracy[""name""], ""accuracy"")\n      self.assertEqual(accuracy[""value""], 0.999)\n      self.assertEqual(accuracy[""unit""], None)\n      self.assertEqual(accuracy[""global_step""], 1e4)\n      self.assertEqual(accuracy[""extras""], [{""name"": ""name"", ""value"": ""value""}])\n\n      loss = json.loads(f.readline())\n      self.assertEqual(loss[""name""], ""loss"")\n      self.assertEqual(loss[""value""], 0.02)\n      self.assertEqual(loss[""unit""], None)\n      self.assertEqual(loss[""global_step""], 1e4)\n      self.assertEqual(loss[""extras""], [])\n\n  def test_log_non_number_value(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    const = tf.constant(1)\n    log.log_metric(""accuracy"", const)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertFalse(tf.io.gfile.exists(metric_log))\n\n  def test_log_evaluation_result(self):\n    eval_result = {""loss"": 0.46237424,\n                   ""global_step"": 207082,\n                   ""accuracy"": 0.9285}\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_evaluation_result(eval_result)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.io.gfile.exists(metric_log))\n    with tf.io.gfile.GFile(metric_log) as f:\n      accuracy = json.loads(f.readline())\n      self.assertEqual(accuracy[""name""], ""accuracy"")\n      self.assertEqual(accuracy[""value""], 0.9285)\n      self.assertEqual(accuracy[""unit""], None)\n      self.assertEqual(accuracy[""global_step""], 207082)\n\n      loss = json.loads(f.readline())\n      self.assertEqual(loss[""name""], ""loss"")\n      self.assertEqual(loss[""value""], 0.46237424)\n      self.assertEqual(loss[""unit""], None)\n      self.assertEqual(loss[""global_step""], 207082)\n\n  def test_log_evaluation_result_with_invalid_type(self):\n    eval_result = ""{\'loss\': 0.46237424, \'global_step\': 207082}""\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_evaluation_result(eval_result)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertFalse(tf.io.gfile.exists(metric_log))\n\n  @mock.patch(""official.utils.logs.logger._gather_run_info"")\n  def test_log_run_info(self, mock_gather_run_info):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    run_info = {""model_name"": ""model_name"",\n                ""dataset"": ""dataset_name"",\n                ""run_info"": ""run_value""}\n    mock_gather_run_info.return_value = run_info\n    log.log_run_info(""model_name"", ""dataset_name"", {})\n\n    run_log = os.path.join(log_dir, ""benchmark_run.log"")\n    self.assertTrue(tf.io.gfile.exists(run_log))\n    with tf.io.gfile.GFile(run_log) as f:\n      run_info = json.loads(f.readline())\n      self.assertEqual(run_info[""model_name""], ""model_name"")\n      self.assertEqual(run_info[""dataset""], ""dataset_name"")\n      self.assertEqual(run_info[""run_info""], ""run_value"")\n\n  def test_collect_tensorflow_info(self):\n    run_info = {}\n    logger._collect_tensorflow_info(run_info)\n    self.assertNotEqual(run_info[""tensorflow_version""], {})\n    self.assertEqual(run_info[""tensorflow_version""][""version""],\n                     tf.version.VERSION)\n    self.assertEqual(run_info[""tensorflow_version""][""git_hash""],\n                     tf.version.GIT_VERSION)\n\n  def test_collect_run_params(self):\n    run_info = {}\n    run_parameters = {\n        ""batch_size"": 32,\n        ""synthetic_data"": True,\n        ""train_epochs"": 100.00,\n        ""dtype"": ""fp16"",\n        ""resnet_size"": 50,\n        ""random_tensor"": tf.constant(2.0)\n    }\n    logger._collect_run_params(run_info, run_parameters)\n    self.assertEqual(len(run_info[""run_parameters""]), 6)\n    self.assertEqual(run_info[""run_parameters""][0],\n                     {""name"": ""batch_size"", ""long_value"": 32})\n    self.assertEqual(run_info[""run_parameters""][1],\n                     {""name"": ""dtype"", ""string_value"": ""fp16""})\n    v1_tensor = {""name"": ""random_tensor"", ""string_value"":\n                     ""Tensor(\\""Const:0\\"", shape=(), dtype=float32)""}\n    v2_tensor = {""name"": ""random_tensor"", ""string_value"":\n                     ""tf.Tensor(2.0, shape=(), dtype=float32)""}\n    self.assertIn(run_info[""run_parameters""][2], [v1_tensor, v2_tensor])\n\n\n    self.assertEqual(run_info[""run_parameters""][3],\n                     {""name"": ""resnet_size"", ""long_value"": 50})\n    self.assertEqual(run_info[""run_parameters""][4],\n                     {""name"": ""synthetic_data"", ""bool_value"": ""True""})\n    self.assertEqual(run_info[""run_parameters""][5],\n                     {""name"": ""train_epochs"", ""float_value"": 100.00})\n\n  def test_collect_tensorflow_environment_variables(self):\n    os.environ[""TF_ENABLE_WINOGRAD_NONFUSED""] = ""1""\n    os.environ[""TF_OTHER""] = ""2""\n    os.environ[""OTHER""] = ""3""\n\n    run_info = {}\n    logger._collect_tensorflow_environment_variables(run_info)\n    self.assertIsNotNone(run_info[""tensorflow_environment_variables""])\n    expected_tf_envs = [\n        {""name"": ""TF_ENABLE_WINOGRAD_NONFUSED"", ""value"": ""1""},\n        {""name"": ""TF_OTHER"", ""value"": ""2""},\n    ]\n    self.assertEqual(run_info[""tensorflow_environment_variables""],\n                     expected_tf_envs)\n\n  def test_collect_memory_info(self):\n    run_info = {""machine_config"": {}}\n    logger._collect_memory_info(run_info)\n    self.assertIsNotNone(run_info[""machine_config""][""memory_total""])\n    self.assertIsNotNone(run_info[""machine_config""][""memory_available""])\n\n\n@unittest.skipIf(bigquery is None, ""Bigquery dependency is not installed."")\nclass BenchmarkBigQueryLoggerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(BenchmarkBigQueryLoggerTest, self).setUp()\n    # Avoid pulling extra env vars from test environment which affects the test\n    # result, eg. Kokoro test has a TF_PKG env which affect the test case\n    # test_collect_tensorflow_environment_variables()\n    self.original_environ = dict(os.environ)\n    os.environ.clear()\n\n    self.mock_bq_uploader = mock.MagicMock()\n    self.logger = logger.BenchmarkBigQueryLogger(\n        self.mock_bq_uploader, ""dataset"", ""run_table"", ""run_status_table"",\n        ""metric_table"", ""run_id"")\n\n  def tearDown(self):\n    super(BenchmarkBigQueryLoggerTest, self).tearDown()\n    tf.io.gfile.rmtree(self.get_temp_dir())\n    os.environ.clear()\n    os.environ.update(self.original_environ)\n\n  def test_log_metric(self):\n    self.logger.log_metric(\n        ""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n    expected_metric_json = [{\n        ""name"": ""accuracy"",\n        ""value"": 0.999,\n        ""unit"": None,\n        ""global_step"": 1e4,\n        ""timestamp"": mock.ANY,\n        ""extras"": [{""name"": ""name"", ""value"": ""value""}]\n    }]\n    # log_metric will call upload_benchmark_metric_json in a separate thread.\n    # Give it some grace period for the new thread before assert.\n    time.sleep(1)\n    self.mock_bq_uploader.upload_benchmark_metric_json.assert_called_once_with(\n        ""dataset"", ""metric_table"", ""run_id"", expected_metric_json)\n\n  @mock.patch(""official.utils.logs.logger._gather_run_info"")\n  def test_log_run_info(self, mock_gather_run_info):\n    run_info = {""model_name"": ""model_name"",\n                ""dataset"": ""dataset_name"",\n                ""run_info"": ""run_value""}\n    mock_gather_run_info.return_value = run_info\n    self.logger.log_run_info(""model_name"", ""dataset_name"", {})\n    # log_metric will call upload_benchmark_metric_json in a separate thread.\n    # Give it some grace period for the new thread before assert.\n    time.sleep(1)\n    self.mock_bq_uploader.upload_benchmark_run_json.assert_called_once_with(\n        ""dataset"", ""run_table"", ""run_id"", run_info)\n    self.mock_bq_uploader.insert_run_status.assert_called_once_with(\n        ""dataset"", ""run_status_table"", ""run_id"", ""running"")\n\n  def test_on_finish(self):\n    self.logger.on_finish(logger.RUN_STATUS_SUCCESS)\n    # log_metric will call upload_benchmark_metric_json in a separate thread.\n    # Give it some grace period for the new thread before assert.\n    time.sleep(1)\n    self.mock_bq_uploader.update_run_status.assert_called_once_with(\n        ""dataset"", ""run_status_table"", ""run_id"", logger.RUN_STATUS_SUCCESS)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/metric_hook.py,3,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Session hook for logging benchmark metric.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\n\nclass LoggingMetricHook(tf.estimator.LoggingTensorHook):\n  """"""Hook to log benchmark metric information.\n\n  This hook is very similar as tf.train.LoggingTensorHook, which logs given\n  tensors every N local steps, every N seconds, or at the end. The metric\n  information will be logged to given log_dir or via metric_logger in JSON\n  format, which can be consumed by data analysis pipeline later.\n\n  Note that if `at_end` is True, `tensors` should not include any tensor\n  whose evaluation produces a side effect such as consuming additional inputs.\n  """"""\n\n  def __init__(self, tensors, metric_logger=None,\n               every_n_iter=None, every_n_secs=None, at_end=False):\n    """"""Initializer for LoggingMetricHook.\n\n    Args:\n      tensors: `dict` that maps string-valued tags to tensors/tensor names,\n          or `iterable` of tensors/tensor names.\n      metric_logger: instance of `BenchmarkLogger`, the benchmark logger that\n          hook should use to write the log.\n      every_n_iter: `int`, print the values of `tensors` once every N local\n          steps taken on the current worker.\n      every_n_secs: `int` or `float`, print the values of `tensors` once every N\n          seconds. Exactly one of `every_n_iter` and `every_n_secs` should be\n          provided.\n      at_end: `bool` specifying whether to print the values of `tensors` at the\n          end of the run.\n\n    Raises:\n      ValueError:\n        1. `every_n_iter` is non-positive, or\n        2. Exactly one of every_n_iter and every_n_secs should be provided.\n        3. Exactly one of log_dir and metric_logger should be provided.\n    """"""\n    super(LoggingMetricHook, self).__init__(\n        tensors=tensors,\n        every_n_iter=every_n_iter,\n        every_n_secs=every_n_secs,\n        at_end=at_end)\n\n    if metric_logger is None:\n      raise ValueError(""metric_logger should be provided."")\n    self._logger = metric_logger\n\n  def begin(self):\n    super(LoggingMetricHook, self).begin()\n    self._global_step_tensor = tf.compat.v1.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          ""Global step should be created to use LoggingMetricHook."")\n    if self._global_step_tensor.name not in self._current_tensors:\n      self._current_tensors[self._global_step_tensor.name] = (\n          self._global_step_tensor)\n\n  def after_run(self, unused_run_context, run_values):\n    # should_trigger is a internal state that populated at before_run, and it is\n    # using self_timer to determine whether it should trigger.\n    if self._should_trigger:\n      self._log_metric(run_values.results)\n\n    self._iter_count += 1\n\n  def end(self, session):\n    if self._log_at_end:\n      values = session.run(self._current_tensors)\n      self._log_metric(values)\n\n  def _log_metric(self, tensor_values):\n    self._timer.update_last_triggered_step(self._iter_count)\n    global_step = tensor_values[self._global_step_tensor.name]\n    # self._tag_order is populated during the init of LoggingTensorHook\n    for tag in self._tag_order:\n      self._logger.log_metric(tag, tensor_values[tag], global_step=global_step)\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/metric_hook_test.py,30,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for metric_hook.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tempfile\nimport time\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\nfrom tensorflow.python.training import monitored_session  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import metric_hook\nfrom official.utils.testing import mock_lib\n\n\nclass LoggingMetricHookTest(tf.test.TestCase):\n  """"""Tests for LoggingMetricHook.""""""\n\n  def setUp(self):\n    super(LoggingMetricHookTest, self).setUp()\n\n    self._log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    self._logger = mock_lib.MockBenchmarkLogger()\n\n  def tearDown(self):\n    super(LoggingMetricHookTest, self).tearDown()\n    tf.io.gfile.rmtree(self.get_temp_dir())\n\n  def test_illegal_args(self):\n    with self.assertRaisesRegexp(ValueError, ""nvalid every_n_iter""):\n      metric_hook.LoggingMetricHook(tensors=[""t""], every_n_iter=0)\n    with self.assertRaisesRegexp(ValueError, ""nvalid every_n_iter""):\n      metric_hook.LoggingMetricHook(tensors=[""t""], every_n_iter=-10)\n    with self.assertRaisesRegexp(ValueError, ""xactly one of""):\n      metric_hook.LoggingMetricHook(\n          tensors=[""t""], every_n_iter=5, every_n_secs=5)\n    with self.assertRaisesRegexp(ValueError, ""xactly one of""):\n      metric_hook.LoggingMetricHook(tensors=[""t""])\n    with self.assertRaisesRegexp(ValueError, ""metric_logger""):\n      metric_hook.LoggingMetricHook(tensors=[""t""], every_n_iter=5)\n\n  def test_print_at_end_only(self):\n    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n      tf.compat.v1.train.get_or_create_global_step()\n      t = tf.constant(42.0, name=""foo"")\n      train_op = tf.constant(3)\n      hook = metric_hook.LoggingMetricHook(\n          tensors=[t.name], at_end=True, metric_logger=self._logger)\n      hook.begin()\n      mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n      sess.run(tf.compat.v1.global_variables_initializer())\n\n      for _ in range(3):\n        mon_sess.run(train_op)\n        self.assertEqual(self._logger.logged_metric, [])\n\n      hook.end(sess)\n      self.assertEqual(len(self._logger.logged_metric), 1)\n      metric = self._logger.logged_metric[0]\n      self.assertRegexpMatches(metric[""name""], ""foo"")\n      self.assertEqual(metric[""value""], 42.0)\n      self.assertEqual(metric[""unit""], None)\n      self.assertEqual(metric[""global_step""], 0)\n\n  def test_global_step_not_found(self):\n    with tf.Graph().as_default():\n      t = tf.constant(42.0, name=""foo"")\n      hook = metric_hook.LoggingMetricHook(\n          tensors=[t.name], at_end=True, metric_logger=self._logger)\n\n      with self.assertRaisesRegexp(\n          RuntimeError, ""should be created to use LoggingMetricHook.""):\n        hook.begin()\n\n  def test_log_tensors(self):\n    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n      tf.compat.v1.train.get_or_create_global_step()\n      t1 = tf.constant(42.0, name=""foo"")\n      t2 = tf.constant(43.0, name=""bar"")\n      train_op = tf.constant(3)\n      hook = metric_hook.LoggingMetricHook(\n          tensors=[t1, t2], at_end=True, metric_logger=self._logger)\n      hook.begin()\n      mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n      sess.run(tf.compat.v1.global_variables_initializer())\n\n      for _ in range(3):\n        mon_sess.run(train_op)\n        self.assertEqual(self._logger.logged_metric, [])\n\n      hook.end(sess)\n      self.assertEqual(len(self._logger.logged_metric), 2)\n      metric1 = self._logger.logged_metric[0]\n      self.assertRegexpMatches(str(metric1[""name""]), ""foo"")\n      self.assertEqual(metric1[""value""], 42.0)\n      self.assertEqual(metric1[""unit""], None)\n      self.assertEqual(metric1[""global_step""], 0)\n\n      metric2 = self._logger.logged_metric[1]\n      self.assertRegexpMatches(str(metric2[""name""]), ""bar"")\n      self.assertEqual(metric2[""value""], 43.0)\n      self.assertEqual(metric2[""unit""], None)\n      self.assertEqual(metric2[""global_step""], 0)\n\n  def _validate_print_every_n_steps(self, sess, at_end):\n    t = tf.constant(42.0, name=""foo"")\n\n    train_op = tf.constant(3)\n    hook = metric_hook.LoggingMetricHook(\n        tensors=[t.name], every_n_iter=10, at_end=at_end,\n        metric_logger=self._logger)\n    hook.begin()\n    mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n    sess.run(tf.compat.v1.global_variables_initializer())\n    mon_sess.run(train_op)\n    self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n    for _ in range(3):\n      self._logger.logged_metric = []\n      for _ in range(9):\n        mon_sess.run(train_op)\n        # assertNotRegexpMatches is not supported by python 3.1 and later\n        self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n      mon_sess.run(train_op)\n      self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n\n    # Add additional run to verify proper reset when called multiple times.\n    self._logger.logged_metric = []\n    mon_sess.run(train_op)\n    # assertNotRegexpMatches is not supported by python 3.1 and later\n    self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n\n    self._logger.logged_metric = []\n    hook.end(sess)\n    if at_end:\n      self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n    else:\n      # assertNotRegexpMatches is not supported by python 3.1 and later\n      self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n\n  def test_print_every_n_steps(self):\n    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n      tf.compat.v1.train.get_or_create_global_step()\n      self._validate_print_every_n_steps(sess, at_end=False)\n      # Verify proper reset.\n      self._validate_print_every_n_steps(sess, at_end=False)\n\n  def test_print_every_n_steps_and_end(self):\n    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n      tf.compat.v1.train.get_or_create_global_step()\n      self._validate_print_every_n_steps(sess, at_end=True)\n      # Verify proper reset.\n      self._validate_print_every_n_steps(sess, at_end=True)\n\n  def _validate_print_every_n_secs(self, sess, at_end):\n    t = tf.constant(42.0, name=""foo"")\n    train_op = tf.constant(3)\n\n    hook = metric_hook.LoggingMetricHook(\n        tensors=[t.name], every_n_secs=1.0, at_end=at_end,\n        metric_logger=self._logger)\n    hook.begin()\n    mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    mon_sess.run(train_op)\n    self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n\n    # assertNotRegexpMatches is not supported by python 3.1 and later\n    self._logger.logged_metric = []\n    mon_sess.run(train_op)\n    self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n    time.sleep(1.0)\n\n    self._logger.logged_metric = []\n    mon_sess.run(train_op)\n    self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n\n    self._logger.logged_metric = []\n    hook.end(sess)\n    if at_end:\n      self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n    else:\n      # assertNotRegexpMatches is not supported by python 3.1 and later\n      self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n\n  def test_print_every_n_secs(self):\n    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n      tf.compat.v1.train.get_or_create_global_step()\n      self._validate_print_every_n_secs(sess, at_end=False)\n      # Verify proper reset.\n      self._validate_print_every_n_secs(sess, at_end=False)\n\n  def test_print_every_n_secs_and_end(self):\n    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n      tf.compat.v1.train.get_or_create_global_step()\n      self._validate_print_every_n_secs(sess, at_end=True)\n      # Verify proper reset.\n      self._validate_print_every_n_secs(sess, at_end=True)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
models/language_translation/tensorflow/transformer_lt_official/inference/fp32/official/utils/logs/mlperf_helper.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Wrapper for the mlperf logging utils.\n\nMLPerf compliance logging is only desired under a limited set of circumstances.\nThis module is intended to keep users from needing to consider logging (or\ninstall the module) unless they are performing mlperf runs.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport typing\nimport shlex\n\nimport tensorflow as tf\n\n_MIN_VERSION = (0, 0, 10)\n_STACK_OFFSET = 2\n\nSUDO = ""sudo"" if os.geteuid() else """"\n\n# This indirection is used in docker.\nDROP_CACHE_LOC = os.getenv(""DROP_CACHE_LOC"", ""/proc/sys/vm/drop_caches"")\n\n_NCF_PREFIX = ""NCF_RAW_""\n\n# TODO(robieta): move line parsing to mlperf util\n_PREFIX = r""(?:{})?:::MLPv([0-9]+).([0-9]+).([0-9]+)"".format(_NCF_PREFIX)\n_BENCHMARK = r""([a-zA-Z0-9_]+)""\n_TIMESTAMP = r""([0-9]+\\.[0-9]+)""\n_CALLSITE = r""\\((.+):([0-9]+)\\)""\n_TAG = r""([a-zA-Z0-9_]+)""\n_VALUE = r""(.*)""\n\nParsedLine = namedtuple(""ParsedLine"", [""version"", ""benchmark"", ""timestamp"",\n                                       ""callsite"", ""tag"", ""value""])\n\nLINE_PATTERN = re.compile(\n    ""^{prefix} {benchmark} {timestamp} {callsite} {tag}(: |$){value}?$"".format(\n        prefix=_PREFIX, benchmark=_BENCHMARK, timestamp=_TIMESTAMP,\n        callsite=_CALLSITE, tag=_TAG, value=_VALUE))\n\n\ndef parse_line(line): # type: (str) -> typing.Optional[ParsedLine]\n  match = LINE_PATTERN.match(line.strip())\n  if not match:\n    return\n\n  major, minor, micro, benchmark, timestamp = match.groups()[:5]\n  call_file, call_line, tag, _, value = match.groups()[5:]\n\n  return ParsedLine(version=(int(major), int(minor), int(micro)),\n                    benchmark=benchmark, timestamp=timestamp,\n                    callsite=(call_file, call_line), tag=tag, value=value)\n\n\ndef unparse_line(parsed_line): # type: (ParsedLine) -> str\n  version_str = ""{}.{}.{}"".format(*parsed_line.version)\n  callsite_str = ""({}:{})"".format(*parsed_line.callsite)\n  value_str = "": {}"".format(parsed_line.value) if parsed_line.value else """"\n  return "":::MLPv{} {} {} {} {} {}"".format(\n      version_str, parsed_line.benchmark, parsed_line.timestamp, callsite_str,\n      parsed_line.tag, value_str)\n\n\ndef get_mlperf_log():\n  """"""Shielded import of mlperf_log module.""""""\n  try:\n    import mlperf_compliance\n\n    def test_mlperf_log_pip_version():\n      """"""Check that mlperf_compliance is up to date.""""""\n      import pkg_resources\n      version = pkg_resources.get_distribution(""mlperf_compliance"")\n      version = tuple(int(i) for i in version.version.split("".""))\n      if version < _MIN_VERSION:\n        tf.compat.v1.logging.warning(\n            ""mlperf_compliance is version {}, must be >= {}"".format(\n                ""."".join([str(i) for i in version]),\n                ""."".join([str(i) for i in _MIN_VERSION])))\n        raise ImportError\n      return mlperf_compliance.mlperf_log\n\n    mlperf_log = test_mlperf_log_pip_version()\n\n  except ImportError:\n    mlperf_log = None\n\n  return mlperf_log\n\n\nclass Logger(object):\n  """"""MLPerf logger indirection class.\n\n  This logger only logs for MLPerf runs, and prevents various errors associated\n  with not having the mlperf_compliance package installed.\n  """"""\n  class Tags(object):\n    def __init__(self, mlperf_log):\n      self._enabled = False\n      self._mlperf_log = mlperf_log\n\n    def __getattr__(self, item):\n      if self._mlperf_log is None or not self._enabled:\n        return\n      return getattr(self._mlperf_log, item)\n\n  def __init__(self):\n    self._enabled = False\n    self._mlperf_log = get_mlperf_log()\n    self.tags = self.Tags(self._mlperf_log)\n\n  def __call__(self, enable=False):\n    if enable and self._mlperf_log is None:\n      raise ImportError(""MLPerf logging was requested, but mlperf_compliance ""\n                        ""module could not be loaded."")\n\n    self._enabled = enable\n    self.tags._enabled = enable\n    return self\n\n  def __enter__(self):\n    pass\n\n  def __exit__(self, exc_type, exc_val, exc_tb):\n    self._enabled = False\n    self.tags._enabled = False\n\n  @property\n  def log_file(self):\n    if self._mlperf_log is None:\n      return\n    return self._mlperf_log.LOG_FILE\n\n  @property\n  def enabled(self):\n    return self._enabled\n\n  def ncf_print(self, key, value=None, stack_offset=_STACK_OFFSET,\n                deferred=False, extra_print=False, prefix=_NCF_PREFIX):\n    if self._mlperf_log is None or not self.enabled:\n      return\n    self._mlperf_log.ncf_print(key=key, value=value, stack_offset=stack_offset,\n                               deferred=deferred, extra_print=extra_print,\n                               prefix=prefix)\n\n  def set_ncf_root(self, path):\n    if self._mlperf_log is None:\n      return\n    self._mlperf_log.ROOT_DIR_NCF = path\n\n\nLOGGER = Logger()\nncf_print, set_ncf_root = LOGGER.ncf_print, LOGGER.set_ncf_root\nTAGS = LOGGER.tags\n\n\ndef clear_system_caches():\n  if not LOGGER.enabled:\n    return\n  ret_code = subprocess.call(\n      [shlex.split(""sync && echo 3 | {} tee {}"".format(SUDO, DROP_CACHE_LOC))],\n      shell=False)\n\n  if ret_code:\n    raise ValueError(""Failed to clear caches"")\n\n\nif __name__ == ""__main__"":\n  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n  with LOGGER(True):\n    ncf_print(key=TAGS.RUN_START)\n'"
