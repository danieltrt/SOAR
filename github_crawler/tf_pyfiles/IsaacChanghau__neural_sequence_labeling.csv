file_path,api_count,code
train_conll_chunk_blstm_cnn_crf.py,46,"b'import tensorflow as tf\nimport os\nfrom utils.conll2003_prepro import process_data\nfrom models.blstm_cnn_crf_model import SequenceLabelModel\nfrom utils import batchnize_dataset\n\n# dataset parameters\ntf.flags.DEFINE_string(""task_name"", ""chunk"", ""task name"")\ntf.flags.DEFINE_string(""language"", ""english"", ""language"")  # used for inference, indicated the source language\ntf.flags.DEFINE_string(""raw_path"", ""data/raw/conll2003/raw"", ""path to raw dataset"")\ntf.flags.DEFINE_string(""save_path"", ""data/dataset/conll2003/chunk"", ""path to save dataset"")\ntf.flags.DEFINE_string(""glove_name"", ""6B"", ""glove embedding name"")\ntf.flags.DEFINE_boolean(""char_lowercase"", True, ""char lowercase"")\n# glove embedding path\nglove_path = os.path.join(os.path.expanduser(\'~\'), ""utilities"", ""embeddings"", ""glove.{}.{}d.txt"")\ntf.flags.DEFINE_string(""glove_path"", glove_path, ""glove embedding path"")\n\n# dataset for train, validate and test\ntf.flags.DEFINE_string(""vocab"", ""data/dataset/conll2003/chunk/vocab.json"", ""path to the word and tag vocabularies"")\ntf.flags.DEFINE_string(""train_set"", ""data/dataset/conll2003/chunk/train.json"", ""path to the training datasets"")\ntf.flags.DEFINE_string(""dev_set"", ""data/dataset/conll2003/chunk/dev.json"", ""path to the development datasets"")\ntf.flags.DEFINE_string(""test_set"", ""data/dataset/conll2003/chunk/test.json"", ""path to the test datasets"")\ntf.flags.DEFINE_string(""pretrained_emb"", ""data/dataset/conll2003/chunk/glove_emb.npz"", ""pretrained embeddings"")\n\n# network parameters\ntf.flags.DEFINE_string(""cell_type"", ""lstm"", ""RNN cell for encoder and decoder: [lstm | gru], default: lstm"")\ntf.flags.DEFINE_integer(""num_units"", 300, ""number of hidden units for rnn cell"")\ntf.flags.DEFINE_integer(""num_layers"", None, ""number of rnn layers"")\ntf.flags.DEFINE_boolean(""use_stack_rnn"", False, ""True: use stacked rnn, False: use normal rnn (used for layers > 1)"")\ntf.flags.DEFINE_boolean(""use_pretrained"", True, ""use pretrained word embedding"")\ntf.flags.DEFINE_boolean(""tuning_emb"", False, ""tune pretrained word embedding while training"")\ntf.flags.DEFINE_integer(""emb_dim"", 300, ""embedding dimension for encoder and decoder input words/tokens"")\ntf.flags.DEFINE_boolean(""use_chars"", True, ""use char embeddings"")\ntf.flags.DEFINE_boolean(""use_residual"", False, ""use residual connection"")\ntf.flags.DEFINE_boolean(""use_layer_norm"", False, ""use layer normalization"")\ntf.flags.DEFINE_integer(""char_emb_dim"", 100, ""character embedding dimension"")\ntf.flags.DEFINE_boolean(""use_highway"", True, ""use highway network"")\ntf.flags.DEFINE_integer(""highway_layers"", 2, ""number of layers for highway network"")\ntf.flags.DEFINE_multi_integer(""filter_sizes"", [100, 100], ""filter size"")\ntf.flags.DEFINE_multi_integer(""channel_sizes"", [5, 5], ""channel size"")\ntf.flags.DEFINE_boolean(""use_crf"", True, ""use CRF decoder"")\n# attention mechanism (normal attention is Lurong/Bahdanau liked attention mechanism)\ntf.flags.DEFINE_string(""use_attention"", None, ""use attention mechanism: [None | self_attention | normal_attention]"")\n# Params for self attention (multi-head)\ntf.flags.DEFINE_integer(""attention_size"", None, ""attention size for multi-head attention mechanism"")\ntf.flags.DEFINE_integer(""num_heads"", 8, ""number of heads"")\n\n# training parameters\ntf.flags.DEFINE_float(""lr"", 0.001, ""learning rate"")\ntf.flags.DEFINE_string(""optimizer"", ""adam"", ""optimizer: [adagrad | sgd | rmsprop | adadelta | adam], default: adam"")\ntf.flags.DEFINE_boolean(""use_lr_decay"", True, ""apply learning rate decay for each epoch"")\ntf.flags.DEFINE_float(""lr_decay"", 0.05, ""learning rate decay factor"")\ntf.flags.DEFINE_float(""minimal_lr"", 1e-5, ""minimal learning rate"")\ntf.flags.DEFINE_float(""grad_clip"", 5.0, ""maximal gradient norm"")\ntf.flags.DEFINE_float(""keep_prob"", 0.5, ""dropout keep probability for embedding while training"")\ntf.flags.DEFINE_integer(""batch_size"", 20, ""batch size"")\ntf.flags.DEFINE_integer(""epochs"", 100, ""train epochs"")\ntf.flags.DEFINE_integer(""max_to_keep"", 5, ""maximum trained models to be saved"")\ntf.flags.DEFINE_integer(""no_imprv_tolerance"", 5, ""no improvement tolerance"")\ntf.flags.DEFINE_string(""checkpoint_path"", ""ckpt/conll2003_chunk/"", ""path to save models checkpoints"")\ntf.flags.DEFINE_string(""summary_path"", ""ckpt/conll2003_chunk/summary/"", ""path to save summaries"")\ntf.flags.DEFINE_string(""model_name"", ""chunk_blstm_cnn_crf_model"", ""models name"")\n\n# convert parameters to dict\nconfig = tf.flags.FLAGS.flag_values_dict()\n\n# create dataset from raw data files\nif not os.path.exists(config[""save_path""]) or not os.listdir(config[""save_path""]):\n    process_data(config)\nif not os.path.exists(config[""pretrained_emb""]) and config[""use_pretrained""]:\n    process_data(config)\n\nprint(""Load datasets..."")\n# used for training\ntrain_set = batchnize_dataset(config[""train_set""], config[""batch_size""], shuffle=True)\n# used for computing validate loss\nvalid_data = batchnize_dataset(config[""dev_set""], batch_size=1000, shuffle=True)[0]\n# used for computing validate accuracy, precision, recall and F1 scores\nvalid_set = batchnize_dataset(config[""dev_set""], config[""batch_size""], shuffle=False)\n# used for computing test accuracy, precision, recall and F1 scores\ntest_set = batchnize_dataset(config[""test_set""], config[""batch_size""], shuffle=False)\n\nprint(""Build models..."")\nmodel = SequenceLabelModel(config)\nmodel.train(train_set, valid_data, valid_set, test_set)\n\nprint(""Inference..."")\nsentences = [""EU rejects German call to boycott British lamb .""]\nground_truths = [""B-NP B-VP    B-NP   I-NP B-VP I-VP    B-NP    I-NP O ""]\nfor sentence, truth in zip(sentences, ground_truths):\n    result = model.inference(sentence)\n    print(result)\n    print(""Ground truth:\\n{}\\n"".format(truth))\n'"
train_conll_ner_blstm_cnn_crf.py,46,"b'import tensorflow as tf\nimport os\nfrom utils.conll2003_prepro import process_data\nfrom models.blstm_cnn_crf_model import SequenceLabelModel\nfrom utils import batchnize_dataset\n\n# dataset parameters\ntf.flags.DEFINE_string(""task_name"", ""ner"", ""task name"")\ntf.flags.DEFINE_string(""language"", ""english"", ""language"")  # used for inference, indicated the source language\ntf.flags.DEFINE_string(""raw_path"", ""data/raw/conll2003/raw"", ""path to raw dataset"")\ntf.flags.DEFINE_string(""save_path"", ""data/dataset/conll2003/ner"", ""path to save dataset"")\ntf.flags.DEFINE_string(""glove_name"", ""6B"", ""glove embedding name"")\ntf.flags.DEFINE_boolean(""char_lowercase"", False, ""char lowercase"")\n# glove embedding path\nglove_path = os.path.join(os.path.expanduser(\'~\'), ""utilities"", ""embeddings"", ""glove.{}.{}d.txt"")\ntf.flags.DEFINE_string(""glove_path"", glove_path, ""glove embedding path"")\n\n# dataset for train, validate and test\ntf.flags.DEFINE_string(""vocab"", ""data/dataset/conll2003/ner/vocab.json"", ""path to the word and tag vocabularies"")\ntf.flags.DEFINE_string(""train_set"", ""data/dataset/conll2003/ner/train.json"", ""path to the training datasets"")\ntf.flags.DEFINE_string(""dev_set"", ""data/dataset/conll2003/ner/dev.json"", ""path to the development datasets"")\ntf.flags.DEFINE_string(""test_set"", ""data/dataset/conll2003/ner/test.json"", ""path to the test datasets"")\ntf.flags.DEFINE_string(""pretrained_emb"", ""data/dataset/conll2003/ner/glove_emb.npz"", ""pretrained embeddings"")\n\n# network parameters\ntf.flags.DEFINE_string(""cell_type"", ""lstm"", ""RNN cell for encoder and decoder: [lstm | gru], default: lstm"")\ntf.flags.DEFINE_integer(""num_units"", 300, ""number of hidden units for rnn cell"")\ntf.flags.DEFINE_integer(""num_layers"", None, ""number of rnn layers"")\ntf.flags.DEFINE_boolean(""use_stack_rnn"", False, ""True: use stacked rnn, False: use normal rnn (used for layers > 1)"")\ntf.flags.DEFINE_boolean(""use_pretrained"", True, ""use pretrained word embedding"")\ntf.flags.DEFINE_boolean(""tuning_emb"", False, ""tune pretrained word embedding while training"")\ntf.flags.DEFINE_integer(""emb_dim"", 300, ""embedding dimension for encoder and decoder input words/tokens"")\ntf.flags.DEFINE_boolean(""use_chars"", True, ""use char embeddings"")\ntf.flags.DEFINE_boolean(""use_residual"", False, ""use residual connection"")\ntf.flags.DEFINE_boolean(""use_layer_norm"", False, ""use layer normalization"")\ntf.flags.DEFINE_integer(""char_emb_dim"", 100, ""character embedding dimension"")\ntf.flags.DEFINE_boolean(""use_highway"", True, ""use highway network"")\ntf.flags.DEFINE_integer(""highway_layers"", 2, ""number of layers for highway network"")\ntf.flags.DEFINE_multi_integer(""filter_sizes"", [100, 100], ""filter size"")\ntf.flags.DEFINE_multi_integer(""channel_sizes"", [5, 5], ""channel size"")\ntf.flags.DEFINE_boolean(""use_crf"", True, ""use CRF decoder"")\n# attention mechanism (normal attention is Lurong/Bahdanau liked attention mechanism)\ntf.flags.DEFINE_string(""use_attention"", None, ""use attention mechanism: [None | self_attention | normal_attention]"")\n# Params for self attention (multi-head)\ntf.flags.DEFINE_integer(""attention_size"", None, ""attention size for multi-head attention mechanism"")\ntf.flags.DEFINE_integer(""num_heads"", 8, ""number of heads"")\n\n# training parameters\ntf.flags.DEFINE_float(""lr"", 0.001, ""learning rate"")\ntf.flags.DEFINE_string(""optimizer"", ""adam"", ""optimizer: [adagrad | sgd | rmsprop | adadelta | adam], default: adam"")\ntf.flags.DEFINE_boolean(""use_lr_decay"", True, ""apply learning rate decay for each epoch"")\ntf.flags.DEFINE_float(""lr_decay"", 0.05, ""learning rate decay factor"")\ntf.flags.DEFINE_float(""minimal_lr"", 1e-5, ""minimal learning rate"")\ntf.flags.DEFINE_float(""grad_clip"", 5.0, ""maximal gradient norm"")\ntf.flags.DEFINE_float(""keep_prob"", 0.5, ""dropout keep probability for embedding while training"")\ntf.flags.DEFINE_integer(""batch_size"", 20, ""batch size"")\ntf.flags.DEFINE_integer(""epochs"", 100, ""train epochs"")\ntf.flags.DEFINE_integer(""max_to_keep"", 5, ""maximum trained models to be saved"")\ntf.flags.DEFINE_integer(""no_imprv_tolerance"", 5, ""no improvement tolerance"")\ntf.flags.DEFINE_string(""checkpoint_path"", ""ckpt/conll2003_ner/"", ""path to save models checkpoints"")\ntf.flags.DEFINE_string(""summary_path"", ""ckpt/conll2003_ner/summary/"", ""path to save summaries"")\ntf.flags.DEFINE_string(""model_name"", ""ner_blstm_cnn_crf_model"", ""models name"")\n\n# convert parameters to dict\nconfig = tf.flags.FLAGS.flag_values_dict()\n\n# create dataset from raw data files\nif not os.path.exists(config[""save_path""]) or not os.listdir(config[""save_path""]):\n    process_data(config)\nif not os.path.exists(config[""pretrained_emb""]) and config[""use_pretrained""]:\n    process_data(config)\n\nprint(""Load datasets..."")\n# used for training\ntrain_set = batchnize_dataset(config[""train_set""], config[""batch_size""], shuffle=True)\n# used for computing validate loss\nvalid_data = batchnize_dataset(config[""dev_set""], batch_size=1000, shuffle=True)[0]\n# used for computing validate accuracy, precision, recall and F1 scores\nvalid_set = batchnize_dataset(config[""dev_set""], config[""batch_size""], shuffle=False)\n# used for computing test accuracy, precision, recall and F1 scores\ntest_set = batchnize_dataset(config[""test_set""], config[""batch_size""], shuffle=False)\n\nprint(""Build models..."")\nmodel = SequenceLabelModel(config)\nmodel.train(train_set, valid_data, valid_set, test_set)\n\nprint(""Inference..."")\nsentences = [""Stanford University located at California .""]\nground_truths = [""B-ORG    I-ORG      O       O  B-LOC      O""]\nfor sentence, truth in zip(sentences, ground_truths):\n    result = model.inference(sentence)\n    print(result)\n    print(""Ground truth:\\n{}\\n"".format(truth))\n'"
train_conll_pos_blstm_cnn_crf.py,46,"b'import tensorflow as tf\nimport os\nfrom utils.conll2003_prepro import process_data\nfrom models.blstm_cnn_crf_model import SequenceLabelModel\nfrom utils import batchnize_dataset\n\n# dataset parameters\ntf.flags.DEFINE_string(""task_name"", ""pos"", ""task name"")\ntf.flags.DEFINE_string(""language"", ""english"", ""language"")  # used for inference, indicated the source language\ntf.flags.DEFINE_string(""raw_path"", ""data/raw/conll2003/raw"", ""path to raw dataset"")\ntf.flags.DEFINE_string(""save_path"", ""data/dataset/conll2003/pos"", ""path to save dataset"")\ntf.flags.DEFINE_string(""glove_name"", ""6B"", ""glove embedding name"")\ntf.flags.DEFINE_boolean(""char_lowercase"", True, ""char lowercase"")\n# glove embedding path\nglove_path = os.path.join(os.path.expanduser(\'~\'), ""utilities"", ""embeddings"", ""glove.{}.{}d.txt"")\ntf.flags.DEFINE_string(""glove_path"", glove_path, ""glove embedding path"")\n\n# dataset for train, validate and test\ntf.flags.DEFINE_string(""vocab"", ""data/dataset/conll2003/pos/vocab.json"", ""path to the word and tag vocabularies"")\ntf.flags.DEFINE_string(""train_set"", ""data/dataset/conll2003/pos/train.json"", ""path to the training datasets"")\ntf.flags.DEFINE_string(""dev_set"", ""data/dataset/conll2003/pos/dev.json"", ""path to the development datasets"")\ntf.flags.DEFINE_string(""test_set"", ""data/dataset/conll2003/pos/test.json"", ""path to the test datasets"")\ntf.flags.DEFINE_string(""pretrained_emb"", ""data/dataset/conll2003/pos/glove_emb.npz"", ""pretrained embeddings"")\n\n# network parameters\ntf.flags.DEFINE_string(""cell_type"", ""lstm"", ""RNN cell for encoder and decoder: [lstm | gru], default: lstm"")\ntf.flags.DEFINE_integer(""num_units"", 300, ""number of hidden units for rnn cell"")\ntf.flags.DEFINE_integer(""num_layers"", None, ""number of rnn layers"")\ntf.flags.DEFINE_boolean(""use_stack_rnn"", False, ""True: use stacked rnn, False: use normal rnn (used for layers > 1)"")\ntf.flags.DEFINE_boolean(""use_pretrained"", True, ""use pretrained word embedding"")\ntf.flags.DEFINE_boolean(""tuning_emb"", False, ""tune pretrained word embedding while training"")\ntf.flags.DEFINE_integer(""emb_dim"", 300, ""embedding dimension for encoder and decoder input words/tokens"")\ntf.flags.DEFINE_boolean(""use_chars"", True, ""use char embeddings"")\ntf.flags.DEFINE_boolean(""use_residual"", False, ""use residual connection"")\ntf.flags.DEFINE_boolean(""use_layer_norm"", False, ""use layer normalization"")\ntf.flags.DEFINE_integer(""char_emb_dim"", 100, ""character embedding dimension"")\ntf.flags.DEFINE_boolean(""use_highway"", True, ""use highway network"")\ntf.flags.DEFINE_integer(""highway_layers"", 2, ""number of layers for highway network"")\ntf.flags.DEFINE_multi_integer(""filter_sizes"", [100, 100], ""filter size"")\ntf.flags.DEFINE_multi_integer(""channel_sizes"", [5, 5], ""channel size"")\ntf.flags.DEFINE_boolean(""use_crf"", True, ""use CRF decoder"")\n# attention mechanism (normal attention is Lurong/Bahdanau liked attention mechanism)\ntf.flags.DEFINE_string(""use_attention"", None, ""use attention mechanism: [None | self_attention | normal_attention]"")\n# Params for self attention (multi-head)\ntf.flags.DEFINE_integer(""attention_size"", None, ""attention size for multi-head attention mechanism"")\ntf.flags.DEFINE_integer(""num_heads"", 8, ""number of heads"")\n\n# training parameters\ntf.flags.DEFINE_float(""lr"", 0.001, ""learning rate"")\ntf.flags.DEFINE_string(""optimizer"", ""adam"", ""optimizer: [adagrad | sgd | rmsprop | adadelta | adam], default: adam"")\ntf.flags.DEFINE_boolean(""use_lr_decay"", True, ""apply learning rate decay for each epoch"")\ntf.flags.DEFINE_float(""lr_decay"", 0.05, ""learning rate decay factor"")\ntf.flags.DEFINE_float(""minimal_lr"", 1e-5, ""minimal learning rate"")\ntf.flags.DEFINE_float(""grad_clip"", 5.0, ""maximal gradient norm"")\ntf.flags.DEFINE_float(""keep_prob"", 0.5, ""dropout keep probability for embedding while training"")\ntf.flags.DEFINE_integer(""batch_size"", 20, ""batch size"")\ntf.flags.DEFINE_integer(""epochs"", 100, ""train epochs"")\ntf.flags.DEFINE_integer(""max_to_keep"", 5, ""maximum trained models to be saved"")\ntf.flags.DEFINE_integer(""no_imprv_tolerance"", 5, ""no improvement tolerance"")\ntf.flags.DEFINE_string(""checkpoint_path"", ""ckpt/conll2003_pos/"", ""path to save models checkpoints"")\ntf.flags.DEFINE_string(""summary_path"", ""ckpt/conll2003_pos/summary/"", ""path to save summaries"")\ntf.flags.DEFINE_string(""model_name"", ""pos_blstm_cnn_crf_model"", ""models name"")\n\n# convert parameters to dict\nconfig = tf.flags.FLAGS.flag_values_dict()\n\n# create dataset from raw data files\nif not os.path.exists(config[""save_path""]) or not os.listdir(config[""save_path""]):\n    process_data(config)\nif not os.path.exists(config[""pretrained_emb""]) and config[""use_pretrained""]:\n    process_data(config)\n\nprint(""Load datasets..."")\n# used for training\ntrain_set = batchnize_dataset(config[""train_set""], config[""batch_size""], shuffle=True)\n# used for computing validate loss\nvalid_data = batchnize_dataset(config[""dev_set""], batch_size=1000, shuffle=True)[0]\n# used for computing validate accuracy, precision, recall and F1 scores\nvalid_set = batchnize_dataset(config[""dev_set""], config[""batch_size""], shuffle=False)\n# used for computing test accuracy, precision, recall and F1 scores\ntest_set = batchnize_dataset(config[""test_set""], config[""batch_size""], shuffle=False)\n\nprint(""Build models..."")\nmodel = SequenceLabelModel(config)\nmodel.train(train_set, valid_data, valid_set, test_set)\n\nprint(""Inference..."")\nsentences = [""EU rejects German call to boycott British lamb .""]\nground_truths = [""NNP VBZ     JJ     NN   TO VB      JJ      NN   .""]\nfor sentence, truth in zip(sentences, ground_truths):\n    result = model.inference(sentence)\n    print(result)\n    print(""Ground truth:\\n{}\\n"".format(truth))\n'"
train_media_multi_attention.py,38,"b'import tensorflow as tf\nimport os\nfrom utils.media_prepro import process_data\nfrom models.multi_attention_model import SequenceLabelModel\nfrom utils import batchnize_dataset, load_dataset\n\n# dataset parameters\ntf.flags.DEFINE_string(""raw_path"", ""data/raw/media"", ""path to raw dataset"")\ntf.flags.DEFINE_string(""save_path"", ""data/dataset/media"", ""path to save dataset"")\ntf.flags.DEFINE_string(""language"", ""french"", ""language"")\n\n# dataset for train, validate and test\ntf.flags.DEFINE_string(""vocab"", ""data/dataset/media/vocab.json"", ""path to the word and tag vocabularies"")\ntf.flags.DEFINE_string(""train_set"", ""data/dataset/media/train.json"", ""path to the training datasets"")\ntf.flags.DEFINE_string(""dev_set"", ""data/dataset/media/dev.json"", ""path to the development datasets"")\ntf.flags.DEFINE_string(""test_set"", ""data/dataset/media/test.json"", ""path to the test datasets"")\n\n# network parameters\ntf.flags.DEFINE_string(""cell_type"", ""lstm"", ""RNN cell for encoder and decoder: [lstm | gru], default: lstm"")\ntf.flags.DEFINE_integer(""num_units"", 128, ""number of hidden units in each layer"")\ntf.flags.DEFINE_integer(""num_layers"", 2, ""number of layers for rnns"")\ntf.flags.DEFINE_integer(""emb_dim"", 200, ""embedding dimension for encoder and decoder input words/tokens"")\ntf.flags.DEFINE_boolean(""use_dropout"", True, ""use dropout for rnn cells"")\ntf.flags.DEFINE_boolean(""use_residual"", True, ""use residual connection for rnn cells"")\ntf.flags.DEFINE_integer(""attention_size"", None, ""attention size for multi-head attention mechanism"")\ntf.flags.DEFINE_integer(""num_heads"", 8, ""number of heads"")\ntf.flags.DEFINE_boolean(""use_chars"", True, ""use char embeddings"")\ntf.flags.DEFINE_integer(""char_emb_dim"", 50, ""character embedding dimension"")\ntf.flags.DEFINE_multi_integer(""filter_sizes"", [25, 25, 25, 25], ""filter size"")\ntf.flags.DEFINE_multi_integer(""channel_sizes"", [5, 5, 5, 5], ""channel size"")\ntf.flags.DEFINE_boolean(""add_positional_emb"", False, ""add positional embedding"")\ntf.flags.DEFINE_boolean(""use_crf"", True, ""use CRF decode"")\n\n# training parameters\ntf.flags.DEFINE_float(""lr"", 0.001, ""learning rate"")\ntf.flags.DEFINE_string(""optimizer"", ""adam"", ""Optimizer: [adagrad | sgd | rmsprop | adadelta | adam], default: adam"")\ntf.flags.DEFINE_boolean(""use_lr_decay"", True, ""apply learning rate decay for each epoch"")\ntf.flags.DEFINE_float(""lr_decay"", 0.9, ""learning rate decay factor"")\ntf.flags.DEFINE_float(""minimal_lr"", 1e-4, ""minimal learning rate"")\ntf.flags.DEFINE_float(""grad_clip"", 1.0, ""maximal gradient norm"")\ntf.flags.DEFINE_float(""emb_keep_prob"", 0.7, ""dropout keep probability for embedding while training"")\ntf.flags.DEFINE_float(""rnn_keep_prob"", 0.6, ""dropout keep probability for RNN while training"")\ntf.flags.DEFINE_float(""attn_keep_prob"", 0.7, ""dropout keep probability for attention while training"")\ntf.flags.DEFINE_integer(""batch_size"", 32, ""batch size"")\ntf.flags.DEFINE_integer(""epochs"", 100, ""train epochs"")\ntf.flags.DEFINE_integer(""max_to_keep"", 5, ""maximum trained models to be saved"")\ntf.flags.DEFINE_integer(""no_imprv_tolerance"", 5, ""no improvement tolerance"")\ntf.flags.DEFINE_string(""checkpoint_path"", ""ckpt/media/"", ""path to save models checkpoints"")\ntf.flags.DEFINE_string(""summary_path"", ""ckpt/media/summary/"", ""path to save summaries"")\ntf.flags.DEFINE_string(""model_name"", ""multi_attention_model"", ""models name"")\n\n# convert parameters to dict\nconfig = tf.flags.FLAGS.flag_values_dict()\n\n# create dataset from raw data files\nif not os.path.exists(config[""save_path""]) or not os.listdir(config[""save_path""]):\n    process_data(config)\n\nprint(""Load datasets..."")\ntrain_data = load_dataset(config[""train_set""])\nvalid_set = batchnize_dataset(config[""dev_set""], config[""batch_size""], shuffle=False)\ntest_set = batchnize_dataset(config[""test_set""], config[""batch_size""], shuffle=False)\nvalid_data = batchnize_dataset(config[""dev_set""], shuffle=False)\n\nprint(""Build models..."")\nmodel = SequenceLabelModel(config)\nmodel.train(train_data, valid_data, valid_set, test_set)\n\nprint(""Inference..."")\nsentences = [""alors une nuit le DIZAINE MOIS UNITE MILLE UNITE"", ""dans un h\xc3\xb4tel \xc3\xa0 XVILLE dans le centre ville""]\nground_truths = [""O B-sejour-nbNuit I-sejour-nbNuit B-temps-date I-temps-date I-temps-date B-temps-annee ""\n                 ""I-temps-annee I-temps-annee"",\n                 ""B-objetBD I-objetBD I-objetBD B-localisation-ville I-localisation-ville ""\n                 ""B-localisation-lieuRelatif-general I-localisation-lieuRelatif-general ""\n                 ""I-localisation-lieuRelatif-general I-localisation-lieuRelatif-general""]\nfor sentence, truth in zip(sentences, ground_truths):\n    result = model.inference(sentence)\n    print(result)\n    print(""Ground truth:\\n{}\\n"".format(truth))\n'"
train_punct_attentive_model.py,49,"b'import tensorflow as tf\nimport os\nfrom utils.punct_prepro import process_data\nfrom utils import batchnize_dataset\nfrom models.punct_attentive_model import SequenceLabelModel\n\n# dataset parameters\ntf.flags.DEFINE_string(""language"", ""english"", ""language"")  # used for inference, indicated the source language\ntf.flags.DEFINE_string(""raw_path"", ""data/raw/LREC_converted"", ""path to raw dataset"")\ntf.flags.DEFINE_string(""save_path"", ""data/dataset/lrec"", ""path to save dataset"")\ntf.flags.DEFINE_string(""glove_name"", ""840B"", ""glove embedding name"")\n# glove embedding path\nglove_path = os.path.join(os.path.expanduser(\'~\'), ""utilities"", ""embeddings"", ""glove.{}.{}d.txt"")\ntf.flags.DEFINE_string(""glove_path"", glove_path, ""glove embedding path"")\ntf.flags.DEFINE_integer(""max_vocab_size"", 50000, ""maximal vocabulary size"")\ntf.flags.DEFINE_integer(""max_sequence_len"", 200, ""maximal sequence length allowed"")\ntf.flags.DEFINE_integer(""min_word_count"", 1, ""minimal word count in word vocabulary"")\ntf.flags.DEFINE_integer(""min_char_count"", 10, ""minimal character count in char vocabulary"")\n\n# dataset for train, validate and test\ntf.flags.DEFINE_string(""vocab"", ""data/dataset/lrec/vocab.json"", ""path to the word and tag vocabularies"")\ntf.flags.DEFINE_string(""train_set"", ""data/dataset/lrec/train.json"", ""path to the training datasets"")\ntf.flags.DEFINE_string(""dev_set"", ""data/dataset/lrec/dev.json"", ""path to the development datasets"")\ntf.flags.DEFINE_string(""dev_text"", ""data/raw/LREC_converted/dev.txt"", ""path to the development text"")\ntf.flags.DEFINE_string(""ref_set"", ""data/dataset/lrec/ref.json"", ""path to the ref test datasets"")\ntf.flags.DEFINE_string(""ref_text"", ""data/raw/LREC_converted/ref.txt"", ""path to the ref text"")\ntf.flags.DEFINE_string(""asr_set"", ""data/dataset/lrec/asr.json"", ""path to the asr test datasets"")\ntf.flags.DEFINE_string(""asr_text"", ""data/raw/LREC_converted/asr.txt"", ""path to the asr text"")\ntf.flags.DEFINE_string(""pretrained_emb"", ""data/dataset/lrec/glove_emb.npz"", ""pretrained embeddings"")\n\n# network parameters\ntf.flags.DEFINE_string(""cell_type"", ""lstm"", ""RNN cell for encoder and decoder: [lstm | gru], default: lstm"")\ntf.flags.DEFINE_integer(""num_layers"", 4, ""number of rnn layers"")\ntf.flags.DEFINE_multi_integer(""num_units_list"", [50, 50, 50, 300], ""number of units for each rnn layer"")\ntf.flags.DEFINE_boolean(""use_pretrained"", True, ""use pretrained word embedding"")\ntf.flags.DEFINE_boolean(""tuning_emb"", False, ""tune pretrained word embedding while training"")\ntf.flags.DEFINE_integer(""emb_dim"", 300, ""embedding dimension for encoder and decoder input words/tokens"")\ntf.flags.DEFINE_boolean(""use_chars"", True, ""use char embeddings"")\ntf.flags.DEFINE_integer(""char_emb_dim"", 50, ""character embedding dimension"")\ntf.flags.DEFINE_boolean(""use_highway"", True, ""use highway network"")\ntf.flags.DEFINE_integer(""highway_layers"", 2, ""number of layers for highway network"")\ntf.flags.DEFINE_boolean(""use_crf"", True, ""use CRF decoder"")\ntf.flags.DEFINE_string(""char_represent_method"", ""rnn"", ""method to represent char embeddings: [rnn | cnn]"")\ntf.flags.DEFINE_integer(""char_num_units"", 50, ""character rnn hidden units"")\ntf.flags.DEFINE_multi_integer(""filter_sizes"", [25, 25], ""filter size"")\ntf.flags.DEFINE_multi_integer(""channel_sizes"", [5, 5], ""channel size"")\n\n# training parameters\ntf.flags.DEFINE_float(""lr"", 0.001, ""learning rate"")\ntf.flags.DEFINE_string(""optimizer"", ""adam"", ""optimizer: [adagrad | sgd | rmsprop | adadelta | adam], default: adam"")\ntf.flags.DEFINE_boolean(""use_lr_decay"", True, ""apply learning rate decay for each epoch"")\ntf.flags.DEFINE_float(""lr_decay"", 0.05, ""learning rate decay factor"")\ntf.flags.DEFINE_float(""l2_reg"", None, ""L2 norm regularization"")\ntf.flags.DEFINE_float(""minimal_lr"", 1e-5, ""minimal learning rate"")\ntf.flags.DEFINE_float(""grad_clip"", 2.0, ""maximal gradient norm"")\ntf.flags.DEFINE_float(""keep_prob"", 0.75, ""dropout keep probability for embedding while training"")\ntf.flags.DEFINE_integer(""batch_size"", 32, ""batch size"")\ntf.flags.DEFINE_integer(""epochs"", 30, ""train epochs"")\ntf.flags.DEFINE_integer(""max_to_keep"", 3, ""maximum trained models to be saved"")\ntf.flags.DEFINE_integer(""no_imprv_tolerance"", 10, ""no improvement tolerance"")\ntf.flags.DEFINE_string(""checkpoint_path"", ""ckpt/punctuator/"", ""path to save models checkpoints"")\ntf.flags.DEFINE_string(""summary_path"", ""ckpt/punctuator/summary/"", ""path to save summaries"")\ntf.flags.DEFINE_string(""model_name"", ""attentive_punctuator_model"", ""models name"")\n\n# convert parameters to dict\nconfig = tf.flags.FLAGS.flag_values_dict()\n\n# create dataset from raw data files\nif not os.path.exists(config[""save_path""]) or not os.listdir(config[""save_path""]):\n    process_data(config)\nif not os.path.exists(config[""pretrained_emb""]) and config[""use_pretrained""]:\n    process_data(config)\n\nprint(""Load datasets..."")\n# used for training\ntrain_set = batchnize_dataset(config[""train_set""], config[""batch_size""], shuffle=True)\n# used for computing validate loss\nvalid_data = batchnize_dataset(config[""dev_set""], batch_size=1000, shuffle=True)[0]\nvalid_text = config[""dev_text""]\ntest_texts = [config[""ref_text""], config[""asr_text""]]\n\nprint(""Build models..."")\nmodel = SequenceLabelModel(config)\nmodel.train(train_set, valid_data, valid_text, test_texts)\n'"
models/__init__.py,0,b'from models.base_model import *\nfrom models.nns import *\n'
models/base_model.py,34,"b'import tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib.crf import viterbi_decode, crf_log_likelihood\nfrom tensorflow.python.ops.rnn_cell import LSTMCell, GRUCell, MultiRNNCell\nfrom utils import CoNLLeval, load_dataset, get_logger, process_batch_data, align_data\nfrom utils.common import word_convert, UNK\nimport os\n\n\nclass BaseModel:\n    def __init__(self, config):\n        self.cfg = config\n        self._initialize_config()\n        self.sess, self.saver = None, None\n        self._add_placeholders()\n        self._build_embedding_op()\n        self._build_model_op()\n        self._build_loss_op()\n        self._build_train_op()\n        print(\'params number: {}\'.format(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])))\n        self.initialize_session()\n\n    def _initialize_config(self):\n        # create folders and logger\n        if not os.path.exists(self.cfg[""checkpoint_path""]):\n            os.makedirs(self.cfg[""checkpoint_path""])\n        if not os.path.exists(self.cfg[""summary_path""]):\n            os.makedirs(self.cfg[""summary_path""])\n        self.logger = get_logger(os.path.join(self.cfg[""checkpoint_path""], ""log.txt""))\n        # load dictionary\n        dict_data = load_dataset(self.cfg[""vocab""])\n        self.word_dict, self.char_dict = dict_data[""word_dict""], dict_data[""char_dict""]\n        self.tag_dict = dict_data[""tag_dict""]\n        del dict_data\n        self.word_vocab_size = len(self.word_dict)\n        self.char_vocab_size = len(self.char_dict)\n        self.tag_vocab_size = len(self.tag_dict)\n        self.rev_word_dict = dict([(idx, word) for word, idx in self.word_dict.items()])\n        self.rev_char_dict = dict([(idx, char) for char, idx in self.char_dict.items()])\n        self.rev_tag_dict = dict([(idx, tag) for tag, idx in self.tag_dict.items()])\n\n    def initialize_session(self):\n        sess_config = tf.ConfigProto()\n        sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=sess_config)\n        self.saver = tf.train.Saver(max_to_keep=self.cfg[""max_to_keep""])\n        self.sess.run(tf.global_variables_initializer())\n\n    def restore_last_session(self, ckpt_path=None):\n        if ckpt_path is not None:\n            ckpt = tf.train.get_checkpoint_state(ckpt_path)\n        else:\n            ckpt = tf.train.get_checkpoint_state(self.cfg[""checkpoint_path""])  # get checkpoint state\n        if ckpt and ckpt.model_checkpoint_path:  # restore session\n            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n\n    def save_session(self, epoch):\n        self.saver.save(self.sess, self.cfg[""checkpoint_path""] + self.cfg[""model_name""], global_step=epoch)\n\n    def close_session(self):\n        self.sess.close()\n\n    def _add_summary(self):\n        self.summary = tf.summary.merge_all()\n        self.train_writer = tf.summary.FileWriter(self.cfg[""summary_path""] + ""train"", self.sess.graph)\n        self.test_writer = tf.summary.FileWriter(self.cfg[""summary_path""] + ""test"")\n\n    def reinitialize_weights(self, scope_name=None):\n        """"""Reinitialize parameters in a scope""""""\n        if scope_name is None:\n            self.sess.run(tf.global_variables_initializer())\n        else:\n            variables = tf.contrib.framework.get_variables(scope_name)\n            self.sess.run(tf.variables_initializer(variables))\n\n    @staticmethod\n    def variable_summaries(variable, name=None):\n        with tf.name_scope(name or ""summary""):\n            mean = tf.reduce_mean(variable)\n            tf.summary.scalar(""mean"", mean)  # add mean value\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(variable - mean)))\n            tf.summary.scalar(""stddev"", stddev)  # add standard deviation value\n            tf.summary.scalar(""max"", tf.reduce_max(variable))  # add maximal value\n            tf.summary.scalar(""min"", tf.reduce_min(variable))  # add minimal value\n            tf.summary.histogram(""histogram"", variable)  # add histogram\n\n    @staticmethod\n    def viterbi_decode(logits, trans_params, seq_len):\n        viterbi_sequences = []\n        for logit, lens in zip(logits, seq_len):\n            logit = logit[:lens]  # keep only the valid steps\n            viterbi_seq, viterbi_score = viterbi_decode(logit, trans_params)\n            viterbi_sequences += [viterbi_seq]\n        return viterbi_sequences\n\n    def _create_single_rnn_cell(self, num_units):\n        cell = GRUCell(num_units) if self.cfg[""cell_type""] == ""gru"" else LSTMCell(num_units)\n        return cell\n\n    def _create_rnn_cell(self):\n        if self.cfg[""num_layers""] is None or self.cfg[""num_layers""] <= 1:\n            return self._create_single_rnn_cell(self.cfg[""num_units""])\n        else:\n            MultiRNNCell([self._create_single_rnn_cell(self.cfg[""num_units""]) for _ in range(self.cfg[""num_layers""])])\n\n    def _add_placeholders(self):\n        raise NotImplementedError(""To be implemented..."")\n\n    def _get_feed_dict(self, data):\n        raise NotImplementedError(""To be implemented..."")\n\n    def _build_embedding_op(self):\n        raise NotImplementedError(""To be implemented..."")\n\n    def _build_model_op(self):\n        raise NotImplementedError(""To be implemented..."")\n\n    def _build_loss_op(self):\n        if self.cfg[""use_crf""]:\n            crf_loss, self.trans_params = crf_log_likelihood(self.logits, self.tags, self.seq_len)\n            self.loss = tf.reduce_mean(-crf_loss)\n        else:  # using softmax\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.tags)\n            mask = tf.sequence_mask(self.seq_len)\n            self.loss = tf.reduce_mean(tf.boolean_mask(losses, mask))\n        tf.summary.scalar(""loss"", self.loss)\n\n    def _build_train_op(self):\n        with tf.variable_scope(""train_step""):\n            if self.cfg[""optimizer""] == \'adagrad\':\n                optimizer = tf.train.AdagradOptimizer(learning_rate=self.lr)\n            elif self.cfg[""optimizer""] == \'sgd\':\n                optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n            elif self.cfg[""optimizer""] == \'rmsprop\':\n                optimizer = tf.train.RMSPropOptimizer(learning_rate=self.lr)\n            elif self.cfg[""optimizer""] == \'adadelta\':\n                optimizer = tf.train.AdadeltaOptimizer(learning_rate=self.lr)\n            else:  # default adam optimizer\n                if self.cfg[""optimizer""] != \'adam\':\n                    print(\'Unsupported optimizing method {}. Using default adam optimizer.\'\n                          .format(self.cfg[""optimizer""]))\n                optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n        if self.cfg[""grad_clip""] is not None and self.cfg[""grad_clip""] > 0:\n            grads, vs = zip(*optimizer.compute_gradients(self.loss))\n            grads, _ = tf.clip_by_global_norm(grads, self.cfg[""grad_clip""])\n            self.train_op = optimizer.apply_gradients(zip(grads, vs))\n        else:\n            self.train_op = optimizer.minimize(self.loss)\n\n    def _predict_op(self, data):\n        feed_dict = self._get_feed_dict(data)\n        if self.cfg[""use_crf""]:\n            logits, trans_params, seq_len = self.sess.run([self.logits, self.trans_params, self.seq_len],\n                                                          feed_dict=feed_dict)\n            return self.viterbi_decode(logits, trans_params, seq_len)\n        else:\n            pred_logits = tf.cast(tf.argmax(self.logits, axis=-1), tf.int32)\n            logits = self.sess.run(pred_logits, feed_dict=feed_dict)\n            return logits\n\n    def train_epoch(self, train_set, valid_data, epoch):\n        raise NotImplementedError(""To be implemented..."")\n\n    def train(self, train_set, valid_data, valid_set, test_set):\n        self.logger.info(""Start training..."")\n        best_f1, no_imprv_epoch, init_lr = -np.inf, 0, self.cfg[""lr""]\n        self._add_summary()\n        for epoch in range(1, self.cfg[""epochs""] + 1):\n            self.logger.info(\'Epoch {}/{}:\'.format(epoch, self.cfg[""epochs""]))\n            self.train_epoch(train_set, valid_data, epoch)  # train epochs\n            self.evaluate(valid_set, ""dev"")\n            score = self.evaluate(test_set, ""test"")\n            if self.cfg[""use_lr_decay""]:  # learning rate decay\n                self.cfg[""lr""] = max(init_lr / (1.0 + self.cfg[""lr_decay""] * epoch), self.cfg[""minimal_lr""])\n            if score[""FB1""] > best_f1:\n                best_f1 = score[""FB1""]\n                no_imprv_epoch = 0\n                self.save_session(epoch)\n                self.logger.info(\' -- new BEST score on test dataset: {:04.2f}\'.format(best_f1))\n            else:\n                no_imprv_epoch += 1\n                if no_imprv_epoch >= self.cfg[""no_imprv_tolerance""]:\n                    self.logger.info(\'early stop at {}th epoch without improvement, BEST score on testset: {:04.2f}\'\n                                     .format(epoch, best_f1))\n                    break\n        self.train_writer.close()\n        self.test_writer.close()\n\n    def evaluate(self, dataset, name):\n        save_path = os.path.join(self.cfg[""checkpoint_path""], ""result.txt"")\n        predictions, groundtruth, words_list = list(), list(), list()\n        for data in dataset:\n            predicts = self._predict_op(data)\n            for tags, preds, words, seq_len in zip(data[""tags""], predicts, data[""words""], data[""seq_len""]):\n                tags = [self.rev_tag_dict[x] for x in tags[:seq_len]]\n                preds = [self.rev_tag_dict[x] for x in preds[:seq_len]]\n                words = [self.rev_word_dict[x] for x in words[:seq_len]]\n                predictions.append(preds)\n                groundtruth.append(tags)\n                words_list.append(words)\n        ce = CoNLLeval()\n        score = ce.conlleval(predictions, groundtruth, words_list, save_path)\n        self.logger.info(""{} dataset -- acc: {:04.2f}, pre: {:04.2f}, rec: {:04.2f}, FB1: {:04.2f}""\n                         .format(name, score[""accuracy""], score[""precision""], score[""recall""], score[""FB1""]))\n        return score\n\n    def words_to_indices(self, words):\n        """"""\n        Convert input words into batchnized word/chars indices for inference\n        :param words: input words\n        :return: batchnized word indices\n        """"""\n        chars_idx = []\n        for word in words:\n            chars = [self.char_dict[char] if char in self.char_dict else self.char_dict[UNK] for char in word]\n            chars_idx.append(chars)\n        words = [word_convert(word) for word in words]\n        words_idx = [self.word_dict[word] if word in self.word_dict else self.word_dict[UNK] for word in words]\n        return process_batch_data([words_idx], [chars_idx])\n\n    def inference(self, sentence):\n        words = sentence.lstrip().rstrip().split("" "")\n        data = self.words_to_indices(words)\n        predicts = self._predict_op(data)\n        predicts = [self.rev_tag_dict[idx] for idx in list(predicts[0])]\n        results = align_data({""input"": words, ""output"": predicts})\n        return ""{}\\n{}"".format(results[""input""], results[""output""])\n'"
models/blstm_cnn_crf_model.py,34,"b'import tensorflow as tf\nimport numpy as np\nfrom models import BaseModel, AttentionCell, multi_head_attention\nfrom tensorflow.python.ops.rnn_cell import MultiRNNCell\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn, dynamic_rnn\nfrom tensorflow.contrib.rnn.python.ops.rnn import stack_bidirectional_dynamic_rnn\nfrom models.nns import multi_conv1d, highway_network, layer_normalize\nfrom utils import Progbar\n\n\nclass SequenceLabelModel(BaseModel):\n    def __init__(self, config):\n        super(SequenceLabelModel, self).__init__(config)\n\n    def _add_placeholders(self):\n        self.words = tf.placeholder(tf.int32, shape=[None, None], name=""words"")\n        self.tags = tf.placeholder(tf.int32, shape=[None, None], name=""tags"")\n        self.seq_len = tf.placeholder(tf.int32, shape=[None], name=""seq_len"")\n        if self.cfg[""use_chars""]:\n            self.chars = tf.placeholder(tf.int32, shape=[None, None, None], name=""chars"")\n            self.char_seq_len = tf.placeholder(tf.int32, shape=[None, None], name=""char_seq_len"")\n        # hyper-parameters\n        self.is_train = tf.placeholder(tf.bool, shape=[], name=""is_train"")\n        self.batch_size = tf.placeholder(tf.int32, name=""batch_size"")\n        self.keep_prob = tf.placeholder(tf.float32, name=""rnn_keep_probability"")\n        self.drop_rate = tf.placeholder(tf.float32, name=""dropout_rate"")\n        self.lr = tf.placeholder(tf.float32, name=""learning_rate"")\n\n    def _get_feed_dict(self, batch, keep_prob=1.0, is_train=False, lr=None):\n        feed_dict = {self.words: batch[""words""], self.seq_len: batch[""seq_len""], self.batch_size: batch[""batch_size""]}\n        if ""tags"" in batch:\n            feed_dict[self.tags] = batch[""tags""]\n        if self.cfg[""use_chars""]:\n            feed_dict[self.chars] = batch[""chars""]\n            feed_dict[self.char_seq_len] = batch[""char_seq_len""]\n        feed_dict[self.keep_prob] = keep_prob\n        feed_dict[self.drop_rate] = 1.0 - keep_prob\n        feed_dict[self.is_train] = is_train\n        if lr is not None:\n            feed_dict[self.lr] = lr\n        return feed_dict\n\n    def _create_rnn_cell(self):\n        if self.cfg[""num_layers""] is None or self.cfg[""num_layers""] <= 1:\n            return self._create_single_rnn_cell(self.cfg[""num_units""])\n        else:\n            if self.cfg[""use_stack_rnn""]:\n                return [self._create_single_rnn_cell(self.cfg[""num_units""]) for _ in range(self.cfg[""num_layers""])]\n            else:\n                return MultiRNNCell([self._create_single_rnn_cell(self.cfg[""num_units""])\n                                     for _ in range(self.cfg[""num_layers""])])\n\n    def _build_embedding_op(self):\n        with tf.variable_scope(""embeddings""):\n            if not self.cfg[""use_pretrained""]:\n                self.word_embeddings = tf.get_variable(name=""emb"", dtype=tf.float32, trainable=True,\n                                                       shape=[self.word_vocab_size, self.cfg[""emb_dim""]])\n            else:\n                self.word_embeddings = tf.Variable(np.load(self.cfg[""pretrained_emb""])[""embeddings""], name=""emb"",\n                                                   dtype=tf.float32, trainable=self.cfg[""tuning_emb""])\n            word_emb = tf.nn.embedding_lookup(self.word_embeddings, self.words, name=""word_emb"")\n            print(""word embedding shape: {}"".format(word_emb.get_shape().as_list()))\n            if self.cfg[""use_chars""]:\n                self.char_embeddings = tf.get_variable(name=""c_emb"", dtype=tf.float32, trainable=True,\n                                                       shape=[self.char_vocab_size, self.cfg[""char_emb_dim""]])\n                char_emb = tf.nn.embedding_lookup(self.char_embeddings, self.chars, name=""chars_emb"")\n                char_represent = multi_conv1d(char_emb, self.cfg[""filter_sizes""], self.cfg[""channel_sizes""],\n                                              drop_rate=self.drop_rate, is_train=self.is_train)\n                print(""chars representation shape: {}"".format(char_represent.get_shape().as_list()))\n                word_emb = tf.concat([word_emb, char_represent], axis=-1)\n            if self.cfg[""use_highway""]:\n                self.word_emb = highway_network(word_emb, self.cfg[""highway_layers""], use_bias=True, bias_init=0.0,\n                                                keep_prob=self.keep_prob, is_train=self.is_train)\n            else:\n                self.word_emb = tf.layers.dropout(word_emb, rate=self.drop_rate, training=self.is_train)\n            print(""word and chars concatenation shape: {}"".format(self.word_emb.get_shape().as_list()))\n\n    def _build_model_op(self):\n        with tf.variable_scope(""bi_directional_rnn""):\n            cell_fw = self._create_rnn_cell()\n            cell_bw = self._create_rnn_cell()\n            if self.cfg[""use_stack_rnn""]:\n                rnn_outs, *_ = stack_bidirectional_dynamic_rnn(cell_fw, cell_bw, self.word_emb, dtype=tf.float32,\n                                                               sequence_length=self.seq_len)\n            else:\n                rnn_outs, *_ = bidirectional_dynamic_rnn(cell_fw, cell_bw, self.word_emb, sequence_length=self.seq_len,\n                                                         dtype=tf.float32)\n            rnn_outs = tf.concat(rnn_outs, axis=-1)\n            rnn_outs = tf.layers.dropout(rnn_outs, rate=self.drop_rate, training=self.is_train)\n            if self.cfg[""use_residual""]:\n                word_project = tf.layers.dense(self.word_emb, units=2 * self.cfg[""num_units""], use_bias=False)\n                rnn_outs = rnn_outs + word_project\n            outputs = layer_normalize(rnn_outs) if self.cfg[""use_layer_norm""] else rnn_outs\n            print(""rnn output shape: {}"".format(outputs.get_shape().as_list()))\n\n        if self.cfg[""use_attention""] == ""self_attention"":\n            with tf.variable_scope(""self_attention""):\n                attn_outs = multi_head_attention(outputs, outputs, self.cfg[""num_heads""], self.cfg[""attention_size""],\n                                                 drop_rate=self.drop_rate, is_train=self.is_train)\n                if self.cfg[""use_residual""]:\n                    attn_outs = attn_outs + outputs\n                outputs = layer_normalize(attn_outs) if self.cfg[""use_layer_norm""] else attn_outs\n                print(""self-attention output shape: {}"".format(outputs.get_shape().as_list()))\n\n        elif self.cfg[""use_attention""] == ""normal_attention"":\n            with tf.variable_scope(""normal_attention""):\n                context = tf.transpose(outputs, [1, 0, 2])\n                p_context = tf.layers.dense(outputs, units=2 * self.cfg[""num_units""], use_bias=False)\n                p_context = tf.transpose(p_context, [1, 0, 2])\n                attn_cell = AttentionCell(self.cfg[""num_units""], context, p_context)  # time major based\n                attn_outs, _ = dynamic_rnn(attn_cell, context, sequence_length=self.seq_len, time_major=True,\n                                           dtype=tf.float32)\n                outputs = tf.transpose(attn_outs, [1, 0, 2])\n                print(""attention output shape: {}"".format(outputs.get_shape().as_list()))\n\n        with tf.variable_scope(""project""):\n            self.logits = tf.layers.dense(outputs, units=self.tag_vocab_size, use_bias=True)\n            print(""logits shape: {}"".format(self.logits.get_shape().as_list()))\n\n    def train_epoch(self, train_set, valid_data, epoch):\n        num_batches = len(train_set)\n        prog = Progbar(target=num_batches)\n        for i, batch_data in enumerate(train_set):\n            feed_dict = self._get_feed_dict(batch_data, is_train=True, keep_prob=self.cfg[""keep_prob""],\n                                            lr=self.cfg[""lr""])\n            _, train_loss, summary = self.sess.run([self.train_op, self.loss, self.summary], feed_dict=feed_dict)\n            cur_step = (epoch - 1) * num_batches + (i + 1)\n            prog.update(i + 1, [(""Global Step"", int(cur_step)), (""Train Loss"", train_loss)])\n            self.train_writer.add_summary(summary, cur_step)\n            if i % 100 == 0:\n                valid_feed_dict = self._get_feed_dict(valid_data)\n                valid_summary = self.sess.run(self.summary, feed_dict=valid_feed_dict)\n                self.test_writer.add_summary(valid_summary, cur_step)\n\n    def train(self, train_set, valid_data, valid_set, test_set):\n        self.logger.info(""Start training..."")\n        best_f1, no_imprv_epoch, init_lr = -np.inf, 0, self.cfg[""lr""]\n        self._add_summary()\n        for epoch in range(1, self.cfg[""epochs""] + 1):\n            self.logger.info(\'Epoch {}/{}:\'.format(epoch, self.cfg[""epochs""]))\n            self.train_epoch(train_set, valid_data, epoch)  # train epochs\n            if self.cfg[""use_lr_decay""]:  # learning rate decay\n                self.cfg[""lr""] = max(init_lr / (1.0 + self.cfg[""lr_decay""] * epoch), self.cfg[""minimal_lr""])\n            if self.cfg[""task_name""] == ""pos"":\n                self.eval_accuracy(valid_set, ""dev"")\n                acc = self.eval_accuracy(test_set, ""test"")\n                cur_test_score = acc\n            else:\n                self.evaluate(valid_set, ""dev"")\n                score = self.evaluate(test_set, ""test"")\n                cur_test_score = score[""FB1""]\n            if cur_test_score > best_f1:\n                best_f1 = cur_test_score\n                no_imprv_epoch = 0\n                self.save_session(epoch)\n                self.logger.info(\' -- new BEST score on test dataset: {:04.2f}\'.format(best_f1))\n            else:\n                no_imprv_epoch += 1\n                if no_imprv_epoch >= self.cfg[""no_imprv_tolerance""]:\n                    self.logger.info(\'early stop at {}th epoch without improvement, BEST score on testset: {:04.2f}\'\n                                     .format(epoch, best_f1))\n                    break\n        self.train_writer.close()\n        self.test_writer.close()\n\n    def eval_accuracy(self, dataset, name):  # Used for POS task\n        accuracy = []\n        for data in dataset:\n            predicts = self._predict_op(data)\n            for preds, tags, seq_len in zip(predicts, data[""tags""], data[""seq_len""]):\n                preds = preds[:seq_len]\n                tags = tags[:seq_len]\n                accuracy += [p == t for p, t in zip(preds, tags)]\n        acc = np.mean(accuracy) * 100.0\n        self.logger.info(""{} dataset -- accuracy: {:04.2f}"".format(name, acc))\n        return acc\n'"
models/multi_attention_model.py,33,"b'import random\nimport tensorflow as tf\nfrom utils import Progbar, batchnize_dataset\nfrom models import BaseModel, add_timing_signal, multi_conv1d, layer_normalize, multi_head_attention\nfrom tensorflow.python.ops.rnn_cell import LSTMCell, GRUCell, DropoutWrapper, ResidualWrapper\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\n\n\nclass SequenceLabelModel(BaseModel):\n    def __init__(self, config):\n        super(SequenceLabelModel, self).__init__(config)\n\n    def _add_placeholders(self):\n        self.words = tf.placeholder(tf.int32, shape=[None, None], name=""words"")\n        self.tags = tf.placeholder(tf.int32, shape=[None, None], name=""tags"")\n        self.seq_len = tf.placeholder(tf.int32, shape=[None], name=""seq_len"")\n        if self.cfg[""use_chars""]:\n            self.chars = tf.placeholder(tf.int32, shape=[None, None, None], name=""chars"")\n            self.char_seq_len = tf.placeholder(tf.int32, shape=[None, None], name=""char_seq_len"")\n        # hyper-parameters\n        self.emb_drop_rate = tf.placeholder(tf.float32, name=""emb_drop_rate"")\n        self.rnn_keep_prob = tf.placeholder(tf.float32, name=""rnn_keep_prob"")\n        self.attn_drop_rate = tf.placeholder(tf.float32, name=""attn_drop_rate"")  # drop_rate = 1.0 - keep_prob\n        self.is_train = tf.placeholder(tf.bool, shape=[], name=""is_train"")\n        self.batch_size = tf.placeholder(tf.int32, name=""batch_size"")\n        self.lr = tf.placeholder(tf.float32, name=""learning_rate"")\n\n    def _get_feed_dict(self, batch, emb_keep_prob=1.0, rnn_keep_prob=1.0, attn_keep_prob=1.0, is_train=False, lr=None):\n        feed_dict = {self.words: batch[""words""], self.seq_len: batch[""seq_len""], self.batch_size: batch[""batch_size""]}\n        if ""tags"" in batch:\n            feed_dict[self.tags] = batch[""tags""]\n        if self.cfg[""use_chars""]:\n            feed_dict[self.chars] = batch[""chars""]\n            feed_dict[self.char_seq_len] = batch[""char_seq_len""]\n        feed_dict[self.emb_drop_rate] = 1.0 - emb_keep_prob\n        feed_dict[self.rnn_keep_prob] = rnn_keep_prob\n        feed_dict[self.attn_drop_rate] = 1.0 - attn_keep_prob\n        feed_dict[self.is_train] = is_train\n        if lr is not None:\n            feed_dict[self.lr] = lr\n        return feed_dict\n\n    def _create_single_rnn_cell(self, num_units):\n        cell = GRUCell(num_units) if self.cfg[""cell_type""] == ""gru"" else LSTMCell(num_units)\n        if self.cfg[""use_dropout""]:\n            cell = DropoutWrapper(cell, output_keep_prob=self.rnn_keep_prob)\n        if self.cfg[""use_residual""]:\n            cell = ResidualWrapper(cell)\n        return cell\n\n    def _build_embedding_op(self):\n        with tf.variable_scope(""embeddings""):\n            pad_emb = tf.Variable(tf.zeros([1, self.cfg[""emb_dim""]], dtype=tf.float32), name=\'pad_emb\', trainable=False)\n            emb = tf.get_variable(name=""emb"", dtype=tf.float32, shape=[self.word_vocab_size - 1, self.cfg[""emb_dim""]],\n                                  trainable=True)\n            self.embeddings = tf.concat([pad_emb, emb], axis=0)\n            word_emb = tf.nn.embedding_lookup(self.embeddings, self.words, name=""word_emb"")\n            if self.cfg[""add_positional_emb""]:\n                word_emb = add_timing_signal(word_emb)\n            self.word_emb = tf.layers.dropout(word_emb, rate=self.emb_drop_rate, training=self.is_train)\n            if self.cfg[""use_chars""]:\n                c_pad_emb = tf.Variable(tf.zeros([1, self.cfg[""char_emb_dim""]], dtype=tf.float32), name=""c_pad_emb"",\n                                        trainable=False)\n                c_emb = tf.get_variable(name=""c_emb"", dtype=tf.float32, trainable=True,\n                                        shape=[self.char_vocab_size - 1, self.cfg[""char_emb_dim""]])\n                self.char_embeddings = tf.concat([c_pad_emb, c_emb], axis=0)\n                char_emb = tf.nn.embedding_lookup(self.char_embeddings, self.chars, name=""char_emb"")\n                char_represent = multi_conv1d(char_emb, self.cfg[""filter_sizes""], self.cfg[""channel_sizes""],\n                                              drop_rate=self.emb_drop_rate, is_train=self.is_train)\n                if self.cfg[""add_positional_emb""]:\n                    char_represent = add_timing_signal(char_represent)\n                self.chars_emb = tf.layers.dropout(char_represent, rate=self.emb_drop_rate, training=self.is_train)\n                print(""chars embeddings shape: {}"".format(self.chars_emb.get_shape().as_list()))\n            print(""word embeddings shape: {}"".format(self.word_emb.get_shape().as_list()))\n\n    def _build_model_op(self):\n        with tf.variable_scope(""bi_directional_rnn""):\n            cell_fw = self._create_single_rnn_cell(self.cfg[""num_units""])\n            cell_bw = self._create_single_rnn_cell(self.cfg[""num_units""])\n            if self.cfg[""use_residual""]:\n                self.word_emb = tf.layers.dense(self.word_emb, units=self.cfg[""num_units""], use_bias=False,\n                                                name=""word_input_project"")\n                if self.cfg[""use_chars""]:\n                    self.chars_emb = tf.layers.dense(self.chars_emb, units=self.cfg[""num_units""], use_bias=False,\n                                                     name=""chars_input_project"")\n\n            rnn_outs, _ = bidirectional_dynamic_rnn(cell_fw, cell_bw, self.word_emb, sequence_length=self.seq_len,\n                                                    dtype=tf.float32, scope=""bi_rnn"")\n            rnn_outs = tf.concat(rnn_outs, axis=-1)\n            print(""Bi-directional RNN output shape on word: {}"".format(rnn_outs.get_shape().as_list()))\n            if self.cfg[""use_chars""]:\n                tf.get_variable_scope().reuse_variables()\n                chars_rnn_outs, _ = bidirectional_dynamic_rnn(cell_fw, cell_bw, self.chars_emb, dtype=tf.float32,\n                                                              sequence_length=self.seq_len, scope=""bi_rnn"")\n                chars_rnn_outs = tf.concat(chars_rnn_outs, axis=-1)\n                print(""Bi-directional RNN output shape on chars: {}"".format(chars_rnn_outs.get_shape().as_list()))\n                rnn_outs = rnn_outs + chars_rnn_outs\n            rnn_outs = layer_normalize(rnn_outs)\n\n        with tf.variable_scope(""multi_head_attention""):\n            attn_outs = multi_head_attention(rnn_outs, rnn_outs, self.cfg[""num_heads""], self.cfg[""attention_size""],\n                                             drop_rate=self.attn_drop_rate, is_train=self.is_train)\n            if self.cfg[""use_residual""]:\n                attn_outs = attn_outs + rnn_outs\n            attn_outs = layer_normalize(attn_outs)  # residual connection and layer norm\n            print(""multi-heads attention output shape: {}"".format(attn_outs.get_shape().as_list()))\n\n        with tf.variable_scope(""projection""):\n            self.logits = tf.layers.dense(attn_outs, units=self.tag_vocab_size, use_bias=True)\n            print(""logits shape: {}"".format(self.logits.get_shape().as_list()))\n\n    def train_epoch(self, train_set, valid_data, epoch, shuffle=True):\n        if shuffle:\n            random.shuffle(train_set)\n        train_set = batchnize_dataset(train_set, self.cfg.batch_size)\n        num_batches = len(train_set)\n        prog = Progbar(target=num_batches)\n        for i, batch_data in enumerate(train_set):\n            feed_dict = self._get_feed_dict(batch_data, emb_keep_prob=self.cfg[""emb_keep_prob""],\n                                            rnn_keep_prob=self.cfg[""rnn_keep_prob""],\n                                            attn_keep_prob=self.cfg[""attn_keep_prob""], is_train=True, lr=self.cfg[""lr""])\n            _, train_loss, summary = self.sess.run([self.train_op, self.loss, self.summary], feed_dict=feed_dict)\n            cur_step = (epoch - 1) * num_batches + (i + 1)\n            prog.update(i + 1, [(""Global Step"", int(cur_step)), (""Train Loss"", train_loss)])\n            self.train_writer.add_summary(summary, cur_step)\n            if i % 100 == 0:\n                valid_feed_dict = self._get_feed_dict(valid_data)\n                valid_summary = self.sess.run(self.summary, feed_dict=valid_feed_dict)\n                self.test_writer.add_summary(valid_summary, cur_step)\n'"
models/nns.py,86,"b'import tensorflow as tf\nimport math\nfrom functools import reduce\nfrom operator import mul\nfrom tensorflow.python.ops.rnn_cell import RNNCell, LSTMCell, GRUCell\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\n\n\ndef layer_normalize(inputs, epsilon=1e-8, scope=None):\n    with tf.variable_scope(scope or ""layer_norm""):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta = tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) / ((variance + epsilon) ** 0.5)\n        outputs = tf.add(tf.multiply(gamma, normalized), beta)\n        return outputs\n\n\ndef highway_layer(inputs, use_bias=True, bias_init=0.0, keep_prob=1.0, is_train=False, scope=None):\n    with tf.variable_scope(scope or ""highway_layer""):\n        hidden = inputs.get_shape().as_list()[-1]\n        with tf.variable_scope(""trans""):\n            trans = tf.layers.dropout(inputs, rate=1.0 - keep_prob, training=is_train)\n            trans = tf.layers.dense(trans, units=hidden, use_bias=use_bias, bias_initializer=tf.constant_initializer(\n                bias_init), activation=None)\n            trans = tf.nn.relu(trans)\n        with tf.variable_scope(""gate""):\n            gate = tf.layers.dropout(inputs, rate=1.0 - keep_prob, training=is_train)\n            gate = tf.layers.dense(gate, units=hidden, use_bias=use_bias, bias_initializer=tf.constant_initializer(\n                bias_init), activation=None)\n            gate = tf.nn.sigmoid(gate)\n        outputs = gate * trans + (1 - gate) * inputs\n        return outputs\n\n\ndef highway_network(inputs, highway_layers=2, use_bias=True, bias_init=0.0, keep_prob=1.0, is_train=False, scope=None):\n    with tf.variable_scope(scope or ""highway_network""):\n        prev = inputs\n        cur = None\n        for idx in range(highway_layers):\n            cur = highway_layer(prev, use_bias, bias_init, keep_prob, is_train, scope=""highway_layer_{}"".format(idx))\n            prev = cur\n        return cur\n\n\ndef conv1d(in_, filter_size, height, padding, is_train=True, drop_rate=0.0, scope=None):\n    with tf.variable_scope(scope or ""conv1d""):\n        num_channels = in_.get_shape()[-1]\n        filter_ = tf.get_variable(""filter"", shape=[1, height, num_channels, filter_size], dtype=tf.float32)\n        bias = tf.get_variable(""bias"", shape=[filter_size], dtype=tf.float32)\n        strides = [1, 1, 1, 1]\n        in_ = tf.layers.dropout(in_, rate=drop_rate, training=is_train)\n        # [batch, max_len_sent, max_len_word / filter_stride, char output size]\n        xxc = tf.nn.conv2d(in_, filter_, strides, padding) + bias\n        out = tf.reduce_max(tf.nn.relu(xxc), axis=2)  # max-pooling, [-1, max_len_sent, char output size]\n        return out\n\n\ndef multi_conv1d(in_, filter_sizes, heights, padding=""VALID"", is_train=True, drop_rate=0.0, scope=None):\n    with tf.variable_scope(scope or ""multi_conv1d""):\n        assert len(filter_sizes) == len(heights)\n        outs = []\n        for i, (filter_size, height) in enumerate(zip(filter_sizes, heights)):\n            if filter_size == 0:\n                continue\n            out = conv1d(in_, filter_size, height, padding, is_train=is_train, drop_rate=drop_rate,\n                         scope=""conv1d_{}"".format(i))\n            outs.append(out)\n        concat_out = tf.concat(axis=2, values=outs)\n        return concat_out\n\n\nclass BiRNN:\n    def __init__(self, num_units, cell_type=\'lstm\', scope=None):\n        self.cell_fw = GRUCell(num_units) if cell_type == \'gru\' else LSTMCell(num_units)\n        self.cell_bw = GRUCell(num_units) if cell_type == \'gru\' else LSTMCell(num_units)\n        self.scope = scope or ""bi_rnn""\n\n    def __call__(self, inputs, seq_len, use_last_state=False, time_major=False):\n        assert not time_major, ""BiRNN class cannot support time_major currently""\n        with tf.variable_scope(self.scope):\n            flat_inputs = flatten(inputs, keep=2)  # reshape to [-1, max_time, dim]\n            seq_len = flatten(seq_len, keep=0)  # reshape to [x] (one dimension sequence)\n            outputs, ((_, h_fw), (_, h_bw)) = bidirectional_dynamic_rnn(self.cell_fw, self.cell_bw, flat_inputs,\n                                                                        sequence_length=seq_len, dtype=tf.float32)\n            if use_last_state:  # return last states\n                output = tf.concat([h_fw, h_bw], axis=-1)  # shape = [-1, 2 * num_units]\n                output = reconstruct(output, ref=inputs, keep=2, remove_shape=1)  # remove the max_time shape\n            else:\n                output = tf.concat(outputs, axis=-1)  # shape = [-1, max_time, 2 * num_units]\n                output = reconstruct(output, ref=inputs, keep=2)  # reshape to same as inputs, except the last two dim\n            return output\n\n\nclass DenselyConnectedBiRNN:\n    """"""Implement according to Densely Connected Bidirectional LSTM with Applications to Sentence Classification\n       https://arxiv.org/pdf/1802.00889.pdf""""""\n    def __init__(self, num_layers, num_units, cell_type=\'lstm\', scope=None):\n        if type(num_units) == list:\n            assert len(num_units) == num_layers, ""if num_units is a list, then its size should equal to num_layers""\n        self.dense_bi_rnn = []\n        for i in range(num_layers):\n            units = num_units[i] if type(num_units) == list else num_units\n            self.dense_bi_rnn.append(BiRNN(units, cell_type, scope=\'bi_rnn_{}\'.format(i)))\n        self.num_layers = num_layers\n        self.scope = scope or ""densely_connected_bi_rnn""\n\n    def __call__(self, inputs, seq_len, time_major=False):\n        assert not time_major, ""DenseConnectBiRNN class cannot support time_major currently""\n        # this function does not support return_last_state method currently\n        with tf.variable_scope(self.scope):\n            flat_inputs = flatten(inputs, keep=2)  # reshape to [-1, max_time, dim]\n            seq_len = flatten(seq_len, keep=0)  # reshape to [x] (one dimension sequence)\n            cur_inputs = flat_inputs\n            for i in range(self.num_layers):\n                cur_outputs = self.dense_bi_rnn[i](cur_inputs, seq_len)\n                if i < self.num_layers - 1:\n                    cur_inputs = tf.concat([cur_inputs, cur_outputs], axis=-1)\n                else:\n                    cur_inputs = cur_outputs\n            output = reconstruct(cur_inputs, ref=inputs, keep=2)\n            return output\n\n\ndef multi_head_attention(queries, keys, num_heads, attention_size, drop_rate=0.0, is_train=True, reuse=None,\n                         scope=None):\n    # borrowed from: https://github.com/Kyubyong/transformer/blob/master/modules.py\n    with tf.variable_scope(scope or ""multi_head_attention"", reuse=reuse):\n        if attention_size is None:\n            attention_size = queries.get_shape().as_list()[-1]\n        # linear projections, shape=(batch_size, max_time, attention_size)\n        query = tf.layers.dense(queries, attention_size, activation=tf.nn.relu, name=""query_project"")\n        key = tf.layers.dense(keys, attention_size, activation=tf.nn.relu, name=""key_project"")\n        value = tf.layers.dense(keys, attention_size, activation=tf.nn.relu, name=""value_project"")\n        # split and concatenation, shape=(batch_size * num_heads, max_time, attention_size / num_heads)\n        query_ = tf.concat(tf.split(query, num_heads, axis=2), axis=0)\n        key_ = tf.concat(tf.split(key, num_heads, axis=2), axis=0)\n        value_ = tf.concat(tf.split(value, num_heads, axis=2), axis=0)\n        # multiplication\n        attn_outs = tf.matmul(query_, tf.transpose(key_, [0, 2, 1]))\n        # scale\n        attn_outs = attn_outs / (key_.get_shape().as_list()[-1] ** 0.5)\n        # key masking\n        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # shape=(batch_size, max_time)\n        key_masks = tf.tile(key_masks, [num_heads, 1])  # shape=(batch_size * num_heads, max_time)\n        # shape=(batch_size * num_heads, max_time, max_time)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])\n        paddings = tf.ones_like(attn_outs) * (-2 ** 32 + 1)\n        # shape=(batch_size, max_time, attention_size)\n        attn_outs = tf.where(tf.equal(key_masks, 0), paddings, attn_outs)\n        # activation\n        attn_outs = tf.nn.softmax(attn_outs)\n        # query masking\n        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))\n        query_masks = tf.tile(query_masks, [num_heads, 1])\n        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])\n        attn_outs *= query_masks\n        # dropout\n        attn_outs = tf.layers.dropout(attn_outs, rate=drop_rate, training=is_train)\n        # weighted sum\n        outputs = tf.matmul(attn_outs, value_)\n        # restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)\n        outputs += queries  # residual connection\n        outputs = layer_normalize(outputs)\n    return outputs\n\n\ndef dot_attention(inputs, memory, hidden, drop_rate=0.0, is_train=True, scope=None):\n    with tf.variable_scope(scope or ""dot_attention""):\n        d_inputs = tf.layers.dropout(inputs, rate=drop_rate, training=tf.convert_to_tensor(is_train))\n        d_memory = tf.layers.dropout(memory, rate=drop_rate, training=tf.convert_to_tensor(is_train))\n\n        with tf.variable_scope(""attention""):\n            inputs_ = tf.nn.relu(tf.layers.dense(d_inputs, hidden, use_bias=False, name=\'inputs\'))\n            memory_ = tf.nn.relu(tf.layers.dense(d_memory, hidden, use_bias=False, name=\'memory\'))\n            outputs = tf.matmul(inputs_, tf.transpose(memory_, [0, 2, 1])) / (hidden ** 0.5)\n            logits = tf.nn.softmax(outputs)\n            outputs = tf.matmul(logits, memory)\n            res = tf.concat([inputs, outputs], axis=-1)\n\n        with tf.variable_scope(""gate""):\n            dim = res.get_shape().as_list()[-1]\n            d_res = tf.layers.dropout(res, rate=drop_rate, training=tf.convert_to_tensor(is_train))\n            gate = tf.nn.sigmoid(tf.layers.dense(d_res, dim, use_bias=False, name=\'gate\'))\n            return res * gate\n\n\nclass AttentionCell(RNNCell):  # time_major based\n    """"""Implement of https://pdfs.semanticscholar.org/8785/efdad2abc384d38e76a84fb96d19bbe788c1.pdf?_ga=2.156364859.18139\n    40814.1518068648-1853451355.1518068648\n    refer: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py""""""\n    def __init__(self, num_units, memory, pmemory, cell_type=\'lstm\'):\n        super(AttentionCell, self).__init__()\n        self._cell = LSTMCell(num_units) if cell_type == \'lstm\' else GRUCell(num_units)\n        self.num_units = num_units\n        self.memory = memory\n        self.pmemory = pmemory\n        self.mem_units = memory.get_shape().as_list()[-1]\n\n    @property\n    def state_size(self):\n        return self._cell.state_size\n\n    @property\n    def output_size(self):\n        return self._cell.output_size\n\n    def compute_output_shape(self, input_shape):\n        pass\n\n    def __call__(self, inputs, state, scope=None):\n        c, m = state\n        # (max_time, batch_size, att_unit)\n        ha = tf.nn.tanh(tf.add(self.pmemory, tf.layers.dense(m, self.mem_units, use_bias=False, name=""wah"")))\n        alphas = tf.squeeze(tf.exp(tf.layers.dense(ha, units=1, use_bias=False, name=\'way\')), axis=[-1])\n        alphas = tf.div(alphas, tf.reduce_sum(alphas, axis=0, keepdims=True))  # (max_time, batch_size)\n        # (batch_size, att_units)\n        w_context = tf.reduce_sum(tf.multiply(self.memory, tf.expand_dims(alphas, axis=-1)), axis=0)\n        h, new_state = self._cell(inputs, state)\n        lfc = tf.layers.dense(w_context, self.num_units, use_bias=False, name=\'wfc\')\n        # (batch_size, num_units)\n        fw = tf.sigmoid(tf.layers.dense(lfc, self.num_units, use_bias=False, name=\'wff\') +\n                        tf.layers.dense(h, self.num_units, name=\'wfh\'))\n        hft = tf.multiply(lfc, fw) + h  # (batch_size, num_units)\n        return hft, new_state\n\n\ndef add_timing_signal(x, min_timescale=1.0, max_timescale=1.0e4):\n    """"""Adds a bunch of sinusoids of different frequencies to a Tensor.\n    Each channel of the input Tensor is incremented by a sinusoid of a different frequency and phase.\n    This allows attention to learn to use absolute and relative positions. Timing signals should be added to some\n    precursors of both the query and the memory inputs to attention.\n    The use of relative position is possible because sin(x+y) and cos(x+y) can be expressed in terms of y,\n    sin(x) and cos(x).\n    In particular, we use a geometric sequence of timescales starting with min_timescale and ending with max_timescale.\n    The number of different timescales is equal to channels / 2. For each timescale, we generate the two sinusoidal\n    signals sin(timestep/timescale) and cos(timestep/timescale).  All of these sinusoids are concatenated in the\n    channels dimension.\n    Args:\n        x: a Tensor with shape [batch, length, channels]\n        min_timescale: a float\n        max_timescale: a float\n    Returns:\n        a Tensor the same shape as x.\n    """"""\n    with tf.name_scope(""add_timing_signal"", values=[x]):\n        length = tf.shape(x)[1]\n        channels = tf.shape(x)[2]\n        position = tf.to_float(tf.range(length))\n        num_timescales = channels // 2\n\n        log_timescale_increment = (math.log(float(max_timescale) / float(min_timescale)) /\n                                   (tf.to_float(num_timescales) - 1))\n        inv_timescales = min_timescale * tf.exp(tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n\n        scaled_time = (tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0))\n        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n        signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n        signal = tf.reshape(signal, [1, length, channels])\n\n        return x + signal\n\n\ndef label_smoothing(inputs, epsilon=0.1):\n    dim = inputs.get_shape().as_list()[-1]  # number of channels\n    return ((1 - epsilon) * inputs) + (epsilon / dim)\n\n\ndef flatten(tensor, keep):\n    fixed_shape = tensor.get_shape().as_list()\n    start = len(fixed_shape) - keep\n    left = reduce(mul, [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start)])\n    out_shape = [left] + [fixed_shape[i] or tf.shape(tensor)[i] for i in range(start, len(fixed_shape))]\n    flat = tf.reshape(tensor, out_shape)\n    return flat\n\n\ndef reconstruct(tensor, ref, keep, remove_shape=None):\n    ref_shape = ref.get_shape().as_list()\n    tensor_shape = tensor.get_shape().as_list()\n    ref_stop = len(ref_shape) - keep\n    tensor_start = len(tensor_shape) - keep\n    if remove_shape is not None:\n        tensor_start = tensor_start + remove_shape\n    pre_shape = [ref_shape[i] or tf.shape(ref)[i] for i in range(ref_stop)]\n    keep_shape = [tensor_shape[i] or tf.shape(tensor)[i] for i in range(tensor_start, len(tensor_shape))]\n    target_shape = pre_shape + keep_shape\n    out = tf.reshape(tensor, target_shape)\n    return out\n'"
models/punct_attentive_model.py,39,"b'import tensorflow as tf\nimport numpy as np\nfrom numpy import nan\nimport codecs\nimport os\nfrom models import BaseModel, AttentionCell, highway_network, BiRNN, DenselyConnectedBiRNN, multi_conv1d\nfrom tensorflow.python.ops.rnn import dynamic_rnn\nfrom tensorflow.contrib.crf import crf_log_likelihood\nfrom utils import Progbar, pad_char_sequences\nfrom utils.punct_prepro import PUNCTUATION_VOCABULARY, PUNCTUATION_MAPPING, END, UNK, EOS_TOKENS, SPACE\n\n\nclass SequenceLabelModel(BaseModel):\n    def __init__(self, config):\n        super(SequenceLabelModel, self).__init__(config)\n\n    def _add_placeholders(self):\n        self.words = tf.placeholder(tf.int32, shape=[None, None], name=""words"")  # shape = (batch_size, max_time)\n        self.tags = tf.placeholder(tf.int32, shape=[None, None], name=""tags"")  # shape = (batch_size, max_time - 1)\n        self.seq_len = tf.placeholder(tf.int32, shape=[None], name=""seq_len"")\n        if self.cfg[""use_chars""]:\n            # shape = (batch_size, max_time, max_word_length)\n            self.chars = tf.placeholder(tf.int32, shape=[None, None, None], name=""chars"")\n            self.char_seq_len = tf.placeholder(tf.int32, shape=[None, None], name=""char_seq_len"")\n        # hyper-parameters\n        self.batch_size = tf.placeholder(tf.int32, name=""batch_size"")\n        self.is_train = tf.placeholder(tf.bool, shape=[], name=""is_train"")\n        self.keep_prob = tf.placeholder(tf.float32, name=""keep_prob"")\n        self.drop_rate = tf.placeholder(tf.float32, name=""dropout_rate"")\n        self.lr = tf.placeholder(tf.float32, name=""learning_rate"")\n\n    def _get_feed_dict(self, batch, keep_prob=1.0, is_train=False, lr=None):\n        feed_dict = {self.words: batch[""words""], self.seq_len: batch[""seq_len""], self.batch_size: batch[""batch_size""]}\n        if ""tags"" in batch:\n            feed_dict[self.tags] = batch[""tags""]\n        if self.cfg[""use_chars""]:\n            feed_dict[self.chars] = batch[""chars""]\n            feed_dict[self.char_seq_len] = batch[""char_seq_len""]\n        feed_dict[self.keep_prob] = keep_prob\n        feed_dict[self.drop_rate] = 1.0 - keep_prob\n        feed_dict[self.is_train] = is_train\n        if lr is not None:\n            feed_dict[self.lr] = lr\n        return feed_dict\n\n    def _build_embedding_op(self):\n        with tf.variable_scope(""embeddings""):\n            if not self.cfg[""use_pretrained""]:\n                self.word_embeddings = tf.get_variable(name=""emb"", dtype=tf.float32, trainable=True,\n                                                       shape=[self.word_vocab_size, self.cfg[""emb_dim""]])\n            else:\n                word_emb_1 = tf.Variable(np.load(self.cfg[""pretrained_emb""])[""embeddings""], name=""word_emb_1"",\n                                         dtype=tf.float32, trainable=self.cfg[""tuning_emb""])\n                word_emb_2 = tf.get_variable(name=""word_emb_2"", shape=[3, self.cfg[""emb_dim""]], dtype=tf.float32,\n                                             trainable=True)  # For UNK, NUM and END\n                self.word_embeddings = tf.concat([word_emb_1, word_emb_2], axis=0)\n            word_emb = tf.nn.embedding_lookup(self.word_embeddings, self.words, name=""word_emb"")\n            print(""word embedding shape: {}"".format(word_emb.get_shape().as_list()))\n            if self.cfg[""use_chars""]:\n                self.char_embeddings = tf.get_variable(name=""c_emb"", dtype=tf.float32, trainable=True,\n                                                       shape=[self.char_vocab_size, self.cfg[""char_emb_dim""]])\n                char_emb = tf.nn.embedding_lookup(self.char_embeddings, self.chars, name=""chars_emb"")\n                # train char representation\n                if self.cfg[""char_represent_method""] == ""rnn"":\n                    char_bi_rnn = BiRNN(self.cfg[""char_num_units""], cell_type=self.cfg[""cell_type""], scope=""c_bi_rnn"")\n                    char_represent = char_bi_rnn(char_emb, self.char_seq_len, use_last_state=True)\n                else:\n                    char_represent = multi_conv1d(char_emb, self.cfg[""filter_sizes""], self.cfg[""channel_sizes""],\n                                                  drop_rate=self.drop_rate,\n                                                  is_train=self.is_train)\n                print(""chars representation shape: {}"".format(char_represent.get_shape().as_list()))\n                word_emb = tf.concat([word_emb, char_represent], axis=-1)\n            if self.cfg[""use_highway""]:\n                self.word_emb = highway_network(word_emb, self.cfg[""highway_layers""], use_bias=True, bias_init=0.0,\n                                                keep_prob=self.keep_prob, is_train=self.is_train)\n            else:\n                self.word_emb = tf.layers.dropout(word_emb, rate=self.drop_rate, training=self.is_train)\n            print(""word and chars concatenation shape: {}"".format(self.word_emb.get_shape().as_list()))\n\n    def _build_model_op(self):\n        with tf.variable_scope(""densely_connected_bi_rnn""):\n            dense_bi_rnn = DenselyConnectedBiRNN(self.cfg[""num_layers""], self.cfg[""num_units_list""],\n                                                 cell_type=self.cfg[""cell_type""])\n            context = dense_bi_rnn(self.word_emb, seq_len=self.seq_len)\n            print(""densely connected bi_rnn output shape: {}"".format(context.get_shape().as_list()))\n\n        with tf.variable_scope(""attention""):\n            p_context = tf.layers.dense(context, units=2 * self.cfg[""num_units_list""][-1], use_bias=True,\n                                        bias_initializer=tf.constant_initializer(0.0))\n            context = tf.transpose(context, [1, 0, 2])\n            p_context = tf.transpose(p_context, [1, 0, 2])\n            attn_cell = AttentionCell(self.cfg[""num_units_list""][-1], context, p_context)\n            attn_outs, _ = dynamic_rnn(attn_cell, context[1:, :, :], sequence_length=self.seq_len - 1, dtype=tf.float32,\n                                       time_major=True)\n            attn_outs = tf.transpose(attn_outs, [1, 0, 2])\n            print(""attention output shape: {}"".format(attn_outs.get_shape().as_list()))\n\n        with tf.variable_scope(""project""):\n            self.logits = tf.layers.dense(attn_outs, units=self.tag_vocab_size, use_bias=True,\n                                          bias_initializer=tf.constant_initializer(0.0))\n            print(""logits shape: {}"".format(self.logits.get_shape().as_list()))\n\n    def _build_loss_op(self):\n        if self.cfg[""use_crf""]:\n            crf_loss, self.trans_params = crf_log_likelihood(self.logits, self.tags, self.seq_len - 1)\n            self.loss = tf.reduce_mean(-crf_loss)\n        else:  # using softmax\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.tags)\n            mask = tf.sequence_mask(self.seq_len)\n            self.loss = tf.reduce_mean(tf.boolean_mask(losses, mask))\n        if self.cfg[""l2_reg""] is not None and self.cfg[""l2_reg""] > 0.0:  # l2 regularization\n            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if ""bias"" not in v.name])\n            self.loss += self.cfg[""l2_reg""] * l2_loss\n        tf.summary.scalar(""loss"", self.loss)\n\n    def _predict_op(self, data):\n        feed_dict = self._get_feed_dict(data)\n        if self.cfg[""use_crf""]:\n            logits, trans_params = self.sess.run([self.logits, self.trans_params], feed_dict=feed_dict)\n            return self.viterbi_decode(logits, trans_params, data[""seq_len""] - 1)\n        else:\n            pred_logits = tf.cast(tf.argmax(self.logits, axis=-1), tf.int32)\n            logits = self.sess.run(pred_logits, feed_dict=feed_dict)\n            return logits\n\n    def train_epoch(self, train_set, valid_data, epoch):\n        num_batches = len(train_set)\n        prog = Progbar(target=num_batches)\n        total_cost, total_samples = 0, 0\n        for i, batch in enumerate(train_set):\n            feed_dict = self._get_feed_dict(batch, is_train=True, keep_prob=self.cfg[""keep_prob""], lr=self.cfg[""lr""])\n            _, train_loss, summary = self.sess.run([self.train_op, self.loss, self.summary], feed_dict=feed_dict)\n            cur_step = (epoch - 1) * num_batches + (i + 1)\n            total_cost += train_loss\n            total_samples += np.array(batch[""words""]).shape[0]\n            prog.update(i + 1, [(""Global Step"", int(cur_step)), (""Train Loss"", train_loss),\n                                (""Perplexity"", np.exp(total_cost / total_samples))])\n            self.train_writer.add_summary(summary, cur_step)\n            if i % 100 == 0:\n                valid_feed_dict = self._get_feed_dict(valid_data)\n                valid_summary = self.sess.run(self.summary, feed_dict=valid_feed_dict)\n                self.test_writer.add_summary(valid_summary, cur_step)\n\n    def train(self, train_set, valid_data, valid_text, test_texts):  # test_texts: [ref, asr]\n        self.logger.info(""Start training..."")\n        best_f1, no_imprv_epoch = -np.inf, 0\n        self._add_summary()\n        for epoch in range(1, self.cfg[""epochs""] + 1):\n            self.logger.info(""Epoch {}/{}:"".format(epoch, self.cfg[""epochs""]))\n            self.train_epoch(train_set, valid_data, epoch)\n            # self.evaluate(valid_text)\n            ref_f1 = self.evaluate_punct(test_texts[0])[""F1""] * 100.0  # use ref to compute best F1\n            asr_f1 = self.evaluate_punct(test_texts[1])[""F1""] * 100.0\n            if ref_f1 >= best_f1:\n                best_f1 = ref_f1\n                no_imprv_epoch = 0\n                self.save_session(epoch)\n                self.logger.info("" -- new BEST score on ref dataset: {:04.2f}, on asr dataset: {:04.2f}""\n                                 .format(best_f1, asr_f1))\n            else:\n                no_imprv_epoch += 1\n                if no_imprv_epoch >= self.cfg[""no_imprv_tolerance""]:\n                    self.logger.info(""early stop at {}th epoch without improvement, BEST score on ref dataset: {:04.2f}""\n                                     .format(epoch, best_f1))\n                    break\n        self.train_writer.close()\n        self.test_writer.close()\n\n    def evaluate_punct(self, file):\n        save_path = os.path.join(self.cfg[""checkpoint_path""], ""result.txt"")\n        with codecs.open(file, mode=""r"", encoding=""utf-8"") as f:\n            text = f.read().split()\n        text = [w for w in text if w not in self.tag_dict and w not in PUNCTUATION_MAPPING] + [END]\n        index = 0\n        with codecs.open(save_path, mode=""w"", encoding=""utf-8"") as f_out:\n            while True:\n                subseq = text[index: index + self.cfg[""max_sequence_len""]]\n                if len(subseq) == 0:\n                    break\n                # create feed data\n                cvrt_seq = np.array([[self.word_dict.get(w, self.word_dict[UNK]) for w in subseq]], dtype=np.int32)\n                seq_len = np.array([len(v) for v in cvrt_seq], dtype=np.int32)\n                cvrt_seq_chars = []\n                for word in subseq:\n                    chars = [self.char_dict.get(c, self.char_dict[UNK]) for c in word]\n                    cvrt_seq_chars.append(chars)\n                cvrt_seq_chars, char_seq_len = pad_char_sequences([cvrt_seq_chars])\n                cvrt_seq_chars = np.array(cvrt_seq_chars, dtype=np.int32)\n                char_seq_len = np.array(char_seq_len, dtype=np.int32)\n                data = {""words"": cvrt_seq, ""seq_len"": seq_len, ""chars"": cvrt_seq_chars, ""char_seq_len"": char_seq_len,\n                        ""batch_size"": 1}\n                # predict\n                predicts = self._predict_op(data)\n                # write to file\n                f_out.write(subseq[0])\n                last_eos_idx = 0\n                punctuations = []\n                for preds_t in predicts[0]:\n                    punctuation = self.rev_tag_dict[preds_t]\n                    punctuations.append(punctuation)\n                    if punctuation in EOS_TOKENS:\n                        last_eos_idx = len(punctuations)\n                if subseq[-1] == END:\n                    step = len(subseq) - 1\n                elif last_eos_idx != 0:\n                    step = last_eos_idx\n                else:\n                    step = len(subseq) - 1\n                for j in range(step):\n                    f_out.write("" "" + punctuations[j] + "" "" if punctuations[j] != SPACE else "" "")\n                    if j < step - 1:\n                        f_out.write(subseq[1 + j])\n                if subseq[-1] == END:\n                    break\n                index += step\n        out_str, f1, err, ser = self.compute_score(file, save_path)\n        score = {""F1"": f1, ""ERR"": err, ""SER"": ser}\n        self.logger.info(""\\nEvaluate on {}:\\n{}\\n"".format(file, out_str))\n        try:  # delete output file after compute scores\n            os.remove(save_path)\n        except OSError:\n            pass\n        return score\n\n    def inference(self, sentence):\n        pass  # TODO\n\n    @staticmethod\n    def compute_score(target_path, predicted_path):\n        """"""Computes and prints the overall classification error and precision, recall, F-score over punctuations.""""""\n        mappings, counter, t_i, p_i = {}, 0, 0, 0\n        total_correct, correct, substitutions, deletions, insertions = 0, 0.0, 0.0, 0.0, 0.0\n        true_pos, false_pos, false_neg = {}, {}, {}\n        with codecs.open(target_path, ""r"", ""utf-8"") as f_target, codecs.open(predicted_path, ""r"", ""utf-8"") as f_predict:\n            target_stream = f_target.read().split()\n            predict_stream = f_predict.read().split()\n            while True:\n                if PUNCTUATION_MAPPING.get(target_stream[t_i], target_stream[t_i]) in PUNCTUATION_VOCABULARY:\n                    # skip multiple consecutive punctuations\n                    target_punct = "" ""\n                    while PUNCTUATION_MAPPING.get(target_stream[t_i], target_stream[t_i]) in PUNCTUATION_VOCABULARY:\n                        target_punct = PUNCTUATION_MAPPING.get(target_stream[t_i], target_stream[t_i])\n                        target_punct = mappings.get(target_punct, target_punct)\n                        t_i += 1\n                else:\n                    target_punct = "" ""\n                if predict_stream[p_i] in PUNCTUATION_VOCABULARY:\n                    predicted_punct = mappings.get(predict_stream[p_i], predict_stream[p_i])\n                    p_i += 1\n                else:\n                    predicted_punct = "" ""\n                is_correct = target_punct == predicted_punct\n                counter += 1\n                total_correct += is_correct\n                if predicted_punct == "" "" and target_punct != "" "":\n                    deletions += 1\n                elif predicted_punct != "" "" and target_punct == "" "":\n                    insertions += 1\n                elif predicted_punct != "" "" and target_punct != "" "" and predicted_punct == target_punct:\n                    correct += 1\n                elif predicted_punct != "" "" and target_punct != "" "" and predicted_punct != target_punct:\n                    substitutions += 1\n                true_pos[target_punct] = true_pos.get(target_punct, 0.0) + float(is_correct)\n                false_pos[predicted_punct] = false_pos.get(predicted_punct, 0.) + float(not is_correct)\n                false_neg[target_punct] = false_neg.get(target_punct, 0.) + float(not is_correct)\n                assert target_stream[t_i] == predict_stream[p_i] or predict_stream[p_i] == ""<unk>"", \\\n                    ""File: %s \\nError: %s (%s) != %s (%s) \\nTarget context: %s \\nPredicted context: %s"" % \\\n                    (target_path, target_stream[t_i], t_i, predict_stream[p_i], p_i,\n                     "" "".join(target_stream[t_i - 2:t_i + 2]), "" "".join(predict_stream[p_i - 2:p_i + 2]))\n                t_i += 1\n                p_i += 1\n                if t_i >= len(target_stream) - 1 and p_i >= len(predict_stream) - 1:\n                    break\n        overall_tp, overall_fp, overall_fn = 0.0, 0.0, 0.0\n        out_str = ""-"" * 46 + ""\\n""\n        out_str += ""{:<16} {:<9} {:<9} {:<9}\\n"".format(""PUNCTUATION"", ""PRECISION"", ""RECALL"", ""F-SCORE"")\n        for p in PUNCTUATION_VOCABULARY:\n            if p == SPACE:\n                continue\n            overall_tp += true_pos.get(p, 0.0)\n            overall_fp += false_pos.get(p, 0.0)\n            overall_fn += false_neg.get(p, 0.0)\n            punctuation = p\n            precision = (true_pos.get(p, 0.0) / (true_pos.get(p, 0.0) + false_pos[p])) if p in false_pos else nan\n            recall = (true_pos.get(p, 0.0) / (true_pos.get(p, 0.0) + false_neg[p])) if p in false_neg else nan\n            f_score = (2. * precision * recall / (precision + recall)) if (precision + recall) > 0 else nan\n            out_str += u""{:<16} {:<9} {:<9} {:<9}\\n"".format(punctuation, ""{:.2f}"".format(precision * 100),\n                                                            ""{:.2f}"".format(recall * 100),\n                                                            ""{:.2f}"".format(f_score * 100))\n        out_str += ""-"" * 46 + ""\\n""\n        pre = overall_tp / (overall_tp + overall_fp) if overall_fp else nan\n        rec = overall_tp / (overall_tp + overall_fn) if overall_fn else nan\n        f1 = (2. * pre * rec) / (pre + rec) if (pre + rec) else nan\n        out_str += ""{:<16} {:<9} {:<9} {:<9}\\n"".format(""Overall"", ""{:.2f}"".format(pre * 100),\n                                                       ""{:.2f}"".format(rec * 100), ""{:.2f}"".format(f1 * 100))\n        err = round((100.0 - float(total_correct) / float(counter - 1) * 100.0), 2)\n        ser = round((substitutions + deletions + insertions) / (correct + substitutions + deletions) * 100, 1)\n        out_str += ""ERR: %s%%\\n"" % err\n        out_str += ""SER: %s%%"" % ser\n        return out_str, f1, err, ser\n'"
utils/CoNLLeval.py,0,"b'import argparse\nimport json\nimport re\nimport sys\nfrom collections import defaultdict\n\n""""""\nBorrowed from: https://github.com/AdolfVonKleist/rnn-slu/blob/master/rnnslu/CoNLLeval.py\n""""""\n\n\nclass CoNLLeval:\n    """"""Evaluate the result of processing CoNLL-2000 shared task\n    Evaluate the result of processing CoNLL-2000 shared tasks. This is a\n    vanilla python port of the original perl script.\n    # usage:     conlleval [-l] [-r] [-d delimiterTag] [-o oTag] < file\n    #            README: http://cnts.uia.ac.be/conll2000/chunking/output.html\n    # options:   l: generate LaTeX output for tables like in\n    #               http://cnts.uia.ac.be/conll2003/ner/example.tex\n    #            r: accept raw result tags (without B- and I- prefix;\n    #                                       assumes one word per chunk)\n    #            d: alternative delimiter tag (default is single space)\n    #            o: alternative outside tag (default is O)\n    # note:      the file should contain lines with items separated\n    #            by $delimiter characters (default space). The final\n    #            two items should contain the correct tag and the\n    #            guessed tag in that order. Sentences should be\n    #            separated from each other by empty lines or lines\n    #            with $boundary fields (default -X-).\n    # url:       http://lcg-www.uia.ac.be/conll2000/chunking/\n    """"""\n\n    def __init__(self, verbose=0, raw=False, delimiter="" "", otag=""O"", boundary=""-X-""):\n        self.verbose = verbose  # verbosity level\n        self.boundary = boundary  # sentence boundary\n        self.correct = None  # current corpus chunk tag (I,O,B)\n        self.correct_chunk = 0  # number of correctly identified chunks\n        self.correct_tags = 0  # number of correct chunk tags\n        self.correct_type = None  # type of current corpus chunk tag (NP,VP,etc.)\n        self.delimiter = delimiter  # field delimiter\n        self.FB1 = 0.0  # FB1 score (Van Rijsbergen 1979)\n        self.accuracy = 0.0\n        self.first_item = None  # first feature (for sentence boundary checks)\n        self.found_correct = 0  # number of chunks in corpus\n        self.found_guessed = 0  # number of identified chunks\n        self.guessed = None  # current guessed chunk tag\n        self.guessed_type = None  # type of current guessed chunk tag\n        self.i = None  # miscellaneous counter\n        self.in_correct = False  # currently processed chunk is correct until now\n        self.last_correct = ""O""  # previous chunk tag in corpus\n        self.latex = 0  # generate LaTeX formatted output\n        self.last_correct_type = """"  # type of previously identified chunk tag\n        self.last_guessed = ""O""  # previously identified chunk tag\n        self.last_guessed_type = """"  # type of previous chunk tag in corpus\n        self.last_type = None  # temporary storage for detecting duplicates\n        self.line = None  # line\n        self.nbr_of_features = -1  # number of features per line\n        self.precision = 0.0  # precision score\n        self.o_tag = otag  # outside tag, default O\n        self.raw = raw  # raw input: add B to every token\n        self.recall = 0.0  # recall score\n        self.token_counter = 0  # token counter (ignores sentence breaks)\n\n        self.correct_chunk = defaultdict(int)  # number of correctly identified chunks per type\n        self.found_correct = defaultdict(int)  # number of chunks in corpus per type\n        self.found_guessed = defaultdict(int)  # number of identified chunks per type\n\n        self.features = []  # features on line\n        self.sorted_types = []  # sorted list of chunk type names\n\n    @staticmethod\n    def endOfChunk(prev_tag, tag, prev_type, tag_type, chunk_end=0):\n        """"""Checks if a chunk ended between the previous and current word.\n        Checks if a chunk ended between the previous and current word.\n        Args:\n            prev_tag (str): Previous chunk tag identifier.\n            tag (str): Current chunk tag identifier.\n            prev_type (str): Previous chunk type identifier.\n            tag_type (str): Current chunk type identifier.\n            chunk_end (int): 0/True true/false identifier.\n        Returns:\n            int: 0/True true/false identifier.\n        """"""\n        if prev_tag == ""B"" and tag == ""B"":\n            chunk_end = True\n        if prev_tag == ""B"" and tag == ""O"":\n            chunk_end = True\n        if prev_tag == ""I"" and tag == ""B"":\n            chunk_end = True\n        if prev_tag == ""I"" and tag == ""O"":\n            chunk_end = True\n        if prev_tag == ""E"" and tag == ""E"":\n            chunk_end = True\n        if prev_tag == ""E"" and tag == ""I"":\n            chunk_end = True\n        if prev_tag == ""E"" and tag == ""O"":\n            chunk_end = True\n        if prev_tag == ""I"" and tag == ""O"":\n            chunk_end = True\n        if prev_tag != ""O"" and prev_tag != ""."" and prev_type != tag_type:\n            chunk_end = True\n        # corrected 1998-12-22: these chunks are assumed to have length 1\n        if prev_tag == ""]"":\n            chunk_end = True\n        if prev_tag == ""["":\n            chunk_end = True\n\n        return chunk_end\n\n    @staticmethod\n    def startOfChunk(prevTag, tag, prevType, tag_type, chunk_start=0):\n        """"""Checks if a chunk started between the previous and current word.\n        Checks if a chunk started between the previous and current word.\n        Args:\n            prevTag (str): Previous chunk tag identifier.\n            tag (str): Current chunk tag identifier.\n            prevType (str): Previous chunk type identifier.\n            tag_type (str): Current chunk type identifier.\n            chunk_start:\n        Returns:\n            int: 0/True true/false identifier.\n        """"""\n        if prevTag == ""B"" and tag == ""B"":\n            chunk_start = True\n        if prevTag == ""I"" and tag == ""B"":\n            chunk_start = True\n        if prevTag == ""O"" and tag == ""B"":\n            chunk_start = True\n        if prevTag == ""O"" and tag == ""I"":\n            chunk_start = True\n        if prevTag == ""E"" and tag == ""E"":\n            chunk_start = True\n        if prevTag == ""E"" and tag == ""I"":\n            chunk_start = True\n        if prevTag == ""O"" and tag == ""E"":\n            chunk_start = True\n        if prevTag == ""O"" and tag == ""I"":\n            chunk_start = True\n        if tag != ""O"" and tag != ""."" and prevType != tag_type:\n            chunk_start = True\n        # corrected 1998-12-22: these chunks are assumed to have length 1\n        if tag == ""["":\n            chunk_start = True\n        if tag == ""]"":\n            chunk_start = True\n        return chunk_start\n\n    def Evaluate(self, infile):\n        """"""Evaluate test outcome for a CoNLLeval shared task.\n        Evaluate test outcome for a CoNLLeval shared task.\n        Args:\n            infile (str): The input file for evaluation.\n        """"""\n        with open(infile, ""r"") as ifp:\n            for line in ifp:\n                line = line.lstrip().rstrip()\n                self.features = re.split(self.delimiter, line)\n                if len(self.features) == 1 and re.match(r""^\\s*$"", self.features[0]):\n                    self.features = []\n                if self.nbr_of_features < 0:\n                    self.nbr_of_features = len(self.features) - 1\n                elif self.nbr_of_features != len(self.features) - 1 and len(self.features) != 0:\n                    raise ValueError(""Unexpected number of features: {0}\\t{1}"".format(len(self.features) + 1,\n                                                                                      self.nbr_of_features + 1))\n                if len(self.features) == 0 or self.features[0] == self.boundary:\n                    self.features = [self.boundary, ""O"", ""O""]\n                if len(self.features) < 2:\n                    raise ValueError(""CoNLLeval: Unexpected number of features in line."")\n\n                if self.raw is True:\n                    if self.features[-1] == self.o_tag:\n                        self.features[-1] = ""O""\n                    if self.features[-2] == self.o_tag:\n                        self.features[-2] = ""O""\n                    if not self.features[-1] == ""O"":\n                        self.features[-1] = ""B-{0}"".format(self.features[-1])\n                    if not self.features[-2] == ""O"":\n                        self.features[-2] = ""B-{0}"".format(self.features[-2])\n                # 20040126 ET code which allows hyphens in the types\n                ffeat = re.search(r""^([^\\-]*)-(.*)$"", self.features[-1])\n                if ffeat:\n                    self.guessed = ffeat.groups()[0]\n                    self.guessed_type = ffeat.groups()[1]\n                else:\n                    self.guessed = self.features[-1]\n                    self.guessed_type = """"\n\n                self.features.pop(-1)\n                ffeat = re.search(r""^([^\\-]*)-(.*)$"", self.features[-1])\n                if ffeat:\n                    self.correct = ffeat.groups()[0]\n                    self.correct_type = ffeat.groups()[1]\n                else:\n                    self.correct = self.features[-1]\n                    self.correct_type = """"\n                self.features.pop(-1)\n\n                if self.guessed_type is None:\n                    self.guessed_type = """"\n                if self.correct_type is None:\n                    self.correct_type = """"\n\n                self.first_item = self.features.pop(0)\n\n                # 1999-06-26 sentence breaks should always be counted as out of chunk\n                if self.first_item == self.boundary:\n                    self.guessed = ""O""\n\n                if self.in_correct is True:\n                    if self.endOfChunk(self.last_correct, self.correct, self.last_correct_type,\n                                       self.correct_type) is True and self.endOfChunk(self.last_guessed, self.guessed,\n                                                                                      self.last_guessed_type,\n                                                                                      self.guessed_type) is True \\\n                            and self.last_guessed_type == self.last_correct_type:\n                        self.in_correct = False\n                        self.correct_chunk[self.last_correct_type] += 1\n                    elif self.endOfChunk(self.last_correct, self.correct, self.last_correct_type,\n                                         self.correct_type) != self.endOfChunk(self.last_guessed, self.guessed,\n                                                                               self.last_guessed_type,\n                                                                               self.guessed_type) \\\n                            or self.guessed_type != self.correct_type:\n                        self.in_correct = False\n\n                if self.startOfChunk(self.last_correct, self.correct, self.last_correct_type,\n                                     self.correct_type) is True and self.startOfChunk(self.last_guessed, self.guessed,\n                                                                                      self.last_guessed_type,\n                                                                                      self.guessed_type) is True \\\n                        and self.guessed_type == self.correct_type:\n                    self.in_correct = True\n\n                if self.startOfChunk(self.last_correct, self.correct, self.last_correct_type,\n                                     self.correct_type) is True:\n                    self.found_correct[self.correct_type] += 1\n\n                if self.startOfChunk(self.last_guessed, self.guessed, self.last_guessed_type,\n                                     self.guessed_type) is True:\n                    self.found_guessed[self.guessed_type] += 1\n\n                if self.first_item != self.boundary:\n                    if self.correct == self.guessed and self.guessed_type == self.correct_type:\n                        self.correct_tags += 1\n                    self.token_counter += 1\n\n                self.last_guessed = self.guessed\n                self.last_correct = self.correct\n                self.last_guessed_type = self.guessed_type\n                self.last_correct_type = self.correct_type\n\n                if self.verbose > 1:\n                    print(""{0} {1} {2} {3} {4} {5} {6}"".format(self.last_guessed, self.last_correct,\n                                                               self.last_guessed_type, self.last_correct_type,\n                                                               self.token_counter, len(self.found_correct.keys()),\n                                                               len(self.found_guessed.keys())))\n\n        if self.in_correct is True:\n            self.correct_chunk[len(self.correct_chunk.keys())] = 0\n            self.correct_chunk[self.last_correct_type] += 1\n\n    def ComputeAccuracy(self):\n        """"""Compute overall precision, recall and FB1 (default values are 0.0).\n        Compute overall precision, recall and FB1 (default values are 0.0).\n        Results:\n            list: accuracy, precision, recall, FB1 float values.\n        """"""\n        if sum(self.found_guessed.values()) > 0:\n            self.precision = 100 * sum(self.correct_chunk.values()) / float(sum(self.found_guessed.values()))\n        if sum(self.found_correct.values()) > 0:\n            self.recall = 100 * sum(self.correct_chunk.values()) / float(sum(self.found_correct.values()))\n        if self.precision + self.recall > 0:\n            self.FB1 = 2 * self.precision * self.recall / (self.precision + self.recall)\n\n        overall = ""processed {0} tokens with {1} phrases; found: {2} phrases; correct: {3}.""\n        overall = overall.format(self.token_counter, sum(self.found_correct.values()), sum(self.found_guessed.values()),\n                                 sum(self.correct_chunk.values()))\n        if self.verbose > 0:\n            print(overall)\n\n        self.accuracy = 100 * self.correct_tags / float(self.token_counter)\n        if self.token_counter > 0 and self.verbose > 0:\n            print(""accuracy:  {0:0.2f}"".format(self.accuracy))\n            print(""precision: {0:0.2f}"".format(self.precision))\n            print(""recall:    {0:0.2f}"".format(self.recall))\n            print(""FB1:       {0:0.2f}"".format(self.FB1))\n\n        return {""accuracy"": self.accuracy, ""precision"": self.precision, ""recall"": self.recall, ""FB1"": self.FB1}\n\n    def conlleval(self, predictions, groundtruth, words, infile):\n        """"""Evaluate the results of one training iteration.\n\n        Evaluate the results of one training iteration.  This now\n        uses the native python port of the CoNLLeval perl script.\n        It computes the accuracy, precision, recall and FB1 scores,\n        and returns these as a dictionary.\n        Args:\n            predictions (list): Predictions from the network.\n            groundtruth (list): Ground truth for evaluation.\n            words (list): Corresponding words for de-referencing.\n            infile:\n        Returns:\n            dict: Accuracy (accuracy), precisions (p), recall (r), and FB1 (f1) scores represented as floats.\n            infile: The inputs written to file in the format understood by the conlleval.pl script and CoNLLeval python\n                    port.\n        """"""\n        ofp = open(infile, ""w"")\n        for sl, sp, sw in zip(groundtruth, predictions, words):\n            ofp.write(u""BOS O O\\n"")\n            for wl, wp, words in zip(sl, sp, sw):\n                line = u""{0} {1} {2}\\n"".format(words, wl, wp)\n                ofp.write(line)\n            ofp.write(u""EOS O O\\n\\n"")\n        ofp.close()\n        self.Evaluate(infile)\n        return self.ComputeAccuracy()\n\n\nif __name__ == ""__main__"":\n\n    example = ""{0} --infile"".format(sys.argv[0])\n    parser = argparse.ArgumentParser(description=example)\n    parser.add_argument(""--infile"", ""-i"", help=""Input CoNLLeval results file."", required=True)\n    parser.add_argument(""--raw"", ""-r"", help=""Accept raw result tags."", default=False, action=""store_true"")\n    parser.add_argument(""--delimiter"", ""-d"", help=""Token delimiter."", default="" "", type=str)\n    parser.add_argument(""--otag"", ""-ot"", help=""Alternative outside tag."", default=""O"", type=str)\n    parser.add_argument(""--boundary"", ""-b"", help=""Boundary tag."", default=""-X-"", type=str)\n    parser.add_argument(""--verbose"", ""-v"", help=""Verbose mode."", default=0, type=int)\n    args = parser.parse_args()\n\n    if args.verbose > 0:\n        for key, val in args.__dict__.iteritems():\n            print(""{0}:  {1}"".format(key, val))\n\n    ce = CoNLLeval(verbose=args.verbose, raw=args.raw, delimiter=args.delimiter, otag=args.otag, boundary=args.boundary)\n    ce.Evaluate(args.infile)\n    results = ce.ComputeAccuracy()\n\n    print()\n    json.dumps(results, indent=4)\n'"
utils/__init__.py,0,b'from utils.logger import *\nfrom utils.CoNLLeval import *\nfrom utils.data_utils import *\n'
utils/common.py,0,"b'import codecs\nimport ujson\nimport re\nimport unicodedata\n\nPAD = ""<PAD>""\nUNK = ""<UNK>""\nNUM = ""<NUM>""\nEND = ""</S>""\nSPACE = ""_SPACE""\n\n\ndef write_json(filename, dataset):\n    with codecs.open(filename, mode=""w"", encoding=""utf-8"") as f:\n        ujson.dump(dataset, f)\n\n\ndef word_convert(word, keep_number=True, lowercase=True):\n    if not keep_number:\n        if is_digit(word):\n            word = NUM\n    if lowercase:\n        word = word.lower()\n    return word\n\n\ndef is_digit(word):\n    try:\n        float(word)\n        return True\n    except ValueError:\n        pass\n    try:\n        unicodedata.numeric(word)\n        return True\n    except (TypeError, ValueError):\n        pass\n    result = re.compile(r\'^[-+]?[0-9]+,[0-9]+$\').match(word)\n    if result:\n        return True\n    return False\n'"
utils/conll2003_prepro.py,0,"b'import os\nimport codecs\nimport numpy as np\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom utils.common import write_json, PAD, UNK, NUM, word_convert\n\nglove_sizes = {\'6B\': int(4e5), \'42B\': int(1.9e6), \'840B\': int(2.2e6), \'2B\': int(1.2e6)}\n\n\ndef raw_dataset_iter(filename, task_name, keep_number, lowercase):\n    with codecs.open(filename, mode=""r"", encoding=""utf-8"") as f:\n        words, tags = [], []\n        for line in f:\n            line = line.lstrip().rstrip()\n            if len(line) == 0 or line.startswith(""-DOCSTART-""):  # means read whole one sentence\n                if len(words) != 0:\n                    yield words, tags\n                    words, tags = [], []\n            else:\n                word, pos, chunk, ner = line.split("" "")\n                if task_name == ""ner"":\n                    tag = ner\n                elif task_name == ""chunk"":\n                    tag = chunk\n                else:\n                    tag = pos\n                word = word_convert(word, keep_number=keep_number, lowercase=lowercase)\n                words.append(word)\n                tags.append(tag)\n\n\ndef load_dataset(filename, task_name, keep_number=False, lowercase=True):\n    dataset = []\n    for words, tags in raw_dataset_iter(filename, task_name, keep_number, lowercase):\n        dataset.append({""words"": words, ""tags"": tags})\n    return dataset\n\n\ndef load_glove_vocab(glove_path, glove_name):\n    vocab = set()\n    total = glove_sizes[glove_name]\n    with codecs.open(glove_path, mode=\'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f, total=total, desc=""Load glove vocabulary""):\n            line = line.lstrip().rstrip().split("" "")\n            vocab.add(line[0])\n    return vocab\n\n\ndef build_word_vocab(datasets):\n    word_counter = Counter()\n    for dataset in datasets:\n        for record in dataset:\n            words = record[""words""]\n            for word in words:\n                word_counter[word] += 1\n    word_vocab = [PAD, UNK, NUM] + [word for word, _ in word_counter.most_common(10000) if word != NUM]\n    word_dict = dict([(word, idx) for idx, word in enumerate(word_vocab)])\n    return word_dict\n\n\ndef build_tag_vocab(datasets, task_name):\n    tag_counter = Counter()\n    for dataset in datasets:\n        for record in dataset:\n            tags = record[""tags""]\n            for tag in tags:\n                tag_counter[tag] += 1\n    if task_name == ""ner"":\n        tag_vocab = [tag for tag, _ in tag_counter.most_common()]  # ""O"" acts as padding\n        tag_dict = dict([(ner, idx) for idx, ner in enumerate(tag_vocab)])\n    else:\n        tag_vocab = [PAD] + [tag for tag, _ in tag_counter.most_common()]  # ""<PAD>"" acts as padding\n        tag_dict = dict([(ner, idx) for idx, ner in enumerate(tag_vocab)])\n    return tag_dict\n\n\ndef build_char_vocab(datasets):\n    char_counter = Counter()\n    for dataset in datasets:\n        for record in dataset:\n            for word in record[""words""]:\n                for char in word:\n                    char_counter[char] += 1\n    word_vocab = [PAD, UNK] + [char for char, _ in char_counter.most_common()]\n    word_dict = dict([(word, idx) for idx, word in enumerate(word_vocab)])\n    return word_dict\n\n\ndef build_word_vocab_pretrained(datasets, glove_vocab):\n    word_counter = Counter()\n    for dataset in datasets:\n        for record in dataset:\n            words = record[""words""]\n            for word in words:\n                word_counter[word] += 1\n    # build word dict\n    word_vocab = [word for word, _ in word_counter.most_common() if word != NUM]\n    word_vocab = [PAD, UNK, NUM] + list(set(word_vocab) & glove_vocab)\n    word_dict = dict([(word, idx) for idx, word in enumerate(word_vocab)])\n    return word_dict\n\n\ndef filter_glove_emb(word_dict, glove_path, glove_name, dim):\n    # filter embeddings\n    vectors = np.zeros([len(word_dict), dim])\n    embeddings = np.zeros([len(word_dict), dim])  # initialize by zeros\n    scale = np.sqrt(3.0 / dim)\n    embeddings[1:3] = np.random.uniform(-scale, scale, [2, dim])  # for NUM and UNK\n    with codecs.open(glove_path, mode=\'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f, total=glove_sizes[glove_name], desc=""Filter glove embeddings""):\n            line = line.lstrip().rstrip().split("" "")\n            word = line[0]\n            vector = [float(x) for x in line[1:]]\n            if word in word_dict:\n                word_idx = word_dict[word]\n                vectors[word_idx] = np.asarray(vector)\n    return vectors\n\n\ndef build_dataset(data, word_dict, char_dict, tag_dict):\n    dataset = []\n    for record in data:\n        chars_list = []\n        words = []\n        for word in record[""words""]:\n            chars = [char_dict[char] if char in char_dict else char_dict[UNK] for char in word]\n            chars_list.append(chars)\n            word = word_convert(word, keep_number=False, lowercase=True)\n            words.append(word_dict[word] if word in word_dict else word_dict[UNK])\n        tags = [tag_dict[tag] for tag in record[""tags""]]\n        dataset.append({""words"": words, ""chars"": chars_list, ""tags"": tags})\n    return dataset\n\n\ndef process_data(config):\n    train_data = load_dataset(os.path.join(config[""raw_path""], ""train.txt""), config[""task_name""])\n    dev_data = load_dataset(os.path.join(config[""raw_path""], ""valid.txt""), config[""task_name""])\n    test_data = load_dataset(os.path.join(config[""raw_path""], ""test.txt""), config[""task_name""])\n    if not os.path.exists(config[""save_path""]):\n        os.makedirs(config[""save_path""])\n    # build vocabulary\n    if not config[""use_pretrained""]:\n        word_dict = build_word_vocab([train_data, dev_data, test_data])\n    else:\n        glove_path = config[""glove_path""].format(config[""glove_name""], config[""emb_dim""])\n        glove_vocab = load_glove_vocab(glove_path, config[""glove_name""])\n        word_dict = build_word_vocab_pretrained([train_data, dev_data, test_data], glove_vocab)\n        vectors = filter_glove_emb(word_dict, glove_path, config[""glove_name""], config[""emb_dim""])\n        np.savez_compressed(config[""pretrained_emb""], embeddings=vectors)\n    tag_dict = build_tag_vocab([train_data, dev_data, test_data], config[""task_name""])\n    # build char dict\n    train_data = load_dataset(os.path.join(config[""raw_path""], ""train.txt""), config[""task_name""], keep_number=True,\n                              lowercase=config[""char_lowercase""])\n    dev_data = load_dataset(os.path.join(config[""raw_path""], ""valid.txt""), config[""task_name""], keep_number=True,\n                            lowercase=config[""char_lowercase""])\n    test_data = load_dataset(os.path.join(config[""raw_path""], ""test.txt""), config[""task_name""], keep_number=True,\n                             lowercase=config[""char_lowercase""])\n    char_dict = build_char_vocab([train_data, dev_data, test_data])\n    # create indices dataset\n    train_set = build_dataset(train_data, word_dict, char_dict, tag_dict)\n    dev_set = build_dataset(dev_data, word_dict, char_dict, tag_dict)\n    test_set = build_dataset(test_data, word_dict, char_dict, tag_dict)\n    vocab = {""word_dict"": word_dict, ""char_dict"": char_dict, ""tag_dict"": tag_dict}\n    # write to file\n    write_json(os.path.join(config[""save_path""], ""vocab.json""), vocab)\n    write_json(os.path.join(config[""save_path""], ""train.json""), train_set)\n    write_json(os.path.join(config[""save_path""], ""dev.json""), dev_set)\n    write_json(os.path.join(config[""save_path""], ""test.json""), test_set)\n'"
utils/data_utils.py,0,"b'import ujson\nimport codecs\nimport random\n\n\ndef load_dataset(filename):\n    with codecs.open(filename, mode=\'r\', encoding=\'utf-8\') as f:\n        dataset = ujson.load(f)\n    return dataset\n\n\ndef pad_sequences(sequences, pad_tok=None, max_length=None):\n    if pad_tok is None:\n        # 0: ""PAD"" for words and chars, ""O"" for tags\n        pad_tok = 0\n    if max_length is None:\n        max_length = max([len(seq) for seq in sequences])\n    sequence_padded, sequence_length = [], []\n    for seq in sequences:\n        seq_ = seq[:max_length] + [pad_tok] * max(max_length - len(seq), 0)\n        sequence_padded.append(seq_)\n        sequence_length.append(min(len(seq), max_length))\n    return sequence_padded, sequence_length\n\n\ndef pad_char_sequences(sequences, max_length=None, max_length_2=None):\n    sequence_padded, sequence_length = [], []\n    if max_length is None:\n        max_length = max(map(lambda x: len(x), sequences))\n    if max_length_2 is None:\n        max_length_2 = max([max(map(lambda x: len(x), seq)) for seq in sequences])\n    for seq in sequences:\n        sp, sl = pad_sequences(seq, max_length=max_length_2)\n        sequence_padded.append(sp)\n        sequence_length.append(sl)\n    sequence_padded, _ = pad_sequences(sequence_padded, pad_tok=[0] * max_length_2, max_length=max_length)\n    sequence_length, _ = pad_sequences(sequence_length, max_length=max_length)\n    return sequence_padded, sequence_length\n\n\ndef process_batch_data(batch_words, batch_chars, batch_tags=None):\n    b_words, b_words_len = pad_sequences(batch_words)\n    b_chars, b_chars_len = pad_char_sequences(batch_chars)\n    if batch_tags is None:\n        return {""words"": b_words, ""chars"": b_chars, ""seq_len"": b_words_len, ""char_seq_len"": b_chars_len,\n                ""batch_size"": len(b_words)}\n    else:\n        b_tags, _ = pad_sequences(batch_tags)\n        return {""words"": b_words, ""chars"": b_chars, ""tags"": b_tags, ""seq_len"": b_words_len, ""char_seq_len"": b_chars_len,\n                ""batch_size"": len(b_words)}\n\n\ndef dataset_batch_iter(dataset, batch_size):\n    batch_words, batch_chars, batch_tags = [], [], []\n    for record in dataset:\n        batch_words.append(record[""words""])\n        batch_chars.append(record[""chars""])\n        batch_tags.append(record[""tags""])\n        if len(batch_words) == batch_size:\n            yield process_batch_data(batch_words, batch_chars, batch_tags)\n            batch_words, batch_chars, batch_tags = [], [], []\n    if len(batch_words) > 0:\n        yield process_batch_data(batch_words, batch_chars, batch_tags)\n\n\ndef batchnize_dataset(data, batch_size=None, shuffle=True):\n    if type(data) == str:\n        dataset = load_dataset(data)\n    else:\n        dataset = data\n    if shuffle:\n        random.shuffle(dataset)\n    batches = []\n    if batch_size is None:\n        for batch in dataset_batch_iter(dataset, len(dataset)):\n            batches.append(batch)\n        return batches[0]\n    else:\n        for batch in dataset_batch_iter(dataset, batch_size):\n            batches.append(batch)\n        return batches\n\n\ndef align_data(data):\n    """"""Given dict with lists, creates aligned strings\n    Args:\n        data: (dict) data[""x""] = [""I"", ""love"", ""you""]\n              (dict) data[""y""] = [""O"", ""O"", ""O""]\n    Returns:\n        data_aligned: (dict) data_align[""x""] = ""I love you""\n                             data_align[""y""] = ""O O    O  ""\n    """"""\n    spacings = [max([len(seq[i]) for seq in data.values()]) for i in range(len(data[list(data.keys())[0]]))]\n    data_aligned = dict()\n    # for each entry, create aligned string\n    for key, seq in data.items():\n        str_aligned = \'\'\n        for token, spacing in zip(seq, spacings):\n            str_aligned += token + \' \' * (spacing - len(token) + 1)\n        data_aligned[key] = str_aligned\n    return data_aligned\n'"
utils/logger.py,0,"b'import time\nimport sys\nimport logging\nimport numpy as np\n\n\ndef get_logger(filename):\n    """"""Return a logger instance that writes in filename\n    Args:\n        filename: (string) path to log.txt\n    Returns:\n        logger: (instance of logger)\n    """"""\n    logger = logging.getLogger(\'logger\')\n    logger.setLevel(logging.DEBUG)\n    logging.basicConfig(format=\'%(message)s\', level=logging.DEBUG)\n    handler = logging.FileHandler(filename)\n    handler.setLevel(logging.DEBUG)\n    handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n    logging.getLogger().addHandler(handler)\n    return logger\n\n\nclass Progbar(object):\n    """"""Progbar class copied from keras (https://github.com/fchollet/keras/)\n    Displays a progress bar.\n    Small edit : added strict arg to update\n    Arguments\n        target: Total number of steps expected.\n        interval: Minimum visual progress update interval (in seconds).\n    """"""\n    def __init__(self, target, width=30, verbose=1):\n        self.width = width\n        self.target = target\n        self.sum_values = {}\n        self.unique_values = []\n        self.start = time.time()\n        self.total_width = 0\n        self.seen_so_far = 0\n        self.verbose = verbose\n\n    def update(self, current, values=None, exact=None, strict=None):\n        """"""Updates the progress bar.\n        Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            exact: List of tuples (name, value_for_last_step).\n                The progress bar will display these values directly.\n        """"""\n        if strict is None:\n            strict = []\n        if exact is None:\n            exact = []\n        if values is None:\n            values = []\n        for k, v in values:\n            if type(v) == int:  # for global steps\n                if k not in self.sum_values:\n                    self.unique_values.append(k)\n                    self.sum_values[k] = v\n                else:\n                    self.sum_values[k] = v\n            else:\n                if k not in self.sum_values:\n                    self.sum_values[k] = [v * (current - self.seen_so_far), current - self.seen_so_far]\n                    self.unique_values.append(k)\n                else:\n                    self.sum_values[k][0] += v * (current - self.seen_so_far)\n                    self.sum_values[k][1] += (current - self.seen_so_far)\n        for k, v in exact:\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = [v, 1]\n\n        for k, v in strict:\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = v\n\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            prev_total_width = self.total_width\n            sys.stdout.write(""\\b"" * prev_total_width)\n            sys.stdout.write(""\\r"")\n            numdigits = int(np.floor(np.log10(self.target))) + 1\n            barstr = \'%%%dd/%%%dd [\' % (numdigits, numdigits)\n            bar = barstr % (current, self.target)\n            prog = float(current)/self.target\n            prog_width = int(self.width*prog)\n            if prog_width > 0:\n                bar += (\'=\'*(prog_width-1))\n                if current < self.target:\n                    bar += \'>\'\n                else:\n                    bar += \'=\'\n            bar += (\'.\'*(self.width-prog_width))\n            bar += \']\'\n            sys.stdout.write(bar)\n            self.total_width = len(bar)\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit*(self.target - current)\n            info = \'\'\n            if current < self.target:\n                info += \' - ETA: %ds\' % eta\n            else:\n                info += \' - %ds\' % (now - self.start)\n            for k in self.unique_values:\n                if type(self.sum_values[k]) is list:\n                    info += \' - %s: %.4f\' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                else:\n                    info += \' - %s: %s\' % (k, self.sum_values[k])\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += ((prev_total_width-self.total_width) * \' \')\n            sys.stdout.write(info)\n            sys.stdout.flush()\n            if current >= self.target:\n                sys.stdout.write(""\\n"")\n        if self.verbose == 2:\n            if current >= self.target:\n                info = \'%ds\' % (now - self.start)\n                for k in self.unique_values:\n                    info += \' - %s: %.4f\' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                sys.stdout.write(info + ""\\n"")\n\n    def add(self, n, values=None):\n        if values is None:\n            values = []\n        self.update(self.seen_so_far+n, values)\n'"
utils/media_prepro.py,0,"b'import os\nimport codecs\nfrom collections import Counter\nfrom utils.common import write_json, PAD, UNK, word_convert\n\n\ndef raw_dataset_iter(filename, encoding=""utf-8""):\n    with codecs.open(filename, mode=""r"", encoding=encoding) as f:\n        words, tags = [], []\n        for line in f:\n            line = line.lstrip().rstrip()\n            if len(line) == 0 or line.startswith(""--------------""):  # means read whole one sentence\n                if len(words) != 0:\n                    yield words, tags\n                    words, tags = [], []\n            else:\n                _, word, tag = line.split(""\\t"")\n                word = word_convert(word)\n                words.append(word)\n                tags.append(tag)\n\n\ndef load_dataset(filename, encoding=""utf-8""):\n    dataset = []\n    for words, tags in raw_dataset_iter(filename, encoding):\n        dataset.append({""words"": words, ""tags"": tags})\n    return dataset\n\n\ndef build_vocab(datasets):\n    char_counter = Counter()\n    word_counter = Counter()\n    tag_counter = Counter()\n    for dataset in datasets:\n        for record in dataset:\n            words = record[""words""]\n            for word in words:\n                word_counter[word] += 1\n                for char in word:\n                    char_counter[char] += 1\n            tags = record[""tags""]\n            for tag in tags:\n                tag_counter[tag] += 1\n    word_vocab = [PAD, UNK] + [word for word, _ in word_counter.most_common()]\n    word_dict = dict([(word, idx) for idx, word in enumerate(word_vocab)])\n    char_vocab = [PAD, UNK] + [char for char, _ in char_counter.most_common()]\n    char_dict = dict([(char, idx) for idx, char in enumerate(char_vocab)])\n    tag_vocab = [tag for tag, _ in tag_counter.most_common()]\n    tag_dict = dict([(tag, idx) for idx, tag in enumerate(tag_vocab)])\n    return word_dict, char_dict, tag_dict\n\n\ndef build_dataset(data, word_dict, char_dict, tag_dict):\n    dataset = []\n    for record in data:\n        chars_list = []\n        for word in record[""words""]:\n            chars = [char_dict[char] if char in char_dict else char_dict[UNK] for char in word]\n            chars_list.append(chars)\n        words = [word_dict[word] if word in word_dict else word_dict[UNK] for word in record[""words""]]\n        tags = [tag_dict[tag] for tag in record[""tags""]]\n        dataset.append({""words"": words, ""chars"": chars_list, ""tags"": tags})\n    return dataset\n\n\ndef process_data(config):\n    # load raw data\n    train_data = load_dataset(os.path.join(config[""raw_path""], ""train.txt""))\n    dev_data = load_dataset(os.path.join(config[""raw_path""], ""valid.txt""))\n    test_data = load_dataset(os.path.join(config[""raw_path""], ""test.txt""))\n    # build vocabulary\n    word_dict, char_dict, _ = build_vocab([train_data, dev_data])\n    *_, tag_dict = build_vocab([train_data, dev_data, test_data])\n    # create indices dataset\n    train_set = build_dataset(train_data, word_dict, char_dict, tag_dict)\n    dev_set = build_dataset(dev_data, word_dict, char_dict, tag_dict)\n    test_set = build_dataset(test_data, word_dict, char_dict, tag_dict)\n    vocab = {""word_dict"": word_dict, ""char_dict"": char_dict, ""tag_dict"": tag_dict}\n    # write to file\n    if not os.path.exists(config[""save_path""]):\n        os.makedirs(config[""save_path""])\n    write_json(os.path.join(config[""save_path""], ""vocab.json""), vocab)\n    write_json(os.path.join(config[""save_path""], ""train.json""), train_set)\n    write_json(os.path.join(config[""save_path""], ""dev.json""), dev_set)\n    write_json(os.path.join(config[""save_path""], ""test.json""), test_set)\n'"
utils/punct_prepro.py,0,"b'import os\nfrom tqdm import tqdm\nfrom collections import Counter\nimport numpy as np\nimport codecs\nimport re\nfrom utils.common import SPACE, UNK, PAD, NUM, END, write_json\n\n# pre-set number of records in different glove embeddings\nglove_sizes = {\'6B\': int(4e5), \'42B\': int(1.9e6), \'840B\': int(2.2e6), \'2B\': int(1.2e6)}\n\n# Comma, period & question mark only:\nPUNCTUATION_VOCABULARY = [SPACE, "",COMMA"", "".PERIOD"", ""?QUESTIONMARK""]\nPUNCTUATION_MAPPING = {""!EXCLAMATIONMARK"": "".PERIOD"", "":COLON"": "",COMMA"", "";SEMICOLON"": "".PERIOD"", ""-DASH"": "",COMMA""}\n\nEOS_TOKENS = {"".PERIOD"", ""?QUESTIONMARK"", ""!EXCLAMATIONMARK""}\n# punctuations that are not included in vocabulary nor mapping, must be added to CRAP_TOKENS\nCRAP_TOKENS = {""<doc>"", ""<doc.>""}\n\n\ndef is_number(word):\n    numbers = re.compile(r""\\d"")\n    return len(numbers.sub("""", word)) / len(word) < 0.6\n\n\ndef build_vocab_list(data_files, min_word_count, min_char_count, max_vocab_size):\n    word_counter = Counter()\n    char_counter = Counter()\n    for file in data_files:\n        with codecs.open(file, mode=""r"", encoding=""utf-8"") as f:\n            for line in f:\n                for word in line.lstrip().rstrip().split():\n                    if word in CRAP_TOKENS or word in PUNCTUATION_VOCABULARY or word in PUNCTUATION_MAPPING:\n                        continue\n                    if is_number(word):\n                        word_counter[NUM] += 1\n                        for char in word:\n                            char_counter[char] += 1\n                        continue\n                    word_counter[word] += 1\n                    for char in word:\n                        char_counter[char] += 1\n    word_vocab = [word for word, count in word_counter.most_common() if count >= min_word_count and word != UNK and\n                  word != NUM][:max_vocab_size]\n    char_vocab = [char for char, count in char_counter.most_common() if count >= min_char_count and char != UNK]\n    return word_vocab, char_vocab\n\n\ndef build_vocabulary(word_vocab, char_vocab):\n    if NUM not in word_vocab:\n        word_vocab.append(NUM)\n    if END not in word_vocab:\n        word_vocab.append(END)\n    if UNK not in word_vocab:\n        word_vocab.append(UNK)\n    word_dict = dict([(word, idx) for idx, word in enumerate(word_vocab)])\n    if END not in char_vocab:\n        char_vocab.append(END)\n    if UNK not in char_vocab:\n        char_vocab.append(UNK)\n    if PAD not in char_vocab:\n        char_vocab = [PAD] + char_vocab\n    char_dict = dict([(char, idx) for idx, char in enumerate(char_vocab)])\n    return word_dict, char_dict\n\n\ndef load_glove_vocab(glove_path, glove_name):\n    vocab = set()\n    total = glove_sizes[glove_name]\n    with codecs.open(glove_path, mode=\'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f, total=total, desc=""Load glove vocabulary""):\n            line = line.lstrip().rstrip().split("" "")\n            vocab.add(line[0])\n    return vocab\n\n\ndef filter_glove_emb(word_dict, glove_path, glove_name, dim):\n    scale = np.sqrt(3.0 / dim)\n    vectors = np.random.uniform(-scale, scale, [len(word_dict), dim])\n    mask = np.zeros([len(word_dict)])\n    with codecs.open(glove_path, mode=\'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f, total=glove_sizes[glove_name], desc=""Filter glove embeddings""):\n            line = line.lstrip().rstrip().split("" "")\n            word = line[0]\n            vector = [float(x) for x in line[1:]]\n            if word in word_dict:\n                word_idx = word_dict[word]\n                mask[word_idx] = 1\n                vectors[word_idx] = np.asarray(vector)\n            # since tokens in train sets are lowercase\n            elif word.lower() in word_dict and mask[word_dict[word.lower()]] == 0:\n                word = word.lower()\n                word_idx = word_dict[word]\n                mask[word_idx] = 1\n                vectors[word_idx] = np.asarray(vector)\n    return vectors\n\n\ndef build_dataset(data_files, word_dict, char_dict, punct_dict, max_sequence_len):\n    """"""\n    data will consist of two sets of aligned sub-sequences (words and punctuations) of MAX_SEQUENCE_LEN tokens\n    (actually punctuation sequence will be 1 element shorter).\n    If a sentence is cut, then it will be added to next subsequence entirely\n    (words before the cut belong to both sequences)\n    """"""\n    dataset = []\n    current_words, current_chars, current_punctuations = [], [], []\n    last_eos_idx = 0  # if it\'s still 0 when MAX_SEQUENCE_LEN is reached, then the sentence is too long and skipped.\n    last_token_was_punctuation = True  # skip first token if it\'s punctuation\n    # if a sentence does not fit into subsequence, then we need to skip tokens until we find a new sentence\n    skip_until_eos = False\n    for file in data_files:\n        with codecs.open(file, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                for token in line.split():\n                    # First map oov punctuations to known punctuations\n                    if token in PUNCTUATION_MAPPING:\n                        token = PUNCTUATION_MAPPING[token]\n                    if skip_until_eos:\n                        if token in EOS_TOKENS:\n                            skip_until_eos = False\n                        continue\n                    elif token in CRAP_TOKENS:\n                        continue\n                    elif token in punct_dict:\n                        # if we encounter sequences like: ""... !EXLAMATIONMARK .PERIOD ..."",\n                        # then we only use the first punctuation and skip the ones that follow\n                        if last_token_was_punctuation:\n                            continue\n                        if token in EOS_TOKENS:\n                            last_eos_idx = len(current_punctuations)  # no -1, because the token is not added yet\n                        punctuation = punct_dict[token]\n                        current_punctuations.append(punctuation)\n                        last_token_was_punctuation = True\n                    else:\n                        if not last_token_was_punctuation:\n                            current_punctuations.append(punct_dict[SPACE])\n                        chars = []\n                        for c in token:\n                            c = char_dict.get(c, char_dict[UNK])\n                            chars.append(c)\n                        if is_number(token):\n                            token = NUM\n                        word = word_dict.get(token, word_dict[UNK])\n                        current_words.append(word)\n                        current_chars.append(chars)\n                        last_token_was_punctuation = False\n                    if len(current_words) == max_sequence_len:  # this also means, that last token was a word\n                        assert len(current_words) == len(current_punctuations) + 1, \\\n                            ""#words: %d; #punctuations: %d"" % (len(current_words), len(current_punctuations))\n                        # Sentence did not fit into subsequence - skip it\n                        if last_eos_idx == 0:\n                            skip_until_eos = True\n                            current_words = []\n                            current_chars = []\n                            current_punctuations = []\n                            # next sequence starts with a new sentence, so is preceded by eos which is punctuation\n                            last_token_was_punctuation = True\n                        else:\n                            subsequence = {""words"": current_words[:-1] + [word_dict[END]],\n                                           ""chars"": current_chars[:-1] + [[char_dict[END]]],\n                                           ""tags"": current_punctuations}\n                            dataset.append(subsequence)\n                            # Carry unfinished sentence to next subsequence\n                            current_words = current_words[last_eos_idx + 1:]\n                            current_chars = current_chars[last_eos_idx + 1:]\n                            current_punctuations = current_punctuations[last_eos_idx + 1:]\n                        last_eos_idx = 0  # sequence always starts with a new sentence\n    return dataset\n\n\ndef process_data(config):\n    train_file = os.path.join(config[""raw_path""], ""train.txt"")\n    dev_file = os.path.join(config[""raw_path""], ""dev.txt"")\n    ref_file = os.path.join(config[""raw_path""], ""ref.txt"")\n    asr_file = os.path.join(config[""raw_path""], ""asr.txt"")\n    if not os.path.exists(config[""save_path""]):\n        os.makedirs(config[""save_path""])\n    # build vocabulary\n    word_vocab, char_vocab = build_vocab_list([train_file], config[""min_word_count""], config[""min_char_count""],\n                                              config[""max_vocab_size""])\n    if not config[""use_pretrained""]:\n        word_dict, char_dict = build_vocabulary(word_vocab, char_vocab)\n    else:\n        glove_path = config[""glove_path""].format(config[""glove_name""], config[""emb_dim""])\n        glove_vocab = load_glove_vocab(glove_path, config[""glove_name""])\n        glove_vocab = glove_vocab & {word.lower() for word in glove_vocab}\n        word_vocab = [word for word in word_vocab if word in glove_vocab]\n        word_dict, char_dict = build_vocabulary(word_vocab, char_vocab)\n        tmp_word_dict = word_dict.copy()\n        del tmp_word_dict[UNK], tmp_word_dict[NUM], tmp_word_dict[END]\n        vectors = filter_glove_emb(tmp_word_dict, glove_path, config[""glove_name""], config[""emb_dim""])\n        np.savez_compressed(config[""pretrained_emb""], embeddings=vectors)\n    # create indices dataset\n    punct_dict = dict([(punct, idx) for idx, punct in enumerate(PUNCTUATION_VOCABULARY)])\n    train_set = build_dataset([train_file], word_dict, char_dict, punct_dict, config[""max_sequence_len""])\n    dev_set = build_dataset([dev_file], word_dict, char_dict, punct_dict, config[""max_sequence_len""])\n    ref_set = build_dataset([ref_file], word_dict, char_dict, punct_dict, config[""max_sequence_len""])\n    asr_set = build_dataset([asr_file], word_dict, char_dict, punct_dict, config[""max_sequence_len""])\n    vocab = {""word_dict"": word_dict, ""char_dict"": char_dict, ""tag_dict"": punct_dict}\n    # write to file\n    write_json(config[""vocab""], vocab)\n    write_json(config[""train_set""], train_set)\n    write_json(config[""dev_set""], dev_set)\n    write_json(config[""ref_set""], ref_set)\n    write_json(config[""asr_set""], asr_set)\n'"
data/raw/LREC/converter.py,0,"b'import sys\r\nimport codecs\r\n\r\nmapping = {""COMMA"": "",COMMA"", ""PERIOD"": "".PERIOD"", ""QUESTION"": ""?QUESTIONMARK"", ""O"": """"}\r\ncounts = dict((p, 0) for p in mapping)\r\n\r\nwith codecs.open(sys.argv[1], \'r\', \'utf-8\', \'ignore\') as f_in, \\\r\n        codecs.open(sys.argv[2], \'w\', \'utf-8\') as f_out:\r\n    for i, line in enumerate(f_in):\r\n\r\n        line = line.replace(\'?\', \'\')\r\n\r\n        parts = line.split()\r\n\r\n        if len(parts) == 0:\r\n            continue\r\n\r\n        if len(parts) == 1:\r\n            word = """"\r\n            punct = parts[0]\r\n        else:\r\n            word, punct = parts\r\n\r\n        counts[punct] += 1\r\n\r\n        f_out.write(""%s %s "" % (word, mapping[punct]))\r\n\r\nprint(""Counts:"")\r\nfor p, c in counts.items():\r\n    print(""%s: %d"" % (p, c))\r\n'"
