file_path,api_count,code
batch_test.py,9,"b'import time\nimport os\nimport argparse\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport neuralgym as ng\n\nfrom inpaint_model import InpaintCAModel\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--flist\', default=\'\', type=str,\n    help=\'The filenames of image to be processed: input, mask, output.\')\nparser.add_argument(\n    \'--image_height\', default=-1, type=int,\n    help=\'The height of images should be defined, otherwise batch mode is not\'\n    \' supported.\')\nparser.add_argument(\n    \'--image_width\', default=-1, type=int,\n    help=\'The width of images should be defined, otherwise batch mode is not\'\n    \' supported.\')\nparser.add_argument(\n    \'--checkpoint_dir\', default=\'\', type=str,\n    help=\'The directory of tensorflow checkpoint.\')\n\n\nif __name__ == ""__main__"":\n    FLAGS = ng.Config(\'inpaint.yml\')\n    ng.get_gpus(1)\n    # os.environ[\'CUDA_VISIBLE_DEVICES\'] =\'\'\n    args = parser.parse_args()\n\n    sess_config = tf.ConfigProto()\n    sess_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=sess_config)\n\n    model = InpaintCAModel()\n    input_image_ph = tf.placeholder(\n        tf.float32, shape=(1, args.image_height, args.image_width*2, 3))\n    output = model.build_server_graph(FLAGS, input_image_ph)\n    output = (output + 1.) * 127.5\n    output = tf.reverse(output, [-1])\n    output = tf.saturate_cast(output, tf.uint8)\n    vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    assign_ops = []\n    for var in vars_list:\n        vname = var.name\n        from_name = vname\n        var_value = tf.contrib.framework.load_variable(\n            args.checkpoint_dir, from_name)\n        assign_ops.append(tf.assign(var, var_value))\n    sess.run(assign_ops)\n    print(\'Model loaded.\')\n\n    with open(args.flist, \'r\') as f:\n        lines = f.read().splitlines()\n    t = time.time()\n    for line in lines:\n    # for i in range(100):\n        image, mask, out = line.split()\n        base = os.path.basename(mask)\n\n        image = cv2.imread(image)\n        mask = cv2.imread(mask)\n        image = cv2.resize(image, (args.image_width, args.image_height))\n        mask = cv2.resize(mask, (args.image_width, args.image_height))\n        # cv2.imwrite(out, image*(1-mask/255.) + mask)\n        # # continue\n        # image = np.zeros((128, 256, 3))\n        # mask = np.zeros((128, 256, 3))\n\n        assert image.shape == mask.shape\n\n        h, w, _ = image.shape\n        grid = 4\n        image = image[:h//grid*grid, :w//grid*grid, :]\n        mask = mask[:h//grid*grid, :w//grid*grid, :]\n        print(\'Shape of image: {}\'.format(image.shape))\n\n        image = np.expand_dims(image, 0)\n        mask = np.expand_dims(mask, 0)\n        input_image = np.concatenate([image, mask], axis=2)\n\n        # load pretrained model\n        result = sess.run(output, feed_dict={input_image_ph: input_image})\n        print(\'Processed: {}\'.format(out))\n        cv2.imwrite(out, result[0][:, :, ::-1])\n\n    print(\'Time total: {}\'.format(time.time() - t))\n'"
guided_batch_test.py,9,"b'import time\nimport os\nimport argparse\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport neuralgym as ng\n\nfrom inpaint_model import InpaintCAModel\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \'--flist\', default=\'\', type=str,\n    help=\'The filenames of image to be processed: input, mask, output.\')\nparser.add_argument(\n    \'--image_height\', default=-1, type=int,\n    help=\'The height of images should be defined, otherwise batch mode is not\'\n    \' supported.\')\nparser.add_argument(\n    \'--image_width\', default=-1, type=int,\n    help=\'The width of images should be defined, otherwise batch mode is not\'\n    \' supported.\')\nparser.add_argument(\n    \'--checkpoint_dir\', default=\'\', type=str,\n    help=\'The directory of tensorflow checkpoint.\')\n\n\nif __name__ == ""__main__"":\n    ng.get_gpus(1)\n    # os.environ[\'CUDA_VISIBLE_DEVICES\'] =\'\'\n    args = parser.parse_args()\n\n    sess_config = tf.ConfigProto()\n    sess_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=sess_config)\n\n    model = InpaintCAModel()\n    input_image_ph = tf.placeholder(\n        tf.float32, shape=(1, args.image_height, args.image_width*3, 3))\n    output = model.build_server_graph(input_image_ph)\n    output = (output + 1.) * 127.5\n    output = tf.reverse(output, [-1])\n    output = tf.saturate_cast(output, tf.uint8)\n    vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    assign_ops = []\n    for var in vars_list:\n        vname = var.name\n        from_name = vname\n        var_value = tf.contrib.framework.load_variable(\n            args.checkpoint_dir, from_name)\n        assign_ops.append(tf.assign(var, var_value))\n    sess.run(assign_ops)\n    print(\'Model loaded.\')\n\n    with open(args.flist, \'r\') as f:\n        lines = f.read().splitlines()\n    t = time.time()\n    for line in lines:\n    # for i in range(100):\n        image, mask, out = line.split()\n        base = os.path.basename(mask)\n\n        guidance = cv2.imread(image[:-4] + \'_edge.jpg\')\n        image = cv2.imread(image)\n        mask = cv2.imread(mask)\n        image = cv2.resize(image, (args.image_width, args.image_height))\n        guidance = cv2.resize(guidance, (args.image_width, args.image_height))\n        mask = cv2.resize(mask, (args.image_width, args.image_height))\n        # cv2.imwrite(out, image*(1-mask/255.) + mask)\n        # # continue\n        # image = np.zeros((128, 256, 3))\n        # mask = np.zeros((128, 256, 3))\n\n        assert image.shape == mask.shape\n\n        h, w, _ = image.shape\n        grid = 4\n        image = image[:h//grid*grid, :w//grid*grid, :]\n        mask = mask[:h//grid*grid, :w//grid*grid, :]\n        guidance = guidance[:h//grid*grid, :w//grid*grid, :]\n        print(\'Shape of image: {}\'.format(image.shape))\n\n        image = np.expand_dims(image, 0)\n        guidance = np.expand_dims(guidance, 0)\n        mask = np.expand_dims(mask, 0)\n        input_image = np.concatenate([image, guidance, mask], axis=2)\n\n        # load pretrained model\n        result = sess.run(output, feed_dict={input_image_ph: input_image})\n        print(\'Processed: {}\'.format(out))\n        cv2.imwrite(out, result[0][:, :, ::-1])\n\n    print(\'Time total: {}\'.format(time.time() - t))\n'"
inpaint_model.py,45,"b'"""""" common model for DCGAN """"""\nimport logging\n\nimport cv2\nimport neuralgym as ng\nimport tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops import arg_scope\n\nfrom neuralgym.models import Model\nfrom neuralgym.ops.summary_ops import scalar_summary, images_summary\nfrom neuralgym.ops.summary_ops import gradients_summary\nfrom neuralgym.ops.layers import flatten, resize\nfrom neuralgym.ops.gan_ops import gan_hinge_loss\nfrom neuralgym.ops.gan_ops import random_interpolates\n\nfrom inpaint_ops import gen_conv, gen_deconv, dis_conv\nfrom inpaint_ops import random_bbox, bbox2mask, local_patch, brush_stroke_mask\nfrom inpaint_ops import resize_mask_like, contextual_attention\n\n\nlogger = logging.getLogger()\n\n\nclass InpaintCAModel(Model):\n    def __init__(self):\n        super().__init__(\'InpaintCAModel\')\n\n    def build_inpaint_net(self, x, mask, reuse=False,\n                          training=True, padding=\'SAME\', name=\'inpaint_net\'):\n        """"""Inpaint network.\n\n        Args:\n            x: incomplete image, [-1, 1]\n            mask: mask region {0, 1}\n        Returns:\n            [-1, 1] as predicted image\n        """"""\n        xin = x\n        offset_flow = None\n        ones_x = tf.ones_like(x)[:, :, :, 0:1]\n        x = tf.concat([x, ones_x, ones_x*mask], axis=3)\n\n        # two stage network\n        cnum = 48\n        with tf.variable_scope(name, reuse=reuse), \\\n                arg_scope([gen_conv, gen_deconv],\n                          training=training, padding=padding):\n            # stage1\n            x = gen_conv(x, cnum, 5, 1, name=\'conv1\')\n            x = gen_conv(x, 2*cnum, 3, 2, name=\'conv2_downsample\')\n            x = gen_conv(x, 2*cnum, 3, 1, name=\'conv3\')\n            x = gen_conv(x, 4*cnum, 3, 2, name=\'conv4_downsample\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'conv5\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'conv6\')\n            mask_s = resize_mask_like(mask, x)\n            x = gen_conv(x, 4*cnum, 3, rate=2, name=\'conv7_atrous\')\n            x = gen_conv(x, 4*cnum, 3, rate=4, name=\'conv8_atrous\')\n            x = gen_conv(x, 4*cnum, 3, rate=8, name=\'conv9_atrous\')\n            x = gen_conv(x, 4*cnum, 3, rate=16, name=\'conv10_atrous\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'conv11\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'conv12\')\n            x = gen_deconv(x, 2*cnum, name=\'conv13_upsample\')\n            x = gen_conv(x, 2*cnum, 3, 1, name=\'conv14\')\n            x = gen_deconv(x, cnum, name=\'conv15_upsample\')\n            x = gen_conv(x, cnum//2, 3, 1, name=\'conv16\')\n            x = gen_conv(x, 3, 3, 1, activation=None, name=\'conv17\')\n            x = tf.nn.tanh(x)\n            x_stage1 = x\n\n            # stage2, paste result as input\n            x = x*mask + xin[:, :, :, 0:3]*(1.-mask)\n            x.set_shape(xin[:, :, :, 0:3].get_shape().as_list())\n            # conv branch\n            # xnow = tf.concat([x, ones_x, ones_x*mask], axis=3)\n            xnow = x\n            x = gen_conv(xnow, cnum, 5, 1, name=\'xconv1\')\n            x = gen_conv(x, cnum, 3, 2, name=\'xconv2_downsample\')\n            x = gen_conv(x, 2*cnum, 3, 1, name=\'xconv3\')\n            x = gen_conv(x, 2*cnum, 3, 2, name=\'xconv4_downsample\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'xconv5\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'xconv6\')\n            x = gen_conv(x, 4*cnum, 3, rate=2, name=\'xconv7_atrous\')\n            x = gen_conv(x, 4*cnum, 3, rate=4, name=\'xconv8_atrous\')\n            x = gen_conv(x, 4*cnum, 3, rate=8, name=\'xconv9_atrous\')\n            x = gen_conv(x, 4*cnum, 3, rate=16, name=\'xconv10_atrous\')\n            x_hallu = x\n            # attention branch\n            x = gen_conv(xnow, cnum, 5, 1, name=\'pmconv1\')\n            x = gen_conv(x, cnum, 3, 2, name=\'pmconv2_downsample\')\n            x = gen_conv(x, 2*cnum, 3, 1, name=\'pmconv3\')\n            x = gen_conv(x, 4*cnum, 3, 2, name=\'pmconv4_downsample\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'pmconv5\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'pmconv6\',\n                                activation=tf.nn.relu)\n            x, offset_flow = contextual_attention(x, x, mask_s, 3, 1, rate=2)\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'pmconv9\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'pmconv10\')\n            pm = x\n            x = tf.concat([x_hallu, pm], axis=3)\n\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'allconv11\')\n            x = gen_conv(x, 4*cnum, 3, 1, name=\'allconv12\')\n            x = gen_deconv(x, 2*cnum, name=\'allconv13_upsample\')\n            x = gen_conv(x, 2*cnum, 3, 1, name=\'allconv14\')\n            x = gen_deconv(x, cnum, name=\'allconv15_upsample\')\n            x = gen_conv(x, cnum//2, 3, 1, name=\'allconv16\')\n            x = gen_conv(x, 3, 3, 1, activation=None, name=\'allconv17\')\n            x = tf.nn.tanh(x)\n            x_stage2 = x\n        return x_stage1, x_stage2, offset_flow\n\n    def build_sn_patch_gan_discriminator(self, x, reuse=False, training=True):\n        with tf.variable_scope(\'sn_patch_gan\', reuse=reuse):\n            cnum = 64\n            x = dis_conv(x, cnum, name=\'conv1\', training=training)\n            x = dis_conv(x, cnum*2, name=\'conv2\', training=training)\n            x = dis_conv(x, cnum*4, name=\'conv3\', training=training)\n            x = dis_conv(x, cnum*4, name=\'conv4\', training=training)\n            x = dis_conv(x, cnum*4, name=\'conv5\', training=training)\n            x = dis_conv(x, cnum*4, name=\'conv6\', training=training)\n            x = flatten(x, name=\'flatten\')\n            return x\n\n    def build_gan_discriminator(\n            self, batch, reuse=False, training=True):\n        with tf.variable_scope(\'discriminator\', reuse=reuse):\n            d = self.build_sn_patch_gan_discriminator(\n                batch, reuse=reuse, training=training)\n            return d\n\n    def build_graph_with_losses(\n            self, FLAGS, batch_data, training=True, summary=False,\n            reuse=False):\n        if FLAGS.guided:\n            batch_data, edge = batch_data\n            edge = edge[:, :, :, 0:1] / 255.\n            edge = tf.cast(edge > FLAGS.edge_threshold, tf.float32)\n        batch_pos = batch_data / 127.5 - 1.\n        # generate mask, 1 represents masked point\n        bbox = random_bbox(FLAGS)\n        regular_mask = bbox2mask(FLAGS, bbox, name=\'mask_c\')\n        irregular_mask = brush_stroke_mask(FLAGS, name=\'mask_c\')\n        mask = tf.cast(\n            tf.logical_or(\n                tf.cast(irregular_mask, tf.bool),\n                tf.cast(regular_mask, tf.bool),\n            ),\n            tf.float32\n        )\n\n        batch_incomplete = batch_pos*(1.-mask)\n        if FLAGS.guided:\n            edge = edge * mask\n            xin = tf.concat([batch_incomplete, edge], axis=3)\n        else:\n            xin = batch_incomplete\n        x1, x2, offset_flow = self.build_inpaint_net(\n            xin, mask, reuse=reuse, training=training,\n            padding=FLAGS.padding)\n        batch_predicted = x2\n        losses = {}\n        # apply mask and complete image\n        batch_complete = batch_predicted*mask + batch_incomplete*(1.-mask)\n        # local patches\n        losses[\'ae_loss\'] = FLAGS.l1_loss_alpha * tf.reduce_mean(tf.abs(batch_pos - x1))\n        losses[\'ae_loss\'] += FLAGS.l1_loss_alpha * tf.reduce_mean(tf.abs(batch_pos - x2))\n        if summary:\n            scalar_summary(\'losses/ae_loss\', losses[\'ae_loss\'])\n            if FLAGS.guided:\n                viz_img = [\n                    batch_pos,\n                    batch_incomplete + edge,\n                    batch_complete]\n            else:\n                viz_img = [batch_pos, batch_incomplete, batch_complete]\n            if offset_flow is not None:\n                viz_img.append(\n                    resize(offset_flow, scale=4,\n                           func=tf.image.resize_bilinear))\n            images_summary(\n                tf.concat(viz_img, axis=2),\n                \'raw_incomplete_predicted_complete\', FLAGS.viz_max_out)\n\n        # gan\n        batch_pos_neg = tf.concat([batch_pos, batch_complete], axis=0)\n        if FLAGS.gan_with_mask:\n            batch_pos_neg = tf.concat([batch_pos_neg, tf.tile(mask, [FLAGS.batch_size*2, 1, 1, 1])], axis=3)\n        if FLAGS.guided:\n            # conditional GANs\n            batch_pos_neg = tf.concat([batch_pos_neg, tf.tile(edge, [2, 1, 1, 1])], axis=3)\n        # wgan with gradient penalty\n        if FLAGS.gan == \'sngan\':\n            pos_neg = self.build_gan_discriminator(batch_pos_neg, training=training, reuse=reuse)\n            pos, neg = tf.split(pos_neg, 2)\n            g_loss, d_loss = gan_hinge_loss(pos, neg)\n            losses[\'g_loss\'] = g_loss\n            losses[\'d_loss\'] = d_loss\n        else:\n            raise NotImplementedError(\'{} not implemented.\'.format(FLAGS.gan))\n        if summary:\n            # summary the magnitude of gradients from different losses w.r.t. predicted image\n            gradients_summary(losses[\'g_loss\'], batch_predicted, name=\'g_loss\')\n            gradients_summary(losses[\'g_loss\'], x2, name=\'g_loss_to_x2\')\n            # gradients_summary(losses[\'ae_loss\'], x1, name=\'ae_loss_to_x1\')\n            gradients_summary(losses[\'ae_loss\'], x2, name=\'ae_loss_to_x2\')\n        losses[\'g_loss\'] = FLAGS.gan_loss_alpha * losses[\'g_loss\']\n        if FLAGS.ae_loss:\n            losses[\'g_loss\'] += losses[\'ae_loss\']\n        g_vars = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, \'inpaint_net\')\n        d_vars = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, \'discriminator\')\n        return g_vars, d_vars, losses\n\n    def build_infer_graph(self, FLAGS, batch_data, bbox=None, name=\'val\'):\n        """"""\n        """"""\n        if FLAGS.guided:\n            batch_data, edge = batch_data\n            edge = edge[:, :, :, 0:1] / 255.\n            edge = tf.cast(edge > FLAGS.edge_threshold, tf.float32)\n        regular_mask = bbox2mask(FLAGS, bbox, name=\'mask_c\')\n        irregular_mask = brush_stroke_mask(FLAGS, name=\'mask_c\')\n        mask = tf.cast(\n            tf.logical_or(\n                tf.cast(irregular_mask, tf.bool),\n                tf.cast(regular_mask, tf.bool),\n            ),\n            tf.float32\n        )\n\n        batch_pos = batch_data / 127.5 - 1.\n        batch_incomplete = batch_pos*(1.-mask)\n        if FLAGS.guided:\n            edge = edge * mask\n            xin = tf.concat([batch_incomplete, edge], axis=3)\n        else:\n            xin = batch_incomplete\n        # inpaint\n        x1, x2, offset_flow = self.build_inpaint_net(\n            xin, mask, reuse=True,\n            training=False, padding=FLAGS.padding)\n        batch_predicted = x2\n        # apply mask and reconstruct\n        batch_complete = batch_predicted*mask + batch_incomplete*(1.-mask)\n        # global image visualization\n        if FLAGS.guided:\n            viz_img = [\n                batch_pos,\n                batch_incomplete + edge,\n                batch_complete]\n        else:\n            viz_img = [batch_pos, batch_incomplete, batch_complete]\n        if offset_flow is not None:\n            viz_img.append(\n                resize(offset_flow, scale=4,\n                       func=tf.image.resize_bilinear))\n        images_summary(\n            tf.concat(viz_img, axis=2),\n            name+\'_raw_incomplete_complete\', FLAGS.viz_max_out)\n        return batch_complete\n\n    def build_static_infer_graph(self, FLAGS, batch_data, name):\n        """"""\n        """"""\n        # generate mask, 1 represents masked point\n        bbox = (tf.constant(FLAGS.height//2), tf.constant(FLAGS.width//2),\n                tf.constant(FLAGS.height), tf.constant(FLAGS.width))\n        return self.build_infer_graph(FLAGS, batch_data, bbox, name)\n\n\n    def build_server_graph(self, FLAGS, batch_data, reuse=False, is_training=False):\n        """"""\n        """"""\n        # generate mask, 1 represents masked point\n        if FLAGS.guided:\n            batch_raw, edge, masks_raw = tf.split(batch_data, 3, axis=2)\n            edge = edge[:, :, :, 0:1] / 255.\n            edge = tf.cast(edge > FLAGS.edge_threshold, tf.float32)\n        else:\n            batch_raw, masks_raw = tf.split(batch_data, 2, axis=2)\n        masks = tf.cast(masks_raw[0:1, :, :, 0:1] > 127.5, tf.float32)\n\n        batch_pos = batch_raw / 127.5 - 1.\n        batch_incomplete = batch_pos * (1. - masks)\n        if FLAGS.guided:\n            edge = edge * masks[:, :, :, 0:1]\n            xin = tf.concat([batch_incomplete, edge], axis=3)\n        else:\n            xin = batch_incomplete\n        # inpaint\n        x1, x2, flow = self.build_inpaint_net(\n            xin, masks, reuse=reuse, training=is_training)\n        batch_predict = x2\n        # apply mask and reconstruct\n        batch_complete = batch_predict*masks + batch_incomplete*(1-masks)\n        return batch_complete\n'"
inpaint_ops.py,80,"b'import logging\nimport math\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\nfrom PIL import Image, ImageDraw\n\nfrom neuralgym.ops.layers import resize\nfrom neuralgym.ops.layers import *\nfrom neuralgym.ops.loss_ops import *\nfrom neuralgym.ops.gan_ops import *\nfrom neuralgym.ops.summary_ops import *\n\n\nlogger = logging.getLogger()\nnp.random.seed(2018)\n\n\n@add_arg_scope\ndef gen_conv(x, cnum, ksize, stride=1, rate=1, name=\'conv\',\n             padding=\'SAME\', activation=tf.nn.elu, training=True):\n    """"""Define conv for generator.\n\n    Args:\n        x: Input.\n        cnum: Channel number.\n        ksize: Kernel size.\n        Stride: Convolution stride.\n        Rate: Rate for or dilated conv.\n        name: Name of layers.\n        padding: Default to SYMMETRIC.\n        activation: Activation function after convolution.\n        training: If current graph is for training or inference, used for bn.\n\n    Returns:\n        tf.Tensor: output\n\n    """"""\n    assert padding in [\'SYMMETRIC\', \'SAME\', \'REFELECT\']\n    if padding == \'SYMMETRIC\' or padding == \'REFELECT\':\n        p = int(rate*(ksize-1)/2)\n        x = tf.pad(x, [[0,0], [p, p], [p, p], [0,0]], mode=padding)\n        padding = \'VALID\'\n    x = tf.layers.conv2d(\n        x, cnum, ksize, stride, dilation_rate=rate,\n        activation=None, padding=padding, name=name)\n    if cnum == 3 or activation is None:\n        # conv for output\n        return x\n    x, y = tf.split(x, 2, 3)\n    x = activation(x)\n    y = tf.nn.sigmoid(y)\n    x = x * y\n    return x\n\n\n@add_arg_scope\ndef gen_deconv(x, cnum, name=\'upsample\', padding=\'SAME\', training=True):\n    """"""Define deconv for generator.\n    The deconv is defined to be a x2 resize_nearest_neighbor operation with\n    additional gen_conv operation.\n\n    Args:\n        x: Input.\n        cnum: Channel number.\n        name: Name of layers.\n        training: If current graph is for training or inference, used for bn.\n\n    Returns:\n        tf.Tensor: output\n\n    """"""\n    with tf.variable_scope(name):\n        x = resize(x, func=tf.image.resize_nearest_neighbor)\n        x = gen_conv(\n            x, cnum, 3, 1, name=name+\'_conv\', padding=padding,\n            training=training)\n    return x\n\n\n@add_arg_scope\ndef dis_conv(x, cnum, ksize=5, stride=2, name=\'conv\', training=True):\n    """"""Define conv for discriminator.\n    Activation is set to leaky_relu.\n\n    Args:\n        x: Input.\n        cnum: Channel number.\n        ksize: Kernel size.\n        Stride: Convolution stride.\n        name: Name of layers.\n        training: If current graph is for training or inference, used for bn.\n\n    Returns:\n        tf.Tensor: output\n\n    """"""\n    x = conv2d_spectral_norm(x, cnum, ksize, stride, \'SAME\', name=name)\n    x = tf.nn.leaky_relu(x)\n    return x\n\n\ndef random_bbox(FLAGS):\n    """"""Generate a random tlhw.\n\n    Returns:\n        tuple: (top, left, height, width)\n\n    """"""\n    img_shape = FLAGS.img_shapes\n    img_height = img_shape[0]\n    img_width = img_shape[1]\n    maxt = img_height - FLAGS.vertical_margin - FLAGS.height\n    maxl = img_width - FLAGS.horizontal_margin - FLAGS.width\n    t = tf.random_uniform(\n        [], minval=FLAGS.vertical_margin, maxval=maxt, dtype=tf.int32)\n    l = tf.random_uniform(\n        [], minval=FLAGS.horizontal_margin, maxval=maxl, dtype=tf.int32)\n    h = tf.constant(FLAGS.height)\n    w = tf.constant(FLAGS.width)\n    return (t, l, h, w)\n\n\ndef bbox2mask(FLAGS, bbox, name=\'mask\'):\n    """"""Generate mask tensor from bbox.\n\n    Args:\n        bbox: tuple, (top, left, height, width)\n\n    Returns:\n        tf.Tensor: output with shape [1, H, W, 1]\n\n    """"""\n    def npmask(bbox, height, width, delta_h, delta_w):\n        mask = np.zeros((1, height, width, 1), np.float32)\n        h = np.random.randint(delta_h//2+1)\n        w = np.random.randint(delta_w//2+1)\n        mask[:, bbox[0]+h:bbox[0]+bbox[2]-h,\n             bbox[1]+w:bbox[1]+bbox[3]-w, :] = 1.\n        return mask\n    with tf.variable_scope(name), tf.device(\'/cpu:0\'):\n        img_shape = FLAGS.img_shapes\n        height = img_shape[0]\n        width = img_shape[1]\n        mask = tf.py_func(\n            npmask,\n            [bbox, height, width,\n             FLAGS.max_delta_height, FLAGS.max_delta_width],\n            tf.float32, stateful=False)\n        mask.set_shape([1] + [height, width] + [1])\n    return mask\n\n\ndef brush_stroke_mask(FLAGS, name=\'mask\'):\n    """"""Generate mask tensor from bbox.\n\n    Returns:\n        tf.Tensor: output with shape [1, H, W, 1]\n\n    """"""\n    min_num_vertex = 4\n    max_num_vertex = 12\n    mean_angle = 2*math.pi / 5\n    angle_range = 2*math.pi / 15\n    min_width = 12\n    max_width = 40\n    def generate_mask(H, W):\n        average_radius = math.sqrt(H*H+W*W) / 8\n        mask = Image.new(\'L\', (W, H), 0)\n\n        for _ in range(np.random.randint(1, 4)):\n            num_vertex = np.random.randint(min_num_vertex, max_num_vertex)\n            angle_min = mean_angle - np.random.uniform(0, angle_range)\n            angle_max = mean_angle + np.random.uniform(0, angle_range)\n            angles = []\n            vertex = []\n            for i in range(num_vertex):\n                if i % 2 == 0:\n                    angles.append(2*math.pi - np.random.uniform(angle_min, angle_max))\n                else:\n                    angles.append(np.random.uniform(angle_min, angle_max))\n\n            h, w = mask.size\n            vertex.append((int(np.random.randint(0, w)), int(np.random.randint(0, h))))\n            for i in range(num_vertex):\n                r = np.clip(\n                    np.random.normal(loc=average_radius, scale=average_radius//2),\n                    0, 2*average_radius)\n                new_x = np.clip(vertex[-1][0] + r * math.cos(angles[i]), 0, w)\n                new_y = np.clip(vertex[-1][1] + r * math.sin(angles[i]), 0, h)\n                vertex.append((int(new_x), int(new_y)))\n\n            draw = ImageDraw.Draw(mask)\n            width = int(np.random.uniform(min_width, max_width))\n            draw.line(vertex, fill=1, width=width)\n            for v in vertex:\n                draw.ellipse((v[0] - width//2,\n                              v[1] - width//2,\n                              v[0] + width//2,\n                              v[1] + width//2),\n                             fill=1)\n\n        if np.random.normal() > 0:\n            mask.transpose(Image.FLIP_LEFT_RIGHT)\n        if np.random.normal() > 0:\n            mask.transpose(Image.FLIP_TOP_BOTTOM)\n        mask = np.asarray(mask, np.float32)\n        mask = np.reshape(mask, (1, H, W, 1))\n        return mask\n    with tf.variable_scope(name), tf.device(\'/cpu:0\'):\n        img_shape = FLAGS.img_shapes\n        height = img_shape[0]\n        width = img_shape[1]\n        mask = tf.py_func(\n            generate_mask,\n            [height, width],\n            tf.float32, stateful=True)\n        mask.set_shape([1] + [height, width] + [1])\n    return mask\n\n\ndef local_patch(x, bbox):\n    """"""Crop local patch according to bbox.\n\n    Args:\n        x: input\n        bbox: (top, left, height, width)\n\n    Returns:\n        tf.Tensor: local patch\n\n    """"""\n    x = tf.image.crop_to_bounding_box(x, bbox[0], bbox[1], bbox[2], bbox[3])\n    return x\n\n\ndef resize_mask_like(mask, x):\n    """"""Resize mask like shape of x.\n\n    Args:\n        mask: Original mask.\n        x: To shape of x.\n\n    Returns:\n        tf.Tensor: resized mask\n\n    """"""\n    mask_resize = resize(\n        mask, to_shape=x.get_shape().as_list()[1:3],\n        func=tf.image.resize_nearest_neighbor)\n    return mask_resize\n\n\ndef contextual_attention(f, b, mask=None, ksize=3, stride=1, rate=1,\n                         fuse_k=3, softmax_scale=10., training=True, fuse=True):\n    """""" Contextual attention layer implementation.\n\n    Contextual attention is first introduced in publication:\n        Generative Image Inpainting with Contextual Attention, Yu et al.\n\n    Args:\n        x: Input feature to match (foreground).\n        t: Input feature for match (background).\n        mask: Input mask for t, indicating patches not available.\n        ksize: Kernel size for contextual attention.\n        stride: Stride for extracting patches from t.\n        rate: Dilation for matching.\n        softmax_scale: Scaled softmax for attention.\n        training: Indicating if current graph is training or inference.\n\n    Returns:\n        tf.Tensor: output\n\n    """"""\n    # get shapes\n    raw_fs = tf.shape(f)\n    raw_int_fs = f.get_shape().as_list()\n    raw_int_bs = b.get_shape().as_list()\n    # extract patches from background with stride and rate\n    kernel = 2*rate\n    raw_w = tf.extract_image_patches(\n        b, [1,kernel,kernel,1], [1,rate*stride,rate*stride,1], [1,1,1,1], padding=\'SAME\')\n    raw_w = tf.reshape(raw_w, [raw_int_bs[0], -1, kernel, kernel, raw_int_bs[3]])\n    raw_w = tf.transpose(raw_w, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw\n    # downscaling foreground option: downscaling both foreground and\n    # background for matching and use original background for reconstruction.\n    f = resize(f, scale=1./rate, func=tf.image.resize_nearest_neighbor)\n    b = resize(b, to_shape=[int(raw_int_bs[1]/rate), int(raw_int_bs[2]/rate)], func=tf.image.resize_nearest_neighbor)  # https://github.com/tensorflow/tensorflow/issues/11651\n    if mask is not None:\n        mask = resize(mask, scale=1./rate, func=tf.image.resize_nearest_neighbor)\n    fs = tf.shape(f)\n    int_fs = f.get_shape().as_list()\n    f_groups = tf.split(f, int_fs[0], axis=0)\n    # from t(H*W*C) to w(b*k*k*c*h*w)\n    bs = tf.shape(b)\n    int_bs = b.get_shape().as_list()\n    w = tf.extract_image_patches(\n        b, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding=\'SAME\')\n    w = tf.reshape(w, [int_fs[0], -1, ksize, ksize, int_fs[3]])\n    w = tf.transpose(w, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw\n    # process mask\n    if mask is None:\n        mask = tf.zeros([1, bs[1], bs[2], 1])\n    m = tf.extract_image_patches(\n        mask, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding=\'SAME\')\n    m = tf.reshape(m, [1, -1, ksize, ksize, 1])\n    m = tf.transpose(m, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw\n    m = m[0]\n    mm = tf.cast(tf.equal(tf.reduce_mean(m, axis=[0,1,2], keep_dims=True), 0.), tf.float32)\n    w_groups = tf.split(w, int_bs[0], axis=0)\n    raw_w_groups = tf.split(raw_w, int_bs[0], axis=0)\n    y = []\n    offsets = []\n    k = fuse_k\n    scale = softmax_scale\n    fuse_weight = tf.reshape(tf.eye(k), [k, k, 1, 1])\n    for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n        # conv for compare\n        wi = wi[0]\n        wi_normed = wi / tf.maximum(tf.sqrt(tf.reduce_sum(tf.square(wi), axis=[0,1,2])), 1e-4)\n        yi = tf.nn.conv2d(xi, wi_normed, strides=[1,1,1,1], padding=""SAME"")\n\n        # conv implementation for fuse scores to encourage large patches\n        if fuse:\n            yi = tf.reshape(yi, [1, fs[1]*fs[2], bs[1]*bs[2], 1])\n            yi = tf.nn.conv2d(yi, fuse_weight, strides=[1,1,1,1], padding=\'SAME\')\n            yi = tf.reshape(yi, [1, fs[1], fs[2], bs[1], bs[2]])\n            yi = tf.transpose(yi, [0, 2, 1, 4, 3])\n            yi = tf.reshape(yi, [1, fs[1]*fs[2], bs[1]*bs[2], 1])\n            yi = tf.nn.conv2d(yi, fuse_weight, strides=[1,1,1,1], padding=\'SAME\')\n            yi = tf.reshape(yi, [1, fs[2], fs[1], bs[2], bs[1]])\n            yi = tf.transpose(yi, [0, 2, 1, 4, 3])\n        yi = tf.reshape(yi, [1, fs[1], fs[2], bs[1]*bs[2]])\n\n        # softmax to match\n        yi *=  mm  # mask\n        yi = tf.nn.softmax(yi*scale, 3)\n        yi *=  mm  # mask\n\n        offset = tf.argmax(yi, axis=3, output_type=tf.int32)\n        offset = tf.stack([offset // fs[2], offset % fs[2]], axis=-1)\n        # deconv for patch pasting\n        # 3.1 paste center\n        wi_center = raw_wi[0]\n        yi = tf.nn.conv2d_transpose(yi, wi_center, tf.concat([[1], raw_fs[1:]], axis=0), strides=[1,rate,rate,1]) / 4.\n        y.append(yi)\n        offsets.append(offset)\n    y = tf.concat(y, axis=0)\n    y.set_shape(raw_int_fs)\n    offsets = tf.concat(offsets, axis=0)\n    offsets.set_shape(int_bs[:3] + [2])\n    # case1: visualize optical flow: minus current position\n    h_add = tf.tile(tf.reshape(tf.range(bs[1]), [1, bs[1], 1, 1]), [bs[0], 1, bs[2], 1])\n    w_add = tf.tile(tf.reshape(tf.range(bs[2]), [1, 1, bs[2], 1]), [bs[0], bs[1], 1, 1])\n    offsets = offsets - tf.concat([h_add, w_add], axis=3)\n    # to flow image\n    flow = flow_to_image_tf(offsets)\n    # # case2: visualize which pixels are attended\n    # flow = highlight_flow_tf(offsets * tf.cast(mask, tf.int32))\n    if rate != 1:\n        flow = resize(flow, scale=rate, func=tf.image.resize_bilinear)\n    return y, flow\n\n\ndef test_contextual_attention(args):\n    """"""Test contextual attention layer with 3-channel image input\n    (instead of n-channel feature).\n\n    """"""\n    import cv2\n    import os\n    # run on cpu\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\n\n    rate = 2\n    stride = 1\n    grid = rate*stride\n\n    b = cv2.imread(args.imageA)\n    b = cv2.resize(b, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_CUBIC)\n    h, w, _ = b.shape\n    b = b[:h//grid*grid, :w//grid*grid, :]\n    b = np.expand_dims(b, 0)\n    logger.info(\'Size of imageA: {}\'.format(b.shape))\n\n    f = cv2.imread(args.imageB)\n    h, w, _ = f.shape\n    f = f[:h//grid*grid, :w//grid*grid, :]\n    f = np.expand_dims(f, 0)\n    logger.info(\'Size of imageB: {}\'.format(f.shape))\n\n    with tf.Session() as sess:\n        bt = tf.constant(b, dtype=tf.float32)\n        ft = tf.constant(f, dtype=tf.float32)\n\n        yt, flow = contextual_attention(\n            ft, bt, stride=stride, rate=rate,\n            training=False, fuse=False)\n        y = sess.run(yt)\n        cv2.imwrite(args.imageOut, y[0])\n\n\ndef make_color_wheel():\n    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n    ncols = RY + YG + GC + CB + BM + MR\n    colorwheel = np.zeros([ncols, 3])\n    col = 0\n    # RY\n    colorwheel[0:RY, 0] = 255\n    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n    col += RY\n    # YG\n    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n    colorwheel[col:col+YG, 1] = 255\n    col += YG\n    # GC\n    colorwheel[col:col+GC, 1] = 255\n    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n    col += GC\n    # CB\n    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n    colorwheel[col:col+CB, 2] = 255\n    col += CB\n    # BM\n    colorwheel[col:col+BM, 2] = 255\n    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n    col += + BM\n    # MR\n    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n    colorwheel[col:col+MR, 0] = 255\n    return colorwheel\n\n\nCOLORWHEEL = make_color_wheel()\n\n\ndef compute_color(u,v):\n    h, w = u.shape\n    img = np.zeros([h, w, 3])\n    nanIdx = np.isnan(u) | np.isnan(v)\n    u[nanIdx] = 0\n    v[nanIdx] = 0\n    # colorwheel = COLORWHEEL\n    colorwheel = make_color_wheel()\n    ncols = np.size(colorwheel, 0)\n    rad = np.sqrt(u**2+v**2)\n    a = np.arctan2(-v, -u) / np.pi\n    fk = (a+1) / 2 * (ncols - 1) + 1\n    k0 = np.floor(fk).astype(int)\n    k1 = k0 + 1\n    k1[k1 == ncols+1] = 1\n    f = fk - k0\n    for i in range(np.size(colorwheel,1)):\n        tmp = colorwheel[:, i]\n        col0 = tmp[k0-1] / 255\n        col1 = tmp[k1-1] / 255\n        col = (1-f) * col0 + f * col1\n        idx = rad <= 1\n        col[idx] = 1-rad[idx]*(1-col[idx])\n        notidx = np.logical_not(idx)\n        col[notidx] *= 0.75\n        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n    return img\n\n\n\ndef flow_to_image(flow):\n    """"""Transfer flow map to image.\n    Part of code forked from flownet.\n    """"""\n    out = []\n    maxu = -999.\n    maxv = -999.\n    minu = 999.\n    minv = 999.\n    maxrad = -1\n    for i in range(flow.shape[0]):\n        u = flow[i, :, :, 0]\n        v = flow[i, :, :, 1]\n        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)\n        u[idxunknow] = 0\n        v[idxunknow] = 0\n        maxu = max(maxu, np.max(u))\n        minu = min(minu, np.min(u))\n        maxv = max(maxv, np.max(v))\n        minv = min(minv, np.min(v))\n        rad = np.sqrt(u ** 2 + v ** 2)\n        maxrad = max(maxrad, np.max(rad))\n        u = u/(maxrad + np.finfo(float).eps)\n        v = v/(maxrad + np.finfo(float).eps)\n        img = compute_color(u, v)\n        out.append(img)\n    return np.float32(np.uint8(out))\n\n\ndef flow_to_image_tf(flow, name=\'flow_to_image\'):\n    """"""Tensorflow ops for computing flow to image.\n    """"""\n    with tf.variable_scope(name), tf.device(\'/cpu:0\'):\n        img = tf.py_func(flow_to_image, [flow], tf.float32, stateful=False)\n        img.set_shape(flow.get_shape().as_list()[0:-1]+[3])\n        img = img / 127.5 - 1.\n        return img\n\n\ndef highlight_flow(flow):\n    """"""Convert flow into middlebury color code image.\n    """"""\n    out = []\n    s = flow.shape\n    for i in range(flow.shape[0]):\n        img = np.ones((s[1], s[2], 3)) * 144.\n        u = flow[i, :, :, 0]\n        v = flow[i, :, :, 1]\n        for h in range(s[1]):\n            for w in range(s[1]):\n                ui = u[h,w]\n                vi = v[h,w]\n                img[ui, vi, :] = 255.\n        out.append(img)\n    return np.float32(np.uint8(out))\n\n\ndef highlight_flow_tf(flow, name=\'flow_to_image\'):\n    """"""Tensorflow ops for highlight flow.\n    """"""\n    with tf.variable_scope(name), tf.device(\'/cpu:0\'):\n        img = tf.py_func(highlight_flow, [flow], tf.float32, stateful=False)\n        img.set_shape(flow.get_shape().as_list()[0:-1]+[3])\n        img = img / 127.5 - 1.\n        return img\n\n\ndef image2edge(image):\n    """"""Convert image to edges.\n    """"""\n    out = []\n    for i in range(image.shape[0]):\n        img = cv2.Laplacian(image[i, :, :, :], cv2.CV_64F, ksize=3, scale=2)\n        out.append(img)\n    return np.float32(np.uint8(out))\n\n\nif __name__ == ""__main__"":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--imageA\', default=\'\', type=str, help=\'Image A as background patches to reconstruct image B.\')\n    parser.add_argument(\'--imageB\', default=\'\', type=str, help=\'Image B is reconstructed with image A.\')\n    parser.add_argument(\'--imageOut\', default=\'result.png\', type=str, help=\'Image B is reconstructed with image A.\')\n    args = parser.parse_args()\n    test_contextual_attention(args)\n'"
test.py,8,"b'import argparse\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport neuralgym as ng\n\nfrom inpaint_model import InpaintCAModel\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--image\', default=\'\', type=str,\n                    help=\'The filename of image to be completed.\')\nparser.add_argument(\'--mask\', default=\'\', type=str,\n                    help=\'The filename of mask, value 255 indicates mask.\')\nparser.add_argument(\'--output\', default=\'output.png\', type=str,\n                    help=\'Where to write output.\')\nparser.add_argument(\'--checkpoint_dir\', default=\'\', type=str,\n                    help=\'The directory of tensorflow checkpoint.\')\n\n\nif __name__ == ""__main__"":\n    FLAGS = ng.Config(\'inpaint.yml\')\n    # ng.get_gpus(1)\n    args, unknown = parser.parse_known_args()\n\n    model = InpaintCAModel()\n    image = cv2.imread(args.image)\n    mask = cv2.imread(args.mask)\n    # mask = cv2.resize(mask, (0,0), fx=0.5, fy=0.5)\n\n    assert image.shape == mask.shape\n\n    h, w, _ = image.shape\n    grid = 8\n    image = image[:h//grid*grid, :w//grid*grid, :]\n    mask = mask[:h//grid*grid, :w//grid*grid, :]\n    print(\'Shape of image: {}\'.format(image.shape))\n\n    image = np.expand_dims(image, 0)\n    mask = np.expand_dims(mask, 0)\n    input_image = np.concatenate([image, mask], axis=2)\n\n    sess_config = tf.ConfigProto()\n    sess_config.gpu_options.allow_growth = True\n    with tf.Session(config=sess_config) as sess:\n        input_image = tf.constant(input_image, dtype=tf.float32)\n        output = model.build_server_graph(FLAGS, input_image)\n        output = (output + 1.) * 127.5\n        output = tf.reverse(output, [-1])\n        output = tf.saturate_cast(output, tf.uint8)\n        # load pretrained model\n        vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n        assign_ops = []\n        for var in vars_list:\n            vname = var.name\n            from_name = vname\n            var_value = tf.contrib.framework.load_variable(args.checkpoint_dir, from_name)\n            assign_ops.append(tf.assign(var, var_value))\n        sess.run(assign_ops)\n        print(\'Model loaded.\')\n        result = sess.run(output)\n        cv2.imwrite(args.output, result[0][:, :, ::-1])\n'"
train.py,5,"b'import os\nimport glob\n\nimport tensorflow as tf\nimport neuralgym as ng\n\nfrom inpaint_model import InpaintCAModel\n\n\ndef multigpu_graph_def(model, FLAGS, data, gpu_id=0, loss_type=\'g\'):\n    with tf.device(\'/cpu:0\'):\n        images = data.data_pipeline(FLAGS.batch_size)\n    if gpu_id == 0 and loss_type == \'g\':\n        _, _, losses = model.build_graph_with_losses(\n            FLAGS, images, FLAGS, summary=True, reuse=True)\n    else:\n        _, _, losses = model.build_graph_with_losses(\n            FLAGS, images, FLAGS, reuse=True)\n    if loss_type == \'g\':\n        return losses[\'g_loss\']\n    elif loss_type == \'d\':\n        return losses[\'d_loss\']\n    else:\n        raise ValueError(\'loss type is not supported.\')\n\n\nif __name__ == ""__main__"":\n    # training data\n    FLAGS = ng.Config(\'inpaint.yml\')\n    img_shapes = FLAGS.img_shapes\n    with open(FLAGS.data_flist[FLAGS.dataset][0]) as f:\n        fnames = f.read().splitlines()\n    if FLAGS.guided:\n        fnames = [(fname, fname[:-4] + \'_edge.jpg\') for fname in fnames]\n        img_shapes = [img_shapes, img_shapes]\n    data = ng.data.DataFromFNames(\n        fnames, img_shapes, random_crop=FLAGS.random_crop,\n        nthreads=FLAGS.num_cpus_per_job)\n    images = data.data_pipeline(FLAGS.batch_size)\n    # main model\n    model = InpaintCAModel()\n    g_vars, d_vars, losses = model.build_graph_with_losses(FLAGS, images)\n    # validation images\n    if FLAGS.val:\n        with open(FLAGS.data_flist[FLAGS.dataset][1]) as f:\n            val_fnames = f.read().splitlines()\n        if FLAGS.guided:\n            val_fnames = [\n                (fname, fname[:-4] + \'_edge.jpg\') for fname in val_fnames]\n        # progress monitor by visualizing static images\n        for i in range(FLAGS.static_view_size):\n            static_fnames = val_fnames[i:i+1]\n            static_images = ng.data.DataFromFNames(\n                static_fnames, img_shapes, nthreads=1,\n                random_crop=FLAGS.random_crop).data_pipeline(1)\n            static_inpainted_images = model.build_static_infer_graph(\n                FLAGS, static_images, name=\'static_view/%d\' % i)\n    # training settings\n    lr = tf.get_variable(\n        \'lr\', shape=[], trainable=False,\n        initializer=tf.constant_initializer(1e-4))\n    d_optimizer = tf.train.AdamOptimizer(lr, beta1=0.5, beta2=0.999)\n    g_optimizer = d_optimizer\n    # train discriminator with secondary trainer, should initialize before\n    # primary trainer.\n    # discriminator_training_callback = ng.callbacks.SecondaryTrainer(\n    discriminator_training_callback = ng.callbacks.SecondaryMultiGPUTrainer(\n        num_gpus=FLAGS.num_gpus_per_job,\n        pstep=1,\n        optimizer=d_optimizer,\n        var_list=d_vars,\n        max_iters=1,\n        grads_summary=False,\n        graph_def=multigpu_graph_def,\n        graph_def_kwargs={\n            \'model\': model, \'FLAGS\': FLAGS, \'data\': data, \'loss_type\': \'d\'},\n    )\n    # train generator with primary trainer\n    # trainer = ng.train.Trainer(\n    trainer = ng.train.MultiGPUTrainer(\n        num_gpus=FLAGS.num_gpus_per_job,\n        optimizer=g_optimizer,\n        var_list=g_vars,\n        max_iters=FLAGS.max_iters,\n        graph_def=multigpu_graph_def,\n        grads_summary=False,\n        gradient_processor=None,\n        graph_def_kwargs={\n            \'model\': model, \'FLAGS\': FLAGS, \'data\': data, \'loss_type\': \'g\'},\n        spe=FLAGS.train_spe,\n        log_dir=FLAGS.log_dir,\n    )\n    # add all callbacks\n    trainer.add_callbacks([\n        discriminator_training_callback,\n        ng.callbacks.WeightsViewer(),\n        ng.callbacks.ModelRestorer(trainer.context[\'saver\'], dump_prefix=FLAGS.model_restore+\'/snap\', optimistic=True),\n        ng.callbacks.ModelSaver(FLAGS.train_spe, trainer.context[\'saver\'], FLAGS.log_dir+\'/snap\'),\n        ng.callbacks.SummaryWriter((FLAGS.val_psteps//1), trainer.context[\'summary_writer\'], tf.summary.merge_all()),\n    ])\n    # launch training\n    trainer.train()\n'"
