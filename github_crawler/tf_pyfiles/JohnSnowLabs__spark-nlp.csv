file_path,api_count,code
python/run-tests.py,0,b'from test.annotators import *\nfrom test.misc import *\nfrom test.base import *\n\n# Annotator tests\nunittest.TextTestRunner().run(BasicAnnotatorsTestSpec())\nunittest.TextTestRunner().run(RegexMatcherTestSpec())\nunittest.TextTestRunner().run(TokenizerTestSpec())\nunittest.TextTestRunner().run(ChunkTokenizerTestSpec())\nunittest.TextTestRunner().run(LemmatizerTestSpec())\nunittest.TextTestRunner().run(DateMatcherTestSpec())\nunittest.TextTestRunner().run(TextMatcherTestSpec())\n\nunittest.TextTestRunner().run(PerceptronApproachTestSpec())\nunittest.TextTestRunner().run(ChunkerTestSpec())\nunittest.TextTestRunner().run(ChunkDocSerializingTestSpec())\n\nunittest.TextTestRunner().run(PragmaticSBDTestSpec())\nunittest.TextTestRunner().run(PragmaticScorerTestSpec())\nunittest.TextTestRunner().run(PipelineTestSpec())\nunittest.TextTestRunner().run(SpellCheckerTestSpec())\nunittest.TextTestRunner().run(SymmetricDeleteTestSpec())\nunittest.TextTestRunner().run(ContextSpellCheckerTestSpec())\nunittest.TextTestRunner().run(ParamsGettersTestSpec())\nunittest.TextTestRunner().run(DependencyParserTreeBankTestSpec())\nunittest.TextTestRunner().run(DependencyParserConllUTestSpec())\nunittest.TextTestRunner().run(TypedDependencyParserConll2009TestSpec())\nunittest.TextTestRunner().run(TypedDependencyParserConllUTestSpec())\nunittest.TextTestRunner().run(DeepSentenceDetectorTestSpec())\nunittest.TextTestRunner().run(SentenceEmbeddingsTestSpec())\nunittest.TextTestRunner().run(StopWordsCleanerTestSpec())\nunittest.TextTestRunner().run(NGramGeneratorTestSpec())\nunittest.TextTestRunner().run(ChunkEmbeddingsTestSpec())\nunittest.TextTestRunner().run(EmbeddingsFinisherTestSpec())\nunittest.TextTestRunner().run(ElmoEmbeddingsTestSpec())\n\n# Should be locally tested\n# unittest.TextTestRunner().run(AlbertEmbeddingsTestSpec())\n# unittest.TextTestRunner().run(XlnetEmbeddingsTestSpec())\n# unittest.TextTestRunner().run(UniversalSentenceEncoderTestSpec())\n# unittest.TextTestRunner().run(ClassifierDLTestSpec())\n# unittest.TextTestRunner().run(SentimentDLTestSpec())\n# unittest.TextTestRunner().run(RecursiveTestSpec())\n\n# Misc tests\nunittest.TextTestRunner().run(UtilitiesTestSpec())\nunittest.TextTestRunner().run(SerializersTestSpec())\n\n'
python/setup.py,0,"b'""""""A setuptools based setup module.\n\nSee:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n""""""\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\n# Arguments marked as ""Required"" below must be included for upload to PyPI.\n# Fields marked as ""Optional"" may be commented out.\n\nsetup(\n    # This is the name of your project. The first time you publish this\n    # package, this name will be registered for you. It will determine how\n    # users can install this project, e.g.:\n    #\n    # $ pip install sampleproject\n    #\n    # And where it will live on PyPI: https://pypi.org/project/sampleproject/\n    #\n    # There are some restrictions on what makes a valid project name\n    # specification here:\n    # https://packaging.python.org/specifications/core-metadata/#name\n    name=\'spark-nlp\',  # Required\n\n    # Versions should comply with PEP 440:\n    # https://www.python.org/dev/peps/pep-0440/\n    #\n    # For a discussion on single-sourcing the version across setup.py and the\n    # project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n\n    version=\'2.5.1\',  # Required\n\n    # This is a one-line description or tagline of what your project does. This\n    # corresponds to the ""Summary"" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#summary\n    description=\'John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.\',  # Required\n\n    # This is an optional longer description of your project that represents\n    # the body of text which users will see when they visit PyPI.\n    #\n    # Often, this is the same as your README, so you can just read it in from\n    # that file directly (as we have already done above)\n    #\n    # This field corresponds to the ""Description"" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#description-optional\n    long_description=long_description,  # Optional\n\n    # Denotes that our long_description is in Markdown; valid values are\n    # text/plain, text/x-rst, and text/markdown\n    #\n    # Optional if long_description is written in reStructuredText (rst) but\n    # required for plain-text or Markdown; if unspecified, ""applications should\n    # attempt to render [the long_description] as text/x-rst; charset=UTF-8 and\n    # fall back to text/plain if it is not valid rst"" (see link below)\n    #\n    # This field corresponds to the ""Description-Content-Type"" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#description-content-type-optional\n    long_description_content_type=\'text/markdown\',  # Optional (see note above)\n\n    # This should be a valid link to your project\'s main homepage.\n    #\n    # This field corresponds to the ""Home-Page"" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#home-page-optional\n    url=\'http://nlp.johnsnowlabs.com\',  # Optional\n\n    # This should be your name or the name of the organization which owns the\n    # project.\n    author=\'John Snow Labs\',  # Optional\n\n    # This should be a valid email address corresponding to the author listed\n    # above.\n    # author_email=\'pypa-dev@googlegroups.com\',  # Optional\n\n    # Classifiers help users find your project by categorizing it.\n    #\n    # For a list of valid classifiers, see https://pypi.org/classifiers/\n    classifiers=[  # Optional\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 5 - Production/Stable\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Topic :: Software Development :: Build Tools\',\n\n        # Pick your license as you wish\n        \'License :: OSI Approved :: Apache Software License\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n\n    # This field adds keywords for your project which will appear on the\n    # project page. What does your project relate to?\n    #\n    # Note that this is a string of words separated by whitespace, not a list.\n    keywords=\'NLP spark development\',  # Optional\n\n    # You can just specify package directories manually here if your project is\n    # simple. Or you can use find_packages().\n    #\n    # Alternatively, if you just want to distribute a single Python file, use\n    # the `py_modules` argument instead as follows, which will expect a file\n    # called `my_module.py` to exist:\n    #\n    #   py_modules=[""my_module""],\n\n    packages=find_packages(exclude=[\'test\']),\n\n    include_package_data=False  # Needed to install jar file\n\n)\n'"
python/com/__init__.py,0,b''
python/sparknlp/__init__.py,0,"b'import sys\nfrom pyspark.sql import SparkSession\nfrom sparknlp import annotator\nfrom sparknlp.base import DocumentAssembler, Finisher, TokenAssembler, Chunk2Doc, Doc2Chunk\n\nsys.modules[\'com.johnsnowlabs.nlp.annotators\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.tokenizer\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.tokenizer.wordpiece\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.ner\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.ner.regex\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.ner.crf\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.ner.dl\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.pos\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.pos.perceptron\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.sbd\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.sbd.pragmatic\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.sbd.deep\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.sda\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.sda.pragmatic\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.sda.vivekn\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.spell\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.spell.norvig\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.spell.symmetric\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.parser\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.parser.dep\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.parser.typdep\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.embeddings\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.classifier\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.classifier.dl\'] = annotator\nsys.modules[\'com.johnsnowlabs.nlp.annotators.spell.context\'] = annotator\n\nannotators = annotator\nembeddings = annotator\n\n\ndef start(gpu=False):\n    builder = SparkSession.builder \\\n        .appName(""Spark NLP"") \\\n        .master(""local[*]"") \\\n        .config(""spark.driver.memory"", ""16G"") \\\n        .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"") \\\n        .config(""spark.kryoserializer.buffer.max"", ""1000M"")\n    if gpu:\n        builder.config(""spark.jars.packages"", ""com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.1"")\n    else:\n        builder.config(""spark.jars.packages"", ""com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1"")\n        \n    return builder.getOrCreate()\n\n\ndef version():\n    return \'2.5.1\'\n'"
python/sparknlp/annotation.py,0,"b'from pyspark.sql.types import *\n\n\nclass Annotation:\n\n    def __init__(self, annotator_type, begin, end, result, metadata, embeddings):\n        self.annotator_type = annotator_type\n        self.begin = begin\n        self.end = end\n        self.result = result\n        self.metadata = metadata\n        self.embeddings = embeddings\n\n    def __str__(self):\n        return ""Annotation(%s, %i, %i, %s, %s)"" % (\n            self.annotator_type,\n            self.begin,\n            self.end,\n            self.result,\n            str(self.metadata)\n        )\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def dataType():\n        return StructType([\n            StructField(\'annotator_type\', StringType(), False),\n            StructField(\'begin\', IntegerType(), False),\n            StructField(\'end\', IntegerType(), False),\n            StructField(\'result\', StringType(), False),\n            StructField(\'metadata\', MapType(StringType(), StringType()), False),\n            StructField(\'embeddings\', ArrayType(FloatType()), False)\n        ])\n\n    @staticmethod\n    def arrayType():\n        return ArrayType(Annotation.dataType())\n'"
python/sparknlp/annotator.py,0,"b'##\n# Prototyping for py4j to pipeline from Python\n##\n\nimport sys\nfrom sparknlp.common import *\n\n# Do NOT delete. Looks redundant but this is key work around for python 2 support.\nif sys.version_info[0] == 2:\n    from sparknlp.base import DocumentAssembler, Finisher, EmbeddingsFinisher, TokenAssembler\nelse:\n    import com.johnsnowlabs.nlp\n\nannotators = sys.modules[__name__]\npos = sys.modules[__name__]\npos.perceptron = sys.modules[__name__]\nner = sys.modules[__name__]\nner.crf = sys.modules[__name__]\nner.dl = sys.modules[__name__]\nregex = sys.modules[__name__]\nsbd = sys.modules[__name__]\nsbd.pragmatic = sys.modules[__name__]\nsbd.deep = sys.modules[__name__]\nsda = sys.modules[__name__]\nsda.pragmatic = sys.modules[__name__]\nsda.vivekn = sys.modules[__name__]\nspell = sys.modules[__name__]\nspell.norvig = sys.modules[__name__]\nspell.symmetric = sys.modules[__name__]\nspell.context = sys.modules[__name__]\nparser = sys.modules[__name__]\nparser.dep = sys.modules[__name__]\nparser.typdep = sys.modules[__name__]\nembeddings = sys.modules[__name__]\nclassifier = sys.modules[__name__]\nclassifier.dl = sys.modules[__name__]\n\n\nclass RecursiveTokenizer(AnnotatorApproach):\n    name = \'RecursiveTokenizer\'\n\n    prefixes = Param(Params._dummy(),\n                          ""prefixes"",\n                          ""strings to be considered independent tokens when found at the beginning of a word"",\n                          typeConverter=TypeConverters.toListString)\n\n    suffixes = Param(Params._dummy(),\n                          ""suffixes"",\n                          ""strings to be considered independent tokens when found at the end of a word"",\n                          typeConverter=TypeConverters.toListString)\n\n    infixes = Param(Params._dummy(),\n                          ""infixes"",\n                          ""strings to be considered independent tokens when found in the middle of a word"",\n                          typeConverter=TypeConverters.toListString)\n\n    whitelist = Param(Params._dummy(),\n                          ""whitelist"",\n                          ""strings to be considered as single tokens"",\n                          typeConverter=TypeConverters.toListString)\n\n    def setPrefixes(self, p):\n        return self._set(prefixes=p)\n\n    def setSuffixes(self, s):\n        return self._set(suffixes=s)\n\n    def setInfixes(self, i):\n        return self._set(infixes=i)\n\n    def setWhitelist(self, w):\n        return self._set(whitelist=w)\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.RecursiveTokenizer""):\n        super(RecursiveTokenizer, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.RecursiveTokenizer"")\n        self._setDefault(\n            prefixes=[""\'"", ""\\"""", ""("", ""["", ""\\n""],\n            infixes=[""\\n"", ""("", "")""],\n            suffixes=[""."", "":"", ""%"", "","", "";"", ""?"", ""\'"", ""\\"""", "")"", ""]"", ""\\n"", ""!"", ""\'s""],\n            whitelist=[""it\'s"", ""that\'s"", ""there\'s"", ""he\'s"", ""she\'s"", ""what\'s"", ""let\'s"", ""who\'s"", \\\n                ""It\'s"", ""That\'s"", ""There\'s"", ""He\'s"", ""She\'s"", ""What\'s"", ""Let\'s"", ""Who\'s""]\n        )\n\n\n    def _create_model(self, java_model):\n        return RecursiveTokenizerModel(java_model=java_model)\n\n\nclass RecursiveTokenizerModel(AnnotatorModel):\n    name = \'RecursiveTokenizerModel\'\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.RecursiveTokenizerModel"", java_model=None):\n        super(RecursiveTokenizerModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n\nclass Tokenizer(AnnotatorApproach):\n\n    targetPattern = Param(Params._dummy(),\n                          ""targetPattern"",\n                          ""pattern to grab from text as token candidates. Defaults \\S+"",\n                          typeConverter=TypeConverters.toString)\n\n    prefixPattern = Param(Params._dummy(),\n                          ""prefixPattern"",\n                          ""regex with groups and begins with \\A to match target prefix. Defaults to \\A([^\\s\\w\\$\\.]*)"",\n                          typeConverter=TypeConverters.toString)\n\n    suffixPattern = Param(Params._dummy(),\n                          ""suffixPattern"",\n                          ""regex with groups and ends with \\z to match target suffix. Defaults to ([^\\s\\w]?)([^\\s\\w]*)\\z"",\n                          typeConverter=TypeConverters.toString)\n\n    infixPatterns = Param(Params._dummy(),\n                          ""infixPatterns"",\n                          ""regex patterns that match tokens within a single target. groups identify different sub-tokens. multiple defaults"",\n                          typeConverter=TypeConverters.toListString)\n\n    exceptions = Param(Params._dummy(),\n                       ""exceptions"",\n                       ""Words that won\'t be affected by tokenization rules"",\n                       typeConverter=TypeConverters.toListString)\n\n    exceptionsPath = Param(Params._dummy(),\n                           ""exceptionsPath"",\n                           ""path to file containing list of exceptions"",\n                           typeConverter=TypeConverters.toString)\n\n    caseSensitiveExceptions = Param(Params._dummy(),\n                                    ""caseSensitiveExceptions"",\n                                    ""Whether to care for case sensitiveness in exceptions"",\n                                    typeConverter=TypeConverters.toBoolean)\n\n    contextChars = Param(Params._dummy(),\n                         ""contextChars"",\n                         ""character list used to separate from token boundaries"",\n                         typeConverter=TypeConverters.toListString)\n\n    splitPattern = Param(Params._dummy(),\n                         ""splitPattern"",\n                         ""character list used to separate from the inside of tokens"",\n                         typeConverter=TypeConverters.toString)\n\n    splitChars = Param(Params._dummy(),\n                       ""splitChars"",\n                       ""character list used to separate from the inside of tokens"",\n                       typeConverter=TypeConverters.toListString)\n\n    minLength = Param(Params._dummy(),\n                      ""minLength"",\n                      ""Set the minimum allowed legth for each token"",\n                      typeConverter=TypeConverters.toInt)\n\n    maxLength = Param(Params._dummy(),\n                      ""maxLength"",\n                      ""Set the maximum allowed legth for each token"",\n                      typeConverter=TypeConverters.toInt)\n\n    name = \'Tokenizer\'\n\n    @keyword_only\n    def __init__(self):\n        super(Tokenizer, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.Tokenizer"")\n        self._setDefault(\n            targetPattern=""\\\\S+"",\n            contextChars=[""."", "","", "";"", "":"", ""!"", ""?"", ""*"", ""-"", ""("", "")"", ""\\"""", ""\'""],\n            caseSensitiveExceptions=True,\n            minLength=0,\n            maxLength=99999\n        )\n\n    def getInfixPatterns(self):\n        return self.getOrDefault(""infixPatterns"")\n\n    def getSuffixPattern(self):\n        return self.getOrDefault(""suffixPattern"")\n\n    def getPrefixPattern(self):\n        return self.getOrDefault(""prefixPattern"")\n\n    def getContextChars(self):\n        return self.getOrDefault(""contextChars"")\n\n    def getSplitChars(self):\n        return self.getOrDefault(""splitChars"")\n\n    def setTargetPattern(self, value):\n        return self._set(targetPattern=value)\n\n    def setPrefixPattern(self, value):\n        return self._set(prefixPattern=value)\n\n    def setSuffixPattern(self, value):\n        return self._set(suffixPattern=value)\n\n    def setInfixPatterns(self, value):\n        return self._set(infixPatterns=value)\n\n    def addInfixPattern(self, value):\n        try:\n            infix_patterns = self.getInfixPatterns()\n        except KeyError:\n            infix_patterns = []\n        infix_patterns.insert(0, value)\n        return self._set(infixPatterns=infix_patterns)\n\n    def setExceptions(self, value):\n        return self._set(exceptions=value)\n\n    def getExceptions(self):\n        return self.getOrDefault(""exceptions"")\n\n    def addException(self, value):\n        try:\n            exception_tokens = self.getExceptions()\n        except KeyError:\n            exception_tokens = []\n        exception_tokens.append(value)\n        return self._set(exceptions=exception_tokens)\n\n    def setCaseSensitiveExceptions(self, value):\n        return self._set(caseSensitiveExceptions=value)\n\n    def getCaseSensitiveExceptions(self):\n        return self.getOrDefault(""caseSensitiveExceptions"")\n\n    def setContextChars(self, value):\n        return self._set(contextChars=value)\n\n    def addContextChars(self, value):\n        try:\n            context_chars = self.getContextChars()\n        except KeyError:\n            context_chars = []\n        context_chars.append(value)\n        return self._set(contextChars=context_chars)\n\n    def setSplitPattern(self, value):\n        return self._set(splitPattern=value)\n\n    def setSplitChars(self, value):\n        return self._set(splitChars=value)\n\n    def addSplitChars(self, value):\n        try:\n            split_chars = self.getSplitChars()\n        except KeyError:\n            split_chars = []\n        split_chars.append(value)\n        return self._set(splitChars=split_chars)\n\n    def setMinLength(self, value):\n        return self._set(minLength=value)\n\n    def setMaxLength(self, value):\n        return self._set(maxLength=value)\n\n    def _create_model(self, java_model):\n        return TokenizerModel(java_model=java_model)\n\n\nclass TokenizerModel(AnnotatorModel):\n    name = ""TokenizerModel""\n\n    exceptions = Param(Params._dummy(),\n                       ""exceptions"",\n                       ""Words that won\'t be affected by tokenization rules"",\n                       typeConverter=TypeConverters.toListString)\n\n    caseSensitiveExceptions = Param(Params._dummy(),\n                                    ""caseSensitiveExceptions"",\n                                    ""Whether to care for case sensitiveness in exceptions"",\n                                    typeConverter=TypeConverters.toBoolean)\n\n    targetPattern = Param(Params._dummy(),\n                          ""targetPattern"",\n                          ""pattern to grab from text as token candidates. Defaults \\S+"",\n                          typeConverter=TypeConverters.toString)\n\n    rules = Param(Params._dummy(),\n                  ""rules"",\n                  ""Rules structure factory containing pre processed regex rules"",\n                  typeConverter=TypeConverters.identity)\n\n    splitPattern = Param(Params._dummy(),\n                         ""splitPattern"",\n                         ""character list used to separate from the inside of tokens"",\n                         typeConverter=TypeConverters.toString)\n\n    splitChars = Param(Params._dummy(),\n                       ""splitChars"",\n                       ""character list used to separate from the inside of tokens"",\n                       typeConverter=TypeConverters.toListString)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.TokenizerModel"", java_model=None):\n        super(TokenizerModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n        self._setDefault(\n            targetPattern=""\\\\S+"",\n            caseSensitiveExceptions=True\n        )\n\n    def setSplitPattern(self, value):\n        return self._set(splitPattern=value)\n\n    def setSplitChars(self, value):\n        return self._set(splitChars=value)\n\n    def addSplitChars(self, value):\n        try:\n            split_chars = self.getSplitChars()\n        except KeyError:\n            split_chars = []\n        split_chars.append(value)\n        return self._set(splitChars=split_chars)\n\n    @staticmethod\n    def pretrained(name=""token_rules"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(TokenizerModel, name, lang, remote_loc)\n\n\nclass ChunkTokenizer(Tokenizer):\n    name = \'ChunkTokenizer\'\n\n    @keyword_only\n    def __init__(self):\n        super(Tokenizer, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.ChunkTokenizer"")\n\n    def _create_model(self, java_model):\n        return ChunkTokenizerModel(java_model=java_model)\n\n\nclass ChunkTokenizerModel(TokenizerModel):\n    name = \'ChunkTokenizerModel\'\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.ChunkTokenizerModel"", java_model=None):\n        super(TokenizerModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n\nclass Token2Chunk(AnnotatorModel):\n    name = ""Token2Chunk""\n\n    def __init__(self):\n        super(Token2Chunk, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.Token2Chunk"")\n\n\nclass Stemmer(AnnotatorModel):\n\n    language = Param(Params._dummy(), ""language"", ""stemmer algorithm"", typeConverter=TypeConverters.toString)\n\n    name = ""Stemmer""\n\n    @keyword_only\n    def __init__(self):\n        super(Stemmer, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.Stemmer"")\n        self._setDefault(\n            language=""english""\n        )\n\n\nclass Chunker(AnnotatorModel):\n\n    regexParsers = Param(Params._dummy(),\n                         ""regexParsers"",\n                         ""an array of grammar based chunk parsers"",\n                         typeConverter=TypeConverters.toListString)\n\n    name = ""Chunker""\n\n    @keyword_only\n    def __init__(self):\n        super(Chunker, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.Chunker"")\n\n    def setRegexParsers(self, value):\n        return self._set(regexParsers=value)\n\n\nclass Normalizer(AnnotatorApproach):\n\n    cleanupPatterns = Param(Params._dummy(),\n                            ""cleanupPatterns"",\n                            ""normalization regex patterns which match will be removed from token"",\n                            typeConverter=TypeConverters.toListString)\n\n    lowercase = Param(Params._dummy(),\n                      ""lowercase"",\n                      ""whether to convert strings to lowercase"")\n\n    slangMatchCase = Param(Params._dummy(),\n                           ""slangMatchCase"",\n                           ""whether or not to be case sensitive to match slangs. Defaults to false."")\n\n    slangDictionary = Param(Params._dummy(),\n                            ""slangDictionary"",\n                            ""slang dictionary is a delimited text. needs \'delimiter\' in options"",\n                            typeConverter=TypeConverters.identity)\n\n    @keyword_only\n    def __init__(self):\n        super(Normalizer, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.Normalizer"")\n        self._setDefault(\n            cleanupPatterns=[""[^\\\\pL+]""],\n            lowercase=False,\n            slangMatchCase=False\n        )\n\n    def setCleanupPatterns(self, value):\n        return self._set(cleanupPatterns=value)\n\n    def setLowercase(self, value):\n        return self._set(lowercase=value)\n\n    def setSlangDictionary(self, path, delimiter, read_as=ReadAs.TEXT, options={""format"": ""text""}):\n        opts = options.copy()\n        if ""delimiter"" not in opts:\n            opts[""delimiter""] = delimiter\n        return self._set(slangDictionary=ExternalResource(path, read_as, opts))\n\n    def _create_model(self, java_model):\n        return NormalizerModel(java_model=java_model)\n\n\nclass NormalizerModel(AnnotatorModel):\n\n    cleanupPatterns = Param(Params._dummy(),\n                            ""cleanupPatterns"",\n                            ""normalization regex patterns which match will be removed from token"",\n                            typeConverter=TypeConverters.toListString)\n\n    lowercase = Param(Params._dummy(),\n                      ""lowercase"",\n                      ""whether to convert strings to lowercase"")\n\n    slangMatchCase = Param(Params._dummy(),\n                           ""slangMatchCase"",\n                           ""whether or not to be case sensitive to match slangs. Defaults to false."")\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.NormalizerModel"", java_model=None):\n        super(NormalizerModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    name = ""NormalizerModel""\n\n\nclass RegexMatcher(AnnotatorApproach):\n\n    strategy = Param(Params._dummy(),\n                     ""strategy"",\n                     ""MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE"",\n                     typeConverter=TypeConverters.toString)\n    externalRules = Param(Params._dummy(),\n                          ""externalRules"",\n                          ""external resource to rules, needs \'delimiter\' in options"",\n                          typeConverter=TypeConverters.identity)\n\n    @keyword_only\n    def __init__(self):\n        super(RegexMatcher, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.RegexMatcher"")\n        self._setDefault(\n            strategy=""MATCH_ALL""\n        )\n\n    def setStrategy(self, value):\n        return self._set(strategy=value)\n\n    def setExternalRules(self, path, delimiter, read_as=ReadAs.TEXT, options={""format"": ""text""}):\n        opts = options.copy()\n        if ""delimiter"" not in opts:\n            opts[""delimiter""] = delimiter\n        return self._set(externalRules=ExternalResource(path, read_as, opts))\n\n    def _create_model(self, java_model):\n        return RegexMatcherModel(java_model=java_model)\n\n\nclass RegexMatcherModel(AnnotatorModel):\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.RegexMatcherModel"", java_model=None):\n        super(RegexMatcherModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    name = ""RegexMatcherModel""\n\n\nclass Lemmatizer(AnnotatorApproach):\n    dictionary = Param(Params._dummy(),\n                       ""dictionary"",\n                       ""lemmatizer external dictionary."" +\n                       "" needs \'keyDelimiter\' and \'valueDelimiter\' in options for parsing target text"",\n                       typeConverter=TypeConverters.identity)\n\n    @keyword_only\n    def __init__(self):\n        super(Lemmatizer, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.Lemmatizer"")\n\n    def _create_model(self, java_model):\n        return LemmatizerModel(java_model=java_model)\n\n    def setDictionary(self, path, key_delimiter, value_delimiter, read_as=ReadAs.TEXT,\n                      options={""format"": ""text""}):\n        opts = options.copy()\n        if ""keyDelimiter"" not in opts:\n            opts[""keyDelimiter""] = key_delimiter\n        if ""valueDelimiter"" not in opts:\n            opts[""valueDelimiter""] = value_delimiter\n        return self._set(dictionary=ExternalResource(path, read_as, opts))\n\n\nclass LemmatizerModel(AnnotatorModel):\n    name = ""LemmatizerModel""\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.LemmatizerModel"", java_model=None):\n        super(LemmatizerModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""lemma_antbnc"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(LemmatizerModel, name, lang, remote_loc)\n\n\nclass DateMatcherUtils(Params):\n    dateFormat = Param(Params._dummy(),\n                       ""dateFormat"",\n                       ""desired format for dates extracted"",\n                       typeConverter=TypeConverters.toString)\n\n    readMonthFirst = Param(Params._dummy(),\n                           ""readMonthFirst"",\n                           ""Whether to parse july 07/05/2015 or as 05/07/2015"",\n                           typeConverter=TypeConverters.toBoolean\n                           )\n\n    defaultDayWhenMissing = Param(Params._dummy(),\n                                  ""defaultDayWhenMissing"",\n                                  ""which day to set when it is missing from parsed input"",\n                                  typeConverter=TypeConverters.toInt\n                                  )\n\n    def setFormat(self, value):\n        return self._set(dateFormat=value)\n\n    def setReadMonthFirst(self, value):\n        return self._set(readMonthFirst=value)\n\n    def setDefaultDayWhenMissing(self, value):\n        return self._set(defaultDayWhenMissing=value)\n\n\nclass DateMatcher(AnnotatorModel, DateMatcherUtils):\n\n    name = ""DateMatcher""\n\n    @keyword_only\n    def __init__(self):\n        super(DateMatcher, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.DateMatcher"")\n        self._setDefault(\n            dateFormat=""yyyy/MM/dd"",\n            readMonthFirst=True,\n            defaultDayWhenMissing=1\n        )\n\n\nclass MultiDateMatcher(AnnotatorModel, DateMatcherUtils):\n\n    name = ""MultiDateMatcher""\n\n    @keyword_only\n    def __init__(self):\n        super(MultiDateMatcher, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.MultiDateMatcher"")\n        self._setDefault(\n            dateFormat=""yyyy/MM/dd"",\n            readMonthFirst=True,\n            defaultDayWhenMissing=1\n        )\n\n\nclass TextMatcher(AnnotatorApproach):\n\n    entities = Param(Params._dummy(),\n                     ""entities"",\n                     ""ExternalResource for entities"",\n                     typeConverter=TypeConverters.identity)\n\n    caseSensitive = Param(Params._dummy(),\n                          ""caseSensitive"",\n                          ""whether to match regardless of case. Defaults true"",\n                          typeConverter=TypeConverters.toBoolean)\n\n    mergeOverlapping = Param(Params._dummy(),\n                             ""mergeOverlapping"",\n                             ""whether to merge overlapping matched chunks. Defaults false"",\n                             typeConverter=TypeConverters.toBoolean)\n\n    @keyword_only\n    def __init__(self):\n        super(TextMatcher, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.TextMatcher"")\n        self._setDefault(caseSensitive=True)\n        self._setDefault(mergeOverlapping=False)\n\n    def _create_model(self, java_model):\n        return TextMatcherModel(java_model=java_model)\n\n    def setEntities(self, path, read_as=ReadAs.TEXT, options={""format"": ""text""}):\n        return self._set(entities=ExternalResource(path, read_as, options.copy()))\n\n    def setCaseSensitive(self, b):\n        return self._set(caseSensitive=b)\n\n    def setMergeOverlapping(self, b):\n        return self._set(mergeOverlapping=b)\n\n\nclass TextMatcherModel(AnnotatorModel):\n    name = ""TextMatcherModel""\n\n    mergeOverlapping = Param(Params._dummy(),\n                             ""mergeOverlapping"",\n                             ""whether to merge overlapping matched chunks. Defaults false"",\n                             typeConverter=TypeConverters.toBoolean)\n\n    searchTrie = Param(Params._dummy(),\n                       ""searchTrie"",\n                       ""searchTrie"",\n                       typeConverter=TypeConverters.identity)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.TextMatcherModel"", java_model=None):\n        super(TextMatcherModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    def setMergeOverlapping(self, b):\n        return self._set(mergeOverlapping=b)\n\n    @staticmethod\n    def pretrained(name, lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(TextMatcherModel, name, lang, remote_loc)\n\n\nclass BigTextMatcher(AnnotatorApproach, HasStorage):\n\n    entities = Param(Params._dummy(),\n                     ""entities"",\n                     ""ExternalResource for entities"",\n                     typeConverter=TypeConverters.identity)\n\n    caseSensitive = Param(Params._dummy(),\n                          ""caseSensitive"",\n                          ""whether to ignore case in index lookups"",\n                          typeConverter=TypeConverters.toBoolean)\n\n    mergeOverlapping = Param(Params._dummy(),\n                             ""mergeOverlapping"",\n                             ""whether to merge overlapping matched chunks. Defaults false"",\n                             typeConverter=TypeConverters.toBoolean)\n\n    tokenizer = Param(Params._dummy(),\n                      ""tokenizer"",\n                      ""TokenizerModel to use to tokenize input file for building a Trie"",\n                      typeConverter=TypeConverters.identity)\n\n    @keyword_only\n    def __init__(self):\n        super(BigTextMatcher, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.btm.BigTextMatcher"")\n        self._setDefault(caseSensitive=True)\n        self._setDefault(mergeOverlapping=False)\n\n    def _create_model(self, java_model):\n        return TextMatcherModel(java_model=java_model)\n\n    def setEntities(self, path, read_as=ReadAs.TEXT, options={""format"": ""text""}):\n        return self._set(entities=ExternalResource(path, read_as, options.copy()))\n\n    def setCaseSensitive(self, b):\n        return self._set(caseSensitive=b)\n\n    def setMergeOverlapping(self, b):\n        return self._set(mergeOverlapping=b)\n\n    def setTokenizer(self, tokenizer_model):\n        tokenizer_model._transfer_params_to_java()\n        return self._set(tokenizer_model._java_obj)\n\n\nclass BigTextMatcherModel(AnnotatorModel, HasStorageModel):\n    name = ""BigTextMatcherModel""\n    databases = [\'TMVOCAB\', \'TMEDGES\', \'TMNODES\']\n\n    caseSensitive = Param(Params._dummy(),\n                          ""caseSensitive"",\n                          ""whether to ignore case in index lookups"",\n                          typeConverter=TypeConverters.toBoolean)\n\n    mergeOverlapping = Param(Params._dummy(),\n                             ""mergeOverlapping"",\n                             ""whether to merge overlapping matched chunks. Defaults false"",\n                             typeConverter=TypeConverters.toBoolean)\n\n    searchTrie = Param(Params._dummy(),\n                       ""searchTrie"",\n                       ""searchTrie"",\n                       typeConverter=TypeConverters.identity)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.btm.TextMatcherModel"", java_model=None):\n        super(BigTextMatcherModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    def setMergeOverlapping(self, b):\n        return self._set(mergeOverlapping=b)\n\n    def setCaseSensitive(self, v):\n        return self._set(caseSensitive=v)\n\n    @staticmethod\n    def pretrained(name, lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(TextMatcherModel, name, lang, remote_loc)\n\n    @staticmethod\n    def loadStorage(path, spark, storage_ref):\n        HasStorageModel.loadStorages(path, spark, storage_ref, BigTextMatcherModel.databases)\n\n\nclass PerceptronApproach(AnnotatorApproach):\n    posCol = Param(Params._dummy(),\n                   ""posCol"",\n                   ""column of Array of POS tags that match tokens"",\n                   typeConverter=TypeConverters.toString)\n\n    nIterations = Param(Params._dummy(),\n                        ""nIterations"",\n                        ""Number of iterations in training, converges to better accuracy"",\n                        typeConverter=TypeConverters.toInt)\n\n    @keyword_only\n    def __init__(self):\n        super(PerceptronApproach, self).__init__(\n            classname=""com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach"")\n        self._setDefault(\n            nIterations=5\n        )\n\n    def setPosCol(self, value):\n        return self._set(posCol=value)\n\n    def setIterations(self, value):\n        return self._set(nIterations=value)\n\n    def getNIterations(self):\n        return self.getOrDefault(self.nIterations)\n\n    def _create_model(self, java_model):\n        return PerceptronModel(java_model=java_model)\n\n\nclass PerceptronModel(AnnotatorModel):\n    name = ""PerceptronModel""\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel"", java_model=None):\n        super(PerceptronModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""pos_anc"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(PerceptronModel, name, lang, remote_loc)\n\n\nclass SentenceDetectorParams:\n    useAbbreviations = Param(Params._dummy(),\n                             ""useAbbreviations"",\n                             ""whether to apply abbreviations at sentence detection"",\n                             typeConverter=TypeConverters.toBoolean)\n\n    customBounds = Param(Params._dummy(),\n                         ""customBounds"",\n                         ""characters used to explicitly mark sentence bounds"",\n                         typeConverter=TypeConverters.toListString)\n\n    useCustomBoundsOnly = Param(Params._dummy(),\n                                ""useCustomBoundsOnly"",\n                                ""Only utilize custom bounds in sentence detection"",\n                                typeConverter=TypeConverters.toBoolean)\n\n    explodeSentences = Param(Params._dummy(),\n                             ""explodeSentences"",\n                             ""whether to explode each sentence into a different row, for better parallelization. Defaults to false."",\n                             typeConverter=TypeConverters.toBoolean)\n\n    splitLength = Param(Params._dummy(),\n                        ""splitLength"",\n                        ""length at which sentences will be forcibly split."",\n                        typeConverter=TypeConverters.toInt)\n\n    minLength = Param(Params._dummy(),\n                      ""minLength"",\n                      ""Set the minimum allowed length for each sentence."",\n                      typeConverter=TypeConverters.toInt)\n\n    maxLength = Param(Params._dummy(),\n                      ""maxLength"",\n                      ""Set the maximum allowed length for each sentence"",\n                      typeConverter=TypeConverters.toInt)\n\n\nclass SentenceDetector(AnnotatorModel, SentenceDetectorParams):\n\n    name = \'SentenceDetector\'\n\n    def setCustomBounds(self, value):\n        return self._set(customBounds=value)\n\n    def setUseAbbreviations(self, value):\n        return self._set(useAbbreviations=value)\n\n    def setUseCustomBoundsOnly(self, value):\n        return self._set(useCustomBoundsOnly=value)\n\n    def setExplodeSentences(self, value):\n        return self._set(explodeSentences=value)\n\n    def setSplitLength(self, value):\n        return self._set(splitLength=value)\n\n    def setMinLength(self, value):\n        return self._set(minLength=value)\n\n    def setMaxLength(self, value):\n        return self._set(maxLength=value)\n\n    @keyword_only\n    def __init__(self):\n        super(SentenceDetector, self).__init__(\n            classname=""com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector"")\n        self._setDefault(\n            useAbbreviations=True,\n            useCustomBoundsOnly=False,\n            customBounds=[],\n            explodeSentences=False,\n            minLength=0,\n            maxLength=99999\n        )\n\n\nclass DeepSentenceDetector(AnnotatorModel, SentenceDetectorParams):\n\n    includesPragmaticSegmenter = Param(Params._dummy(),\n                                       ""includesPragmaticSegmenter"",\n                                       ""Whether to include rule-based sentence detector as first filter"",\n                                       typeConverter=TypeConverters.toBoolean)\n\n    endPunctuation = Param(\n        Params._dummy(), ""endPunctuation"",\n        ""An array of symbols that deep sentence detector will consider as end of sentence punctuation"",\n        typeConverter=TypeConverters.toListString)\n\n    name = ""DeepSentenceDetector""\n\n    def setIncludePragmaticSegmenter(self, value):\n        return self._set(includesPragmaticSegmenter=value)\n\n    def setEndPunctuation(self, value):\n        return self._set(endPunctuation=value)\n\n    def setExplodeSentences(self, value):\n        return self._set(explodeSentences=value)\n\n    def setCustomBounds(self, value):\n        return self._set(customBounds=value)\n\n    def setUseAbbreviations(self, value):\n        return self._set(useAbbreviations=value)\n\n    def setUseCustomBoundsOnly(self, value):\n        return self._set(useCustomBoundsOnly=value)\n\n    def setSplitLength(self, value):\n        return self._set(splitLength=value)\n\n    @keyword_only\n    def __init__(self):\n        super(DeepSentenceDetector, self).__init__(\n            classname=""com.johnsnowlabs.nlp.annotators.sbd.deep.DeepSentenceDetector"")\n        self._setDefault(includesPragmaticSegmenter=False, endPunctuation=[""."", ""!"", ""?""],\n                         explodeSentences=False)\n\n\nclass SentimentDetector(AnnotatorApproach):\n    dictionary = Param(Params._dummy(),\n                       ""dictionary"",\n                       ""path for dictionary to sentiment analysis"",\n                       typeConverter=TypeConverters.identity)\n\n    positiveMultiplier = Param(Params._dummy(),\n                               ""positiveMultiplier"",\n                               ""multiplier for positive sentiments. Defaults 1.0"",\n                               typeConverter=TypeConverters.toFloat)\n\n    negativeMultiplier = Param(Params._dummy(),\n                               ""negativeMultiplier"",\n                               ""multiplier for negative sentiments. Defaults -1.0"",\n                               typeConverter=TypeConverters.toFloat)\n\n    incrementMultiplier = Param(Params._dummy(),\n                                ""incrementMultiplier"",\n                                ""multiplier for increment sentiments. Defaults 2.0"",\n                                typeConverter=TypeConverters.toFloat)\n\n    decrementMultiplier = Param(Params._dummy(),\n                                ""decrementMultiplier"",\n                                ""multiplier for decrement sentiments. Defaults -2.0"",\n                                typeConverter=TypeConverters.toFloat)\n\n    reverseMultiplier = Param(Params._dummy(),\n                              ""reverseMultiplier"",\n                              ""multiplier for revert sentiments. Defaults -1.0"",\n                              typeConverter=TypeConverters.toFloat)\n\n    enableScore = Param(Params._dummy(),\n                        ""enableScore"",\n                        ""if true, score will show as the double value, else will output string \\""positive\\"" or \\""negative\\"". Defaults false"",\n                        typeConverter=TypeConverters.toBoolean)\n\n\n    def __init__(self):\n        super(SentimentDetector, self).__init__(\n            classname=""com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector"")\n        self._setDefault(positiveMultiplier=1.0, negativeMultiplier=-1.0, incrementMultiplier=2.0,\n                         decrementMultiplier=-2.0, reverseMultiplier=-1.0, enableScore=False)\n\n    def setDictionary(self, path, delimiter, read_as=ReadAs.TEXT, options={\'format\': \'text\'}):\n        opts = options.copy()\n        if ""delimiter"" not in opts:\n            opts[""delimiter""] = delimiter\n        return self._set(dictionary=ExternalResource(path, read_as, opts))\n\n    def _create_model(self, java_model):\n        return SentimentDetectorModel(java_model=java_model)\n\n\nclass SentimentDetectorModel(AnnotatorModel):\n    name = ""SentimentDetectorModel""\n\n    positiveMultiplier = Param(Params._dummy(),\n                               ""positiveMultiplier"",\n                               ""multiplier for positive sentiments. Defaults 1.0"",\n                               typeConverter=TypeConverters.toFloat)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetectorModel"",\n                 java_model=None):\n        super(SentimentDetectorModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n\nclass ViveknSentimentApproach(AnnotatorApproach):\n    sentimentCol = Param(Params._dummy(),\n                         ""sentimentCol"",\n                         ""column with the sentiment result of every row. Must be \'positive\' or \'negative\'"",\n                         typeConverter=TypeConverters.toString)\n\n    pruneCorpus = Param(Params._dummy(),\n                        ""pruneCorpus"",\n                        ""Removes unfrequent scenarios from scope. The higher the better performance. Defaults 1"",\n                        typeConverter=TypeConverters.toInt)\n\n    importantFeatureRatio = Param(Params._dummy(),\n                                  ""importantFeatureRatio"",\n                                  ""proportion of feature content to be considered relevant. Defaults to 0.5"",\n                                  typeConverter=TypeConverters.toFloat)\n\n    unimportantFeatureStep = Param(Params._dummy(),\n                                   ""unimportantFeatureStep"",\n                                   ""proportion to lookahead in unimportant features. Defaults to 0.025"",\n                                   typeConverter=TypeConverters.toFloat)\n\n    featureLimit = Param(Params._dummy(),\n                         ""featureLimit"",\n                         ""content feature limit, to boost performance in very dirt text. Default disabled with -1"",\n                         typeConverter=TypeConverters.toInt)\n\n    @keyword_only\n    def __init__(self):\n        super(ViveknSentimentApproach, self).__init__(\n            classname=""com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach"")\n        self._setDefault(pruneCorpus=1, importantFeatureRatio=0.5, unimportantFeatureStep=0.025, featureLimit=-1)\n\n    def setSentimentCol(self, value):\n        return self._set(sentimentCol=value)\n\n    def setPruneCorpus(self, value):\n        return self._set(pruneCorpus=value)\n\n    def _create_model(self, java_model):\n        return ViveknSentimentModel(java_model=java_model)\n\n\nclass ViveknSentimentModel(AnnotatorModel):\n    name = ""ViveknSentimentModel""\n\n    importantFeatureRatio = Param(Params._dummy(),\n                                  ""importantFeatureRatio"",\n                                  ""proportion of feature content to be considered relevant. Defaults to 0.5"",\n                                  typeConverter=TypeConverters.toFloat)\n\n    unimportantFeatureStep = Param(Params._dummy(),\n                                   ""unimportantFeatureStep"",\n                                   ""proportion to lookahead in unimportant features. Defaults to 0.025"",\n                                   typeConverter=TypeConverters.toFloat)\n\n    featureLimit = Param(Params._dummy(),\n                         ""featureLimit"",\n                         ""content feature limit, to boost performance in very dirt text. Default disabled with -1"",\n                         typeConverter=TypeConverters.toInt)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentModel"", java_model=None):\n        super(ViveknSentimentModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""sentiment_vivekn"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(ViveknSentimentModel, name, lang, remote_loc)\n\n\nclass NorvigSweetingApproach(AnnotatorApproach):\n    dictionary = Param(Params._dummy(),\n                       ""dictionary"",\n                       ""dictionary needs \'tokenPattern\' regex in dictionary for separating words"",\n                       typeConverter=TypeConverters.identity)\n\n    caseSensitive = Param(Params._dummy(),\n                          ""caseSensitive"",\n                          ""whether to ignore case sensitivty"",\n                          typeConverter=TypeConverters.toBoolean)\n\n    doubleVariants = Param(Params._dummy(),\n                           ""doubleVariants"",\n                           ""whether to use more expensive spell checker"",\n                           typeConverter=TypeConverters.toBoolean)\n\n    shortCircuit = Param(Params._dummy(),\n                         ""shortCircuit"",\n                         ""whether to use faster mode"",\n                         typeConverter=TypeConverters.toBoolean)\n\n    frequencyPriority = Param(Params._dummy(),\n                              ""frequencyPriority"",\n                              ""applies frequency over hamming in intersections. When false hamming takes priority"",\n                              typeConverter=TypeConverters.toBoolean)\n\n    wordSizeIgnore = Param(Params._dummy(),\n                           ""wordSizeIgnore"",\n                           ""minimum size of word before ignoring. Defaults to 3"",\n                           typeConverter=TypeConverters.toInt)\n\n    dupsLimit = Param(Params._dummy(),\n                      ""dupsLimit"",\n                      ""maximum duplicate of characters in a word to consider. Defaults to 2"",\n                      typeConverter=TypeConverters.toInt)\n\n    reductLimit = Param(Params._dummy(),\n                        ""reductLimit"",\n                        ""word reductions limit. Defaults to 3"",\n                        typeConverter=TypeConverters.toInt)\n\n    intersections = Param(Params._dummy(),\n                          ""intersections"",\n                          ""hamming intersections to attempt. Defaults to 10"",\n                          typeConverter=TypeConverters.toInt)\n\n    vowelSwapLimit = Param(Params._dummy(),\n                           ""vowelSwapLimit"",\n                           ""vowel swap attempts. Defaults to 6"",\n                           typeConverter=TypeConverters.toInt)\n\n    @keyword_only\n    def __init__(self):\n        super(NorvigSweetingApproach, self).__init__(\n            classname=""com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingApproach"")\n        self._setDefault(caseSensitive=False, doubleVariants=False, shortCircuit=False, wordSizeIgnore=3, dupsLimit=2,\n                         reductLimit=3, intersections=10, vowelSwapLimit=6, frequencyPriority=True)\n        self.dictionary_path = """"\n\n    def setDictionary(self, path, token_pattern=""\\S+"", read_as=ReadAs.TEXT, options={""format"": ""text""}):\n        self.dictionary_path = path\n        opts = options.copy()\n        if ""tokenPattern"" not in opts:\n            opts[""tokenPattern""] = token_pattern\n        return self._set(dictionary=ExternalResource(path, read_as, opts))\n\n    def setCaseSensitive(self, value):\n        return self._set(caseSensitive=value)\n\n    def setDoubleVariants(self, value):\n        return self._set(doubleVariants=value)\n\n    def setShortCircuit(self, value):\n        return self._set(shortCircuit=value)\n\n    def setFrequencyPriority(self, value):\n        return self._set(frequencyPriority=value)\n\n    def _create_model(self, java_model):\n        return NorvigSweetingModel(java_model=java_model)\n\n\nclass NorvigSweetingModel(AnnotatorModel):\n    name = ""NorvigSweetingModel""\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingModel"", java_model=None):\n        super(NorvigSweetingModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""spellcheck_norvig"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(NorvigSweetingModel, name, lang, remote_loc)\n\n\nclass SymmetricDeleteApproach(AnnotatorApproach):\n    corpus = Param(Params._dummy(),\n                   ""corpus"",\n                   ""folder or file with text that teaches about the language"",\n                   typeConverter=TypeConverters.identity)\n\n    dictionary = Param(Params._dummy(),\n                       ""dictionary"",\n                       ""folder or file with text that teaches about the language"",\n                       typeConverter=TypeConverters.identity)\n\n    maxEditDistance = Param(Params._dummy(),\n                            ""maxEditDistance"",\n                            ""max edit distance characters to derive strings from a word"",\n                            typeConverter=TypeConverters.toInt)\n\n    frequencyThreshold = Param(Params._dummy(),\n                               ""frequencyThreshold"",\n                               ""minimum frequency of words to be considered from training. "" +\n                               ""Increase if training set is LARGE. Defaults to 0"",\n                               typeConverter=TypeConverters.toInt)\n\n    deletesThreshold = Param(Params._dummy(),\n                             ""deletesThreshold"",\n                             ""minimum frequency of corrections a word needs to have to be considered from training."" +\n                             ""Increase if training set is LARGE. Defaults to 0"",\n                             typeConverter=TypeConverters.toInt)\n\n    dupsLimit = Param(Params._dummy(),\n                      ""dupsLimit"",\n                      ""maximum duplicate of characters in a word to consider. Defaults to 2"",\n                      typeConverter=TypeConverters.toInt)\n\n    @keyword_only\n    def __init__(self):\n        super(SymmetricDeleteApproach, self).__init__(\n            classname=""com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach"")\n        self._setDefault(maxEditDistance=3, frequencyThreshold=0, deletesThreshold=0, dupsLimit=2)\n        self.dictionary_path = """"\n\n    def setDictionary(self, path, token_pattern=""\\S+"", read_as=ReadAs.TEXT, options={""format"": ""text""}):\n        self.dictionary_path = path\n        opts = options.copy()\n        if ""tokenPattern"" not in opts:\n            opts[""tokenPattern""] = token_pattern\n        return self._set(dictionary=ExternalResource(path, read_as, opts))\n\n    def setMaxEditDistance(self, v):\n        return self._set(maxEditDistance=v)\n\n    def setFrequencyThreshold(self, v):\n        return self._set(frequencyThreshold=v)\n\n    def setDeletesThreshold(self, v):\n        return self._set(deletesThreshold=v)\n\n    def _create_model(self, java_model):\n        return SymmetricDeleteModel(java_model=java_model)\n\n\nclass SymmetricDeleteModel(AnnotatorModel):\n    name = ""SymmetricDeleteModel""\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteModel"",\n                 java_model=None):\n        super(SymmetricDeleteModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""spellcheck_sd"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(SymmetricDeleteModel, name, lang, remote_loc)\n\n\nclass NerApproach(Params):\n    labelColumn = Param(Params._dummy(),\n                        ""labelColumn"",\n                        ""Column with label per each token"",\n                        typeConverter=TypeConverters.toString)\n\n    entities = Param(Params._dummy(), ""entities"", ""Entities to recognize"", TypeConverters.toListString)\n\n    minEpochs = Param(Params._dummy(), ""minEpochs"", ""Minimum number of epochs to train"", TypeConverters.toInt)\n    maxEpochs = Param(Params._dummy(), ""maxEpochs"", ""Maximum number of epochs to train"", TypeConverters.toInt)\n\n    verbose = Param(Params._dummy(), ""verbose"", ""Level of verbosity during training"", TypeConverters.toInt)\n    randomSeed = Param(Params._dummy(), ""randomSeed"", ""Random seed"", TypeConverters.toInt)\n\n    def setLabelColumn(self, value):\n        return self._set(labelColumn=value)\n\n    def setEntities(self, tags):\n        return self._set(entities=tags)\n\n    def setMinEpochs(self, epochs):\n        return self._set(minEpochs=epochs)\n\n    def setMaxEpochs(self, epochs):\n        return self._set(maxEpochs=epochs)\n\n    def setVerbose(self, verboseValue):\n        return self._set(verbose=verboseValue)\n\n    def setRandomSeed(self, seed):\n        return self._set(randomSeed=seed)\n\n    def getLabelColumn(self):\n        return self.getOrDefault(self.labelColumn)\n\n\nclass NerCrfApproach(AnnotatorApproach, NerApproach):\n\n    l2 = Param(Params._dummy(), ""l2"", ""L2 regularization coefficient"", TypeConverters.toFloat)\n\n    c0 = Param(Params._dummy(), ""c0"", ""c0 params defining decay speed for gradient"", TypeConverters.toInt)\n\n    lossEps = Param(Params._dummy(), ""lossEps"", ""If Epoch relative improvement less than eps then training is stopped"",\n                    TypeConverters.toFloat)\n\n    minW = Param(Params._dummy(), ""minW"", ""Features with less weights then this param value will be filtered"",\n                 TypeConverters.toFloat)\n\n    includeConfidence = Param(Params._dummy(), ""includeConfidence"", ""external features is a delimited text. needs \'delimiter\' in options"",\n                              TypeConverters.toBoolean)\n\n    externalFeatures = Param(Params._dummy(), ""externalFeatures"", ""Additional dictionaries paths to use as a features"",\n                             TypeConverters.identity)\n\n    def setL2(self, l2value):\n        return self._set(l2=l2value)\n\n    def setC0(self, c0value):\n        return self._set(c0=c0value)\n\n    def setLossEps(self, eps):\n        return self._set(lossEps=eps)\n\n    def setMinW(self, w):\n        return self._set(minW=w)\n\n    def setExternalFeatures(self, path, delimiter, read_as=ReadAs.TEXT, options={""format"": ""text""}):\n        opts = options.copy()\n        if ""delimiter"" not in opts:\n            opts[""delimiter""] = delimiter\n        return self._set(externalFeatures=ExternalResource(path, read_as, opts))\n\n    def setIncludeConfidence(self, b):\n        return self._set(includeConfidence=b)\n\n    def _create_model(self, java_model):\n        return NerCrfModel(java_model=java_model)\n\n    @keyword_only\n    def __init__(self):\n        super(NerCrfApproach, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfApproach"")\n        self._setDefault(\n            minEpochs=0,\n            maxEpochs=1000,\n            l2=float(1),\n            c0=2250000,\n            lossEps=float(1e-3),\n            verbose=4,\n            includeConfidence=False\n        )\n\n\nclass NerCrfModel(AnnotatorModel):\n    name = ""NerCrfModel""\n\n    includeConfidence = Param(Params._dummy(), ""includeConfidence"", ""external features is a delimited text. needs \'delimiter\' in options"",\n                              TypeConverters.toBoolean)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfModel"", java_model=None):\n        super(NerCrfModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    def setIncludeConfidence(self, b):\n        return self._set(includeConfidence=b)\n\n    @staticmethod\n    def pretrained(name=""ner_crf"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(NerCrfModel, name, lang, remote_loc)\n\n\nclass NerDLApproach(AnnotatorApproach, NerApproach):\n\n    lr = Param(Params._dummy(), ""lr"", ""Learning Rate"", TypeConverters.toFloat)\n\n    po = Param(Params._dummy(), ""po"", ""Learning rate decay coefficient. Real Learning Rage = lr / (1 + po * epoch)"",\n               TypeConverters.toFloat)\n\n    batchSize = Param(Params._dummy(), ""batchSize"", ""Batch size"", TypeConverters.toInt)\n\n    dropout = Param(Params._dummy(), ""dropout"", ""Dropout coefficient"", TypeConverters.toFloat)\n\n    graphFolder = Param(Params._dummy(), ""graphFolder"", ""Folder path that contain external graph files"", TypeConverters.toString)\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n\n    useContrib = Param(Params._dummy(), ""useContrib"", ""whether to use contrib LSTM Cells. Not compatible with Windows. Might slightly improve accuracy."", TypeConverters.toBoolean)\n\n    validationSplit = Param(Params._dummy(), ""validationSplit"", ""Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off."",\n                            TypeConverters.toFloat)\n\n    evaluationLogExtended = Param(Params._dummy(), ""evaluationLogExtended"", ""Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off."",\n                                  TypeConverters.toBoolean)\n\n    testDataset = Param(Params._dummy(), ""testDataset"",\n                        ""Path to test dataset. If set used to calculate statistic on it during training."",\n                        TypeConverters.identity)\n\n    includeConfidence = Param(Params._dummy(), ""includeConfidence"",\n                              ""whether to include confidence scores in annotation metadata"",\n                              TypeConverters.toBoolean)\n\n    enableOutputLogs = Param(Params._dummy(), ""enableOutputLogs"",\n                             ""Whether to use stdout in addition to Spark logs."",\n                             TypeConverters.toBoolean)\n\n    outputLogsPath = Param(Params._dummy(), ""outputLogsPath"", ""Folder path to save training logs"", TypeConverters.toString)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setGraphFolder(self, p):\n        return self._set(graphFolder=p)\n\n    def setUseContrib(self, v):\n        if v and sys.version == \'win32\':\n            raise Exception(""Windows not supported to use contrib"")\n        return self._set(useContrib=v)\n\n    def setLr(self, v):\n        self._set(lr=v)\n        return self\n\n    def setPo(self, v):\n        self._set(po=v)\n        return self\n\n    def setBatchSize(self, v):\n        self._set(batchSize=v)\n        return self\n\n    def setDropout(self, v):\n        self._set(dropout=v)\n        return self\n\n    def _create_model(self, java_model):\n        return NerDLModel(java_model=java_model)\n\n    def setValidationSplit(self, v):\n        self._set(validationSplit=v)\n        return self\n\n    def setEvaluationLogExtended(self, v):\n        self._set(evaluationLogExtended=v)\n        return self\n\n    def setTestDataset(self, path, read_as=ReadAs.SPARK, options={""format"": ""parquet""}):\n        return self._set(testDataset=ExternalResource(path, read_as, options.copy()))\n\n    def setIncludeConfidence(self, value):\n        return self._set(includeConfidence=value)\n\n    def setEnableOutputLogs(self, value):\n        return self._set(enableOutputLogs=value)\n\n    def setOutputLogsPath(self, p):\n        return self._set(outputLogsPath=p)\n\n    @keyword_only\n    def __init__(self):\n        super(NerDLApproach, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach"")\n        uc = False if sys.platform == \'win32\' else True\n        self._setDefault(\n            minEpochs=0,\n            maxEpochs=50,\n            lr=float(0.001),\n            po=float(0.005),\n            batchSize=8,\n            dropout=float(0.5),\n            verbose=2,\n            useContrib=uc,\n            validationSplit=float(0.0),\n            evaluationLogExtended=False,\n            includeConfidence=False,\n            enableOutputLogs=False\n        )\n\n\nclass NerDLModel(AnnotatorModel, HasStorageRef):\n    name = ""NerDLModel""\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel"", java_model=None):\n        super(NerDLModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n        self._setDefault(includeConfidence=False)\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n    includeConfidence = Param(Params._dummy(), ""includeConfidence"",\n                              ""whether to include confidence scores in annotation metadata"",\n                              TypeConverters.toBoolean)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setIncludeConfidence(self, value):\n        return self._set(includeConfidence=value)\n\n    @staticmethod\n    def pretrained(name=""ner_dl"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(NerDLModel, name, lang, remote_loc)\n\n\nclass NerConverter(AnnotatorModel):\n    name = \'NerConverter\'\n\n    whiteList = Param(\n        Params._dummy(),\n        ""whiteList"",\n        ""If defined, list of entities to process. The rest will be ignored. Do not include IOB prefix on labels"",\n        typeConverter=TypeConverters.toListString\n    )\n\n    def setWhiteList(self, entities):\n        return self._set(whiteList=entities)\n\n    @keyword_only\n    def __init__(self):\n        super(NerConverter, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.ner.NerConverter"")\n\n\nclass DependencyParserApproach(AnnotatorApproach):\n    dependencyTreeBank = Param(Params._dummy(),\n                               ""dependencyTreeBank"",\n                               ""Dependency treebank source files"",\n                               typeConverter=TypeConverters.identity)\n\n    conllU = Param(Params._dummy(),\n                   ""conllU"",\n                   ""Universal Dependencies source files"",\n                   typeConverter=TypeConverters.identity)\n\n    numberOfIterations = Param(Params._dummy(),\n                               ""numberOfIterations"",\n                               ""Number of iterations in training, converges to better accuracy"",\n                               typeConverter=TypeConverters.toInt)\n\n    @keyword_only\n    def __init__(self):\n        super(DependencyParserApproach,\n              self).__init__(classname=""com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserApproach"")\n        self._setDefault(numberOfIterations=10)\n\n    def setNumberOfIterations(self, value):\n        return self._set(numberOfIterations=value)\n\n    def setDependencyTreeBank(self, path, read_as=ReadAs.TEXT, options={""key"": ""value""}):\n        opts = options.copy()\n        return self._set(dependencyTreeBank=ExternalResource(path, read_as, opts))\n\n    def setConllU(self, path, read_as=ReadAs.TEXT, options={""key"": ""value""}):\n        opts = options.copy()\n        return self._set(conllU=ExternalResource(path, read_as, opts))\n\n    def _create_model(self, java_model):\n        return DependencyParserModel(java_model=java_model)\n\n\nclass DependencyParserModel(AnnotatorModel):\n    name = ""DependencyParserModel""\n\n    perceptron = Param(Params._dummy(),\n                       ""perceptron"",\n                       ""Dependency parsing perceptron features"",\n                       typeConverter=TypeConverters.identity)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel"", java_model=None):\n        super(DependencyParserModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""dependency_conllu"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(DependencyParserModel, name, lang, remote_loc)\n\n\nclass TypedDependencyParserApproach(AnnotatorApproach):\n    conll2009 = Param(Params._dummy(),\n                      ""conll2009"",\n                      ""Path to file with CoNLL 2009 format"",\n                      typeConverter=TypeConverters.identity)\n\n    conllU = Param(Params._dummy(),\n                   ""conllU"",\n                   ""Universal Dependencies source files"",\n                   typeConverter=TypeConverters.identity)\n\n    numberOfIterations = Param(Params._dummy(),\n                               ""numberOfIterations"",\n                               ""Number of iterations in training, converges to better accuracy"",\n                               typeConverter=TypeConverters.toInt)\n\n    @keyword_only\n    def __init__(self):\n        super(TypedDependencyParserApproach,\n              self).__init__(classname=""com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserApproach"")\n\n    def setConll2009(self, path, read_as=ReadAs.TEXT, options={""key"": ""value""}):\n        opts = options.copy()\n        return self._set(conll2009=ExternalResource(path, read_as, opts))\n\n    def setConllU(self, path, read_as=ReadAs.TEXT, options={""key"": ""value""}):\n        opts = options.copy()\n        return self._set(conllU=ExternalResource(path, read_as, opts))\n\n    def setNumberOfIterations(self, value):\n        return self._set(numberOfIterations=value)\n\n    def _create_model(self, java_model):\n        return TypedDependencyParserModel(java_model=java_model)\n\n\nclass TypedDependencyParserModel(AnnotatorModel):\n\n    name = ""TypedDependencyParserModel""\n\n    trainOptions = Param(Params._dummy(),\n                         ""trainOptions"",\n                         ""Training Options"",\n                         typeConverter=TypeConverters.identity)\n\n    trainParameters = Param(Params._dummy(),\n                            ""trainParameters"",\n                            ""Training Parameters"",\n                            typeConverter=TypeConverters.identity)\n\n    trainDependencyPipe = Param(Params._dummy(),\n                                ""trainDependencyPipe"",\n                                ""Training dependency pipe"",\n                                typeConverter=TypeConverters.identity)\n\n    conllFormat = Param(Params._dummy(),\n                        ""conllFormat"",\n                        ""CoNLL Format"",\n                        typeConverter=TypeConverters.toString)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserModel"",\n                 java_model=None):\n        super(TypedDependencyParserModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""dependency_typed_conllu"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(TypedDependencyParserModel, name, lang, remote_loc)\n\n\nclass WordEmbeddings(AnnotatorApproach, HasEmbeddingsProperties, HasStorage):\n\n    name = ""WordEmbeddings""\n\n    writeBufferSize = Param(Params._dummy(),\n                            ""writeBufferSize"",\n                            ""buffer size limit before dumping to disk storage while writing"",\n                            typeConverter=TypeConverters.toInt)\n\n    readCacheSize = Param(Params._dummy(),\n                          ""readCacheSize"",\n                          ""cache size for items retrieved from storage. Increase for performance but higher memory consumption"",\n                          typeConverter=TypeConverters.toInt)\n\n    def setWriteBufferSize(self, v):\n        return self._set(writeBufferSize=v)\n\n    def setReadCacheSize(self, v):\n        return self._set(readCacheSize=v)\n\n    @keyword_only\n    def __init__(self):\n        super(WordEmbeddings, self).__init__(classname=""com.johnsnowlabs.nlp.embeddings.WordEmbeddings"")\n        self._setDefault(\n            caseSensitive=False,\n            writeBufferSize=10000,\n            storageRef=self.uid\n        )\n\n    def _create_model(self, java_model):\n        return WordEmbeddingsModel(java_model=java_model)\n\n\nclass WordEmbeddingsModel(AnnotatorModel, HasEmbeddingsProperties, HasStorageModel):\n\n    name = ""WordEmbeddingsModel""\n    databases = [\'EMBEDDINGS\']\n\n    readCacheSize = Param(Params._dummy(),\n                          ""readCacheSize"",\n                          ""cache size for items retrieved from storage. Increase for performance but higher memory consumption"",\n                          typeConverter=TypeConverters.toInt)\n\n    def setReadCacheSize(self, v):\n        return self._set(readCacheSize=v)\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel"", java_model=None):\n        super(WordEmbeddingsModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def overallCoverage(dataset, embeddings_col):\n        from sparknlp.internal import _EmbeddingsOverallCoverage\n        from sparknlp.common import CoverageResult\n        return CoverageResult(_EmbeddingsOverallCoverage(dataset, embeddings_col).apply())\n\n    @staticmethod\n    def withCoverageColumn(dataset, embeddings_col, output_col=\'coverage\'):\n        from sparknlp.internal import _EmbeddingsCoverageColumn\n        from pyspark.sql import DataFrame\n        return DataFrame(_EmbeddingsCoverageColumn(dataset, embeddings_col, output_col).apply(), dataset.sql_ctx)\n\n    @staticmethod\n    def pretrained(name=""glove_100d"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(WordEmbeddingsModel, name, lang, remote_loc)\n\n    @staticmethod\n    def loadStorage(path, spark, storage_ref):\n        HasStorageModel.loadStorages(path, spark, storage_ref, WordEmbeddingsModel.databases)\n\n\nclass BertEmbeddings(AnnotatorModel, HasEmbeddingsProperties, HasCaseSensitiveProperties, HasStorageRef):\n\n    name = ""BertEmbeddings""\n\n    maxSentenceLength = Param(Params._dummy(),\n                              ""maxSentenceLength"",\n                              ""Max sentence length to process"",\n                              typeConverter=TypeConverters.toInt)\n\n    batchSize = Param(Params._dummy(),\n                      ""batchSize"",\n                      ""Batch size. Large values allows faster processing but requires more memory."",\n                      typeConverter=TypeConverters.toInt)\n\n    configProtoBytes = Param(Params._dummy(),\n                             ""configProtoBytes"",\n                             ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"",\n                             TypeConverters.toListString)\n\n    poolingLayer = Param(Params._dummy(),\n                         ""poolingLayer"", ""Set BERT pooling layer to: -1 for last hidden layer, -2 for second-to-last hidden layer, and 0 for first layer which is called embeddings"",\n                         typeConverter=TypeConverters.toInt)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setMaxSentenceLength(self, value):\n        return self._set(maxSentenceLength=value)\n\n    def setBatchSize(self, value):\n        return self._set(batchSize=value)\n\n    def setPoolingLayer(self, layer):\n        if layer == 0:\n            return self._set(poolingLayer=layer)\n        elif layer == -1:\n            return self._set(poolingLayer=layer)\n        elif layer == -2:\n            return self._set(poolingLayer=layer)\n        else:\n            return self._set(poolingLayer=0)\n\n    def getPoolingLayer(self):\n        return self.getOrDefault(self.poolingLayer)\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.embeddings.BertEmbeddings"", java_model=None):\n        super(BertEmbeddings, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n        self._setDefault(\n            dimension=768,\n            batchSize=32,\n            maxSentenceLength=128,\n            caseSensitive=True,\n            poolingLayer=0\n        )\n\n    @staticmethod\n    def loadSavedModel(folder, spark_session):\n        from sparknlp.internal import _BertLoader\n        jModel = _BertLoader(folder, spark_session._jsparkSession)._java_obj\n        return BertEmbeddings(java_model=jModel)\n\n\n    @staticmethod\n    def pretrained(name=""bert_base_cased"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(BertEmbeddings, name, lang, remote_loc)\n\n\nclass SentenceEmbeddings(AnnotatorModel, HasEmbeddingsProperties, HasStorageRef):\n\n    name = ""SentenceEmbeddings""\n\n    @keyword_only\n    def __init__(self):\n        super(SentenceEmbeddings, self).__init__(classname=""com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings"")\n        self._setDefault(\n            poolingStrategy=""AVERAGE""\n        )\n\n    poolingStrategy = Param(Params._dummy(),\n                            ""poolingStrategy"",\n                            ""Choose how you would like to aggregate Word Embeddings to Sentence Embeddings: AVERAGE or SUM"",\n                            typeConverter=TypeConverters.toString)\n\n    def setPoolingStrategy(self, strategy):\n        if strategy == ""AVERAGE"":\n            return self._set(poolingStrategy=strategy)\n        elif strategy == ""SUM"":\n            return self._set(poolingStrategy=strategy)\n        else:\n            return self._set(poolingStrategy=""AVERAGE"")\n\n\nclass StopWordsCleaner(AnnotatorModel):\n\n    name = ""StopWordsCleaner""\n\n    @keyword_only\n    def __init__(self):\n        super(StopWordsCleaner, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.StopWordsCleaner"")\n        self._setDefault(\n            stopWords=StopWordsCleaner.loadDefaultStopWords(""english""),\n            caseSensitive=False,\n            locale=self._java_obj.getLocale()\n        )\n\n    stopWords = Param(Params._dummy(), ""stopWords"", ""The words to be filtered out"",\n                      typeConverter=TypeConverters.toListString)\n    caseSensitive = Param(Params._dummy(), ""caseSensitive"", ""whether to do a case sensitive "" +\n                          ""comparison over the stop words"", typeConverter=TypeConverters.toBoolean)\n    locale = Param(Params._dummy(), ""locale"", ""locale of the input. ignored when case sensitive "" +\n                   ""is true"", typeConverter=TypeConverters.toString)\n\n    def setStopWords(self, value):\n        return self._set(stopWords=value)\n\n    def setCaseSensitive(self, value):\n        return self._set(caseSensitive=value)\n\n    def setLocale(self, value):\n        return self._set(locale=value)\n\n    def loadDefaultStopWords(language=""english""):\n        from pyspark.ml.wrapper import _jvm\n\n        """"""\n        Loads the default stop words for the given language.\n        Supported languages: danish, dutch, english, finnish, french, german, hungarian,\n        italian, norwegian, portuguese, russian, spanish, swedish, turkish\n        """"""\n        stopWordsObj = _jvm().org.apache.spark.ml.feature.StopWordsRemover\n        return list(stopWordsObj.loadDefaultStopWords(language))\n\n\nclass NGramGenerator(AnnotatorModel):\n\n    name = ""NGramGenerator""\n\n    @keyword_only\n    def __init__(self):\n        super(NGramGenerator, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.NGramGenerator"")\n        self._setDefault(\n            n=2,\n            enableCumulative=False\n        )\n\n    n = Param(Params._dummy(), ""n"", ""number elements per n-gram (>=1)"", typeConverter=TypeConverters.toInt)\n    enableCumulative = Param(Params._dummy(), ""enableCumulative"", ""whether to calculate just the actual n-grams "" +\n                             ""or all n-grams from 1 through n"", typeConverter=TypeConverters.toBoolean)\n\n    delimiter = Param(Params._dummy(), ""delimiter"", ""String to use to join the tokens "", typeConverter=TypeConverters.toString)\n\n    def setN(self, value):\n        """"""\n        Sets the value of :py:attr:`n`.\n        """"""\n        return self._set(n=value)\n\n    def setEnableCumulative(self, value):\n        """"""\n        Sets the value of :py:attr:`enableCumulative`.\n        """"""\n        return self._set(enableCumulative=value)\n\n    def setDelimiter(self, value):\n        """"""\n        Sets the value of :py:attr:`delimiter`.\n        """"""\n        if len(value) > 1:\n            raise Exception(""Delimiter should have length == 1"")\n        return self._set(delimiter=value)\n\n\nclass ChunkEmbeddings(AnnotatorModel):\n\n    name = ""ChunkEmbeddings""\n\n    @keyword_only\n    def __init__(self):\n        super(ChunkEmbeddings, self).__init__(classname=""com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings"")\n        self._setDefault(\n            poolingStrategy=""AVERAGE""\n        )\n\n    poolingStrategy = Param(Params._dummy(),\n                            ""poolingStrategy"",\n                            ""Choose how you would like to aggregate Word Embeddings to Chunk Embeddings:"" +\n                            ""AVERAGE or SUM"",\n                            typeConverter=TypeConverters.toString)\n    skipOOV = Param(Params._dummy(), ""skipOOV"", ""Whether to discard default vectors for OOV words from the aggregation / pooling "", typeConverter=TypeConverters.toBoolean)\n\n    def setPoolingStrategy(self, strategy):\n        """"""\n        Sets the value of :py:attr:`poolingStrategy`.\n        """"""\n        if strategy == ""AVERAGE"":\n            return self._set(poolingStrategy=strategy)\n        elif strategy == ""SUM"":\n            return self._set(poolingStrategy=strategy)\n        else:\n            return self._set(poolingStrategy=""AVERAGE"")\n\n    def setSkipOOV(self, value):\n        """"""\n        Sets the value of :py:attr:`skipOOV`.\n        """"""\n        return self._set(skipOOV=value)\n\n\nclass NerOverwriter(AnnotatorModel):\n\n    name = ""NerOverwriter""\n\n    @keyword_only\n    def __init__(self):\n        super(NerOverwriter, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.ner.NerOverwriter"")\n        self._setDefault(\n            newResult=""I-OVERWRITE""\n        )\n\n    stopWords = Param(Params._dummy(), ""stopWords"", ""The words to be overwritten"",\n                      typeConverter=TypeConverters.toListString)\n    newResult = Param(Params._dummy(), ""newResult"", ""new NER class to apply to those stopwords"",\n                      typeConverter=TypeConverters.toString)\n\n    def setStopWords(self, value):\n        return self._set(stopWords=value)\n\n    def setNewResult(self, value):\n        return self._set(newResult=value)\n\n\nclass UniversalSentenceEncoder(AnnotatorModel, HasEmbeddingsProperties, HasStorageRef):\n\n    name = ""UniversalSentenceEncoder""\n\n    configProtoBytes = Param(Params._dummy(),\n                             ""configProtoBytes"",\n                             ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"",\n                             TypeConverters.toListString)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder"", java_model=None):\n        super(UniversalSentenceEncoder, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def loadSavedModel(folder, spark_session):\n        from sparknlp.internal import _USELoader\n        jModel = _USELoader(folder, spark_session._jsparkSession)._java_obj\n        return UniversalSentenceEncoder(java_model=jModel)\n\n\n    @staticmethod\n    def pretrained(name=""tfhub_use"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(UniversalSentenceEncoder, name, lang, remote_loc)\n\n\nclass ElmoEmbeddings(AnnotatorModel, HasEmbeddingsProperties, HasCaseSensitiveProperties, HasStorageRef):\n\n    name = ""ElmoEmbeddings""\n\n    batchSize = Param(Params._dummy(),\n                      ""batchSize"",\n                      ""Batch size. Large values allows faster processing but requires more memory."",\n                      typeConverter=TypeConverters.toInt)\n\n    configProtoBytes = Param(Params._dummy(),\n                             ""configProtoBytes"",\n                             ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"",\n                             TypeConverters.toListString)\n\n    poolingLayer = Param(Params._dummy(),\n                         ""poolingLayer"", ""Set ELMO pooling layer to: word_emb, lstm_outputs1, lstm_outputs2, or elmo"",\n                         typeConverter=TypeConverters.toString)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setBatchSize(self, value):\n        return self._set(batchSize=value)\n\n    def setPoolingLayer(self, layer):\n        if layer == ""word_emb"":\n            return self._set(poolingLayer=layer)\n        elif layer == ""lstm_outputs1"":\n            return self._set(poolingLayer=layer)\n        elif layer == ""lstm_outputs2"":\n            return self._set(poolingLayer=layer)\n        elif layer == ""elmo"":\n            return self._set(poolingLayer=layer)\n        else:\n            return self._set(poolingLayer=""word_emb"")\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings"", java_model=None):\n        super(ElmoEmbeddings, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n        self._setDefault(\n            batchSize=32,\n            poolingLayer=""word_emb""\n        )\n\n    @staticmethod\n    def loadSavedModel(folder, spark_session):\n        from sparknlp.internal import _ElmoLoader\n        jModel = _ElmoLoader(folder, spark_session._jsparkSession)._java_obj\n        return ElmoEmbeddings(java_model=jModel)\n\n\n    @staticmethod\n    def pretrained(name=""elmo"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(ElmoEmbeddings, name, lang, remote_loc)\n\n\nclass ClassifierDLApproach(AnnotatorApproach):\n\n    lr = Param(Params._dummy(), ""lr"", ""Learning Rate"", TypeConverters.toFloat)\n\n    batchSize = Param(Params._dummy(), ""batchSize"", ""Batch size"", TypeConverters.toInt)\n\n    dropout = Param(Params._dummy(), ""dropout"", ""Dropout coefficient"", TypeConverters.toFloat)\n\n    maxEpochs = Param(Params._dummy(), ""maxEpochs"", ""Maximum number of epochs to train"", TypeConverters.toInt)\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n\n    validationSplit = Param(Params._dummy(), ""validationSplit"", ""Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off."",\n                            TypeConverters.toFloat)\n\n    enableOutputLogs = Param(Params._dummy(), ""enableOutputLogs"",\n                             ""Whether to use stdout in addition to Spark logs."",\n                             TypeConverters.toBoolean)\n\n    outputLogsPath = Param(Params._dummy(), ""outputLogsPath"", ""Folder path to save training logs"", TypeConverters.toString)\n\n    labelColumn = Param(Params._dummy(),\n                        ""labelColumn"",\n                        ""Column with label per each token"",\n                        typeConverter=TypeConverters.toString)\n\n    verbose = Param(Params._dummy(), ""verbose"", ""Level of verbosity during training"", TypeConverters.toInt)\n    randomSeed = Param(Params._dummy(), ""randomSeed"", ""Random seed"", TypeConverters.toInt)\n\n    def setVerbose(self, value):\n        return self._set(verbose=value)\n\n    def setRandomSeed(self, seed):\n        return self._set(randomSeed=seed)\n\n    def setLabelColumn(self, value):\n        return self._set(labelColumn=value)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setLr(self, v):\n        self._set(lr=v)\n        return self\n\n    def setBatchSize(self, v):\n        self._set(batchSize=v)\n        return self\n\n    def setDropout(self, v):\n        self._set(dropout=v)\n        return self\n\n    def setMaxEpochs(self, epochs):\n        return self._set(maxEpochs=epochs)\n\n    def _create_model(self, java_model):\n        return ClassifierDLModel(java_model=java_model)\n\n    def setValidationSplit(self, v):\n        self._set(validationSplit=v)\n        return self\n\n    def setEnableOutputLogs(self, value):\n        return self._set(enableOutputLogs=value)\n\n    def setOutputLogsPath(self, p):\n        return self._set(outputLogsPath=p)\n\n    @keyword_only\n    def __init__(self):\n        super(ClassifierDLApproach, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach"")\n        self._setDefault(\n            maxEpochs=30,\n            lr=float(0.005),\n            batchSize=64,\n            dropout=float(0.5),\n            enableOutputLogs=False\n        )\n\n\nclass ClassifierDLModel(AnnotatorModel, HasStorageRef):\n    name = ""ClassifierDLModel""\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLModel"", java_model=None):\n        super(ClassifierDLModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    @staticmethod\n    def pretrained(name=""classifierdl_use_trec6"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(ClassifierDLModel, name, lang, remote_loc)\n\n\nclass AlbertEmbeddings(AnnotatorModel, HasEmbeddingsProperties, HasCaseSensitiveProperties, HasStorageRef):\n\n    name = ""AlbertEmbeddings""\n\n    batchSize = Param(Params._dummy(),\n                      ""batchSize"",\n                      ""Batch size. Large values allows faster processing but requires more memory."",\n                      typeConverter=TypeConverters.toInt)\n\n    configProtoBytes = Param(Params._dummy(),\n                             ""configProtoBytes"",\n                             ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"",\n                             TypeConverters.toListString)\n\n    maxSentenceLength = Param(Params._dummy(),\n                              ""maxSentenceLength"",\n                              ""Max sentence length to process"",\n                              typeConverter=TypeConverters.toInt)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setBatchSize(self, value):\n        return self._set(batchSize=value)\n\n    def setMaxSentenceLength(self, value):\n        return self._set(maxSentenceLength=value)\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings"", java_model=None):\n        super(AlbertEmbeddings, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n        self._setDefault(\n            batchSize=32,\n            dimension=768,\n            maxSentenceLength=128\n        )\n\n    @staticmethod\n    def loadSavedModel(folder, spark_session):\n        from sparknlp.internal import _AlbertLoader\n        jModel = _AlbertLoader(folder, spark_session._jsparkSession)._java_obj\n        return AlbertEmbeddings(java_model=jModel)\n\n    @staticmethod\n    def pretrained(name=""albert_base_uncased"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(AlbertEmbeddings, name, lang, remote_loc)\n\n\nclass XlnetEmbeddings(AnnotatorModel, HasEmbeddingsProperties, HasCaseSensitiveProperties, HasStorageRef):\n\n    name = ""XlnetEmbeddings""\n\n    batchSize = Param(Params._dummy(),\n                      ""batchSize"",\n                      ""Batch size. Large values allows faster processing but requires more memory."",\n                      typeConverter=TypeConverters.toInt)\n\n    configProtoBytes = Param(Params._dummy(),\n                             ""configProtoBytes"",\n                             ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"",\n                             TypeConverters.toListString)\n\n    maxSentenceLength = Param(Params._dummy(),\n                              ""maxSentenceLength"",\n                              ""Max sentence length to process"",\n                              typeConverter=TypeConverters.toInt)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setBatchSize(self, value):\n        return self._set(batchSize=value)\n\n    def setMaxSentenceLength(self, value):\n        return self._set(maxSentenceLength=value)\n\n    @keyword_only\n    def __init__(self, classname=""com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings"", java_model=None):\n        super(XlnetEmbeddings, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n        self._setDefault(\n            batchSize=32,\n            dimension=768,\n            maxSentenceLength=128\n        )\n\n    @staticmethod\n    def loadSavedModel(folder, spark_session):\n        from sparknlp.internal import _XlnetLoader\n        jModel = _XlnetLoader(folder, spark_session._jsparkSession)._java_obj\n        return XlnetEmbeddings(java_model=jModel)\n\n    @staticmethod\n    def pretrained(name=""xlnet_base_cased"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(XlnetEmbeddings, name, lang, remote_loc)\n\n\nclass ContextSpellCheckerApproach(AnnotatorApproach):\n\n    name = ""ContextSpellCheckerApproach""\n\n    languageModelClasses = Param(Params._dummy(),\n                                 ""languageModelClasses"",\n                                 ""Number of classes to use during factorization of the softmax output in the LM."",\n                                 typeConverter=TypeConverters.toInt)\n\n    wordMaxDistance = Param(Params._dummy(),\n                            ""wordMaxDistance"",\n                            ""Maximum distance for the generated candidates for every word."",\n                            typeConverter=TypeConverters.toInt)\n\n    maxCandidates = Param(Params._dummy(),\n                          ""maxCandidates"",\n                          ""Maximum number of candidates for every word."",\n                          typeConverter=TypeConverters.toInt)\n\n    caseStrategy = Param(Params._dummy(),\n                         ""caseStrategy"",\n                         ""What case combinations to try when generating candidates."",\n                         typeConverter=TypeConverters.toInt)\n\n    errorThreshold = Param(Params._dummy(),\n                           ""errorThreshold"",\n                           ""Threshold perplexity for a word to be considered as an error."",\n                           typeConverter=TypeConverters.toFloat)\n\n    epochs = Param(Params._dummy(),\n                   ""epochs"",\n                   ""Number of epochs to train the language model."",\n                   typeConverter=TypeConverters.toInt)\n\n    batchSize = Param(Params._dummy(),\n                      ""batchSize"",\n                      ""Batch size for the training in NLM."",\n                      typeConverter=TypeConverters.toInt)\n\n    initialRate = Param(Params._dummy(),\n                        ""initialRate"",\n                        ""Initial learning rate for the LM."",\n                        typeConverter=TypeConverters.toFloat)\n\n    finalRate = Param(Params._dummy(),\n                      ""finalRate"",\n                      ""Final learning rate for the LM."",\n                      typeConverter=TypeConverters.toFloat)\n\n    validationFraction = Param(Params._dummy(),\n                               ""validationFraction"",\n                               ""Percentage of datapoints to use for validation."",\n                               typeConverter=TypeConverters.toFloat)\n\n    minCount = Param(Params._dummy(),\n                     ""minCount"",\n                     ""Min number of times a token should appear to be included in vocab."",\n                     typeConverter=TypeConverters.toInt)\n\n    compoundCount = Param(Params._dummy(),\n                          ""compoundCount"",\n                          ""Min number of times a compound word should appear to be included in vocab."",\n                          typeConverter=TypeConverters.toInt)\n\n    classCount = Param(Params._dummy(),\n                       ""classCount"",\n                       ""Min number of times the word need to appear in corpus to not be considered of a special class."",\n                       typeConverter=TypeConverters.toInt)\n\n    tradeoff = Param(Params._dummy(),\n                     ""tradeoff"",\n                     ""Tradeoff between the cost of a word error and a transition in the language model."",\n                     typeConverter=TypeConverters.toFloat)\n\n    weightedDistPath = Param(Params._dummy(),\n                             ""weightedDistPath"",\n                             ""The path to the file containing the weights for the levenshtein distance."",\n                             typeConverter=TypeConverters.toString)\n\n    maxWindowLen = Param(Params._dummy(),\n                         ""maxWindowLen"",\n                         ""Maximum size for the window used to remember history prior to every correction."",\n                         typeConverter=TypeConverters.toInt)\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n\n\n    def setLanguageModelClasses(self, count):\n        return self._set(languageModelClasses=count)\n\n    def setWordMaxDistance(self, dist):\n        return self._set(wordMaxDistance=dist)\n\n    def setMaxCandidates(self, candidates):\n        return self._set(maxCandidates=candidates)\n\n    def setCaseStrategy(self, strategy):\n        return self._set(caseStrategy=strategy)\n\n    def setErrorThreshold(self, threshold):\n        return self._set(errorThreshold=threshold)\n\n    def setEpochs(self, count):\n        return self._set(epochs=count)\n\n    def setInitialBatchSize(self, size):\n        return self._set(batchSize=size)\n\n    def setInitialRate(self, rate):\n        return self._set(initialRate=rate)\n\n    def setFinalRate(self, rate):\n        return self._set(finalRate=rate)\n\n    def setValidationFraction(self, fraction):\n        return self._set(validationFraction=fraction)\n\n    def setMinCount(self, count):\n        return self._set(minCount=count)\n\n    def setCompoundCount(self, count):\n        return self._set(compoundCount=count)\n\n    def setClassCount(self, count):\n        return self._set(classCount=count)\n\n    def setTradeoff(self, alpha):\n        return self._set(tradeoff=alpha)\n\n    def setWeightedDistPath(self, path):\n        return self._set(weightedDistPath=path)\n\n    def setWeightedDistPath(self, path):\n        return self._set(weightedDistPath=path)\n\n    def setMaxWindowLen(self, length):\n        return self._set(maxWindowLen=length)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def addVocabClass(self, label, vocab, userdist=3):\n        self._call_java(\'addVocabClass\', label, vocab, userdist)\n        return self\n\n    def addRegexClass(self, label, regex, userdist=3):\n        self._call_java(\'addRegexClass\', label, regex, userdist)\n        return self\n\n    @keyword_only\n    def __init__(self):\n        super(ContextSpellCheckerApproach, self). \\\n            __init__(classname=""com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach"")\n\n    def _create_model(self, java_model):\n        return ContextSpellCheckerModel(java_model=java_model)\n\n\nclass ContextSpellCheckerModel(AnnotatorModel):\n    name = ""ContextSpellCheckerModel""\n\n    languageModelClasses = Param(Params._dummy(),\n                                 ""languageModelClasses"",\n                                 ""Number of classes to use during factorization of the softmax output in the LM."",\n                                 typeConverter=TypeConverters.toInt)\n\n    wordMaxDistance = Param(Params._dummy(),\n                            ""wordMaxDistance"",\n                            ""Maximum distance for the generated candidates for every word."",\n                            typeConverter=TypeConverters.toInt)\n\n    maxCandidates = Param(Params._dummy(),\n                          ""maxCandidates"",\n                          ""Maximum number of candidates for every word."",\n                          typeConverter=TypeConverters.toInt)\n\n    caseStrategy = Param(Params._dummy(),\n                         ""caseStrategy"",\n                         ""What case combinations to try when generating candidates."",\n                         typeConverter=TypeConverters.toInt)\n\n    errorThreshold = Param(Params._dummy(),\n                           ""errorThreshold"",\n                           ""Threshold perplexity for a word to be considered as an error."",\n                           typeConverter=TypeConverters.toFloat)\n\n    tradeoff = Param(Params._dummy(),\n                     ""tradeoff"",\n                     ""Tradeoff between the cost of a word error and a transition in the language model."",\n                     typeConverter=TypeConverters.toFloat)\n\n    weightedDistPath = Param(Params._dummy(),\n                             ""weightedDistPath"",\n                             ""The path to the file containing the weights for the levenshtein distance."",\n                             typeConverter=TypeConverters.toString)\n\n    maxWindowLen = Param(Params._dummy(),\n                         ""maxWindowLen"",\n                         ""Maximum size for the window used to remember history prior to every correction."",\n                         typeConverter=TypeConverters.toInt)\n\n    gamma = Param(Params._dummy(),\n                  ""gamma"",\n                  ""Controls the influence of individual word frequency in the decision."",\n                  typeConverter=TypeConverters.toFloat)\n\n    correctSymbols = Param(Params._dummy(), ""correctSymbols"", ""Whether to correct special symbols or skip spell checking for them"", typeConverter=TypeConverters.toBoolean)\n\n    compareLowcase = Param(Params._dummy(), ""compareLowcase"", ""If true will compare tokens in low case with vocabulary"", typeConverter=TypeConverters.toBoolean)\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n\n    def setLanguageModelClasses(self, count):\n        return self._set(languageModelClasses=count)\n\n    def setWordMaxDistance(self, dist):\n        return self._set(wordMaxDistance=dist)\n\n    def setMaxCandidates(self, candidates):\n        return self._set(maxCandidates=candidates)\n\n    def setCaseStrategy(self, strategy):\n        return self._set(caseStrategy=strategy)\n\n    def setErrorThreshold(self, threshold):\n        return self._set(errorThreshold=threshold)\n\n    def setTradeoff(self, alpha):\n        return self._set(tradeoff=alpha)\n\n    def setWeights(self, weights):\n        self._call_java(\'setWeights\', weights)\n\n    def setMaxWindowLen(self, length):\n        return self._set(maxWindowLen=length)\n\n    def setGamma(self, g):\n        return self._set(gamma=g)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def getWordClasses(self):\n        it = self._call_java(\'getWordClasses\').toIterator()\n        result = []\n        while(it.hasNext()):\n            result.append(it.next().toString())\n        return result\n\n    def updateRegexClass(self, label, regex):\n        self._call_java(\'updateRegexClass\', label, regex)\n        return self\n\n    def updateVocabClass(self, label, vocab, append=True):\n        self._call_java(\'updateVocabClass\', label, vocab, append)\n        return self\n\n    def setCorrectSymbols(self, value):\n        return self._set(correctSymbols=value)\n\n    def setCompareLowcase(self, value):\n        return self._set(compareLowcase=value)\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel"", java_model=None):\n        super(ContextSpellCheckerModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n    @staticmethod\n    def pretrained(name=""spellcheck_dl"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(ContextSpellCheckerModel, name, lang, remote_loc)\n\n\nclass SentimentDLApproach(AnnotatorApproach):\n\n    lr = Param(Params._dummy(), ""lr"", ""Learning Rate"", TypeConverters.toFloat)\n\n    batchSize = Param(Params._dummy(), ""batchSize"", ""Batch size"", TypeConverters.toInt)\n\n    dropout = Param(Params._dummy(), ""dropout"", ""Dropout coefficient"", TypeConverters.toFloat)\n\n    maxEpochs = Param(Params._dummy(), ""maxEpochs"", ""Maximum number of epochs to train"", TypeConverters.toInt)\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n\n    validationSplit = Param(Params._dummy(), ""validationSplit"", ""Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off."",\n                            TypeConverters.toFloat)\n\n    enableOutputLogs = Param(Params._dummy(), ""enableOutputLogs"",\n                             ""Whether to use stdout in addition to Spark logs."",\n                             TypeConverters.toBoolean)\n\n    outputLogsPath = Param(Params._dummy(), ""outputLogsPath"", ""Folder path to save training logs"", TypeConverters.toString)\n\n    labelColumn = Param(Params._dummy(),\n                        ""labelColumn"",\n                        ""Column with label per each token"",\n                        typeConverter=TypeConverters.toString)\n\n    verbose = Param(Params._dummy(), ""verbose"", ""Level of verbosity during training"", TypeConverters.toInt)\n    randomSeed = Param(Params._dummy(), ""randomSeed"", ""Random seed"", TypeConverters.toInt)\n    threshold = Param(Params._dummy(), ""threshold"", ""The minimum threshold for the final result otheriwse it will be neutral"", TypeConverters.toFloat)\n    thresholdLabel = Param(Params._dummy(), ""thresholdLabel"", ""In case the score is less than threshold, what should be the label. Default is neutral."", TypeConverters.toString)\n\n    def setVerbose(self, value):\n        return self._set(verbose=value)\n\n    def setRandomSeed(self, seed):\n        return self._set(randomSeed=seed)\n\n    def setLabelColumn(self, value):\n        return self._set(labelColumn=value)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setLr(self, v):\n        self._set(lr=v)\n        return self\n\n    def setBatchSize(self, v):\n        self._set(batchSize=v)\n        return self\n\n    def setDropout(self, v):\n        self._set(dropout=v)\n        return self\n\n    def setMaxEpochs(self, epochs):\n        return self._set(maxEpochs=epochs)\n\n    def _create_model(self, java_model):\n        return SentimentDLModel(java_model=java_model)\n\n    def setValidationSplit(self, v):\n        self._set(validationSplit=v)\n        return self\n\n    def setEnableOutputLogs(self, value):\n        return self._set(enableOutputLogs=value)\n\n    def setOutputLogsPath(self, p):\n        return self._set(outputLogsPath=p)\n\n    def setThreshold(self, v):\n        self._set(threshold=v)\n        return self\n\n    def setThresholdLabel(self, p):\n        return self._set(thresholdLabel=p)\n\n    @keyword_only\n    def __init__(self):\n        super(SentimentDLApproach, self).__init__(classname=""com.johnsnowlabs.nlp.annotators.classifier.dl.SentimentDLApproach"")\n        self._setDefault(\n            maxEpochs=30,\n            lr=float(0.005),\n            batchSize=64,\n            dropout=float(0.5),\n            enableOutputLogs=False,\n            threshold=0.6,\n            thresholdLabel=""neutral""\n        )\n\n\nclass SentimentDLModel(AnnotatorModel, HasStorageRef):\n    name = ""SentimentDLModel""\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.annotators.classifier.dl.SentimentDLModel"", java_model=None):\n        super(SentimentDLModel, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n        self._setDefault(\n            threshold=0.6,\n            thresholdLabel=""neutral""\n        )\n\n    configProtoBytes = Param(Params._dummy(), ""configProtoBytes"", ""ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()"", TypeConverters.toListString)\n    threshold = Param(Params._dummy(), ""threshold"", ""The minimum threshold for the final result otheriwse it will be neutral"", TypeConverters.toFloat)\n    thresholdLabel = Param(Params._dummy(), ""thresholdLabel"", ""In case the score is less than threshold, what should be the label. Default is neutral."", TypeConverters.toString)\n\n    def setConfigProtoBytes(self, b):\n        return self._set(configProtoBytes=b)\n\n    def setThreshold(self, v):\n        self._set(threshold=v)\n        return self\n\n    def setThresholdLabel(self, p):\n        return self._set(thresholdLabel=p)\n\n    @staticmethod\n    def pretrained(name=""sentimentdl_use_imdb"", lang=""en"", remote_loc=None):\n        from sparknlp.pretrained import ResourceDownloader\n        return ResourceDownloader.downloadModel(SentimentDLModel, name, lang, remote_loc)'"
python/sparknlp/base.py,0,"b'from abc import ABC\n\nfrom pyspark import keyword_only\nfrom pyspark.ml.wrapper import JavaEstimator\nfrom pyspark.ml.param.shared import Param, Params, TypeConverters\nfrom pyspark.ml.pipeline import Pipeline, PipelineModel, Estimator, Transformer\nfrom sparknlp.common import AnnotatorProperties\nfrom sparknlp.internal import AnnotatorTransformer, RecursiveEstimator, RecursiveTransformer\n\nfrom sparknlp.annotation import Annotation\nimport sparknlp.internal as _internal\n\n\nclass LightPipeline:\n    def __init__(self, pipelineModel, parse_embeddings=False):\n        self.pipeline_model = pipelineModel\n        self._lightPipeline = _internal._LightPipeline(pipelineModel, parse_embeddings).apply()\n\n    @staticmethod\n    def _annotation_from_java(java_annotations):\n        annotations = []\n        for annotation in java_annotations:\n            annotations.append(Annotation(annotation.annotatorType(),\n                                          annotation.begin(),\n                                          annotation.end(),\n                                          annotation.result(),\n                                          annotation.metadata(),\n                                          annotation.embeddings\n                                          )\n                               )\n        return annotations\n\n    def fullAnnotate(self, target):\n        result = []\n        if type(target) is str:\n            target = [target]\n        for row in self._lightPipeline.fullAnnotateJava(target):\n            kas = {}\n            for atype, annotations in row.items():\n                kas[atype] = self._annotation_from_java(annotations)\n            result.append(kas)\n        return result\n\n    def annotate(self, target):\n\n        def reformat(annotations):\n            return {k: list(v) for k, v in annotations.items()}\n\n        annotations = self._lightPipeline.annotateJava(target)\n\n        if type(target) is str:\n            result = reformat(annotations)\n        elif type(target) is list:\n            result = list(map(lambda a: reformat(a), list(annotations)))\n        else:\n            raise TypeError(""target for annotation may be \'str\' or \'list\'"")\n\n        return result\n\n    def transform(self, dataframe):\n        return self.pipeline_model.transform(dataframe)\n\n    def setIgnoreUnsupported(self, value):\n        self._lightPipeline.setIgnoreUnsupported(value)\n        return self\n\n    def getIgnoreUnsupported(self):\n        return self._lightPipeline.getIgnoreUnsupported()\n\n\nclass RecursivePipeline(Pipeline, JavaEstimator):\n    @keyword_only\n    def __init__(self, *args, **kwargs):\n        super(RecursivePipeline, self).__init__(*args, **kwargs)\n        self._java_obj = self._new_java_obj(""com.johnsnowlabs.nlp.RecursivePipeline"", self.uid)\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    def _fit(self, dataset):\n        stages = self.getStages()\n        for stage in stages:\n            if not (isinstance(stage, Estimator) or isinstance(stage, Transformer)):\n                raise TypeError(\n                    ""Cannot recognize a pipeline stage of type %s."" % type(stage))\n        indexOfLastEstimator = -1\n        for i, stage in enumerate(stages):\n            if isinstance(stage, Estimator):\n                indexOfLastEstimator = i\n        transformers = []\n        for i, stage in enumerate(stages):\n            if i <= indexOfLastEstimator:\n                if isinstance(stage, Transformer):\n                    transformers.append(stage)\n                    dataset = stage.transform(dataset)\n                elif isinstance(stage, RecursiveEstimator):\n                    model = stage.fit(dataset, pipeline=PipelineModel(transformers))\n                    transformers.append(model)\n                    if i < indexOfLastEstimator:\n                        dataset = model.transform(dataset)\n                else:\n                    model = stage.fit(dataset)\n                    transformers.append(model)\n                    if i < indexOfLastEstimator:\n                        dataset = model.transform(dataset)\n            else:\n                transformers.append(stage)\n        return PipelineModel(transformers)\n\n\nclass RecursivePipelineModel(PipelineModel):\n\n    def __init__(self, pipeline_model):\n        super(PipelineModel, self).__init__()\n        self.stages = pipeline_model.stages\n\n    def _transform(self, dataset):\n        for t in self.stages:\n            if isinstance(t, HasRecursiveTransform):\n                # drops current stage from the recursive pipeline within\n                dataset = t.transform_recursive(dataset, PipelineModel(self.stages[:-1]))\n            elif isinstance(t, AnnotatorProperties) and t.getLazyAnnotator():\n                pass\n            else:\n                dataset = t.transform(dataset)\n        return dataset\n\n\nclass HasRecursiveFit(RecursiveEstimator, ABC):\n    pass\n\n\nclass HasRecursiveTransform(RecursiveTransformer):\n    pass\n\n\nclass DocumentAssembler(AnnotatorTransformer):\n\n    inputCol = Param(Params._dummy(), ""inputCol"", ""input column name"", typeConverter=TypeConverters.toString)\n    outputCol = Param(Params._dummy(), ""outputCol"", ""output column name"", typeConverter=TypeConverters.toString)\n    idCol = Param(Params._dummy(), ""idCol"", ""column for setting an id to such string in row"", typeConverter=TypeConverters.toString)\n    metadataCol = Param(Params._dummy(), ""metadataCol"", ""String to String map column to use as metadata"", typeConverter=TypeConverters.toString)\n    calculationsCol = Param(Params._dummy(), ""calculationsCol"", ""String to Float vector map column to use as embeddigns and other representations"", typeConverter=TypeConverters.toString)\n    cleanupMode = Param(Params._dummy(), ""cleanupMode"", ""possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full"", typeConverter=TypeConverters.toString)\n    name = \'DocumentAssembler\'\n\n    @keyword_only\n    def __init__(self):\n        super(DocumentAssembler, self).__init__(classname=""com.johnsnowlabs.nlp.DocumentAssembler"")\n        self._setDefault(outputCol=""document"", cleanupMode=\'disabled\')\n\n    @keyword_only\n    def setParams(self):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputCol(self, value):\n        return self._set(inputCol=value)\n\n    def setOutputCol(self, value):\n        return self._set(outputCol=value)\n\n    def setIdCol(self, value):\n        return self._set(idCol=value)\n\n    def setMetadataCol(self, value):\n        return self._set(metadataCol=value)\n\n    def setCalculationsCol(self, value):\n        return self._set(metadataCol=value)\n\n    def setCleanupMode(self, value):\n        if value.strip().lower() not in [\'disabled\', \'inplace\', \'inplace_full\', \'shrink\', \'shrink_full\', \'each\', \'each_full\', \'delete_full\']:\n            raise Exception(""Cleanup mode possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full"")\n        return self._set(cleanupMode=value)\n\n\nclass TokenAssembler(AnnotatorTransformer, AnnotatorProperties):\n\n    name = ""TokenAssembler""\n    preservePosition = Param(Params._dummy(), ""preservePosition"", ""whether to preserve the actual position of the tokens or reduce them to one space"", typeConverter=TypeConverters.toBoolean)\n\n    @keyword_only\n    def __init__(self):\n        super(TokenAssembler, self).__init__(classname=""com.johnsnowlabs.nlp.TokenAssembler"")\n\n    @keyword_only\n    def setParams(self):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setPreservePosition(self, value):\n        return self._set(preservePosition=value)\n\n\nclass Doc2Chunk(AnnotatorTransformer, AnnotatorProperties):\n\n    chunkCol = Param(Params._dummy(), ""chunkCol"", ""column that contains string. Must be part of DOCUMENT"", typeConverter=TypeConverters.toString)\n    startCol = Param(Params._dummy(), ""startCol"", ""column that has a reference of where chunk begins"", typeConverter=TypeConverters.toString)\n    startColByTokenIndex = Param(Params._dummy(), ""startColByTokenIndex"", ""whether start col is by whitespace tokens"", typeConverter=TypeConverters.toBoolean)\n    isArray = Param(Params._dummy(), ""isArray"", ""whether the chunkCol is an array of strings"", typeConverter=TypeConverters.toBoolean)\n    failOnMissing = Param(Params._dummy(), ""failOnMissing"", ""whether to fail the job if a chunk is not found within document. return empty otherwise"", typeConverter=TypeConverters.toBoolean)\n    lowerCase = Param(Params._dummy(), ""lowerCase"", ""whether to lower case for matching case"", typeConverter=TypeConverters.toBoolean)\n    name = ""Doc2Chunk""\n\n    @keyword_only\n    def __init__(self):\n        super(Doc2Chunk, self).__init__(classname=""com.johnsnowlabs.nlp.Doc2Chunk"")\n        self._setDefault(\n            isArray=False\n        )\n\n    @keyword_only\n    def setParams(self):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setChunkCol(self, value):\n        return self._set(chunkCol=value)\n\n    def setIsArray(self, value):\n        return self._set(isArray=value)\n\n    def setStartCol(self, value):\n        return self._set(startCol=value)\n\n    def setStartColByTokenIndex(self, value):\n        return self._set(startColByTokenIndex=value)\n\n    def setFailOnMissing(self, value):\n        return self._set(failOnMissing=value)\n\n    def setLowerCase(self, value):\n        return self._set(lowerCase=value)\n\n\nclass Chunk2Doc(AnnotatorTransformer, AnnotatorProperties):\n\n    name = ""Chunk2Doc""\n\n    @keyword_only\n    def __init__(self):\n        super(Chunk2Doc, self).__init__(classname=""com.johnsnowlabs.nlp.Chunk2Doc"")\n\n    @keyword_only\n    def setParams(self):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n\nclass Finisher(AnnotatorTransformer):\n\n    inputCols = Param(Params._dummy(), ""inputCols"", ""input annotations"", typeConverter=TypeConverters.toListString)\n    outputCols = Param(Params._dummy(), ""outputCols"", ""output finished annotation cols"", typeConverter=TypeConverters.toListString)\n    valueSplitSymbol = Param(Params._dummy(), ""valueSplitSymbol"", ""character separating annotations"", typeConverter=TypeConverters.toString)\n    annotationSplitSymbol = Param(Params._dummy(), ""annotationSplitSymbol"", ""character separating annotations"", typeConverter=TypeConverters.toString)\n    cleanAnnotations = Param(Params._dummy(), ""cleanAnnotations"", ""whether to remove annotation columns"", typeConverter=TypeConverters.toBoolean)\n    includeMetadata = Param(Params._dummy(), ""includeMetadata"", ""annotation metadata format"", typeConverter=TypeConverters.toBoolean)\n    outputAsArray = Param(Params._dummy(), ""outputAsArray"", ""finisher generates an Array with the results instead of string"", typeConverter=TypeConverters.toBoolean)\n    parseEmbeddingsVectors = Param(Params._dummy(), ""parseEmbeddingsVectors"", ""whether to include embeddings vectors in the process"", typeConverter=TypeConverters.toBoolean)\n\n    name = ""Finisher""\n\n    @keyword_only\n    def __init__(self):\n        super(Finisher, self).__init__(classname=""com.johnsnowlabs.nlp.Finisher"")\n        self._setDefault(\n            cleanAnnotations=True,\n            includeMetadata=False,\n            outputAsArray=True,\n            parseEmbeddingsVectors=False\n        )\n\n    @keyword_only\n    def setParams(self):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputCols(self, *value):\n        if len(value) == 1 and type(value[0]) == list:\n            return self._set(inputCols=value[0])\n        else:\n            return self._set(inputCols=list(value))\n\n    def setOutputCols(self, *value):\n        if len(value) == 1 and type(value[0]) == list:\n            return self._set(outputCols=value[0])\n        else:\n            return self._set(outputCols=list(value))\n\n    def setValueSplitSymbol(self, value):\n        return self._set(valueSplitSymbol=value)\n\n    def setAnnotationSplitSymbol(self, value):\n        return self._set(annotationSplitSymbol=value)\n\n    def setCleanAnnotations(self, value):\n        return self._set(cleanAnnotations=value)\n\n    def setIncludeMetadata(self, value):\n        return self._set(includeMetadata=value)\n\n    def setOutputAsArray(self, value):\n        return self._set(outputAsArray=value)\n\n    def setParseEmbeddingsVectors(self, value):\n        return self._set(parseEmbeddingsVectors=value)\n\n\nclass EmbeddingsFinisher(AnnotatorTransformer):\n\n    inputCols = Param(Params._dummy(), ""inputCols"", ""name of input annotation cols containing embeddings"", typeConverter=TypeConverters.toListString)\n    outputCols = Param(Params._dummy(), ""outputCols"", ""output EmbeddingsFinisher ouput cols"", typeConverter=TypeConverters.toListString)\n    cleanAnnotations = Param(Params._dummy(), ""cleanAnnotations"", ""whether to remove all the existing annotation columns"", typeConverter=TypeConverters.toBoolean)\n    outputAsVector = Param(Params._dummy(), ""outputAsVector"", ""if enabled it will output the embeddings as Vectors instead of arrays"", typeConverter=TypeConverters.toBoolean)\n\n    name = ""EmbeddingsFinisher""\n\n    @keyword_only\n    def __init__(self):\n        super(EmbeddingsFinisher, self).__init__(classname=""com.johnsnowlabs.nlp.EmbeddingsFinisher"")\n        self._setDefault(\n            cleanAnnotations=False,\n            outputAsVector=False\n        )\n\n    @keyword_only\n    def setParams(self):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputCols(self, *value):\n        if len(value) == 1 and type(value[0]) == list:\n            return self._set(inputCols=value[0])\n        else:\n            return self._set(inputCols=list(value))\n\n    def setOutputCols(self, *value):\n        if len(value) == 1 and type(value[0]) == list:\n            return self._set(outputCols=value[0])\n        else:\n            return self._set(outputCols=list(value))\n\n    def setCleanAnnotations(self, value):\n        return self._set(cleanAnnotations=value)\n\n    def setOutputAsVector(self, value):\n        return self._set(outputAsVector=value)\n'"
python/sparknlp/common.py,0,"b'from pyspark.ml.util import JavaMLWritable\nfrom pyspark.ml.wrapper import JavaModel, JavaEstimator\nfrom pyspark.ml.param.shared import Param, TypeConverters\nfrom pyspark.ml.param import Params\nfrom pyspark import keyword_only\nimport sparknlp.internal as _internal\n\n\nclass AnnotatorProperties(Params):\n\n    inputCols = Param(Params._dummy(),\n                      ""inputCols"",\n                      ""previous annotations columns, if renamed"",\n                      typeConverter=TypeConverters.toListString)\n    outputCol = Param(Params._dummy(),\n                      ""outputCol"",\n                      ""output annotation column. can be left default."",\n                      typeConverter=TypeConverters.toString)\n    lazyAnnotator = Param(Params._dummy(),\n                          ""lazyAnnotator"",\n                          ""Whether this AnnotatorModel acts as lazy in RecursivePipelines"",\n                          typeConverter=TypeConverters.toBoolean\n                          )\n\n    def setInputCols(self, *value):\n        if len(value) == 1 and type(value[0]) == list:\n            return self._set(inputCols=value[0])\n        else:\n            return self._set(inputCols=list(value))\n\n    def getInputCols(self):\n        self.getOrDefault(self.inputCols)\n\n    def setOutputCol(self, value):\n        return self._set(outputCol=value)\n\n    def getOutputCol(self):\n        self.getOrDefault(self.outputCol)\n\n    def setLazyAnnotator(self, value):\n        return self._set(lazyAnnotator=value)\n\n    def getLazyAnnotator(self):\n        self.getOrDefault(self.lazyAnnotator)\n\n\nclass AnnotatorModel(JavaModel, _internal.AnnotatorJavaMLReadable, JavaMLWritable, AnnotatorProperties, _internal.ParamsGettersSetters):\n\n    @keyword_only\n    def setParams(self):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    @keyword_only\n    def __init__(self, classname, java_model=None):\n        super(AnnotatorModel, self).__init__(java_model=java_model)\n        if classname and not java_model:\n            self.__class__._java_class_name = classname\n            self._java_obj = self._new_java_obj(classname, self.uid)\n        if java_model is not None:\n            self._transfer_params_from_java()\n        self._setDefault(lazyAnnotator=False)\n\n\nclass HasEmbeddingsProperties(Params):\n    dimension = Param(Params._dummy(),\n                      ""dimension"",\n                      ""Number of embedding dimensions"",\n                      typeConverter=TypeConverters.toInt)\n\n    def setDimension(self, value):\n        return self._set(dimension=value)\n\n    def getDimension(self):\n        return self.getOrDefault(self.dimension)\n\n\nclass HasStorageRef:\n\n    storageRef = Param(Params._dummy(), ""storageRef"",\n                       ""unique reference name for identification"",\n                       TypeConverters.toString)\n\n    def setStorageRef(self, value):\n        return self._set(storageRef=value)\n\n    def getStorageRef(self):\n        return self.getOrDefault(""storageRef"")\n\n\nclass HasCaseSensitiveProperties:\n    caseSensitive = Param(Params._dummy(),\n                          ""caseSensitive"",\n                          ""whether to ignore case in tokens for embeddings matching"",\n                          typeConverter=TypeConverters.toBoolean)\n\n    def setCaseSensitive(self, value):\n        return self._set(caseSensitive=value)\n\n    def getCaseSensitive(self):\n        return self.getOrDefault(self.caseSensitive)\n\n\nclass HasExcludableStorage:\n\n    includeStorage = Param(Params._dummy(),\n                           ""includeStorage"",\n                           ""whether to include indexed storage in trained model"",\n                           typeConverter=TypeConverters.toBoolean)\n\n    def setIncludeStorage(self, value):\n        return self._set(includeStorage=value)\n\n    def getIncludeStorage(self):\n        return self.getOrDefault(""includeStorage"")\n\n\nclass HasStorage(HasStorageRef, HasCaseSensitiveProperties, HasExcludableStorage):\n\n    storagePath = Param(Params._dummy(),\n                        ""storagePath"",\n                        ""path to file"",\n                        typeConverter=TypeConverters.identity)\n\n    def setStoragePath(self, path, read_as):\n        return self._set(storagePath=ExternalResource(path, read_as, {}))\n\n    def getStoragePath(self):\n        return self.getOrDefault(""storagePath"")\n\n\nclass HasStorageModel(HasStorageRef, HasCaseSensitiveProperties, HasExcludableStorage):\n\n    def saveStorage(self, path, spark):\n        self._transfer_params_to_java()\n        self._java_obj.saveStorage(path, spark._jsparkSession, False)\n\n    @staticmethod\n    def loadStorage(path, spark, storage_ref):\n        raise NotImplementedError(""AnnotatorModel with HasStorageModel did not implement \'loadStorage\'"")\n\n    @staticmethod\n    def loadStorages(path, spark, storage_ref, databases):\n        for database in databases:\n            _internal._StorageHelper(path, spark, database, storage_ref, within_storage=False)\n\n\nclass AnnotatorApproach(JavaEstimator, JavaMLWritable, _internal.AnnotatorJavaMLReadable, AnnotatorProperties,\n                        _internal.ParamsGettersSetters):\n\n    @keyword_only\n    def __init__(self, classname):\n        _internal.ParamsGettersSetters.__init__(self)\n        self.__class__._java_class_name = classname\n        self._java_obj = self._new_java_obj(classname, self.uid)\n        self._setDefault(lazyAnnotator=False)\n\n    def _create_model(self, java_model):\n        raise NotImplementedError(\'Please implement _create_model in %s\' % self)\n\n\nclass RecursiveAnnotatorApproach(_internal.RecursiveEstimator, JavaMLWritable, _internal.AnnotatorJavaMLReadable, AnnotatorProperties,\n                                 _internal.ParamsGettersSetters):\n    @keyword_only\n    def __init__(self, classname):\n        _internal.ParamsGettersSetters.__init__(self)\n        self.__class__._java_class_name = classname\n        self._java_obj = self._new_java_obj(classname, self.uid)\n        self._setDefault(lazyAnnotator=False)\n\n    def _create_model(self, java_model):\n        raise NotImplementedError(\'Please implement _create_model in %s\' % self)\n\n\ndef RegexRule(rule, identifier):\n    return _internal._RegexRule(rule, identifier).apply()\n\n\nclass ReadAs(object):\n    TEXT = ""TEXT""\n    SPARK = ""SPARK""\n    BINARY = ""BINARY""\n\n\ndef ExternalResource(path, read_as=ReadAs.TEXT, options={}):\n    return _internal._ExternalResource(path, read_as, options).apply()\n\n\nclass CoverageResult:\n    def __init__(self, cov_obj):\n        self.covered = cov_obj.covered()\n        self.total = cov_obj.total()\n        self.percentage = cov_obj.percentage()\n'"
python/sparknlp/functions.py,0,"b'from pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nfrom pyspark.sql import DataFrame\nfrom sparknlp.annotation import Annotation\n\n\ndef map_annotations(f, output_type: DataType):\n    return udf(\n        lambda content: f(content),\n        output_type\n    )\n\n\ndef map_annotations_strict(f):\n    return udf(\n        lambda content: f(content),\n        ArrayType(Annotation.dataType())\n    )\n\n\ndef map_annotations_col(dataframe: DataFrame, f, column, output_column, output_type):\n    return dataframe.withColumn(output_column, map_annotations(f, output_type)(column))\n\n\ndef filter_by_annotations_col(dataframe, f, column):\n    this_udf = udf(\n        lambda content: f(content),\n        BooleanType()\n    )\n    return dataframe.filter(this_udf(column))\n\n\ndef explode_annotations_col(dataframe: DataFrame, column, output_column):\n    from pyspark.sql.functions import explode\n    return dataframe.withColumn(output_column, explode(column))\n'"
python/sparknlp/internal.py,0,"b'from abc import ABC\n\nfrom pyspark import SparkContext, keyword_only\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.wrapper import JavaWrapper, JavaTransformer, JavaEstimator, JavaModel\nfrom pyspark.ml.util import JavaMLWritable, JavaMLReadable, JavaMLReader\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.ml.param.shared import Params\nimport re\n\n\n# Helper class used to generate the getters for all params\nclass ParamsGettersSetters(Params):\n    getter_attrs = []\n\n    def __init__(self):\n        super(ParamsGettersSetters, self).__init__()\n        for param in self.params:\n            param_name = param.name\n            fg_attr = ""get"" + re.sub(r""(?:^|_)(.)"", lambda m: m.group(1).upper(), param_name)\n            fs_attr = ""set"" + re.sub(r""(?:^|_)(.)"", lambda m: m.group(1).upper(), param_name)\n            # Generates getter and setter only if not exists\n            try:\n                getattr(self, fg_attr)\n            except AttributeError:\n                setattr(self, fg_attr, self.getParamValue(param_name))\n            try:\n                getattr(self, fs_attr)\n            except AttributeError:\n                setattr(self, fs_attr, self.setParamValue(param_name))\n\n    def getParamValue(self, paramName):\n        def r():\n            try:\n                return self.getOrDefault(paramName)\n            except KeyError:\n                return None\n        return r\n\n    def setParamValue(self, paramName):\n        def r(v):\n            self.set(self.getParam(paramName), v)\n            return self\n        return r\n\n\nclass AnnotatorJavaMLReadable(JavaMLReadable):\n    @classmethod\n    def read(cls):\n        """"""Returns an MLReader instance for this class.""""""\n        return AnnotatorJavaMLReader(cls())\n\n\nclass AnnotatorJavaMLReader(JavaMLReader):\n    @classmethod\n    def _java_loader_class(cls, clazz):\n        if hasattr(clazz, \'_java_class_name\') and clazz._java_class_name is not None:\n            return clazz._java_class_name\n        else:\n            return JavaMLReader._java_loader_class(clazz)\n\n\nclass AnnotatorTransformer(JavaTransformer, AnnotatorJavaMLReadable, JavaMLWritable, ParamsGettersSetters):\n    @keyword_only\n    def __init__(self, classname):\n        super(AnnotatorTransformer, self).__init__()\n        kwargs = self._input_kwargs\n        if \'classname\' in kwargs:\n            kwargs.pop(\'classname\')\n        self.setParams(**kwargs)\n        self.__class__._java_class_name = classname\n        self._java_obj = self._new_java_obj(classname, self.uid)\n\n\nclass RecursiveEstimator(JavaEstimator, ABC):\n\n    def _fit_java(self, dataset, pipeline=None):\n        self._transfer_params_to_java()\n        if pipeline:\n            return self._java_obj.recursiveFit(dataset._jdf, pipeline._to_java())\n        else:\n            return self._java_obj.fit(dataset._jdf)\n\n    def _fit(self, dataset, pipeline=None):\n        java_model = self._fit_java(dataset, pipeline)\n        model = self._create_model(java_model)\n        return self._copyValues(model)\n\n    def fit(self, dataset, params=None, pipeline=None):\n        if params is None:\n            params = dict()\n        if isinstance(params, (list, tuple)):\n            models = [None] * len(params)\n            for index, model in self.fitMultiple(dataset, params):\n                models[index] = model\n            return models\n        elif isinstance(params, dict):\n            if params:\n                return self.copy(params)._fit(dataset, pipeline=pipeline)\n            else:\n                return self._fit(dataset, pipeline=pipeline)\n        else:\n            raise ValueError(""Params must be either a param map or a list/tuple of param maps, ""\n                             ""but got %s."" % type(params))\n\n\nclass RecursiveTransformer(JavaModel):\n\n    def _transform_recursive(self, dataset, recursive_pipeline):\n        self._transfer_params_to_java()\n        return DataFrame(self._java_obj.recursiveTransform(dataset._jdf, recursive_pipeline._to_java()), dataset.sql_ctx)\n\n    def transform_recursive(self, dataset, recursive_pipeline, params=None):\n        if params is None:\n            params = dict()\n        if isinstance(params, dict):\n            if params:\n                return self.copy(params)._transform_recursive(dataset, recursive_pipeline)\n            else:\n                return self._transform_recursive(dataset, recursive_pipeline)\n        else:\n            raise ValueError(""Params must be a param map but got %s."" % type(params))\n\n\nclass ExtendedJavaWrapper(JavaWrapper):\n    def __init__(self, java_obj, *args):\n        super(ExtendedJavaWrapper, self).__init__(java_obj)\n        self.sc = SparkContext._active_spark_context\n        self._java_obj = self.new_java_obj(java_obj, *args)\n        self.java_obj = self._java_obj\n\n    def __del__(self):\n        pass\n\n    def apply(self):\n        return self._java_obj\n\n    def new_java_obj(self, java_class, *args):\n        return self._new_java_obj(java_class, *args)\n\n    def new_java_array(self, pylist, java_class):\n        """"""\n        ToDo: Inspired from spark 2.0. Review if spark changes\n        """"""\n        java_array = self.sc._gateway.new_array(java_class, len(pylist))\n        for i in range(len(pylist)):\n            java_array[i] = pylist[i]\n        return java_array\n\n    def new_java_array_string(self, pylist):\n        java_array = self._new_java_array(pylist, self.sc._gateway.jvm.java.lang.String)\n        return java_array\n\n    def new_java_array_integer(self, pylist):\n        java_array = self._new_java_array(pylist, self.sc._gateway.jvm.java.lang.Integer)\n        return java_array\n\n\nclass _RegexRule(ExtendedJavaWrapper):\n    def __init__(self, rule, identifier):\n        super(_RegexRule, self).__init__(""com.johnsnowlabs.nlp.util.regex.RegexRule"", rule, identifier)\n\n\nclass _ExternalResource(ExtendedJavaWrapper):\n    def __init__(self, path, read_as, options):\n        super(_ExternalResource, self).__init__(""com.johnsnowlabs.nlp.util.io.ExternalResource.fromJava"", path, read_as, options)\n\n\nclass _ConfigLoaderGetter(ExtendedJavaWrapper):\n    def __init__(self):\n        super(_ConfigLoaderGetter, self).__init__(""com.johnsnowlabs.util.ConfigLoader.getConfigPath"")\n\n\nclass _DownloadModel(ExtendedJavaWrapper):\n    def __init__(self, reader, name, language, remote_loc, validator):\n        super(_DownloadModel, self).__init__(""com.johnsnowlabs.nlp.pretrained.""+validator+"".downloadModel"", reader, name, language, remote_loc)\n\n\nclass _DownloadPipeline(ExtendedJavaWrapper):\n    def __init__(self, name, language, remote_loc):\n        super(_DownloadPipeline, self).__init__(""com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadPipeline"", name, language, remote_loc)\n\n\nclass _ClearCache(ExtendedJavaWrapper):\n    def __init__(self, name, language, remote_loc):\n        super(_ClearCache, self).__init__(""com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.clearCache"", name, language, remote_loc)\n\n\nclass _GetResourceSize(ExtendedJavaWrapper):\n    def __init__(self, name, language, remote_loc):\n        super(_GetResourceSize, self).__init__(\n            ""com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize"", name, language, remote_loc)\n\n\nclass _ShowUnCategorizedResources(ExtendedJavaWrapper):\n    def __init__(self):\n        super(_ShowUnCategorizedResources, self).__init__(\n            ""com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.showUnCategorizedResources"")\n\n\nclass _ShowPublicPipelines(ExtendedJavaWrapper):\n    def __init__(self):\n        super(_ShowPublicPipelines, self).__init__(\n            ""com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.showPublicPipelines"")\n\n\nclass _ShowPublicModels(ExtendedJavaWrapper):\n    def __init__(self):\n        super(_ShowPublicModels, self).__init__(\n            ""com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.showPublicModels"")\n\n\n# predefined pipelines\nclass _DownloadPredefinedPipeline(ExtendedJavaWrapper):\n    def __init__(self, java_path):\n        super(_DownloadPredefinedPipeline, self).__init__(java_path)\n\n\nclass _LightPipeline(ExtendedJavaWrapper):\n    def __init__(self, pipelineModel, parse_embeddings):\n        super(_LightPipeline, self).__init__(""com.johnsnowlabs.nlp.LightPipeline"", pipelineModel._to_java(), parse_embeddings)\n\n# ==================\n# Utils\n# ==================\n\n\nclass _StorageHelper(ExtendedJavaWrapper):\n    def __init__(self, path, spark, database, storage_ref, within_storage):\n        super(_StorageHelper, self).__init__(""com.johnsnowlabs.storage.StorageHelper.load"", path, spark._jsparkSession, database, storage_ref, within_storage)\n\n\nclass _CoNLLGeneratorExport(ExtendedJavaWrapper):\n    def __init__(self, spark, target, pipeline, output_path):\n        if type(pipeline) == PipelineModel:\n            pipeline = pipeline._to_java()\n        if type(target) == DataFrame:\n            super(_CoNLLGeneratorExport, self).__init__(""com.johnsnowlabs.util.CoNLLGenerator.exportConllFiles"", target._jdf, pipeline, output_path)\n        else:\n            super(_CoNLLGeneratorExport, self).__init__(""com.johnsnowlabs.util.CoNLLGenerator.exportConllFiles"", spark._jsparkSession, target, pipeline, output_path)\n\n\nclass _EmbeddingsOverallCoverage(ExtendedJavaWrapper):\n    def __init__(self, dataset, embeddings_col):\n        super(_EmbeddingsOverallCoverage, self).__init__(""com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel.overallCoverage"", dataset._jdf, embeddings_col)\n\n\nclass _EmbeddingsCoverageColumn(ExtendedJavaWrapper):\n    def __init__(self, dataset, embeddings_col, output_col):\n        super(_EmbeddingsCoverageColumn, self).__init__(""com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel.withCoverageColumn"", dataset._jdf, embeddings_col, output_col)\n\n\nclass _CoverageResult(ExtendedJavaWrapper):\n    def __init__(self, covered, total, percentage):\n        super(_CoverageResult, self).__init__(""com.johnsnowlabs.nlp.embeddings.CoverageResult"", covered, total, percentage)\n\n\nclass _BertLoader(ExtendedJavaWrapper):\n    def __init__(self, path, jspark):\n        super(_BertLoader, self).__init__(""com.johnsnowlabs.nlp.embeddings.BertEmbeddings.loadSavedModel"", path, jspark)\n\n\nclass _USELoader(ExtendedJavaWrapper):\n    def __init__(self, path, jspark):\n        super(_USELoader, self).__init__(""com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder.loadSavedModel"", path, jspark)\n\n\nclass _ElmoLoader(ExtendedJavaWrapper):\n    def __init__(self, path, jspark):\n        super(_ElmoLoader, self).__init__(""com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings.loadSavedModel"", path, jspark)\n\n\nclass _AlbertLoader(ExtendedJavaWrapper):\n    def __init__(self, path, jspark):\n        super(_AlbertLoader, self).__init__(""com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings.loadSavedModel"", path, jspark)\n\n\nclass _XlnetLoader(ExtendedJavaWrapper):\n    def __init__(self, path, jspark):\n        super(_XlnetLoader, self).__init__(""com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings.loadSavedModel"", path, jspark)\n'"
python/sparknlp/pretrained.py,0,"b'import sparknlp.internal as _internal\nimport threading\nimport time\nfrom pyspark.sql import DataFrame\nfrom sparknlp.annotator import *\nfrom sparknlp.base import LightPipeline\nfrom pyspark.ml import PipelineModel\n\n\ndef printProgress(stop):\n    states = [\' | \', \' / \', \' \xe2\x80\x94 \', \' \\\\ \']\n    nextc = 0\n    while True:\n        sys.stdout.write(\'\\r[{}]\'.format(states[nextc]))\n        sys.stdout.flush()\n        time.sleep(2.5)\n        nextc = nextc + 1 if nextc < 3 else 0\n        if stop():\n            sys.stdout.write(\'\\r[{}]\'.format(\'OK!\'))\n            sys.stdout.flush()\n            break\n\n    sys.stdout.write(\'\\n\')\n    return\n\n\nclass ResourceDownloader(object):\n\n    @staticmethod\n    def downloadModel(reader, name, language, remote_loc=None, j_dwn=\'PythonResourceDownloader\'):\n        print(name + "" download started this may take some time."")\n        file_size = _internal._GetResourceSize(name, language, remote_loc).apply()\n        if file_size == ""-1"":\n            print(""Can not find the model to download please check the name!"")\n        else:\n            print(""Approximate size to download "" + file_size)\n            stop_threads = False\n            t1 = threading.Thread(target=printProgress, args=(lambda: stop_threads,))\n            t1.start()\n            try:\n                j_obj = _internal._DownloadModel(reader.name, name, language, remote_loc, j_dwn).apply()\n            finally:\n                stop_threads = True\n                t1.join()\n\n            return reader(classname=None, java_model=j_obj)\n\n    @staticmethod\n    def downloadPipeline(name, language, remote_loc=None):\n        print(name + "" download started this may take some time."")\n        file_size = _internal._GetResourceSize(name, language, remote_loc).apply()\n        if file_size == ""-1"":\n            print(""Can not find the model to download please check the name!"")\n        else:\n            print(""Approx size to download "" + file_size)\n            stop_threads = False\n            t1 = threading.Thread(target=printProgress, args=(lambda: stop_threads,))\n            t1.start()\n            try:\n                j_obj = _internal._DownloadPipeline(name, language, remote_loc).apply()\n                jmodel = PipelineModel._from_java(j_obj)\n            finally:\n                stop_threads = True\n                t1.join()\n\n            return jmodel\n\n    @staticmethod\n    def clearCache(name, language, remote_loc=None):\n        _internal._ClearCache(name, language, remote_loc).apply()\n\n    @staticmethod\n    def showPublicModels():\n        print(""test"")\n        _internal._ShowPublicModels().apply()\n\n    @staticmethod\n    def showPublicPipelines():\n        _internal._ShowPublicPipelines().apply()\n\n\n    @staticmethod\n    def showUnCategorizedResources():\n        _internal._ShowUnCategorizedResources().apply()\n\n\nclass PretrainedPipeline:\n\n    def __init__(self, name, lang=\'en\', remote_loc=None, parse_embeddings=False, disk_location=None):\n        if not disk_location:\n            self.model = ResourceDownloader().downloadPipeline(name, lang, remote_loc)\n        else:\n            self.model = PipelineModel.load(disk_location)\n        self.light_model = LightPipeline(self.model, parse_embeddings)\n\n    @staticmethod\n    def from_disk(path, parse_embeddings=False):\n        return PretrainedPipeline(None, None, None, parse_embeddings, path)\n\n    def annotate(self, target, column=None):\n        if type(target) is DataFrame:\n            if not column:\n                raise Exception(""annotate() column arg needed when targeting a DataFrame"")\n            return self.model.transform(target.withColumnRenamed(column, ""text""))\n        elif type(target) is list or type(target) is str:\n            pipeline = self.light_model\n            return pipeline.annotate(target)\n        else:\n            raise Exception(""target must be either a spark DataFrame, a list of strings or a string"")\n\n    def fullAnnotate(self, target, column=None):\n        if type(target) is DataFrame:\n            if not column:\n                raise Exception(""annotate() column arg needed when targeting a DataFrame"")\n            return self.model.transform(target.withColumnRenamed(column, ""text""))\n        elif type(target) is list or type(target) is str:\n            pipeline = self.light_model\n            return pipeline.fullAnnotate(target)\n        else:\n            raise Exception(""target must be either a spark DataFrame, a list of strings or a string"")\n\n    def transform(self, data):\n        return self.model.transform(data)\n'"
python/sparknlp/training.py,0,"b'from sparknlp.internal import ExtendedJavaWrapper\nfrom sparknlp.common import ExternalResource, ReadAs\nfrom pyspark.sql import SparkSession, DataFrame\n\n\nclass CoNLL(ExtendedJavaWrapper):\n    def __init__(self,\n                 documentCol = \'document\',\n                 sentenceCol = \'sentence\',\n                 tokenCol = \'token\',\n                 posCol = \'pos\',\n                 conllLabelIndex = 3,\n                 conllPosIndex = 1,\n                 textCol = \'text\',\n                 labelCol = \'label\',\n                 explodeSentences = True,\n                 ):\n        super(CoNLL, self).__init__(""com.johnsnowlabs.nlp.training.CoNLL"",\n                                    documentCol,\n                                    sentenceCol,\n                                    tokenCol,\n                                    posCol,\n                                    conllLabelIndex,\n                                    conllPosIndex,\n                                    textCol,\n                                    labelCol,\n                                    explodeSentences)\n\n    def readDataset(self, spark, path, read_as=ReadAs.TEXT):\n\n        # ToDo Replace with std pyspark\n        jSession = spark._jsparkSession\n\n        jdf = self._java_obj.readDataset(jSession, path, read_as)\n        return DataFrame(jdf, spark._wrapped)\n\n\nclass POS(ExtendedJavaWrapper):\n    def __init__(self):\n        super(POS, self).__init__(""com.johnsnowlabs.nlp.training.POS"")\n\n    def readDataset(self, spark, path, delimiter=""|"", outputPosCol=""tags"", outputDocumentCol=""document"", outputTextCol=""text""):\n\n        # ToDo Replace with std pyspark\n        jSession = spark._jsparkSession\n\n        jdf = self._java_obj.readDataset(jSession, path, delimiter, outputPosCol, outputDocumentCol, outputTextCol)\n        return DataFrame(jdf, spark._wrapped)\n\n\nclass PubTator(ExtendedJavaWrapper):\n    def __init__(self):\n        super(PubTator, self).__init__(""com.johnsnowlabs.nlp.training.PubTator"")\n\n    def readDataset(self, spark, path):\n\n        # ToDo Replace with std pyspark\n        jSession = spark._jsparkSession\n\n        jdf = self._java_obj.readDataset(jSession, path)\n        return DataFrame(jdf, spark._wrapped)\n\n'"
python/sparknlp/util.py,0,"b'import sparknlp.internal as _internal\n\n\ndef get_config_path():\n    return _internal._ConfigLoaderGetter().apply()\n\n\nclass CoNLLGenerator:\n    @staticmethod\n    def exportConllFiles(spark, files_path, pipeline, output_path):\n        _internal._CoNLLGeneratorExport(spark, files_path, pipeline, output_path).apply()\n'"
python/test/__init__.py,0,b''
python/test/annotators.py,0,"b'import re\nimport unittest\nimport os\nfrom sparknlp.annotator import *\nfrom sparknlp.base import *\nfrom test.util import SparkContextForTest\nfrom test.util import SparkSessionForTest\nfrom pyspark.ml.feature import SQLTransformer\nfrom pyspark.ml.clustering import KMeans\n\n\nclass BasicAnnotatorsTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        # This implicitly sets up py4j for us\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"") \\\n            .setExceptions([""New York""]) \\\n            .addInfixPattern(""(%\\\\d+)"")\n        stemmer = Stemmer() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""stem"")\n        normalizer = Normalizer() \\\n            .setInputCols([""stem""]) \\\n            .setOutputCol(""normalize"")\n        token_assembler = TokenAssembler() \\\n            .setInputCols([""document"", ""normalize""]) \\\n            .setOutputCol(""assembled"")\n        finisher = Finisher() \\\n            .setInputCols([""assembled""]) \\\n            .setOutputCols([""reassembled_view""]) \\\n            .setCleanAnnotations(True)\n        assembled = document_assembler.transform(self.data)\n        tokenized = tokenizer.fit(assembled).transform(assembled)\n        stemmed = stemmer.transform(tokenized)\n        normalized = normalizer.fit(stemmed).transform(stemmed)\n        reassembled = token_assembler.transform(normalized)\n        finisher.transform(reassembled).show()\n\n\nclass RegexMatcherTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        # This implicitly sets up py4j for us\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        regex_matcher = RegexMatcher() \\\n            .setInputCols([\'document\']) \\\n            .setStrategy(""MATCH_ALL"") \\\n            .setExternalRules(path=""file:///"" + os.getcwd() + ""/../src/test/resources/regex-matcher/rules.txt"",\n                              delimiter="","") \\\n            .setOutputCol(""regex"")\n        assembled = document_assembler.transform(self.data)\n        regex_matcher.fit(assembled).transform(assembled).show()\n\n\nclass LemmatizerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n        lemmatizer = Lemmatizer() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""lemma"") \\\n            .setDictionary(path=""file:///"" + os.getcwd() + ""/../src/test/resources/lemma-corpus-small/lemmas_small.txt"",\n                           key_delimiter=""->"", value_delimiter=""\\t"")\n        assembled = document_assembler.transform(self.data)\n        tokenized = tokenizer.fit(assembled).transform(assembled)\n        lemmatizer.fit(tokenized).transform(tokenized).show()\n\n\nclass TokenizerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.session = SparkContextForTest.spark\n\n    def runTest(self):\n        data = self.session.createDataFrame([(""this is some/text I wrote"",)], [""text""])\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"") \\\n            .addInfixPattern(""(\\\\p{L}+)(\\\\/)(\\\\p{L}+\\\\b)"") \\\n            .setMinLength(3) \\\n            .setMaxLength(6)\n        finisher = Finisher() \\\n            .setInputCols([""token""]) \\\n            .setOutputCols([""token_out""]) \\\n            .setOutputAsArray(True)\n        assembled = document_assembler.transform(data)\n        tokenized = tokenizer.fit(assembled).transform(assembled)\n        finished = finisher.transform(tokenized)\n        print(finished.first()[\'token_out\'])\n        self.assertEqual(len(finished.first()[\'token_out\']), 4)\n\n\nclass ChunkTokenizerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.session = SparkContextForTest.spark\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n        entity_extractor = TextMatcher() \\\n            .setInputCols([\'document\', \'token\']) \\\n            .setOutputCol(""entity"") \\\n            .setEntities(path=""file:///"" + os.getcwd() + ""/../src/test/resources/entity-extractor/test-chunks.txt"")\n        chunk_tokenizer = ChunkTokenizer() \\\n            .setInputCols([\'entity\']) \\\n            .setOutputCol(\'chunk_token\')\n\n        pipeline = Pipeline(stages=[document_assembler, tokenizer, entity_extractor, chunk_tokenizer])\n\n        data = self.session.createDataFrame([\n            [""Hello world, my name is Michael, I am an artist and I work at Benezar""],\n            [""Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.""]\n        ]).toDF(""text"")\n\n        pipeline.fit(data).transform(data).show()\n\n\nclass NormalizerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n        lemmatizer = Normalizer() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""normalized_token"") \\\n            .setLowercase(False)\n\n        assembled = document_assembler.transform(self.data)\n        tokenized = tokenizer.fit(assembled).transform(assembled)\n        lemmatizer.transform(tokenized).show()\n\n\nclass DateMatcherTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        date_matcher = DateMatcher() \\\n            .setInputCols([\'document\']) \\\n            .setOutputCol(""date"") \\\n            .setFormat(""yyyyMM"")\n        assembled = document_assembler.transform(self.data)\n        date_matcher.transform(assembled).show()\n\n\nclass TextMatcherTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n        entity_extractor = TextMatcher() \\\n            .setInputCols([\'document\', \'token\']) \\\n            .setOutputCol(""entity"") \\\n            .setEntities(path=""file:///"" + os.getcwd() + ""/../src/test/resources/entity-extractor/test-phrases.txt"")\n        assembled = document_assembler.transform(self.data)\n        tokenized = tokenizer.fit(assembled).transform(assembled)\n        entity_extractor.fit(tokenized).transform(tokenized).show()\n\n\nclass PerceptronApproachTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        from sparknlp.training import POS\n        self.data = SparkContextForTest.data\n        self.train = POS().readDataset(SparkContextForTest.spark,\n                                       os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/test-training.txt"",\n                                       delimiter=""|"", outputPosCol=""tags"", outputDocumentCol=""document"",\n                                       outputTextCol=""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        pos_tagger = PerceptronApproach() \\\n            .setInputCols([""token"", ""sentence""]) \\\n            .setOutputCol(""pos"") \\\n            .setIterations(1) \\\n            .fit(self.train)\n\n        assembled = document_assembler.transform(self.data)\n        sentenced = sentence_detector.transform(assembled)\n        tokenized = tokenizer.fit(sentenced).transform(sentenced)\n        pos_tagger.transform(tokenized).show()\n\n\nclass ChunkerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        from sparknlp.training import POS\n        self.data = SparkContextForTest.data\n        self.train_pos = POS().readDataset(SparkContextForTest.spark,\n                                           os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/test-training.txt"",\n                                           delimiter=""|"", outputPosCol=""tags"", outputDocumentCol=""document"",\n                                           outputTextCol=""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        pos_tagger = PerceptronApproach() \\\n            .setInputCols([""token"", ""sentence""]) \\\n            .setOutputCol(""pos"") \\\n            .setIterations(3) \\\n            .fit(self.train_pos)\n        chunker = Chunker() \\\n            .setInputCols([""sentence"", ""pos""]) \\\n            .setOutputCol(""chunk"") \\\n            .setRegexParsers([""<NNP>+"", ""<DT|PP\\\\$>?<JJ>*<NN>""])\n        assembled = document_assembler.transform(self.data)\n        sentenced = sentence_detector.transform(assembled)\n        tokenized = tokenizer.fit(sentenced).transform(sentenced)\n        pos_sentence_format = pos_tagger.transform(tokenized)\n        chunk_phrases = chunker.transform(pos_sentence_format)\n        chunk_phrases.show()\n\n\nclass PragmaticSBDTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"") \\\n            .setCustomBounds([""%%""]) \\\n            .setSplitLength(235) \\\n            .setMinLength(4) \\\n            .setMaxLength(50)\n\n        assembled = document_assembler.transform(self.data)\n        sentence_detector.transform(assembled).show()\n\n\nclass DeepSentenceDetectorTestSpec(unittest.TestCase):\n    def setUp(self):\n        from sparknlp.training import CoNLL\n        self.data = SparkContextForTest.data\n        self.embeddings = os.getcwd() + ""/../src/test/resources/ner-corpus/embeddings.100d.test.txt""\n        external_dataset = os.getcwd() + ""/../src/test/resources/ner-corpus/sentence-detector/unpunctuated_dataset.txt""\n        self.training_set = CoNLL().readDataset(SparkContextForTest.spark, external_dataset)\n\n    def runTest(self):\n        glove = WordEmbeddings() \\\n            .setInputCols([""document"", ""token""]) \\\n            .setOutputCol(""glove"") \\\n            .setStoragePath(self.embeddings, ""TEXT"") \\\n            .setStorageRef(\'embeddings_100\') \\\n            .setDimension(100)\n\n        ner_tagger = NerDLApproach() \\\n            .setInputCols([""document"", ""token"", ""glove""]) \\\n            .setLabelColumn(""label"") \\\n            .setOutputCol(""ner"") \\\n            .setMaxEpochs(100) \\\n            .setPo(0.01) \\\n            .setLr(0.1) \\\n            .setBatchSize(9) \\\n            .setRandomSeed(0)\n        ner_converter = NerConverter() \\\n            .setInputCols([""document"", ""token"", ""ner""]) \\\n            .setOutputCol(""ner_con"")\n        deep_sentence_detector = DeepSentenceDetector() \\\n            .setInputCols([""document"", ""token"", ""ner_con""]) \\\n            .setOutputCol(""sentence"") \\\n            .setIncludePragmaticSegmenter(True) \\\n            .setEndPunctuation([""."", ""?""])\n        embedded_training_set = glove.fit(self.training_set).transform(self.training_set)\n        ner_tagged = ner_tagger.fit(embedded_training_set).transform(embedded_training_set)\n        ner_converted = ner_converter.transform(ner_tagged)\n        deep_sentence_detected = deep_sentence_detector.transform(ner_converted)\n        deep_sentence_detected.show()\n\n\nclass PragmaticScorerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        lemmatizer = Lemmatizer() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""lemma"") \\\n            .setDictionary(path=""file:///"" + os.getcwd() + ""/../src/test/resources/lemma-corpus-small/lemmas_small.txt"",\n                           key_delimiter=""->"", value_delimiter=""\\t"")\n        sentiment_detector = SentimentDetector() \\\n            .setInputCols([""lemma"", ""sentence""]) \\\n            .setOutputCol(""sentiment"") \\\n            .setDictionary(\n            ""file:///"" + os.getcwd() + ""/../src/test/resources/sentiment-corpus/default-sentiment-dict.txt"",\n            delimiter="","")\n        assembled = document_assembler.transform(self.data)\n        sentenced = sentence_detector.transform(assembled)\n        tokenized = tokenizer.fit(sentenced).transform(sentenced)\n        lemmatized = lemmatizer.fit(tokenized).transform(tokenized)\n        sentiment_detector.fit(lemmatized).transform(lemmatized).show()\n\n\nclass DeepSentenceDetectorPipelinePersistenceTestSpec(unittest.TestCase):\n    @staticmethod\n    def runTest():\n        pipeline = Pipeline(stages=[DeepSentenceDetector()])\n        pipe_path = ""file:///"" + os.getcwd() + ""/tmp_pipeline""\n        pipeline.write().overwrite().save(pipe_path)\n        loaded_pipeline = Pipeline.read().load(pipe_path)\n        if loaded_pipeline:\n            assert True\n\n\nclass PipelineTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n        lemmatizer = Lemmatizer() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""lemma"") \\\n            .setDictionary(""file:///"" + os.getcwd() + ""/../src/test/resources/lemma-corpus-small/simple.txt"",\n                           key_delimiter=""->"", value_delimiter=""\\t"")\n        finisher = Finisher() \\\n            .setInputCols([""token"", ""lemma""]) \\\n            .setOutputCols([""token_views"", ""lemma_views""]) \\\n            .setOutputAsArray(False) \\\n            .setAnnotationSplitSymbol(\'@\') \\\n            .setValueSplitSymbol(\'#\')\n        pipeline = Pipeline(stages=[document_assembler, tokenizer, lemmatizer, finisher])\n        model = pipeline.fit(self.data)\n        token_before_save = model.transform(self.data).select(""token_views"").take(1)[0].token_views.split(""@"")[2]\n        lemma_before_save = model.transform(self.data).select(""lemma_views"").take(1)[0].lemma_views.split(""@"")[2]\n        pipe_path = ""file:///"" + os.getcwd() + ""/tmp_pipeline""\n        pipeline.write().overwrite().save(pipe_path)\n        loaded_pipeline = Pipeline.read().load(pipe_path)\n        token_after_save = model.transform(self.data).select(""token_views"").take(1)[0].token_views.split(""@"")[2]\n        lemma_after_save = model.transform(self.data).select(""lemma_views"").take(1)[0].lemma_views.split(""@"")[2]\n        assert token_before_save == ""sad""\n        assert lemma_before_save == ""unsad""\n        assert token_after_save == token_before_save\n        assert lemma_after_save == lemma_before_save\n        pipeline_model = loaded_pipeline.fit(self.data)\n        pipeline_model.transform(self.data).show()\n        pipeline_model.write().overwrite().save(pipe_path)\n        loaded_model = PipelineModel.read().load(pipe_path)\n        loaded_model.transform(self.data).show()\n        locdata = list(map(lambda d: d[0], self.data.select(""text"").collect()))\n        spless = LightPipeline(loaded_model).annotate(locdata)\n        fullSpless = LightPipeline(loaded_model).fullAnnotate(locdata)\n        for row in spless[:2]:\n            for _, annotations in row.items():\n                for annotation in annotations[:2]:\n                    print(annotation)\n        for row in fullSpless[:5]:\n            for _, annotations in row.items():\n                for annotation in annotations[:2]:\n                    print(annotation.result)\n        single = LightPipeline(loaded_model).annotate(""Joe was running under the rain."")\n        print(single)\n        assert single[""lemma""][2] == ""run""\n\n\nclass SpellCheckerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.prediction_data = SparkContextForTest.data\n        text_file = ""file:///"" + os.getcwd() + ""/../src/test/resources/spell/sherlockholmes.txt""\n        self.train_data = SparkContextForTest.spark.read.text(text_file)\n        self.train_data = self.train_data.withColumnRenamed(""value"", ""text"")\n\n    def runTest(self):\n\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n\n        spell_checker = NorvigSweetingApproach() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""spell"") \\\n            .setDictionary(""file:///"" + os.getcwd() + ""/../src/test/resources/spell/words.txt"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            tokenizer,\n            spell_checker\n        ])\n\n        model = pipeline.fit(self.train_data)\n        checked = model.transform(self.prediction_data)\n        checked.show()\n\n\nclass SymmetricDeleteTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.prediction_data = SparkContextForTest.data\n        text_file = ""file:///"" + os.getcwd() + ""/../src/test/resources/spell/sherlockholmes.txt""\n        self.train_data = SparkContextForTest.spark.read.text(text_file)\n        self.train_data = self.train_data.withColumnRenamed(""value"", ""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n\n        spell_checker = SymmetricDeleteApproach() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""symmspell"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            tokenizer,\n            spell_checker\n        ])\n\n        model = pipeline.fit(self.train_data)\n        checked = model.transform(self.prediction_data)\n        checked.show()\n\n\nclass ContextSpellCheckerTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.prediction_data = SparkContextForTest.data\n        text_file = ""file:///"" + os.getcwd() + ""/../src/test/resources/spell/sherlockholmes.txt""\n        self.train_data = SparkContextForTest.spark.read.text(text_file)\n        self.train_data = self.train_data.withColumnRenamed(""value"", ""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n\n        spell_checker = ContextSpellCheckerModel \\\n            .pretrained(\'spellcheck_dl\', \'en\') \\\n            .setInputCols(""token"") \\\n            .setOutputCol(""checked"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            tokenizer,\n            spell_checker\n        ])\n\n        model = pipeline.fit(self.train_data)\n        checked = model.transform(self.prediction_data)\n        checked.show()\n\n\nclass ParamsGettersTestSpec(unittest.TestCase):\n    @staticmethod\n    def runTest():\n        annotators = [DocumentAssembler, PerceptronApproach, Lemmatizer, TokenAssembler, NorvigSweetingApproach]\n        for annotator in annotators:\n            a = annotator()\n            for param in a.params:\n                param_name = param.name\n                camelized_param = re.sub(r""(?:^|_)(.)"", lambda m: m.group(1).upper(), param_name)\n                assert(hasattr(a, param_name))\n                param_value = getattr(a, ""get"" + camelized_param)()\n                assert(param_value is None or param_value is not None)\n        # Try a getter\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"") \\\n            .setCustomBounds([""%%""])\n        assert(sentence_detector.getOutputCol() == ""sentence"")\n        assert(sentence_detector.getCustomBounds() == [""%%""])\n        # Try a default getter\n        document_assembler = DocumentAssembler()\n        assert(document_assembler.getOutputCol() == ""document"")\n\n\nclass DependencyParserTreeBankTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.spark \\\n            .createDataFrame([[""I saw a girl with a telescope""]]).toDF(""text"")\n        self.corpus = os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/""\n        self.dependency_treebank = os.getcwd() + ""/../src/test/resources/parser/unlabeled/dependency_treebank""\n        from sparknlp.training import POS\n        self.train_pos = POS().readDataset(SparkContextForTest.spark,\n                                           os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/test-training.txt"",\n                                           delimiter=""|"", outputPosCol=""tags"", outputDocumentCol=""document"",\n                                           outputTextCol=""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n\n        pos_tagger = PerceptronApproach() \\\n            .setInputCols([""token"", ""sentence""]) \\\n            .setOutputCol(""pos"") \\\n            .setIterations(1) \\\n            .fit(self.train_pos)\n\n        dependency_parser = DependencyParserApproach() \\\n            .setInputCols([""sentence"", ""pos"", ""token""]) \\\n            .setOutputCol(""dependency"") \\\n            .setDependencyTreeBank(self.dependency_treebank) \\\n            .setNumberOfIterations(10)\n\n        assembled = document_assembler.transform(self.data)\n        sentenced = sentence_detector.transform(assembled)\n        tokenized = tokenizer.fit(sentenced).transform(sentenced)\n        pos_tagged = pos_tagger.transform(tokenized)\n        dependency_parsed = dependency_parser.fit(pos_tagged).transform(pos_tagged)\n        dependency_parsed.show()\n\n\nclass DependencyParserConllUTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.spark \\\n            .createDataFrame([[""I saw a girl with a telescope""]]).toDF(""text"")\n        self.corpus = os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/""\n        self.conllu = os.getcwd() + ""/../src/test/resources/parser/unlabeled/conll-u/train_small.conllu.txt""\n        from sparknlp.training import POS\n        self.train_pos = POS().readDataset(SparkContextForTest.spark,\n                                           os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/test-training.txt"",\n                                           delimiter=""|"", outputPosCol=""tags"", outputDocumentCol=""document"",\n                                           outputTextCol=""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n\n        pos_tagger = PerceptronApproach() \\\n            .setInputCols([""token"", ""sentence""]) \\\n            .setOutputCol(""pos"") \\\n            .setIterations(1) \\\n            .fit(self.train_pos)\n\n        dependency_parser = DependencyParserApproach() \\\n            .setInputCols([""sentence"", ""pos"", ""token""]) \\\n            .setOutputCol(""dependency"") \\\n            .setConllU(self.conllu) \\\n            .setNumberOfIterations(10)\n\n        assembled = document_assembler.transform(self.data)\n        sentenced = sentence_detector.transform(assembled)\n        tokenized = tokenizer.fit(sentenced).transform(sentenced)\n        pos_tagged = pos_tagger.transform(tokenized)\n        dependency_parsed = dependency_parser.fit(pos_tagged).transform(pos_tagged)\n        dependency_parsed.show()\n\n\nclass TypedDependencyParserConllUTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.spark \\\n            .createDataFrame([[""I saw a girl with a telescope""]]).toDF(""text"")\n        self.corpus = os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/""\n        self.conllu = os.getcwd() + ""/../src/test/resources/parser/unlabeled/conll-u/train_small.conllu.txt""\n        self.conllu = os.getcwd() + ""/../src/test/resources/parser/labeled/train_small.conllu.txt""\n        from sparknlp.training import POS\n        self.train_pos = POS().readDataset(SparkContextForTest.spark,\n                                           os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/test-training.txt"",\n                                           delimiter=""|"", outputPosCol=""tags"", outputDocumentCol=""document"",\n                                           outputTextCol=""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n\n        pos_tagger = PerceptronApproach() \\\n            .setInputCols([""token"", ""sentence""]) \\\n            .setOutputCol(""pos"") \\\n            .setIterations(1) \\\n            .fit(self.train_pos)\n\n        dependency_parser = DependencyParserApproach() \\\n            .setInputCols([""sentence"", ""pos"", ""token""]) \\\n            .setOutputCol(""dependency"") \\\n            .setConllU(self.conllu) \\\n            .setNumberOfIterations(10)\n\n        typed_dependency_parser = TypedDependencyParserApproach() \\\n            .setInputCols([""token"", ""pos"", ""dependency""]) \\\n            .setOutputCol(""labdep"") \\\n            .setConllU(self.conllu) \\\n            .setNumberOfIterations(10)\n\n        assembled = document_assembler.transform(self.data)\n        sentenced = sentence_detector.transform(assembled)\n        tokenized = tokenizer.fit(sentenced).transform(sentenced)\n        pos_tagged = pos_tagger.transform(tokenized)\n        dependency_parsed = dependency_parser.fit(pos_tagged).transform(pos_tagged)\n        typed_dependency_parsed = typed_dependency_parser.fit(dependency_parsed).transform(dependency_parsed)\n        typed_dependency_parsed.show()\n\n\nclass TypedDependencyParserConll2009TestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.spark \\\n            .createDataFrame([[""I saw a girl with a telescope""]]).toDF(""text"")\n        self.corpus = os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/""\n        self.tree_bank = os.getcwd() + ""/../src/test/resources/parser/unlabeled/dependency_treebank""\n        self.conll2009 = os.getcwd() + ""/../src/test/resources/parser/labeled/example.train.conll2009""\n        from sparknlp.training import POS\n        self.train_pos = POS().readDataset(SparkContextForTest.spark,\n                                           os.getcwd() + ""/../src/test/resources/anc-pos-corpus-small/test-training.txt"",\n                                           delimiter=""|"", outputPosCol=""tags"", outputDocumentCol=""document"",\n                                           outputTextCol=""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n\n        pos_tagger = PerceptronApproach() \\\n            .setInputCols([""token"", ""sentence""]) \\\n            .setOutputCol(""pos"") \\\n            .setIterations(1) \\\n            .fit(self.train_pos)\n\n        dependency_parser = DependencyParserApproach() \\\n            .setInputCols([""sentence"", ""pos"", ""token""]) \\\n            .setOutputCol(""dependency"") \\\n            .setDependencyTreeBank(self.tree_bank) \\\n            .setNumberOfIterations(10)\n\n        typed_dependency_parser = TypedDependencyParserApproach() \\\n            .setInputCols([""token"", ""pos"", ""dependency""]) \\\n            .setOutputCol(""labdep"") \\\n            .setConll2009(self.conll2009) \\\n            .setNumberOfIterations(10)\n\n        assembled = document_assembler.transform(self.data)\n        sentenced = sentence_detector.transform(assembled)\n        tokenized = tokenizer.fit(sentenced).transform(sentenced)\n        pos_tagged = pos_tagger.transform(tokenized)\n        dependency_parsed = dependency_parser.fit(pos_tagged).transform(pos_tagged)\n        typed_dependency_parsed = typed_dependency_parser.fit(dependency_parsed).transform(dependency_parsed)\n        typed_dependency_parsed.show()\n\n\nclass ChunkDocSerializingTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkContextForTest.spark \\\n            .createDataFrame([[""I saw a girl with a telescope""]]).toDF(""text"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n        entity_extractor = TextMatcher() \\\n            .setOutputCol(""entity"") \\\n            .setEntities(path=""file:///"" + os.getcwd() + ""/../src/test/resources/entity-extractor/test-chunks.txt"")\n        chunk2doc = Chunk2Doc() \\\n            .setInputCols([\'entity\']) \\\n            .setOutputCol(\'entity_doc\')\n        doc2chunk = Doc2Chunk() \\\n            .setInputCols([\'entity_doc\']) \\\n            .setOutputCol(\'entity_rechunk\')\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            tokenizer,\n            entity_extractor,\n            chunk2doc,\n            doc2chunk\n        ])\n\n        model = pipeline.fit(self.data)\n        pipe_path = ""file:///"" + os.getcwd() + ""/tmp_chunkdoc""\n        model.write().overwrite().save(pipe_path)\n        PipelineModel.load(pipe_path)\n\n\nclass SentenceEmbeddingsTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkContextForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/embeddings/sentence_embeddings.csv"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        glove = WordEmbeddingsModel.pretrained() \\\n            .setInputCols([""sentence"", ""token""]) \\\n            .setOutputCol(""embeddings"")\n        sentence_embeddings = SentenceEmbeddings() \\\n            .setInputCols([""sentence"", ""embeddings""]) \\\n            .setOutputCol(""sentence_embeddings"") \\\n            .setPoolingStrategy(""AVERAGE"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            glove,\n            sentence_embeddings\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).show()\n\n\nclass StopWordsCleanerTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkContextForTest.spark.createDataFrame([\n            [""This is my first sentence. This is my second.""],\n            [""This is my third sentence. This is my forth.""]]) \\\n            .toDF(""text"").cache()\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        stop_words_cleaner = StopWordsCleaner() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""cleanTokens"") \\\n            .setCaseSensitive(False) \\\n            .setStopWords([""this"", ""is""])\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            stop_words_cleaner\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).select(""cleanTokens.result"").show()\n\n\nclass NGramGeneratorTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkContextForTest.spark.createDataFrame([\n            [""This is my first sentence. This is my second.""],\n            [""This is my third sentence. This is my forth.""]]) \\\n            .toDF(""text"").cache()\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        ngrams = NGramGenerator() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""ngrams"") \\\n            .setN(2)\n        ngrams_cum = NGramGenerator() \\\n            .setInputCols([""token""]) \\\n            .setOutputCol(""ngrams_cum"") \\\n            .setN(2) \\\n            .setEnableCumulative(True)\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            ngrams,\n            ngrams_cum,\n        ])\n\n        model = pipeline.fit(self.data)\n        transformed_data = model.transform(self.data)\n        transformed_data.select(""ngrams.result"", ""ngrams_cum.result"").show(2, False)\n\n        assert transformed_data.select(""ngrams.result"").rdd.flatMap(lambda x: x).collect() == \\\n               [[\'This is\', \'is my\', \'my first\', \'first sentence\', \'sentence .\', \'This is\', \'is my\', \'my second\', \'second .\'], [\'This is\', \'is my\', \'my third\', \'third sentence\', \'sentence .\', \'This is\', \'is my\', \'my forth\', \'forth .\']]\n\n        assert transformed_data.select(""ngrams_cum.result"").rdd.flatMap(lambda x: x).collect() == \\\n               [[\'This\', \'is\', \'my\', \'first\', \'sentence\', \'.\', \'This is\', \'is my\', \'my first\', \'first sentence\', \'sentence .\', \'This\', \'is\', \'my\', \'second\', \'.\', \'This is\', \'is my\', \'my second\', \'second .\'], [\'This\', \'is\', \'my\', \'third\', \'sentence\', \'.\', \'This is\', \'is my\', \'my third\', \'third sentence\', \'sentence .\', \'This\', \'is\', \'my\', \'forth\', \'.\', \'This is\', \'is my\', \'my forth\', \'forth .\']]\n\n\nclass ChunkEmbeddingsTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkContextForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/embeddings/sentence_embeddings.csv"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        pos_tagger = PerceptronModel.pretrained() \\\n            .setInputCols([""token"", ""sentence""]) \\\n            .setOutputCol(""pos"")\n        chunker = Chunker() \\\n            .setInputCols([""sentence"", ""pos""]) \\\n            .setOutputCol(""chunk"") \\\n            .setRegexParsers([""<DT>?<JJ>*<NN>+""])\n        glove = WordEmbeddingsModel.pretrained() \\\n            .setInputCols([""sentence"", ""token""]) \\\n            .setOutputCol(""embeddings"")\n        chunk_embeddings = ChunkEmbeddings() \\\n            .setInputCols([""chunk"", ""embeddings""]) \\\n            .setOutputCol(""chunk_embeddings"") \\\n            .setPoolingStrategy(""AVERAGE"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            pos_tagger,\n            chunker,\n            glove,\n            chunk_embeddings\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).show()\n\n\nclass EmbeddingsFinisherTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/embeddings/sentence_embeddings.csv"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        glove = WordEmbeddingsModel.pretrained() \\\n            .setInputCols([""sentence"", ""token""]) \\\n            .setOutputCol(""embeddings"")\n        sentence_embeddings = SentenceEmbeddings() \\\n            .setInputCols([""sentence"", ""embeddings""]) \\\n            .setOutputCol(""sentence_embeddings"") \\\n            .setPoolingStrategy(""AVERAGE"")\n        embeddings_finisher = EmbeddingsFinisher() \\\n            .setInputCols(""sentence_embeddings"") \\\n            .setOutputCols(""sentence_embeddings_vectors"") \\\n            .setOutputAsVector(True)\n        explode_vectors = SQLTransformer(statement=""SELECT EXPLODE(sentence_embeddings_vectors) AS features, * FROM __THIS__"")\n        kmeans = KMeans().setK(2).setSeed(1).setFeaturesCol(""features"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            glove,\n            sentence_embeddings,\n            embeddings_finisher,\n            explode_vectors,\n            kmeans\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).show()\n\n\nclass UniversalSentenceEncoderTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkSessionForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/embeddings/sentence_embeddings.csv"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        sentence_embeddings = UniversalSentenceEncoder.pretrained() \\\n            .setInputCols(""sentence"") \\\n            .setOutputCol(""sentence_embeddings"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            sentence_embeddings\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).show()\n\n\nclass ElmoEmbeddingsTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/embeddings/sentence_embeddings.csv"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        elmo = ElmoEmbeddings.pretrained() \\\n            .setInputCols([""sentence"", ""token""]) \\\n            .setOutputCol(""embeddings"") \\\n            .setPoolingLayer(""word_emb"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            elmo\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).show()\n\n\nclass ClassifierDLTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkSessionForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/classifier/sentiment.csv"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        sentence_embeddings = UniversalSentenceEncoder.pretrained() \\\n            .setInputCols(""document"") \\\n            .setOutputCol(""sentence_embeddings"")\n\n        classifier = ClassifierDLApproach() \\\n            .setInputCols(""sentence_embeddings"") \\\n            .setOutputCol(""category"") \\\n            .setLabelColumn(""label"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_embeddings,\n            classifier\n        ])\n\n        model = pipeline.fit(self.data)\n        model.stages[-1].write().overwrite().save(\'./tmp_classifierDL_model\')\n\n        classsifierdlModel = ClassifierDLModel.load(""./tmp_classifierDL_model"") \\\n            .setInputCols([""sentence_embeddings""]) \\\n            .setOutputCol(""class"")\n\n\nclass AlbertEmbeddingsTestSpec(unittest.TestCase):\n\n    def setUp(self):\n        self.data = SparkContextForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/embeddings/sentence_embeddings.csv"")\n\n    def runTest(self):\n\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        albert = AlbertEmbeddings.pretrained() \\\n            .setInputCols([""sentence"", ""token""]) \\\n            .setOutputCol(""embeddings"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            albert\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).show()\n\n\nclass SentimentDLTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkSessionForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/classifier/sentiment.csv"")\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n\n        sentence_embeddings = UniversalSentenceEncoder.pretrained() \\\n            .setInputCols(""document"") \\\n            .setOutputCol(""sentence_embeddings"")\n\n        classifier = SentimentDLApproach() \\\n            .setInputCols(""sentence_embeddings"") \\\n            .setOutputCol(""category"") \\\n            .setLabelColumn(""label"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_embeddings,\n            classifier\n        ])\n\n        model = pipeline.fit(self.data)\n        model.stages[-1].write().overwrite().save(\'./tmp_sentimentDL_model\')\n\n        sentimentdlModel = SentimentDLModel.load(""./tmp_sentimentDL_model"") \\\n            .setInputCols([""sentence_embeddings""]) \\\n            .setOutputCol(""class"")\n\n\nclass XlnetEmbeddingsTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkContextForTest.spark.read.option(""header"", ""true"") \\\n            .csv(path=""file:///"" + os.getcwd() + ""/../src/test/resources/embeddings/sentence_embeddings.csv"")\n\n    def runTest(self):\n        sentence_detector = SentenceDetector() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""sentence"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""sentence""]) \\\n            .setOutputCol(""token"")\n        xlnet = XlnetEmbeddings.pretrained() \\\n            .setInputCols([""sentence"", ""token""]) \\\n            .setOutputCol(""embeddings"")\n\n        pipeline = Pipeline(stages=[\n            document_assembler,\n            sentence_detector,\n            tokenizer,\n            xlnet\n        ])\n\n        model = pipeline.fit(self.data)\n        model.transform(self.data).show()\n\n'"
python/test/base.py,0,"b'import unittest\nfrom sparknlp.annotator import *\nfrom sparknlp.base import *\nfrom test.util import SparkContextForTest\n\n""""""\n----\nCREATE THE FOLLOWING SCALA CLASS IN ORDER TO RUN THIS TEST\n----\n\npackage com.johnsnowlabs.nlp\n\nimport com.johnsnowlabs.nlp.annotators.TokenizerModel\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.sql.Dataset\n\nclass SomeApproachTest(override val uid: String) extends AnnotatorApproach[SomeModelTest] with HasRecursiveFit[SomeModelTest] {\n  override val description: String = ""Some Approach""\n\n  override def train(dataset: Dataset[_], recursivePipeline: Option[PipelineModel]): SomeModelTest = {\n    require(recursivePipeline.isDefined)\n    require(recursivePipeline.get.stages.length == 2)\n    require(recursivePipeline.get.stages.last.isInstanceOf[TokenizerModel])\n    new SomeModelTest()\n  }\n\n  override val inputAnnotatorTypes: Array[String] = Array(AnnotatorType.TOKEN)\n  override val outputAnnotatorType: AnnotatorType = ""BAR""\n}\n\nclass SomeModelTest(override val uid: String) extends AnnotatorModel[SomeModelTest] with HasRecursiveTransform[SomeModelTest] {\n\n  def this() = this(""bar_uid"")\n\n  override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = {\n    require(recursivePipeline.isDefined)\n    require(recursivePipeline.get.stages.length == 2)\n    require(recursivePipeline.get.stages.last.isInstanceOf[TokenizerModel])\n    Seq.empty\n  }\n\n  override val inputAnnotatorTypes: Array[String] = Array(AnnotatorType.TOKEN)\n  override val outputAnnotatorType: AnnotatorType = ""BAR""\n}\n""""""\n\n\nclass SomeAnnotatorTest(AnnotatorApproach, HasRecursiveFit):\n\n    def __init__(self):\n        super(SomeAnnotatorTest, self).__init__(classname=""com.johnsnowlabs.nlp.SomeApproachTest"")\n\n    def _create_model(self, java_model):\n        return SomeModelTest(java_model=java_model)\n\n\nclass SomeModelTest(AnnotatorModel, HasRecursiveTransform):\n\n    def __init__(self, classname=""com.johnsnowlabs.nlp.SomeModelTest"", java_model=None):\n        super(SomeModelTest, self).__init__(\n            classname=classname,\n            java_model=java_model\n        )\n\n\nclass RecursiveTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.data = SparkContextForTest.data\n\n    def runTest(self):\n        document_assembler = DocumentAssembler() \\\n            .setInputCol(""text"") \\\n            .setOutputCol(""document"")\n        tokenizer = Tokenizer() \\\n            .setInputCols([""document""]) \\\n            .setOutputCol(""token"")\n        some_annotator = SomeAnnotatorTest() \\\n            .setInputCols([\'token\']) \\\n            .setOutputCol(\'baaar\')\n        pipeline = RecursivePipeline().setStages([document_assembler, tokenizer, some_annotator])\n        model = pipeline.fit(self.data)\n        RecursivePipelineModel(model).transform(self.data).show()\n'"
python/test/misc.py,0,"b'import unittest\nimport shutil\nimport tempfile\n\nfrom sparknlp.common import RegexRule\nfrom sparknlp.util import *\n\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\n\n\nclass UtilitiesTestSpec(unittest.TestCase):\n\n    @staticmethod\n    def runTest():\n        regex_rule = RegexRule(""\\w+"", ""word split"")\n        assert(regex_rule.rule() == ""\\w+"")\n\n\nclass SerializersTestSpec(unittest.TestCase):\n    def setUp(self):\n        self.test_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.test_dir)\n\n    def serialize_them(self, cls, dirname):\n        f = self.test_dir + dirname\n        c1 = cls()\n        c1.save(f)\n        c2 = cls().load(f)\n        assert(c1.uid == c2.uid)\n\n    def runTest(self):\n        self.serialize_them(DocumentAssembler, ""assembler"")\n        self.serialize_them(TokenAssembler, ""token_assembler"")\n        self.serialize_them(Finisher, ""finisher"")\n        self.serialize_them(Tokenizer, ""tokenizer"")\n        self.serialize_them(Stemmer, ""stemmer"")\n        self.serialize_them(Normalizer, ""normalizer"")\n        self.serialize_them(RegexMatcher, ""regex_matcher"")\n        self.serialize_them(Lemmatizer, ""lemmatizer"")\n        self.serialize_them(DateMatcher, ""date_matcher"")\n        self.serialize_them(TextMatcher, ""entity_extractor"")\n        self.serialize_them(PerceptronApproach, ""perceptron_approach"")\n        self.serialize_them(SentenceDetector, ""sentence_detector"")\n        self.serialize_them(SentimentDetector, ""sentiment_detector"")\n        self.serialize_them(ViveknSentimentApproach, ""vivekn"")\n        self.serialize_them(NorvigSweetingApproach, ""norvig"")\n        self.serialize_them(NerCrfApproach, ""ner_crf"")\n'"
python/test/util.py,0,"b'from pyspark.sql import SparkSession\n\nimport os\n\n\nclass SparkContextForTest:\n    spark = SparkSession.builder \\\n        .master(""local[4]"") \\\n        .config(""spark.jars"", \'lib/sparknlp.jar\') \\\n        .config(""spark.driver.memory"", ""6500M"") \\\n        .getOrCreate()\n    spark.sparkContext.setLogLevel(""WARN"")\n    data = spark. \\\n        read \\\n        .parquet(""file:///"" + os.getcwd() + ""/../src/test/resources/sentiment.parquet"") \\\n        .limit(100)\n    data.cache()\n    data.count()\n\n\nclass SparkSessionForTest:\n    spark = SparkSession.builder \\\n        .master(""local[*]"") \\\n        .config(""spark.jars"", \'lib/sparknlp.jar\') \\\n        .config(""spark.driver.memory"",""12G"") \\\n        .config(""spark.driver.maxResultSize"", ""2G"") \\\n        .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"") \\\n        .config(""spark.kryoserializer.buffer.max"", ""500m"") \\\n        .getOrCreate()\n    spark.sparkContext.setLogLevel(""WARN"")\n\n'"
python/com/johnsnowlabs/__init__.py,0,b''
python/tensorflow/ner/create_models.py,6,"b'import os\nimport sys\nimport tensorflow as tf\nimport ner_model\nimport argparse\n\n\ndef create_graph(output_path, number_of_tags, embeddings_dimension, number_of_chars, lstm_size=128):\n    if sys.version_info[0] != 3 or sys.version_info[1] >= 7:\n        raise Exception(\'Python 3.7 or above not supported by TensorFlow\')\n    if tf.__version__ != \'1.15.0\':\n        raise Exception(f\'Spark NLP is compiled with TensorFlow 1.15.0. Please use such version and not {tf.__version__}.\')\n    tf.reset_default_graph()\n    name_prefix = \'blstm\'\n    model_name = name_prefix+\'_{}_{}_{}_{}\'.format(number_of_tags, embeddings_dimension, lstm_size, number_of_chars)\n    with tf.Session() as session:\n        ner = ner_model.NerModel(session=None)\n        ner.add_cnn_char_repr(number_of_chars, 25, 30)\n        ner.add_bilstm_char_repr(number_of_chars, 25, 30)\n        ner.add_pretrained_word_embeddings(embeddings_dimension)\n        ner.add_context_repr(number_of_tags, lstm_size, 3)\n        ner.add_inference_layer(True)\n        ner.add_training_op(5)\n        ner.init_variables()\n        tf.train.Saver()\n        file_name = model_name + \'.pb\'\n        tf.train.write_graph(ner.session.graph, output_path, file_name, False)\n        ner.close()\n        session.close()\n        print(f\'Graph created successfully\')\n\n\ndef main(arguments):\n    use_contrib = False if os.name == \'nt\' else arguments.use_contrib\n    create_graph(arguments.output, arguments.n_tags, arguments.e_dim, arguments.n_chars)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Generates a tensorflow graph for latter use in NERDLApproach setting setGraphFolder parameter.\',\n        epilog=\'\')\n    parser.add_argument(\'--use_contrib\', action=\'store_true\',\n                        help=\'Whether to use contrib LSTM Cells. Improves accuracy (Linux or Mac only)\')\n    parser.add_argument(\'n_tags\', type=int, help=\'Number of tags\')\n    parser.add_argument(\'e_dim\', type=int, help=\'Embeddings dimension\')\n    parser.add_argument(\'n_chars\', type=int, help=\'Number of chars\')\n    parser.add_argument(\'output\', type=str, help=\'Output path location for the graph\')\n    args = parser.parse_args()\n    main(args)\n\n'"
python/com/johnsnowlabs/nlp/__init__.py,0,"b""import sys\n\nif sys.version_info[0] == 2:\n    import sparknlp.annotator\n    sys.modules['com.johnsnowlabs.nlp'] = sparknlp.annotator\nelse:\n    import sparknlp\n    sys.modules['com.johnsnowlabs.nlp'] = sparknlp\n"""
python/tensorflow/lib/ner/dataset_encoder.py,0,"b'import numpy as np\nimport os\nimport tensorflow as tf\nimport string\nimport random\nimport math\nimport sys\n\nclass DatasetEncoder:\n    # Each sentence must be array of tuple (word, tag)\n    def __init__(self, embeddings_resolver, tag2id = {\'O\': 0}, piece_tag = \'[X]\'):\n        self.char2id = {c:i + 1 for i, c in enumerate(string.printable)}\n        self.tag2id = tag2id\n        self.embeddings_resolver = embeddings_resolver\n        self.piece_tag = piece_tag\n        \n    def shuffle(self):\n        random.shuffle(self.sentences)\n    \n    @staticmethod\n    def normalize(word):\n        return word.strip().lower()\n    \n    def get_char_indexes(self, word):\n        result = []\n        for c in word:\n            char_id = self.char2id.get(c, len(self.char2id) - 1)\n            result.append(char_id)\n\n        return result\n    \n    def encode(self, sentences, output=False):\n        for sentence in sentences:\n            dataset_words = [word for (word, tag) in sentence]\n            word_embeddings = self.embeddings_resolver.resolve_sentence(dataset_words)\n            \n            # Zip Embeddings and Tags\n            words = []\n            tags = []\n            char_ids = []\n            tag_ids = []\n            is_word_start = []\n            embeddings = []\n            \n            i = 0\n            \n            for item in word_embeddings:\n                words.append(item.piece)\n                                \n                if item.is_word_start:\n                    assert i < len(sentence), \'i = {} is more or equal than length of {}, during zip with {}\'.format(i, sentence, word_embeddings)\n                    tag = sentence[i][1]\n                    i += 1\n                else:\n                    tag = self.piece_tag\n                \n                tag_id = self.tag2id.get(tag, len(self.tag2id))\n                self.tag2id[tag] = tag_id\n                \n                tags.append(tag)\n                tag_ids.append(tag_id)\n\n                embeddings.append(item.vector)\n                is_word_start.append(item.is_word_start)\n                \n                char_ids.append(self.get_char_indexes(item.piece))\n               \n            if len(sentence) > 0:\n                yield {\n                        ""words"": words,\n                        ""tags"": tags,\n                        ""char_ids"": char_ids,\n                        ""tag_ids"": tag_ids,\n                        ""is_word_start"": is_word_start,\n                        ""word_embeddings"": np.array(embeddings, dtype=np.float16)\n                    }\n\n    \n'"
python/tensorflow/lib/ner/embeddings_resolver.py,4,"b'import shutil\nimport numpy as np\nimport plyvel\nimport os.path\nimport sys\nsys.path.append(\'../\')\nfrom bert.modeling import *\nfrom bert.tokenization import *\nimport json\nimport os.path\nimport numpy as np\n\n\nclass TokenEmbeddings:\n    def __init__(self, piece, is_word_start, vector):\n        self.piece = piece\n        self.is_word_start = is_word_start\n        self.vector = vector\n    \n    @staticmethod\n    def create_sentence(pieces, is_word_starts, embeddings):\n        # Array of TokenEmbeddings\n        return [TokenEmbeddings(piece, is_start, vector)\n            for (piece, is_start, vector) in zip(pieces, is_word_starts, embeddings)]\n    \n    def __str__(self):\n        return \'TokenEmbeddings({}, {}, [{}])\'.format(self.piece, self.is_word_start, np.shape(self.vector))\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass EmbeddingsDbResolver:\n    \n    @staticmethod\n    def get_index_name(prefix, dim):\n        return prefix + \'-\' + str(dim)\n    \n    @staticmethod\n    def read_from_file(glove_file, dim, index_file = \'embeddings_index\', \n                       lowercase=False, clear_if_exists = False):\n        \n        full_index_file = EmbeddingsDbResolver.get_index_name(index_file, dim)\n        try:\n            resolver = None\n\n            index_existed = os.path.exists(full_index_file) and not clear_if_exists\n            resolver = EmbeddingsDbResolver(dim, index_file, lowercase, clear_if_exists)\n\n            if not index_existed:\n                resolver.read_glove(glove_file)\n\n            return resolver\n        except:\n            if resolver and resolver.db:\n                resolver.close()\n            \n            raise()\n            \n    def read_glove(self, glove_file):\n        portion = 500000\n        print(\'reading file: \', glove_file)\n        wb = None\n        with open(glove_file, encoding=\'utf-8\') as f:\n            for num, line in enumerate(f):\n                items = line.split(\' \')\n                word = items[0]\n                vector = [float(x) for x in items[1:]]\n                if num % portion == portion - 1:\n                    print(\'read lines: {}\'.format(num))\n                    wb.write()\n                    wb = None\n                \n                if not wb:\n                    wb = self.db.write_batch()\n\n                self.add_vector(word, vector, wb)\n            if wb:\n                wb.write()\n        \n    def __init__(self, dim, index_file = \'embeddings_index\', lowercase = False, clear_if_exists=False):        \n        full_index_file = EmbeddingsDbResolver.get_index_name(index_file, dim)\n        \n        if clear_if_exists and os.path.exists(full_index_file):\n            shutil.rmtree(db_index)\n        \n        dummy_added = False\n        self.db = plyvel.DB(full_index_file, create_if_missing=True)\n        self.add_vector(""__oov__"", [0.] * dim)\n        self.lowercase = lowercase\n        \n    def get_embeddings(self, word):\n        word = word.strip()\n        if self.lowercase:\n            word = word.lower()\n            \n        result = self.db.get(word.encode()) or self.db.get(\'__oov__\'.encode())\n        return np.frombuffer(result)\n    \n    def resolve_sentence(self, sentence):\n        """"""\n        sentence - array of words\n        """"""\n        embeddings =  list([self.get_embeddings(word) for word in sentence])\n        is_word_start = [True] * len(sentence)\n        \n        return TokenEmbeddings.create_sentence(sentence, is_word_start, embeddings)\n\n            \n    def add_vector(self, word, vector, wb = None):\n        array = np.array(vector)\n        if wb:\n            wb.put(word.encode(), array.tobytes())\n        else:\n            self.db.put(word.encode(), array.tobytes())\n    \n    def close(self):\n        self.db.close()\n        \n\nclass BertEmbeddingsResolver:\n    \n    def __init__(self, model_folder, max_length = 256, lowercase = True):\n        \n        # 1. Create tokenizer\n        self.max_length = max_length\n        vocab_file = os.path.join(model_folder, \'vocab.txt\')\n        self.tokenizer = FullTokenizer(vocab_file, do_lower_case = lowercase)\n        \n        # 2. Read Config\n        config_file = os.path.join(model_folder, \'bert_config.json\')        \n        self.config = BertConfig.from_json_file(config_file)\n        \n        # 3. Create Model\n        self.session = tf.Session()\n        self.token_ids_op = tf.placeholder(tf.int32, shape=(None, max_length), name=\'token_ids\')\n        self.model = BertModel(config = self.config, \n                          is_training = False, \n                          input_ids = self.token_ids_op, \n                          use_one_hot_embeddings = False)\n        \n        # 4. Restore Trained Model\n        self.saver = tf.train.Saver()\n        ckpt_file = os.path.join(model_folder, \'bert_model.ckpt\')\n        self.saver.restore(self.session, ckpt_file)\n        \n        hidden_layers = self.config.num_hidden_layers\n        self.embeddings_op = tf.get_default_graph().get_tensor_by_name(\n            ""bert/encoder/Reshape_{}:0"".format(hidden_layers + 1))\n        \n    def tokenize_sentence(self, tokens, add_service_tokens = True):\n        result = []\n        is_word_start = []\n        for token in tokens:\n            pieces = self.tokenizer.tokenize(token)\n            result.extend(pieces)\n            starts = [False] * len(pieces)\n            starts[0] = True\n            is_word_start.extend(starts)\n\n        if add_service_tokens:\n            if len(result) > self.max_length - 2:\n                result = result[:self.max_length -2]\n                is_word_start = is_word_start[:self.max_length -2]\n            \n            result = [\'[CLS]\'] + result + [\'[SEP]\']\n            is_word_start = [False] + is_word_start + [False]\n        else:\n            if len(result) > self.max_length:\n                result = result[:self.max_length]\n                is_word_start = is_word_start[:self.max_length]\n        \n        return (result, is_word_start)\n\n    def resolve_sentences(self, sentences):\n        batch_is_word_start = []\n        batch_token_ids = []\n        batch_tokens = []\n        \n        for sentence in sentences:\n            tokens, is_word_start = self.tokenize_sentence(sentence)\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            to_input = np.pad(token_ids, [(0, self.max_length - len(token_ids))], mode=\'constant\')\n            batch_token_ids.append(to_input)\n            batch_tokens.append(tokens)\n            batch_is_word_start.append(is_word_start)\n\n        embeddings = self.session.run(self.embeddings_op, feed_dict = {self.token_ids_op: batch_token_ids})\n        \n        result = []\n        for i in range(len(sentences)):\n            tokens = batch_tokens[i]\n            is_word_start = batch_is_word_start[i]\n            item_embeddings = embeddings[i, :len(tokens), :]\n\n            resolved = TokenEmbeddings.create_sentence(tokens, is_word_start, item_embeddings)\n            result.append(resolved)\n        \n        return result\n\n    \n    def resolve_sentence(self, sentence):\n        tokens, is_word_start = self.tokenize_sentence(sentence)\n        \n        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        to_input = np.pad(token_ids, [(0, self.max_length - len(token_ids))], mode=\'constant\')\n        to_input = to_input.reshape((1, self.max_length))\n\n        embeddings = self.session.run(self.embeddings_op, feed_dict = {self.token_ids_op: to_input})\n        embeddings = np.squeeze(embeddings)\n        embeddings = embeddings[:len(token_ids), :]\n\n        return TokenEmbeddings.create_sentence(tokens, is_word_start, embeddings)\n        '"
python/tensorflow/lib/ner/ner_model.py,102,"b'import numpy as np\nimport tensorflow as tf\nimport random\nimport math\nimport sys\nfrom sentence_grouper import SentenceGrouper\n\n\nclass NerModel:\n    # If session is not defined than default session will be used\n    def __init__(self, session=None, dummy_tags=None, use_contrib=True, use_gpu_device=0):\n        tf.disable_v2_behavior()\n\n        self.word_repr = None\n        self.word_embeddings = None\n        self.session = session\n        self.session_created = False\n        self.dummy_tags = dummy_tags or []\n        self.use_contrib = use_contrib\n        self.use_gpu_device = use_gpu_device\n\n        if self.session is None:\n            self.session_created = True\n            self.session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(\n                allow_soft_placement=True,\n                log_device_placement=True))\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n\n            with tf.compat.v1.variable_scope(""char_repr"") as scope:\n                # shape = (batch size, sentence, word)\n                self.char_ids = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None], name=""char_ids"")\n\n                # shape = (batch_size, sentence)\n                self.word_lengths = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name=""word_lengths"")\n\n            with tf.compat.v1.variable_scope(""word_repr"") as scope:\n                # shape = (batch size)\n                self.sentence_lengths = tf.compat.v1.placeholder(tf.int32, shape=[None], name=""sentence_lengths"")\n\n            with tf.compat.v1.variable_scope(""training"", reuse=None) as scope:\n                # shape = (batch, sentence)\n                self.labels = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name=""labels"")\n\n                self.lr = tf.compat.v1.placeholder_with_default(0.005,  shape=(), name=""lr"")\n                self.dropout = tf.compat.v1.placeholder(tf.float32, shape=(), name=""dropout"")\n\n        self._char_bilstm_added = False\n        self._char_cnn_added = False\n        self._word_embeddings_added = False\n        self._context_added = False\n        self._encode_added = False\n\n    def add_bilstm_char_repr(self, nchars=101, dim=25, hidden=25):\n        self._char_bilstm_added = True\n\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n\n            with tf.compat.v1.variable_scope(""char_repr_lstm"") as scope:\n                # 1. Lookup for character embeddings\n                char_range = math.sqrt(3 / dim)\n                embeddings = tf.compat.v1.get_variable(name=""char_embeddings"",\n                                                       dtype=tf.float32,\n                                                       shape=[nchars, dim],\n                                                       initializer=tf.compat.v1.random_uniform_initializer(\n                                                           -char_range,\n                                                           char_range\n                                                       ),\n                                                       use_resource=False)\n\n                # shape = (batch, sentence, word, char embeddings dim)\n                char_embeddings = tf.nn.embedding_lookup(params=embeddings, ids=self.char_ids)\n                # char_embeddings = tf.nn.dropout(char_embeddings, self.dropout)\n                s = tf.shape(input=char_embeddings)\n\n                # shape = (batch x sentence, word, char embeddings dim)\n                char_embeddings_seq = tf.reshape(char_embeddings, shape=[-1, s[-2], dim])\n\n                # shape = (batch x sentence)\n                word_lengths_seq = tf.reshape(self.word_lengths, shape=[-1])\n\n                # 2. Add Bidirectional LSTM\n                model = tf.keras.Sequential([\n                    tf.keras.layers.Bidirectional(\n                        layer=tf.keras.layers.LSTM(hidden, return_sequences=False),\n                        merge_mode=""concat""\n                    )\n                ])\n\n                inputs = char_embeddings_seq\n                mask = tf.expand_dims(tf.sequence_mask(word_lengths_seq, dtype=tf.float32), axis=-1)\n\n                # shape = (batch x sentence, 2 x hidden)\n                output = model(inputs, mask=mask)\n\n                # shape = (batch, sentence, 2 x hidden)\n                char_repr = tf.reshape(output, shape=[-1, s[1], 2*hidden])\n\n                if self.word_repr is not None:\n                    self.word_repr = tf.concat([self.word_repr, char_repr], axis=-1)\n                else:\n                    self.word_repr = char_repr\n\n    def add_cnn_char_repr(self, nchars=101, dim=25, nfilters=25, pad=2):\n        self._char_cnn_added = True\n\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n\n            with tf.compat.v1.variable_scope(""char_repr_cnn"") as scope:\n                # 1. Lookup for character embeddings\n                char_range = math.sqrt(3 / dim)\n                embeddings = tf.compat.v1.get_variable(name=""char_embeddings"", dtype=tf.float32,\n                                             shape=[nchars, dim],\n                                             initializer=tf.compat.v1.random_uniform_initializer(-char_range, char_range),\n                                             use_resource=False)\n\n                # shape = (batch, sentence, word_len, embeddings dim)\n                char_embeddings = tf.nn.embedding_lookup(params=embeddings, ids=self.char_ids)\n                # char_embeddings = tf.nn.dropout(char_embeddings, self.dropout)\n                s = tf.shape(input=char_embeddings)\n\n                # shape = (batch x sentence, word_len, embeddings dim)\n                char_embeddings = tf.reshape(char_embeddings, shape=[-1, s[-2], dim])\n\n                # batch x sentence, word_len, nfilters\n                conv1d = tf.keras.layers.Conv1D(\n                    filters=nfilters,\n                    kernel_size=[3],\n                    padding=\'same\',\n                    activation=tf.nn.relu\n                )(char_embeddings)\n\n                # Max across each filter, shape = (batch x sentence, nfilters)\n                char_repr = tf.reduce_max(input_tensor=conv1d, axis=1, keepdims=True)\n                char_repr = tf.squeeze(char_repr, axis=[1])\n\n                # (batch, sentence, nfilters)\n                char_repr = tf.reshape(char_repr, shape=[s[0], s[1], nfilters])\n\n                if self.word_repr is not None:\n                    self.word_repr = tf.concat([self.word_repr, char_repr], axis=-1)\n                else:\n                    self.word_repr = char_repr\n\n    def add_pretrained_word_embeddings(self, dim=100):\n        self._word_embeddings_added = True\n\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n            with tf.compat.v1.variable_scope(""word_repr"") as scope:\n                # shape = (batch size, sentence, dim)\n                self.word_embeddings = tf.compat.v1.placeholder(tf.float32, shape=[None, None, dim],\n                                                      name=""word_embeddings"")\n\n                if self.word_repr is not None:\n                    self.word_repr = tf.concat([self.word_repr, self.word_embeddings], axis=-1)\n                else:\n                    self.word_repr = self.word_embeddings\n\n    def _create_lstm_layer(self, inputs, hidden_size, lengths):\n\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n\n            if not self.use_contrib:\n                model = tf.keras.Sequential([\n                    tf.keras.layers.Bidirectional(\n                        layer=tf.keras.layers.LSTM(hidden_size, return_sequences=False),\n                        merge_mode=""concat""\n                    )\n                ])\n\n                mask = tf.expand_dims(tf.sequence_mask(lengths, dtype=tf.float32), axis=-1)\n                # shape = (batch x sentence, 2 x hidden)\n                output = model(inputs, mask=mask)\n                # inputs shape = (batch, sentence, inp)\n                batch = tf.shape(input=lengths)[0]\n\n                return tf.reshape(output, shape=[batch, -1, 2*hidden_size])\n\n            time_based = tf.transpose(a=inputs, perm=[1, 0, 2])\n\n            cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(hidden_size, use_peephole=True)\n            cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(hidden_size, use_peephole=True)\n            cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(cell_bw)\n\n            output_fw, _ = cell_fw(time_based, dtype=tf.float32, sequence_length=lengths)\n            output_bw, _ = cell_bw(time_based, dtype=tf.float32, sequence_length=lengths)\n\n            result = tf.concat([output_fw, output_bw], axis=-1)\n            return tf.transpose(a=result, perm=[1, 0, 2])\n\n    def _multiply_layer(self, source, result_size, activation=tf.nn.relu):\n\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n\n            ntime_steps = tf.shape(input=source)[1]\n            source_size = source.shape[2]\n\n            W = tf.compat.v1.get_variable(""W"", shape=[source_size, result_size],\n                                dtype=tf.float32,\n                                initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=""fan_avg"", distribution=""uniform""),\n                                use_resource=False)\n\n            b = tf.compat.v1.get_variable(""b"", shape=[result_size], dtype=tf.float32, use_resource=False)\n\n            # batch x time, source_size\n            source = tf.reshape(source, [-1, source_size])\n            # batch x time, result_size\n            result = tf.matmul(source, W) + b\n\n            result = tf.reshape(result, [-1, ntime_steps, result_size])\n            if activation:\n                result = activation(result)\n\n            return result\n\n    # Adds Bi LSTM with size of each cell hidden_size\n    def add_context_repr(self, ntags, hidden_size=100, height=1, residual=True):\n        assert(self._word_embeddings_added or self._char_cnn_added or self._char_bilstm_added,\n               ""Add word embeddings by method add_word_embeddings "" +\n               ""or add char representation by method add_bilstm_char_repr "" +\n               ""or add_bilstm_char_repr before adding context layer"")\n\n        self._context_added = True\n        self.ntags = ntags\n\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n            context_repr = self._multiply_layer(self.word_repr, 2*hidden_size)\n            # Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`\n            context_repr = tf.nn.dropout(x=context_repr, rate=1-self.dropout)\n\n            with tf.compat.v1.variable_scope(""context_repr"") as scope:\n                for i in range(height):\n                    with tf.compat.v1.variable_scope(\'lstm-{}\'.format(i)):\n                        new_repr = self._create_lstm_layer(context_repr, hidden_size,\n                                                           lengths=self.sentence_lengths)\n\n                        context_repr = new_repr + context_repr if residual else new_repr\n\n                context_repr = tf.nn.dropout(x=context_repr, rate=1-self.dropout)\n\n                # batch, sentence, ntags\n                self.scores = self._multiply_layer(context_repr, ntags, activation=None)\n\n                tf.identity(self.scores, ""scores"")\n\n                self.predicted_labels = tf.argmax(input=self.scores, axis=-1)\n                tf.identity(self.predicted_labels, ""predicted_labels"")\n\n    def add_inference_layer(self, crf=False):\n        assert(self._context_added,\n               ""Add context representation layer by method add_context_repr before adding inference layer"")\n        self._inference_added = True\n\n        with tf.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n\n            with tf.compat.v1.variable_scope(""inference"", reuse=None) as scope:\n\n                self.crf = tf.constant(crf, dtype=tf.bool, name=""crf"")\n\n                if crf:\n                    transition_params = tf.compat.v1.get_variable(""transition_params"",\n                                                        shape=[self.ntags, self.ntags],\n                                                        initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=""fan_avg"", distribution=""uniform""),\n                                                        use_resource=False)\n\n                    # CRF shape = (batch, sentence)\n                    log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(\n                        self.scores,\n                        self.labels,\n                        self.sentence_lengths,\n                        transition_params\n                    )\n\n                    tf.identity(log_likelihood, ""log_likelihood"")\n                    tf.identity(self.transition_params, ""transition_params"")\n\n                    self.loss = tf.reduce_mean(input_tensor=-log_likelihood)\n                    self.prediction, _ = tf.contrib.crf.crf_decode(self.scores, self.transition_params, self.sentence_lengths)\n\n                else:\n                    # Softmax\n                    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.labels)\n                    # shape = (batch, sentence, ntags)\n                    mask = tf.sequence_mask(self.sentence_lengths)\n                    # apply mask\n                    losses = tf.boolean_mask(tensor=losses, mask=mask)\n\n                    self.loss = tf.reduce_mean(input_tensor=losses)\n\n                    self.prediction = tf.math.argmax(input=self.scores, axis=-1)\n\n                tf.identity(self.loss, ""loss"")\n\n    # clip_gradient < 0  - no gradient clipping\n    def add_training_op(self, clip_gradient = 2.0):\n        assert(self._inference_added,\n               ""Add inference layer by method add_inference_layer before adding training layer"")\n        self._training_added = True\n\n        with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n\n            with tf.compat.v1.variable_scope(""training"", reuse=None) as scope:\n                optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr)\n                if clip_gradient > 0:\n                    gvs = optimizer.compute_gradients(self.loss)\n                    capped_gvs = [(tf.clip_by_value(grad, -clip_gradient, clip_gradient), var) for grad, var in gvs if grad is not None]\n                    self.train_op = optimizer.apply_gradients(capped_gvs)\n                else:\n                    self.train_op = optimizer.minimize(self.loss)\n\n                self.init_op = tf.compat.v1.variables_initializer(tf.compat.v1.global_variables(), name=""init"")\n\n    @staticmethod\n    def num_trues(array):\n        result = 0\n        for item in array:\n            if item == True:\n                result += 1\n\n        return result\n\n    @staticmethod\n    def fill(array, l, val):\n        result = array[:]\n        for i in range(l - len(array)):\n            result.append(val)\n        return result\n\n    @staticmethod\n    def get_sentence_lengths(batch, idx=""word_embeddings""):\n        return [len(row[idx]) for row in batch]\n\n    @staticmethod\n    def get_sentence_token_lengths(batch, idx=""tag_ids""):\n        return [len(row[idx]) for row in batch]\n\n    @staticmethod\n    def get_word_lengths(batch, idx=""char_ids""):\n        max_words = max([len(row[idx]) for row in batch])\n        return [NerModel.fill([len(chars) for chars in row[idx]], max_words, 0)\n                for row in batch]\n\n    @staticmethod\n    def get_char_ids(batch, idx=""char_ids""):\n        max_chars = max([max([len(char_ids) for char_ids in sentence[idx]]) for sentence in batch])\n        max_words = max([len(sentence[idx]) for sentence in batch])\n\n        return [\n            NerModel.fill(\n                [NerModel.fill(char_ids, max_chars, 0) for char_ids in sentence[idx]],\n                max_words, [0]*max_chars\n            )\n            for sentence in batch]\n\n    @staticmethod\n    def get_from_batch(batch, idx):\n        k = max([len(row[idx]) for row in batch])\n        return list([NerModel.fill(row[idx], k, 0) for row in batch])\n\n    @staticmethod\n    def get_tag_ids(batch, idx=""tag_ids""):\n        return NerModel.get_from_batch(batch, idx)\n\n    @staticmethod\n    def get_word_embeddings(batch, idx=""word_embeddings""):\n        embeddings_dim = len(batch[0][idx][0])\n        max_words = max([len(sentence[idx]) for sentence in batch])\n        return [\n            NerModel.fill([word_embedding for word_embedding in sentence[idx]],\n                          max_words, [0]*embeddings_dim\n                          )\n            for sentence in batch]\n\n    @staticmethod\n    def slice(dataset, batch_size=10):\n        grouper = SentenceGrouper([5, 10, 20, 50])\n        return grouper.slice(dataset, batch_size)\n\n    def init_variables(self):\n        self.session.run(self.init_op)\n\n    def train(self, train,\n              epoch_start=0,\n              epoch_end=100,\n              batch_size=32,\n              lr=0.01,\n              po=0,\n              dropout=0.65,\n              init_variables=False\n              ):\n\n        assert(self._training_added, ""Add training layer by method add_training_op before running training"")\n\n        if init_variables:\n            with tf.compat.v1.device(\'/gpu:{}\'.format(self.use_gpu_device)):\n                self.session.run(tf.compat.v1.global_variables_initializer())\n\n        print(\'trainig started\')\n        for epoch in range(epoch_start, epoch_end):\n            random.shuffle(train)\n            sum_loss = 0\n            for batch in NerModel.slice(train, batch_size):\n                feed_dict = {\n                    self.sentence_lengths: NerModel.get_sentence_lengths(batch),\n                    self.word_embeddings: NerModel.get_word_embeddings(batch),\n\n                    self.word_lengths: NerModel.get_word_lengths(batch),\n                    self.char_ids: NerModel.get_char_ids(batch),\n                    self.labels: NerModel.get_tag_ids(batch),\n\n                    self.dropout: dropout,\n                    self.lr: lr / (1 + po * epoch)\n                }\n                mean_loss, _ = self.session.run([self.loss, self.train_op], feed_dict=feed_dict)\n                sum_loss += mean_loss\n\n            print(""epoch {}"".format(epoch))\n            print(""mean loss: {}"".format(sum_loss))\n            print()\n            sys.stdout.flush()\n\n    def measure(self, dataset, batch_size=20, dropout=1.0):\n        predicted = {}\n        correct = {}\n        correct_predicted = {}\n\n        for batch in NerModel.slice(dataset, batch_size):\n            tags_ids = NerModel.get_tag_ids(batch)\n            sentence_lengths = NerModel.get_sentence_lengths(batch)\n\n            feed_dict = {\n                self.sentence_lengths: sentence_lengths,\n                self.word_embeddings: NerModel.get_word_embeddings(batch),\n\n                self.word_lengths: NerModel.get_word_lengths(batch),\n                self.char_ids: NerModel.get_char_ids(batch),\n                self.labels: tags_ids,\n\n                self.dropout: dropout\n            }\n\n            prediction = self.session.run(self.prediction, feed_dict=feed_dict)\n            batch_prediction = np.reshape(prediction, (len(batch), -1))\n\n            for i in range(len(batch)):\n                is_word_start = batch[i][\'is_word_start\']\n\n                for word in range(sentence_lengths[i]):\n                    if not is_word_start[word]:\n                        continue\n\n                    p = batch_prediction[i][word]\n                    c = tags_ids[i][word]\n\n                    if c in self.dummy_tags:\n                        continue\n\n                    predicted[p] = predicted.get(p, 0) + 1\n                    correct[c] = correct.get(c, 0) + 1\n                    if p == c:\n                        correct_predicted[p] = correct_predicted.get(p, 0) + 1\n\n        num_correct_predicted = sum([correct_predicted.get(i, 0) for i in range(1, self.ntags)])\n        num_predicted = sum([predicted.get(i, 0) for i in range(1, self.ntags)])\n        num_correct = sum([correct.get(i, 0) for i in range(1, self.ntags)])\n\n        prec = num_correct_predicted / (num_predicted or 1.)\n        rec = num_correct_predicted / (num_correct or 1.)\n\n        f1 = 2 * prec * rec / (rec + prec)\n\n        return prec, rec, f1\n\n    @staticmethod\n    def get_softmax(scores, threshold=None):\n        exp_scores = np.exp(scores)\n\n        for batch in exp_scores:\n            for sentence in exp_scores:\n                for i in range(len(sentence)):\n                    probabilities = sentence[i] / np.sum(sentence[i])\n                    sentence[i] = [p if threshold is None or p >= threshold else 0 for p in probabilities]\n\n        return exp_scores\n\n    def predict(self, sentences, batch_size=20, threshold=None):\n        result = []\n\n        for batch in NerModel.slice(sentences, batch_size):\n            sentence_lengths = NerModel.get_sentence_lengths(batch)\n\n            feed_dict = {\n                self.sentence_lengths: sentence_lengths,\n                self.word_embeddings: NerModel.get_word_embeddings(batch),\n\n                self.word_lengths: NerModel.get_word_lengths(batch),\n                self.char_ids: NerModel.get_char_ids(batch),\n\n                self.dropout: 1.1\n            }\n\n            prediction = self.session.run(self.prediction, feed_dict=feed_dict)\n            batch_prediction = np.reshape(prediction, (len(batch), -1))\n\n            for i in range(len(batch)):\n                sentence = []\n                for word in range(sentence_lengths[i]):\n                    tag = batch_prediction[i][word]\n                    sentence.append(tag)\n\n                result.append(sentence)\n\n        return result\n\n    def close(self):\n        if self.session_created:\n            self.session.close()\n'"
python/tensorflow/lib/ner/ner_model_saver.py,7,"b'import numpy as np\nimport os\nimport tensorflow as tf\nimport string\nimport random\nimport math\nimport sys\n\nclass NerModelSaver:\n    def __init__(self, ner, encoder, embeddings_file = None):\n        self.ner = ner\n        self.encoder = encoder\n        self.embeddings_file = embeddings_file\n\n    @staticmethod\n    def restore_tensorflow_state(session, export_dir):\n        with tf.device(\'/gpu:0\'):\n            saveNodes = list([n.name for n in tf.get_default_graph().as_graph_def().node if n.name.startswith(\'save/\')])\n            if len(saveNodes) == 0:\n                saver = tf.train.Saver()\n\n            variables_file = os.path.join(export_dir, \'variables\')\n            session.run(""save/restore_all"", feed_dict={\'save/Const:0\': variables_file})\n        \n    def save_models(self, folder):\n        with tf.device(\'/gpu:0\'):\n            saveNodes = list([n.name for n in tf.get_default_graph().as_graph_def().node if n.name.startswith(\'save/\')])\n            if len(saveNodes) == 0:\n                saver = tf.train.Saver()\n\n            variables_file = os.path.join(folder, \'variables\')\n            self.ner.session.run(\'save/control_dependency\', feed_dict={\'save/Const:0\': variables_file})\n            tf.train.write_graph(self.ner.session.graph, folder, \'saved_model.pb\', False)\n\n     \n    def save(self, export_dir):\n        def save_tags(file):\n            id2tag = {id:tag for (tag, id) in self.encoder.tag2id.items()}\n\n            with open(file, \'w\') as f:\n                for i in range(len(id2tag)):\n                    tag = id2tag[i]\n                    f.write(tag)\n                    f.write(\'\\n\')\n\n\n        def save_embeddings(src, dst):\n            from shutil import copyfile\n            copyfile(src, dst)\n            with open(dst + \'.meta\', \'w\') as f:\n                embeddings = self.encoder.embeddings\n                dim = len(embeddings[0]) if embeddings else 0\n                f.write(str(dim))\n        \n        def save_chars(file):\n            id2char = {id:char for (char, id) in self.encoder.char2id.items()}\n            with open(file, \'w\') as f:\n                for i in range(1, len(id2char) + 1):\n                    f.write(id2char[i])\n        \n        \n        save_models(export_dir)\n        save_tags(os.path.join(export_dir, \'tags.csv\'))\n        \n        if self.embeddings_file:\n            save_embeddings(self.embeddings_file, os.path.join(export_dir, \'embeddings\'))\n            \n        save_chars(os.path.join(export_dir, \'chars.csv\'))\n\n'"
python/tensorflow/lib/ner/sentence_grouper.py,0,"b""class SentenceGrouper:\n    def __init__(self, bucket_lengths):\n        self.bucket_lengths = bucket_lengths\n        \n    def get_bucket_id(self, length):\n        for i, bucket_len in enumerate(self.bucket_lengths):\n            if length <= bucket_len:\n                return i\n        \n        return len(self.bucket_lengths)\n        \n    def slice(self, dataset, batch_size = 32):\n        buckets =  [[] for item in self.bucket_lengths]\n        buckets.append([])\n        \n        for entry in dataset:\n            length = len(entry['words'])\n            bucket_id = self.get_bucket_id(length)\n            buckets[bucket_id].append(entry)\n            \n            if len(buckets[bucket_id]) >= batch_size:\n                result = buckets[bucket_id][:]\n                yield result\n                buckets[bucket_id] = []\n        \n        for bucket in buckets:\n            if len(bucket) > 0:\n                yield bucket                \n"""
